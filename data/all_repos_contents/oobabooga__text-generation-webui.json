{"convert-to-safetensors.py": "'''\n\nConverts a transformers model to safetensors format and shards it.\n\nThis makes it faster to load (because of safetensors) and lowers its RAM usage\nwhile loading (because of sharding).\n\nBased on the original script by 81300:\n\nhttps://gist.github.com/81300/fe5b08bff1cba45296a829b9d6b0f303\n\n'''\n\nimport argparse\nfrom pathlib import Path\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nparser = argparse.ArgumentParser(formatter_class=lambda prog: argparse.HelpFormatter(prog, max_help_position=54))\nparser.add_argument('MODEL', type=str, default=None, nargs='?', help=\"Path to the input model.\")\nparser.add_argument('--output', type=str, default=None, help='Path to the output folder (default: models/{model_name}_safetensors).')\nparser.add_argument(\"--max-shard-size\", type=str, default=\"2GB\", help=\"Maximum size of a shard in GB or MB (default: %(default)s).\")\nparser.add_argument('--bf16', action='store_true', help='Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.')\nargs = parser.parse_args()\n\nif __name__ == '__main__':\n    path = Path(args.MODEL)\n    model_name = path.name\n\n    print(f\"Loading {model_name}...\")\n    model = AutoModelForCausalLM.from_pretrained(path, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if args.bf16 else torch.float16)\n    tokenizer = AutoTokenizer.from_pretrained(path)\n\n    out_folder = args.output or Path(f\"models/{model_name}_safetensors\")\n    print(f\"Saving the converted model to {out_folder} with a maximum shard size of {args.max_shard_size}...\")\n    model.save_pretrained(out_folder, max_shard_size=args.max_shard_size, safe_serialization=True)\n    tokenizer.save_pretrained(out_folder)\n", "one_click.py": "import argparse\nimport glob\nimport hashlib\nimport os\nimport platform\nimport re\nimport signal\nimport site\nimport subprocess\nimport sys\n\n# Remove the '# ' from the following lines as needed for your AMD GPU on Linux\n# os.environ[\"ROCM_PATH\"] = '/opt/rocm'\n# os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = '10.3.0'\n# os.environ[\"HCC_AMDGPU_TARGET\"] = 'gfx1030'\n\n\n# Define the required PyTorch version\nTORCH_VERSION = \"2.2.1\"\nTORCHVISION_VERSION = \"0.17.1\"\nTORCHAUDIO_VERSION = \"2.2.1\"\n\n# Environment\nscript_dir = os.getcwd()\nconda_env_path = os.path.join(script_dir, \"installer_files\", \"env\")\n\n# Command-line flags\ncmd_flags_path = os.path.join(script_dir, \"CMD_FLAGS.txt\")\nif os.path.exists(cmd_flags_path):\n    with open(cmd_flags_path, 'r') as f:\n        CMD_FLAGS = ' '.join(line.strip().rstrip('\\\\').strip() for line in f if line.strip().rstrip('\\\\').strip() and not line.strip().startswith('#'))\nelse:\n    CMD_FLAGS = ''\n\nflags = f\"{' '.join([flag for flag in sys.argv[1:] if flag != '--update-wizard'])} {CMD_FLAGS}\"\n\n\ndef signal_handler(sig, frame):\n    sys.exit(0)\n\n\nsignal.signal(signal.SIGINT, signal_handler)\n\n\ndef is_linux():\n    return sys.platform.startswith(\"linux\")\n\n\ndef is_windows():\n    return sys.platform.startswith(\"win\")\n\n\ndef is_macos():\n    return sys.platform.startswith(\"darwin\")\n\n\ndef is_x86_64():\n    return platform.machine() == \"x86_64\"\n\n\ndef cpu_has_avx2():\n    try:\n        import cpuinfo\n\n        info = cpuinfo.get_cpu_info()\n        if 'avx2' in info['flags']:\n            return True\n        else:\n            return False\n    except:\n        return True\n\n\ndef cpu_has_amx():\n    try:\n        import cpuinfo\n\n        info = cpuinfo.get_cpu_info()\n        if 'amx' in info['flags']:\n            return True\n        else:\n            return False\n    except:\n        return True\n\n\ndef torch_version():\n    site_packages_path = None\n    for sitedir in site.getsitepackages():\n        if \"site-packages\" in sitedir and conda_env_path in sitedir:\n            site_packages_path = sitedir\n            break\n\n    if site_packages_path:\n        torch_version_file = open(os.path.join(site_packages_path, 'torch', 'version.py')).read().splitlines()\n        torver = [line for line in torch_version_file if line.startswith('__version__')][0].split('__version__ = ')[1].strip(\"'\")\n    else:\n        from torch import __version__ as torver\n\n    return torver\n\n\ndef update_pytorch():\n    print_big_message(\"Checking for PyTorch updates\")\n\n    torver = torch_version()\n    is_cuda = '+cu' in torver\n    is_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n    is_rocm = '+rocm' in torver  # 2.0.1+rocm5.4.2\n    is_intel = '+cxx11' in torver  # 2.0.1a0+cxx11.abi\n    is_cpu = '+cpu' in torver  # 2.0.1+cpu\n\n    install_pytorch = f\"python -m pip install --upgrade torch=={TORCH_VERSION} torchvision=={TORCHVISION_VERSION} torchaudio=={TORCHAUDIO_VERSION} \"\n\n    if is_cuda118:\n        install_pytorch += \"--index-url https://download.pytorch.org/whl/cu118\"\n    elif is_cuda:\n        install_pytorch += \"--index-url https://download.pytorch.org/whl/cu121\"\n    elif is_rocm:\n        install_pytorch += \"--index-url https://download.pytorch.org/whl/rocm5.6\"\n    elif is_cpu:\n        install_pytorch += \"--index-url https://download.pytorch.org/whl/cpu\"\n    elif is_intel:\n        if is_linux():\n            install_pytorch = \"python -m pip install --upgrade torch==2.1.0a0 torchvision==0.16.0a0 torchaudio==2.1.0a0 intel-extension-for-pytorch==2.1.10+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\"\n        else:\n            install_pytorch = \"python -m pip install --upgrade torch==2.1.0a0 torchvision==0.16.0a0 torchaudio==2.1.0a0 intel-extension-for-pytorch==2.1.10 --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\"\n\n    run_cmd(f\"{install_pytorch}\", assert_success=True, environment=True)\n\n\ndef is_installed():\n    site_packages_path = None\n    for sitedir in site.getsitepackages():\n        if \"site-packages\" in sitedir and conda_env_path in sitedir:\n            site_packages_path = sitedir\n            break\n\n    if site_packages_path:\n        return os.path.isfile(os.path.join(site_packages_path, 'torch', '__init__.py'))\n    else:\n        return os.path.isdir(conda_env_path)\n\n\ndef check_env():\n    # If we have access to conda, we are probably in an environment\n    conda_exist = run_cmd(\"conda\", environment=True, capture_output=True).returncode == 0\n    if not conda_exist:\n        print(\"Conda is not installed. Exiting...\")\n        sys.exit(1)\n\n    # Ensure this is a new environment and not the base environment\n    if os.environ[\"CONDA_DEFAULT_ENV\"] == \"base\":\n        print(\"Create an environment for this project and activate it. Exiting...\")\n        sys.exit(1)\n\n\ndef clear_cache():\n    run_cmd(\"conda clean -a -y\", environment=True)\n    run_cmd(\"python -m pip cache purge\", environment=True)\n\n\ndef print_big_message(message):\n    message = message.strip()\n    lines = message.split('\\n')\n    print(\"\\n\\n*******************************************************************\")\n    for line in lines:\n        print(\"*\", line)\n\n    print(\"*******************************************************************\\n\\n\")\n\n\ndef calculate_file_hash(file_path):\n    p = os.path.join(script_dir, file_path)\n    if os.path.isfile(p):\n        with open(p, 'rb') as f:\n            return hashlib.sha256(f.read()).hexdigest()\n    else:\n        return ''\n\n\ndef run_cmd(cmd, assert_success=False, environment=False, capture_output=False, env=None):\n    # Use the conda environment\n    if environment:\n        if is_windows():\n            conda_bat_path = os.path.join(script_dir, \"installer_files\", \"conda\", \"condabin\", \"conda.bat\")\n            cmd = f'\"{conda_bat_path}\" activate \"{conda_env_path}\" >nul && {cmd}'\n        else:\n            conda_sh_path = os.path.join(script_dir, \"installer_files\", \"conda\", \"etc\", \"profile.d\", \"conda.sh\")\n            cmd = f'. \"{conda_sh_path}\" && conda activate \"{conda_env_path}\" && {cmd}'\n\n    # Run shell commands\n    result = subprocess.run(cmd, shell=True, capture_output=capture_output, env=env)\n\n    # Assert the command ran successfully\n    if assert_success and result.returncode != 0:\n        print(f\"Command '{cmd}' failed with exit status code '{str(result.returncode)}'.\\n\\nExiting now.\\nTry running the start/update script again.\")\n        sys.exit(1)\n\n    return result\n\n\ndef generate_alphabetic_sequence(index):\n    result = ''\n    while index >= 0:\n        index, remainder = divmod(index, 26)\n        result = chr(ord('A') + remainder) + result\n        index -= 1\n\n    return result\n\n\ndef get_user_choice(question, options_dict):\n    print()\n    print(question)\n    print()\n\n    for key, value in options_dict.items():\n        print(f\"{key}) {value}\")\n\n    print()\n\n    choice = input(\"Input> \").upper()\n    while choice not in options_dict.keys():\n        print(\"Invalid choice. Please try again.\")\n        choice = input(\"Input> \").upper()\n\n    return choice\n\n\ndef install_webui():\n\n    # Ask the user for the GPU vendor\n    if \"GPU_CHOICE\" in os.environ:\n        choice = os.environ[\"GPU_CHOICE\"].upper()\n        print_big_message(f\"Selected GPU choice \\\"{choice}\\\" based on the GPU_CHOICE environment variable.\")\n    else:\n        choice = get_user_choice(\n            \"What is your GPU?\",\n            {\n                'A': 'NVIDIA',\n                'B': 'AMD (Linux/MacOS only. Requires ROCm SDK 5.6 on Linux)',\n                'C': 'Apple M Series',\n                'D': 'Intel Arc (IPEX)',\n                'N': 'None (I want to run models in CPU mode)'\n            },\n        )\n\n    gpu_choice_to_name = {\n        \"A\": \"NVIDIA\",\n        \"B\": \"AMD\",\n        \"C\": \"APPLE\",\n        \"D\": \"INTEL\",\n        \"N\": \"NONE\"\n    }\n\n    selected_gpu = gpu_choice_to_name[choice]\n    use_cuda118 = \"N\"\n\n    # Write a flag to CMD_FLAGS.txt for CPU mode\n    if selected_gpu == \"NONE\":\n        with open(cmd_flags_path, 'r+') as cmd_flags_file:\n            if \"--cpu\" not in cmd_flags_file.read():\n                print_big_message(\"Adding the --cpu flag to CMD_FLAGS.txt.\")\n                cmd_flags_file.write(\"\\n--cpu\\n\")\n\n    # Check if the user wants CUDA 11.8\n    elif any((is_windows(), is_linux())) and selected_gpu == \"NVIDIA\":\n        if \"USE_CUDA118\" in os.environ:\n            use_cuda118 = \"Y\" if os.environ.get(\"USE_CUDA118\", \"\").lower() in (\"yes\", \"y\", \"true\", \"1\", \"t\", \"on\") else \"N\"\n        else:\n            print(\"\\nDo you want to use CUDA 11.8 instead of 12.1?\\nOnly choose this option if your GPU is very old (Kepler or older).\\n\\nFor RTX and GTX series GPUs, say \\\"N\\\".\\nIf unsure, say \\\"N\\\".\\n\")\n            use_cuda118 = input(\"Input (Y/N)> \").upper().strip('\"\\'').strip()\n            while use_cuda118 not in 'YN':\n                print(\"Invalid choice. Please try again.\")\n                use_cuda118 = input(\"Input> \").upper().strip('\"\\'').strip()\n\n        if use_cuda118 == 'Y':\n            print(\"CUDA: 11.8\")\n        else:\n            print(\"CUDA: 12.1\")\n\n    # No PyTorch for AMD on Windows (?)\n    elif is_windows() and selected_gpu == \"AMD\":\n        print(\"PyTorch setup on Windows is not implemented yet. Exiting...\")\n        sys.exit(1)\n\n    # Find the Pytorch installation command\n    install_pytorch = f\"python -m pip install torch=={TORCH_VERSION} torchvision=={TORCHVISION_VERSION} torchaudio=={TORCHAUDIO_VERSION} \"\n\n    if selected_gpu == \"NVIDIA\":\n        if use_cuda118 == 'Y':\n            install_pytorch += \"--index-url https://download.pytorch.org/whl/cu118\"\n        else:\n            install_pytorch += \"--index-url https://download.pytorch.org/whl/cu121\"\n    elif selected_gpu == \"AMD\":\n        install_pytorch += \"--index-url https://download.pytorch.org/whl/rocm5.6\"\n    elif selected_gpu in [\"APPLE\", \"NONE\"]:\n        install_pytorch += \"--index-url https://download.pytorch.org/whl/cpu\"\n    elif selected_gpu == \"INTEL\":\n        if is_linux():\n            install_pytorch = \"python -m pip install torch==2.1.0a0 torchvision==0.16.0a0 torchaudio==2.1.0a0 intel-extension-for-pytorch==2.1.10+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\"\n        else:\n            install_pytorch = \"python -m pip install torch==2.1.0a0 torchvision==0.16.0a0 torchaudio==2.1.0a0 intel-extension-for-pytorch==2.1.10 --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\"\n\n    # Install Git and then Pytorch\n    print_big_message(\"Installing PyTorch.\")\n    run_cmd(f\"conda install -y -k ninja git && {install_pytorch} && python -m pip install py-cpuinfo==9.0.0\", assert_success=True, environment=True)\n\n    if selected_gpu == \"INTEL\":\n        # Install oneAPI dependencies via conda\n        print_big_message(\"Installing Intel oneAPI runtime libraries.\")\n        run_cmd(\"conda install -y -c intel dpcpp-cpp-rt=2024.0 mkl-dpcpp=2024.0\")\n        # Install libuv required by Intel-patched torch\n        run_cmd(\"conda install -y libuv\")\n\n    # Install the webui requirements\n    update_requirements(initial_installation=True)\n\n\ndef get_extensions_names():\n    return [foldername for foldername in os.listdir('extensions') if os.path.isfile(os.path.join('extensions', foldername, 'requirements.txt'))]\n\n\ndef install_extensions_requirements():\n    print_big_message(\"Installing extensions requirements.\\nSome of these may fail on Windows.\\nDon\\'t worry if you see error messages, as they will not affect the main program.\")\n    extensions = get_extensions_names()\n    for i, extension in enumerate(extensions):\n        print(f\"\\n\\n--- [{i+1}/{len(extensions)}]: {extension}\\n\\n\")\n        extension_req_path = os.path.join(\"extensions\", extension, \"requirements.txt\")\n        run_cmd(f\"python -m pip install -r {extension_req_path} --upgrade\", assert_success=False, environment=True)\n\n\ndef update_requirements(initial_installation=False, pull=True):\n    # Create .git directory if missing\n    if not os.path.exists(os.path.join(script_dir, \".git\")):\n        git_creation_cmd = 'git init -b main && git remote add origin https://github.com/oobabooga/text-generation-webui && git fetch && git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main && git reset --hard origin/main && git branch --set-upstream-to=origin/main'\n        run_cmd(git_creation_cmd, environment=True, assert_success=True)\n\n    if pull:\n        print_big_message(\"Updating the local copy of the repository with \\\"git pull\\\"\")\n\n        files_to_check = [\n            'start_linux.sh', 'start_macos.sh', 'start_windows.bat', 'start_wsl.bat',\n            'update_wizard_linux.sh', 'update_wizard_macos.sh', 'update_wizard_windows.bat', 'update_wizard_wsl.bat',\n            'one_click.py'\n        ]\n\n        before_pull_hashes = {file_name: calculate_file_hash(file_name) for file_name in files_to_check}\n        run_cmd(\"git pull --autostash\", assert_success=True, environment=True)\n        after_pull_hashes = {file_name: calculate_file_hash(file_name) for file_name in files_to_check}\n\n        # Check for differences in installation file hashes\n        for file_name in files_to_check:\n            if before_pull_hashes[file_name] != after_pull_hashes[file_name]:\n                print_big_message(f\"File '{file_name}' was updated during 'git pull'. Please run the script again.\")\n                exit(1)\n\n    if os.environ.get(\"INSTALL_EXTENSIONS\", \"\").lower() in (\"yes\", \"y\", \"true\", \"1\", \"t\", \"on\"):\n        install_extensions_requirements()\n\n    # Update PyTorch\n    if not initial_installation:\n        update_pytorch()\n\n    # Detect the PyTorch version\n    torver = torch_version()\n    is_cuda = '+cu' in torver\n    is_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n    is_rocm = '+rocm' in torver  # 2.0.1+rocm5.4.2\n    is_intel = '+cxx11' in torver  # 2.0.1a0+cxx11.abi\n    is_cpu = '+cpu' in torver  # 2.0.1+cpu\n\n    if is_rocm:\n        base_requirements = \"requirements_amd\" + (\"_noavx2\" if not cpu_has_avx2() else \"\") + \".txt\"\n    elif is_cpu or is_intel:\n        base_requirements = \"requirements_cpu_only\" + (\"_noavx2\" if not cpu_has_avx2() else \"\") + \".txt\"\n    elif is_macos():\n        base_requirements = \"requirements_apple_\" + (\"intel\" if is_x86_64() else \"silicon\") + \".txt\"\n    else:\n        base_requirements = \"requirements\" + (\"_noavx2\" if not cpu_has_avx2() else \"\") + \".txt\"\n\n    requirements_file = base_requirements\n\n    print_big_message(f\"Installing webui requirements from file: {requirements_file}\")\n    print(f\"TORCH: {torver}\\n\")\n\n    # Prepare the requirements file\n    textgen_requirements = open(requirements_file).read().splitlines()\n    if is_cuda118:\n        textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements if \"auto-gptq\" not in req]\n    if is_windows() and is_cuda118:  # No flash-attention on Windows for CUDA 11\n        textgen_requirements = [req for req in textgen_requirements if 'oobabooga/flash-attention' not in req]\n\n    with open('temp_requirements.txt', 'w') as file:\n        file.write('\\n'.join(textgen_requirements))\n\n    # Workaround for git+ packages not updating properly.\n    git_requirements = [req for req in textgen_requirements if req.startswith(\"git+\")]\n    for req in git_requirements:\n        url = req.replace(\"git+\", \"\")\n        package_name = url.split(\"/\")[-1].split(\"@\")[0].rstrip(\".git\")\n        run_cmd(f\"python -m pip uninstall -y {package_name}\", environment=True)\n        print(f\"Uninstalled {package_name}\")\n\n    # Install/update the project requirements\n    run_cmd(\"python -m pip install -r temp_requirements.txt --upgrade\", assert_success=True, environment=True)\n    os.remove('temp_requirements.txt')\n\n    # Check for '+cu' or '+rocm' in version string to determine if torch uses CUDA or ROCm. Check for pytorch-cuda as well for backwards compatibility\n    if not any((is_cuda, is_rocm)) and run_cmd(\"conda list -f pytorch-cuda | grep pytorch-cuda\", environment=True, capture_output=True).returncode == 1:\n        clear_cache()\n        return\n\n    if not os.path.exists(\"repositories/\"):\n        os.mkdir(\"repositories\")\n\n    clear_cache()\n\n\ndef launch_webui():\n    run_cmd(f\"python server.py {flags}\", environment=True)\n\n\nif __name__ == \"__main__\":\n    # Verifies we are in a conda environment\n    check_env()\n\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument('--update-wizard', action='store_true', help='Launch a menu with update options.')\n    args, _ = parser.parse_known_args()\n\n    if args.update_wizard:\n        while True:\n            choice = get_user_choice(\n                \"What would you like to do?\",\n                {\n                    'A': 'Update the web UI',\n                    'B': 'Install/update extensions requirements',\n                    'C': 'Revert local changes to repository files with \\\"git reset --hard\\\"',\n                    'N': 'Nothing (exit)'\n                },\n            )\n\n            if choice == 'A':\n                update_requirements()\n            elif choice == 'B':\n                choices = {'A': 'All extensions'}\n                for i, name in enumerate(get_extensions_names()):\n                    key = generate_alphabetic_sequence(i + 1)\n                    choices[key] = name\n\n                choice = get_user_choice(\"What extension?\", choices)\n\n                if choice == 'A':\n                    install_extensions_requirements()\n                else:\n                    extension_req_path = os.path.join(\"extensions\", choices[choice], \"requirements.txt\")\n                    run_cmd(f\"python -m pip install -r {extension_req_path} --upgrade\", assert_success=False, environment=True)\n\n                update_requirements(pull=False)\n            elif choice == 'C':\n                run_cmd(\"git reset --hard\", assert_success=True, environment=True)\n            elif choice == 'N':\n                sys.exit()\n    else:\n        if not is_installed():\n            install_webui()\n            os.chdir(script_dir)\n\n        if os.environ.get(\"LAUNCH_AFTER_INSTALL\", \"\").lower() in (\"no\", \"n\", \"false\", \"0\", \"f\", \"off\"):\n            print_big_message(\"Will now exit due to LAUNCH_AFTER_INSTALL.\")\n            sys.exit()\n\n        # Check if a model has been downloaded yet\n        if '--model-dir' in flags:\n            # Splits on ' ' or '=' while maintaining spaces within quotes\n            flags_list = re.split(' +(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)|=', flags)\n            model_dir = [flags_list[(flags_list.index(flag) + 1)] for flag in flags_list if flag == '--model-dir'][0].strip('\"\\'')\n        else:\n            model_dir = 'models'\n\n        if len([item for item in glob.glob(f'{model_dir}/*') if not item.endswith(('.txt', '.yaml'))]) == 0:\n            print_big_message(\"You haven't downloaded any model yet.\\nOnce the web UI launches, head over to the \\\"Model\\\" tab and download one.\")\n\n        # Workaround for llama-cpp-python loading paths in CUDA env vars even if they do not exist\n        conda_path_bin = os.path.join(conda_env_path, \"bin\")\n        if not os.path.exists(conda_path_bin):\n            os.mkdir(conda_path_bin)\n\n        # Launch the webui\n        launch_webui()\n", "download-model.py": "'''\nDownloads models from Hugging Face to models/username_modelname.\n\nExample:\npython download-model.py facebook/opt-1.3b\n\n'''\n\nimport argparse\nimport base64\nimport datetime\nimport hashlib\nimport json\nimport os\nimport re\nimport sys\nfrom pathlib import Path\nfrom time import sleep\n\nimport requests\nimport tqdm\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import ConnectionError, RequestException, Timeout\nfrom tqdm.contrib.concurrent import thread_map\n\nbase = os.environ.get(\"HF_ENDPOINT\") or \"https://huggingface.co\"\n\n\nclass ModelDownloader:\n    def __init__(self, max_retries=5):\n        self.max_retries = max_retries\n\n    def get_session(self):\n        session = requests.Session()\n        if self.max_retries:\n            session.mount('https://cdn-lfs.huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n            session.mount('https://huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n\n        if os.getenv('HF_USER') is not None and os.getenv('HF_PASS') is not None:\n            session.auth = (os.getenv('HF_USER'), os.getenv('HF_PASS'))\n\n        try:\n            from huggingface_hub import get_token\n            token = get_token()\n        except ImportError:\n            token = os.getenv(\"HF_TOKEN\")\n\n        if token is not None:\n            session.headers = {'authorization': f'Bearer {token}'}\n\n        return session\n\n    def sanitize_model_and_branch_names(self, model, branch):\n        if model[-1] == '/':\n            model = model[:-1]\n\n        if model.startswith(base + '/'):\n            model = model[len(base) + 1:]\n\n        model_parts = model.split(\":\")\n        model = model_parts[0] if len(model_parts) > 0 else model\n        branch = model_parts[1] if len(model_parts) > 1 else branch\n\n        if branch is None:\n            branch = \"main\"\n        else:\n            pattern = re.compile(r\"^[a-zA-Z0-9._-]+$\")\n            if not pattern.match(branch):\n                raise ValueError(\n                    \"Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed.\")\n\n        return model, branch\n\n    def get_download_links_from_huggingface(self, model, branch, text_only=False, specific_file=None):\n        session = self.get_session()\n        page = f\"/api/models/{model}/tree/{branch}\"\n        cursor = b\"\"\n\n        links = []\n        sha256 = []\n        classifications = []\n        has_pytorch = False\n        has_pt = False\n        has_gguf = False\n        has_safetensors = False\n        is_lora = False\n        while True:\n            url = f\"{base}{page}\" + (f\"?cursor={cursor.decode()}\" if cursor else \"\")\n            r = session.get(url, timeout=10)\n            r.raise_for_status()\n            content = r.content\n\n            dict = json.loads(content)\n            if len(dict) == 0:\n                break\n\n            for i in range(len(dict)):\n                fname = dict[i]['path']\n                if specific_file not in [None, ''] and fname != specific_file:\n                    continue\n\n                if not is_lora and fname.endswith(('adapter_config.json', 'adapter_model.bin')):\n                    is_lora = True\n\n                is_pytorch = re.match(r\"(pytorch|adapter|gptq)_model.*\\.bin\", fname)\n                is_safetensors = re.match(r\".*\\.safetensors\", fname)\n                is_pt = re.match(r\".*\\.pt\", fname)\n                is_gguf = re.match(r'.*\\.gguf', fname)\n                is_tiktoken = re.match(r\".*\\.tiktoken\", fname)\n                is_tokenizer = re.match(r\"(tokenizer|ice|spiece).*\\.model\", fname) or is_tiktoken\n                is_text = re.match(r\".*\\.(txt|json|py|md)\", fname) or is_tokenizer\n                if any((is_pytorch, is_safetensors, is_pt, is_gguf, is_tokenizer, is_text)):\n                    if 'lfs' in dict[i]:\n                        sha256.append([fname, dict[i]['lfs']['oid']])\n\n                    if is_text:\n                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n                        classifications.append('text')\n                        continue\n\n                    if not text_only:\n                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n                        if is_safetensors:\n                            has_safetensors = True\n                            classifications.append('safetensors')\n                        elif is_pytorch:\n                            has_pytorch = True\n                            classifications.append('pytorch')\n                        elif is_pt:\n                            has_pt = True\n                            classifications.append('pt')\n                        elif is_gguf:\n                            has_gguf = True\n                            classifications.append('gguf')\n\n            cursor = base64.b64encode(f'{{\"file_name\":\"{dict[-1][\"path\"]}\"}}'.encode()) + b':50'\n            cursor = base64.b64encode(cursor)\n            cursor = cursor.replace(b'=', b'%3D')\n\n        # If both pytorch and safetensors are available, download safetensors only\n        # Also if GGUF and safetensors are available, download only safetensors\n        # (why do people do this?)\n        if (has_pytorch or has_pt or has_gguf) and has_safetensors:\n            has_gguf = False\n            for i in range(len(classifications) - 1, -1, -1):\n                if classifications[i] in ['pytorch', 'pt', 'gguf']:\n                    links.pop(i)\n\n        # For GGUF, try to download only the Q4_K_M if no specific file is specified.\n        # If not present, exclude all GGUFs, as that's likely a repository with both\n        # GGUF and fp16 files.\n        if has_gguf and specific_file is None:\n            has_q4km = False\n            for i in range(len(classifications) - 1, -1, -1):\n                if 'q4_k_m' in links[i].lower():\n                    has_q4km = True\n\n            if has_q4km:\n                for i in range(len(classifications) - 1, -1, -1):\n                    if 'q4_k_m' not in links[i].lower():\n                        links.pop(i)\n            else:\n                for i in range(len(classifications) - 1, -1, -1):\n                    if links[i].lower().endswith('.gguf'):\n                        links.pop(i)\n\n        is_llamacpp = has_gguf and specific_file is not None\n        return links, sha256, is_lora, is_llamacpp\n\n    def get_output_folder(self, model, branch, is_lora, is_llamacpp=False, model_dir=None):\n        if model_dir:\n            base_folder = model_dir\n        else:\n            base_folder = 'models' if not is_lora else 'loras'\n\n        # If the model is of type GGUF, save directly in the base_folder\n        if is_llamacpp:\n            return Path(base_folder)\n\n        output_folder = f\"{'_'.join(model.split('/')[-2:])}\"\n        if branch != 'main':\n            output_folder += f'_{branch}'\n\n        output_folder = Path(base_folder) / output_folder\n        return output_folder\n\n    def get_single_file(self, url, output_folder, start_from_scratch=False):\n        filename = Path(url.rsplit('/', 1)[1])\n        output_path = output_folder / filename\n\n        max_retries = 7\n        attempt = 0\n        while attempt < max_retries:\n            attempt += 1\n            session = self.get_session()\n            headers = {}\n            mode = 'wb'\n\n            try:\n                if output_path.exists() and not start_from_scratch:\n                    # Resume download\n                    r = session.get(url, stream=True, timeout=20)\n                    total_size = int(r.headers.get('content-length', 0))\n                    if output_path.stat().st_size >= total_size:\n                        return\n\n                    headers = {'Range': f'bytes={output_path.stat().st_size}-'}\n                    mode = 'ab'\n\n                with session.get(url, stream=True, headers=headers, timeout=30) as r:\n                    r.raise_for_status()  # If status is not 2xx, raise an error\n                    total_size = int(r.headers.get('content-length', 0))\n                    block_size = 1024 * 1024  # 1MB\n\n                    tqdm_kwargs = {\n                        'total': total_size,\n                        'unit': 'iB',\n                        'unit_scale': True,\n                        'bar_format': '{l_bar}{bar}| {n_fmt}/{total_fmt} {rate_fmt}'\n                    }\n\n                    if 'COLAB_GPU' in os.environ:\n                        tqdm_kwargs.update({\n                            'position': 0,\n                            'leave': True\n                        })\n\n                    with open(output_path, mode) as f:\n                        with tqdm.tqdm(**tqdm_kwargs) as t:\n                            count = 0\n                            for data in r.iter_content(block_size):\n                                f.write(data)\n                                t.update(len(data))\n                                if total_size != 0 and self.progress_bar is not None:\n                                    count += len(data)\n                                    self.progress_bar(float(count) / float(total_size), f\"{filename}\")\n\n                    break  # Exit loop if successful\n            except (RequestException, ConnectionError, Timeout) as e:\n                print(f\"Error downloading {filename}: {e}.\")\n                print(f\"That was attempt {attempt}/{max_retries}.\", end=' ')\n                if attempt < max_retries:\n                    print(f\"Retry begins in {2 ** attempt} seconds.\")\n                    sleep(2 ** attempt)\n                else:\n                    print(\"Failed to download after the maximum number of attempts.\")\n\n    def start_download_threads(self, file_list, output_folder, start_from_scratch=False, threads=4):\n        thread_map(lambda url: self.get_single_file(url, output_folder, start_from_scratch=start_from_scratch), file_list, max_workers=threads, disable=True)\n\n    def download_model_files(self, model, branch, links, sha256, output_folder, progress_bar=None, start_from_scratch=False, threads=4, specific_file=None, is_llamacpp=False):\n        self.progress_bar = progress_bar\n\n        # Create the folder and writing the metadata\n        output_folder.mkdir(parents=True, exist_ok=True)\n\n        if not is_llamacpp:\n            metadata = f'url: https://huggingface.co/{model}\\n' \\\n                       f'branch: {branch}\\n' \\\n                       f'download date: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n'\n\n            sha256_str = '\\n'.join([f'    {item[1]} {item[0]}' for item in sha256])\n            if sha256_str:\n                metadata += f'sha256sum:\\n{sha256_str}'\n\n            metadata += '\\n'\n            (output_folder / 'huggingface-metadata.txt').write_text(metadata)\n\n        if specific_file:\n            print(f\"Downloading {specific_file} to {output_folder}\")\n        else:\n            print(f\"Downloading the model to {output_folder}\")\n\n        self.start_download_threads(links, output_folder, start_from_scratch=start_from_scratch, threads=threads)\n\n    def check_model_files(self, model, branch, links, sha256, output_folder):\n        # Validate the checksums\n        validated = True\n        for i in range(len(sha256)):\n            fpath = (output_folder / sha256[i][0])\n\n            if not fpath.exists():\n                print(f\"The following file is missing: {fpath}\")\n                validated = False\n                continue\n\n            with open(output_folder / sha256[i][0], \"rb\") as f:\n                bytes = f.read()\n                file_hash = hashlib.sha256(bytes).hexdigest()\n                if file_hash != sha256[i][1]:\n                    print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')\n                    validated = False\n                else:\n                    print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')\n\n        if validated:\n            print('[+] Validated checksums of all model files!')\n        else:\n            print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('MODEL', type=str, default=None, nargs='?')\n    parser.add_argument('--branch', type=str, default='main', help='Name of the Git branch to download from.')\n    parser.add_argument('--threads', type=int, default=4, help='Number of files to download simultaneously.')\n    parser.add_argument('--text-only', action='store_true', help='Only download text files (txt/json).')\n    parser.add_argument('--specific-file', type=str, default=None, help='Name of the specific file to download (if not provided, downloads all).')\n    parser.add_argument('--output', type=str, default=None, help='Save the model files to this folder.')\n    parser.add_argument('--model-dir', type=str, default=None, help='Save the model files to a subfolder of this folder instead of the default one (text-generation-webui/models).')\n    parser.add_argument('--clean', action='store_true', help='Does not resume the previous download.')\n    parser.add_argument('--check', action='store_true', help='Validates the checksums of model files.')\n    parser.add_argument('--max-retries', type=int, default=5, help='Max retries count when get error in download time.')\n    args = parser.parse_args()\n\n    branch = args.branch\n    model = args.MODEL\n    specific_file = args.specific_file\n\n    if model is None:\n        print(\"Error: Please specify the model you'd like to download (e.g. 'python download-model.py facebook/opt-1.3b').\")\n        sys.exit()\n\n    downloader = ModelDownloader(max_retries=args.max_retries)\n    # Clean up the model/branch names\n    try:\n        model, branch = downloader.sanitize_model_and_branch_names(model, branch)\n    except ValueError as err_branch:\n        print(f\"Error: {err_branch}\")\n        sys.exit()\n\n    # Get the download links from Hugging Face\n    links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(model, branch, text_only=args.text_only, specific_file=specific_file)\n\n    # Get the output folder\n    if args.output:\n        output_folder = Path(args.output)\n    else:\n        output_folder = downloader.get_output_folder(model, branch, is_lora, is_llamacpp=is_llamacpp, model_dir=args.model_dir)\n\n    if args.check:\n        # Check previously downloaded files\n        downloader.check_model_files(model, branch, links, sha256, output_folder)\n    else:\n        # Download files\n        downloader.download_model_files(model, branch, links, sha256, output_folder, specific_file=specific_file, threads=args.threads, is_llamacpp=is_llamacpp)\n", "server.py": "import os\nimport warnings\n\nfrom modules import shared\n\nimport accelerate  # This early import makes Intel GPUs happy\n\nimport modules.one_click_installer_check\nfrom modules.block_requests import OpenMonkeyPatch, RequestBlocker\nfrom modules.logging_colors import logger\n\nos.environ['GRADIO_ANALYTICS_ENABLED'] = 'False'\nos.environ['BITSANDBYTES_NOWELCOME'] = '1'\nwarnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\nwarnings.filterwarnings('ignore', category=UserWarning, message='Using the update method is deprecated')\nwarnings.filterwarnings('ignore', category=UserWarning, message='Field \"model_name\" has conflict')\nwarnings.filterwarnings('ignore', category=UserWarning, message='The value passed into gr.Dropdown()')\nwarnings.filterwarnings('ignore', category=UserWarning, message='Field \"model_names\" has conflict')\n\nwith RequestBlocker():\n    from modules import gradio_hijack\n    import gradio as gr\n\nimport matplotlib\n\nmatplotlib.use('Agg')  # This fixes LaTeX rendering on some systems\n\nimport json\nimport os\nimport signal\nimport sys\nimport time\nfrom functools import partial\nfrom pathlib import Path\nfrom threading import RLock, Thread\n\nimport yaml\n\nimport modules.extensions as extensions_module\nfrom modules import (\n    chat,\n    training,\n    ui,\n    ui_chat,\n    ui_default,\n    ui_file_saving,\n    ui_model_menu,\n    ui_notebook,\n    ui_parameters,\n    ui_session,\n    utils\n)\nfrom modules.extensions import apply_extensions\nfrom modules.LoRA import add_lora_to_model\nfrom modules.models import load_model, unload_model_if_idle\nfrom modules.models_settings import (\n    get_fallback_settings,\n    get_model_metadata,\n    update_model_parameters\n)\nfrom modules.shared import do_cmd_flags_warnings\nfrom modules.utils import gradio\n\n\ndef signal_handler(sig, frame):\n    logger.info(\"Received Ctrl+C. Shutting down Text generation web UI gracefully.\")\n    sys.exit(0)\n\n\nsignal.signal(signal.SIGINT, signal_handler)\n\n\ndef create_interface():\n\n    title = 'Text generation web UI'\n\n    # Password authentication\n    auth = []\n    if shared.args.gradio_auth:\n        auth.extend(x.strip() for x in shared.args.gradio_auth.strip('\"').replace('\\n', '').split(',') if x.strip())\n    if shared.args.gradio_auth_path:\n        with open(shared.args.gradio_auth_path, 'r', encoding=\"utf8\") as file:\n            auth.extend(x.strip() for line in file for x in line.split(',') if x.strip())\n    auth = [tuple(cred.split(':')) for cred in auth]\n\n    # Import the extensions and execute their setup() functions\n    if shared.args.extensions is not None and len(shared.args.extensions) > 0:\n        extensions_module.load_extensions()\n\n    # Force some events to be triggered on page load\n    shared.persistent_interface_state.update({\n        'loader': shared.args.loader or 'Transformers',\n        'mode': shared.settings['mode'],\n        'character_menu': shared.args.character or shared.settings['character'],\n        'instruction_template_str': shared.settings['instruction_template_str'],\n        'prompt_menu-default': shared.settings['prompt-default'],\n        'prompt_menu-notebook': shared.settings['prompt-notebook'],\n        'filter_by_loader': shared.args.loader or 'All'\n    })\n\n    if Path(\"cache/pfp_character.png\").exists():\n        Path(\"cache/pfp_character.png\").unlink()\n\n    # css/js strings\n    css = ui.css\n    js = ui.js\n    css += apply_extensions('css')\n    js += apply_extensions('js')\n\n    # Interface state elements\n    shared.input_elements = ui.list_interface_input_elements()\n\n    with gr.Blocks(css=css, analytics_enabled=False, title=title, theme=ui.theme) as shared.gradio['interface']:\n\n        # Interface state\n        shared.gradio['interface_state'] = gr.State({k: None for k in shared.input_elements})\n\n        # Audio notification\n        if Path(\"notification.mp3\").exists():\n            shared.gradio['audio_notification'] = gr.Audio(interactive=False, value=\"notification.mp3\", elem_id=\"audio_notification\", visible=False)\n\n        # Floating menus for saving/deleting files\n        ui_file_saving.create_ui()\n\n        # Temporary clipboard for saving files\n        shared.gradio['temporary_text'] = gr.Textbox(visible=False)\n\n        # Text Generation tab\n        ui_chat.create_ui()\n        ui_default.create_ui()\n        ui_notebook.create_ui()\n\n        ui_parameters.create_ui(shared.settings['preset'])  # Parameters tab\n        ui_model_menu.create_ui()  # Model tab\n        training.create_ui()  # Training tab\n        ui_session.create_ui()  # Session tab\n\n        # Generation events\n        ui_chat.create_event_handlers()\n        ui_default.create_event_handlers()\n        ui_notebook.create_event_handlers()\n\n        # Other events\n        ui_file_saving.create_event_handlers()\n        ui_parameters.create_event_handlers()\n        ui_model_menu.create_event_handlers()\n\n        # Interface launch events\n        shared.gradio['interface'].load(lambda: None, None, None, js=f\"() => {{if ({str(shared.settings['dark_theme']).lower()}) {{ document.getElementsByTagName('body')[0].classList.add('dark'); }} }}\")\n        shared.gradio['interface'].load(lambda: None, None, None, js=f\"() => {{{js}}}\")\n        shared.gradio['interface'].load(lambda x: None, gradio('show_controls'), None, js=f'(x) => {{{ui.show_controls_js}; toggle_controls(x)}}')\n        shared.gradio['interface'].load(partial(ui.apply_interface_values, {}, use_persistent=True), None, gradio(ui.list_interface_input_elements()), show_progress=False)\n        shared.gradio['interface'].load(chat.redraw_html, gradio(ui_chat.reload_arr), gradio('display'))\n\n        extensions_module.create_extensions_tabs()  # Extensions tabs\n        extensions_module.create_extensions_block()  # Extensions block\n\n    # Launch the interface\n    shared.gradio['interface'].queue()\n    with OpenMonkeyPatch():\n        shared.gradio['interface'].launch(\n            max_threads=64,\n            prevent_thread_lock=True,\n            share=shared.args.share,\n            server_name=None if not shared.args.listen else (shared.args.listen_host or '0.0.0.0'),\n            server_port=shared.args.listen_port,\n            inbrowser=shared.args.auto_launch,\n            auth=auth or None,\n            ssl_verify=False if (shared.args.ssl_keyfile or shared.args.ssl_certfile) else True,\n            ssl_keyfile=shared.args.ssl_keyfile,\n            ssl_certfile=shared.args.ssl_certfile,\n            allowed_paths=[\"cache\", \"css\", \"extensions\", \"js\"]\n        )\n\n\nif __name__ == \"__main__\":\n\n    logger.info(\"Starting Text generation web UI\")\n    do_cmd_flags_warnings()\n\n    # Load custom settings\n    settings_file = None\n    if shared.args.settings is not None and Path(shared.args.settings).exists():\n        settings_file = Path(shared.args.settings)\n    elif Path('settings.yaml').exists():\n        settings_file = Path('settings.yaml')\n    elif Path('settings.json').exists():\n        settings_file = Path('settings.json')\n\n    if settings_file is not None:\n        logger.info(f\"Loading settings from \\\"{settings_file}\\\"\")\n        file_contents = open(settings_file, 'r', encoding='utf-8').read()\n        new_settings = json.loads(file_contents) if settings_file.suffix == \"json\" else yaml.safe_load(file_contents)\n        shared.settings.update(new_settings)\n\n    # Fallback settings for models\n    shared.model_config['.*'] = get_fallback_settings()\n    shared.model_config.move_to_end('.*', last=False)  # Move to the beginning\n\n    # Activate the extensions listed on settings.yaml\n    extensions_module.available_extensions = utils.get_available_extensions()\n    for extension in shared.settings['default_extensions']:\n        shared.args.extensions = shared.args.extensions or []\n        if extension not in shared.args.extensions:\n            shared.args.extensions.append(extension)\n\n    available_models = utils.get_available_models()\n\n    # Model defined through --model\n    if shared.args.model is not None:\n        shared.model_name = shared.args.model\n\n    # Select the model from a command-line menu\n    elif shared.args.model_menu:\n        if len(available_models) == 0:\n            logger.error('No models are available! Please download at least one.')\n            sys.exit(0)\n        else:\n            print('The following models are available:\\n')\n            for i, model in enumerate(available_models):\n                print(f'{i+1}. {model}')\n\n            print(f'\\nWhich one do you want to load? 1-{len(available_models)}\\n')\n            i = int(input()) - 1\n            print()\n\n        shared.model_name = available_models[i]\n\n    # If any model has been selected, load it\n    if shared.model_name != 'None':\n        p = Path(shared.model_name)\n        if p.exists():\n            model_name = p.parts[-1]\n            shared.model_name = model_name\n        else:\n            model_name = shared.model_name\n\n        model_settings = get_model_metadata(model_name)\n        update_model_parameters(model_settings, initial=True)  # hijack the command-line arguments\n\n        # Load the model\n        shared.model, shared.tokenizer = load_model(model_name)\n        if shared.args.lora:\n            add_lora_to_model(shared.args.lora)\n\n    shared.generation_lock = RLock()\n\n    if shared.args.idle_timeout > 0:\n        timer_thread = Thread(target=unload_model_if_idle)\n        timer_thread.daemon = True\n        timer_thread.start()\n\n    if shared.args.nowebui:\n        # Start the API in standalone mode\n        shared.args.extensions = [x for x in shared.args.extensions if x != 'gallery']\n        if shared.args.extensions is not None and len(shared.args.extensions) > 0:\n            extensions_module.load_extensions()\n    else:\n        # Launch the web UI\n        create_interface()\n        while True:\n            time.sleep(0.5)\n            if shared.need_restart:\n                shared.need_restart = False\n                time.sleep(0.5)\n                shared.gradio['interface'].close()\n                time.sleep(0.5)\n                create_interface()\n", "extensions/character_bias/script.py": "import os\n\nimport gradio as gr\n\n# get the current directory of the script\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\n# check if the bias_options.txt file exists, if not, create it\nbias_file = os.path.join(current_dir, \"bias_options.txt\")\nif not os.path.isfile(bias_file):\n    with open(bias_file, \"w\") as f:\n        f.write(\"*I am so happy*\\n*I am so sad*\\n*I am so excited*\\n*I am so bored*\\n*I am so angry*\")\n\n# read bias options from the text file\nwith open(bias_file, \"r\") as f:\n    bias_options = [line.strip() for line in f.readlines()]\n\nparams = {\n    \"activate\": True,\n    \"bias string\": \" *I am so happy*\",\n    \"custom string\": \"\",\n}\n\n\ndef input_modifier(string):\n    \"\"\"\n    This function is applied to your text inputs before\n    they are fed into the model.\n    \"\"\"\n    return string\n\n\ndef output_modifier(string):\n    \"\"\"\n    This function is applied to the model outputs.\n    \"\"\"\n    return string\n\n\ndef bot_prefix_modifier(string):\n    \"\"\"\n    This function is only applied in chat mode. It modifies\n    the prefix text for the Bot and can be used to bias its\n    behavior.\n    \"\"\"\n    if params['activate']:\n        if params['custom string'].strip() != '':\n            return f'{string} {params[\"custom string\"].strip()} '\n        else:\n            return f'{string} {params[\"bias string\"].strip()} '\n    else:\n        return string\n\n\ndef ui():\n    # Gradio elements\n    activate = gr.Checkbox(value=params['activate'], label='Activate character bias')\n    dropdown_string = gr.Dropdown(choices=bias_options, value=params[\"bias string\"], label='Character bias', info='To edit the options in this dropdown edit the \"bias_options.txt\" file')\n    custom_string = gr.Textbox(value=params['custom string'], placeholder=\"Enter custom bias string\", label=\"Custom Character Bias\", info='If not empty, will be used instead of the value above')\n\n    # Event functions to update the parameters in the backend\n    def update_bias_string(x):\n        if x:\n            params.update({\"bias string\": x})\n        else:\n            params.update({\"bias string\": dropdown_string.get()})\n        return x\n\n    def update_custom_string(x):\n        params.update({\"custom string\": x})\n\n    dropdown_string.change(update_bias_string, dropdown_string, None)\n    custom_string.change(update_custom_string, custom_string, None)\n    activate.change(lambda x: params.update({\"activate\": x}), activate, None)\n", "extensions/whisper_stt/script.py": "import gradio as gr\nimport speech_recognition as sr\n\nfrom modules import shared\n\ninput_hijack = {\n    'state': False,\n    'value': [\"\", \"\"]\n}\n\n# parameters which can be customized in settings.json of webui\nparams = {\n    'whipser_language': 'english',\n    'whipser_model': 'small.en',\n    'auto_submit': True\n}\n\n\ndef chat_input_modifier(text, visible_text, state):\n    global input_hijack\n    if input_hijack['state']:\n        input_hijack['state'] = False\n        return input_hijack['value']\n    else:\n        return text, visible_text\n\n\ndef do_stt(audio, whipser_model, whipser_language):\n    transcription = \"\"\n    r = sr.Recognizer()\n\n    # Convert to AudioData\n    audio_data = sr.AudioData(sample_rate=audio[0], frame_data=audio[1], sample_width=4)\n\n    try:\n        transcription = r.recognize_whisper(audio_data, language=whipser_language, model=whipser_model)\n    except sr.UnknownValueError:\n        print(\"Whisper could not understand audio\")\n    except sr.RequestError as e:\n        print(\"Could not request results from Whisper\", e)\n\n    return transcription\n\n\ndef auto_transcribe(audio, auto_submit, whipser_model, whipser_language):\n    if audio is None:\n        return \"\", \"\"\n    transcription = do_stt(audio, whipser_model, whipser_language)\n    if auto_submit:\n        input_hijack.update({\"state\": True, \"value\": [transcription, transcription]})\n\n    return transcription, None\n\n\ndef ui():\n    with gr.Accordion(\"Whisper STT\", open=True):\n        with gr.Row():\n            audio = gr.Audio(source=\"microphone\")\n        with gr.Row():\n            with gr.Accordion(\"Settings\", open=False):\n                auto_submit = gr.Checkbox(label='Submit the transcribed audio automatically', value=params['auto_submit'])\n                whipser_model = gr.Dropdown(label='Whisper Model', value=params['whipser_model'], choices=[\"tiny.en\", \"base.en\", \"small.en\", \"medium.en\", \"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n                whipser_language = gr.Dropdown(label='Whisper Language', value=params['whipser_language'], choices=[\"chinese\", \"german\", \"spanish\", \"russian\", \"korean\", \"french\", \"japanese\", \"portuguese\", \"turkish\", \"polish\", \"catalan\", \"dutch\", \"arabic\", \"swedish\", \"italian\", \"indonesian\", \"hindi\", \"finnish\", \"vietnamese\", \"hebrew\", \"ukrainian\", \"greek\", \"malay\", \"czech\", \"romanian\", \"danish\", \"hungarian\", \"tamil\", \"norwegian\", \"thai\", \"urdu\", \"croatian\", \"bulgarian\", \"lithuanian\", \"latin\", \"maori\", \"malayalam\", \"welsh\", \"slovak\", \"telugu\", \"persian\", \"latvian\", \"bengali\", \"serbian\", \"azerbaijani\", \"slovenian\", \"kannada\", \"estonian\", \"macedonian\", \"breton\", \"basque\", \"icelandic\", \"armenian\", \"nepali\", \"mongolian\", \"bosnian\", \"kazakh\", \"albanian\", \"swahili\", \"galician\", \"marathi\", \"punjabi\", \"sinhala\", \"khmer\", \"shona\", \"yoruba\", \"somali\", \"afrikaans\", \"occitan\", \"georgian\", \"belarusian\", \"tajik\", \"sindhi\", \"gujarati\", \"amharic\", \"yiddish\", \"lao\", \"uzbek\", \"faroese\", \"haitian creole\", \"pashto\", \"turkmen\", \"nynorsk\", \"maltese\", \"sanskrit\", \"luxembourgish\", \"myanmar\", \"tibetan\", \"tagalog\", \"malagasy\", \"assamese\", \"tatar\", \"hawaiian\", \"lingala\", \"hausa\", \"bashkir\", \"javanese\", \"sundanese\"])\n\n    audio.stop_recording(\n        auto_transcribe, [audio, auto_submit, whipser_model, whipser_language], [shared.gradio['textbox'], audio]).then(\n        None, auto_submit, None, js=\"(check) => {if (check) { document.getElementById('Generate').click() }}\")\n\n    whipser_model.change(lambda x: params.update({\"whipser_model\": x}), whipser_model, None)\n    whipser_language.change(lambda x: params.update({\"whipser_language\": x}), whipser_language, None)\n    auto_submit.change(lambda x: params.update({\"auto_submit\": x}), auto_submit, None)\n", "extensions/Training_PRO/matplotgraph.py": "import os\nimport json\n\ndef create_graph(lora_path, lora_name):\n    try:\n        import matplotlib.pyplot as plt\n        from matplotlib.ticker import ScalarFormatter\n        \n        peft_model_path = f'{lora_path}/training_graph.json'\n        image_model_path = f'{lora_path}/training_graph.png'\n        # Check if the JSON file exists\n        if os.path.exists(peft_model_path):\n            # Load data from JSON file\n            with open(peft_model_path, 'r') as file:\n                data = json.load(file)\n            # Extract x, y1, and y2 values\n            x = [item['epoch'] for item in data]\n            y1 = [item['learning_rate'] for item in data]\n            y2 = [item['loss'] for item in data]\n\n            # Create the line chart\n            fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n\n            # Plot y1 (learning rate) on the first y-axis\n            ax1.plot(x, y1, 'b-', label='Learning Rate')\n            ax1.set_xlabel('Epoch')\n            ax1.set_ylabel('Learning Rate', color='b')\n            ax1.tick_params('y', colors='b')\n\n            # Create a second y-axis\n            ax2 = ax1.twinx()\n\n            # Plot y2 (loss) on the second y-axis\n            ax2.plot(x, y2, 'r-', label='Loss')\n            ax2.set_ylabel('Loss', color='r')\n            ax2.tick_params('y', colors='r')\n\n            # Set the y-axis formatter to display numbers in scientific notation\n            ax1.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n            ax1.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n\n            # Add grid\n            ax1.grid(True)\n\n            # Combine the legends for both plots\n            lines, labels = ax1.get_legend_handles_labels()\n            lines2, labels2 = ax2.get_legend_handles_labels()\n            ax2.legend(lines + lines2, labels + labels2, loc='best')\n\n            # Set the title\n            plt.title(f'{lora_name} LR and Loss vs Epoch')\n\n            # Save the chart as an image\n            plt.savefig(image_model_path)\n\n            print(f\"Graph saved in {image_model_path}\")\n        else:\n            print(f\"File 'training_graph.json' does not exist in the {lora_path}\")\n      \n    except ImportError:\n        print(\"matplotlib is not installed. Please install matplotlib to create PNG graphs\")", "extensions/Training_PRO/custom_scheduler.py": "from functools import partial\nimport torch\nimport transformers\nimport math\nfrom torch.optim.lr_scheduler import LambdaLR\n\nfrom peft import (\n    PeftModel,\n)\n\nRED = \"\\033[91m\"\nYELLOW = \"\\033[93m\"\nGREEN = \"\\033[92m\"\nRESET = \"\\033[0m\"\n\nlast_print_label = ''\n\ncustom_scheduler_params = {'trigger_loss': 0.0, 'ramp_down_ratio':1.0, 'current_loss': 0.0,'dynamic_scheduler_stop': False, 'calc_ramp_down_at_step': 0, 'calc_num_training_steps': 0}\n\n\ndef custom_scheduler_global_update(current_loss: float):\n    custom_scheduler_params.update({'current_loss': current_loss})\n  \ndef custom_scheduler_global_setup(trigger_loss: float, ramp_down_ratio: float):\n    custom_scheduler_params.update({'trigger_loss': trigger_loss})\n    custom_scheduler_params.update({'ramp_down_ratio': ramp_down_ratio})\n\n    # calculates the total num steps after trigger\n    custom_scheduler_params.update({'calc_num_training_steps': 0})\n    #calculates steps when the ramp_down trigger occured\n    custom_scheduler_params.update({'calc_ramp_down_at_step': 0})\n    # triggers scheduler stopping after it reached calc_num_training_steps\n    custom_scheduler_params.update({'dynamic_scheduler_stop': False})\n\n\n# hold constant to the half of epochs then cosine down to 0\ndef _get_fp_half_schedule_with_warmup_lr_lambda(current_step: int, *, num_warmup_steps: int, num_training_steps: int, num_firstepoch_steps: int):\n    \n    global last_print_label\n    print_label = ''\n\n    half_steps = num_training_steps//2\n    \n    num_warmup_steps = min(num_warmup_steps,half_steps)\n\n    if current_step < num_warmup_steps:\n        print_label = 'Scheduler: Warmup'\n    elif current_step < half_steps:\n        print_label = 'Scheduler: Hold'\n    else:\n        print_label = 'Scheduler: Annealing'\n    \n    if print_label != last_print_label:\n        print(print_label)\n    \n    last_print_label = print_label\n\n    if current_step < num_warmup_steps:\n        return float(current_step) / float(max(1, num_warmup_steps))\n    \n    if current_step < half_steps:\n        return 1.0 \n    \n    progress = float(current_step - half_steps) / float(max(1, num_training_steps - half_steps))\n    num_cycles = 0.5\n    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))    \n \n\n# raise up in cosine, then fall back in cosine\ndef _get_fp_cosine_raise_and_fall_lr_lambda(current_step: int, *, num_warmup_steps: int, num_training_steps: int, num_firstepoch_steps: int):\n    \n    global last_print_label\n    print_label = ''\n\n    half_steps = num_training_steps//2\n    \n    #num_warmup_steps = min(num_warmup_steps,half_steps)\n\n    if current_step < half_steps:\n        print_label = 'Scheduler: Raise'\n    else:\n        print_label = 'Scheduler: Fall'\n    \n    if print_label != last_print_label:\n        print(print_label)\n    \n    last_print_label = print_label\n\n    \n    # linear\n    #    return float(current_step) / float(max(1, num_warmup_steps))\n    \n    progress = float(current_step - half_steps) / float(max(1, num_training_steps - half_steps))\n    num_cycles = 0.5\n    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))    \n \n# constant to the first epochs then cosine down to 0 over the rest epochs\ndef _get_fp_cosine_schedule_with_warmup_lr_lambda(current_step: int, *, num_warmup_steps: int, num_training_steps: int, num_firstepoch_steps: int):\n    \n    global last_print_label\n    print_label = ''\n    \n    num_warmup_steps = min(num_warmup_steps,num_firstepoch_steps)\n\n    if current_step < num_warmup_steps:\n        print_label = 'Scheduler: Warmup'\n    elif current_step < num_firstepoch_steps:\n        print_label = 'Scheduler: Hold'\n    else:\n        print_label = 'Scheduler: Annealing'\n    \n    if print_label != last_print_label:\n        print(print_label)\n    \n    last_print_label = print_label\n\n    if current_step < num_warmup_steps:\n        return float(current_step) / float(max(1, num_warmup_steps))\n    \n    if current_step < num_firstepoch_steps:\n        return 1.0 \n    \n    progress = float(current_step - num_firstepoch_steps) / float(max(1, num_training_steps - num_firstepoch_steps))\n    num_cycles = 0.5\n    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))    \n    \n# halve lr each epoch   \n\ndef _get_fp_cdrop_rate_schedule_with_warmup_lr_lambda(current_step: int, *, num_warmup_steps: int, num_training_steps: int, num_firstepoch_steps: int):\n    \n    global last_print_label\n    print_label = ''\n    \n    num_warmup_steps = min(num_warmup_steps, num_firstepoch_steps)\n\n    current_epoch = (current_step // num_firstepoch_steps) + 1\n    \n    \n    if current_step < num_warmup_steps:\n        print_label = 'Scheduler: Warmup'\n    elif current_step < num_firstepoch_steps:\n        print_label = 'Scheduler: Hold'\n    else:\n        print_label = 'Scheduler: Drop Rate'\n    \n    if print_label != last_print_label:\n        print(print_label)\n    \n    last_print_label = print_label\n\n    if current_step < num_warmup_steps:\n        return float(current_step) / float(max(1, num_warmup_steps))\n    \n    if current_step < num_firstepoch_steps:\n        return 1.0 \n\n    # Compute the learning rate for the annealing phase\n    \n    learning_rate = 1.0 / float(2 ** (current_epoch - 1))\n   \n    return learning_rate\n\n# epoch decay: 1/(1 + decay * epoch)\n\ndef custom_cosine_scheduler_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_firstepoch_steps, last_epoch=-1):\n    \"\"\"\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    \n    lr_lambda = partial(\n        _get_fp_cosine_schedule_with_warmup_lr_lambda,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps,\n        num_firstepoch_steps = num_firstepoch_steps,\n    )\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef custom_half_scheduler_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_firstepoch_steps, last_epoch=-1):\n    \"\"\"\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    \n    lr_lambda = partial(\n        _get_fp_half_schedule_with_warmup_lr_lambda,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps,\n        num_firstepoch_steps = num_firstepoch_steps,\n    )\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef custom_raise_fall_scheduler_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_firstepoch_steps, last_epoch=-1):\n    \"\"\"\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    \n    lr_lambda = partial(\n        _get_fp_cosine_raise_and_fall_lr_lambda,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps,\n        num_firstepoch_steps = num_firstepoch_steps,\n    )\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\n\ndef neftune_forward(self, input: torch.Tensor):\n    \"\"\"\n    Implements the NEFTune forward pass for the model. Note this works only for\n    torch.nn.Embedding layers. This method is slightly adapted from the original source code\n    that can be found here: https://github.com/neelsjain/NEFTune\n\n    Args:\n        input (`torch.Tensor`):\n            The input tensor to the model.\n        noise_alpha (`float`):\n            The noise alpha value to use for the NEFTune forward pass.\n    \"\"\"\n    embeddings = torch.nn.functional.embedding(\n        input, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse\n    )\n\n    if self.training:\n        # Add noise to the embeddings\n        dims = torch.tensor(embeddings.size(1) * embeddings.size(2))\n        mag_norm = self.neftune_noise_alpha / torch.sqrt(dims)\n        embeddings = embeddings + torch.zeros_like(embeddings).uniform_(-mag_norm, mag_norm)\n\n    return embeddings    \n\n\nclass FPNEFtuneTrainer(transformers.Trainer):\n    def __init__(self,neftune_noise_alpha:float = 0.0, model = None, *args, **kwargs):\n        self.neftune_noise_alpha = neftune_noise_alpha\n        if self.neftune_noise_alpha > 0.0:\n            model = self._activate_neftune(model)\n        super().__init__(model = model, *args, **kwargs)\n\n   \n    def _activate_neftune(self, model):\n        r\"\"\"\n        Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper: https://arxiv.org/abs/2310.05914\n        \"\"\"\n        print(f\"Activating {RED}NEFtune{RESET} with scale: {self.neftune_noise_alpha}\")\n        if isinstance(model, transformers.PreTrainedModel):\n            embeddings = model.get_input_embeddings()\n        elif isinstance(model, PeftModel):\n            embeddings = model.base_model.get_input_embeddings()\n\n        embeddings.neftune_noise_alpha = self.neftune_noise_alpha\n        old_forward = embeddings.forward\n\n        # This hack seems to be needed to properly use a custom forward pass\n        # all credits to: https://discuss.pytorch.org/t/how-can-i-replace-the-forward-method-of-a-predefined-torchvision-model-with-my-customized-forward-function/54224/11\n        bound_method = neftune_forward.__get__(embeddings, embeddings.__class__)\n        setattr(embeddings, \"forward\", bound_method)\n\n        # embeddings.forward = neftune_forward\n        embeddings._trl_old_forward = old_forward\n\n        return model\n    \n    def train(self, *args, **kwargs):\n        output = super().train(*args, **kwargs)\n\n        # After training we make sure to retrieve back the original forward pass method\n        # for the embedding layer\n        if self.neftune_noise_alpha is not None:\n\n            if isinstance(self.model, transformers.PreTrainedModel):\n                embeddings = self.model.get_input_embeddings()\n            elif isinstance(self.model, PeftModel):\n                embeddings = self.model.base_model.get_input_embeddings()\n\n            if hasattr(embeddings, \"_trl_old_forward\"):\n                embeddings.forward = embeddings._trl_old_forward\n                del embeddings._trl_old_forward\n                del embeddings.neftune_noise_alpha\n\n        return output\n\n\nclass FPSchedulerTrainer(transformers.Trainer):\n    def __init__(self,neftune_noise_alpha:float = 0.0, model = None, *args, **kwargs):\n        self.neftune_noise_alpha = neftune_noise_alpha\n        if self.neftune_noise_alpha > 0.0:\n            model = self._activate_neftune(model)\n        super().__init__(model = model, *args, **kwargs)\n\n   \n    def _activate_neftune(self, model):\n        r\"\"\"\n        Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper: https://arxiv.org/abs/2310.05914\n        \"\"\"\n        print(f\"Activating {RED}NEFtune{RESET} with scale: {self.neftune_noise_alpha}\")\n        if isinstance(model, transformers.PreTrainedModel):\n            embeddings = model.get_input_embeddings()\n        elif isinstance(model, PeftModel):\n            embeddings = model.base_model.get_input_embeddings()\n\n        embeddings.neftune_noise_alpha = self.neftune_noise_alpha\n        old_forward = embeddings.forward\n\n        # This hack seems to be needed to properly use a custom forward pass\n        # all credits to: https://discuss.pytorch.org/t/how-can-i-replace-the-forward-method-of-a-predefined-torchvision-model-with-my-customized-forward-function/54224/11\n        bound_method = neftune_forward.__get__(embeddings, embeddings.__class__)\n        setattr(embeddings, \"forward\", bound_method)\n\n        # embeddings.forward = neftune_forward\n        embeddings._trl_old_forward = old_forward\n\n        return model\n    \n    def train(self, *args, **kwargs):\n        output = super().train(*args, **kwargs)\n\n        # After training we make sure to retrieve back the original forward pass method\n        # for the embedding layer\n        if self.neftune_noise_alpha is not None:\n\n            if isinstance(self.model, transformers.PreTrainedModel):\n                embeddings = self.model.get_input_embeddings()\n            elif isinstance(self.model, PeftModel):\n                embeddings = self.model.base_model.get_input_embeddings()\n\n            if hasattr(embeddings, \"_trl_old_forward\"):\n                embeddings.forward = embeddings._trl_old_forward\n                del embeddings._trl_old_forward\n                del embeddings.neftune_noise_alpha\n\n        return output\n\n\n    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n        #Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or passed as an argument.\n        \n        num_train_epochs = self.args.num_train_epochs\n        num_warmup_steps=self.args.get_warmup_steps(num_training_steps)\n        num_firstepoch_steps = math.ceil(num_training_steps/num_train_epochs)\n        num_warmup_acc = num_warmup_steps*self.args.gradient_accumulation_steps \n        num_firstepoch_steps_acc = num_firstepoch_steps*self.args.gradient_accumulation_steps\n        num_training_steps_acc = num_training_steps*self.args.gradient_accumulation_steps\n\n        custom_scheduler_params.update({'dynamic_scheduler_stop': False})\n \n        print (f\"Warm-up steps aligned to Gradient accumulation ({self.args.gradient_accumulation_steps}) = {num_warmup_acc} actual warmup steps\")\n        if self.args.lr_scheduler_type == 'cosine':\n            \n            num_warmup_acc_min = min(num_warmup_acc, num_firstepoch_steps_acc)\n\n            if num_warmup_acc>num_firstepoch_steps_acc:\n                print(f\"\\033[1;31;1mWARNING: The number of warmup steps is set too high! It will be clamped to 1 epoch, essentially going from warmup to annealing.\\033[0;37;0m\")\n                print (f\"FP Scheduler Warmup: 0-[{num_warmup_acc_min}], Hold [{num_warmup_acc_min}]-{num_firstepoch_steps_acc}, Annealing {num_firstepoch_steps_acc}-{num_training_steps_acc}\")\n            else:\n                print (f\"FP Scheduler Warmup: 0-{num_warmup_acc_min}, Hold {num_warmup_acc_min}-{num_firstepoch_steps_acc}, Annealing {num_firstepoch_steps_acc}-{num_training_steps_acc}\")\n\n            self.lr_scheduler = custom_cosine_scheduler_with_warmup(\n                    optimizer=self.optimizer if optimizer is None else optimizer,\n                    num_warmup_steps=num_warmup_steps,\n                    num_training_steps=num_training_steps, \n                    num_firstepoch_steps = num_firstepoch_steps,\n                )\n            self._created_lr_scheduler = True\n            return self.lr_scheduler\n        elif self.args.lr_scheduler_type == 'constant':\n           \n            half_step_acc = num_training_steps_acc//2\n            num_warmup_acc_min = min(num_warmup_acc, half_step_acc)\n\n            if num_warmup_acc>half_step_acc:\n                print(f\"\\033[1;31;1mWARNING: The number of warmup steps is set too high! It will be clamped to half of all epochs, essentially going from warmup to annealing in the middle.\\033[0;37;0m\")\n                print (f\"FP Scheduler Warmup: 0-[{num_warmup_acc_min}], Hold [{num_warmup_acc_min}]-{half_step_acc}, Annealing {half_step_acc}-{num_training_steps_acc}\")\n            else:\n                print (f\"FP Scheduler Warmup: 0-{num_warmup_acc_min}, Hold {num_warmup_acc_min}-{half_step_acc}, Annealing {half_step_acc}-{num_training_steps_acc}\")\n\n            self.lr_scheduler = custom_half_scheduler_with_warmup(\n                    optimizer=self.optimizer if optimizer is None else optimizer,\n                    num_warmup_steps=num_warmup_steps,\n                    num_training_steps=num_training_steps, \n                    num_firstepoch_steps = num_firstepoch_steps,\n                )\n            self._created_lr_scheduler = True\n            return self.lr_scheduler\n        elif self.args.lr_scheduler_type == 'constant_with_warmup':\n           \n            half_step_acc = num_training_steps_acc//2\n            \n            if num_warmup_steps>0:\n                print(f\"Warmup doesn't apply to this scheduler [Raise-Fall]\")\n\n            print (f\"Scheduler Raise: 0-{half_step_acc}, Fall {half_step_acc}-{num_training_steps_acc}\")\n\n            self.lr_scheduler = custom_raise_fall_scheduler_with_warmup(\n                    optimizer=self.optimizer if optimizer is None else optimizer,\n                    num_warmup_steps=num_warmup_steps,\n                    num_training_steps=num_training_steps, \n                    num_firstepoch_steps = num_firstepoch_steps,\n                )\n            self._created_lr_scheduler = True\n            return self.lr_scheduler        \n        else:\n            return  super().create_scheduler(num_training_steps=num_training_steps, optimizer=optimizer)", "extensions/Training_PRO/script.py": "import os\n\nos.environ[\"WANDB_MODE\"] = \"offline\"\n# os.environ[\"WANDB_DISABLED\"] = \"true\"\n\nimport json\nimport math\nimport random\nimport shutil\nimport sys\nimport threading\nimport time\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport gradio as gr\nimport pandas as pd\nimport torch\nimport transformers\n\nfrom functools import partial\n\nfrom .custom_scheduler import FPSchedulerTrainer, FPNEFtuneTrainer\n\nfrom .matplotgraph import create_graph\nfrom .train_utils import get_available_loras_local, precise_cut, sliding_block_cut, download_file_from_url\n\nfrom datasets import Dataset, load_dataset\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    set_peft_model_state_dict\n)\nfrom peft.utils.other import \\\n    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING as model_to_lora_modules\nfrom transformers.models.auto.modeling_auto import (\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n)\n\nfrom modules import shared, utils\nfrom modules.ui import create_refresh_button\n\nfrom modules.evaluate import (\n    calculate_perplexity,\n    generate_markdown_table,\n    save_past_evaluations\n)\nfrom modules.logging_colors import logger\nfrom modules.models import reload_model\nfrom modules.utils import natural_keys\n\nimport warnings\nwarnings.filterwarnings(action = \"ignore\", message=\"torch.utils.checkpoint:\")\nwarnings.filterwarnings(action = \"ignore\", message=\"`do_sample` is set to `False`\")\n\nparams = {\n        \"display_name\": \"Training PRO\",\n        \"is_tab\": True\n}\n\nnon_serialized_params = {\n        \"debug_slicer\": False,\n        \"Lora_sortedByTime\": False,\n        \"stop_at_loss\": 0,\n        \"save_steps_under_loss\": 0.0,\n        \"save_checkpoint_now\": False,\n        \"training_loop\": False,\n        \"current_stability\": 0,\n        \"save_epochs\": 0,\n        \"checkpoint_offset\": 0,\n        \"epoch_offset\":0,\n        \"safe_serialization\": False,\n}\n\nMODEL_CLASSES = {v[1]: v[0] for v in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.items()}\n\nPARAMETERS = [\"lora_name\", \"always_override\", \"save_steps\", \"micro_batch_size\", \"batch_size\", \"epochs\", \"learning_rate\", \"lr_scheduler_type\", \"lora_rank\", \"lora_alpha\", \"lora_dropout\", \"cutoff_len\", \"dataset\", \"eval_dataset\", \"format\", \"eval_steps\", \"raw_text_file\", \"higher_rank_limit\", \"warmup_steps\", \"optimizer\", \"hard_cut_string\", \"train_only_after\", \"stop_at_loss\", \"add_eos_token\", \"min_chars\", \"report_to\", \"precize_slicing_overlap\", \"add_eos_token_type\", \"save_steps_under_loss\", \"add_bos_token\", \"training_projection\",\"sliding_window\",\"warmup_ratio\",\"grad_accumulation\",\"neft_noise_alpha\"]\nWANT_INTERRUPT = False\n\ntrain_log = {}\ntrain_template = {}\ntrain_log_graph = []\ntrain_choices = [\"all\",\"q-k-v-o\",\"q-k-v\",\"k-v-down\",\"q-v\"]\n\nstatistics = {\n\t\t\t'loss': [],\n\t\t\t'lr': [],\n}\n\nRED = \"\\033[91m\"\nYELLOW = \"\\033[93m\"\nGREEN = \"\\033[92m\"\nRESET = \"\\033[0m\"\n\ndef ui():\n\n    with gr.Tab('Train LoRA', elem_id='lora-train-tab'):\n        tmp = gr.State('')\n        with gr.Row():\n            with gr.Column():\n                # YY.MM.DD\n                gr.Markdown(\"`Ver: 23.10.20 (REV2)` This is enhanced version of QLora Training. [Maintained by FP](https://github.com/FartyPants/Training_PRO/tree/main)\")\n\n                with gr.Row():\n                    with gr.Column(scale=5):\n                        with gr.Row():\n                            copy_from = gr.Dropdown(label='Copy parameters from', value='None', choices=get_available_loras_local(non_serialized_params['Lora_sortedByTime']), elem_classes=['slim-dropdown'])\n                            create_refresh_button(copy_from, lambda: None, lambda: {'choices': get_available_loras_local(non_serialized_params['Lora_sortedByTime'])}, 'refresh-button')\n                    with gr.Column():\n                        sort_byTime = gr.Checkbox(label='Sort list by Date', value=False, info='Sorts Loras by date created.', elem_classes=['no-background'])                        \n\n                with gr.Row():\n                    with gr.Column(scale=5):\n                        lora_name = gr.Textbox(label='Name', info='The name of your new LoRA file')\n    \n                    with gr.Column():\n                        always_override = gr.Checkbox(label='Override Existing Files', value=False, info='If the name is the same, checking will replace the existing file, and unchecking will load and continue from it (the rank must be the same).', elem_classes=['no-background'])\n\n                with gr.Row():\n                    with gr.Column():\n                        lora_rank = gr.Slider(label='LoRA Rank', value=32, minimum=0, maximum=1024, step=4, info='Also called dimension count. Higher values = larger file, more content control. Smaller values = smaller file, less control. Use 4 or 8 for style, 128 or 256 to teach, 1024+ for fine-detail on big data. More VRAM is needed for higher ranks.')\n                        lora_alpha = gr.Slider(label='LoRA Alpha', value=64, minimum=0, maximum=2048, step=4, info='This divided by the rank becomes the scaling of the LoRA. Higher means stronger. A good standard value is twice your Rank.')\n                        batch_size = gr.Slider(visible= False, label='Batch Size', value=0, minimum=0, maximum=1024, step=4, info='Now Replaced with Gradient accumulation. Keeping it for sake of old saved data')\n                        micro_batch_size = gr.Slider(label='True Batch Size', value=4, minimum=1, maximum=128, step=1, info='Specifies how many text blocks per step will be trained. The higher value, the better the concept of training will be, but it requires more GPU memory and it reduces speed.')\n                        grad_accumulation = gr.Slider(label='Gradient Accumulation Steps', value=1, minimum=1, maximum=256, step=1, info=\"Virtually multiplies the Batch Size by averaging the learning over more than one step. VRAM friendly. Evens out loss fluctuations but can also degrade training fidelity.\")\n\n                    with gr.Column():\n                        stop_at_loss = gr.Slider(label='Stop at loss (Can be changed during training)', minimum=0.0, maximum=3.0, step=0.1, value=0.00, info='The process will automatically stop once the desired loss value is reached.')\n                        gr.Markdown(\" \")\n                        epochs = gr.Number(label='Epochs', value=3, info='Number of times every entry in the dataset should be fed into training. So 1 means feed each item in once, 5 means feed it in five times, etc.')\n                        learning_rate = gr.Textbox(label='Learning Rate', value='3e-4', info='In scientific notation. 3e-4 is a good starting base point. 1e-2 is extremely high, 1e-6 is extremely low.')\n                        lr_scheduler_type = gr.Dropdown(label='LR Scheduler', value='linear', choices=['linear', 'constant', 'constant_with_warmup', 'cosine', 'cosine_with_restarts', 'polynomial', 'inverse_sqrt', 'FP_low_epoch_annealing', 'FP_half_time_annealing','FP_raise_fall_creative'], info='Learning rate scheduler - defines how the learning rate changes over time. Custom schedulers: FP_low_epoch_annealing, FP_half_time_annealing, FP_raise_fall_creative (see README)', elem_classes=['slim-dropdown'])\n                        \n                with gr.Accordion(label='Checkpoints', open=True):\n                    with gr.Row():\n                        with gr.Column():\n                            save_steps = gr.Number(label='Save every n steps', value=0, info='A checkpoint will be saved every n steps and at each Epoch boundary. (0 = OFF)')\n                        with gr.Column():    \n                            save_steps_under_loss = gr.Slider(label='Save at 10% Loss change', value=1.8, minimum=0.0, maximum=3.0, step=0.1, info=\"Saves checkpoints at (or bellow) this loss and then each time loss falls by at least 10% This works independently from 'Save every n steps'\")    \n                    with gr.Row():        \n                        save_chackpoint_now = gr.Button('Queue Checkpoint Now')\n\n                with gr.Accordion(label='Advanced Options', open=True):\n                    with gr.Row():\n                        with gr.Column():\n                            warmup_steps = gr.Number(label='Warmup Steps', value=100, info='Number of max steps used for a linear warmup. Reduces early over-fitting by the first training blocks. Value has precedent over Warmup Ratio. Aligns to the closest multiple of graddient accumulation')\n                            warmup_ratio = gr.Slider(label='Warmup Ratio', minimum=0.0, maximum=0.2, step=0.025, value=0.0, info='Ratio of total training steps that will be used for a linear warmup. It applies only if Warmup Step is 0.')\n                            neft_noise_alpha = gr.Slider(label='NEFtune noise scale', minimum=0.0, maximum=15, step=1, value=0.0, info='Add noise to the training to improve generalization. [0 - OFF, Starting value to experiment: 5]')\n                            training_projection = gr.Radio(value = train_choices[4], label='LLaMA Target Projections', info='Change the targets (LORA is typically q-v)', choices=train_choices)    \n                            lora_dropout = gr.Slider(label='LoRA Dropout', minimum=0.0, maximum=1.0, step=0.025, value=0.05, info='Percentage probability for dropout of LoRA layers. This can help reduce overfitting. Most users should leave at default.')\n                            optimizer = gr.Dropdown(label='Optimizer', value='adamw_torch', choices=['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'adamw_anyprecision', 'sgd', 'adagrad'], info='Different optimizer implementation options, for advanced users. Effects of different options are not well documented yet.', elem_classes=['slim-dropdown'])\n\n                        with gr.Column():\n                            train_only_after = gr.Textbox(label='Train Only After', value='', info='Only consider text *after* this string in any given chunk for training. For Alpaca datasets, use \"### Response:\" to only train the response and ignore the input.')\n                            add_bos_token = gr.Checkbox(label='Add BOS token', value=True, info=\"Adds BOS token for each dataset item\")\n                            add_eos_token = gr.Checkbox(label='Add EOS token', value=False, info=\"Adds EOS token for each dataset item\")\n                            add_eos_token_type = gr.Dropdown(label='EOS placement (Text file)', choices=['Every Block', 'Hard Cut Blocks Only'], value='Every Block', info='', allow_custom_value = False)\n                            \n                            higher_rank_limit = gr.Checkbox(label='Enable higher ranks', value=False, info='If checked, changes Rank/Alpha slider above to go much higher. This will not work without a datacenter-class GPU.')\n                            report_to = gr.Radio(label=\"Save detailed logs with\", value=\"None\", choices=[\"None\", \"wandb\", \"tensorboard\"], interactive=True)\n                # for future            \n                #with gr.Accordion(label='Dynamic Scheduler', open = False):\n                #    ds_min_epochs = gr.Number(label='Minimum Epochs', value='1', info='Minimum epochs that will be always performed before ramp down can be triggered')\n                #    ds_max_epochs = gr.Number(label='Maximum Epochs (fallback)', value='50', info='Maximum Epochs before the training will bail out completely (should be a large number)')\n                #    ds_loss_trigger = gr.Slider(label='Trigger Loss', minimum=0.0, maximum=2.8, step=0.1, value=1.6, info='Loss at which the ramp down schedule will be triggered')\n                #    ds_loss_rolling_window = gr.Number(label='Loss rolling average', value='4', info='Calculate loss by averaging last x numbers to avoid jumps and noise')\n                #    ds_epochs_to_ramp = gr.Slider(label='Ramp down ratio', minimum=0.0, maximum=2.0, step=0.1, value=1.00, info='How long the ramp down will last relative to ellapsed steps (before trigger)')\n                #    gr.Markdown('These are settings for FP_dynamic_loss_trigger scheduler. The scheduler will do warm up, then hold constant untill a loss falls under Trigger Loss, then it will commence linear ramp down schedule and stop. The length of ramp down is set by Ramp down ratio where (ramp down steps) = ratio * (elapsed steps). (The time to completition shown will be very high untill ramp down is triggered.)')\n                        \n\n            with gr.Column():\n                with gr.Tab(label='Formatted Dataset'):\n                    with gr.Row():\n                        with gr.Column():\n                            with gr.Row():\n                                dataset = gr.Dropdown(choices=get_datasets('training/datasets', 'json'), value='None', label='Dataset', info='The dataset file to use for training.', elem_classes=['slim-dropdown'])\n                                create_refresh_button(dataset, lambda: None, lambda: {'choices': get_datasets('training/datasets', 'json')}, 'refresh-button')\n                            with gr.Row():\n                                eval_dataset = gr.Dropdown(choices=get_datasets('training/datasets', 'json'), value='None', label='Evaluation Dataset', info='The (optional) dataset file used to evaluate the model after training.', elem_classes=['slim-dropdown'])\n                                create_refresh_button(eval_dataset, lambda: None, lambda: {'choices': get_datasets('training/datasets', 'json')}, 'refresh-button')\n\n                        with gr.Column():\n                            with gr.Row():\n                                format = gr.Dropdown(choices=get_datasets('training/formats', 'json'), value='None', label='Data Format', info='The format file used to decide how to format the dataset input.', elem_classes=['slim-dropdown'])\n                                create_refresh_button(format, lambda: None, lambda: {'choices': get_datasets('training/formats', 'json')}, 'refresh-button')\n                            with gr.Row():\n                                eval_steps = gr.Number(label='Evaluate every n steps', value=100, info='If an evaluation dataset is given, test it every time this many steps pass.')\n\n                with gr.Tab(label=\"Text file\"):\n                    with gr.Row():\n                        raw_text_file = gr.Dropdown(choices=get_datasets('training/datasets', 'txt'), value='None', label='Text file', info='The text file to use for training.', elem_classes=['slim-dropdown'])\n                        create_refresh_button(raw_text_file, lambda: None, lambda: {'choices': get_datasets('training/datasets', 'txt')}, 'refresh-button')\n\n                    with gr.Row():\n                        with gr.Column():\n                            precize_slicing_overlap = gr.Checkbox(label='Add Overlapping blocks', value = True)\n                            sliding_window = gr.Checkbox(label='DEMENTOR Long-form Learning by FP (Highly Experimental, use low epochs)', value = False, info='Deep Memorization Enforcement Through Overlapping and Repetition. (I named it, so shush). Special process for learning long-form text using low amount of epochs.')\n                            #debug_slicer = gr.Checkbox(label='Dump sentencelist.json to logs', value = non_serialized_params['debug_slicer'], info='Debug Slicer')\n\n                        with gr.Column():\n                            hard_cut_string = gr.Textbox(label='Hard Cut String', value='\\\\n\\\\n\\\\n', info='String that indicates a cut between logical blocks of text (ex. Ideas or Chapters). Helps prevent unwanted overlap between unrelated ideas.')\n                            min_chars = gr.Number(label='Ignore small blocks', value=0, info='Ignore Text blocks that have less or equal characters than this number.')\n                with gr.Tab(label=\"URL\"):\n                    with gr.Row():\n                        with gr.Column():\n                            download_file_url = gr.Textbox(label='Download JSON or txt file to datasets (or formats) folder', value='',info='The URL of a file to download. If on github, make sure you get url of the raw file (https://raw.githubusercontent.com/...). If huggin face, make sure the url has /resolve/ in it not /blob/')\n                            with gr.Row():\n                                download_check_overwrite = gr.Checkbox(label='Overwrite', value=False, info='Overwrite if file exist')\n                                download_folder = gr.Radio(label=\"Destination\", value='training/datasets', choices=['training/datasets', 'training/formats'], interactive=True)\n                            download_button = gr.Button('Download')\n                            download_status = gr.Textbox(label='Download Status', value='', interactive=False)\n                with gr.Row():\n                    with gr.Column():\n                        with gr.Row():\n                            cutoff_len = gr.Slider(label='Chunk Length (Cutoff Length)', minimum=32, maximum=2048, value=256, step=32, info='The maximum length of a chunk (in tokens). Applies to both JSON dataset and text files. Higher values require much more VRAM.')\n                with gr.Row():\n                    with gr.Column():\n                        check_dataset_btn = gr.Button('Verify Dataset/Text File and suggest data entries')    \n                        check_dataset_txt = gr.Textbox(label='Dataset info', value='')\n\n                with gr.Row():\n                    start_button = gr.Button(\"Start LoRA Training\", variant='primary')\n                    stop_button = gr.Button(\"Interrupt\")\n\n                with gr.Accordion(label=\"Graph\", open=True):\n                    with gr.Row():\n                        # show_actions_button = False - we use old gradio\n                        plot_graph = gr.LinePlot(x=\"epoch\", y=\"value\", title=\"Loss Metrics\", overlay_point=True, tooltip=[\"epoch\", \"value\"], x_lim=[0, 1], y_lim=[0, 3.5], width=500, height=250) \n \n                output = gr.Markdown(value=\"Ready\")\n\n    with gr.Tab('Perplexity evaluation', elem_id='evaluate-tab'):\n        with gr.Row():\n            with gr.Column():\n                models = gr.Dropdown(utils.get_available_models(), label='Models', multiselect=True)\n                evaluate_text_file = gr.Dropdown(choices=['wikitext', 'ptb', 'ptb_new'] + get_datasets('training/datasets', 'txt')[1:], value='wikitext', label='Input dataset', info='The text file on which the model will be evaluated. The first options are automatically downloaded: wikitext, ptb, and ptb_new. The next options are your local text files under training/datasets.')\n                with gr.Row():\n                    with gr.Column():\n                        stride_length = gr.Slider(label='Stride', minimum=1, maximum=2048, value=512, step=1, info='Used to make the evaluation faster at the cost of accuracy. 1 = slowest but most accurate. 512 is a common value.')\n\n                    with gr.Column():\n                        max_length = gr.Slider(label='max_length', minimum=0, maximum=shared.settings['truncation_length_max'], value=0, step=1, info='The context for each evaluation. If set to 0, the maximum context length for the model will be used.')\n\n                with gr.Row():\n                    start_current_evaluation = gr.Button(\"Evaluate loaded model\")\n                    start_evaluation = gr.Button(\"Evaluate selected models\")\n                    stop_evaluation = gr.Button(\"Interrupt\")\n\n            with gr.Column():\n                evaluation_log = gr.Markdown(value='')\n\n        evaluation_table = gr.Dataframe(value=generate_markdown_table(), interactive=True)\n        with gr.Row():\n            save_comments = gr.Button('Save comments', elem_classes=\"small-button\")\n            refresh_table = gr.Button('Refresh the table', elem_classes=\"small-button\")\n\n    # Training events\n    all_params = [lora_name, always_override, save_steps, micro_batch_size, batch_size, epochs, learning_rate, lr_scheduler_type, lora_rank, lora_alpha, lora_dropout, cutoff_len, dataset, eval_dataset, format, eval_steps, raw_text_file, higher_rank_limit, warmup_steps, optimizer, hard_cut_string, train_only_after, stop_at_loss, add_eos_token, min_chars, report_to, precize_slicing_overlap, add_eos_token_type, save_steps_under_loss, add_bos_token, training_projection,sliding_window,warmup_ratio,grad_accumulation, neft_noise_alpha]\n\n    def fix_old_version(batch_size_val,micro_batch_size_val, grad_accumulation_val):\n        if batch_size_val>0:\n            gradient_acc =  batch_size_val // micro_batch_size_val\n            print(f\"Using Old version of Batch Size ({batch_size_val}) to set Gradient Accumulation: {gradient_acc}\")\n            return gradient_acc\n\n        return grad_accumulation_val\n\n    \n    copy_from.change(partial(do_copy_params, all_params= all_params), copy_from, all_params).then(fix_old_version,[batch_size,micro_batch_size, grad_accumulation],grad_accumulation)\n    start_button.click(do_train, all_params, [output,plot_graph])\n    stop_button.click(do_interrupt, None, None, queue=False)\n    higher_rank_limit.change(change_rank_limit, [higher_rank_limit], [lora_rank, lora_alpha])\n\n    def trigger_stop_at_loss(stop_at_loss_value):\n        non_serialized_params.update({\"stop_at_loss\": stop_at_loss_value})\n        if non_serialized_params['training_loop']:\n            print(f\"Queue: [Stop at loss Change] to {stop_at_loss_value}\")\n\n\n    stop_at_loss.change(trigger_stop_at_loss, stop_at_loss, None)\n\n    def trigger_save_checkpoint():\n        non_serialized_params.update({\"save_checkpoint_now\": True})\n        if non_serialized_params['training_loop']:\n            print(\"Queue: [Save checkpoint] Checkpoint will be saved after the current step is finished.\")\n        else:\n            print(\"Use during the training to save the checkpoint at any time.\")\n\n\n    def update_button():\n        return gr.Button.update('[Checkpoint in Queue]', variant='stop', interactive=True)\n\n    def update_button2():\n        time.sleep(1.0)\n        return gr.Button.update('Queue Checkpoint Now', variant='secondary',interactive = True)\n\n    save_chackpoint_now.click(trigger_save_checkpoint, None, None).then(update_button, None,save_chackpoint_now).then(update_button2, None,save_chackpoint_now)\n\n    dataset_calc_params = [save_steps,micro_batch_size, epochs, cutoff_len, dataset, format, raw_text_file, warmup_steps, hard_cut_string, min_chars, precize_slicing_overlap,sliding_window,warmup_ratio,grad_accumulation]\n\n    def check_dataset(save_steps:int, micro_batch_size: int, epochs: int, cutoff_len: int, dataset:str, format:str, raw_text_file:str, warmup_steps:int, hard_cut_string:str, min_chars:int, precize_slicing_overlap:bool,sliding_window:bool,warmup_ratio:float,grad_accumulation:int):\n        result = \"Specify JSON dastaset or Text file\"\n        total_blocks = 0\n        if shared.tokenizer is None:\n            yield \"Tokenizer is not available. Please Load some Model first.\"\n            return\n        \n        \n        if raw_text_file not in ['None', '']:\n            logger.info(\"Loading Text file...\")\n            fullpath = clean_path('training/datasets', f'{raw_text_file}')\n            fullpath = Path(fullpath)\n            if fullpath.is_dir():\n                logger.info('Training path directory {}'.format(raw_text_file))\n                raw_text = \"\"\n                file_paths = sorted(fullpath.glob('*.txt'), key=lambda path: natural_keys(path.name))\n                for file_path in file_paths:\n                    if file_path.is_file():\n                        with file_path.open('r', encoding='utf-8') as file:\n                            raw_text += file.read().replace('\\r', '')\n\n                        logger.info(f\"Loaded training file: {file_path.name}\")\n            else:\n                try:\n                    with open(clean_path('training/datasets', f'{raw_text_file}.txt'), 'r', encoding='utf-8') as file:\n                        raw_text = file.read().replace('\\r', '')\n                except:\n                    yield f\"{raw_text_file}.txt doesn't seem to exsist anymore... check your training/datasets folder\"\n                    return\n            \n \n            if min_chars<0:\n                min_chars = 0\n\n            # == New more precise slicing on sentence boundary ==\n            if sliding_window:\n                text_chunks = sliding_block_cut(raw_text, min_chars, False, cutoff_len, hard_cut_string,non_serialized_params['debug_slicer'])\n            else:\n                text_chunks = precise_cut(raw_text, precize_slicing_overlap, min_chars, False, cutoff_len, hard_cut_string,non_serialized_params['debug_slicer'])\n\n            total_blocks = len(text_chunks)\n            result = f\"Text: ({raw_text_file}.txt) has {total_blocks} blocks (Block Size {cutoff_len} tokens)\"\n            del text_chunks\n       \n        else:\n            if dataset in ['None', '']:\n                yield \"Select dataset or text file.\"\n                return \n\n            if format in ['None', '']:\n                yield \"Select format choice for dataset.\"\n                return\n\n            with open(clean_path('training/formats', f'{format}.json'), 'r', encoding='utf-8-sig') as formatFile:\n                format_data: dict[str, str] = json.load(formatFile)\n\n            def generate_prompt(data_point: dict[str, str]):\n                for options, data in format_data.items():\n                    if set(options.split(',')) == set(x[0] for x in data_point.items() if (type(x[1]) is str and len(x[1].strip()) > 0)):\n                        for key, val in data_point.items():\n                            if type(val) is str:\n                                data = data.replace(f'%{key}%', val)\n                        return data\n                raise RuntimeError(f'Data-point \"{data_point}\" has no keyset match within format \"{list(format_data.keys())}\"')\n\n            def tokenize_dummy(prompt):\n\n                input_ids = shared.tokenizer.encode(prompt, truncation=True, max_length=cutoff_len)\n                labels = [1] * len(input_ids)\n                input_ids = torch.tensor(input_ids)\n                return {\n                    \"input_ids\": input_ids,\n                    \"labels\": labels,\n                    \"attention_mask\": input_ids.ne(shared.tokenizer.pad_token_id),\n                }\n\n            def generate_and_tokenize_prompt(data_point):\n                prompt = generate_prompt(data_point)\n                return tokenize_dummy(prompt)\n\n            logger.info(\"Loading JSON datasets...\")\n            data = load_dataset(\"json\", data_files=clean_path('training/datasets', f'{dataset}.json'))\n            \n            data_keys = [] \n\n            if data:\n                if 'train' in data:  # Check if the 'train' split exists in the dataset\n                    data_keys = list(data['train'][0].keys())\n                    print(\"Data Keys:\", data_keys)\n            else:\n                print(\"The dataset is empty.\")\n\n            train_data = data['train'].map(generate_and_tokenize_prompt, new_fingerprint='%030x' % random.randrange(16**30))\n            total_blocks = train_data.num_rows\n\n            result = f\"Dataset: ({dataset}.json) has {total_blocks} blocks @ length = {cutoff_len} tokens\\n(Keys: {data_keys} - Format: {format}.json): \"\n\n            #for options, data in format_data.items():\n            #    format_keys = options.split(',')\n            #    result += f\"{format_keys}, \"\n            #result = result.rstrip()    \n            #result = result.rstrip(',')  \n\n        if total_blocks>0:\n            number_ofSteps = int(math.ceil(total_blocks / micro_batch_size) * epochs) \n            num_stepsPer_epoch = int(math.ceil(number_ofSteps/epochs))\n            min_warm = math.ceil(100 / grad_accumulation)\n\n            warmup_steps_suggest = min(int(min_warm*grad_accumulation), int(math.ceil(number_ofSteps * 0.1)))\n            warmup_steps_suggest = min(warmup_steps_suggest,num_stepsPer_epoch)\n\n            save_each_n_min = int(math.ceil(number_ofSteps/10))\n            save_each_n_max = int(math.ceil(number_ofSteps/5))\n            gradient_accumulation_max = int(total_blocks)//micro_batch_size\n\n \n            result += f\"\\n[Batch Size: {micro_batch_size}, Epochs: {epochs}, Gradient Accumulation: {grad_accumulation}]\\n\"\n            result += f\"Total number of steps: {number_ofSteps}\\n\"\n            result += f\"Steps per each Epoch: {num_stepsPer_epoch}\\n\"\n            result += f\"Suggestions:\\n\"\n            result += f\"Checkpoints: Save every {save_each_n_min} - {save_each_n_max} steps (Current: {int(save_steps)})\\n\"\n            result += f\"Warmup steps: {warmup_steps_suggest} (Current: {int(warmup_steps)})\"\n            if gradient_accumulation_max < grad_accumulation: \n                result += f\"\\n\\nWARNING: Gradient Accumulation {grad_accumulation} is too high: It should be below {gradient_accumulation_max}\"\n\n\n        yield result\n        return\n    \n    check_dataset_btn.click(check_dataset, dataset_calc_params ,check_dataset_txt)\n\n    # Evaluation events. For some reason, the interrupt event\n    # doesn't work with the .then() syntax, so I write them one\n    # by one in this ugly but functional way.\n    ev = start_evaluation.click(calculate_perplexity, [models, evaluate_text_file, stride_length, max_length], evaluation_log, show_progress=False)\n    start_evaluation.click(generate_markdown_table, None, evaluation_table, show_progress=False)\n\n    start_current_evaluation.click(lambda: ['current model'], None, tmp)\n    ev_cur = start_current_evaluation.click(calculate_perplexity, [tmp, evaluate_text_file, stride_length, max_length], evaluation_log, show_progress=False)\n    start_current_evaluation.click(generate_markdown_table, None, evaluation_table, show_progress=False)\n\n    stop_evaluation.click(None, None, None, cancels=[ev, ev_cur], queue=False)\n    refresh_table.click(generate_markdown_table, None, evaluation_table, show_progress=True)\n    save_comments.click(\n        save_past_evaluations, evaluation_table, None).then(\n        lambda: \"Comments saved.\", None, evaluation_log, show_progress=False)\n\n    def reload_lora():\n        return gr.Dropdown.update(choices=get_available_loras_local(non_serialized_params['Lora_sortedByTime']))\n \n    # nonserialized items\n\n    sort_byTime.change(lambda x: non_serialized_params.update({\"Lora_sortedByTime\": x}), sort_byTime, None).then(reload_lora,None,copy_from) \n    #debug_slicer.change(lambda x: non_serialized_params.update({\"debug_slicer\": x}), debug_slicer, None)\n\n    def update_dataset():\n        return gr.update(choices=get_datasets('training/datasets', 'json')), gr.update(choices=get_datasets('training/datasets', 'txt'))\n\n    download_button.click(download_file_from_url, [download_file_url,download_check_overwrite,download_folder] , download_status).then(update_dataset,None,[dataset , raw_text_file])\n\ndef get_datasets(path: str, ext: str):\n    # include subdirectories for raw txt files to allow training from a subdirectory of txt files\n    #if ext == \"txt\":\n    #    return ['None'] + sorted(set([k.stem for k in list(Path(path).glob('txt')) + list(Path(path).glob('*/')) if k.stem != 'put-trainer-datasets-here']), key=natural_keys)\n\n    return ['None'] + sorted(set([k.stem for k in Path(path).glob(f'*.{ext}') if k.stem != 'put-trainer-datasets-here']), key=natural_keys)\n\ndef do_interrupt():\n    global WANT_INTERRUPT\n    WANT_INTERRUPT = True\n\n\ndef do_copy_params(lora_name: str, all_params):\n\n    if lora_name:\n        f_name = f\"{shared.args.lora_dir}/{clean_path(None, lora_name)}/training_parameters.json\"\n        if Path(f_name).is_file():\n            with open(f_name, 'r', encoding='utf-8') as format_file:\n                params: dict[str, str] = json.load(format_file)\n        else:\n            params = {}\n    else:\n        params = {}        \n\n    result = list()\n    for i in range(0, len(PARAMETERS)):\n        key = PARAMETERS[i]\n        if key in params:\n            result.append(params[key])\n        else:\n            result.append(all_params[i])\n\n    return result\n\n\ndef change_rank_limit(use_higher_ranks: bool):\n    mult = 2 if use_higher_ranks else 1\n    return {\"maximum\": 1024 * mult, \"__type__\": \"update\"}, {\"maximum\": 2048 * mult, \"__type__\": \"update\"}\n\n\ndef clean_path(base_path: str, path: str):\n    \"\"\"Strips unusual symbols and forcibly builds a path as relative to the intended directory.\"\"\"\n    path = path.replace('\\\\', '/').replace('..', '_')\n    if base_path is None:\n        return path\n\n    return f'{Path(base_path).absolute()}/{path}'\n\n\ndef backup_adapter(input_folder):\n    # Get the creation date of the file adapter_model.bin\n    try:\n        adapter_file = Path(f\"{input_folder}/adapter_model.bin\")\n        if adapter_file.is_file():\n\n            logger.info(\"Backing up existing LoRA adapter...\")\n            creation_date = datetime.fromtimestamp(adapter_file.stat().st_ctime)\n            creation_date_str = creation_date.strftime(\"Backup-%Y-%m-%d\")\n\n            # Create the new subfolder\n            subfolder_path = Path(f\"{input_folder}/{creation_date_str}\")\n            subfolder_path.mkdir(parents=True, exist_ok=True)\n\n            # Check if the file already exists in the subfolder\n            backup_adapter_file = Path(f\"{input_folder}/{creation_date_str}/adapter_model.bin\")\n            if backup_adapter_file.is_file():\n                print(\" - Backup already exists. Skipping backup process.\")\n                return\n\n            # Copy existing files to the new subfolder\n            existing_files = Path(input_folder).iterdir()\n            for file in existing_files:\n                if file.is_file():\n                    shutil.copy2(file, subfolder_path)\n    except Exception as e:\n        print(\"An error occurred in backup_adapter:\", str(e))\n\n\ndef calc_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        num_params = param.numel()\n        # if using DS Zero 3 and the weights are initialized empty\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    return trainable_params, all_param\n\n\n\ndef do_train(lora_name: str, always_override: bool, save_steps: int, micro_batch_size: int, batch_size: int, epochs: int, learning_rate: str, lr_scheduler_type: str, lora_rank: int, lora_alpha: int, lora_dropout: float, cutoff_len: int, dataset: str, eval_dataset: str, format: str, eval_steps: int, raw_text_file: str, higher_rank_limit: bool, warmup_steps: int, optimizer: str, hard_cut_string: str, train_only_after: str, stop_at_loss: float, add_eos_token: bool, min_chars: int, report_to: str, precize_slicing_overlap: bool, add_eos_token_type: str, save_steps_under_loss: float, add_bos_token: bool, training_projection: str,sliding_window:bool,warmup_ratio:float, grad_accumulation: int,neft_noise_alpha:float):\n\n    if shared.args.monkey_patch:\n        from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import (\n            replace_peft_model_with_int4_lora_model\n        )\n        replace_peft_model_with_int4_lora_model()\n    \n    global train_log_graph\n    global WANT_INTERRUPT\n    WANT_INTERRUPT = False\n\n    statistics['loss'] = []\n\n    statistics['loss'].append({'epoch': 0, 'value': 0})\n    zero_pd = pd.DataFrame(statistics['loss'])\n\n    # == Input validation / processing ==\n    yield \"Preparing the input...\", zero_pd\n    lora_file_path = clean_path(None, lora_name)\n    if lora_file_path.strip() == '':\n        yield \"Missing or invalid LoRA file name input.\", zero_pd\n        return\n\n    lora_file_path = f\"{Path(shared.args.lora_dir)}/{lora_file_path}\"\n    actual_lr = float(learning_rate)\n    model_type = type(shared.model).__name__\n\n    if model_type in MODEL_CLASSES:\n        model_id = MODEL_CLASSES[model_type]\n    else:\n        model_id = \"llama\"\n        if model_type == \"PeftModelForCausalLM\":\n            if len(shared.lora_names) > 0:\n                yield \"You are trying to train a LoRA while you already have another LoRA loaded. This will work, but may have unexpected effects. *(Will continue anyway in 5 seconds, press `Interrupt` to stop.)*\", zero_pd\n                logger.warning(\"Training LoRA over top of another LoRA. May have unexpected effects.\")\n            else:\n                yield \"Model ID not matched due to LoRA loading. Consider reloading base model. *(Will continue anyway in 5 seconds, press `Interrupt` to stop.)*\", zero_pd\n                logger.warning(\"Model ID not matched due to LoRA loading. Consider reloading base model.\")\n        else:\n            yield \"LoRA training has only currently been validated for LLaMA, OPT, GPT-J, and GPT-NeoX models. Unexpected errors may follow. *(Will continue anyway in 5 seconds, press `Interrupt` to stop.)*\", zero_pd\n            logger.warning(f\"LoRA training has only currently been validated for LLaMA, OPT, GPT-J, and GPT-NeoX models. (Found model type: {model_type})\")\n\n        time.sleep(5)\n\n    if shared.args.loader == 'GPTQ-for-LLaMa' and not shared.args.monkey_patch:\n        yield \"LoRA training with GPTQ-for-LLaMa requires loading with `--monkey-patch`\", zero_pd\n        return\n\n    if cutoff_len <= 0 or micro_batch_size <= 0 or actual_lr <= 0 or lora_rank <= 0 or lora_alpha <= 0:\n        yield \"Cannot input zeroes.\", zero_pd\n        return\n\n    #in new version we dumped this in favor of grad_accumulation\n    #set it to zero fo new save\n    batch_size = 0\n\n    gradient_accumulation_steps = grad_accumulation #batch_size // micro_batch_size\n    shared.tokenizer.pad_token_id = 0\n    shared.tokenizer.padding_side = \"left\"\n\n    def encode(text, prepend_bos_token):\n       \n        result = shared.tokenizer.encode(text, truncation=True, max_length=cutoff_len)\n        # Check if the first two tokens are BOS\n        if len(result) >= 2 and result[:2] == [shared.tokenizer.bos_token_id, shared.tokenizer.bos_token_id]:\n            result = result[1:]\n\n        if not prepend_bos_token and result[0] == shared.tokenizer.bos_token_id:\n            result = result[1:]\n        return result\n\n    def tokenize(prompt, append_eos_token=False, prepend_bos_token = False):\n\n        if train_only_after == '' or train_only_after not in prompt:\n            input_ids = encode(prompt, prepend_bos_token)\n\n            if append_eos_token and input_ids[-1] != shared.tokenizer.eos_token_id and len(input_ids) < cutoff_len:\n                input_ids.append(shared.tokenizer.eos_token_id)\n\n            input_ids = [shared.tokenizer.pad_token_id] * (cutoff_len - len(input_ids)) + input_ids\n            \n            labels = [1] * len(input_ids)\n        else:\n            ind = prompt.index(train_only_after) + len(train_only_after)\n            before_tokens = encode(prompt[:ind], prepend_bos_token)\n            after_tokens = encode(prompt[ind:], False)\n\n            if append_eos_token and after_tokens[-1] != shared.tokenizer.eos_token_id:\n                after_tokens.append(shared.tokenizer.eos_token_id)\n\n            full_length = len(after_tokens) + len(before_tokens)\n            if full_length > cutoff_len:\n                after_tokens = after_tokens[:cutoff_len - len(before_tokens)]\n            else:\n                before_tokens = [shared.tokenizer.pad_token_id] * (cutoff_len - full_length) + before_tokens\n\n            input_ids = before_tokens + after_tokens\n            labels = [-100] * len(before_tokens) + [1] * len(after_tokens)\n\n        input_ids = torch.tensor(input_ids)\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n            \"attention_mask\": input_ids.ne(shared.tokenizer.pad_token_id),\n        }\n\n    train_template.clear()\n            \n    #reset stuff\n    print(f\"*** LoRA: {lora_name} ***\")\n    non_serialized_params.update({\"stop_at_loss\": stop_at_loss})\n    non_serialized_params.update({\"save_steps_under_loss\": save_steps_under_loss+0.01})\n    non_serialized_params.update({\"save_checkpoint_now\": False})\n    non_serialized_params.update({\"training_loop\": False})\n    non_serialized_params.update({\"current_stability\": 0})\n    non_serialized_params.update({\"save_epochs\": 0})\n    non_serialized_params.update({\"checkpoint_offset\": 0})\n    non_serialized_params.update({\"epoch_offset\": 0})\n    train_log_graph.clear()\n  \n     # == Prep the dataset, format, etc ==\n    if raw_text_file not in ['None', '']:\n        train_template[\"template_type\"] = \"raw_text\"\n        logger.info(\"Loading text file...\")\n        fullpath = clean_path('training/datasets', f'{raw_text_file}')\n        fullpath = Path(fullpath)\n        if fullpath.is_dir():\n            logger.info('Training path directory {}'.format(raw_text_file))\n            raw_text = \"\"\n            file_paths = sorted(fullpath.glob('*.txt'), key=lambda path: natural_keys(path.name))\n            for file_path in file_paths:\n                if file_path.is_file():\n                    with file_path.open('r', encoding='utf-8') as file:\n                        raw_text += file.read().replace('\\r', '')\n\n                    logger.info(f\"Loaded training file: {file_path.name}\")\n        else:\n            with open(clean_path('training/datasets', f'{raw_text_file}.txt'), 'r', encoding='utf-8') as file:\n                raw_text = file.read().replace('\\r', '')\n        \n        # FPHAM PRECISE SLICING        \n        if min_chars<0:\n            min_chars = 0\n\n        add_EOS_to_all = add_eos_token and add_eos_token_type == 'Every Block'\n        add_EOS_to_HC = add_eos_token and add_eos_token_type != 'Every Block'\n\n        #print (f\"add_eos_token {add_eos_token}, add_EOS_to_all {add_EOS_to_all}, add_EOS_to_HC {add_EOS_to_HC}\")\n\n        # == New more precise slicing on sentence boundary ==\n        if sliding_window:\n            text_chunks = sliding_block_cut(raw_text, min_chars, add_EOS_to_HC, cutoff_len, hard_cut_string,non_serialized_params['debug_slicer'])\n        else:\n            text_chunks = precise_cut(raw_text, precize_slicing_overlap, min_chars, add_EOS_to_HC, cutoff_len, hard_cut_string,non_serialized_params['debug_slicer'])\n\n        train_data = Dataset.from_list([tokenize(x, add_EOS_to_all, add_bos_token) for x in text_chunks])\n        if add_EOS_to_all:\n            print(f\"Added EOS to {len(text_chunks)} blocks\") \n\n        print(f\"All Data Blocks: {len(text_chunks)}\")\n\n        del text_chunks\n        eval_data = None\n    else:\n        if dataset in ['None', '']:\n            yield \"Missing dataset choice input, cannot continue.\", zero_pd\n            return\n\n        if format in ['None', '']:\n            yield \"Missing format choice input, cannot continue.\", zero_pd\n            return\n\n        train_template[\"template_type\"] = \"dataset\"\n\n        with open(clean_path('training/formats', f'{format}.json'), 'r', encoding='utf-8-sig') as formatFile:\n            format_data: dict[str, str] = json.load(formatFile)\n\n        # == store training prompt ==\n        for _, value in format_data.items():\n            prompt_key = f\"template_{len(train_template)}\"\n            train_template[prompt_key] = value\n\n        def generate_prompt(data_point: dict[str, str]):\n            for options, data in format_data.items():\n                if set(options.split(',')) == set(x[0] for x in data_point.items() if (type(x[1]) is str and len(x[1].strip()) > 0)):\n                    for key, val in data_point.items():\n                        if type(val) is str:\n                            data = data.replace(f'%{key}%', val)\n                    return data\n            raise RuntimeError(f'Data-point \"{data_point}\" has no keyset match within format \"{list(format_data.keys())}\"')\n\n        def generate_and_tokenize_prompt(data_point):\n            prompt = generate_prompt(data_point)\n            return tokenize(prompt, add_eos_token, add_bos_token)\n\n        logger.info(\"Loading JSON datasets...\")\n        data = load_dataset(\"json\", data_files=clean_path('training/datasets', f'{dataset}.json'))\n        train_data = data['train'].map(generate_and_tokenize_prompt, new_fingerprint='%030x' % random.randrange(16**30))\n\n        print(f\"BOS: {add_bos_token} EOS: {add_eos_token}\") \n        print(f\"Data Blocks: {train_data.num_rows}\")\n\n        if eval_dataset == 'None':\n            eval_data = None\n        else:\n            eval_data = load_dataset(\"json\", data_files=clean_path('training/datasets', f'{eval_dataset}.json'))\n            eval_data = eval_data['train'].map(generate_and_tokenize_prompt, new_fingerprint='%030x' % random.randrange(16**30))\n\n    # == We MUST reload model if it went through any previous training, even failed one ==\n    if shared.model_dirty_from_training:\n        selected_model = shared.model_name\n        if selected_model:\n            print(\"\\033[1;31;1m(Model has been modified by previous training, it needs to be reloaded...)\\033[0;37;0m\")\n            try:\n                yield f\"Reloading {selected_model}...\", zero_pd\n                reload_model()\n                shared.tokenizer.pad_token_id = 0\n                shared.tokenizer.padding_side = \"left\"\n\n                if shared.model is not None:\n                    print(\"Model reloaded OK, continue with training.\")\n                else:\n                    return f\"Failed to load {selected_model}.\"\n            except:\n                exc = traceback.format_exc()\n                logger.error('Failed to reload the model.')\n                print(exc)\n                return exc.replace('\\n', '\\n\\n')\n\n    # == Start prepping the model itself ==\n    if not hasattr(shared.model, 'lm_head') or hasattr(shared.model.lm_head, 'weight'):\n        logger.info(\"Getting model ready...\")\n        # here we can disable gradient checkpoint, by default = true,  use_gradient_checkpointing=True\n        prepare_model_for_kbit_training(shared.model)\n\n    # base model is now frozen and should not be reused for any other LoRA training than this one\n    shared.model_dirty_from_training = True\n    print(f\"Transformers Model Type: {YELLOW}{model_type}{RESET}\")\n\n    if training_projection==train_choices[0]:\n        model_to_lora_modules[model_id] = [\"gate_proj\",\"down_proj\",\"up_proj\",\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n    elif training_projection==train_choices[1]:\n        model_to_lora_modules[model_id] = [\"q_proj\",\"k_proj\", \"v_proj\", \"o_proj\"]\n    elif training_projection==train_choices[2]:\n        model_to_lora_modules[model_id] = [\"q_proj\",\"k_proj\", \"v_proj\"]\n    elif training_projection==train_choices[3]:\n        model_to_lora_modules[model_id] = [\"k_proj\", \"v_proj\", \"down_proj\"]        \n    else:\n        model_to_lora_modules[model_id] = [\"q_proj\", \"v_proj\"]\n\n\n    logger.info(\"Preparing for training...\")\n    config = LoraConfig(\n        r=lora_rank,\n        lora_alpha=lora_alpha,\n        target_modules=model_to_lora_modules[model_id],\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n    # == Backup the existing adapter ==\n    if not always_override:\n        backup_adapter(lora_file_path)\n\n    # == get model trainable params\n    model_trainable_params, model_all_params = calc_trainable_parameters(shared.model)\n\n    try:\n        logger.info(\"Creating LoRA model...\")\n        lora_model = get_peft_model(shared.model, config)\n        if not always_override and Path(f\"{lora_file_path}/adapter_model.bin\").is_file():\n            logger.info(\"Loading existing LoRA data...\")\n            state_dict_peft = torch.load(f\"{lora_file_path}/adapter_model.bin\")\n            set_peft_model_state_dict(lora_model, state_dict_peft)\n\n            print(f\" + Continue Training on {RED}{lora_file_path}/adapter_model.bin{RESET}\")\n            \n            #load training_log.json if exist\n           \n            if Path(f\"{lora_file_path}/training_log.json\").is_file():\n                with open(f\"{lora_file_path}/training_log.json\", 'r') as json_file:\n                    json_ilog = json.load(json_file)\n                    for key, value in json_ilog.items():\n                        if key=='current_steps':\n                            non_serialized_params.update({\"checkpoint_offset\": int(value+1)})\n                            print(f\" + Checkpoints will be saved with offset: {RED}{non_serialized_params['checkpoint_offset']}{RESET}\")\n                        if key=='epoch':\n                            non_serialized_params.update({\"epoch_offset\": value})\n                            print(f\" + Epoch offset: {RED}{non_serialized_params['epoch_offset']}{RESET}\")\n           \n\n            if Path(f\"{lora_file_path}/training_graph.json\").is_file():\n                try:\n                    with open(f\"{lora_file_path}/training_graph.json\", 'r') as json_file:\n                        train_log_graph = json.load(json_file)\n                        print(\" + Training Graph loaded\")   \n                except:\n                    print(f\"Can't read training_graph\")\n\n\n    except:\n        yield traceback.format_exc().replace('\\n', '\\n\\n'), zero_pd\n        return\n\n    if shared.args.monkey_patch:\n        from alpaca_lora_4bit.autograd_4bit import Autograd4bitQuantLinear\n        from alpaca_lora_4bit.models import Linear4bitLt\n        for _, m in lora_model.named_modules():\n            if isinstance(m, Autograd4bitQuantLinear) or isinstance(m, Linear4bitLt):\n                if m.is_v1_model:\n                    m.zeros = m.zeros.half()\n                m.scales = m.scales.half()\n\n    class Tracked():\n        def __init__(self):\n            self.current_steps = 0\n            self.max_steps = 0\n            self.did_save = False\n\n    tracked = Tracked()\n    actual_save_steps = math.ceil(save_steps / gradient_accumulation_steps)\n\n    class Callbacks(transformers.TrainerCallback):\n        def on_step_begin(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs):\n            tracked.current_steps = state.global_step * gradient_accumulation_steps\n            tracked.max_steps = state.max_steps * gradient_accumulation_steps\n            ssteps10 = int(max(2,(state.max_steps/epochs)*0.1))\n\n            if WANT_INTERRUPT:\n                control.should_epoch_stop = True\n                control.should_training_stop = True\n            else:\n                current_loss = float(train_log.get('loss', 0.0))\n                current_epoch_int = int(float(train_log.get('epoch', 0.0)))\n              \n                force_save = False\n\n                current_steps_offset = tracked.current_steps + non_serialized_params['checkpoint_offset']\n\n                folder_save = f\"checkpoint-{current_steps_offset}\"    \n\n                # save if triggered by user\n                if non_serialized_params['save_checkpoint_now']:\n                    force_save = True\n                    non_serialized_params.update({\"save_checkpoint_now\": False})\n                    print(f\"\\033[1;31;1mSave Checkpoint manually trigerred.\\033[0;37;0m\")\n                    folder_save = f\"checkpoint-{current_steps_offset}-user\"  \n\n                patience = 3     # Set the number of consecutive steps for tracking stability\n                \n                if gradient_accumulation_steps==1:\n                    patience = 4\n\n                min_steps = ssteps10\n\n                # Save each time the loss is below the threshold \n                if current_loss < non_serialized_params['save_steps_under_loss'] and current_loss > 0 and state.global_step > min_steps:\n                    current_stability = non_serialized_params['current_stability']\n                    current_stability += 1\n                    non_serialized_params.update({\"current_stability\": current_stability}) \n\n                    if current_stability >= patience:\n                        current_stability = 0\n                        non_serialized_params.update({\"current_stability\": current_stability})     \n                        current_loss_dec = round(current_loss, 2)\n                        loss_str = f\"{current_loss_dec:.2f}\"\n                        loss_str = loss_str.replace('.', '_')\n                        new_save = (current_loss_dec-0.1) + 0.01\n                        non_serialized_params.update({\"save_steps_under_loss\": new_save})\n\n                        folder_save = f\"checkpoint-{current_steps_offset}-loss-{loss_str}\" \n                        force_save = True   \n\n                   \n                else:\n                    # Reset stability if the loss goes above the threshold\n                    non_serialized_params.update({\"current_stability\": 0})   \n\n                # Save full epochs\n                if actual_save_steps>0 and current_epoch_int > non_serialized_params['save_epochs'] and state.global_step > min_steps: \n\n                    \n                    current_epoch_offset = current_epoch_int\n                    \n                    if non_serialized_params['epoch_offset'] > 0:\n                        current_epoch_offset = current_epoch_int + round(non_serialized_params['epoch_offset'], 2)\n                    \n                    ep_off_str = f\"{current_epoch_offset}\"\n                    ep_off_str = ep_off_str.replace('.', '_')\n                    folder_save = f\"checkpoint-{current_steps_offset}-epoch-{ep_off_str}\" \n\n                    non_serialized_params.update({\"save_epochs\": current_epoch_int})\n                    force_save = True\n\n                # save each actual_save_steps\n                if state.global_step > 0 and actual_save_steps > 0 and state.global_step % actual_save_steps == 0:\n                    folder_save = f\"checkpoint-{current_steps_offset}\"  \n                    force_save = True   \n\n                if force_save:       \n                    lora_model.save_pretrained(f\"{lora_file_path}/{folder_save}/\", safe_serialization = non_serialized_params['safe_serialization'])\n                    print(f\"\\033[1;30;40mStep: {tracked.current_steps:6} \\033[0;37;0m Saved: [{folder_save}]\")\n                    # Save log\n                    with open(f\"{lora_file_path}/{folder_save}/training_log.json\", 'w', encoding='utf-8') as file:\n                        json.dump(train_log, file, indent=2)\n                    # == Save training prompt ==\n                    with open(f\"{lora_file_path}/{folder_save}/training_prompt.json\", 'w', encoding='utf-8') as file:\n                        json.dump(train_template, file, indent=2)\n                \n\n        def on_substep_end(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs):\n            tracked.current_steps += 1\n            if WANT_INTERRUPT:\n                control.should_epoch_stop = True\n                control.should_training_stop = True\n\n        def on_log(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, logs, **kwargs):\n            train_log.update(logs)\n\n            current_steps_offset = tracked.current_steps + non_serialized_params['checkpoint_offset']\n            current_epoch_offset = train_log.get('epoch', 0.0) + non_serialized_params['epoch_offset']\n\n            train_log.update({\"current_steps\": tracked.current_steps})\n            train_log.update({\"current_steps_adjusted\": current_steps_offset})\n            train_log.update({\"epoch_adjusted\": current_epoch_offset})\n\n            if WANT_INTERRUPT:\n                print(\"\\033[1;31;1mInterrupted by user\\033[0;37;0m\")\n\n            if non_serialized_params['checkpoint_offset']>0:\n                print(f\"\\033[1;30;40mStep: {tracked.current_steps:6} [+{non_serialized_params['checkpoint_offset']}] \\033[0;37;0m\", end='')\n            else:\n                print(f\"\\033[1;30;40mStep: {tracked.current_steps:6} \\033[0;37;0m\", end='')\n            \n            graphentry = {\n                'current_steps': int(train_log.get('current_steps_adjusted',0)),\n                'loss': float(train_log.get('loss', 0.0)),\n                'learning_rate': float(train_log.get('learning_rate', 0.0)),\n                'epoch': float(train_log.get('epoch_adjusted', 0.0))\n            }\n\n            cur_loss = float(train_log.get('loss', 0.0))\n            cur_lr = float(train_log.get('learning_rate', 0.0))\n            cur_epoch = float(train_log.get('epoch', 0.0))\n            \n            if len(statistics['loss']) == 1:\n                first_epoch = statistics['loss'][0]['epoch']\n                first_value = statistics['loss'][0]['value']\n                if first_value ==0:\n                     statistics['loss'] = []\n\n\n            statistics['loss'].append({'epoch': cur_epoch, 'value': cur_loss})\n            statistics['lr'].append({'epoch': cur_epoch, 'value': cur_lr})\n\n            # Add the entry to the continuous log\n            train_log_graph.append(graphentry)\n\n            # Save the graph log for now, we can later generate full graph\n            with open(f\"{lora_file_path}/training_graph.json\", 'w') as file:\n                json.dump(train_log_graph, file, indent=4)\n\n            if 'loss' in logs:\n                loss = float(logs['loss'])\n                if loss <= stop_at_loss:\n                    control.should_epoch_stop = True\n                    control.should_training_stop = True\n                    print(f\"{RED}Stop Loss {stop_at_loss} reached.{RESET}\")\n\n    # FPHAM SAMPLE REQ Transformers error handling\n    gradient_accumulation_max = int(train_data.num_rows)//micro_batch_size\n    \n    if gradient_accumulation_max < gradient_accumulation_steps:\n        print(f\"{RED}WARNING:{RESET} Current gradient accumulation is {RED}too high{RESET} for the amount of training data.\")\n        print(f\"Gradient accumulation: {gradient_accumulation_steps} should be less than: {gradient_accumulation_max}. {RED}This could crash Accelerate/Transformers{RESET}\")\n        #min_batchSize = sample_req*micro_batch_size\n        print(f\"Preferable fix: {RED}Increase the size of dataset{RESET}\")\n        print(f\"... or Decrerase Gradient Accumulation {RED}{gradient_accumulation_steps}{RESET} to below {GREEN}{gradient_accumulation_max}{RESET}\")\n        gradient_accumulation_steps = max(1,gradient_accumulation_max-1)\n        print(f\"Last resort fix for this run: Lowering Gradient accumulation to {GREEN}{gradient_accumulation_steps}{RESET} [Good luck]\")\n\n    else:\n        print(f\"Data Size Check: Gradient accumulation: {YELLOW}{gradient_accumulation_steps}{RESET} <= Blocks/Batch {gradient_accumulation_max} ... {GREEN}[OK]{RESET}\")\n\n    #END OF FPHAM SAMPLE REQ\n\n    # FPHAM Custom Scheduler ==\n    custom_scheduller = False\n    lr_scheduler_type_arg = lr_scheduler_type\n\n    if lr_scheduler_type == 'FP_low_epoch_annealing':\n        custom_scheduller = True\n        lr_scheduler_type_arg = 'cosine'\n    elif lr_scheduler_type == 'FP_half_time_annealing':\n        custom_scheduller = True\n        lr_scheduler_type_arg = 'constant'\n    elif lr_scheduler_type =='FP_raise_fall_creative':\n        custom_scheduller = True\n        lr_scheduler_type_arg = 'constant_with_warmup'\n    \n    #gradient_checkpointing=True\n    \n    args=transformers.TrainingArguments(\n            report_to=report_to if report_to != \"None\" else None,\n            per_device_train_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_steps=math.ceil(warmup_steps / gradient_accumulation_steps),\n            warmup_ratio = warmup_ratio,\n            num_train_epochs=epochs,\n            learning_rate=actual_lr,\n            fp16=False if shared.args.cpu else True,\n            optim=optimizer,\n            logging_steps=1,\n            evaluation_strategy=\"steps\" if eval_data is not None else \"no\",\n            eval_steps=math.ceil(eval_steps / gradient_accumulation_steps) if eval_data is not None else None,\n            save_strategy=\"steps\" if eval_data is not None else \"no\",\n            output_dir=lora_file_path,\n            lr_scheduler_type=lr_scheduler_type_arg,\n            load_best_model_at_end=eval_data is not None,\n            # TODO: Enable multi-device support\n            ddp_find_unused_parameters=None,\n            no_cuda=shared.args.cpu,\n        )\n\n    if custom_scheduller:\n        trainer = FPSchedulerTrainer(\n            neftune_noise_alpha=neft_noise_alpha,\n            model=lora_model,\n            train_dataset=train_data,\n            eval_dataset=eval_data,\n            args=args,\n            data_collator=transformers.DataCollatorForLanguageModeling(shared.tokenizer, mlm=False),\n            callbacks=list([Callbacks()])\n        )\n    elif neft_noise_alpha > 0:\n            trainer = FPNEFtuneTrainer(\n            neftune_noise_alpha=neft_noise_alpha,\n            model=lora_model,\n            train_dataset=train_data,\n            eval_dataset=eval_data,\n            args=args,\n            data_collator=transformers.DataCollatorForLanguageModeling(shared.tokenizer, mlm=False),\n            callbacks=list([Callbacks()])\n        )\n    else:\n        trainer = transformers.Trainer(\n            model=lora_model,\n            train_dataset=train_data,\n            eval_dataset=eval_data,\n            args=args,\n            data_collator=transformers.DataCollatorForLanguageModeling(shared.tokenizer, mlm=False),\n            callbacks=list([Callbacks()])\n        )\n    \n    # END OF FPHAM CUSTOM SCHEDULER\n\n    lora_model.config.use_cache = False\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        lora_model = torch.compile(lora_model)\n\n    # == Save parameters for reuse ==\n    with open(f\"{lora_file_path}/training_parameters.json\", 'w', encoding='utf-8') as file:\n        vars = locals()\n        json.dump({x: vars[x] for x in PARAMETERS}, file, indent=2)\n\n    # == Save training prompt ==\n    with open(f\"{lora_file_path}/training_prompt.json\", 'w', encoding='utf-8') as file:\n        json.dump(train_template, file, indent=2)\n\n    # == Main run and monitor loop ==\n    logger.info(\"Starting training...\")\n    yield \"Starting...\", zero_pd\n\n    lora_trainable_param, lora_all_param = calc_trainable_parameters(lora_model)\n\n    projections_string = \", \".join([projection.replace(\"_proj\", \"\") for projection in model_to_lora_modules[model_id]])\n\n    print(f\"Training '{model_id}' model using {YELLOW}({projections_string}){RESET} projections\")\n\n    if lora_all_param > 0:\n        print(f\"Trainable params: {lora_trainable_param:,d} ({RED}{100 * lora_trainable_param / lora_all_param:.4f} %{RESET}), All params: {lora_all_param:,d} (Model: {model_all_params:,d})\")\n\n    train_log.update({\"base_model_name\": shared.model_name})\n    train_log.update({\"base_model_class\": shared.model.__class__.__name__})\n    train_log.update({\"base_loaded_in_4bit\": getattr(lora_model, \"is_loaded_in_4bit\", False)})\n    train_log.update({\"base_loaded_in_8bit\": getattr(lora_model, \"is_loaded_in_8bit\", False)})\n    train_log.update({\"projections\": projections_string})\n    if non_serialized_params['checkpoint_offset'] > 0:\n        train_log.update({\"last_run_steps_offset\": non_serialized_params['checkpoint_offset']})\n        train_log.update({\"last_run_epoch_offset\": non_serialized_params['epoch_offset']})\n\n\n    if non_serialized_params['checkpoint_offset'] > 0:\n        print(f\"Continue training on {RED}previous adapter{RESET} from epoch: {RED}{non_serialized_params['epoch_offset']}{RESET}\")\n\n    if stop_at_loss > 0:\n        print(f\"Monitoring loss {RED}(Auto-Stop at: {stop_at_loss}){RESET}\")\n\n    \n\n    if WANT_INTERRUPT:\n        yield \"Interrupted before start.\", zero_pd\n        return\n\n    def log_train_dataset(trainer):\n        decoded_entries = []\n        # Try to decode the entries and write the log file\n        try:\n            # Iterate over the first 10 elements in the dataset (or fewer if there are less than 10)\n            for i in range(min(10, len(trainer.train_dataset))):\n                decoded_text = shared.tokenizer.decode(trainer.train_dataset[i]['input_ids'])\n                decoded_entries.append({\"value\": decoded_text})\n\n            # Write the log file\n            Path('logs').mkdir(exist_ok=True)\n            with open(Path('logs/train_dataset_sample.json'), 'w') as json_file:\n                json.dump(decoded_entries, json_file, indent=4)\n\n            logger.info(\"Log file 'train_dataset_sample.json' created in the 'logs' directory.\")\n        except Exception as e:\n            logger.error(f\"Failed to create log file due to error: {e}\")\n\n    def threaded_run():\n        log_train_dataset(trainer)\n        trainer.train()\n        # Note: save in the thread in case the gradio thread breaks (eg browser closed)\n        lora_model.save_pretrained(lora_file_path, safe_serialization = non_serialized_params['safe_serialization'])\n        logger.info(\"LoRA training run is completed and saved.\")\n        # Save log\n        with open(f\"{lora_file_path}/training_log.json\", 'w', encoding='utf-8') as file:\n            json.dump(train_log, file, indent=2)\n\n    thread = threading.Thread(target=threaded_run)\n    thread.start()\n    last_step = 0\n    start_time = time.perf_counter()\n\n    while thread.is_alive():\n        time.sleep(0.5)\n\n        if statistics['loss']:\n            max_value_dict = max(statistics['loss'], key=lambda x: x['value'])\n            max_value = max_value_dict['value']+0.4\n            first_epoch = statistics['loss'][0]['epoch']\n            last_epoch = statistics['loss'][-1]['epoch']\n        else:\n            max_value = 3.5\n            last_epoch = 0\n            first_epoch = 0           \n\n        if WANT_INTERRUPT:\n\n            losses = gr.LinePlot.update(\n\t\t\t\tvalue = pd.DataFrame(statistics['loss']),\n                x=\"epoch\", y=\"value\",\n                title=\"Loss Metrics\",\n                overlay_point=True, tooltip=[\"epoch\", \"value\"],\n\t\t\t\tx_lim=[first_epoch,last_epoch], y_lim=[0,max_value],\n                width=500, height=250 )\n\n            yield \"Interrupting, please wait... *(Run will stop after the current training step completes.)*\", losses\n\n        elif tracked.current_steps != last_step:\n            last_step = tracked.current_steps\n            time_elapsed = time.perf_counter() - start_time\n            lastloss = float(train_log.get('loss', 0.0))\n\n            non_serialized_params.update({\"training_loop\": True})               \n\n            if lastloss > 0:\n                lastloss_str = f\", ... Current Loss: `{lastloss:.2f}`\"\n            else:\n                lastloss_str = \"\"\n\n            if time_elapsed <= 0:\n                timer_info = \"\"\n                total_time_estimate = 999\n            else:\n                its = tracked.current_steps / time_elapsed\n                if its > 1:\n                    timer_info = f\"`{its:.2f}` it/s\"\n                else:\n                    timer_info = f\"`{1.0/its:.2f}` s/it\"\n\n                total_time_estimate = (1.0 / its) * (tracked.max_steps)\n\n            if stop_at_loss != non_serialized_params['stop_at_loss']:\n                stop_at_loss = non_serialized_params['stop_at_loss']\n                print(f\"Stop at loss changed {RED}(Auto-Stop at: {stop_at_loss}){RESET}\")\n            \n            losses = gr.LinePlot.update(\n\t\t\t\tvalue = pd.DataFrame(statistics['loss']),\n                x=\"epoch\", y=\"value\",\n                title=\"Loss Metrics\",\n                overlay_point=True, tooltip=[\"epoch\", \"value\"],\n\t\t\t\tx_lim=[first_epoch,last_epoch], y_lim=[0,max_value],\n                width=500, height=250 )\n\t\t\t\t\n\n            yield f\"Running... **{tracked.current_steps}** / **{tracked.max_steps}** ... {timer_info}, {format_time(time_elapsed)} / {format_time(total_time_estimate)} ... {format_time(total_time_estimate - time_elapsed)} remaining {lastloss_str}\", losses\n\n    # Saving in the train thread might fail if an error occurs, so save here if so.\n\n    #return_pd = pd.DataFrame(statistics['loss'])\n\n    if statistics['loss']:\n        max_value_dict = max(statistics['loss'], key=lambda x: x['value'])\n        max_value = max_value_dict['value']+0.4\n        first_epoch = statistics['loss'][0]['epoch']\n        last_epoch = statistics['loss'][-1]['epoch']\n    else:\n        max_value = 3.5\n        last_epoch = 0\n        first_epoch = 0 \n\n    return_pd = gr.LinePlot.update(\n        value = pd.DataFrame(statistics['loss']),\n        x=\"epoch\", y=\"value\",\n        title=\"Loss Metrics\",\n        overlay_point=True, tooltip=[\"epoch\", \"value\"],\n        x_lim=[first_epoch,last_epoch], y_lim=[0,max_value],\n        width=500, height=250)\n\n    non_serialized_params.update({\"training_loop\": False})\n\n    if not tracked.did_save:\n        logger.info(\"Training complete, saving...\")\n        lora_model.save_pretrained(lora_file_path, safe_serialization = non_serialized_params['safe_serialization'])\n\n    if WANT_INTERRUPT:\n        logger.info(\"Training interrupted.\")\n        yield f\"Interrupted by user. LoRA saved to `{lora_file_path}`.\", return_pd\n    else:\n        logger.info(\"Training complete!\")\n        yield f\"Done! LoRA saved to `{lora_file_path}`.\\n\\nBefore testing your new LoRA, make sure to first reload the model, as it is currently dirty from training.\", return_pd\n\n    create_graph(lora_file_path, lora_name)\n\ndef format_time(seconds: float):\n    if seconds < 120:\n        return f\"`{seconds:.0f}` seconds\"\n\n    minutes = seconds / 60\n    if minutes < 120:\n        return f\"`{minutes:.0f}` minutes\"\n\n    hours = minutes / 60\n    return f\"`{hours:.0f}` hours\"\n", "extensions/Training_PRO/train_utils.py": "import os\nfrom modules import shared, utils\nfrom pathlib import Path\nimport requests\nimport tqdm\nimport json\n\n'''\ndef get_gpu_memory_usage(rank):\n    return {\n        'total': round(torch.cuda.get_device_properties(rank).total_memory / (1024**3), 2),\n        'max': round(torch.cuda.max_memory_allocated(rank) / (1024**3), 2),\n        'reserved': round(torch.cuda.memory_reserved(rank) / (1024**3), 2),\n        'allocated': round(torch.cuda.memory_allocated(rank) / (1024**3), 2)\n    }\n'''\n\ndef list_subfoldersByTime(directory):\n\n    if not directory.endswith('/'):\n        directory += '/'\n    subfolders = []\n    subfolders.append('None') \n    path = directory\n    name_list = os.listdir(path)\n    full_list = [os.path.join(path,i) for i in name_list]\n    time_sorted_list = sorted(full_list, key=os.path.getmtime,reverse=True)\n\n    for entry in time_sorted_list:\n        if os.path.isdir(entry):\n            entry_str = f\"{entry}\"  # Convert entry to a string\n            full_path = entry_str\n            entry_str = entry_str.replace('\\\\','/')\n            entry_str = entry_str.replace(f\"{directory}\", \"\")  # Remove directory part\n            subfolders.append(entry_str)\n\n    return subfolders\n\ndef get_available_loras_local(_sortedByTime):\n    \n    model_dir = shared.args.lora_dir  # Update with the appropriate directory path\n    subfolders = []\n    if _sortedByTime:\n        subfolders = list_subfoldersByTime(model_dir)\n    else:\n        subfolders = utils.get_available_loras()        \n\n    return subfolders\n\n\n# FPHAM SPLIT BY SENTENCE BLOCK ===============\n     \ndef split_sentences(text: str, cutoff_len: int):\n    sentences = []\n    sentence = ''\n    delimiters = ['. ', '? ', '! ', '... ', '.\\n', '?\\n', '!\\n','...\\n','</s>','<//>']\n    abbreviations = ['Mr. ', 'Mrs. ', 'Dr. ', 'Ms. ', 'St. ', 'Prof. ', 'Jr. ', 'Ltd. ', 'Capt. ', 'Col. ', 'Gen. ', 'Ave. ', 'Blvd. ', 'Co. ', 'Corp. ', 'Dept. ', 'Est. ', 'Gov. ', 'Inc. ', 'Ph.D. ', 'Univ. ']\n    errors = 0\n    max_cut = cutoff_len-1\n    prev_char = ''  \n\n    for char in text:\n        sentence += char\n\n    \n        if (any(sentence.endswith(delimiter) for delimiter in delimiters) and\n            not (prev_char.isupper() and len(sentence) >= 3 and sentence[-3] != ' ') and \n            not any(sentence.endswith(abbreviation) for abbreviation in abbreviations)):\n            tokens = shared.tokenizer.encode(sentence)\n            \n            if len(tokens) > max_cut:\n                tokens = tokens[:max_cut]\n                sentence = shared.tokenizer.decode(tokens, skip_special_tokens=True)\n                errors = errors + 1\n\n            sentences.append({'text': sentence, 'size': len(tokens)})\n            \n            sentence = ''\n\n        prev_char = char\n\n    if sentence:\n        tokens = shared.tokenizer.encode(sentence)\n        if len(tokens) > max_cut:\n            tokens = tokens[:max_cut]\n            sentence = shared.tokenizer.decode(tokens, skip_special_tokens=True)  \n            errors = errors + 1\n\n        sentences.append({'text': sentence, 'size': len(tokens)})\n\n    if errors > 0:\n        print(f\"Trimmed sentences beyond Cutoff Length: {errors}\")\n\n    return sentences\n\n# The goal of following code is to create blocks of text + overlapping blocks while:\n# respects sentence boundaries\n# always uses all the text \n# hard cut defined by hard_cut_string or </s> will always end at the end of data block\n# no overlapping blocks will be created across hard cut or across </s> token\n\ndef precise_cut(text: str, overlap: bool, min_chars_cut: int, eos_to_hc: bool, cutoff_len: int, hard_cut_string: str, debug_slicer:bool):\n\n    EOSX_str = '<//>' #hardcut placeholder\n    EOS_str = '</s>' \n    print(\"Precise raw text slicer: ON\")\n    \n    cut_string = hard_cut_string.replace('\\\\n', '\\n')\n    text = text.replace(cut_string, EOSX_str)\n    sentences = split_sentences(text, cutoff_len)\n\n    print(f\"Sentences: {len(sentences)}\")\n    sentencelist = []\n    currentSentence = ''\n    totalLength = 0\n    max_cut = cutoff_len-1\n    half_cut = cutoff_len//2\n    halfcut_length = 0\n\n    edgeindex = []\n    half_index = 0\n\n    for index, item in enumerate(sentences):\n        \n        if halfcut_length+ item['size'] < half_cut:\n            halfcut_length += item['size']\n            half_index = index\n        else:\n            edgeindex.append(half_index)\n            halfcut_length = -2 * max_cut\n\n\n        if totalLength + item['size'] < max_cut and not currentSentence.endswith(EOSX_str): \n            currentSentence += item['text']\n            totalLength += item['size']\n        else:\n\n            if len(currentSentence.strip()) > min_chars_cut:\n                sentencelist.append(currentSentence.strip())\n\n            currentSentence = item['text']\n            totalLength = item['size']\n            halfcut_length = item['size']\n            \n    if len(currentSentence.strip()) > min_chars_cut:    \n        sentencelist.append(currentSentence.strip())\n\n    unique_blocks = len(sentencelist)\n    print(f\"Text Blocks: {unique_blocks}\")\n\n    #overlap strategies: \n    # don't overlap across HARD CUT (EOSX)\n    if overlap:\n        for edge_idx in edgeindex:\n            currentSentence = ''\n            totalLength = 0\n\n            for item in sentences[edge_idx:]:\n                if totalLength + item['size'] < max_cut:\n                    currentSentence += item['text']\n                    totalLength += item['size']\n                else:\n                    #if by chance EOSX is at the end then it's acceptable\n                    if currentSentence.endswith(EOSX_str) and len(currentSentence.strip()) > min_chars_cut:\n                            sentencelist.append(currentSentence.strip())    \n                    # otherwise don't cross hard cut    \n                    elif EOSX_str not in currentSentence and len(currentSentence.strip()) > min_chars_cut:\n                        sentencelist.append(currentSentence.strip())\n                    \n                    currentSentence = ''\n                    totalLength = 0\n                    break\n        \n        print(f\"+ Overlapping blocks: {len(sentencelist)-unique_blocks}\")\n\n    num_EOS = 0\n    for i in range(len(sentencelist)):\n        if eos_to_hc:\n            sentencelist[i] = sentencelist[i].replace(EOSX_str, EOS_str)\n        else:\n            sentencelist[i] = sentencelist[i].replace(EOSX_str, '')\n        \n        #someone may have had stop strings in the raw text...\n        sentencelist[i] = sentencelist[i].replace(\"</s></s>\", EOS_str)\n        num_EOS += sentencelist[i].count(EOS_str)\n\n    if num_EOS > 0:\n        print(f\"+ EOS count: {num_EOS}\")\n\n    #final check for useless lines\n    sentencelist = [item for item in sentencelist if item.strip() != \"</s>\"]\n    sentencelist = [item for item in sentencelist if item.strip() != \"\"]\n\n\n    if debug_slicer:\n                    # Write the log file\n        Path('logs').mkdir(exist_ok=True)\n        sentencelist_dict = {index: sentence for index, sentence in enumerate(sentencelist)}\n        output_file = \"logs/sentencelist.json\"\n        with open(output_file, 'w') as f:\n            json.dump(sentencelist_dict, f,indent=2)\n        \n        print(\"Saved sentencelist.json in logs folder\")\n    \n    return sentencelist   \n\n\ndef sliding_block_cut(text: str, min_chars_cut: int, eos_to_hc: bool, cutoff_len: int, hard_cut_string: str, debug_slicer:bool):\n\n    EOSX_str = '<//>' #hardcut placeholder\n    EOS_str = '</s>' \n    print(\"Mega Block Overlap: ON\")\n    \n    cut_string = hard_cut_string.replace('\\\\n', '\\n')\n    text = text.replace(cut_string, EOSX_str)\n    sentences = split_sentences(text, cutoff_len)\n\n    print(f\"Sentences: {len(sentences)}\")\n    sentencelist = []\n    \n    max_cut = cutoff_len-1\n\n    #print(f\"max_cut: {max_cut}\")\n    advancing_to = 0\n\n    prev_block_lastsentence = \"\"\n    \n\n    for i in range(len(sentences)):\n        totalLength = 0\n        currentSentence = ''\n        lastsentence = \"\"\n        \n        if i >= advancing_to:\n            for k in range(i, len(sentences)):\n                \n                current_length = sentences[k]['size']\n\n                if totalLength + current_length <= max_cut and not currentSentence.endswith(EOSX_str):\n                    currentSentence += sentences[k]['text']\n                    totalLength += current_length\n                    lastsentence = sentences[k]['text']\n                else:\n                    if len(currentSentence.strip()) > min_chars_cut:\n                        if prev_block_lastsentence!=lastsentence:\n                            sentencelist.append(currentSentence.strip())\n                            prev_block_lastsentence = lastsentence\n                        \n                    advancing_to = 0\n                    if currentSentence.endswith(EOSX_str):\n                        advancing_to = k\n\n                    currentSentence = \"\"\n                    totalLength = 0\n                    break\n            \n            if currentSentence != \"\":\n                if len(currentSentence.strip()) > min_chars_cut:\n                    sentencelist.append(currentSentence.strip())\n\n    unique_blocks = len(sentencelist)\n    print(f\"Text Blocks: {unique_blocks}\")\n    num_EOS = 0\n    for i in range(len(sentencelist)):\n        if eos_to_hc:\n            sentencelist[i] = sentencelist[i].replace(EOSX_str, EOS_str)\n        else:\n            sentencelist[i] = sentencelist[i].replace(EOSX_str, '')\n        \n        #someone may have had stop strings in the raw text...\n        sentencelist[i] = sentencelist[i].replace(\"</s></s>\", EOS_str)\n        num_EOS += sentencelist[i].count(EOS_str)\n\n    if num_EOS > 0:\n        print(f\"+ EOS count: {num_EOS}\")\n\n    #final check for useless lines\n    sentencelist = [item for item in sentencelist if item.strip() != \"</s>\"]\n    sentencelist = [item for item in sentencelist if item.strip() != \"\"]\n\n\n    if debug_slicer:\n                    # Write the log file\n        Path('logs').mkdir(exist_ok=True)\n        sentencelist_dict = {index: sentence for index, sentence in enumerate(sentencelist)}\n        output_file = \"logs/sentencelist.json\"\n        with open(output_file, 'w') as f:\n            json.dump(sentencelist_dict, f,indent=2)\n        \n        print(\"Saved sentencelist.json in logs folder\")\n    \n    return sentencelist   \n\n# Example usage:\n# download_file_from_url('https://example.com/path/to/your/file.ext', '/output/directory')\n\ndef download_file_from_url(url, overwrite, output_dir_in, valid_extensions = {'.txt', '.json'}):\n    try:\n    # Validate and sanitize the URL\n    #parsed_url = urllib.parse.urlparse(url)\n    #if not parsed_url.netloc:\n    #    raise ValueError(\"Invalid URL\")\n    #filename = os.path.basename(parsed_url.path)\n\n    # Get the filename from the URL\n\n        session = requests.Session()\n        headers = {}\n        mode = 'wb'\n        filename = url.split('/')[-1]\n\n        output_dir = str(output_dir_in)\n        # Construct the full path to the output file\n        local_filename = os.path.join(output_dir, filename)\n\n        # Check if the local file already exists\n        overw = ''\n        if os.path.exists(local_filename):\n            if not overwrite:\n                yield f\"File '{local_filename}' already exists. Aborting.\"\n                return\n            else:\n                overw = ' [Overwrite existing]'\n\n        filename_lower = filename.lower()\n\n        # Send an HTTP GET request to the URL with a timeout\n        file_extension = os.path.splitext(filename_lower)[-1]\n        \n        if file_extension not in valid_extensions:\n            yield f\"Invalid file extension: {file_extension}. Only {valid_extensions} files are supported.\"\n            return\n\n        with session.get(url, stream=True, headers=headers, timeout=10) as r:\n            r.raise_for_status() \n            # total size can be wildly inaccurate\n            #total_size = int(r.headers.get('content-length', 0))\n            \n            block_size = 1024 * 4  \n            with open(local_filename, mode) as f:\n                count = 0\n                for data in r.iter_content(block_size):\n                    f.write(data)\n                    count += len(data)\n\n                    yield f\"Downloaded: {count} \" + overw\n\n            # Verify file size if possible\n            if os.path.exists(local_filename):\n                downloaded_size = os.path.getsize(local_filename)\n                if downloaded_size > 0:\n                    yield f\"File '{filename}' downloaded to '{output_dir}' ({downloaded_size} bytes).\"\n                    print(\"File Downloaded\")\n                else:\n                    print(\"Downloaded file is zero\")\n                    yield f\"Failed. Downloaded file size is zero).\"\n            else:\n                print(f\"Error: {local_filename} failed to download.\")\n                yield f\"Error: {local_filename} failed to download\"\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        yield f\"An error occurred: {e}\"\n\n    finally:\n        # Close the session to release resources\n        session.close()\n\n", "extensions/send_pictures/script.py": "import base64\nfrom io import BytesIO\n\nimport gradio as gr\nimport torch\nfrom transformers import BlipForConditionalGeneration, BlipProcessor\n\nfrom modules import chat, shared, ui_chat\nfrom modules.ui import gather_interface_values\nfrom modules.utils import gradio\n\ninput_hijack = {\n    'state': False,\n    'value': [\"\", \"\"]\n}\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float32).to(\"cpu\")\n\n\ndef chat_input_modifier(text, visible_text, state):\n    global input_hijack\n    if input_hijack['state']:\n        input_hijack['state'] = False\n        return input_hijack['value']\n    else:\n        return text, visible_text\n\n\ndef caption_image(raw_image):\n    inputs = processor(raw_image.convert('RGB'), return_tensors=\"pt\").to(\"cpu\", torch.float32)\n    out = model.generate(**inputs, max_new_tokens=100)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n\ndef generate_chat_picture(picture, name1, name2):\n    text = f'*{name1} sends {name2} a picture that contains the following: \u201c{caption_image(picture)}\u201d*'\n    # lower the resolution of sent images for the chat, otherwise the log size gets out of control quickly with all the base64 values in visible history\n    picture.thumbnail((300, 300))\n    buffer = BytesIO()\n    picture.save(buffer, format=\"JPEG\")\n    img_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n    visible_text = f'<img src=\"data:image/jpeg;base64,{img_str}\" alt=\"{text}\">'\n    return text, visible_text\n\n\ndef ui():\n    picture_select = gr.Image(label='Send a picture', type='pil')\n\n    # Prepare the input hijack, update the interface values, call the generation function, and clear the picture\n    picture_select.upload(\n        lambda picture, name1, name2: input_hijack.update({\n            \"state\": True,\n            \"value\": generate_chat_picture(picture, name1, name2)\n        }), [picture_select, shared.gradio['name1'], shared.gradio['name2']], None).then(\n        gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.generate_chat_reply_wrapper, gradio(ui_chat.inputs), gradio('display', 'history'), show_progress=False).then(\n        lambda: None, None, picture_select, show_progress=False)\n", "extensions/superbooga/chromadb.py": "import random\n\nimport chromadb\nimport posthog\nfrom chromadb.config import Settings\nfrom chromadb.utils import embedding_functions\n\n# Intercept calls to posthog\nposthog.capture = lambda *args, **kwargs: None\n\n\nembedder = embedding_functions.SentenceTransformerEmbeddingFunction(\"sentence-transformers/all-mpnet-base-v2\")\n\n\nclass ChromaCollector():\n    def __init__(self):\n        name = ''.join(random.choice('ab') for _ in range(10))\n\n        self.name = name\n        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n        self.collection = self.chroma_client.create_collection(name=name, embedding_function=embedder)\n        self.ids = []\n\n    def add(self, texts: list[str]):\n        if len(texts) == 0:\n            return\n\n        self.ids = [f\"id{i}\" for i in range(len(texts))]\n        self.collection.add(documents=texts, ids=self.ids)\n\n    def get_documents_ids_distances(self, search_strings: list[str], n_results: int):\n        n_results = min(len(self.ids), n_results)\n        if n_results == 0:\n            return [], [], []\n\n        result = self.collection.query(query_texts=search_strings, n_results=n_results, include=['documents', 'distances'])\n        documents = result['documents'][0]\n        ids = list(map(lambda x: int(x[2:]), result['ids'][0]))\n        distances = result['distances'][0]\n        return documents, ids, distances\n\n    # Get chunks by similarity\n    def get(self, search_strings: list[str], n_results: int) -> list[str]:\n        documents, _, _ = self.get_documents_ids_distances(search_strings, n_results)\n        return documents\n\n    # Get ids by similarity\n    def get_ids(self, search_strings: list[str], n_results: int) -> list[str]:\n        _, ids, _ = self.get_documents_ids_distances(search_strings, n_results)\n        return ids\n\n    # Get chunks by similarity and then sort by insertion order\n    def get_sorted(self, search_strings: list[str], n_results: int) -> list[str]:\n        documents, ids, _ = self.get_documents_ids_distances(search_strings, n_results)\n        return [x for _, x in sorted(zip(ids, documents))]\n\n    # Multiply distance by factor within [0, time_weight] where more recent is lower\n    def apply_time_weight_to_distances(self, ids: list[int], distances: list[float], time_weight: float = 1.0) -> list[float]:\n        if len(self.ids) <= 1:\n            return distances.copy()\n\n        return [distance * (1 - _id / (len(self.ids) - 1) * time_weight) for _id, distance in zip(ids, distances)]\n\n    # Get ids by similarity and then sort by insertion order\n    def get_ids_sorted(self, search_strings: list[str], n_results: int, n_initial: int = None, time_weight: float = 1.0) -> list[str]:\n        do_time_weight = time_weight > 0\n        if not (do_time_weight and n_initial is not None):\n            n_initial = n_results\n        elif n_initial == -1:\n            n_initial = len(self.ids)\n\n        if n_initial < n_results:\n            raise ValueError(f\"n_initial {n_initial} should be >= n_results {n_results}\")\n\n        _, ids, distances = self.get_documents_ids_distances(search_strings, n_initial)\n        if do_time_weight:\n            distances_w = self.apply_time_weight_to_distances(ids, distances, time_weight=time_weight)\n            results = zip(ids, distances, distances_w)\n            results = sorted(results, key=lambda x: x[2])[:n_results]\n            results = sorted(results, key=lambda x: x[0])\n            ids = [x[0] for x in results]\n\n        return sorted(ids)\n\n    def clear(self):\n        self.ids = []\n        self.chroma_client.delete_collection(name=self.name)\n        self.collection = self.chroma_client.create_collection(name=self.name, embedding_function=embedder)\n\n\ndef make_collector():\n    return ChromaCollector()\n\n\ndef add_chunks_to_collector(chunks, collector):\n    collector.clear()\n    collector.add(chunks)\n", "extensions/superbooga/download_urls.py": "import concurrent.futures\n\nimport requests\n\n\ndef download_single(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    }\n    response = requests.get(url, headers=headers, timeout=5)\n    if response.status_code == 200:\n        return response.content\n    else:\n        raise Exception(\"Failed to download URL\")\n\n\ndef download_urls(urls, threads=1):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n        futures = []\n        for url in urls:\n            future = executor.submit(download_single, url)\n            futures.append(future)\n\n        results = []\n        i = 0\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                result = future.result()\n                results.append(result)\n                i += 1\n                yield f\"{i}/{len(urls)}\", results\n            except Exception:\n                pass\n\n        yield \"Done\", results\n", "extensions/superbooga/script.py": "import re\nimport textwrap\n\nimport gradio as gr\nfrom bs4 import BeautifulSoup\n\nfrom modules import chat\nfrom modules.logging_colors import logger\n\nfrom .chromadb import add_chunks_to_collector, make_collector\nfrom .download_urls import download_urls\n\nparams = {\n    'chunk_count': 5,\n    'chunk_count_initial': 10,\n    'time_weight': 0,\n    'chunk_length': 700,\n    'chunk_separator': '',\n    'strong_cleanup': False,\n    'threads': 4,\n}\n\ncollector = make_collector()\nchat_collector = make_collector()\n\n\ndef feed_data_into_collector(corpus, chunk_len, chunk_sep):\n    global collector\n\n    # Defining variables\n    chunk_len = int(chunk_len)\n    chunk_sep = chunk_sep.replace(r'\\n', '\\n')\n    cumulative = ''\n\n    # Breaking the data into chunks and adding those to the db\n    cumulative += \"Breaking the input dataset...\\n\\n\"\n    yield cumulative\n    if chunk_sep:\n        data_chunks = corpus.split(chunk_sep)\n        data_chunks = [[data_chunk[i:i + chunk_len] for i in range(0, len(data_chunk), chunk_len)] for data_chunk in data_chunks]\n        data_chunks = [x for y in data_chunks for x in y]\n    else:\n        data_chunks = [corpus[i:i + chunk_len] for i in range(0, len(corpus), chunk_len)]\n\n    cumulative += f\"{len(data_chunks)} chunks have been found.\\n\\nAdding the chunks to the database...\\n\\n\"\n    yield cumulative\n    add_chunks_to_collector(data_chunks, collector)\n    cumulative += \"Done.\"\n    yield cumulative\n\n\ndef feed_file_into_collector(file, chunk_len, chunk_sep):\n    yield 'Reading the input dataset...\\n\\n'\n    text = file.decode('utf-8')\n    for i in feed_data_into_collector(text, chunk_len, chunk_sep):\n        yield i\n\n\ndef feed_url_into_collector(urls, chunk_len, chunk_sep, strong_cleanup, threads):\n    all_text = ''\n    cumulative = ''\n\n    urls = urls.strip().split('\\n')\n    cumulative += f'Loading {len(urls)} URLs with {threads} threads...\\n\\n'\n    yield cumulative\n    for update, contents in download_urls(urls, threads=threads):\n        yield cumulative + update\n\n    cumulative += 'Processing the HTML sources...'\n    yield cumulative\n    for content in contents:\n        soup = BeautifulSoup(content, features=\"lxml\")\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        strings = soup.stripped_strings\n        if strong_cleanup:\n            strings = [s for s in strings if re.search(\"[A-Za-z] \", s)]\n\n        text = '\\n'.join([s.strip() for s in strings])\n        all_text += text\n\n    for i in feed_data_into_collector(all_text, chunk_len, chunk_sep):\n        yield i\n\n\ndef apply_settings(chunk_count, chunk_count_initial, time_weight):\n    global params\n    params['chunk_count'] = int(chunk_count)\n    params['chunk_count_initial'] = int(chunk_count_initial)\n    params['time_weight'] = time_weight\n    settings_to_display = {k: params[k] for k in params if k in ['chunk_count', 'chunk_count_initial', 'time_weight']}\n    yield f\"The following settings are now active: {str(settings_to_display)}\"\n\n\ndef custom_generate_chat_prompt(user_input, state, **kwargs):\n    global chat_collector\n\n    # get history as being modified when using regenerate.\n    history = kwargs['history']\n\n    if state['mode'] == 'instruct':\n        results = collector.get_sorted(user_input, n_results=params['chunk_count'])\n        additional_context = '\\nYour reply should be based on the context below:\\n\\n' + '\\n'.join(results)\n        user_input += additional_context\n    else:\n\n        def make_single_exchange(id_):\n            output = ''\n            output += f\"{state['name1']}: {history['internal'][id_][0]}\\n\"\n            output += f\"{state['name2']}: {history['internal'][id_][1]}\\n\"\n            return output\n\n        if len(history['internal']) > params['chunk_count'] and user_input != '':\n            chunks = []\n            hist_size = len(history['internal'])\n            for i in range(hist_size - 1):\n                chunks.append(make_single_exchange(i))\n\n            add_chunks_to_collector(chunks, chat_collector)\n            query = '\\n'.join(history['internal'][-1] + [user_input])\n            try:\n                best_ids = chat_collector.get_ids_sorted(query, n_results=params['chunk_count'], n_initial=params['chunk_count_initial'], time_weight=params['time_weight'])\n                additional_context = '\\n'\n                for id_ in best_ids:\n                    if history['internal'][id_][0] != '<|BEGIN-VISIBLE-CHAT|>':\n                        additional_context += make_single_exchange(id_)\n\n                logger.warning(f'Adding the following new context:\\n{additional_context}')\n                state['context'] = state['context'].strip() + '\\n' + additional_context\n                kwargs['history'] = {\n                    'internal': [history['internal'][i] for i in range(hist_size) if i not in best_ids],\n                    'visible': ''\n                }\n            except RuntimeError:\n                logger.error(\"Couldn't query the database, moving on...\")\n\n    return chat.generate_chat_prompt(user_input, state, **kwargs)\n\n\ndef remove_special_tokens(string):\n    pattern = r'(<\\|begin-user-input\\|>|<\\|end-user-input\\|>|<\\|injection-point\\|>)'\n    return re.sub(pattern, '', string)\n\n\ndef input_modifier(string, state, is_chat=False):\n    if is_chat:\n        return string\n\n    # Find the user input\n    pattern = re.compile(r\"<\\|begin-user-input\\|>(.*?)<\\|end-user-input\\|>\", re.DOTALL)\n    match = re.search(pattern, string)\n    if match:\n        user_input = match.group(1).strip()\n\n        # Get the most similar chunks\n        results = collector.get_sorted(user_input, n_results=params['chunk_count'])\n\n        # Make the injection\n        string = string.replace('<|injection-point|>', '\\n'.join(results))\n\n    return remove_special_tokens(string)\n\n\ndef ui():\n    with gr.Accordion(\"Click for more information...\", open=False):\n        gr.Markdown(textwrap.dedent(\"\"\"\n\n        ## About\n\n        This extension takes a dataset as input, breaks it into chunks, and adds the result to a local/offline Chroma database.\n\n        The database is then queried during inference time to get the excerpts that are closest to your input. The idea is to create an arbitrarily large pseudo context.\n\n        The core methodology was developed and contributed by kaiokendev, who is working on improvements to the method in this repository: https://github.com/kaiokendev/superbig\n\n        ## Data input\n\n        Start by entering some data in the interface below and then clicking on \"Load data\".\n\n        Each time you load some new data, the old chunks are discarded.\n\n        ## Chat mode\n\n        #### Instruct\n\n        On each turn, the chunks will be compared to your current input and the most relevant matches will be appended to the input in the following format:\n\n        ```\n        Consider the excerpts below as additional context:\n        ...\n        ```\n\n        The injection doesn't make it into the chat history. It is only used in the current generation.\n\n        #### Regular chat\n\n        The chunks from the external data sources are ignored, and the chroma database is built based on the chat history instead. The most relevant past exchanges relative to the present input are added to the context string. This way, the extension acts as a long term memory.\n\n        ## Notebook/default modes\n\n        Your question must be manually specified between `<|begin-user-input|>` and `<|end-user-input|>` tags, and the injection point must be specified with `<|injection-point|>`.\n\n        The special tokens mentioned above (`<|begin-user-input|>`, `<|end-user-input|>`, and `<|injection-point|>`) are removed in the background before the text generation begins.\n\n        Here is an example in Vicuna 1.1 format:\n\n        ```\n        A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\n        USER:\n\n        <|begin-user-input|>\n        What datasets are mentioned in the text below?\n        <|end-user-input|>\n\n        <|injection-point|>\n\n        ASSISTANT:\n        ```\n\n        \u26a0\ufe0f  For best results, make sure to remove the spaces and new line characters after `ASSISTANT:`.\n\n        *This extension is currently experimental and under development.*\n\n        \"\"\"))\n\n    with gr.Row():\n        with gr.Column(min_width=600):\n            with gr.Tab(\"Text input\"):\n                data_input = gr.Textbox(lines=20, label='Input data')\n                update_data = gr.Button('Load data')\n\n            with gr.Tab(\"URL input\"):\n                url_input = gr.Textbox(lines=10, label='Input URLs', info='Enter one or more URLs separated by newline characters.')\n                strong_cleanup = gr.Checkbox(value=params['strong_cleanup'], label='Strong cleanup', info='Only keeps html elements that look like long-form text.')\n                threads = gr.Number(value=params['threads'], label='Threads', info='The number of threads to use while downloading the URLs.', precision=0)\n                update_url = gr.Button('Load data')\n\n            with gr.Tab(\"File input\"):\n                file_input = gr.File(label='Input file', type='binary')\n                update_file = gr.Button('Load data')\n\n            with gr.Tab(\"Generation settings\"):\n                chunk_count = gr.Number(value=params['chunk_count'], label='Chunk count', info='The number of closest-matching chunks to include in the prompt.')\n                gr.Markdown('Time weighting (optional, used in to make recently added chunks more likely to appear)')\n                time_weight = gr.Slider(0, 1, value=params['time_weight'], label='Time weight', info='Defines the strength of the time weighting. 0 = no time weighting.')\n                chunk_count_initial = gr.Number(value=params['chunk_count_initial'], label='Initial chunk count', info='The number of closest-matching chunks retrieved for time weight reordering in chat mode. This should be >= chunk count. -1 = All chunks are retrieved. Only used if time_weight > 0.')\n\n                update_settings = gr.Button('Apply changes')\n\n            chunk_len = gr.Number(value=params['chunk_length'], label='Chunk length', info='In characters, not tokens. This value is used when you click on \"Load data\".')\n            chunk_sep = gr.Textbox(value=params['chunk_separator'], label='Chunk separator', info='Used to manually split chunks. Manually split chunks longer than chunk length are split again. This value is used when you click on \"Load data\".')\n        with gr.Column():\n            last_updated = gr.Markdown()\n\n    update_data.click(feed_data_into_collector, [data_input, chunk_len, chunk_sep], last_updated, show_progress=False)\n    update_url.click(feed_url_into_collector, [url_input, chunk_len, chunk_sep, strong_cleanup, threads], last_updated, show_progress=False)\n    update_file.click(feed_file_into_collector, [file_input, chunk_len, chunk_sep], last_updated, show_progress=False)\n    update_settings.click(apply_settings, [chunk_count, chunk_count_initial, time_weight], last_updated, show_progress=False)\n", "extensions/silero_tts/tts_preprocessor.py": "import re\n\nfrom num2words import num2words\n\npunctuation = r'[\\s,.?!/)\\'\\]>]'\nalphabet_map = {\n    \"A\": \" Ei \",\n    \"B\": \" Bee \",\n    \"C\": \" See \",\n    \"D\": \" Dee \",\n    \"E\": \" Eee \",\n    \"F\": \" Eff \",\n    \"G\": \" Jee \",\n    \"H\": \" Eich \",\n    \"I\": \" Eye \",\n    \"J\": \" Jay \",\n    \"K\": \" Kay \",\n    \"L\": \" El \",\n    \"M\": \" Emm \",\n    \"N\": \" Enn \",\n    \"O\": \" Ohh \",\n    \"P\": \" Pee \",\n    \"Q\": \" Queue \",\n    \"R\": \" Are \",\n    \"S\": \" Ess \",\n    \"T\": \" Tee \",\n    \"U\": \" You \",\n    \"V\": \" Vee \",\n    \"W\": \" Double You \",\n    \"X\": \" Ex \",\n    \"Y\": \" Why \",\n    \"Z\": \" Zed \"  # Zed is weird, as I (da3dsoul) am American, but most of the voice models sound British, so it matches\n}\n\n\ndef preprocess(string):\n    # the order for some of these matter\n    # For example, you need to remove the commas in numbers before expanding them\n    string = remove_surrounded_chars(string)\n    string = string.replace('\"', '')\n    string = string.replace('\\u201D', '').replace('\\u201C', '')  # right and left quote\n    string = string.replace('\\u201F', '')  # italic looking quote\n    string = string.replace('\\n', ' ')\n    string = convert_num_locale(string)\n    string = replace_negative(string)\n    string = replace_roman(string)\n    string = hyphen_range_to(string)\n    string = num_to_words(string)\n\n    # TODO Try to use a ML predictor to expand abbreviations. It's hard, dependent on context, and whether to actually\n    # try to say the abbreviation or spell it out as I've done below is not agreed upon\n\n    # For now, expand abbreviations to pronunciations\n    # replace_abbreviations adds a lot of unnecessary whitespace to ensure separation\n    string = replace_abbreviations(string)\n    string = replace_lowercase_abbreviations(string)\n\n    # cleanup whitespaces\n    # remove whitespace before punctuation\n    string = re.sub(rf'\\s+({punctuation})', r'\\1', string)\n    string = string.strip()\n    # compact whitespace\n    string = ' '.join(string.split())\n\n    return string\n\n\ndef remove_surrounded_chars(string):\n    # first this expression will check if there is a string nested exclusively between a alt=\n    # and a style= string. This would correspond to only a the alt text of an embedded image\n    # If it matches it will only keep that part as the string, and rend it for further processing\n    # Afterwards this expression matches to 'as few symbols as possible (0 upwards) between any\n    # asterisks' OR' as few symbols as possible (0 upwards) between an asterisk and the end of the string'\n    if re.search(r'(?<=alt=)(.*)(?=style=)', string, re.DOTALL):\n        m = re.search(r'(?<=alt=)(.*)(?=style=)', string, re.DOTALL)\n        string = m.group(0)\n    return re.sub(r'\\*[^*]*?(\\*|$)', '', string)\n\n\ndef convert_num_locale(text):\n    # This detects locale and converts it to American without comma separators\n    pattern = re.compile(r'(?:\\s|^)\\d{1,3}(?:\\.\\d{3})+(,\\d+)(?:\\s|$)')\n    result = text\n    while True:\n        match = pattern.search(result)\n        if match is None:\n            break\n\n        start = match.start()\n        end = match.end()\n        result = result[0:start] + result[start:end].replace('.', '').replace(',', '.') + result[end:len(result)]\n\n    # removes comma separators from existing American numbers\n    pattern = re.compile(r'(\\d),(\\d)')\n    result = pattern.sub(r'\\1\\2', result)\n\n    return result\n\n\ndef replace_negative(string):\n    # handles situations like -5. -5 would become negative 5, which would then be expanded to negative five\n    return re.sub(rf'(\\s)(-)(\\d+)({punctuation})', r'\\1negative \\3\\4', string)\n\n\ndef replace_roman(string):\n    # find a string of roman numerals.\n    # Only 2 or more, to avoid capturing I and single character abbreviations, like names\n    pattern = re.compile(rf'\\s[IVXLCDM]{{2,}}{punctuation}')\n    result = string\n    while True:\n        match = pattern.search(result)\n        if match is None:\n            break\n\n        start = match.start()\n        end = match.end()\n        result = result[0:start + 1] + str(roman_to_int(result[start + 1:end - 1])) + result[end - 1:len(result)]\n\n    return result\n\n\ndef roman_to_int(s):\n    rom_val = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n    int_val = 0\n    for i in range(len(s)):\n        if i > 0 and rom_val[s[i]] > rom_val[s[i - 1]]:\n            int_val += rom_val[s[i]] - 2 * rom_val[s[i - 1]]\n        else:\n            int_val += rom_val[s[i]]\n    return int_val\n\n\ndef hyphen_range_to(text):\n    pattern = re.compile(r'(\\d+)[-\u2013](\\d+)')\n    result = pattern.sub(lambda x: x.group(1) + ' to ' + x.group(2), text)\n    return result\n\n\ndef num_to_words(text):\n    # 1000 or 10.23\n    pattern = re.compile(r'\\d+\\.\\d+|\\d+')\n    result = pattern.sub(lambda x: num2words(float(x.group())), text)\n    return result\n\n\ndef replace_abbreviations(string):\n    # abbreviations 1 to 4 characters long. It will get things like A and I, but those are pronounced with their letter\n    pattern = re.compile(rf'(^|[\\s(.\\'\\[<])([A-Z]{{1,4}})({punctuation}|$)')\n    result = string\n    while True:\n        match = pattern.search(result)\n        if match is None:\n            break\n\n        start = match.start()\n        end = match.end()\n        result = result[0:start] + replace_abbreviation(result[start:end]) + result[end:len(result)]\n\n    return result\n\n\ndef replace_lowercase_abbreviations(string):\n    # abbreviations 1 to 4 characters long, separated by dots i.e. e.g.\n    pattern = re.compile(rf'(^|[\\s(.\\'\\[<])(([a-z]\\.){{1,4}})({punctuation}|$)')\n    result = string\n    while True:\n        match = pattern.search(result)\n        if match is None:\n            break\n\n        start = match.start()\n        end = match.end()\n        result = result[0:start] + replace_abbreviation(result[start:end].upper()) + result[end:len(result)]\n\n    return result\n\n\ndef replace_abbreviation(string):\n    result = \"\"\n    for char in string:\n        result += match_mapping(char)\n\n    return result\n\n\ndef match_mapping(char):\n    for mapping in alphabet_map.keys():\n        if char == mapping:\n            return alphabet_map[char]\n\n    return char\n\n\ndef __main__(args):\n    print(preprocess(args[1]))\n\n\nif __name__ == \"__main__\":\n    import sys\n    __main__(sys.argv)\n", "extensions/silero_tts/script.py": "import html\nimport json\nimport random\nimport time\nfrom pathlib import Path\n\nimport gradio as gr\nimport torch\n\nfrom extensions.silero_tts import tts_preprocessor\nfrom modules import chat, shared, ui_chat\nfrom modules.utils import gradio\n\ntorch._C._jit_set_profiling_mode(False)\n\n\nparams = {\n    'activate': True,\n    'speaker': 'en_56',\n    'language': 'English',\n    'model_id': 'v3_en',\n    'sample_rate': 48000,\n    'device': 'cpu',\n    'show_text': False,\n    'autoplay': True,\n    'voice_pitch': 'medium',\n    'voice_speed': 'medium',\n    'local_cache_path': ''  # User can override the default cache path to something other via settings.json\n}\n\ncurrent_params = params.copy()\n\nwith open(Path(\"extensions/silero_tts/languages.json\"), encoding='utf8') as f:\n    languages = json.load(f)\n\nvoice_pitches = ['x-low', 'low', 'medium', 'high', 'x-high']\nvoice_speeds = ['x-slow', 'slow', 'medium', 'fast', 'x-fast']\n\n# Used for making text xml compatible, needed for voice pitch and speed control\ntable = str.maketrans({\n    \"<\": \"&lt;\",\n    \">\": \"&gt;\",\n    \"&\": \"&amp;\",\n    \"'\": \"&apos;\",\n    '\"': \"&quot;\",\n})\n\n\ndef xmlesc(txt):\n    return txt.translate(table)\n\n\ndef load_model():\n    torch_cache_path = torch.hub.get_dir() if params['local_cache_path'] == '' else params['local_cache_path']\n    model_path = torch_cache_path + \"/snakers4_silero-models_master/src/silero/model/\" + params['model_id'] + \".pt\"\n    if Path(model_path).is_file():\n        print(f'\\nUsing Silero TTS cached checkpoint found at {torch_cache_path}')\n        model, example_text = torch.hub.load(repo_or_dir=torch_cache_path + '/snakers4_silero-models_master/', model='silero_tts', language=languages[params['language']][\"lang_id\"], speaker=params['model_id'], source='local', path=model_path, force_reload=True)\n    else:\n        print(f'\\nSilero TTS cache not found at {torch_cache_path}. Attempting to download...')\n        model, example_text = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=languages[params['language']][\"lang_id\"], speaker=params['model_id'])\n    model.to(params['device'])\n    return model\n\n\ndef remove_tts_from_history(history):\n    for i, entry in enumerate(history['internal']):\n        history['visible'][i] = [history['visible'][i][0], entry[1]]\n\n    return history\n\n\ndef toggle_text_in_history(history):\n    for i, entry in enumerate(history['visible']):\n        visible_reply = entry[1]\n        if visible_reply.startswith('<audio'):\n            if params['show_text']:\n                reply = history['internal'][i][1]\n                history['visible'][i] = [history['visible'][i][0], f\"{visible_reply.split('</audio>')[0]}</audio>\\n\\n{reply}\"]\n            else:\n                history['visible'][i] = [history['visible'][i][0], f\"{visible_reply.split('</audio>')[0]}</audio>\"]\n\n    return history\n\n\ndef state_modifier(state):\n    if not params['activate']:\n        return state\n\n    state['stream'] = False\n    return state\n\n\ndef input_modifier(string, state):\n    if not params['activate']:\n        return string\n\n    shared.processing_message = \"*Is recording a voice message...*\"\n    return string\n\n\ndef history_modifier(history):\n    # Remove autoplay from the last reply\n    if len(history['internal']) > 0:\n        history['visible'][-1] = [\n            history['visible'][-1][0],\n            history['visible'][-1][1].replace('controls autoplay>', 'controls>')\n        ]\n\n    return history\n\n\ndef output_modifier(string, state):\n    global model, current_params, streaming_state\n\n    for i in params:\n        if params[i] != current_params[i]:\n            model = load_model()\n            current_params = params.copy()\n            break\n\n    if not params['activate']:\n        return string\n\n    original_string = string\n\n    string = tts_preprocessor.preprocess(html.unescape(string))\n\n    if string == '':\n        string = '*Empty reply, try regenerating*'\n    else:\n        output_file = Path(f'extensions/silero_tts/outputs/{state[\"character_menu\"]}_{int(time.time())}.wav')\n        prosody = '<prosody rate=\"{}\" pitch=\"{}\">'.format(params['voice_speed'], params['voice_pitch'])\n        silero_input = f'<speak>{prosody}{xmlesc(string)}</prosody></speak>'\n        model.save_wav(ssml_text=silero_input, speaker=params['speaker'], sample_rate=int(params['sample_rate']), audio_path=str(output_file))\n\n        autoplay = 'autoplay' if params['autoplay'] else ''\n        string = f'<audio src=\"file/{output_file.as_posix()}\" controls {autoplay}></audio>'\n        if params['show_text']:\n            string += f'\\n\\n{original_string}'\n\n    shared.processing_message = \"*Is typing...*\"\n    return string\n\n\ndef setup():\n    global model\n    model = load_model()\n\n\ndef random_sentence():\n    with open(Path(\"extensions/silero_tts/harvard_sentences.txt\")) as f:\n        return random.choice(list(f))\n\n\ndef voice_preview(string):\n    global model, current_params, streaming_state\n\n    for i in params:\n        if params[i] != current_params[i]:\n            model = load_model()\n            current_params = params.copy()\n            break\n\n    string = tts_preprocessor.preprocess(string or random_sentence())\n\n    output_file = Path('extensions/silero_tts/outputs/voice_preview.wav')\n    prosody = f\"<prosody rate=\\\"{params['voice_speed']}\\\" pitch=\\\"{params['voice_pitch']}\\\">\"\n    silero_input = f'<speak>{prosody}{xmlesc(string)}</prosody></speak>'\n    model.save_wav(ssml_text=silero_input, speaker=params['speaker'], sample_rate=int(params['sample_rate']), audio_path=str(output_file))\n\n    return f'<audio src=\"file/{output_file.as_posix()}?{int(time.time())}\" controls autoplay></audio>'\n\n\ndef language_change(lang):\n    global params\n    params.update({\"language\": lang, \"speaker\": languages[lang][\"default_voice\"], \"model_id\": languages[lang][\"model_id\"]})\n    return gr.update(choices=languages[lang][\"voices\"], value=languages[lang][\"default_voice\"])\n\n\ndef custom_css():\n    path_to_css = Path(__file__).parent.resolve() / 'style.css'\n    return open(path_to_css, 'r').read()\n\n\ndef ui():\n    # Gradio elements\n    with gr.Accordion(\"Silero TTS\"):\n        with gr.Row():\n            activate = gr.Checkbox(value=params['activate'], label='Activate TTS')\n            autoplay = gr.Checkbox(value=params['autoplay'], label='Play TTS automatically')\n\n        show_text = gr.Checkbox(value=params['show_text'], label='Show message text under audio player')\n        \n        with gr.Row():\n            language = gr.Dropdown(value=params['language'], choices=sorted(languages.keys()), label='Language')\n            voice = gr.Dropdown(value=params['speaker'], choices=languages[params['language']][\"voices\"], label='TTS voice')\n        with gr.Row():\n            v_pitch = gr.Dropdown(value=params['voice_pitch'], choices=voice_pitches, label='Voice pitch')\n            v_speed = gr.Dropdown(value=params['voice_speed'], choices=voice_speeds, label='Voice speed')\n\n        with gr.Row():\n            preview_text = gr.Text(show_label=False, placeholder=\"Preview text\", elem_id=\"silero_preview_text\")\n            preview_play = gr.Button(\"Preview\")\n            preview_audio = gr.HTML(visible=False)\n\n        with gr.Row():\n            convert = gr.Button('Permanently replace audios with the message texts')\n            convert_cancel = gr.Button('Cancel', visible=False)\n            convert_confirm = gr.Button('Confirm (cannot be undone)', variant=\"stop\", visible=False)\n\n    # Convert history with confirmation\n    convert_arr = [convert_confirm, convert, convert_cancel]\n    convert.click(lambda: [gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)], None, convert_arr)\n    convert_confirm.click(\n        lambda: [gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)], None, convert_arr).then(\n        remove_tts_from_history, gradio('history'), gradio('history')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        chat.redraw_html, gradio(ui_chat.reload_arr), gradio('display'))\n\n    convert_cancel.click(lambda: [gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)], None, convert_arr)\n\n    # Toggle message text in history\n    show_text.change(\n        lambda x: params.update({\"show_text\": x}), show_text, None).then(\n        toggle_text_in_history, gradio('history'), gradio('history')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        chat.redraw_html, gradio(ui_chat.reload_arr), gradio('display'))\n\n    # Event functions to update the parameters in the backend\n    activate.change(lambda x: params.update({\"activate\": x}), activate, None)\n    autoplay.change(lambda x: params.update({\"autoplay\": x}), autoplay, None)\n    language.change(language_change, language, voice, show_progress=False)\n    voice.change(lambda x: params.update({\"speaker\": x}), voice, None)\n    v_pitch.change(lambda x: params.update({\"voice_pitch\": x}), v_pitch, None)\n    v_speed.change(lambda x: params.update({\"voice_speed\": x}), v_speed, None)\n\n    # Play preview\n    preview_text.submit(voice_preview, preview_text, preview_audio)\n    preview_play.click(voice_preview, preview_text, preview_audio)\n", "extensions/long_replies/script.py": "import torch\nfrom modules import chat, shared\nfrom modules.text_generation import (\n    decode,\n    encode,\n    generate_reply,\n)\nfrom transformers import LogitsProcessor\nimport gradio as gr\n\nparams = {\n    \"display_name\": \"Long replies\",\n    \"is_tab\": False,\n    \"min_length\": 120,\n}\n\ninitial_size = 0\n\nclass MyLogits(LogitsProcessor):\n    \"\"\"\n    Manipulates the probabilities for the next token before it gets sampled.\n    Used in the logits_processor_modifier function below.\n    \"\"\"\n    def __init__(self):\n        self.newline_id = shared.tokenizer.encode('\\n')[-1]\n        pass\n\n    def __call__(self, input_ids, scores):\n        if input_ids.shape[-1] - initial_size < params[\"min_length\"]:\n            scores[...,self.newline_id] = -1000\n            # scores[...,shared.tokenizer.eos_token_id] = -1000\n\n        # probs = torch.softmax(scores, dim=-1, dtype=torch.float)\n        # probs[0] /= probs[0].sum()\n        # scores = torch.log(probs / (1 - probs))\n        return scores\n\ndef history_modifier(history):\n    \"\"\"\n    Modifies the chat history.\n    Only used in chat mode.\n    \"\"\"\n    return history\n\ndef state_modifier(state):\n    \"\"\"\n    Modifies the state variable, which is a dictionary containing the input\n    values in the UI like sliders and checkboxes.\n    \"\"\"\n    return state\n\ndef chat_input_modifier(text, visible_text, state):\n    \"\"\"\n    Modifies the user input string in chat mode (visible_text).\n    You can also modify the internal representation of the user\n    input (text) to change how it will appear in the prompt.\n    \"\"\"\n    return text, visible_text\n\ndef input_modifier(string, state):\n    \"\"\"\n    In default/notebook modes, modifies the whole prompt.\n\n    In chat mode, it is the same as chat_input_modifier but only applied\n    to \"text\", here called \"string\", and not to \"visible_text\".\n    \"\"\"\n    return string\n\ndef bot_prefix_modifier(string, state):\n    \"\"\"\n    Modifies the prefix for the next bot reply in chat mode.\n    By default, the prefix will be something like \"Bot Name:\".\n    \"\"\"\n    return string\n\ndef tokenizer_modifier(state, prompt, input_ids, input_embeds):\n    \"\"\"\n    Modifies the input ids and embeds.\n    Used by the multimodal extension to put image embeddings in the prompt.\n    Only used by loaders that use the transformers library for sampling.\n    \"\"\"\n\n    global initial_size\n    initial_size = input_ids.shape[-1]\n\n    return prompt, input_ids, input_embeds\n\ndef logits_processor_modifier(processor_list, input_ids):\n    \"\"\"\n    Adds logits processors to the list, allowing you to access and modify\n    the next token probabilities.\n    Only used by loaders that use the transformers library for sampling.\n    \"\"\"\n    processor_list.append(MyLogits())\n    return processor_list\n\ndef output_modifier(string, state):\n    \"\"\"\n    Modifies the LLM output before it gets presented.\n\n    In chat mode, the modified version goes into history['visible'],\n    and the original version goes into history['internal'].\n    \"\"\"\n    return string\n\ndef custom_generate_chat_prompt(user_input, state, **kwargs):\n    \"\"\"\n    Replaces the function that generates the prompt from the chat history.\n    Only used in chat mode.\n    \"\"\"\n    result = chat.generate_chat_prompt(user_input, state, **kwargs)\n    return result\n\ndef custom_css():\n    \"\"\"\n    Returns a CSS string that gets appended to the CSS for the webui.\n    \"\"\"\n    return ''\n\ndef custom_js():\n    \"\"\"\n    Returns a javascript string that gets appended to the javascript\n    for the webui.\n    \"\"\"\n    return ''\n\ndef setup():\n    \"\"\"\n    Gets executed only once, when the extension is imported.\n    \"\"\"\n    pass\n\ndef ui():\n    \"\"\"\n    Gets executed when the UI is drawn. Custom gradio elements and\n    their corresponding event handlers should be defined here.\n\n    To learn about gradio components, check out the docs:\n    https://gradio.app/docs/\n    \"\"\"\n\n    min_length = gr.Slider(0, 800, step=10, value=params['min_length'], label='Minimum reply length')\n    min_length.change(lambda x: params.update({'min_length': x}), min_length, None)\n", "extensions/ngrok/script.py": "# Adds ngrok ingress, to use add `--extension ngrok` to the command line options\n#\n# Parameters can be customized in settings.json of webui, e.g.:\n# {\"ngrok\": {\"basic_auth\":\"user:password\"} }\n# or\n# {\"ngrok\": {\"oauth_provider\":\"google\", \"oauth_allow_emails\":[\"asdf@asdf.com\"]} }\n#\n# See this example for full list of options: https://github.com/ngrok/ngrok-py/blob/main/examples/ngrok-connect-full.py\n# or the README.md in this directory.\n\nfrom modules import shared\n\n# Pick up host/port command line arguments\nhost = shared.args.listen_host if shared.args.listen_host and shared.args.listen else '127.0.0.1'\nport = shared.args.listen_port if shared.args.listen_port else '7860'\n\n# Default options\noptions = {\n    'addr': f\"{host}:{port}\",\n    'authtoken_from_env': True,\n    'session_metadata': 'text-generation-webui',\n}\n\n\ndef ui():\n    settings = shared.settings.get(\"ngrok\")\n    if settings:\n        options.update(settings)\n\n    try:\n        import ngrok\n        tunnel = ngrok.connect(**options)\n        shared.logger.info(f\"Ingress established at: {tunnel.url()}\")\n    except ModuleNotFoundError:\n        shared.logger.error(\"===> ngrok library not found, please run `pip install -r extensions/ngrok/requirements.txt`\")\n", "extensions/gallery/script.py": "from pathlib import Path\n\nimport gradio as gr\n\nfrom modules.html_generator import get_image_cache\nfrom modules.shared import gradio\n\n\nparams = {\n    'items_per_page': 50,\n    'open': False,\n}\n\ncards = []\n\n\ndef generate_css():\n    css = \"\"\"\n      .highlighted-border {\n        border-color: rgb(249, 115, 22) !important;\n      }\n\n      .character-gallery > .gallery {\n        margin: 1rem 0;\n        display: grid !important;\n        grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));\n        grid-column-gap: 0.4rem;\n        grid-row-gap: 1.2rem;\n      }\n\n      .character-gallery > .label {\n        display: none !important;\n      }\n\n      .character-gallery button.gallery-item {\n        display: contents;\n      }\n\n      .character-container {\n        cursor: pointer;\n        text-align: center;\n        position: relative;\n        opacity: 0.85;\n      }\n\n      .character-container:hover {\n        opacity: 1;\n      }\n\n      .character-container .placeholder, .character-container img {\n        width: 150px;\n        height: 200px;\n        background-color: gray;\n        object-fit: cover;\n        margin: 0 auto;\n        border-radius: 1rem;\n        border: 3px solid white;\n        box-shadow: 3px 3px 6px 0px rgb(0 0 0 / 50%);\n      }\n\n      .character-name {\n        margin-top: 0.3rem;\n        display: block;\n        font-size: 1.2rem;\n        font-weight: 600;\n        overflow-wrap: anywhere;\n      }\n    \"\"\"\n    return css\n\n\ndef generate_html():\n    global cards\n    cards = []\n    # Iterate through files in image folder\n    for file in sorted(Path(\"characters\").glob(\"*\")):\n        if file.suffix in [\".json\", \".yml\", \".yaml\"]:\n            character = file.stem\n            container_html = '<div class=\"character-container\">'\n            image_html = \"<div class='placeholder'></div>\"\n\n            for path in [Path(f\"characters/{character}.{extension}\") for extension in ['png', 'jpg', 'jpeg']]:\n                if path.exists():\n                    image_html = f'<img src=\"file/{get_image_cache(path)}\">'\n                    break\n\n            container_html += f'{image_html} <span class=\"character-name\">{character}</span>'\n            container_html += \"</div>\"\n            cards.append([container_html, character])\n\n    return cards\n\n\ndef filter_cards(filter_str=''):\n    if filter_str == '':\n        return cards\n\n    filter_upper = filter_str.upper()\n    return [k for k in cards if filter_upper in k[1].upper()]\n\n\ndef select_character(evt: gr.SelectData):\n    return (evt.value[1])\n\n\ndef custom_js():\n    path_to_js = Path(__file__).parent.resolve() / 'script.js'\n    return open(path_to_js, 'r').read()\n\n\ndef ui():\n    with gr.Accordion(\"Character gallery\", open=params[\"open\"], elem_id='gallery-extension'):\n        gr.HTML(value=\"<style>\" + generate_css() + \"</style>\")\n        with gr.Row():\n            filter_box = gr.Textbox(label='', placeholder='Filter', lines=1, max_lines=1, container=False, elem_id='gallery-filter-box')\n            gr.ClearButton(filter_box, value='Clear', elem_classes='refresh-button')\n            update = gr.Button(\"Refresh\", elem_classes='refresh-button')\n\n        gallery = gr.Dataset(\n            components=[gr.HTML(visible=False)],\n            label=\"\",\n            samples=generate_html(),\n            elem_classes=[\"character-gallery\"],\n            samples_per_page=params[\"items_per_page\"]\n        )\n\n    filter_box.change(lambda: None, None, None, js=f'() => {{{custom_js()}; gotoFirstPage()}}').success(\n        filter_cards, filter_box, gallery).then(\n        lambda x: gr.update(elem_classes='highlighted-border' if x != '' else ''), filter_box, filter_box, show_progress=False)\n\n    update.click(generate_html, [], None).success(\n        filter_cards, filter_box, gallery)\n\n    gallery.select(select_character, None, gradio['character_menu'])\n", "extensions/sd_api_pictures/script.py": "import base64\nimport io\nimport re\nimport time\nfrom datetime import date\nfrom pathlib import Path\n\nimport gradio as gr\nimport requests\nimport torch\nfrom PIL import Image\n\nfrom modules import shared\nfrom modules.models import reload_model, unload_model\nfrom modules.ui import create_refresh_button\n\ntorch._C._jit_set_profiling_mode(False)\n\n# parameters which can be customized in settings.json of webui\nparams = {\n    'address': 'http://127.0.0.1:7860',\n    'mode': 0,  # modes of operation: 0 (Manual only), 1 (Immersive/Interactive - looks for words to trigger), 2 (Picturebook Adventure - Always on)\n    'manage_VRAM': False,\n    'save_img': False,\n    'SD_model': 'NeverEndingDream',  # not used right now\n    'prompt_prefix': '(Masterpiece:1.1), detailed, intricate, colorful',\n    'negative_prompt': '(worst quality, low quality:1.3)',\n    'width': 512,\n    'height': 512,\n    'denoising_strength': 0.61,\n    'restore_faces': False,\n    'enable_hr': False,\n    'hr_upscaler': 'ESRGAN_4x',\n    'hr_scale': '1.0',\n    'seed': -1,\n    'sampler_name': 'DPM++ 2M Karras',\n    'steps': 32,\n    'cfg_scale': 7,\n    'textgen_prefix': 'Please provide a detailed and vivid description of [subject]',\n    'sd_checkpoint': ' ',\n    'checkpoint_list': [\" \"]\n}\n\n\ndef give_VRAM_priority(actor):\n    global shared, params\n\n    if actor == 'SD':\n        unload_model()\n        print(\"Requesting Auto1111 to re-load last checkpoint used...\")\n        response = requests.post(url=f'{params[\"address\"]}/sdapi/v1/reload-checkpoint', json='')\n        response.raise_for_status()\n\n    elif actor == 'LLM':\n        print(\"Requesting Auto1111 to vacate VRAM...\")\n        response = requests.post(url=f'{params[\"address\"]}/sdapi/v1/unload-checkpoint', json='')\n        response.raise_for_status()\n        reload_model()\n\n    elif actor == 'set':\n        print(\"VRAM mangement activated -- requesting Auto1111 to vacate VRAM...\")\n        response = requests.post(url=f'{params[\"address\"]}/sdapi/v1/unload-checkpoint', json='')\n        response.raise_for_status()\n\n    elif actor == 'reset':\n        print(\"VRAM mangement deactivated -- requesting Auto1111 to reload checkpoint\")\n        response = requests.post(url=f'{params[\"address\"]}/sdapi/v1/reload-checkpoint', json='')\n        response.raise_for_status()\n\n    else:\n        raise RuntimeError(f'Managing VRAM: \"{actor}\" is not a known state!')\n\n    response.raise_for_status()\n    del response\n\n\nif params['manage_VRAM']:\n    give_VRAM_priority('set')\n\nSD_models = ['NeverEndingDream']  # TODO: get with http://{address}}/sdapi/v1/sd-models and allow user to select\n\npicture_response = False  # specifies if the next model response should appear as a picture\n\n\ndef remove_surrounded_chars(string):\n    # this expression matches to 'as few symbols as possible (0 upwards) between any asterisks' OR\n    # 'as few symbols as possible (0 upwards) between an asterisk and the end of the string'\n    return re.sub('\\*[^\\*]*?(\\*|$)', '', string)\n\n\ndef triggers_are_in(string):\n    string = remove_surrounded_chars(string)\n    # regex searches for send|main|message|me (at the end of the word) followed by\n    # a whole word of image|pic|picture|photo|snap|snapshot|selfie|meme(s),\n    # (?aims) are regex parser flags\n    return bool(re.search('(?aims)(send|mail|message|me)\\\\b.+?\\\\b(image|pic(ture)?|photo|snap(shot)?|selfie|meme)s?\\\\b', string))\n\n\ndef state_modifier(state):\n    if picture_response:\n        state['stream'] = False\n\n    return state\n\n\ndef input_modifier(string):\n    \"\"\"\n    This function is applied to your text inputs before\n    they are fed into the model.\n    \"\"\"\n\n    global params\n\n    if not params['mode'] == 1:  # if not in immersive/interactive mode, do nothing\n        return string\n\n    if triggers_are_in(string):  # if we're in it, check for trigger words\n        toggle_generation(True)\n        string = string.lower()\n        if \"of\" in string:\n            subject = string.split('of', 1)[1]  # subdivide the string once by the first 'of' instance and get what's coming after it\n            string = params['textgen_prefix'].replace(\"[subject]\", subject)\n        else:\n            string = params['textgen_prefix'].replace(\"[subject]\", \"your appearance, your surroundings and what you are doing right now\")\n\n    return string\n\n# Get and save the Stable Diffusion-generated picture\ndef get_SD_pictures(description, character):\n\n    global params\n\n    if params['manage_VRAM']:\n        give_VRAM_priority('SD')\n\n    description = re.sub('<audio.*?</audio>', ' ', description)\n    description = f\"({description}:1)\"\n\n    payload = {\n        \"prompt\": params['prompt_prefix'] + description,\n        \"seed\": params['seed'],\n        \"sampler_name\": params['sampler_name'],\n        \"enable_hr\": params['enable_hr'],\n        \"hr_scale\": params['hr_scale'],\n        \"hr_upscaler\": params['hr_upscaler'],\n        \"denoising_strength\": params['denoising_strength'],\n        \"steps\": params['steps'],\n        \"cfg_scale\": params['cfg_scale'],\n        \"width\": params['width'],\n        \"height\": params['height'],\n        \"restore_faces\": params['restore_faces'],\n        \"override_settings_restore_afterwards\": True,\n        \"negative_prompt\": params['negative_prompt']\n    }\n\n    print(f'Prompting the image generator via the API on {params[\"address\"]}...')\n    response = requests.post(url=f'{params[\"address\"]}/sdapi/v1/txt2img', json=payload)\n    response.raise_for_status()\n    r = response.json()\n\n    visible_result = \"\"\n    for img_str in r['images']:\n        if params['save_img']:\n            img_data = base64.b64decode(img_str)\n\n            variadic = f'{date.today().strftime(\"%Y_%m_%d\")}/{character}_{int(time.time())}'\n            output_file = Path(f'extensions/sd_api_pictures/outputs/{variadic}.png')\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n\n            with open(output_file.as_posix(), 'wb') as f:\n                f.write(img_data)\n\n            visible_result = visible_result + f'<img src=\"/file/extensions/sd_api_pictures/outputs/{variadic}.png\" alt=\"{description}\" style=\"max-width: unset; max-height: unset;\">\\n'\n        else:\n            image = Image.open(io.BytesIO(base64.b64decode(img_str.split(\",\", 1)[0])))\n            # lower the resolution of received images for the chat, otherwise the log size gets out of control quickly with all the base64 values in visible history\n            image.thumbnail((300, 300))\n            buffered = io.BytesIO()\n            image.save(buffered, format=\"JPEG\")\n            buffered.seek(0)\n            image_bytes = buffered.getvalue()\n            img_str = \"data:image/jpeg;base64,\" + base64.b64encode(image_bytes).decode()\n            visible_result = visible_result + f'<img src=\"{img_str}\" alt=\"{description}\">\\n'\n\n    if params['manage_VRAM']:\n        give_VRAM_priority('LLM')\n\n    return visible_result\n\n# TODO: how do I make the UI history ignore the resulting pictures (I don't want HTML to appear in history)\n# and replace it with 'text' for the purposes of logging?\ndef output_modifier(string, state):\n    \"\"\"\n    This function is applied to the model outputs.\n    \"\"\"\n\n    global picture_response, params\n\n    if not picture_response:\n        return string\n\n    string = remove_surrounded_chars(string)\n    string = string.replace('\"', '')\n    string = string.replace('\u201c', '')\n    string = string.replace('\\n', ' ')\n    string = string.strip()\n\n    if string == '':\n        string = 'no viable description in reply, try regenerating'\n        return string\n\n    text = \"\"\n    if (params['mode'] < 2):\n        toggle_generation(False)\n        text = f'*Sends a picture which portrays: \u201c{string}\u201d*'\n    else:\n        text = string\n\n    string = get_SD_pictures(string, state['character_menu']) + \"\\n\" + text\n\n    return string\n\n\ndef bot_prefix_modifier(string):\n    \"\"\"\n    This function is only applied in chat mode. It modifies\n    the prefix text for the Bot and can be used to bias its\n    behavior.\n    \"\"\"\n\n    return string\n\n\ndef toggle_generation(*args):\n    global picture_response, shared\n\n    if not args:\n        picture_response = not picture_response\n    else:\n        picture_response = args[0]\n\n    shared.processing_message = \"*Is sending a picture...*\" if picture_response else \"*Is typing...*\"\n\n\ndef filter_address(address):\n    address = address.strip()\n    # address = re.sub('http(s)?:\\/\\/|\\/$','',address) # remove starting http:// OR https:// OR trailing slash\n    address = re.sub('\\/$', '', address)  # remove trailing /s\n    if not address.startswith('http'):\n        address = 'http://' + address\n    return address\n\n\ndef SD_api_address_update(address):\n    global params\n\n    msg = \"\u2714\ufe0f SD API is found on:\"\n    address = filter_address(address)\n    params.update({\"address\": address})\n    try:\n        response = requests.get(url=f'{params[\"address\"]}/sdapi/v1/sd-models')\n        response.raise_for_status()\n        # r = response.json()\n    except:\n        msg = \"\u274c No SD API endpoint on:\"\n\n    return gr.Textbox.update(label=msg)\n\n\ndef custom_css():\n    path_to_css = Path(__file__).parent.resolve() / 'style.css'\n    return open(path_to_css, 'r').read()\n\n\ndef get_checkpoints():\n    global params\n\n    try:\n        models = requests.get(url=f'{params[\"address\"]}/sdapi/v1/sd-models')\n        options = requests.get(url=f'{params[\"address\"]}/sdapi/v1/options')\n        options_json = options.json()\n        params['sd_checkpoint'] = options_json['sd_model_checkpoint']\n        params['checkpoint_list'] = [result[\"title\"] for result in models.json()]\n    except:\n        params['sd_checkpoint'] = \"\"\n        params['checkpoint_list'] = []\n\n    return gr.update(choices=params['checkpoint_list'], value=params['sd_checkpoint'])\n\n\ndef load_checkpoint(checkpoint):\n    payload = {\n        \"sd_model_checkpoint\": checkpoint\n    }\n\n    try:\n        requests.post(url=f'{params[\"address\"]}/sdapi/v1/options', json=payload)\n    except:\n        pass\n\n\ndef get_samplers():\n    try:\n        response = requests.get(url=f'{params[\"address\"]}/sdapi/v1/samplers')\n        response.raise_for_status()\n        samplers = [x[\"name\"] for x in response.json()]\n    except:\n        samplers = []\n\n    return samplers\n\n\ndef ui():\n\n    # Gradio elements\n    # gr.Markdown('### Stable Diffusion API Pictures') # Currently the name of extension is shown as the title\n    with gr.Accordion(\"Parameters\", open=True, elem_classes=\"SDAP\"):\n        with gr.Row():\n            address = gr.Textbox(placeholder=params['address'], value=params['address'], label='Auto1111\\'s WebUI address')\n            modes_list = [\"Manual\", \"Immersive/Interactive\", \"Picturebook/Adventure\"]\n            mode = gr.Dropdown(modes_list, value=modes_list[params['mode']], label=\"Mode of operation\", type=\"index\")\n            with gr.Column(scale=1, min_width=300):\n                manage_VRAM = gr.Checkbox(value=params['manage_VRAM'], label='Manage VRAM')\n                save_img = gr.Checkbox(value=params['save_img'], label='Keep original images and use them in chat')\n\n            force_pic = gr.Button(\"Force the picture response\")\n            suppr_pic = gr.Button(\"Suppress the picture response\")\n        with gr.Row():\n            checkpoint = gr.Dropdown(params['checkpoint_list'], value=params['sd_checkpoint'], label=\"Checkpoint\", type=\"value\")\n            update_checkpoints = gr.Button(\"Get list of checkpoints\")\n\n        with gr.Accordion(\"Generation parameters\", open=False):\n            prompt_prefix = gr.Textbox(placeholder=params['prompt_prefix'], value=params['prompt_prefix'], label='Prompt Prefix (best used to describe the look of the character)')\n            textgen_prefix = gr.Textbox(placeholder=params['textgen_prefix'], value=params['textgen_prefix'], label='textgen prefix (type [subject] where the subject should be placed)')\n            negative_prompt = gr.Textbox(placeholder=params['negative_prompt'], value=params['negative_prompt'], label='Negative Prompt')\n            with gr.Row():\n                with gr.Column():\n                    width = gr.Slider(64, 2048, value=params['width'], step=64, label='Width')\n                    height = gr.Slider(64, 2048, value=params['height'], step=64, label='Height')\n                with gr.Column(variant=\"compact\", elem_id=\"sampler_col\"):\n                    with gr.Row(elem_id=\"sampler_row\"):\n                        sampler_name = gr.Dropdown(value=params['sampler_name'], allow_custom_value=True, label='Sampling method', elem_id=\"sampler_box\")\n                        create_refresh_button(sampler_name, lambda: None, lambda: {'choices': get_samplers()}, 'refresh-button')\n                    steps = gr.Slider(1, 150, value=params['steps'], step=1, label=\"Sampling steps\", elem_id=\"steps_box\")\n            with gr.Row():\n                seed = gr.Number(label=\"Seed\", value=params['seed'], elem_id=\"seed_box\")\n                cfg_scale = gr.Number(label=\"CFG Scale\", value=params['cfg_scale'], elem_id=\"cfg_box\")\n                with gr.Column() as hr_options:\n                    restore_faces = gr.Checkbox(value=params['restore_faces'], label='Restore faces')\n                    enable_hr = gr.Checkbox(value=params['enable_hr'], label='Hires. fix')\n            with gr.Row(visible=params['enable_hr'], elem_classes=\"hires_opts\") as hr_options:\n                hr_scale = gr.Slider(1, 4, value=params['hr_scale'], step=0.1, label='Upscale by')\n                denoising_strength = gr.Slider(0, 1, value=params['denoising_strength'], step=0.01, label='Denoising strength')\n                hr_upscaler = gr.Textbox(placeholder=params['hr_upscaler'], value=params['hr_upscaler'], label='Upscaler')\n\n    # Event functions to update the parameters in the backend\n    address.change(lambda x: params.update({\"address\": filter_address(x)}), address, None)\n    mode.select(lambda x: params.update({\"mode\": x}), mode, None)\n    mode.select(lambda x: toggle_generation(x > 1), inputs=mode, outputs=None)\n    manage_VRAM.change(lambda x: params.update({\"manage_VRAM\": x}), manage_VRAM, None)\n    manage_VRAM.change(lambda x: give_VRAM_priority('set' if x else 'reset'), inputs=manage_VRAM, outputs=None)\n    save_img.change(lambda x: params.update({\"save_img\": x}), save_img, None)\n\n    address.submit(fn=SD_api_address_update, inputs=address, outputs=address)\n    prompt_prefix.change(lambda x: params.update({\"prompt_prefix\": x}), prompt_prefix, None)\n    textgen_prefix.change(lambda x: params.update({\"textgen_prefix\": x}), textgen_prefix, None)\n    negative_prompt.change(lambda x: params.update({\"negative_prompt\": x}), negative_prompt, None)\n    width.change(lambda x: params.update({\"width\": x}), width, None)\n    height.change(lambda x: params.update({\"height\": x}), height, None)\n    hr_scale.change(lambda x: params.update({\"hr_scale\": x}), hr_scale, None)\n    denoising_strength.change(lambda x: params.update({\"denoising_strength\": x}), denoising_strength, None)\n    restore_faces.change(lambda x: params.update({\"restore_faces\": x}), restore_faces, None)\n    hr_upscaler.change(lambda x: params.update({\"hr_upscaler\": x}), hr_upscaler, None)\n    enable_hr.change(lambda x: params.update({\"enable_hr\": x}), enable_hr, None)\n    enable_hr.change(lambda x: hr_options.update(visible=params[\"enable_hr\"]), enable_hr, hr_options)\n    update_checkpoints.click(get_checkpoints, None, checkpoint)\n    checkpoint.change(lambda x: params.update({\"sd_checkpoint\": x}), checkpoint, None)\n    checkpoint.change(load_checkpoint, checkpoint, None)\n\n    sampler_name.change(lambda x: params.update({\"sampler_name\": x}), sampler_name, None)\n    steps.change(lambda x: params.update({\"steps\": x}), steps, None)\n    seed.change(lambda x: params.update({\"seed\": x}), seed, None)\n    cfg_scale.change(lambda x: params.update({\"cfg_scale\": x}), cfg_scale, None)\n\n    force_pic.click(lambda x: toggle_generation(True), inputs=force_pic, outputs=None)\n    suppr_pic.click(lambda x: toggle_generation(False), inputs=suppr_pic, outputs=None)\n", "extensions/superboogav2/optimize.py": "\"\"\"\nThis module implements a hyperparameter optimization routine for the embedding application. It utilizes TPE optimization from Optuna.\n\nEach run, the optimizer will set the default values inside the hyperparameters. At the end, it will output the best ones it has found.\n\"\"\"\nimport hashlib\nimport json\nimport logging\nimport re\n\nimport gradio as gr\nimport numpy as np\nimport optuna\n\nlogging.getLogger('optuna').setLevel(logging.WARNING)\n\nfrom pathlib import Path\n\nimport extensions.superboogav2.parameters as parameters\nfrom modules.logging_colors import logger\n\nfrom .benchmark import benchmark\nfrom .parameters import Parameters\n\n\n# Format the parameters into markdown format.\ndef _markdown_hyperparams():\n    res = []\n    for param_name, param_value in Parameters.getInstance().hyperparameters.items():\n        # Escape any markdown syntax\n        param_name = re.sub(r\"([_*\\[\\]()~`>#+-.!])\", r\"\\\\\\1\", param_name)\n        param_value_default = re.sub(r\"([_*\\[\\]()~`>#+-.!])\", r\"\\\\\\1\", str(param_value['default'])) if param_value['default'] else ' '\n\n        res.append('* {}: **{}**'.format(param_name, param_value_default))\n\n    return '\\n'.join(res)\n\n\n# Convert numpy types to python types.\ndef _convert_np_types(params):\n    for key in params:\n        if type(params[key]) == np.bool_:\n            params[key] = bool(params[key])\n        elif type(params[key]) == np.int64:\n            params[key] = int(params[key])\n        elif type(params[key]) == np.float64:\n            params[key] = float(params[key])\n    return params\n\n\n# Set the default values for the hyperparameters.\ndef _set_hyperparameters(params):\n    for param_name, param_value in params.items():\n        if param_name in Parameters.getInstance().hyperparameters:\n            Parameters.getInstance().hyperparameters[param_name]['default'] = param_value\n\n\n# Check if the parameter is for optimization.\ndef _is_optimization_param(val):\n    is_opt = val.get('should_optimize', False)  # Either does not exist or is false\n    return is_opt\n\n\n# Create a hashable representation of the parameters\ndef _get_params_hash(params):\n    params_str = json.dumps(params, sort_keys=True)\n    return hashlib.sha256(params_str.encode()).hexdigest()\n\n\ndef optimize(collector, progress=gr.Progress()):\n    # Inform the user that something is happening.\n    progress(0, desc='Setting Up...')\n\n    # Track the current step\n    current_step = 0\n\n    # Track the best score\n    best_score = 0\n\n    # Dictionary for caching scores\n    scores_cache = {}\n\n    def objective_function(trial):\n        nonlocal current_step\n        nonlocal best_score\n        nonlocal scores_cache\n\n        params = {}\n        for key, val in Parameters.getInstance().hyperparameters.items():\n            if _is_optimization_param(val):\n                params[key] = trial.suggest_categorical(key, val['categories'])\n\n        _set_hyperparameters(params)\n\n        params_hash = _get_params_hash(params)\n\n        # If the score for these parameters is in the cache, return it\n        if params_hash in scores_cache:\n            return scores_cache[params_hash]\n\n        # Benchmark the current set of parameters.\n        score, max_score = benchmark(Path(\"extensions/superboogav2/benchmark_texts/questions.json\"), collector)\n\n        # Cache the score\n        scores_cache[params_hash] = score\n\n        result = json.dumps(_convert_np_types(params), indent=4)\n        result += f'\\nScore: {score}/{max_score}'\n\n        logger.debug(result)\n\n        # Increment the current step\n        current_step += 1\n\n        # Update the best score\n        best_score = max(best_score, score)\n\n        # Update the progress\n        progress(current_step / parameters.get_optimization_steps(), desc=f'Optimizing... {current_step}/{parameters.get_optimization_steps()}')\n\n        return -score\n\n    # Run the optimization.\n    study = optuna.create_study()\n    study.optimize(objective_function, n_trials=int(parameters.get_optimization_steps()))\n\n    best_params = study.best_params\n    _set_hyperparameters(best_params)\n\n    # Convert results to a markdown string.\n    str_result = f\"## Best parameters:\\n\\n{_markdown_hyperparams()}\\n\\n## Score:\\n\\n{best_score}\"\n\n    # Save to JSON file\n    with open('best_params.json', 'w') as fp:\n        json.dump(_convert_np_types(best_params), fp, indent=4)\n\n    return str_result\n", "extensions/superboogav2/benchmark.py": "\"\"\"\nThis module implements a benchmark function to evaluate the performance of the embedding pipeline. It expects a configuration JSON file. It must have questions and expected retrieved text.\nFor each question, it's essential to have variants of that question. Language is fluid and each person might have their own spin on how they may ask it.\n\nAt the end, it will save the results inside a benchmark_{sysdate}.txt file in the main directory.\n\nThe benchmark function will return the score as an integer.\n\"\"\"\nimport datetime\nimport json\nimport os\nfrom pathlib import Path\n\nfrom .data_processor import preprocess_text, process_and_add_to_collector\nfrom .parameters import get_chunk_count, get_max_token_count\nfrom .utils import create_metadata_source\n\n\ndef benchmark(config_path, collector):\n    # Get the current system date\n    sysdate = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"benchmark_{sysdate}.txt\"\n\n    # Open the log file in append mode\n    with open(filename, 'a') as log:\n        with open(config_path, 'r') as f:\n            data = json.load(f)\n\n        total_points = 0\n        max_points = 0\n\n        for item in data:\n            filepath = item[\"text\"]\n            corpus = \"\"\n\n            # Check if the file exists\n            if os.path.isfile(Path(filepath)):\n                # Open the file and read its content\n                with open(Path(filepath), 'r') as file:\n                    corpus = file.read()\n                process_and_add_to_collector(corpus, collector, True, create_metadata_source('benchmark'))\n            else:\n                raise f'Cannot find specified file {filepath}.'\n\n            for question_group in item[\"questions\"]:\n                question_variants = question_group[\"question_variants\"]\n                criteria = question_group[\"criteria\"]\n\n                for q in question_variants:\n                    max_points += len(criteria)\n                    processed_text = preprocess_text(q)\n\n                    # Get the most similar chunks\n                    results = collector.get_sorted_by_dist(processed_text, n_results=get_chunk_count(), max_token_count=get_max_token_count())\n\n                    points = 0\n\n                    for c in criteria:\n                        for p in results:\n                            if c in p:\n                                points += 1\n                                total_points += 1\n                                break\n\n                    info = f\"The question '{q}' scored {points}/{len(criteria)} points.\"\n                    print(info, file=log)\n\n                print('\\n---\\n', file=log)\n\n        print(f'##Total points:\\n\\n{total_points}/{max_points}', file=log)\n\n    return total_points, max_points\n", "extensions/superboogav2/api.py": "\"\"\"\nThis module is responsible for the VectorDB API. It currently supports:\n* DELETE api/v1/clear\n    - Clears the whole DB.\n* POST api/v1/add\n    - Add some corpus to the DB. You can also specify metadata to be added alongside it.\n* POST api/v1/delete\n    - Delete specific records with given metadata.\n* POST api/v1/get\n    - Get results from chromaDB.\n\"\"\"\n\nimport json\nfrom http.server import BaseHTTPRequestHandler, ThreadingHTTPServer\nfrom threading import Thread\nfrom urllib.parse import parse_qs, urlparse\n\nimport extensions.superboogav2.parameters as parameters\nfrom modules import shared\nfrom modules.logging_colors import logger\n\nfrom .chromadb import ChromaCollector\nfrom .data_processor import process_and_add_to_collector\n\n\nclass CustomThreadingHTTPServer(ThreadingHTTPServer):\n    def __init__(self, server_address, RequestHandlerClass, collector: ChromaCollector, bind_and_activate=True):\n        self.collector = collector\n        super().__init__(server_address, RequestHandlerClass, bind_and_activate)\n\n    def finish_request(self, request, client_address):\n        self.RequestHandlerClass(request, client_address, self, self.collector)\n\n\nclass Handler(BaseHTTPRequestHandler):\n    def __init__(self, request, client_address, server, collector: ChromaCollector):\n        self.collector = collector\n        super().__init__(request, client_address, server)\n\n    def _send_412_error(self, message):\n        self.send_response(412)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.end_headers()\n        response = json.dumps({\"error\": message})\n        self.wfile.write(response.encode('utf-8'))\n\n    def _send_404_error(self):\n        self.send_response(404)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.end_headers()\n        response = json.dumps({\"error\": \"Resource not found\"})\n        self.wfile.write(response.encode('utf-8'))\n\n    def _send_400_error(self, error_message: str):\n        self.send_response(400)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.end_headers()\n        response = json.dumps({\"error\": error_message})\n        self.wfile.write(response.encode('utf-8'))\n\n    def _send_200_response(self, message: str):\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"application/json\")\n        self.end_headers()\n\n        if isinstance(message, str):\n            response = json.dumps({\"message\": message})\n        else:\n            response = json.dumps(message)\n\n        self.wfile.write(response.encode('utf-8'))\n\n    def _handle_get(self, search_strings: list[str], n_results: int, max_token_count: int, sort_param: str):\n        if sort_param == parameters.SORT_DISTANCE:\n            results = self.collector.get_sorted_by_dist(search_strings, n_results, max_token_count)\n        elif sort_param == parameters.SORT_ID:\n            results = self.collector.get_sorted_by_id(search_strings, n_results, max_token_count)\n        else:  # Default is dist\n            results = self.collector.get_sorted_by_dist(search_strings, n_results, max_token_count)\n\n        return {\n            \"results\": results\n        }\n\n    def do_GET(self):\n        self._send_404_error()\n\n    def do_POST(self):\n        try:\n            content_length = int(self.headers['Content-Length'])\n            body = json.loads(self.rfile.read(content_length).decode('utf-8'))\n\n            parsed_path = urlparse(self.path)\n            path = parsed_path.path\n            query_params = parse_qs(parsed_path.query)\n\n            if path in ['/api/v1/add', '/api/add']:\n                corpus = body.get('corpus')\n                if corpus is None:\n                    self._send_412_error(\"Missing parameter 'corpus'\")\n                    return\n\n                clear_before_adding = body.get('clear_before_adding', False)\n                metadata = body.get('metadata')\n                process_and_add_to_collector(corpus, self.collector, clear_before_adding, metadata)\n                self._send_200_response(\"Data successfully added\")\n\n            elif path in ['/api/v1/delete', '/api/delete']:\n                metadata = body.get('metadata')\n                if corpus is None:\n                    self._send_412_error(\"Missing parameter 'metadata'\")\n                    return\n\n                self.collector.delete(ids_to_delete=None, where=metadata)\n                self._send_200_response(\"Data successfully deleted\")\n\n            elif path in ['/api/v1/get', '/api/get']:\n                search_strings = body.get('search_strings')\n                if search_strings is None:\n                    self._send_412_error(\"Missing parameter 'search_strings'\")\n                    return\n\n                n_results = body.get('n_results')\n                if n_results is None:\n                    n_results = parameters.get_chunk_count()\n\n                max_token_count = body.get('max_token_count')\n                if max_token_count is None:\n                    max_token_count = parameters.get_max_token_count()\n\n                sort_param = query_params.get('sort', ['distance'])[0]\n\n                results = self._handle_get(search_strings, n_results, max_token_count, sort_param)\n                self._send_200_response(results)\n\n            else:\n                self._send_404_error()\n        except Exception as e:\n            self._send_400_error(str(e))\n\n    def do_DELETE(self):\n        try:\n            parsed_path = urlparse(self.path)\n            path = parsed_path.path\n            query_params = parse_qs(parsed_path.query)\n\n            if path in ['/api/v1/clear', '/api/clear']:\n                self.collector.clear()\n                self._send_200_response(\"Data successfully cleared\")\n            else:\n                self._send_404_error()\n        except Exception as e:\n            self._send_400_error(str(e))\n\n    def do_OPTIONS(self):\n        self.send_response(200)\n        self.end_headers()\n\n    def end_headers(self):\n        self.send_header('Access-Control-Allow-Origin', '*')\n        self.send_header('Access-Control-Allow-Methods', '*')\n        self.send_header('Access-Control-Allow-Headers', '*')\n        self.send_header('Cache-Control', 'no-store, no-cache, must-revalidate')\n        super().end_headers()\n\n\nclass APIManager:\n    def __init__(self, collector: ChromaCollector):\n        self.server = None\n        self.collector = collector\n        self.is_running = False\n\n    def start_server(self, port: int):\n        if self.server is not None:\n            print(\"Server already running.\")\n            return\n\n        address = '0.0.0.0' if shared.args.listen else '127.0.0.1'\n        self.server = CustomThreadingHTTPServer((address, port), Handler, self.collector)\n\n        logger.info(f'Starting chromaDB API at http://{address}:{port}/api')\n\n        Thread(target=self.server.serve_forever, daemon=True).start()\n\n        self.is_running = True\n\n    def stop_server(self):\n        if self.server is not None:\n            logger.info('Stopping chromaDB API.')\n            self.server.shutdown()\n            self.server.server_close()\n            self.server = None\n            self.is_running = False\n\n    def is_server_running(self):\n        return self.is_running\n", "extensions/superboogav2/utils.py": "\"\"\"\nThis module contains common functions across multiple other modules.\n\"\"\"\n\nimport extensions.superboogav2.parameters as parameters\n\n\n# Create the context using the prefix + data_separator + postfix from parameters.\ndef create_context_text(results):\n    context = parameters.get_prefix() + parameters.get_data_separator().join(results) + parameters.get_postfix()\n\n    return context\n\n\n# Create metadata with the specified source\ndef create_metadata_source(source: str):\n    return {'source': source}\n", "extensions/superboogav2/chromadb.py": "import math\nimport random\nimport threading\n\nimport chromadb\nimport numpy as np\nimport posthog\nfrom chromadb.config import Settings\nfrom chromadb.utils import embedding_functions\n\nimport extensions.superboogav2.parameters as parameters\nfrom modules.logging_colors import logger\nfrom modules.text_generation import decode, encode\n\n# Intercept calls to posthog\nposthog.capture = lambda *args, **kwargs: None\n\n\nembedder = embedding_functions.SentenceTransformerEmbeddingFunction(\"sentence-transformers/all-mpnet-base-v2\")\n\n\nclass Info:\n    def __init__(self, start_index, text_with_context, distance, id):\n        self.text_with_context = text_with_context\n        self.start_index = start_index\n        self.distance = distance\n        self.id = id\n\n    def calculate_distance(self, other_info):\n        if parameters.get_new_dist_strategy() == parameters.DIST_MIN_STRATEGY:\n            # Min\n            return min(self.distance, other_info.distance)\n        elif parameters.get_new_dist_strategy() == parameters.DIST_HARMONIC_STRATEGY:\n            # Harmonic mean\n            return 2 * (self.distance * other_info.distance) / (self.distance + other_info.distance)\n        elif parameters.get_new_dist_strategy() == parameters.DIST_GEOMETRIC_STRATEGY:\n            # Geometric mean\n            return (self.distance * other_info.distance) ** 0.5\n        elif parameters.get_new_dist_strategy() == parameters.DIST_ARITHMETIC_STRATEGY:\n            # Arithmetic mean\n            return (self.distance + other_info.distance) / 2\n        else:  # Min is default\n            return min(self.distance, other_info.distance)\n\n    def merge_with(self, other_info):\n        s1 = self.text_with_context\n        s2 = other_info.text_with_context\n        s1_start = self.start_index\n        s2_start = other_info.start_index\n\n        new_dist = self.calculate_distance(other_info)\n\n        if self.should_merge(s1, s2, s1_start, s2_start):\n            if s1_start <= s2_start:\n                if s1_start + len(s1) >= s2_start + len(s2):  # if s1 completely covers s2\n                    return Info(s1_start, s1, new_dist, self.id)\n                else:\n                    overlap = max(0, s1_start + len(s1) - s2_start)\n                    return Info(s1_start, s1 + s2[overlap:], new_dist, self.id)\n            else:\n                if s2_start + len(s2) >= s1_start + len(s1):  # if s2 completely covers s1\n                    return Info(s2_start, s2, new_dist, other_info.id)\n                else:\n                    overlap = max(0, s2_start + len(s2) - s1_start)\n                    return Info(s2_start, s2 + s1[overlap:], new_dist, other_info.id)\n\n        return None\n\n    @staticmethod\n    def should_merge(s1, s2, s1_start, s2_start):\n        # Check if s1 and s2 are adjacent or overlapping\n        s1_end = s1_start + len(s1)\n        s2_end = s2_start + len(s2)\n\n        return not (s1_end < s2_start or s2_end < s1_start)\n\n\nclass ChromaCollector():\n    def __init__(self):\n        name = ''.join(random.choice('ab') for _ in range(10))\n\n        self.name = name\n        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n        self.collection = self.chroma_client.create_collection(name=name, embedding_function=embedder)\n\n        self.ids = []\n        self.id_to_info = {}\n        self.embeddings_cache = {}\n        self.lock = threading.Lock()  # Locking so the server doesn't break.\n\n    def add(self, texts: list[str], texts_with_context: list[str], starting_indices: list[int], metadatas: list[dict] = None):\n        with self.lock:\n            assert metadatas is None or len(metadatas) == len(texts), \"metadatas must be None or have the same length as texts\"\n\n            if len(texts) == 0:\n                return\n\n            new_ids = self._get_new_ids(len(texts))\n\n            (existing_texts, existing_embeddings, existing_ids, existing_metas), \\\n                (non_existing_texts, non_existing_ids, non_existing_metas) = self._split_texts_by_cache_hit(texts, new_ids, metadatas)\n\n            # If there are any already existing texts, add them all at once.\n            if existing_texts:\n                logger.info(f'Adding {len(existing_embeddings)} cached embeddings.')\n                args = {'embeddings': existing_embeddings, 'documents': existing_texts, 'ids': existing_ids}\n                if metadatas is not None:\n                    args['metadatas'] = existing_metas\n                self.collection.add(**args)\n\n            # If there are any non-existing texts, compute their embeddings all at once. Each call to embed has significant overhead.\n            if non_existing_texts:\n                non_existing_embeddings = embedder(non_existing_texts)\n                for text, embedding in zip(non_existing_texts, non_existing_embeddings):\n                    self.embeddings_cache[text] = embedding\n\n                logger.info(f'Adding {len(non_existing_embeddings)} new embeddings.')\n                args = {'embeddings': non_existing_embeddings, 'documents': non_existing_texts, 'ids': non_existing_ids}\n                if metadatas is not None:\n                    args['metadatas'] = non_existing_metas\n                self.collection.add(**args)\n\n            # Create a dictionary that maps each ID to its context and starting index\n            new_info = {\n                id_: {'text_with_context': context, 'start_index': start_index}\n                for id_, context, start_index in zip(new_ids, texts_with_context, starting_indices)\n            }\n\n            self.id_to_info.update(new_info)\n            self.ids.extend(new_ids)\n\n    def _split_texts_by_cache_hit(self, texts: list[str], new_ids: list[str], metadatas: list[dict]):\n        existing_texts, non_existing_texts = [], []\n        existing_embeddings = []\n        existing_ids, non_existing_ids = [], []\n        existing_metas, non_existing_metas = [], []\n\n        for i, text in enumerate(texts):\n            id_ = new_ids[i]\n            metadata = metadatas[i] if metadatas is not None else None\n            embedding = self.embeddings_cache.get(text)\n            if embedding:\n                existing_texts.append(text)\n                existing_embeddings.append(embedding)\n                existing_ids.append(id_)\n                existing_metas.append(metadata)\n            else:\n                non_existing_texts.append(text)\n                non_existing_ids.append(id_)\n                non_existing_metas.append(metadata)\n\n        return (existing_texts, existing_embeddings, existing_ids, existing_metas), \\\n               (non_existing_texts, non_existing_ids, non_existing_metas)\n\n    def _get_new_ids(self, num_new_ids: int):\n        if self.ids:\n            max_existing_id = max(int(id_) for id_ in self.ids)\n        else:\n            max_existing_id = -1\n\n        return [str(i + max_existing_id + 1) for i in range(num_new_ids)]\n\n    def _find_min_max_start_index(self):\n        max_index, min_index = 0, float('inf')\n        for _, val in self.id_to_info.items():\n            if val['start_index'] > max_index:\n                max_index = val['start_index']\n            if val['start_index'] < min_index:\n                min_index = val['start_index']\n        return min_index, max_index\n\n    # NB: Does not make sense to weigh excerpts from different documents.\n    # But let's say that's the user's problem. Perfect world scenario:\n    # Apply time weighing to different documents. For each document, then, add\n    # separate time weighing.\n\n    def _apply_sigmoid_time_weighing(self, infos: list[Info], document_len: int, time_steepness: float, time_power: float):\n        def sigmoid(x):\n            return 1 / (1 + np.exp(-x))\n\n        weights = sigmoid(time_steepness * np.linspace(-10, 10, document_len))\n\n        # Scale to [0,time_power] and shift it up to [1-time_power, 1]\n        weights = weights - min(weights)\n        weights = weights * (time_power / max(weights))\n        weights = weights + (1 - time_power)\n\n        # Reverse the weights\n        weights = weights[::-1]\n\n        for info in infos:\n            index = info.start_index\n            info.distance *= weights[index]\n\n    def _filter_outliers_by_median_distance(self, infos: list[Info], significant_level: float):\n        # Ensure there are infos to filter\n        if not infos:\n            return []\n\n        # Find info with minimum distance\n        min_info = min(infos, key=lambda x: x.distance)\n\n        # Calculate median distance among infos\n        median_distance = np.median([inf.distance for inf in infos])\n\n        # Filter out infos that have a distance significantly greater than the median\n        filtered_infos = [inf for inf in infos if inf.distance <= significant_level * median_distance]\n\n        # Always include the info with minimum distance\n        if min_info not in filtered_infos:\n            filtered_infos.append(min_info)\n\n        return filtered_infos\n\n    def _merge_infos(self, infos: list[Info]):\n        merged_infos = []\n        current_info = infos[0]\n\n        for next_info in infos[1:]:\n            merged = current_info.merge_with(next_info)\n            if merged is not None:\n                current_info = merged\n            else:\n                merged_infos.append(current_info)\n                current_info = next_info\n\n        merged_infos.append(current_info)\n        return merged_infos\n\n    # Main function for retrieving chunks by distance. It performs merging, time weighing, and mean filtering.\n\n    def _get_documents_ids_distances(self, search_strings: list[str], n_results: int):\n        n_results = min(len(self.ids), n_results)\n        if n_results == 0:\n            return [], [], []\n\n        if isinstance(search_strings, str):\n            search_strings = [search_strings]\n\n        infos = []\n        min_start_index, max_start_index = self._find_min_max_start_index()\n\n        for search_string in search_strings:\n            result = self.collection.query(query_texts=search_string, n_results=math.ceil(n_results / len(search_strings)), include=['distances'])\n            curr_infos = [Info(start_index=self.id_to_info[id]['start_index'],\n                               text_with_context=self.id_to_info[id]['text_with_context'],\n                               distance=distance, id=id)\n                          for id, distance in zip(result['ids'][0], result['distances'][0])]\n\n            self._apply_sigmoid_time_weighing(infos=curr_infos, document_len=max_start_index - min_start_index + 1, time_steepness=parameters.get_time_steepness(), time_power=parameters.get_time_power())\n            curr_infos = self._filter_outliers_by_median_distance(curr_infos, parameters.get_significant_level())\n            infos.extend(curr_infos)\n\n        infos.sort(key=lambda x: x.start_index)\n        infos = self._merge_infos(infos)\n\n        texts_with_context = [inf.text_with_context for inf in infos]\n        ids = [inf.id for inf in infos]\n        distances = [inf.distance for inf in infos]\n\n        return texts_with_context, ids, distances\n\n    # Get chunks by similarity\n\n    def get(self, search_strings: list[str], n_results: int) -> list[str]:\n        with self.lock:\n            documents, _, _ = self._get_documents_ids_distances(search_strings, n_results)\n            return documents\n\n    # Get ids by similarity\n\n    def get_ids(self, search_strings: list[str], n_results: int) -> list[str]:\n        with self.lock:\n            _, ids, _ = self._get_documents_ids_distances(search_strings, n_results)\n            return ids\n\n    # Cutoff token count\n\n    def _get_documents_up_to_token_count(self, documents: list[str], max_token_count: int):\n        # TODO: Move to caller; We add delimiters there which might go over the limit.\n        current_token_count = 0\n        return_documents = []\n\n        for doc in documents:\n            doc_tokens = encode(doc)[0]\n            doc_token_count = len(doc_tokens)\n            if current_token_count + doc_token_count > max_token_count:\n                # If adding this document would exceed the max token count,\n                # truncate the document to fit within the limit.\n                remaining_tokens = max_token_count - current_token_count\n\n                truncated_doc = decode(doc_tokens[:remaining_tokens], skip_special_tokens=True)\n                return_documents.append(truncated_doc)\n                break\n            else:\n                return_documents.append(doc)\n                current_token_count += doc_token_count\n\n        return return_documents\n\n    # Get chunks by similarity and then sort by ids\n\n    def get_sorted_by_ids(self, search_strings: list[str], n_results: int, max_token_count: int) -> list[str]:\n        with self.lock:\n            documents, ids, _ = self._get_documents_ids_distances(search_strings, n_results)\n            sorted_docs = [x for _, x in sorted(zip(ids, documents))]\n\n            return self._get_documents_up_to_token_count(sorted_docs, max_token_count)\n\n    # Get chunks by similarity and then sort by distance (lowest distance is last).\n\n    def get_sorted_by_dist(self, search_strings: list[str], n_results: int, max_token_count: int) -> list[str]:\n        with self.lock:\n            documents, _, distances = self._get_documents_ids_distances(search_strings, n_results)\n            sorted_docs = [doc for doc, _ in sorted(zip(documents, distances), key=lambda x: x[1])]  # sorted lowest -> highest\n\n            # If a document is truncated or competely skipped, it would be with high distance.\n            return_documents = self._get_documents_up_to_token_count(sorted_docs, max_token_count)\n            return_documents.reverse()  # highest -> lowest\n\n            return return_documents\n\n    def delete(self, ids_to_delete: list[str], where: dict):\n        with self.lock:\n            ids_to_delete = self.collection.get(ids=ids_to_delete, where=where)['ids']\n            self.collection.delete(ids=ids_to_delete, where=where)\n\n            # Remove the deleted ids from self.ids and self.id_to_info\n            ids_set = set(ids_to_delete)\n            self.ids = [id_ for id_ in self.ids if id_ not in ids_set]\n            for id_ in ids_to_delete:\n                self.id_to_info.pop(id_, None)\n\n            logger.info(f'Successfully deleted {len(ids_to_delete)} records from chromaDB.')\n\n    def clear(self):\n        with self.lock:\n            self.chroma_client.reset()\n\n            self.ids = []\n            self.chroma_client.delete_collection(name=self.name)\n            self.collection = self.chroma_client.create_collection(name=self.name, embedding_function=embedder)\n\n            logger.info('Successfully cleared all records and reset chromaDB.')\n\n\ndef make_collector():\n    return ChromaCollector()\n", "extensions/superboogav2/chat_handler.py": "\"\"\"\nThis module is responsible for modifying the chat prompt and history.\n\"\"\"\nimport re\n\nimport extensions.superboogav2.parameters as parameters\nfrom extensions.superboogav2.utils import (\n    create_context_text,\n    create_metadata_source\n)\nfrom modules import chat, shared\nfrom modules.chat import load_character_memoized\nfrom modules.logging_colors import logger\nfrom modules.text_generation import get_encoded_length\n\nfrom .chromadb import ChromaCollector\nfrom .data_processor import process_and_add_to_collector\n\nCHAT_METADATA = create_metadata_source('automatic-chat-insert')\n\n\ndef _remove_tag_if_necessary(user_input: str):\n    if not parameters.get_is_manual():\n        return user_input\n\n    return re.sub(r'^\\s*!c\\s*|\\s*!c\\s*$', '', user_input)\n\n\ndef _should_query(input: str):\n    if not parameters.get_is_manual():\n        return True\n\n    if re.search(r'^\\s*!c|!c\\s*$', input, re.MULTILINE):\n        return True\n\n    return False\n\n\ndef _format_single_exchange(name, text):\n    if re.search(r':\\s*$', name):\n        return '{} {}\\n'.format(name, text)\n    else:\n        return '{}: {}\\n'.format(name, text)\n\n\ndef _get_names(state: dict):\n    default_char = shared.settings.get('character', \"Assistant\")\n    default_user = shared.settings.get('name1', \"You\")\n    character = state.get('character', default_char)\n    user_name = state.get('name1', default_user)\n    user_name, bot_name, _, _, _ = load_character_memoized(character, user_name, '')\n\n    return user_name, bot_name\n\n\ndef _concatinate_history(history: dict, state: dict):\n    full_history_text = ''\n    user_name, bot_name = _get_names(state)\n\n    # Grab the internal history.\n    internal_history = history['internal']\n    assert isinstance(internal_history, list)\n\n    # Iterate through the history.\n    for exchange in internal_history:\n        assert isinstance(exchange, list)\n\n        if len(exchange) >= 1:\n            full_history_text += _format_single_exchange(user_name, exchange[0])\n        if len(exchange) >= 2:\n            full_history_text += _format_single_exchange(bot_name, exchange[1])\n\n    return full_history_text[:-1]  # Remove the last new line.\n\n\ndef _hijack_last(context_text: str, history: dict, max_len: int, state: dict):\n    num_context_tokens = get_encoded_length(context_text)\n\n    names = _get_names(state)[::-1]\n\n    history_tokens = 0\n    replace_position = None\n    for i, messages in enumerate(reversed(history['internal'])):\n        for j, message in enumerate(reversed(messages)):\n            num_message_tokens = get_encoded_length(_format_single_exchange(names[j], message))\n\n            # TODO: This is an extremely naive solution. A more robust implementation must be made.\n            if history_tokens + num_context_tokens <= max_len:\n                # This message can be replaced\n                replace_position = (i, j)\n\n            history_tokens += num_message_tokens\n\n    if replace_position is None:\n        logger.warn(\"The provided context_text is too long to replace any message in the history.\")\n    else:\n        # replace the message at replace_position with context_text\n        i, j = replace_position\n        history['internal'][-i - 1][-j - 1] = context_text\n\n\ndef custom_generate_chat_prompt_internal(user_input: str, state: dict, collector: ChromaCollector, **kwargs):\n    if parameters.get_add_chat_to_data():\n        # Get the whole history as one string\n        history_as_text = _concatinate_history(kwargs['history'], state)\n\n        if history_as_text:\n            # Delete all documents that were auto-inserted\n            collector.delete(ids_to_delete=None, where=CHAT_METADATA)\n            # Insert the processed history\n            process_and_add_to_collector(history_as_text, collector, False, CHAT_METADATA)\n\n    if _should_query(user_input):\n        user_input = _remove_tag_if_necessary(user_input)\n        results = collector.get_sorted_by_dist(user_input, n_results=parameters.get_chunk_count(), max_token_count=int(parameters.get_max_token_count()))\n\n        # Check if the strategy is to modify the last message. If so, prepend or append to the user query.\n        if parameters.get_injection_strategy() == parameters.APPEND_TO_LAST:\n            user_input = user_input + create_context_text(results)\n        elif parameters.get_injection_strategy() == parameters.PREPEND_TO_LAST:\n            user_input = create_context_text(results) + user_input\n        elif parameters.get_injection_strategy() == parameters.HIJACK_LAST_IN_CONTEXT:\n            _hijack_last(create_context_text(results), kwargs['history'], state['truncation_length'], state)\n\n    return chat.generate_chat_prompt(user_input, state, **kwargs)\n", "extensions/superboogav2/notebook_handler.py": "\"\"\"\nThis module is responsible for handling and modifying the notebook text.\n\"\"\"\nimport re\n\nimport extensions.superboogav2.parameters as parameters\nfrom extensions.superboogav2.utils import create_context_text\nfrom modules.logging_colors import logger\n\nfrom .data_processor import preprocess_text\n\n\ndef _remove_special_tokens(string):\n    pattern = r'(<\\|begin-user-input\\|>|<\\|end-user-input\\|>|<\\|injection-point\\|>)'\n    return re.sub(pattern, '', string)\n\n\ndef input_modifier_internal(string, collector, is_chat):\n    # Sanity check.\n    if is_chat:\n        return string\n\n    # Find the user input\n    pattern = re.compile(r\"<\\|begin-user-input\\|>(.*?)<\\|end-user-input\\|>\", re.DOTALL)\n    match = re.search(pattern, string)\n    if match:\n        # Preprocess the user prompt.\n        user_input = match.group(1).strip()\n        user_input = preprocess_text(user_input)\n\n        logger.debug(f\"Preprocessed User Input: {user_input}\")\n\n        # Get the most similar chunks\n        results = collector.get_sorted_by_dist(user_input, n_results=parameters.get_chunk_count(), max_token_count=int(parameters.get_max_token_count()))\n\n        # Make the injection\n        string = string.replace('<|injection-point|>', create_context_text(results))\n\n    return _remove_special_tokens(string)\n", "extensions/superboogav2/parameters.py": "\"\"\"\nThis module provides a singleton class `Parameters` that is used to manage all hyperparameters for the embedding application.\nIt expects a JSON file in `extensions/superboogav2/config.json`.\n\nEach element in the JSON must have a `default` value which will be used for the current run. Elements can have `categories`.\nThese categories define the range in which the optimizer will search. If the element is tagged with `\"should_optimize\": false`,\nthen the optimizer will only ever use the default value.\n\"\"\"\nimport json\nfrom pathlib import Path\n\nfrom modules.logging_colors import logger\n\nNUM_TO_WORD_METHOD = 'Number to Word'\nNUM_TO_CHAR_METHOD = 'Number to Char'\nNUM_TO_CHAR_LONG_METHOD = 'Number to Multi-Char'\n\n\nDIST_MIN_STRATEGY = 'Min of Two'\nDIST_HARMONIC_STRATEGY = 'Harmonic Mean'\nDIST_GEOMETRIC_STRATEGY = 'Geometric Mean'\nDIST_ARITHMETIC_STRATEGY = 'Arithmetic Mean'\n\n\nPREPEND_TO_LAST = 'Prepend to Last Message'\nAPPEND_TO_LAST = 'Append to Last Message'\nHIJACK_LAST_IN_CONTEXT = 'Hijack Last Message in Context \u26a0\ufe0f WIP \u26a0\ufe0f (Works Partially)'\n\n\nSORT_DISTANCE = 'distance'\nSORT_ID = 'id'\n\n\nclass Parameters:\n    _instance = None\n\n    variable_mapping = {\n        'NUM_TO_WORD_METHOD': NUM_TO_WORD_METHOD,\n        'NUM_TO_CHAR_METHOD': NUM_TO_CHAR_METHOD,\n        'NUM_TO_CHAR_LONG_METHOD': NUM_TO_CHAR_LONG_METHOD,\n        'DIST_MIN_STRATEGY': DIST_MIN_STRATEGY,\n        'DIST_HARMONIC_STRATEGY': DIST_HARMONIC_STRATEGY,\n        'DIST_GEOMETRIC_STRATEGY': DIST_GEOMETRIC_STRATEGY,\n        'DIST_ARITHMETIC_STRATEGY': DIST_ARITHMETIC_STRATEGY,\n        'PREPEND_TO_LAST': PREPEND_TO_LAST,\n        'APPEND_TO_LAST': APPEND_TO_LAST,\n        'HIJACK_LAST_IN_CONTEXT': HIJACK_LAST_IN_CONTEXT,\n    }\n\n    @staticmethod\n    def getInstance():\n        if Parameters._instance is None:\n            Parameters()\n        return Parameters._instance\n\n    def __init__(self):\n        if Parameters._instance is not None:\n            raise Exception(\"This class is a singleton!\")\n        else:\n            Parameters._instance = self\n            self.hyperparameters = self._load_from_json(Path(\"extensions/superboogav2/config.json\"))\n\n    def _load_from_json(self, file_path):\n        logger.debug('Loading hyperparameters...')\n\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n\n        # Replace variable names in the dict and create Categorical objects\n        for key in data:\n            if \"default\" in data[key] and data[key][\"default\"] in self.variable_mapping:\n                data[key][\"default\"] = self.variable_mapping[data[key][\"default\"]]\n            if \"categories\" in data[key]:\n                data[key][\"categories\"] = [self.variable_mapping.get(cat, cat) for cat in data[key][\"categories\"]]\n\n        return data\n\n\ndef should_to_lower() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['to_lower']['default'])\n\n\ndef get_num_conversion_strategy() -> str:\n    return Parameters.getInstance().hyperparameters['num_conversion']['default']\n\n\ndef should_merge_spaces() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['merge_spaces']['default'])\n\n\ndef should_strip() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['strip']['default'])\n\n\ndef should_remove_punctuation() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['remove_punctuation']['default'])\n\n\ndef should_remove_stopwords() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['remove_stopwords']['default'])\n\n\ndef should_remove_specific_pos() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['remove_specific_pos']['default'])\n\n\ndef should_lemmatize() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['lemmatize']['default'])\n\n\ndef get_min_num_sentences() -> int:\n    return int(Parameters.getInstance().hyperparameters['min_num_sent']['default'])\n\n\ndef get_delta_start() -> int:\n    return int(Parameters.getInstance().hyperparameters['delta_start']['default'])\n\n\ndef set_to_lower(value: bool):\n    Parameters.getInstance().hyperparameters['to_lower']['default'] = value\n\n\ndef set_num_conversion_strategy(value: str):\n    Parameters.getInstance().hyperparameters['num_conversion']['default'] = value\n\n\ndef set_merge_spaces(value: bool):\n    Parameters.getInstance().hyperparameters['merge_spaces']['default'] = value\n\n\ndef set_strip(value: bool):\n    Parameters.getInstance().hyperparameters['strip']['default'] = value\n\n\ndef set_remove_punctuation(value: bool):\n    Parameters.getInstance().hyperparameters['remove_punctuation']['default'] = value\n\n\ndef set_remove_stopwords(value: bool):\n    Parameters.getInstance().hyperparameters['remove_stopwords']['default'] = value\n\n\ndef set_remove_specific_pos(value: bool):\n    Parameters.getInstance().hyperparameters['remove_specific_pos']['default'] = value\n\n\ndef set_lemmatize(value: bool):\n    Parameters.getInstance().hyperparameters['lemmatize']['default'] = value\n\n\ndef set_min_num_sentences(value: int):\n    Parameters.getInstance().hyperparameters['min_num_sent']['default'] = value\n\n\ndef set_delta_start(value: int):\n    Parameters.getInstance().hyperparameters['delta_start']['default'] = value\n\n\ndef get_chunk_len() -> str:\n    lens = []\n    mask = Parameters.getInstance().hyperparameters['chunk_len_mask']['default']\n\n    lens.append(Parameters.getInstance().hyperparameters['chunk_len1']['default'] if mask & (1 << 0) else None)\n    lens.append(Parameters.getInstance().hyperparameters['chunk_len2']['default'] if mask & (1 << 1) else None)\n    lens.append(Parameters.getInstance().hyperparameters['chunk_len3']['default'] if mask & (1 << 2) else None)\n    lens.append(Parameters.getInstance().hyperparameters['chunk_len4']['default'] if mask & (1 << 3) else None)\n\n    return ','.join([str(len) for len in lens if len])\n\n\ndef set_chunk_len(val: str):\n    chunk_lens = sorted([int(len.strip()) for len in val.split(',')])\n\n    # Reset the mask to zero\n    Parameters.getInstance().hyperparameters['chunk_len_mask']['default'] = 0\n\n    if len(chunk_lens) > 0:\n        Parameters.getInstance().hyperparameters['chunk_len1']['default'] = chunk_lens[0]\n        Parameters.getInstance().hyperparameters['chunk_len_mask']['default'] |= (1 << 0)\n    if len(chunk_lens) > 1:\n        Parameters.getInstance().hyperparameters['chunk_len2']['default'] = chunk_lens[1]\n        Parameters.getInstance().hyperparameters['chunk_len_mask']['default'] |= (1 << 1)\n    if len(chunk_lens) > 2:\n        Parameters.getInstance().hyperparameters['chunk_len3']['default'] = chunk_lens[2]\n        Parameters.getInstance().hyperparameters['chunk_len_mask']['default'] |= (1 << 2)\n    if len(chunk_lens) > 3:\n        Parameters.getInstance().hyperparameters['chunk_len4']['default'] = chunk_lens[3]\n        Parameters.getInstance().hyperparameters['chunk_len_mask']['default'] |= (1 << 3)\n\n    if len(chunk_lens) > 4:\n        logger.warning(f'Only up to four chunk lengths are supported. Skipping {chunk_lens[4:]}')\n\n\ndef get_context_len() -> str:\n    context_len = str(Parameters.getInstance().hyperparameters['context_len_left']['default']) + ',' + str(Parameters.getInstance().hyperparameters['context_len_right']['default'])\n    return context_len\n\n\ndef set_context_len(val: str):\n    context_lens = [int(len.strip()) for len in val.split(',') if len.isdigit()]\n    if len(context_lens) == 1:\n        Parameters.getInstance().hyperparameters['context_len_left']['default'] = Parameters.getInstance().hyperparameters['context_len_right']['default'] = context_lens[0]\n    elif len(context_lens) == 2:\n        Parameters.getInstance().hyperparameters['context_len_left']['default'] = context_lens[0]\n        Parameters.getInstance().hyperparameters['context_len_right']['default'] = context_lens[1]\n    else:\n        logger.warning(f'Incorrect context length received {val}. Skipping.')\n\n\ndef get_new_dist_strategy() -> str:\n    return Parameters.getInstance().hyperparameters['new_dist_strategy']['default']\n\n\ndef get_chunk_count() -> int:\n    return int(Parameters.getInstance().hyperparameters['chunk_count']['default'])\n\n\ndef get_min_num_length() -> int:\n    return int(Parameters.getInstance().hyperparameters['min_num_length']['default'])\n\n\ndef get_significant_level() -> float:\n    return float(Parameters.getInstance().hyperparameters['significant_level']['default'])\n\n\ndef get_time_steepness() -> float:\n    return float(Parameters.getInstance().hyperparameters['time_steepness']['default'])\n\n\ndef get_time_power() -> float:\n    return float(Parameters.getInstance().hyperparameters['time_power']['default'])\n\n\ndef get_chunk_separator() -> str:\n    return Parameters.getInstance().hyperparameters['chunk_separator']['default']\n\n\ndef get_prefix() -> str:\n    return Parameters.getInstance().hyperparameters['prefix']['default']\n\n\ndef get_data_separator() -> str:\n    return Parameters.getInstance().hyperparameters['data_separator']['default']\n\n\ndef get_postfix() -> str:\n    return Parameters.getInstance().hyperparameters['postfix']['default']\n\n\ndef get_is_manual() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['manual']['default'])\n\n\ndef get_add_chat_to_data() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['add_chat_to_data']['default'])\n\n\ndef get_injection_strategy() -> str:\n    return Parameters.getInstance().hyperparameters['injection_strategy']['default']\n\n\ndef get_chunk_regex() -> str:\n    return Parameters.getInstance().hyperparameters['chunk_regex']['default']\n\n\ndef get_is_strong_cleanup() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['strong_cleanup']['default'])\n\n\ndef get_max_token_count() -> int:\n    return int(Parameters.getInstance().hyperparameters['max_token_count']['default'])\n\n\ndef get_num_threads() -> int:\n    return int(Parameters.getInstance().hyperparameters['threads']['default'])\n\n\ndef get_optimization_steps() -> int:\n    return int(Parameters.getInstance().hyperparameters['optimization_steps']['default'])\n\n\ndef get_api_port() -> int:\n    return int(Parameters.getInstance().hyperparameters['api_port']['default'])\n\n\ndef get_api_on() -> bool:\n    return bool(Parameters.getInstance().hyperparameters['api_on']['default'])\n\n\ndef set_new_dist_strategy(value: str):\n    Parameters.getInstance().hyperparameters['new_dist_strategy']['default'] = value\n\n\ndef set_chunk_count(value: int):\n    Parameters.getInstance().hyperparameters['chunk_count']['default'] = value\n\n\ndef set_min_num_length(value: int):\n    Parameters.getInstance().hyperparameters['min_num_length']['default'] = value\n\n\ndef set_significant_level(value: float):\n    Parameters.getInstance().hyperparameters['significant_level']['default'] = value\n\n\ndef set_time_steepness(value: float):\n    Parameters.getInstance().hyperparameters['time_steepness']['default'] = value\n\n\ndef set_time_power(value: float):\n    Parameters.getInstance().hyperparameters['time_power']['default'] = value\n\n\ndef set_chunk_separator(value: str):\n    Parameters.getInstance().hyperparameters['chunk_separator']['default'] = value\n\n\ndef set_prefix(value: str):\n    Parameters.getInstance().hyperparameters['prefix']['default'] = value\n\n\ndef set_data_separator(value: str):\n    Parameters.getInstance().hyperparameters['data_separator']['default'] = value\n\n\ndef set_postfix(value: str):\n    Parameters.getInstance().hyperparameters['postfix']['default'] = value\n\n\ndef set_manual(value: bool):\n    Parameters.getInstance().hyperparameters['manual']['default'] = value\n\n\ndef set_add_chat_to_data(value: bool):\n    Parameters.getInstance().hyperparameters['add_chat_to_data']['default'] = value\n\n\ndef set_injection_strategy(value: str):\n    Parameters.getInstance().hyperparameters['injection_strategy']['default'] = value\n\n\ndef set_chunk_regex(value: str):\n    Parameters.getInstance().hyperparameters['chunk_regex']['default'] = value\n\n\ndef set_strong_cleanup(value: bool):\n    Parameters.getInstance().hyperparameters['strong_cleanup']['default'] = value\n\n\ndef set_max_token_count(value: int):\n    Parameters.getInstance().hyperparameters['max_token_count']['default'] = value\n\n\ndef set_num_threads(value: int):\n    Parameters.getInstance().hyperparameters['threads']['default'] = value\n\n\ndef set_optimization_steps(value: int):\n    Parameters.getInstance().hyperparameters['optimization_steps']['default'] = value\n\n\ndef set_api_port(value: int):\n    Parameters.getInstance().hyperparameters['api_port']['default'] = value\n\n\ndef set_api_on(value: bool):\n    Parameters.getInstance().hyperparameters['api_on']['default'] = value\n", "extensions/superboogav2/download_urls.py": "import concurrent.futures\nimport re\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nimport extensions.superboogav2.parameters as parameters\n\nfrom .data_processor import process_and_add_to_collector\nfrom .utils import create_metadata_source\n\n\ndef _download_single(url):\n    response = requests.get(url, timeout=5)\n    if response.status_code == 200:\n        return response.content\n    else:\n        raise Exception(\"Failed to download URL\")\n\n\ndef _download_urls(urls, threads=1):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n        futures = []\n        for url in urls:\n            future = executor.submit(_download_single, url)\n            futures.append(future)\n\n        results = []\n        i = 0\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                result = future.result()\n                results.append(result)\n                i += 1\n                yield f\"{i}/{len(urls)}\", results\n            except Exception:\n                pass\n\n        yield \"Done\", results\n\n\ndef feed_url_into_collector(urls, collector):\n    all_text = ''\n    cumulative = ''\n\n    urls = urls.strip().split('\\n')\n    cumulative += f'Loading {len(urls)} URLs with {parameters.get_num_threads()} threads...\\n\\n'\n    yield cumulative\n    for update, contents in _download_urls(urls, threads=parameters.get_num_threads()):\n        yield cumulative + update\n\n    cumulative += 'Processing the HTML sources...'\n    yield cumulative\n    for content in contents:\n        soup = BeautifulSoup(content, features=\"lxml\")\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        strings = soup.stripped_strings\n        if parameters.get_is_strong_cleanup():\n            strings = [s for s in strings if re.search(\"[A-Za-z] \", s)]\n\n        text = '\\n'.join([s.strip() for s in strings])\n        all_text += text\n\n    process_and_add_to_collector(all_text, collector, False, create_metadata_source('url-download'))\n", "extensions/superboogav2/script.py": "\"\"\"\nThis file is responsible for the UI and how the application interracts with the rest of the system.\n\"\"\"\nimport os\nfrom pathlib import Path\n\n# Point to where nltk will find the required data.\nos.environ['NLTK_DATA'] = str(Path(\"extensions/superboogav2/nltk_data\").resolve())\n\nimport codecs\nimport textwrap\n\nimport gradio as gr\n\nimport extensions.superboogav2.parameters as parameters\nfrom modules import shared\nfrom modules.logging_colors import logger\n\nfrom .api import APIManager\nfrom .benchmark import benchmark\nfrom .chat_handler import custom_generate_chat_prompt_internal\nfrom .chromadb import make_collector\nfrom .data_processor import process_and_add_to_collector\nfrom .download_urls import feed_url_into_collector\nfrom .notebook_handler import input_modifier_internal\nfrom .optimize import optimize\nfrom .utils import create_metadata_source\n\ncollector = None\napi_manager = None\n\n\ndef setup():\n    global collector\n    global api_manager\n    collector = make_collector()\n    api_manager = APIManager(collector)\n\n    if parameters.get_api_on():\n        api_manager.start_server(parameters.get_api_port())\n\n\ndef _feed_data_into_collector(corpus):\n    yield '### Processing data...'\n    process_and_add_to_collector(corpus, collector, False, create_metadata_source('direct-text'))\n    yield '### Done.'\n\n\ndef _feed_file_into_collector(file):\n    yield '### Reading and processing the input dataset...'\n    text = file.decode('utf-8')\n    process_and_add_to_collector(text, collector, False, create_metadata_source('file'))\n    yield '### Done.'\n\n\ndef _feed_url_into_collector(urls):\n    for i in feed_url_into_collector(urls, collector):\n        yield i\n    yield '### Done.'\n\n\ndef _begin_benchmark():\n    score, max_score = benchmark(Path(\"extensions/superboogav2/benchmark_texts/questions.json\"), collector)\n    return f'**Score**: {score}/{max_score}'\n\n\ndef _begin_optimization(progress=gr.Progress()):\n    return optimize(collector, progress), *_get_optimizable_settings()\n\n\ndef _clear_data():\n    collector.clear()\n    return \"### Data Cleared!\"\n\n\ndef _get_optimizable_settings() -> list:\n    preprocess_pipeline = []\n    if parameters.should_to_lower():\n        preprocess_pipeline.append('Lower Cases')\n    if parameters.should_remove_punctuation():\n        preprocess_pipeline.append('Remove Punctuation')\n    if parameters.should_remove_specific_pos():\n        preprocess_pipeline.append('Remove Adverbs')\n    if parameters.should_remove_stopwords():\n        preprocess_pipeline.append('Remove Stop Words')\n    if parameters.should_lemmatize():\n        preprocess_pipeline.append('Lemmatize')\n    if parameters.should_merge_spaces():\n        preprocess_pipeline.append('Merge Spaces')\n    if parameters.should_strip():\n        preprocess_pipeline.append('Strip Edges')\n\n    return [\n        parameters.get_time_power(),\n        parameters.get_time_steepness(),\n        parameters.get_significant_level(),\n        parameters.get_min_num_sentences(),\n        parameters.get_new_dist_strategy(),\n        parameters.get_delta_start(),\n        parameters.get_min_num_length(),\n        parameters.get_num_conversion_strategy(),\n        preprocess_pipeline,\n        parameters.get_chunk_count(),\n        parameters.get_context_len(),\n        parameters.get_chunk_len()\n    ]\n\n\ndef _apply_settings(optimization_steps, time_power, time_steepness, significant_level, min_sentences, new_dist_strat, delta_start, min_number_length, num_conversion,\n                    preprocess_pipeline, api_port, api_on, injection_strategy, add_chat_to_data, manual, postfix, data_separator, prefix, max_token_count,\n                    chunk_count, chunk_sep, context_len, chunk_regex, chunk_len, threads, strong_cleanup):\n    logger.debug('Applying settings.')\n\n    try:\n        parameters.set_optimization_steps(optimization_steps)\n        parameters.set_significant_level(significant_level)\n        parameters.set_min_num_sentences(min_sentences)\n        parameters.set_new_dist_strategy(new_dist_strat)\n        parameters.set_delta_start(delta_start)\n        parameters.set_min_num_length(min_number_length)\n        parameters.set_num_conversion_strategy(num_conversion)\n        parameters.set_api_port(api_port)\n        parameters.set_api_on(api_on)\n        parameters.set_injection_strategy(injection_strategy)\n        parameters.set_add_chat_to_data(add_chat_to_data)\n        parameters.set_manual(manual)\n        parameters.set_postfix(codecs.decode(postfix, 'unicode_escape'))\n        parameters.set_data_separator(codecs.decode(data_separator, 'unicode_escape'))\n        parameters.set_prefix(codecs.decode(prefix, 'unicode_escape'))\n        parameters.set_max_token_count(max_token_count)\n        parameters.set_time_power(time_power)\n        parameters.set_time_steepness(time_steepness)\n        parameters.set_chunk_count(chunk_count)\n        parameters.set_chunk_separator(codecs.decode(chunk_sep, 'unicode_escape'))\n        parameters.set_context_len(context_len)\n        parameters.set_chunk_regex(chunk_regex)\n        parameters.set_chunk_len(chunk_len)\n        parameters.set_num_threads(threads)\n        parameters.set_strong_cleanup(strong_cleanup)\n\n        preprocess_choices = ['Lower Cases', 'Remove Punctuation', 'Remove Adverbs', 'Remove Stop Words', 'Lemmatize', 'Merge Spaces', 'Strip Edges']\n        for preprocess_method in preprocess_choices:\n            if preprocess_method == 'Lower Cases':\n                parameters.set_to_lower(preprocess_method in preprocess_pipeline)\n            elif preprocess_method == 'Remove Punctuation':\n                parameters.set_remove_punctuation(preprocess_method in preprocess_pipeline)\n            elif preprocess_method == 'Remove Adverbs':\n                parameters.set_remove_specific_pos(preprocess_method in preprocess_pipeline)\n            elif preprocess_method == 'Remove Stop Words':\n                parameters.set_remove_stopwords(preprocess_method in preprocess_pipeline)\n            elif preprocess_method == 'Lemmatize':\n                parameters.set_lemmatize(preprocess_method in preprocess_pipeline)\n            elif preprocess_method == 'Merge Spaces':\n                parameters.set_merge_spaces(preprocess_method in preprocess_pipeline)\n            elif preprocess_method == 'Strip Edges':\n                parameters.set_strip(preprocess_method in preprocess_pipeline)\n\n        # Based on API on/off, start or stop the server\n        if api_manager is not None:\n            if parameters.get_api_on() and (not api_manager.is_server_running()):\n                api_manager.start_server(parameters.get_api_port())\n            elif (not parameters.get_api_on()) and api_manager.is_server_running():\n                api_manager.stop_server()\n    except Exception as e:\n        logger.warn(f'Could not properly apply settings: {str(e)}')\n\n\ndef custom_generate_chat_prompt(user_input, state, **kwargs):\n    return custom_generate_chat_prompt_internal(user_input, state, collector, **kwargs)\n\n\ndef input_modifier(string, state, is_chat=False):\n    return input_modifier_internal(string, collector, is_chat)\n\n\ndef ui():\n    with gr.Accordion(\"Click for more information...\", open=False):\n        gr.Markdown(textwrap.dedent(\"\"\"\n\n        ## About\n\n        This extension takes a dataset as input, breaks it into chunks, and adds the result to a local/offline Chroma database.\n\n        The database is then queried during inference time to get the excerpts that are closest to your input. The idea is to create an arbitrarily large pseudo context.\n\n        The core methodology was developed and contributed by kaiokendev, who is working on improvements to the method in this repository: https://github.com/kaiokendev/superbig\n\n        ## Data input\n\n        Start by entering some data in the interface below and then clicking on \"Load data\".\n\n        Each time you load some new data, the old chunks are discarded.\n\n        ## Chat mode\n\n        #### Instruct\n\n        On each turn, the chunks will be compared to your current input and the most relevant matches will be appended to the input in the following format:\n\n        ```\n        Consider the excerpts below as additional context:\n        ...\n        ```\n\n        The injection doesn't make it into the chat history. It is only used in the current generation.\n\n        #### Regular chat\n\n        The chunks from the external data sources are ignored, and the chroma database is built based on the chat history instead. The most relevant past exchanges relative to the present input are added to the context string. This way, the extension acts as a long term memory.\n\n        ## Notebook/default modes\n\n        Your question must be manually specified between `<|begin-user-input|>` and `<|end-user-input|>` tags, and the injection point must be specified with `<|injection-point|>`.\n\n        The special tokens mentioned above (`<|begin-user-input|>`, `<|end-user-input|>`, and `<|injection-point|>`) are removed in the background before the text generation begins.\n\n        Here is an example in Vicuna 1.1 format:\n\n        ```\n        A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\n        USER:\n        <|injection-point|>\n\n        <|begin-user-input|>What datasets are mentioned in the text above?<|end-user-input|>\n        ASSISTANT:\n        ```\n        \"\"\"))\n\n    with gr.Row():\n        with gr.Column(min_width=600):\n            with gr.Tab(\"Text input\"):\n                data_input = gr.Textbox(lines=20, label='Input data')\n                update_data = gr.Button('Load data')\n\n            with gr.Tab(\"URL input\"):\n                url_input = gr.Textbox(lines=10, label='Input URLs', info='Enter one or more URLs separated by newline characters.')\n                strong_cleanup = gr.Checkbox(value=parameters.get_is_strong_cleanup(), label='Strong cleanup', info='Only keeps html elements that look like long-form text.')\n                threads = gr.Number(value=parameters.get_num_threads(), label='Threads', info='The number of threads to use while downloading the URLs.', precision=0)\n                update_url = gr.Button('Load data')\n\n            with gr.Tab(\"File input\"):\n                file_input = gr.File(label='Input file', type='binary')\n                update_file = gr.Button('Load data')\n\n            with gr.Tab(\"Settings\"):\n                with gr.Accordion(\"Processing settings\", open=True):\n                    chunk_len = gr.Textbox(value=parameters.get_chunk_len(), label='Chunk length', info='In characters, not tokens. This value is used when you click on \"Load data\".')\n                    chunk_regex = gr.Textbox(value=parameters.get_chunk_regex(), label='Chunk regex', info='Will specifically add the captured text to the embeddings.')\n                    context_len = gr.Textbox(value=parameters.get_context_len(), label='Context length', info='In characters, not tokens. How much context to load around each chunk.')\n                    chunk_sep = gr.Textbox(value=codecs.encode(parameters.get_chunk_separator(), 'unicode_escape').decode(), label='Chunk separator', info='Used to manually split chunks. Manually split chunks longer than chunk length are split again. This value is used when you click on \"Load data\".')\n\n                with gr.Accordion(\"Generation settings\", open=False):\n                    chunk_count = gr.Number(value=parameters.get_chunk_count(), label='Chunk count', info='The number of closest-matching chunks to include in the prompt.')\n                    max_token_count = gr.Number(value=parameters.get_max_token_count(), label='Max Context Tokens', info='The context length in tokens will not exceed this value.')\n                    prefix = gr.Textbox(value=codecs.encode(parameters.get_prefix(), 'unicode_escape').decode(), label='Prefix', info='What to put before the injection point.')\n                    data_separator = gr.Textbox(value=codecs.encode(parameters.get_data_separator(), 'unicode_escape').decode(), label='Data separator', info='When multiple pieces of distant data are added, they might be unrelated. It\\'s important to separate them.')\n                    postfix = gr.Textbox(value=codecs.encode(parameters.get_postfix(), 'unicode_escape').decode(), label='Postfix', info='What to put after the injection point.')\n                    with gr.Row():\n                        manual = gr.Checkbox(value=parameters.get_is_manual(), label=\"Is Manual\", info=\"Manually specify when to use ChromaDB. Insert `!c` at the start or end of the message to trigger a query.\", visible=shared.is_chat())\n                        add_chat_to_data = gr.Checkbox(value=parameters.get_add_chat_to_data(), label=\"Add Chat to Data\", info=\"Automatically feed the chat history as you chat.\", visible=shared.is_chat())\n                    injection_strategy = gr.Radio(choices=[parameters.PREPEND_TO_LAST, parameters.APPEND_TO_LAST, parameters.HIJACK_LAST_IN_CONTEXT], value=parameters.get_injection_strategy(), label='Injection Strategy', info='Where to inject the messages in chat or instruct mode.', visible=shared.is_chat())\n                    with gr.Row():\n                        api_on = gr.Checkbox(value=parameters.get_api_on(), label=\"Turn on API\", info=\"Check this to turn on the API service.\")\n                        api_port = gr.Number(value=parameters.get_api_port(), label=\"API Port\", info=\"The port on which the API service will run.\")\n\n                with gr.Accordion(\"Advanced settings\", open=False):\n                    preprocess_set_choices = []\n                    if parameters.should_to_lower():\n                        preprocess_set_choices.append('Lower Cases')\n                    if parameters.should_remove_punctuation():\n                        preprocess_set_choices.append('Remove Punctuation')\n                    if parameters.should_remove_specific_pos():\n                        preprocess_set_choices.append('Remove Adverbs')\n                    if parameters.should_remove_stopwords():\n                        preprocess_set_choices.append('Remove Stop Words')\n                    if parameters.should_lemmatize():\n                        preprocess_set_choices.append('Lemmatize')\n                    if parameters.should_merge_spaces():\n                        preprocess_set_choices.append('Merge Spaces')\n                    if parameters.should_strip():\n                        preprocess_set_choices.append('Strip Edges')\n\n                    preprocess_pipeline = gr.CheckboxGroup(label='Preprocessing pipeline', choices=[\n                        'Lower Cases',\n                        'Remove Punctuation',\n                        'Remove Adverbs',\n                        'Remove Stop Words',\n                        'Lemmatize',\n                        'Merge Spaces',\n                        'Strip Edges',\n                    ], value=preprocess_set_choices, interactive=True, info='How to preprocess the text before it is turned into an embedding.')\n\n                    with gr.Row():\n                        num_conversion = gr.Dropdown(choices=[parameters.NUM_TO_WORD_METHOD, parameters.NUM_TO_CHAR_METHOD, parameters.NUM_TO_CHAR_LONG_METHOD, 'None'], value=parameters.get_num_conversion_strategy(), label=\"Number Conversion Method\", info='How to preprocess numbers before creating the embeddings.', interactive=True)\n                        min_number_length = gr.Number(value=parameters.get_min_num_length(), label='Number Length Threshold', info='In digits. Only numbers that have at least that many digits will be converted.', interactive=True)\n\n                    delta_start = gr.Number(value=parameters.get_delta_start(), label='Delta Start Index', info='If the system encounters two identical embeddings, and they both start within the same delta, then only the first will be considered.', interactive=True)\n                    new_dist_strat = gr.Dropdown(choices=[parameters.DIST_MIN_STRATEGY, parameters.DIST_HARMONIC_STRATEGY, parameters.DIST_GEOMETRIC_STRATEGY, parameters.DIST_ARITHMETIC_STRATEGY], value=parameters.get_new_dist_strategy(), label=\"Distance Strategy\", info='When two embedding texts are merged, the distance of the new piece will be decided using one of these strategies.', interactive=True)\n                    min_sentences = gr.Number(value=parameters.get_min_num_sentences(), label='Summary Threshold', info='In sentences. The minumum number of sentences to trigger text-rank summarization.', interactive=True)\n                    significant_level = gr.Slider(0.8, 2, value=parameters.get_significant_level(), label='Significant Level', info='Defines the cut-off for what is considered a \"significant\" distance relative to the median distance among the returned samples.', interactive=True)\n                    time_steepness = gr.Slider(0.01, 1.0, value=parameters.get_time_steepness(), label='Time Weighing Steepness', info='How differently two close excerpts are going to be weighed.')\n                    time_power = gr.Slider(0.0, 1.0, value=parameters.get_time_power(), label='Time Weighing Power', info='How influencial is the weighing. At 1.0, old entries won\\'t be considered')\n\n            with gr.Tab(\"Benchmark\"):\n                benchmark_button = gr.Button('Benchmark')\n                optimize_button = gr.Button('Optimize')\n                optimization_steps = gr.Number(value=parameters.get_optimization_steps(), label='Optimization Steps', info='For how many steps to optimize.', interactive=True)\n\n            clear_button = gr.Button('\u274c Clear Data')\n\n        with gr.Column():\n            last_updated = gr.Markdown()\n\n    all_params = [optimization_steps, time_power, time_steepness, significant_level, min_sentences, new_dist_strat, delta_start, min_number_length, num_conversion,\n                  preprocess_pipeline, api_port, api_on, injection_strategy, add_chat_to_data, manual, postfix, data_separator, prefix, max_token_count,\n                  chunk_count, chunk_sep, context_len, chunk_regex, chunk_len, threads, strong_cleanup]\n    optimizable_params = [time_power, time_steepness, significant_level, min_sentences, new_dist_strat, delta_start, min_number_length, num_conversion,\n                          preprocess_pipeline, chunk_count, context_len, chunk_len]\n\n    update_data.click(_feed_data_into_collector, [data_input], last_updated, show_progress=False)\n    update_url.click(_feed_url_into_collector, [url_input], last_updated, show_progress=False)\n    update_file.click(_feed_file_into_collector, [file_input], last_updated, show_progress=False)\n    benchmark_button.click(_begin_benchmark, [], last_updated, show_progress=True)\n    optimize_button.click(_begin_optimization, [], [last_updated] + optimizable_params, show_progress=True)\n    clear_button.click(_clear_data, [], last_updated, show_progress=False)\n\n    optimization_steps.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    time_power.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    time_steepness.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    significant_level.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    min_sentences.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    new_dist_strat.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    delta_start.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    min_number_length.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    num_conversion.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    preprocess_pipeline.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    api_port.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    api_on.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    injection_strategy.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    add_chat_to_data.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    manual.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    postfix.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    data_separator.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    prefix.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    max_token_count.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    chunk_count.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    chunk_sep.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    context_len.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    chunk_regex.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    chunk_len.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    threads.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n    strong_cleanup.input(fn=_apply_settings, inputs=all_params, show_progress=False)\n", "extensions/openai/tokens.py": "from modules.text_generation import decode, encode\n\n\ndef token_count(prompt):\n    tokens = encode(prompt)[0]\n    return {\n        'length': len(tokens)\n    }\n\n\ndef token_encode(input):\n    tokens = encode(input)[0]\n    if tokens.__class__.__name__ in ['Tensor', 'ndarray']:\n        tokens = tokens.tolist()\n\n    return {\n        'tokens': tokens,\n        'length': len(tokens),\n    }\n\n\ndef token_decode(tokens):\n    output = decode(tokens)\n    return {\n        'text': output\n    }\n", "extensions/openai/logits.py": "from extensions.openai.completions import process_parameters\nfrom modules.logits import get_next_logits\n\n\ndef _get_next_logits(body):\n    # Pre-process the input payload to simulate a real generation\n    use_samplers = body['use_samplers']\n    state = process_parameters(body) if use_samplers else {}\n    state['stream'] = True\n\n    return get_next_logits(body['prompt'], state, use_samplers, \"\", top_logits=body['top_logits'], return_dict=True)\n", "extensions/openai/cache_embedding_model.py": "#!/usr/bin/env python3\n# preload the embedding model, useful for Docker images to prevent re-download on config change\n# Dockerfile:\n# ENV OPENEDAI_EMBEDDING_MODEL=\"sentence-transformers/all-mpnet-base-v2\"  # Optional\n# RUN python3 cache_embedded_model.py\nimport os\n\nimport sentence_transformers\n\nst_model = os.environ.get(\"OPENEDAI_EMBEDDING_MODEL\", \"sentence-transformers/all-mpnet-base-v2\")\nmodel = sentence_transformers.SentenceTransformer(st_model)\n", "extensions/openai/typing.py": "import json\nimport time\nfrom typing import Dict, List\n\nfrom pydantic import BaseModel, Field\n\n\nclass GenerationOptions(BaseModel):\n    preset: str | None = Field(default=None, description=\"The name of a file under text-generation-webui/presets (without the .yaml extension). The sampling parameters that get overwritten by this option are the keys in the default_preset() function in modules/presets.py.\")\n    min_p: float = 0\n    dynamic_temperature: bool = False\n    dynatemp_low: float = 1\n    dynatemp_high: float = 1\n    dynatemp_exponent: float = 1\n    smoothing_factor: float = 0\n    smoothing_curve: float = 1\n    top_k: int = 0\n    repetition_penalty: float = 1\n    repetition_penalty_range: int = 1024\n    typical_p: float = 1\n    tfs: float = 1\n    top_a: float = 0\n    epsilon_cutoff: float = 0\n    eta_cutoff: float = 0\n    guidance_scale: float = 1\n    negative_prompt: str = ''\n    penalty_alpha: float = 0\n    mirostat_mode: int = 0\n    mirostat_tau: float = 5\n    mirostat_eta: float = 0.1\n    temperature_last: bool = False\n    do_sample: bool = True\n    seed: int = -1\n    encoder_repetition_penalty: float = 1\n    no_repeat_ngram_size: int = 0\n    dry_multiplier: float = 0\n    dry_base: float = 1.75\n    dry_allowed_length: int = 2\n    dry_sequence_breakers: str = '\"\\\\n\", \":\", \"\\\\\"\", \"*\"'\n    truncation_length: int = 0\n    max_tokens_second: int = 0\n    prompt_lookup_num_tokens: int = 0\n    custom_token_bans: str = \"\"\n    sampler_priority: List[str] | str | None = Field(default=None, description=\"List of samplers where the first items will appear first in the stack. Example: [\\\"top_k\\\", \\\"temperature\\\", \\\"top_p\\\"].\")\n    auto_max_new_tokens: bool = False\n    ban_eos_token: bool = False\n    add_bos_token: bool = True\n    skip_special_tokens: bool = True\n    grammar_string: str = \"\"\n\n\nclass CompletionRequestParams(BaseModel):\n    model: str | None = Field(default=None, description=\"Unused parameter. To change the model, use the /v1/internal/model/load endpoint.\")\n    prompt: str | List[str]\n    best_of: int | None = Field(default=1, description=\"Unused parameter.\")\n    echo: bool | None = False\n    frequency_penalty: float | None = 0\n    logit_bias: dict | None = None\n    logprobs: int | None = None\n    max_tokens: int | None = 16\n    n: int | None = Field(default=1, description=\"Unused parameter.\")\n    presence_penalty: float | None = 0\n    stop: str | List[str] | None = None\n    stream: bool | None = False\n    suffix: str | None = None\n    temperature: float | None = 1\n    top_p: float | None = 1\n    user: str | None = Field(default=None, description=\"Unused parameter.\")\n\n\nclass CompletionRequest(GenerationOptions, CompletionRequestParams):\n    pass\n\n\nclass CompletionResponse(BaseModel):\n    id: str\n    choices: List[dict]\n    created: int = int(time.time())\n    model: str\n    object: str = \"text_completion\"\n    usage: dict\n\n\nclass ChatCompletionRequestParams(BaseModel):\n    messages: List[dict]\n    model: str | None = Field(default=None, description=\"Unused parameter. To change the model, use the /v1/internal/model/load endpoint.\")\n    frequency_penalty: float | None = 0\n    function_call: str | dict | None = Field(default=None, description=\"Unused parameter.\")\n    functions: List[dict] | None = Field(default=None, description=\"Unused parameter.\")\n    logit_bias: dict | None = None\n    max_tokens: int | None = None\n    n: int | None = Field(default=1, description=\"Unused parameter.\")\n    presence_penalty: float | None = 0\n    stop: str | List[str] | None = None\n    stream: bool | None = False\n    temperature: float | None = 1\n    top_p: float | None = 1\n    user: str | None = Field(default=None, description=\"Unused parameter.\")\n\n    mode: str = Field(default='instruct', description=\"Valid options: instruct, chat, chat-instruct.\")\n\n    instruction_template: str | None = Field(default=None, description=\"An instruction template defined under text-generation-webui/instruction-templates. If not set, the correct template will be automatically obtained from the model metadata.\")\n    instruction_template_str: str | None = Field(default=None, description=\"A Jinja2 instruction template. If set, will take precedence over everything else.\")\n\n    character: str | None = Field(default=None, description=\"A character defined under text-generation-webui/characters. If not set, the default \\\"Assistant\\\" character will be used.\")\n    bot_name: str | None = Field(default=None, description=\"Overwrites the value set by character field.\", alias=\"name2\")\n    context: str | None = Field(default=None, description=\"Overwrites the value set by character field.\")\n    greeting: str | None = Field(default=None, description=\"Overwrites the value set by character field.\")\n    user_name: str | None = Field(default=None, description=\"Your name (the user). By default, it's \\\"You\\\".\", alias=\"name1\")\n    user_bio: str | None = Field(default=None, description=\"The user description/personality.\")\n    chat_template_str: str | None = Field(default=None, description=\"Jinja2 template for chat.\")\n\n    chat_instruct_command: str | None = None\n\n    continue_: bool = Field(default=False, description=\"Makes the last bot message in the history be continued instead of starting a new message.\")\n\n\nclass ChatCompletionRequest(GenerationOptions, ChatCompletionRequestParams):\n    pass\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str\n    choices: List[dict]\n    created: int = int(time.time())\n    model: str\n    object: str = \"chat.completion\"\n    usage: dict\n\n\nclass ChatPromptResponse(BaseModel):\n    prompt: str\n\n\nclass EmbeddingsRequest(BaseModel):\n    input: str | List[str] | List[int] | List[List[int]]\n    model: str | None = Field(default=None, description=\"Unused parameter. To change the model, set the OPENEDAI_EMBEDDING_MODEL and OPENEDAI_EMBEDDING_DEVICE environment variables before starting the server.\")\n    encoding_format: str = Field(default=\"float\", description=\"Can be float or base64.\")\n    user: str | None = Field(default=None, description=\"Unused parameter.\")\n\n\nclass EmbeddingsResponse(BaseModel):\n    index: int\n    embedding: List[float]\n    object: str = \"embedding\"\n\n\nclass EncodeRequest(BaseModel):\n    text: str\n\n\nclass EncodeResponse(BaseModel):\n    tokens: List[int]\n    length: int\n\n\nclass DecodeRequest(BaseModel):\n    tokens: List[int]\n\n\nclass DecodeResponse(BaseModel):\n    text: str\n\n\nclass TokenCountResponse(BaseModel):\n    length: int\n\n\nclass LogitsRequestParams(BaseModel):\n    prompt: str\n    use_samplers: bool = False\n    top_logits: int | None = 50\n    frequency_penalty: float | None = 0\n    max_tokens: int | None = 16\n    presence_penalty: float | None = 0\n    temperature: float | None = 1\n    top_p: float | None = 1\n\n\nclass LogitsRequest(GenerationOptions, LogitsRequestParams):\n    pass\n\n\nclass LogitsResponse(BaseModel):\n    logits: Dict[str, float]\n\n\nclass ModelInfoResponse(BaseModel):\n    model_name: str\n    lora_names: List[str]\n\n\nclass ModelListResponse(BaseModel):\n    model_names: List[str]\n\n\nclass LoadModelRequest(BaseModel):\n    model_name: str\n    args: dict | None = None\n    settings: dict | None = None\n\n\nclass LoraListResponse(BaseModel):\n    lora_names: List[str]\n\n\nclass LoadLorasRequest(BaseModel):\n    lora_names: List[str]\n\n\ndef to_json(obj):\n    return json.dumps(obj.__dict__, indent=4)\n\n\ndef to_dict(obj):\n    return obj.__dict__\n", "extensions/openai/utils.py": "import base64\nimport os\nimport time\nimport traceback\nfrom typing import Callable, Optional\n\nimport numpy as np\n\n\ndef float_list_to_base64(float_array: np.ndarray) -> str:\n    # Convert the list to a float32 array that the OpenAPI client expects\n    # float_array = np.array(float_list, dtype=\"float32\")\n\n    # Get raw bytes\n    bytes_array = float_array.tobytes()\n\n    # Encode bytes into base64\n    encoded_bytes = base64.b64encode(bytes_array)\n\n    # Turn raw base64 encoded bytes into ASCII\n    ascii_string = encoded_bytes.decode('ascii')\n    return ascii_string\n\n\ndef debug_msg(*args, **kwargs):\n    from extensions.openai.script import params\n    if os.environ.get(\"OPENEDAI_DEBUG\", params.get('debug', 0)):\n        print(*args, **kwargs)\n\n\ndef _start_cloudflared(port: int, tunnel_id: str, max_attempts: int = 3, on_start: Optional[Callable[[str], None]] = None):\n    try:\n        from flask_cloudflared import _run_cloudflared\n    except ImportError:\n        print('You should install flask_cloudflared manually')\n        raise Exception(\n            'flask_cloudflared not installed. Make sure you installed the requirements.txt for this extension.')\n\n    for _ in range(max_attempts):\n        try:\n            if tunnel_id is not None:\n                public_url = _run_cloudflared(port, port + 1, tunnel_id=tunnel_id)\n            else:\n                public_url = _run_cloudflared(port, port + 1)\n\n            if on_start:\n                on_start(public_url)\n\n            return\n        except Exception:\n            traceback.print_exc()\n            time.sleep(3)\n\n        raise Exception('Could not start cloudflared.')\n", "extensions/openai/models.py": "from modules import shared\nfrom modules.logging_colors import logger\nfrom modules.LoRA import add_lora_to_model\nfrom modules.models import load_model, unload_model\nfrom modules.models_settings import get_model_metadata, update_model_parameters\nfrom modules.utils import get_available_loras, get_available_models\n\n\ndef get_current_model_info():\n    return {\n        'model_name': shared.model_name,\n        'lora_names': shared.lora_names,\n        'loader': shared.args.loader\n    }\n\n\ndef list_models():\n    return {'model_names': get_available_models()[1:]}\n\n\ndef list_dummy_models():\n    result = {\n        \"object\": \"list\",\n        \"data\": []\n    }\n\n    # these are expected by so much, so include some here as a dummy\n    for model in ['gpt-3.5-turbo', 'text-embedding-ada-002']:\n        result[\"data\"].append(model_info_dict(model))\n\n    return result\n\n\ndef model_info_dict(model_name: str) -> dict:\n    return {\n        \"id\": model_name,\n        \"object\": \"model\",\n        \"created\": 0,\n        \"owned_by\": \"user\"\n    }\n\n\ndef _load_model(data):\n    model_name = data[\"model_name\"]\n    args = data[\"args\"]\n    settings = data[\"settings\"]\n\n    unload_model()\n    model_settings = get_model_metadata(model_name)\n    update_model_parameters(model_settings)\n\n    # Update shared.args with custom model loading settings\n    if args:\n        for k in args:\n            if hasattr(shared.args, k):\n                setattr(shared.args, k, args[k])\n\n    shared.model, shared.tokenizer = load_model(model_name)\n\n    # Update shared.settings with custom generation defaults\n    if settings:\n        for k in settings:\n            if k in shared.settings:\n                shared.settings[k] = settings[k]\n                if k == 'truncation_length':\n                    logger.info(f\"TRUNCATION LENGTH (UPDATED): {shared.settings['truncation_length']}\")\n                elif k == 'instruction_template':\n                    logger.info(f\"INSTRUCTION TEMPLATE (UPDATED): {shared.settings['instruction_template']}\")\n\n\ndef list_loras():\n    return {'lora_names': get_available_loras()[1:]}\n\n\ndef load_loras(lora_names):\n    add_lora_to_model(lora_names)\n\n\ndef unload_all_loras():\n    add_lora_to_model([])\n", "extensions/openai/errors.py": "class OpenAIError(Exception):\n    def __init__(self, message=None, code=500, internal_message=''):\n        self.message = message\n        self.code = code\n        self.internal_message = internal_message\n\n    def __repr__(self):\n        return \"%s(message=%r, code=%d)\" % (\n            self.__class__.__name__,\n            self.message,\n            self.code,\n        )\n\n\nclass InvalidRequestError(OpenAIError):\n    def __init__(self, message, param, code=400, internal_message=''):\n        super().__init__(message, code, internal_message)\n        self.param = param\n\n    def __repr__(self):\n        return \"%s(message=%r, code=%d, param=%s)\" % (\n            self.__class__.__name__,\n            self.message,\n            self.code,\n            self.param,\n        )\n\n\nclass ServiceUnavailableError(OpenAIError):\n    def __init__(self, message=\"Service unavailable, please try again later.\", code=503, internal_message=''):\n        super().__init__(message, code, internal_message)\n", "extensions/openai/completions.py": "import base64\nimport copy\nimport re\nimport time\nfrom collections import deque\nfrom io import BytesIO\n\nimport requests\nimport tiktoken\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom transformers import LogitsProcessor, LogitsProcessorList\n\nfrom extensions.openai.errors import InvalidRequestError\nfrom extensions.openai.utils import debug_msg\nfrom modules import shared\nfrom modules.chat import (\n    generate_chat_prompt,\n    generate_chat_reply,\n    load_character_memoized,\n    load_instruction_template_memoized\n)\nfrom modules.presets import load_preset_memoized\nfrom modules.text_generation import (\n    decode,\n    encode,\n    generate_reply,\n    get_reply_from_output_ids\n)\n\n\nclass LogitsBiasProcessor(LogitsProcessor):\n    def __init__(self, logit_bias={}):\n        self.logit_bias = logit_bias\n        if self.logit_bias:\n            self.keys = list([int(key) for key in self.logit_bias.keys()])\n            values = [self.logit_bias[str(key)] for key in self.keys]\n            self.values = torch.tensor(values, dtype=torch.float, device=shared.model.device)\n            debug_msg(f\"{self})\")\n\n    def __call__(self, input_ids: torch.LongTensor, logits: torch.FloatTensor) -> torch.FloatTensor:\n        if self.logit_bias:\n            debug_msg(logits[0, self.keys], \" + \", self.values)\n            logits[0, self.keys] += self.values\n            debug_msg(\" --> \", logits[0, self.keys])\n            debug_msg(\" max/min \", float(torch.max(logits[0])), float(torch.min(logits[0])))\n\n        return logits\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__}(logit_bias={self.logit_bias})>\"\n\n\nclass LogprobProcessor(LogitsProcessor):\n    def __init__(self, logprobs=None):\n        self.logprobs = logprobs\n        self.token_alternatives = {}\n\n    def __call__(self, input_ids: torch.LongTensor, logits: torch.FloatTensor) -> torch.FloatTensor:\n        if self.logprobs is not None:  # 0-5\n            log_e_probabilities = F.log_softmax(logits, dim=1)\n            top_values, top_indices = torch.topk(log_e_probabilities, k=self.logprobs + 1)\n            top_tokens = [get_reply_from_output_ids([tok]) for tok in top_indices[0]]\n            top_probs = [float(x) for x in top_values[0]]\n            self.token_alternatives = dict(zip(top_tokens, top_probs))\n            debug_msg(repr(self))\n\n        return logits\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__}(logprobs={self.logprobs}, token_alternatives={self.token_alternatives})>\"\n\n\ndef convert_logprobs_to_tiktoken(model, logprobs):\n    # more problems than it's worth.\n    # try:\n    #     encoder = tiktoken.encoding_for_model(model)\n    #     # just pick the first one if it encodes to multiple tokens... 99.9% not required and maybe worse overall.\n    #     return dict([(encoder.decode([encoder.encode(token)[0]]), prob) for token, prob in logprobs.items()])\n    # except KeyError:\n    #     # assume native tokens if we can't find the tokenizer\n    #     return logprobs\n\n    return logprobs\n\n\ndef process_parameters(body, is_legacy=False):\n    generate_params = body\n    max_tokens_str = 'length' if is_legacy else 'max_tokens'\n    generate_params['max_new_tokens'] = body.pop(max_tokens_str)\n    if generate_params['truncation_length'] == 0:\n        generate_params['truncation_length'] = shared.settings['truncation_length']\n\n    if generate_params['temperature'] == 0:\n        generate_params['do_sample'] = False\n        generate_params['top_k'] = 1\n\n    if body['preset'] is not None:\n        preset = load_preset_memoized(body['preset'])\n        generate_params.update(preset)\n\n    generate_params['custom_stopping_strings'] = []\n    if 'stop' in body:  # str or array, max len 4 (ignored)\n        if isinstance(body['stop'], str):\n            generate_params['custom_stopping_strings'] = [body['stop']]\n        elif isinstance(body['stop'], list):\n            generate_params['custom_stopping_strings'] = body['stop']\n\n    logits_processor = []\n    logit_bias = body.get('logit_bias', None)\n    if logit_bias:  # {str: float, ...}\n        logits_processor = [LogitsBiasProcessor(logit_bias)]\n\n    logprobs = None  # coming to chat eventually\n    if 'logprobs' in body:\n        logprobs = body.get('logprobs', 0)  # maybe cap at topk? don't clamp 0-5.\n        generate_params['logprob_proc'] = LogprobProcessor(logprobs)\n        logits_processor.extend([generate_params['logprob_proc']])\n    else:\n        logprobs = None\n\n    if logits_processor:  # requires logits_processor support\n        generate_params['logits_processor'] = LogitsProcessorList(logits_processor)\n\n    return generate_params\n\n\ndef convert_history(history):\n    '''\n    Chat histories in this program are in the format [message, reply].\n    This function converts OpenAI histories to that format.\n    '''\n    chat_dialogue = []\n    current_message = \"\"\n    current_reply = \"\"\n    user_input = \"\"\n    user_input_last = True\n    system_message = \"\"\n\n    # Multimodal: convert OpenAI format to multimodal extension format\n    if any('content' in entry and isinstance(entry['content'], list) for entry in history):\n        new_history = []\n        for entry in history:\n            if isinstance(entry['content'], list):\n                image_url = None\n                content = None\n                for item in entry['content']:\n                    if not isinstance(item, dict):\n                        continue\n\n                    if item['type'] == 'image_url' and isinstance(item['image_url'], dict):\n                        image_url = item['image_url']['url']\n                    elif item['type'] == 'text' and isinstance(item['text'], str):\n                        content = item['text']\n\n                if image_url and content:\n                    new_history.append({\"image_url\": image_url, \"role\": \"user\"})\n                    new_history.append({\"content\": content, \"role\": \"user\"})\n            else:\n                new_history.append(entry)\n\n        history = new_history\n\n    for entry in history:\n        if \"image_url\" in entry:\n            image_url = entry['image_url']\n            if \"base64\" in image_url:\n                image_url = re.sub('^data:image/.+;base64,', '', image_url)\n                img = Image.open(BytesIO(base64.b64decode(image_url)))\n            else:\n                try:\n                    my_res = requests.get(image_url)\n                    img = Image.open(BytesIO(my_res.content))\n                except Exception:\n                    raise 'Image cannot be loaded from the URL!'\n\n            buffered = BytesIO()\n            if img.mode in (\"RGBA\", \"P\"):\n                img = img.convert(\"RGB\")\n\n            img.save(buffered, format=\"JPEG\")\n            img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n            content = f'<img src=\"data:image/jpeg;base64,{img_str}\">'\n        else:\n            content = entry[\"content\"]\n\n        role = entry[\"role\"]\n\n        if role == \"user\":\n            user_input = content\n            user_input_last = True\n            if current_message:\n                chat_dialogue.append([current_message, ''])\n                current_message = \"\"\n\n            current_message = content\n        elif role == \"assistant\":\n            current_reply = content\n            user_input_last = False\n            if current_message:\n                chat_dialogue.append([current_message, current_reply])\n                current_message = \"\"\n                current_reply = \"\"\n            else:\n                chat_dialogue.append(['', current_reply])\n        elif role == \"system\":\n            system_message = content\n\n    if not user_input_last:\n        user_input = \"\"\n\n    return user_input, system_message, {'internal': chat_dialogue, 'visible': copy.deepcopy(chat_dialogue)}\n\n\ndef chat_completions_common(body: dict, is_legacy: bool = False, stream=False, prompt_only=False) -> dict:\n    if body.get('functions', []):\n        raise InvalidRequestError(message=\"functions is not supported.\", param='functions')\n\n    if body.get('function_call', ''):\n        raise InvalidRequestError(message=\"function_call is not supported.\", param='function_call')\n\n    if 'messages' not in body:\n        raise InvalidRequestError(message=\"messages is required\", param='messages')\n\n    messages = body['messages']\n    for m in messages:\n        if 'role' not in m:\n            raise InvalidRequestError(message=\"messages: missing role\", param='messages')\n        elif m['role'] == 'function':\n            raise InvalidRequestError(message=\"role: function is not supported.\", param='messages')\n\n        if 'content' not in m and \"image_url\" not in m:\n            raise InvalidRequestError(message=\"messages: missing content\", param='messages')\n\n    # Chat Completions\n    object_type = 'chat.completions' if not stream else 'chat.completions.chunk'\n    created_time = int(time.time())\n    cmpl_id = \"chatcmpl-%d\" % (int(time.time() * 1000000000))\n    resp_list = 'data' if is_legacy else 'choices'\n\n    # generation parameters\n    generate_params = process_parameters(body, is_legacy=is_legacy)\n    continue_ = body['continue_']\n\n    # Instruction template\n    if body['instruction_template_str']:\n        instruction_template_str = body['instruction_template_str']\n    elif body['instruction_template']:\n        instruction_template = body['instruction_template']\n        instruction_template = \"Alpaca\" if instruction_template == \"None\" else instruction_template\n        instruction_template_str = load_instruction_template_memoized(instruction_template)\n    else:\n        instruction_template_str = shared.settings['instruction_template_str']\n\n    chat_template_str = body['chat_template_str'] or shared.default_settings['chat_template_str']\n    chat_instruct_command = body['chat_instruct_command'] or shared.default_settings['chat-instruct_command']\n\n    # Chat character\n    character = body['character'] or shared.default_settings['character']\n    character = \"Assistant\" if character == \"None\" else character\n    name1 = body['user_name'] or shared.default_settings['name1']\n    name1, name2, _, greeting, context = load_character_memoized(character, name1, '')\n    name2 = body['bot_name'] or name2\n    context = body['context'] or context\n    greeting = body['greeting'] or greeting\n    user_bio = body['user_bio'] or ''\n\n    # History\n    user_input, custom_system_message, history = convert_history(messages)\n\n    generate_params.update({\n        'mode': body['mode'],\n        'name1': name1,\n        'name2': name2,\n        'context': context,\n        'greeting': greeting,\n        'user_bio': user_bio,\n        'instruction_template_str': instruction_template_str,\n        'custom_system_message': custom_system_message,\n        'chat_template_str': chat_template_str,\n        'chat-instruct_command': chat_instruct_command,\n        'history': history,\n        'stream': stream\n    })\n\n    max_tokens = generate_params['max_new_tokens']\n    if max_tokens in [None, 0]:\n        generate_params['max_new_tokens'] = 512\n        generate_params['auto_max_new_tokens'] = True\n\n    requested_model = generate_params.pop('model')\n    logprob_proc = generate_params.pop('logprob_proc', None)\n\n    def chat_streaming_chunk(content):\n        # begin streaming\n        chunk = {\n            \"id\": cmpl_id,\n            \"object\": object_type,\n            \"created\": created_time,\n            \"model\": shared.model_name,\n            resp_list: [{\n                \"index\": 0,\n                \"finish_reason\": None,\n                \"delta\": {'role': 'assistant', 'content': content},\n            }],\n        }\n\n        if logprob_proc:  # not official for chat yet\n            top_logprobs = convert_logprobs_to_tiktoken(model=requested_model, logprobs=logprob_proc.token_alternatives)\n            chunk[resp_list][0][\"logprobs\"] = {'top_logprobs': [top_logprobs]}\n        # else:\n        #    chunk[resp_list][0][\"logprobs\"] = None\n        return chunk\n\n    # generate reply #######################################\n    prompt = generate_chat_prompt(user_input, generate_params, _continue=continue_)\n    if prompt_only:\n        yield {'prompt': prompt}\n        return\n\n    token_count = len(encode(prompt)[0])\n    debug_msg({'prompt': prompt, 'generate_params': generate_params})\n\n    if stream:\n        yield chat_streaming_chunk('')\n\n    generator = generate_chat_reply(\n        user_input, generate_params, regenerate=False, _continue=continue_, loading_message=False)\n\n    answer = ''\n    seen_content = ''\n    completion_token_count = 0\n\n    for a in generator:\n        answer = a['internal'][-1][1]\n        if stream:\n            len_seen = len(seen_content)\n            new_content = answer[len_seen:]\n\n            if not new_content or chr(0xfffd) in new_content:  # partial unicode character, don't send it yet.\n                continue\n\n            seen_content = answer\n            chunk = chat_streaming_chunk(new_content)\n            yield chunk\n\n    completion_token_count = len(encode(answer)[0])\n    stop_reason = \"stop\"\n    if token_count + completion_token_count >= generate_params['truncation_length'] or completion_token_count >= generate_params['max_new_tokens']:\n        stop_reason = \"length\"\n\n    if stream:\n        chunk = chat_streaming_chunk('')\n        chunk[resp_list][0]['finish_reason'] = stop_reason\n        chunk['usage'] = {\n            \"prompt_tokens\": token_count,\n            \"completion_tokens\": completion_token_count,\n            \"total_tokens\": token_count + completion_token_count\n        }\n\n        yield chunk\n    else:\n        resp = {\n            \"id\": cmpl_id,\n            \"object\": object_type,\n            \"created\": created_time,\n            \"model\": shared.model_name,\n            resp_list: [{\n                \"index\": 0,\n                \"finish_reason\": stop_reason,\n                \"message\": {\"role\": \"assistant\", \"content\": answer}\n            }],\n            \"usage\": {\n                \"prompt_tokens\": token_count,\n                \"completion_tokens\": completion_token_count,\n                \"total_tokens\": token_count + completion_token_count\n            }\n        }\n        if logprob_proc:  # not official for chat yet\n            top_logprobs = convert_logprobs_to_tiktoken(model=requested_model, logprobs=logprob_proc.token_alternatives)\n            resp[resp_list][0][\"logprobs\"] = {'top_logprobs': [top_logprobs]}\n        # else:\n        #     resp[resp_list][0][\"logprobs\"] = None\n\n        yield resp\n\n\ndef completions_common(body: dict, is_legacy: bool = False, stream=False):\n    object_type = 'text_completion.chunk' if stream else 'text_completion'\n    created_time = int(time.time())\n    cmpl_id = \"conv-%d\" % (int(time.time() * 1000000000))\n    resp_list = 'data' if is_legacy else 'choices'\n\n    prompt_str = 'context' if is_legacy else 'prompt'\n\n    # ... encoded as a string, array of strings, array of tokens, or array of token arrays.\n    if prompt_str not in body:\n        raise InvalidRequestError(\"Missing required input\", param=prompt_str)\n\n    # common params\n    generate_params = process_parameters(body, is_legacy=is_legacy)\n    max_tokens = generate_params['max_new_tokens']\n    generate_params['stream'] = stream\n    requested_model = generate_params.pop('model')\n    logprob_proc = generate_params.pop('logprob_proc', None)\n    suffix = body['suffix'] if body['suffix'] else ''\n    echo = body['echo']\n\n    if not stream:\n        prompt_arg = body[prompt_str]\n        if isinstance(prompt_arg, str) or (isinstance(prompt_arg, list) and isinstance(prompt_arg[0], int)):\n            prompt_arg = [prompt_arg]\n\n        resp_list_data = []\n        total_completion_token_count = 0\n        total_prompt_token_count = 0\n\n        for idx, prompt in enumerate(prompt_arg, start=0):\n            if isinstance(prompt[0], int):\n                # token lists\n                if requested_model == shared.model_name:\n                    prompt = decode(prompt)[0]\n                else:\n                    try:\n                        encoder = tiktoken.encoding_for_model(requested_model)\n                        prompt = encoder.decode(prompt)\n                    except KeyError:\n                        prompt = decode(prompt)[0]\n\n            prefix = prompt if echo else ''\n            token_count = len(encode(prompt)[0])\n            total_prompt_token_count += token_count\n\n            # generate reply #######################################\n            debug_msg({'prompt': prompt, 'generate_params': generate_params})\n            generator = generate_reply(prompt, generate_params, is_chat=False)\n            answer = ''\n\n            for a in generator:\n                answer = a\n\n            completion_token_count = len(encode(answer)[0])\n            total_completion_token_count += completion_token_count\n            stop_reason = \"stop\"\n            if token_count + completion_token_count >= generate_params['truncation_length'] or completion_token_count >= max_tokens:\n                stop_reason = \"length\"\n\n            respi = {\n                \"index\": idx,\n                \"finish_reason\": stop_reason,\n                \"text\": prefix + answer + suffix,\n                \"logprobs\": {'top_logprobs': [logprob_proc.token_alternatives]} if logprob_proc else None,\n            }\n\n            resp_list_data.extend([respi])\n\n        resp = {\n            \"id\": cmpl_id,\n            \"object\": object_type,\n            \"created\": created_time,\n            \"model\": shared.model_name,\n            resp_list: resp_list_data,\n            \"usage\": {\n                \"prompt_tokens\": total_prompt_token_count,\n                \"completion_tokens\": total_completion_token_count,\n                \"total_tokens\": total_prompt_token_count + total_completion_token_count\n            }\n        }\n\n        yield resp\n    else:\n        prompt = body[prompt_str]\n        if isinstance(prompt, list):\n            if prompt and isinstance(prompt[0], int):\n                try:\n                    encoder = tiktoken.encoding_for_model(requested_model)\n                    prompt = encoder.decode(prompt)\n                except KeyError:\n                    prompt = decode(prompt)[0]\n            else:\n                raise InvalidRequestError(message=\"API Batched generation not yet supported.\", param=prompt_str)\n\n        prefix = prompt if echo else ''\n        token_count = len(encode(prompt)[0])\n\n        def text_streaming_chunk(content):\n            # begin streaming\n            chunk = {\n                \"id\": cmpl_id,\n                \"object\": object_type,\n                \"created\": created_time,\n                \"model\": shared.model_name,\n                resp_list: [{\n                    \"index\": 0,\n                    \"finish_reason\": None,\n                    \"text\": content,\n                    \"logprobs\": {'top_logprobs': [logprob_proc.token_alternatives]} if logprob_proc else None,\n                }],\n            }\n\n            return chunk\n\n        yield text_streaming_chunk(prefix)\n\n        # generate reply #######################################\n        debug_msg({'prompt': prompt, 'generate_params': generate_params})\n        generator = generate_reply(prompt, generate_params, is_chat=False)\n\n        answer = ''\n        seen_content = ''\n        completion_token_count = 0\n\n        for a in generator:\n            answer = a\n\n            len_seen = len(seen_content)\n            new_content = answer[len_seen:]\n\n            if not new_content or chr(0xfffd) in new_content:  # partial unicode character, don't send it yet.\n                continue\n\n            seen_content = answer\n            chunk = text_streaming_chunk(new_content)\n            yield chunk\n\n        completion_token_count = len(encode(answer)[0])\n        stop_reason = \"stop\"\n        if token_count + completion_token_count >= generate_params['truncation_length'] or completion_token_count >= max_tokens:\n            stop_reason = \"length\"\n\n        chunk = text_streaming_chunk(suffix)\n        chunk[resp_list][0][\"finish_reason\"] = stop_reason\n        chunk[\"usage\"] = {\n            \"prompt_tokens\": token_count,\n            \"completion_tokens\": completion_token_count,\n            \"total_tokens\": token_count + completion_token_count\n        }\n\n        yield chunk\n\n\ndef chat_completions(body: dict, is_legacy: bool = False) -> dict:\n    generator = chat_completions_common(body, is_legacy, stream=False)\n    return deque(generator, maxlen=1).pop()\n\n\ndef stream_chat_completions(body: dict, is_legacy: bool = False):\n    for resp in chat_completions_common(body, is_legacy, stream=True):\n        yield resp\n\n\ndef completions(body: dict, is_legacy: bool = False) -> dict:\n    generator = completions_common(body, is_legacy, stream=False)\n    return deque(generator, maxlen=1).pop()\n\n\ndef stream_completions(body: dict, is_legacy: bool = False):\n    for resp in completions_common(body, is_legacy, stream=True):\n        yield resp\n", "extensions/openai/embeddings.py": "import os\n\nimport numpy as np\nfrom transformers import AutoModel\n\nfrom extensions.openai.errors import ServiceUnavailableError\nfrom extensions.openai.utils import debug_msg, float_list_to_base64\nfrom modules.logging_colors import logger\n\nembeddings_params_initialized = False\n\n\ndef initialize_embedding_params():\n    '''\n    using 'lazy loading' to avoid circular import\n    so this function will be executed only once\n    '''\n    global embeddings_params_initialized\n    if not embeddings_params_initialized:\n        from extensions.openai.script import params\n\n        global st_model, embeddings_model, embeddings_device\n\n        st_model = os.environ.get(\"OPENEDAI_EMBEDDING_MODEL\", params.get('embedding_model', 'all-mpnet-base-v2'))\n        embeddings_model = None\n        # OPENEDAI_EMBEDDING_DEVICE: auto (best or cpu), cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone\n        embeddings_device = os.environ.get(\"OPENEDAI_EMBEDDING_DEVICE\", params.get('embedding_device', 'cpu'))\n        if embeddings_device.lower() == 'auto':\n            embeddings_device = None\n\n        embeddings_params_initialized = True\n\n\ndef load_embedding_model(model: str):\n    try:\n        from sentence_transformers import SentenceTransformer\n    except ModuleNotFoundError:\n        logger.error(\"The sentence_transformers module has not been found. Please install it manually with pip install -U sentence-transformers.\")\n        raise ModuleNotFoundError\n\n    initialize_embedding_params()\n    global embeddings_device, embeddings_model\n    try:\n        print(f\"Try embedding model: {model} on {embeddings_device}\")\n        if 'jina-embeddings' in model:\n            embeddings_model = AutoModel.from_pretrained(model, trust_remote_code=True)  # trust_remote_code is needed to use the encode method\n            embeddings_model = embeddings_model.to(embeddings_device)\n        else:\n            embeddings_model = SentenceTransformer(model, device=embeddings_device)\n\n        print(f\"Loaded embedding model: {model}\")\n    except Exception as e:\n        embeddings_model = None\n        raise ServiceUnavailableError(f\"Error: Failed to load embedding model: {model}\", internal_message=repr(e))\n\n\ndef get_embeddings_model():\n    initialize_embedding_params()\n    global embeddings_model, st_model\n    if st_model and not embeddings_model:\n        load_embedding_model(st_model)  # lazy load the model\n\n    return embeddings_model\n\n\ndef get_embeddings_model_name() -> str:\n    initialize_embedding_params()\n    global st_model\n    return st_model\n\n\ndef get_embeddings(input: list) -> np.ndarray:\n    model = get_embeddings_model()\n    debug_msg(f\"embedding model : {model}\")\n    embedding = model.encode(input, convert_to_numpy=True, normalize_embeddings=True, convert_to_tensor=False)\n    debug_msg(f\"embedding result : {embedding}\")  # might be too long even for debug, use at you own will\n    return embedding\n\n\ndef embeddings(input: list, encoding_format: str) -> dict:\n    embeddings = get_embeddings(input)\n    if encoding_format == \"base64\":\n        data = [{\"object\": \"embedding\", \"embedding\": float_list_to_base64(emb), \"index\": n} for n, emb in enumerate(embeddings)]\n    else:\n        data = [{\"object\": \"embedding\", \"embedding\": emb.tolist(), \"index\": n} for n, emb in enumerate(embeddings)]\n\n    response = {\n        \"object\": \"list\",\n        \"data\": data,\n        \"model\": st_model,  # return the real model\n        \"usage\": {\n            \"prompt_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n    }\n\n    debug_msg(f\"Embeddings return size: {len(embeddings[0])}, number: {len(embeddings)}\")\n    return response\n", "extensions/openai/images.py": "import os\nimport time\n\nimport requests\n\nfrom extensions.openai.errors import ServiceUnavailableError\n\n\ndef generations(prompt: str, size: str, response_format: str, n: int):\n    # Stable Diffusion callout wrapper for txt2img\n    # Low effort implementation for compatibility. With only \"prompt\" being passed and assuming DALL-E\n    # the results will be limited and likely poor. SD has hundreds of models and dozens of settings.\n    # If you want high quality tailored results you should just use the Stable Diffusion API directly.\n    # it's too general an API to try and shape the result with specific tags like negative prompts\n    # or \"masterpiece\", etc. SD configuration is beyond the scope of this API.\n    # At this point I will not add the edits and variations endpoints (ie. img2img) because they\n    # require changing the form data handling to accept multipart form data, also to properly support\n    # url return types will require file management and a web serving files... Perhaps later!\n    base_model_size = 512 if 'SD_BASE_MODEL_SIZE' not in os.environ else int(os.environ.get('SD_BASE_MODEL_SIZE', 512))\n    sd_defaults = {\n        'sampler_name': 'DPM++ 2M Karras',  # vast improvement\n        'steps': 30,\n    }\n\n    width, height = [int(x) for x in size.split('x')]  # ignore the restrictions on size\n\n    # to hack on better generation, edit default payload.\n    payload = {\n        'prompt': prompt,  # ignore prompt limit of 1000 characters\n        'width': width,\n        'height': height,\n        'batch_size': n,\n    }\n    payload.update(sd_defaults)\n\n    scale = min(width, height) / base_model_size\n    if scale >= 1.2:\n        # for better performance with the default size (1024), and larger res.\n        scaler = {\n            'width': width // scale,\n            'height': height // scale,\n            'hr_scale': scale,\n            'enable_hr': True,\n            'hr_upscaler': 'Latent',\n            'denoising_strength': 0.68,\n        }\n        payload.update(scaler)\n\n    resp = {\n        'created': int(time.time()),\n        'data': []\n    }\n    from extensions.openai.script import params\n\n    # TODO: support SD_WEBUI_AUTH username:password pair.\n    sd_url = f\"{os.environ.get('SD_WEBUI_URL', params.get('sd_webui_url', ''))}/sdapi/v1/txt2img\"\n\n    response = requests.post(url=sd_url, json=payload)\n    r = response.json()\n    if response.status_code != 200 or 'images' not in r:\n        print(r)\n        raise ServiceUnavailableError(r.get('error', 'Unknown error calling Stable Diffusion'), code=response.status_code, internal_message=r.get('errors', None))\n    # r['parameters']...\n    for b64_json in r['images']:\n        if response_format == 'b64_json':\n            resp['data'].extend([{'b64_json': b64_json}])\n        else:\n            resp['data'].extend([{'url': f'data:image/png;base64,{b64_json}'}])  # yeah it's lazy. requests.get() will not work with this\n\n    return resp\n", "extensions/openai/script.py": "import asyncio\nimport json\nimport logging\nimport os\nimport traceback\nfrom collections import deque\nfrom threading import Thread\n\nimport speech_recognition as sr\nimport uvicorn\nfrom fastapi import Depends, FastAPI, Header, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.requests import Request\nfrom fastapi.responses import JSONResponse\nfrom pydub import AudioSegment\nfrom sse_starlette import EventSourceResponse\n\nimport extensions.openai.completions as OAIcompletions\nimport extensions.openai.embeddings as OAIembeddings\nimport extensions.openai.images as OAIimages\nimport extensions.openai.logits as OAIlogits\nimport extensions.openai.models as OAImodels\nimport extensions.openai.moderations as OAImoderations\nfrom extensions.openai.errors import ServiceUnavailableError\nfrom extensions.openai.tokens import token_count, token_decode, token_encode\nfrom extensions.openai.utils import _start_cloudflared\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.models import unload_model\nfrom modules.text_generation import stop_everything_event\n\nfrom .typing import (\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ChatPromptResponse,\n    CompletionRequest,\n    CompletionResponse,\n    DecodeRequest,\n    DecodeResponse,\n    EmbeddingsRequest,\n    EmbeddingsResponse,\n    EncodeRequest,\n    EncodeResponse,\n    LoadLorasRequest,\n    LoadModelRequest,\n    LogitsRequest,\n    LogitsResponse,\n    LoraListResponse,\n    ModelInfoResponse,\n    ModelListResponse,\n    TokenCountResponse,\n    to_dict\n)\n\nparams = {\n    'embedding_device': 'cpu',\n    'embedding_model': 'sentence-transformers/all-mpnet-base-v2',\n    'sd_webui_url': '',\n    'debug': 0\n}\n\n\nstreaming_semaphore = asyncio.Semaphore(1)\n\n\ndef verify_api_key(authorization: str = Header(None)) -> None:\n    expected_api_key = shared.args.api_key\n    if expected_api_key and (authorization is None or authorization != f\"Bearer {expected_api_key}\"):\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n\ndef verify_admin_key(authorization: str = Header(None)) -> None:\n    expected_api_key = shared.args.admin_key\n    if expected_api_key and (authorization is None or authorization != f\"Bearer {expected_api_key}\"):\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n\napp = FastAPI()\ncheck_key = [Depends(verify_api_key)]\ncheck_admin_key = [Depends(verify_admin_key)]\n\n# Configure CORS settings to allow all origins, methods, and headers\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"]\n)\n\n\n@app.options(\"/\", dependencies=check_key)\nasync def options_route():\n    return JSONResponse(content=\"OK\")\n\n\n@app.post('/v1/completions', response_model=CompletionResponse, dependencies=check_key)\nasync def openai_completions(request: Request, request_data: CompletionRequest):\n    path = request.url.path\n    is_legacy = \"/generate\" in path\n\n    if request_data.stream:\n        async def generator():\n            async with streaming_semaphore:\n                response = OAIcompletions.stream_completions(to_dict(request_data), is_legacy=is_legacy)\n                for resp in response:\n                    disconnected = await request.is_disconnected()\n                    if disconnected:\n                        break\n\n                    yield {\"data\": json.dumps(resp)}\n\n        return EventSourceResponse(generator())  # SSE streaming\n\n    else:\n        response = OAIcompletions.completions(to_dict(request_data), is_legacy=is_legacy)\n        return JSONResponse(response)\n\n\n@app.post('/v1/chat/completions', response_model=ChatCompletionResponse, dependencies=check_key)\nasync def openai_chat_completions(request: Request, request_data: ChatCompletionRequest):\n    path = request.url.path\n    is_legacy = \"/generate\" in path\n\n    if request_data.stream:\n        async def generator():\n            async with streaming_semaphore:\n                response = OAIcompletions.stream_chat_completions(to_dict(request_data), is_legacy=is_legacy)\n                for resp in response:\n                    disconnected = await request.is_disconnected()\n                    if disconnected:\n                        break\n\n                    yield {\"data\": json.dumps(resp)}\n\n        return EventSourceResponse(generator())  # SSE streaming\n\n    else:\n        response = OAIcompletions.chat_completions(to_dict(request_data), is_legacy=is_legacy)\n        return JSONResponse(response)\n\n\n@app.get(\"/v1/models\", dependencies=check_key)\n@app.get(\"/v1/models/{model}\", dependencies=check_key)\nasync def handle_models(request: Request):\n    path = request.url.path\n    is_list = request.url.path.split('?')[0].split('#')[0] == '/v1/models'\n\n    if is_list:\n        response = OAImodels.list_dummy_models()\n    else:\n        model_name = path[len('/v1/models/'):]\n        response = OAImodels.model_info_dict(model_name)\n\n    return JSONResponse(response)\n\n\n@app.get('/v1/billing/usage', dependencies=check_key)\ndef handle_billing_usage():\n    '''\n    Ex. /v1/dashboard/billing/usage?start_date=2023-05-01&end_date=2023-05-31\n    '''\n    return JSONResponse(content={\"total_usage\": 0})\n\n\n@app.post('/v1/audio/transcriptions', dependencies=check_key)\nasync def handle_audio_transcription(request: Request):\n    r = sr.Recognizer()\n\n    form = await request.form()\n    audio_file = await form[\"file\"].read()\n    audio_data = AudioSegment.from_file(audio_file)\n\n    # Convert AudioSegment to raw data\n    raw_data = audio_data.raw_data\n\n    # Create AudioData object\n    audio_data = sr.AudioData(raw_data, audio_data.frame_rate, audio_data.sample_width)\n    whisper_language = form.getvalue('language', None)\n    whisper_model = form.getvalue('model', 'tiny')  # Use the model from the form data if it exists, otherwise default to tiny\n\n    transcription = {\"text\": \"\"}\n\n    try:\n        transcription[\"text\"] = r.recognize_whisper(audio_data, language=whisper_language, model=whisper_model)\n    except sr.UnknownValueError:\n        print(\"Whisper could not understand audio\")\n        transcription[\"text\"] = \"Whisper could not understand audio UnknownValueError\"\n    except sr.RequestError as e:\n        print(\"Could not request results from Whisper\", e)\n        transcription[\"text\"] = \"Whisper could not understand audio RequestError\"\n\n    return JSONResponse(content=transcription)\n\n\n@app.post('/v1/images/generations', dependencies=check_key)\nasync def handle_image_generation(request: Request):\n\n    if not os.environ.get('SD_WEBUI_URL', params.get('sd_webui_url', '')):\n        raise ServiceUnavailableError(\"Stable Diffusion not available. SD_WEBUI_URL not set.\")\n\n    body = await request.json()\n    prompt = body['prompt']\n    size = body.get('size', '1024x1024')\n    response_format = body.get('response_format', 'url')  # or b64_json\n    n = body.get('n', 1)  # ignore the batch limits of max 10\n\n    response = await OAIimages.generations(prompt=prompt, size=size, response_format=response_format, n=n)\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/embeddings\", response_model=EmbeddingsResponse, dependencies=check_key)\nasync def handle_embeddings(request: Request, request_data: EmbeddingsRequest):\n    input = request_data.input\n    if not input:\n        raise HTTPException(status_code=400, detail=\"Missing required argument input\")\n\n    if type(input) is str:\n        input = [input]\n\n    response = OAIembeddings.embeddings(input, request_data.encoding_format)\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/moderations\", dependencies=check_key)\nasync def handle_moderations(request: Request):\n    body = await request.json()\n    input = body[\"input\"]\n    if not input:\n        raise HTTPException(status_code=400, detail=\"Missing required argument input\")\n\n    response = OAImoderations.moderations(input)\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/internal/encode\", response_model=EncodeResponse, dependencies=check_key)\nasync def handle_token_encode(request_data: EncodeRequest):\n    response = token_encode(request_data.text)\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/internal/decode\", response_model=DecodeResponse, dependencies=check_key)\nasync def handle_token_decode(request_data: DecodeRequest):\n    response = token_decode(request_data.tokens)\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/internal/token-count\", response_model=TokenCountResponse, dependencies=check_key)\nasync def handle_token_count(request_data: EncodeRequest):\n    response = token_count(request_data.text)\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/internal/logits\", response_model=LogitsResponse, dependencies=check_key)\nasync def handle_logits(request_data: LogitsRequest):\n    '''\n    Given a prompt, returns the top 50 most likely logits as a dict.\n    The keys are the tokens, and the values are the probabilities.\n    '''\n    response = OAIlogits._get_next_logits(to_dict(request_data))\n    return JSONResponse(response)\n\n\n@app.post('/v1/internal/chat-prompt', response_model=ChatPromptResponse, dependencies=check_key)\nasync def handle_chat_prompt(request: Request, request_data: ChatCompletionRequest):\n    path = request.url.path\n    is_legacy = \"/generate\" in path\n    generator = OAIcompletions.chat_completions_common(to_dict(request_data), is_legacy=is_legacy, prompt_only=True)\n    response = deque(generator, maxlen=1).pop()\n    return JSONResponse(response)\n\n\n@app.post(\"/v1/internal/stop-generation\", dependencies=check_key)\nasync def handle_stop_generation(request: Request):\n    stop_everything_event()\n    return JSONResponse(content=\"OK\")\n\n\n@app.get(\"/v1/internal/model/info\", response_model=ModelInfoResponse, dependencies=check_key)\nasync def handle_model_info():\n    payload = OAImodels.get_current_model_info()\n    return JSONResponse(content=payload)\n\n\n@app.get(\"/v1/internal/model/list\", response_model=ModelListResponse, dependencies=check_admin_key)\nasync def handle_list_models():\n    payload = OAImodels.list_models()\n    return JSONResponse(content=payload)\n\n\n@app.post(\"/v1/internal/model/load\", dependencies=check_admin_key)\nasync def handle_load_model(request_data: LoadModelRequest):\n    '''\n    This endpoint is experimental and may change in the future.\n\n    The \"args\" parameter can be used to modify flags like \"--load-in-4bit\"\n    or \"--n-gpu-layers\" before loading a model. Example:\n\n    ```\n    \"args\": {\n      \"load_in_4bit\": true,\n      \"n_gpu_layers\": 12\n    }\n    ```\n\n    Note that those settings will remain after loading the model. So you\n    may need to change them back to load a second model.\n\n    The \"settings\" parameter is also a dict but with keys for the\n    shared.settings object. It can be used to modify the default instruction\n    template like this:\n\n    ```\n    \"settings\": {\n      \"instruction_template\": \"Alpaca\"\n    }\n    ```\n    '''\n\n    try:\n        OAImodels._load_model(to_dict(request_data))\n        return JSONResponse(content=\"OK\")\n    except:\n        traceback.print_exc()\n        return HTTPException(status_code=400, detail=\"Failed to load the model.\")\n\n\n@app.post(\"/v1/internal/model/unload\", dependencies=check_admin_key)\nasync def handle_unload_model():\n    unload_model()\n\n\n@app.get(\"/v1/internal/lora/list\", response_model=LoraListResponse, dependencies=check_admin_key)\nasync def handle_list_loras():\n    response = OAImodels.list_loras()\n    return JSONResponse(content=response)\n\n\n@app.post(\"/v1/internal/lora/load\", dependencies=check_admin_key)\nasync def handle_load_loras(request_data: LoadLorasRequest):\n    try:\n        OAImodels.load_loras(request_data.lora_names)\n        return JSONResponse(content=\"OK\")\n    except:\n        traceback.print_exc()\n        return HTTPException(status_code=400, detail=\"Failed to apply the LoRA(s).\")\n\n\n@app.post(\"/v1/internal/lora/unload\", dependencies=check_admin_key)\nasync def handle_unload_loras():\n    OAImodels.unload_all_loras()\n    return JSONResponse(content=\"OK\")\n\n\ndef run_server():\n    server_addr = '0.0.0.0' if shared.args.listen else '127.0.0.1'\n    port = int(os.environ.get('OPENEDAI_PORT', shared.args.api_port))\n\n    ssl_certfile = os.environ.get('OPENEDAI_CERT_PATH', shared.args.ssl_certfile)\n    ssl_keyfile = os.environ.get('OPENEDAI_KEY_PATH', shared.args.ssl_keyfile)\n\n    if shared.args.public_api:\n        def on_start(public_url: str):\n            logger.info(f'OpenAI-compatible API URL:\\n\\n{public_url}\\n')\n\n        _start_cloudflared(port, shared.args.public_api_id, max_attempts=3, on_start=on_start)\n    else:\n        if ssl_keyfile and ssl_certfile:\n            logger.info(f'OpenAI-compatible API URL:\\n\\nhttps://{server_addr}:{port}\\n')\n        else:\n            logger.info(f'OpenAI-compatible API URL:\\n\\nhttp://{server_addr}:{port}\\n')\n\n    if shared.args.api_key:\n        if not shared.args.admin_key:\n            shared.args.admin_key = shared.args.api_key\n\n        logger.info(f'OpenAI API key:\\n\\n{shared.args.api_key}\\n')\n\n    if shared.args.admin_key and shared.args.admin_key != shared.args.api_key:\n        logger.info(f'OpenAI API admin key (for loading/unloading models):\\n\\n{shared.args.admin_key}\\n')\n\n    logging.getLogger(\"uvicorn.error\").propagate = False\n    uvicorn.run(app, host=server_addr, port=port, ssl_certfile=ssl_certfile, ssl_keyfile=ssl_keyfile)\n\n\ndef setup():\n    if shared.args.nowebui:\n        run_server()\n    else:\n        Thread(target=run_server, daemon=True).start()\n", "extensions/openai/moderations.py": "import time\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nfrom extensions.openai.embeddings import get_embeddings\n\nmoderations_disabled = False  # return 0/false\ncategory_embeddings = None\nantonym_embeddings = None\ncategories = [\"sexual\", \"hate\", \"harassment\", \"self-harm\", \"sexual/minors\", \"hate/threatening\", \"violence/graphic\", \"self-harm/intent\", \"self-harm/instructions\", \"harassment/threatening\", \"violence\"]\nflag_threshold = 0.5\n\n\ndef get_category_embeddings() -> dict:\n    global category_embeddings, categories\n    if category_embeddings is None:\n        embeddings = get_embeddings(categories).tolist()\n        category_embeddings = dict(zip(categories, embeddings))\n\n    return category_embeddings\n\n\ndef cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n    return np.dot(a, b) / (norm(a) * norm(b))\n\n\n# seems most openai like with all-mpnet-base-v2\ndef mod_score(a: np.ndarray, b: np.ndarray) -> float:\n    return 2.0 * np.dot(a, b)\n\n\ndef moderations(input):\n    global category_embeddings, categories, flag_threshold, moderations_disabled\n    results = {\n        \"id\": f\"modr-{int(time.time()*1e9)}\",\n        \"model\": \"text-moderation-001\",\n        \"results\": [],\n    }\n\n    if moderations_disabled:\n        results['results'] = [{\n            'categories': dict([(C, False) for C in categories]),\n            'category_scores': dict([(C, 0.0) for C in categories]),\n            'flagged': False,\n        }]\n        return results\n\n    category_embeddings = get_category_embeddings()\n\n    # input, string or array\n    if isinstance(input, str):\n        input = [input]\n\n    for in_str in input:\n        for ine in get_embeddings([in_str]):\n            category_scores = dict([(C, mod_score(category_embeddings[C], ine)) for C in categories])\n            category_flags = dict([(C, bool(category_scores[C] > flag_threshold)) for C in categories])\n            flagged = any(category_flags.values())\n\n            results['results'].extend([{\n                'flagged': flagged,\n                'categories': category_flags,\n                'category_scores': category_scores,\n            }])\n\n    print(results)\n\n    return results\n", "extensions/coqui_tts/script.py": "import html\nimport json\nimport os\nimport random\nimport time\nfrom pathlib import Path\n\nimport gradio as gr\nimport torch\nfrom TTS.api import TTS\nfrom TTS.utils.synthesizer import Synthesizer\n\nfrom modules import chat, shared, ui_chat\nfrom modules.ui import create_refresh_button\nfrom modules.utils import gradio\n\nos.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n\nparams = {\n    \"activate\": True,\n    \"autoplay\": True,\n    \"show_text\": False,\n    \"remove_trailing_dots\": False,\n    \"voice\": \"female_01.wav\",\n    \"language\": \"English\",\n    \"model_name\": \"tts_models/multilingual/multi-dataset/xtts_v2\",\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n}\n\nthis_dir = str(Path(__file__).parent.resolve())\nmodel = None\nwith open(Path(f\"{this_dir}/languages.json\"), encoding='utf8') as f:\n    languages = json.load(f)\n\n\ndef get_available_voices():\n    return sorted([voice.name for voice in Path(f\"{this_dir}/voices\").glob(\"*.wav\")])\n\n\ndef preprocess(raw_input):\n    raw_input = html.unescape(raw_input)\n    # raw_input = raw_input.strip(\"\\\"\")\n    return raw_input\n\n\ndef new_split_into_sentences(self, text):\n    sentences = self.seg.segment(text)\n    if params['remove_trailing_dots']:\n        sentences_without_dots = []\n        for sentence in sentences:\n            if sentence.endswith('.') and not sentence.endswith('...'):\n                sentence = sentence[:-1]\n\n            sentences_without_dots.append(sentence)\n\n        return sentences_without_dots\n    else:\n        return sentences\n\n\nSynthesizer.split_into_sentences = new_split_into_sentences\n\n\ndef load_model():\n    model = TTS(params[\"model_name\"]).to(params[\"device\"])\n    return model\n\n\ndef remove_tts_from_history(history):\n    for i, entry in enumerate(history['internal']):\n        history['visible'][i] = [history['visible'][i][0], entry[1]]\n\n    return history\n\n\ndef toggle_text_in_history(history):\n    for i, entry in enumerate(history['visible']):\n        visible_reply = entry[1]\n        if visible_reply.startswith('<audio'):\n            if params['show_text']:\n                reply = history['internal'][i][1]\n                history['visible'][i] = [history['visible'][i][0], f\"{visible_reply.split('</audio>')[0]}</audio>\\n\\n{reply}\"]\n            else:\n                history['visible'][i] = [history['visible'][i][0], f\"{visible_reply.split('</audio>')[0]}</audio>\"]\n\n    return history\n\n\ndef random_sentence():\n    with open(Path(\"extensions/coqui_tts/harvard_sentences.txt\")) as f:\n        return random.choice(list(f))\n\n\ndef voice_preview(string):\n    string = html.unescape(string) or random_sentence()\n\n    output_file = Path('extensions/coqui_tts/outputs/voice_preview.wav')\n    model.tts_to_file(\n        text=string,\n        file_path=output_file,\n        speaker_wav=[f\"{this_dir}/voices/{params['voice']}\"],\n        language=languages[params[\"language\"]]\n    )\n\n    return f'<audio src=\"file/{output_file.as_posix()}?{int(time.time())}\" controls autoplay></audio>'\n\n\ndef history_modifier(history):\n    # Remove autoplay from the last reply\n    if len(history['internal']) > 0:\n        history['visible'][-1] = [\n            history['visible'][-1][0],\n            history['visible'][-1][1].replace('controls autoplay>', 'controls>')\n        ]\n\n    return history\n\n\ndef state_modifier(state):\n    if not params['activate']:\n        return state\n\n    state['stream'] = False\n    return state\n\n\ndef input_modifier(string, state):\n    if not params['activate']:\n        return string\n\n    shared.processing_message = \"*Is recording a voice message...*\"\n    return string\n\n\ndef output_modifier(string, state):\n    if not params['activate']:\n        return string\n\n    original_string = string\n    string = preprocess(html.unescape(string))\n    if string == '':\n        string = '*Empty reply, try regenerating*'\n    else:\n        output_file = Path(f'extensions/coqui_tts/outputs/{state[\"character_menu\"]}_{int(time.time())}.wav')\n        model.tts_to_file(\n            text=string,\n            file_path=output_file,\n            speaker_wav=[f\"{this_dir}/voices/{params['voice']}\"],\n            language=languages[params[\"language\"]]\n        )\n\n        autoplay = 'autoplay' if params['autoplay'] else ''\n        string = f'<audio src=\"file/{output_file.as_posix()}\" controls {autoplay}></audio>'\n        if params['show_text']:\n            string += f'\\n\\n{original_string}'\n\n    shared.processing_message = \"*Is typing...*\"\n    return string\n\n\ndef custom_css():\n    path_to_css = Path(f\"{this_dir}/style.css\")\n    return open(path_to_css, 'r').read()\n\n\ndef setup():\n    global model\n    print(\"[XTTS] Loading XTTS...\")\n    model = load_model()\n    print(\"[XTTS] Done!\")\n    Path(f\"{this_dir}/outputs\").mkdir(parents=True, exist_ok=True)\n\n\ndef ui():\n    with gr.Accordion(\"Coqui TTS (XTTSv2)\"):\n        with gr.Row():\n            activate = gr.Checkbox(value=params['activate'], label='Activate TTS')\n            autoplay = gr.Checkbox(value=params['autoplay'], label='Play TTS automatically')\n\n        with gr.Row():\n            show_text = gr.Checkbox(value=params['show_text'], label='Show message text under audio player')\n            remove_trailing_dots = gr.Checkbox(value=params['remove_trailing_dots'], label='Remove trailing \".\" from text segments before converting to audio')\n\n        with gr.Row():\n            with gr.Row():\n                voice = gr.Dropdown(get_available_voices(), label=\"Voice wav\", value=params[\"voice\"])\n                create_refresh_button(voice, lambda: None, lambda: {'choices': get_available_voices(), 'value': params[\"voice\"]}, 'refresh-button')\n\n            language = gr.Dropdown(languages.keys(), label=\"Language\", value=params[\"language\"])\n\n        with gr.Row():\n            preview_text = gr.Text(show_label=False, placeholder=\"Preview text\", elem_id=\"silero_preview_text\")\n            preview_play = gr.Button(\"Preview\")\n            preview_audio = gr.HTML(visible=False)\n\n        with gr.Row():\n            convert = gr.Button('Permanently replace audios with the message texts')\n            convert_cancel = gr.Button('Cancel', visible=False)\n            convert_confirm = gr.Button('Confirm (cannot be undone)', variant=\"stop\", visible=False)\n\n    # Convert history with confirmation\n    convert_arr = [convert_confirm, convert, convert_cancel]\n    convert.click(lambda: [gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)], None, convert_arr)\n    convert_confirm.click(\n        lambda: [gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)], None, convert_arr).then(\n        remove_tts_from_history, gradio('history'), gradio('history')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        chat.redraw_html, gradio(ui_chat.reload_arr), gradio('display'))\n\n    convert_cancel.click(lambda: [gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)], None, convert_arr)\n\n    # Toggle message text in history\n    show_text.change(\n        lambda x: params.update({\"show_text\": x}), show_text, None).then(\n        toggle_text_in_history, gradio('history'), gradio('history')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        chat.redraw_html, gradio(ui_chat.reload_arr), gradio('display'))\n\n    # Event functions to update the parameters in the backend\n    activate.change(lambda x: params.update({\"activate\": x}), activate, None)\n    autoplay.change(lambda x: params.update({\"autoplay\": x}), autoplay, None)\n    remove_trailing_dots.change(lambda x: params.update({\"remove_trailing_dots\": x}), remove_trailing_dots, None)\n    voice.change(lambda x: params.update({\"voice\": x}), voice, None)\n    language.change(lambda x: params.update({\"language\": x}), language, None)\n\n    # Play preview\n    preview_text.submit(voice_preview, preview_text, preview_audio)\n    preview_play.click(voice_preview, preview_text, preview_audio)\n", "extensions/perplexity_colors/script.py": "import time\n\nimport gradio\nimport numpy as np\nimport torch\nfrom transformers import LogitsProcessor\n\nfrom modules import html_generator, shared\n\nparams = {\n    'active': True,\n    'color_by_perplexity': False,\n    'color_by_probability': False,\n    'ppl_scale': 15.0,  # No slider for this right now, because I don't think it really needs to be changed. Very large perplexity scores don't show up often.\n    'probability_dropdown': False,\n    'verbose': False  # For debugging mostly\n}\n\n\nclass PerplexityLogits(LogitsProcessor):\n    def __init__(self, verbose=False):\n        self.generated_token_ids = []\n        self.selected_probs = []\n        self.top_token_ids_list = []\n        self.top_probs_list = []\n        self.perplexities_list = []\n        self.last_probs = None\n        self.verbose = verbose\n\n    def __call__(self, input_ids, scores):\n        # t0 = time.time()\n        probs = torch.softmax(scores, dim=-1, dtype=torch.float)\n        log_probs = torch.nan_to_num(torch.log(probs))  # Note: This is to convert log(0) nan to 0, but probs*log_probs makes this 0 not affect the perplexity.\n        entropy = -torch.sum(probs * log_probs)\n        entropy = entropy.cpu().numpy()\n        perplexity = round(float(np.exp(entropy)), 4)\n        self.perplexities_list.append(perplexity)\n        last_token_id = int(input_ids[0][-1].cpu().numpy().item())\n        # Store the generated tokens (not sure why this isn't accessible in the output endpoint!)\n        self.generated_token_ids.append(last_token_id)\n        # Get last probability, and add to the list if it wasn't there\n        if len(self.selected_probs) > 0:\n            # Is the selected token in the top tokens?\n            if self.verbose:\n                print('Probs: Token after', shared.tokenizer.decode(last_token_id))\n                print('Probs:', [shared.tokenizer.decode(token_id) for token_id in self.top_token_ids_list[-1][0]])\n                print('Probs:', [round(float(prob), 4) for prob in self.top_probs_list[-1][0]])\n            if last_token_id in self.top_token_ids_list[-1][0]:\n                idx = self.top_token_ids_list[-1][0].index(last_token_id)\n                self.selected_probs.append(self.top_probs_list[-1][0][idx])\n            else:\n                self.top_token_ids_list[-1][0].append(last_token_id)\n                last_prob = round(float(self.last_probs[last_token_id]), 4)\n                self.top_probs_list[-1][0].append(last_prob)\n                self.selected_probs.append(last_prob)\n        else:\n            self.selected_probs.append(1.0)  # Placeholder for the last token of the prompt\n\n        if self.verbose:\n            pplbar = \"-\"\n            if not np.isnan(perplexity):\n                pplbar = \"*\" * round(perplexity)\n            print(f\"PPL: Token after {shared.tokenizer.decode(last_token_id)}\\t{perplexity:.2f}\\t{pplbar}\")\n\n        # Get top 5 probabilities\n        top_tokens_and_probs = torch.topk(probs, 5)\n        top_probs = top_tokens_and_probs.values.cpu().numpy().astype(float).tolist()\n        top_token_ids = top_tokens_and_probs.indices.cpu().numpy().astype(int).tolist()\n\n        self.top_token_ids_list.append(top_token_ids)\n        self.top_probs_list.append(top_probs)\n\n        probs = probs.cpu().numpy().flatten()\n        self.last_probs = probs  # Need to keep this as a reference for top probs\n\n        # t1 = time.time()\n        # print(f\"PPL Processor: {(t1-t0):.3f} s\")\n        # About 1 ms, though occasionally up to around 100 ms, not sure why...\n        # Doesn't actually modify the logits!\n        return scores\n\n\n# Stores the perplexity and top probabilities\nppl_logits_processor = None\n\n\ndef logits_processor_modifier(logits_processor_list, input_ids):\n    global ppl_logits_processor\n    if params['active']:\n        ppl_logits_processor = PerplexityLogits(verbose=params['verbose'])\n        logits_processor_list.append(ppl_logits_processor)\n\n\ndef output_modifier(text):\n    global ppl_logits_processor\n    # t0 = time.time()\n\n    if not params['active']:\n        return text\n\n    # TODO: It's probably more efficient to do this above rather than modifying all these lists\n    # Remove last element of perplexities_list, top_token_ids_list, top_tokens_list, top_probs_list since everything is off by one because this extension runs before generation\n    perplexities = ppl_logits_processor.perplexities_list[:-1]\n    top_token_ids_list = ppl_logits_processor.top_token_ids_list[:-1]\n    top_tokens_list = [[shared.tokenizer.decode(token_id) for token_id in top_token_ids[0]] for top_token_ids in top_token_ids_list]\n    top_probs_list = ppl_logits_processor.top_probs_list[:-1]\n    # Remove first element of generated_token_ids, generated_tokens, selected_probs because they are for the last token of the prompt\n    gen_token_ids = ppl_logits_processor.generated_token_ids[1:]\n    gen_tokens = [shared.tokenizer.decode(token_id) for token_id in gen_token_ids]\n    sel_probs = ppl_logits_processor.selected_probs[1:]\n\n    end_part = '</div></div>' if params['probability_dropdown'] else '</span>'  # Helps with finding the index after replacing part of the text.\n\n    i = 0\n    for token, prob, ppl, top_tokens, top_probs in zip(gen_tokens, sel_probs, perplexities, top_tokens_list, top_probs_list):\n        color = 'ffffff'\n        if params['color_by_probability'] and params['color_by_perplexity']:\n            color = probability_perplexity_color_scale(prob, ppl)\n        elif params['color_by_perplexity']:\n            color = perplexity_color_scale(ppl)\n        elif params['color_by_probability']:\n            color = probability_color_scale(prob)\n        if token in text[i:]:\n            if params['probability_dropdown']:\n                text = text[:i] + text[i:].replace(token, add_dropdown_html(token, color, top_tokens, top_probs[0], ppl), 1)\n            else:\n                text = text[:i] + text[i:].replace(token, add_color_html(token, color), 1)\n            i += text[i:].find(end_part) + len(end_part)\n\n    # Use full perplexity list for calculating the average here.\n    print('Average perplexity:', round(np.mean(ppl_logits_processor.perplexities_list[:-1]), 4))\n    # t1 = time.time()\n    # print(f\"Modifier: {(t1-t0):.3f} s\")\n    # About 50 ms\n    return text\n\n\ndef probability_color_scale(prob):\n    '''\n    Green-yellow-red color scale\n    '''\n\n    rv = 0\n    gv = 0\n    if prob <= 0.5:\n        rv = 'ff'\n        gv = hex(int(255 * prob * 2))[2:]\n        if len(gv) < 2:\n            gv = '0' * (2 - len(gv)) + gv\n    else:\n        rv = hex(int(255 - 255 * (prob - 0.5) * 2))[2:]\n        gv = 'ff'\n        if len(rv) < 2:\n            rv = '0' * (2 - len(rv)) + rv\n\n    return rv + gv + '00'\n\n\ndef perplexity_color_scale(ppl):\n    '''\n    Red component only, white for 0 perplexity (sorry if you're not in dark mode)\n    '''\n    value = hex(max(int(255.0 - params['ppl_scale'] * (float(ppl) - 1.0)), 0))[2:]\n    if len(value) < 2:\n        value = '0' * (2 - len(value)) + value\n\n    return 'ff' + value + value\n\n\ndef probability_perplexity_color_scale(prob, ppl):\n    '''\n    Green-yellow-red for probability and blue component for perplexity\n    '''\n\n    rv = 0\n    gv = 0\n    bv = hex(min(max(int(params['ppl_scale'] * (float(ppl) - 1.0)), 0), 255))[2:]\n    if len(bv) < 2:\n        bv = '0' * (2 - len(bv)) + bv\n\n    if prob <= 0.5:\n        rv = 'ff'\n        gv = hex(int(255 * prob * 2))[2:]\n        if len(gv) < 2:\n            gv = '0' * (2 - len(gv)) + gv\n    else:\n        rv = hex(int(255 - 255 * (prob - 0.5) * 2))[2:]\n        gv = 'ff'\n        if len(rv) < 2:\n            rv = '0' * (2 - len(rv)) + rv\n\n    return rv + gv + bv\n\n\ndef add_color_html(token, color):\n    return f'<span style=\"color: #{color}\">{token}</span>'\n\n\n# TODO: Major issue: Applying this to too many tokens will cause a permanent slowdown in generation speed until the messages are removed from the history.\n# I think the issue is from HTML elements taking up space in the visible history, and things like history deepcopy add latency proportional to the size of the history.\n# Potential solution is maybe to modify the main generation code to send just the internal text and not the visible history, to avoid moving too much around.\n# I wonder if we can also avoid using deepcopy here.\ndef add_dropdown_html(token, color, top_tokens, top_probs, perplexity=0):\n    html = f'<div class=\"hoverable\"><span style=\"color: #{color}\">{token}</span><div class=\"dropdown\"><table class=\"dropdown-content\"><tbody>'\n    for token_option, prob in zip(top_tokens, top_probs):\n        # TODO: Bold for selected token?\n        # Using divs prevented the problem of divs inside spans causing issues.\n        # Now the problem is that divs show the same whitespace of one space between every token.\n        # There is probably some way to fix this in CSS that I don't know about.\n        row_color = probability_color_scale(prob)\n        row_class = ' class=\"selected\"' if token_option == token else ''\n        html += f'<tr{row_class}><td style=\"color: #{row_color}\">{token_option}</td><td style=\"color: #{row_color}\">{prob:.4f}</td></tr>'\n    if perplexity != 0:\n        ppl_color = perplexity_color_scale(perplexity)\n        html += f'<tr><td>Perplexity:</td><td style=\"color: #{ppl_color}\">{perplexity:.4f}</td></tr>'\n    html += '</tbody></table></div></div>'\n    return html  # About 750 characters per token...\n\n\ndef custom_css():\n    return \"\"\"\n        .dropdown {\n            display: none;\n            position: absolute;\n            z-index: 50;\n            background-color: var(--block-background-fill);\n            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);\n            width: max-content;\n            overflow: visible;\n            padding: 5px;\n            border-radius: 10px;\n            border: 1px solid var(--border-color-primary);\n        }\n\n        .dropdown-content {\n            border: none;\n            z-index: 50;\n        }\n\n        .dropdown-content tr.selected {\n            background-color: var(--block-label-background-fill);\n        }\n\n        .dropdown-content td {\n            color: var(--body-text-color);\n        }\n\n        .hoverable {\n            color: var(--body-text-color);\n            position: relative;\n            display: inline-block;\n            overflow: visible;\n            font-size: 15px;\n            line-height: 1.75;\n            margin: 0;\n            padding: 0;\n        }\n\n        .hoverable:hover .dropdown {\n            display: block;\n        }\n\n        pre {\n            white-space: pre-wrap;\n        }\n\n        # TODO: This makes the hover menus extend outside the bounds of the chat area, which is good.\n        # However, it also makes the scrollbar disappear, which is bad.\n        # The scroll bar needs to still be present. So for now, we can't see dropdowns that extend past the edge of the chat area.\n        #.chat {\n        #    overflow-y: auto;\n        #}\n    \"\"\"\n\n\n# Monkeypatch applied to html_generator.py\n# We simply don't render markdown into HTML. We wrap everything in <pre> tags to preserve whitespace\n# formatting. If you're coloring tokens by perplexity or probability, or especially if you're using\n# the probability dropdown, you probably care more about seeing the tokens the model actually outputted\n# rather than rendering ```code blocks``` or *italics*.\ndef convert_to_markdown(string):\n    return '<pre>' + string + '</pre>'\n\n\nhtml_generator.convert_to_markdown = convert_to_markdown\n\n\ndef ui():\n    def update_active_check(x):\n        params.update({'active': x})\n\n    def update_color_by_ppl_check(x):\n        params.update({'color_by_perplexity': x})\n\n    def update_color_by_prob_check(x):\n        params.update({'color_by_probability': x})\n\n    def update_prob_dropdown_check(x):\n        params.update({'probability_dropdown': x})\n\n    active_check = gradio.Checkbox(value=True, label=\"Compute probabilities and perplexity scores\", info=\"Activate this extension. Note that this extension currently does not work with exllama or llama.cpp.\")\n    color_by_ppl_check = gradio.Checkbox(value=False, label=\"Color by perplexity\", info=\"Higher perplexity is more red. If also showing probability, higher perplexity has more blue component.\")\n    color_by_prob_check = gradio.Checkbox(value=False, label=\"Color by probability\", info=\"Green-yellow-red linear scale, with 100% green, 50% yellow, 0% red.\")\n    prob_dropdown_check = gradio.Checkbox(value=False, label=\"Probability dropdown\", info=\"Hover over a token to show a dropdown of top token probabilities. Currently slightly buggy with whitespace between tokens.\")\n\n    active_check.change(update_active_check, active_check, None)\n    color_by_ppl_check.change(update_color_by_ppl_check, color_by_ppl_check, None)\n    color_by_prob_check.change(update_color_by_prob_check, color_by_prob_check, None)\n    prob_dropdown_check.change(update_prob_dropdown_check, prob_dropdown_check, None)\n", "extensions/example/script.py": "\"\"\"\nAn example of extension. It does nothing, but you can add transformations\nbefore the return statements to customize the webui behavior.\n\nStarting from history_modifier and ending in output_modifier, the\nfunctions are declared in the same order that they are called at\ngeneration time.\n\"\"\"\n\nimport gradio as gr\nimport torch\nfrom transformers import LogitsProcessor\n\nfrom modules import chat, shared\nfrom modules.text_generation import (\n    decode,\n    encode,\n    generate_reply,\n)\n\nparams = {\n    \"display_name\": \"Example Extension\",\n    \"is_tab\": False,\n}\n\nclass MyLogits(LogitsProcessor):\n    \"\"\"\n    Manipulates the probabilities for the next token before it gets sampled.\n    Used in the logits_processor_modifier function below.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def __call__(self, input_ids, scores):\n        # probs = torch.softmax(scores, dim=-1, dtype=torch.float)\n        # probs[0] /= probs[0].sum()\n        # scores = torch.log(probs / (1 - probs))\n        return scores\n\ndef history_modifier(history):\n    \"\"\"\n    Modifies the chat history.\n    Only used in chat mode.\n    \"\"\"\n    return history\n\ndef state_modifier(state):\n    \"\"\"\n    Modifies the state variable, which is a dictionary containing the input\n    values in the UI like sliders and checkboxes.\n    \"\"\"\n    return state\n\ndef chat_input_modifier(text, visible_text, state):\n    \"\"\"\n    Modifies the user input string in chat mode (visible_text).\n    You can also modify the internal representation of the user\n    input (text) to change how it will appear in the prompt.\n    \"\"\"\n    return text, visible_text\n\ndef input_modifier(string, state, is_chat=False):\n    \"\"\"\n    In default/notebook modes, modifies the whole prompt.\n\n    In chat mode, it is the same as chat_input_modifier but only applied\n    to \"text\", here called \"string\", and not to \"visible_text\".\n    \"\"\"\n    return string\n\ndef bot_prefix_modifier(string, state):\n    \"\"\"\n    Modifies the prefix for the next bot reply in chat mode.\n    By default, the prefix will be something like \"Bot Name:\".\n    \"\"\"\n    return string\n\ndef tokenizer_modifier(state, prompt, input_ids, input_embeds):\n    \"\"\"\n    Modifies the input ids and embeds.\n    Used by the multimodal extension to put image embeddings in the prompt.\n    Only used by loaders that use the transformers library for sampling.\n    \"\"\"\n    return prompt, input_ids, input_embeds\n\ndef logits_processor_modifier(processor_list, input_ids):\n    \"\"\"\n    Adds logits processors to the list, allowing you to access and modify\n    the next token probabilities.\n    Only used by loaders that use the transformers library for sampling.\n    \"\"\"\n    processor_list.append(MyLogits())\n    return processor_list\n\ndef output_modifier(string, state, is_chat=False):\n    \"\"\"\n    Modifies the LLM output before it gets presented.\n\n    In chat mode, the modified version goes into history['visible'],\n    and the original version goes into history['internal'].\n    \"\"\"\n    return string\n\ndef custom_generate_chat_prompt(user_input, state, **kwargs):\n    \"\"\"\n    Replaces the function that generates the prompt from the chat history.\n    Only used in chat mode.\n    \"\"\"\n    result = chat.generate_chat_prompt(user_input, state, **kwargs)\n    return result\n\ndef custom_css():\n    \"\"\"\n    Returns a CSS string that gets appended to the CSS for the webui.\n    \"\"\"\n    return ''\n\ndef custom_js():\n    \"\"\"\n    Returns a javascript string that gets appended to the javascript\n    for the webui.\n    \"\"\"\n    return ''\n\ndef setup():\n    \"\"\"\n    Gets executed only once, when the extension is imported.\n    \"\"\"\n    pass\n\ndef ui():\n    \"\"\"\n    Gets executed when the UI is drawn. Custom gradio elements and\n    their corresponding event handlers should be defined here.\n\n    To learn about gradio components, check out the docs:\n    https://gradio.app/docs/\n    \"\"\"\n    pass\n", "extensions/google_translate/script.py": "import html\n\nimport gradio as gr\nfrom deep_translator import GoogleTranslator\n\nparams = {\n    \"activate\": True,\n    \"language string\": \"ja\",\n}\n\nlanguage_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n\n\ndef input_modifier(string):\n    \"\"\"\n    This function is applied to your text inputs before\n    they are fed into the model.\n    \"\"\"\n    if not params['activate']:\n        return string\n\n    return GoogleTranslator(source=params['language string'], target='en').translate(string)\n\n\ndef output_modifier(string):\n    \"\"\"\n    This function is applied to the model outputs.\n    \"\"\"\n    if not params['activate']:\n        return string\n\n    translated_str = GoogleTranslator(source='en', target=params['language string']).translate(html.unescape(string))\n    return html.escape(translated_str)\n\n\ndef bot_prefix_modifier(string):\n    \"\"\"\n    This function is only applied in chat mode. It modifies\n    the prefix text for the Bot and can be used to bias its\n    behavior.\n    \"\"\"\n\n    return string\n\n\ndef ui():\n    # Finding the language name from the language code to use as the default value\n    language_name = list(language_codes.keys())[list(language_codes.values()).index(params['language string'])]\n\n    # Gradio elements\n    with gr.Row():\n        activate = gr.Checkbox(value=params['activate'], label='Activate translation')\n\n    with gr.Row():\n        language = gr.Dropdown(value=language_name, choices=[k for k in language_codes], label='Language')\n\n    # Event functions to update the parameters in the backend\n    activate.change(lambda x: params.update({\"activate\": x}), activate, None)\n    language.change(lambda x: params.update({\"language string\": language_codes[x]}), language, None)\n", "extensions/multimodal/pipeline_loader.py": "import traceback\nfrom importlib import import_module\nfrom pathlib import Path\nfrom typing import Tuple\n\nfrom extensions.multimodal.abstract_pipeline import AbstractMultimodalPipeline\nfrom modules import shared\nfrom modules.logging_colors import logger\n\n\ndef _get_available_pipeline_modules():\n    pipeline_path = Path(__file__).parent / 'pipelines'\n    modules = [p for p in pipeline_path.iterdir() if p.is_dir()]\n    return [m.name for m in modules if (m / 'pipelines.py').exists()]\n\n\ndef load_pipeline(params: dict) -> Tuple[AbstractMultimodalPipeline, str]:\n    pipeline_modules = {}\n    available_pipeline_modules = _get_available_pipeline_modules()\n    for name in available_pipeline_modules:\n        try:\n            pipeline_modules[name] = import_module(f'extensions.multimodal.pipelines.{name}.pipelines')\n        except:\n            logger.warning(f'Failed to get multimodal pipelines from {name}')\n            logger.warning(traceback.format_exc())\n\n    if shared.args.multimodal_pipeline is not None:\n        for k in pipeline_modules:\n            if hasattr(pipeline_modules[k], 'get_pipeline'):\n                pipeline = getattr(pipeline_modules[k], 'get_pipeline')(shared.args.multimodal_pipeline, params)\n                if pipeline is not None:\n                    return (pipeline, k)\n    else:\n        model_name = shared.args.model.lower()\n        for k in pipeline_modules:\n            if hasattr(pipeline_modules[k], 'get_pipeline_from_model_name'):\n                pipeline = getattr(pipeline_modules[k], 'get_pipeline_from_model_name')(model_name, params)\n                if pipeline is not None:\n                    return (pipeline, k)\n\n    available = []\n    for k in pipeline_modules:\n        if hasattr(pipeline_modules[k], 'available_pipelines'):\n            pipelines = getattr(pipeline_modules[k], 'available_pipelines')\n            available += pipelines\n\n    if shared.args.multimodal_pipeline is not None:\n        log = f'Multimodal - ERROR: Failed to load multimodal pipeline \"{shared.args.multimodal_pipeline}\", available pipelines are: {available}.'\n    else:\n        log = f'Multimodal - ERROR: Failed to determine multimodal pipeline for model {shared.args.model}, please select one manually using --multimodal-pipeline [PIPELINE]. Available pipelines are: {available}.'\n    logger.critical(f'{log} Please specify a correct pipeline, or disable the extension')\n    raise RuntimeError(f'{log} Please specify a correct pipeline, or disable the extension')\n", "extensions/multimodal/abstract_pipeline.py": "from abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nimport torch\nfrom PIL import Image\nfrom transformers import is_torch_xpu_available\n\n\nclass AbstractMultimodalPipeline(ABC):\n    @staticmethod\n    @abstractmethod\n    def name() -> str:\n        'name of the pipeline, should be same as in --multimodal-pipeline'\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def image_start() -> Optional[str]:\n        'return image start string, string representation of image start token, or None if not applicable'\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def image_end() -> Optional[str]:\n        'return image end string, string representation of image end token, or None if not applicable'\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def placeholder_token_id() -> int:\n        'return placeholder token id'\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def num_image_embeds() -> int:\n        'return the number of embeds used by a single image (for example: 256 for LLaVA)'\n        pass\n\n    @abstractmethod\n    def embed_images(self, images: List[Image.Image]) -> torch.Tensor:\n        'forward the images through vision pipeline, and return their embeddings'\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def embed_tokens(input_ids: torch.Tensor) -> torch.Tensor:\n        'embed tokens, the exact function varies by LLM, for LLaMA it is `shared.model.model.embed_tokens`'\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def placeholder_embeddings() -> torch.Tensor:\n        'get placeholder embeddings if there are multiple images, and `add_all_images_to_prompt` is False'\n        pass\n\n    def _get_device(self, setting_name: str, params: dict):\n        if params[setting_name] is None:\n            return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"xpu:0\" if is_torch_xpu_available() else \"cpu\")\n        return torch.device(params[setting_name])\n\n    def _get_dtype(self, setting_name: str, params: dict):\n        return torch.float32 if int(params[setting_name]) == 32 else torch.float16\n", "extensions/multimodal/multimodal_embedder.py": "import base64\nimport re\nfrom dataclasses import dataclass\nfrom io import BytesIO\nfrom typing import Any, List, Optional\n\nimport torch\nfrom PIL import Image\n\nfrom extensions.multimodal.pipeline_loader import load_pipeline\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.text_generation import encode, get_max_prompt_length\n\n\n@dataclass\nclass PromptPart:\n    text: str\n    image: Optional[Image.Image] = None\n    is_image: bool = False\n    input_ids: Optional[torch.Tensor] = None\n    embedding: Optional[torch.Tensor] = None\n\n\nclass MultimodalEmbedder:\n    def __init__(self, params: dict):\n        pipeline, source = load_pipeline(params)\n        self.pipeline = pipeline\n        logger.info(f'Multimodal: loaded pipeline {self.pipeline.name()} from pipelines/{source} ({self.pipeline.__class__.__name__})')\n\n    def _split_prompt(self, prompt: str, load_images: bool = False) -> List[PromptPart]:\n        \"\"\"Splits a prompt into a list of `PromptParts` to separate image data from text.\n        It will also append `image_start` and `image_end` before and after the image, and optionally parse and load the images,\n        if `load_images` is `True`.\n        \"\"\"\n        parts: List[PromptPart] = []\n        curr = 0\n        while True:\n            match = re.search(r'<img src=\"data:image/jpeg;base64,([A-Za-z0-9+/=]+)\">', prompt[curr:])\n            if match is None:\n                # no more image tokens, append the rest of the prompt\n                if curr > 0:\n                    # add image end token after last image\n                    parts.append(PromptPart(text=self.pipeline.image_end() + prompt[curr:]))\n                else:\n                    parts.append(PromptPart(text=prompt))\n                break\n            # found an image, append image start token to the text\n            if match.start() > 0:\n                parts.append(PromptPart(text=prompt[curr:curr + match.start()] + self.pipeline.image_start()))\n            else:\n                parts.append(PromptPart(text=self.pipeline.image_start()))\n            # append the image\n            parts.append(PromptPart(\n                text=match.group(0),\n                image=Image.open(BytesIO(base64.b64decode(match.group(1)))) if load_images else None,\n                is_image=True\n            ))\n            curr += match.end()\n        return parts\n\n    def _len_in_tokens_prompt_parts(self, parts: List[PromptPart]) -> int:\n        \"\"\"Total length in tokens of all `parts`\"\"\"\n        tokens = 0\n        for part in parts:\n            if part.is_image:\n                tokens += self.pipeline.num_image_embeds()\n            elif part.input_ids is not None:\n                tokens += len(part.input_ids)\n            else:\n                tokens += len(encode(part.text)[0])\n        return tokens\n\n    def len_in_tokens(self, prompt: str) -> int:\n        \"\"\"Total length in tokens for a given text `prompt`\"\"\"\n        parts = self._split_prompt(prompt, False)\n        return self._len_in_tokens_prompt_parts(parts)\n\n    def _encode_single_text(self, part: PromptPart, add_bos_token: bool) -> PromptPart:\n        \"\"\"Encode a single prompt `part` to `input_ids`. Returns a `PromptPart`\"\"\"\n        if part.is_image:\n            placeholders = torch.ones((self.pipeline.num_image_embeds())) * self.pipeline.placeholder_token_id()\n            part.input_ids = placeholders.to(shared.model.device, dtype=torch.int64)\n        else:\n            part.input_ids = encode(part.text, add_bos_token=add_bos_token)[0].to(shared.model.device, dtype=torch.int64)\n        return part\n\n    @staticmethod\n    def _num_images(parts: List[PromptPart]) -> int:\n        count = 0\n        for part in parts:\n            if part.is_image:\n                count += 1\n        return count\n\n    def _encode_text(self, state, parts: List[PromptPart]) -> List[PromptPart]:\n        \"\"\"Encode text to token_ids, also truncate the prompt, if necessary.\n\n        The chat/instruct mode should make prompts that fit in get_max_prompt_length, but if max_new_tokens are set\n        such that the context + min_rows don't fit, we can get a prompt which is too long.\n        We can't truncate image embeddings, as it leads to broken generation, so remove the images instead and warn the user\n        \"\"\"\n        encoded: List[PromptPart] = []\n        for i, part in enumerate(parts):\n            encoded.append(self._encode_single_text(part, i == 0 and state['add_bos_token']))\n\n        # truncation:\n        max_len = get_max_prompt_length(state)\n        removed_images = 0\n\n        # 1. remove entire text/image blocks\n        while self._len_in_tokens_prompt_parts(encoded[1:]) > max_len:\n            if encoded[0].is_image:\n                removed_images += 1\n            encoded = encoded[1:]\n\n        # 2. check if the last prompt part doesn't need to get truncated\n        if self._len_in_tokens_prompt_parts(encoded) > max_len:\n            if encoded[0].is_image:\n                # don't truncate image embeddings, just remove the image, otherwise generation will be broken\n                removed_images += 1\n                encoded = encoded[1:]\n            elif len(encoded) > 1 and encoded[0].text.endswith(self.pipeline.image_start()):\n                # see if we can keep image_start token\n                len_image_start = len(encode(self.pipeline.image_start(), add_bos_token=state['add_bos_token'])[0])\n                if self._len_in_tokens_prompt_parts(encoded[1:]) + len_image_start > max_len:\n                    # we can't -> remove this text, and the image\n                    encoded = encoded[2:]\n                    removed_images += 1\n                else:\n                    # we can -> just truncate the text\n                    trunc_len = self._len_in_tokens_prompt_parts(encoded) - max_len\n                    encoded[0].input_ids = encoded[0].input_ids[trunc_len:]\n            elif len(encoded) > 0:\n                # only one text left, truncate it normally\n                trunc_len = self._len_in_tokens_prompt_parts(encoded) - max_len\n                encoded[0].input_ids = encoded[0].input_ids[trunc_len:]\n\n        # notify user if we truncated an image\n        if removed_images > 0:\n            logger.warning(f\"Multimodal: removed {removed_images} image(s) from prompt. Try decreasing max_new_tokens if generation is broken\")\n\n        return encoded\n\n    def _embed(self, parts: List[PromptPart]) -> List[PromptPart]:\n        # batch images\n        image_indicies = [i for i, part in enumerate(parts) if part.is_image]\n        embedded = self.pipeline.embed_images([parts[i].image for i in image_indicies])\n        for i, embeds in zip(image_indicies, embedded):\n            parts[i].embedding = embeds\n        # embed text\n        for (i, part) in enumerate(parts):\n            if not part.is_image:\n                parts[i].embedding = self.pipeline.embed_tokens(part.input_ids)\n        return parts\n\n    def _remove_old_images(self, parts: List[PromptPart], params: dict) -> List[PromptPart]:\n        if params['add_all_images_to_prompt']:\n            return parts\n        already_added = False\n        for i, part in reversed(list(enumerate(parts))):\n            if part.is_image:\n                if already_added:\n                    parts[i].embedding = self.pipeline.placeholder_embeddings()\n                else:\n                    already_added = True\n        return parts\n\n    def forward(self, prompt: str, state: Any, params: dict):\n        prompt_parts = self._split_prompt(prompt, True)\n        prompt_parts = self._encode_text(state, prompt_parts)\n        prompt_parts = self._embed(prompt_parts)\n        prompt_parts = self._remove_old_images(prompt_parts, params)\n        embeds = tuple(part.embedding for part in prompt_parts)\n        ids = tuple(part.input_ids for part in prompt_parts)\n        input_embeds = torch.cat(embeds, dim=0)\n        input_ids = torch.cat(ids, dim=0)\n        return prompt, input_ids, input_embeds, self._num_images(prompt_parts)\n", "extensions/multimodal/script.py": "import base64\nimport re\nimport time\nfrom functools import partial\nfrom io import BytesIO\n\nimport gradio as gr\nimport torch\n\nfrom extensions.multimodal.multimodal_embedder import MultimodalEmbedder\nfrom modules import shared\nfrom modules.logging_colors import logger\n\nparams = {\n    \"add_all_images_to_prompt\": False,\n    # device to run vision encoder on\n    \"vision_device\": None,\n    # bits to load vision encoder in, either 16 or 32\n    \"vision_bits\": 32,\n    # device to run multimodal projector on\n    \"projector_device\": None,\n    # multimodal projector bits, either 32 or 16\n    \"projector_bits\": 32\n}\n\n\n# If 'state' is True, will hijack the next chat generation\ninput_hijack = {\n    'state': False,\n    'value': [\"\", \"\"]\n}\n\n\n# initialized in ui, so that params are loaded from settings\nmultimodal_embedder: MultimodalEmbedder = None\n\n\ndef chat_input_modifier(text, visible_text, state):\n    global input_hijack\n    if input_hijack['state']:\n        input_hijack['state'] = False\n        return input_hijack['value'](text, visible_text)\n    else:\n        return text, visible_text\n\n\ndef add_chat_picture(picture, text, visible_text):\n    # resize the image, so that shortest edge is at least 224 (size for CLIP), and at most 300 (to keep history manageable)\n    # Adjusted to 336 for the values here, due to the increased resolution in llava-v1.5\n    max_hw, min_hw = max(picture.size), min(picture.size)\n    aspect_ratio = max_hw / min_hw\n    shortest_edge = int(max(336 / aspect_ratio, 336))\n    longest_edge = int(shortest_edge * aspect_ratio)\n    w = shortest_edge if picture.width < picture.height else longest_edge\n    h = shortest_edge if picture.width >= picture.height else longest_edge\n    picture = picture.resize((w, h))\n\n    buffer = BytesIO()\n    picture.save(buffer, format=\"PNG\")\n    img_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n    image = f'<img src=\"data:image/jpeg;base64,{img_str}\">'\n\n    if '<image>' in text:\n        text = text.replace('<image>', image)\n    else:\n        text = image + '\\n' + text\n\n    if visible_text == '' or visible_text is None:\n        visible_text = text\n    elif '<image>' in visible_text:\n        visible_text = visible_text.replace('<image>', image)\n    else:\n        visible_text = visible_text + '\\n' + image\n\n    return text, visible_text\n\n\ndef custom_tokenized_length(prompt):\n    return multimodal_embedder.len_in_tokens(prompt)\n\n\ndef tokenizer_modifier(state, prompt, input_ids, input_embeds):\n    global params\n    start_ts = time.time()\n    image_match = re.search(r'<img src=\"data:image/jpeg;base64,[A-Za-z0-9+/=]+\">', prompt)\n\n    if image_match is None:\n        return prompt, input_ids, input_embeds\n\n    prompt, input_ids, input_embeds, total_embedded = multimodal_embedder.forward(prompt, state, params)\n    logger.info(f'Embedded {total_embedded} image(s) in {time.time()-start_ts:.2f}s')\n    return (prompt,\n            input_ids.unsqueeze(0).to(shared.model.device, dtype=torch.int64),\n            input_embeds.unsqueeze(0).to(shared.model.device, dtype=shared.model.dtype))\n\n\ndef ui():\n    global multimodal_embedder\n    multimodal_embedder = MultimodalEmbedder(params)\n    with gr.Column():\n        picture_select = gr.Image(label='Send a picture', type='pil')\n        # The models don't seem to deal well with multiple images\n        single_image_checkbox = gr.Checkbox(False, label='Embed all images, not only the last one')\n    # Prepare the input hijack\n    picture_select.upload(\n        lambda picture: input_hijack.update({\"state\": True, \"value\": partial(add_chat_picture, picture)}),\n        [picture_select],\n        None\n    )\n    picture_select.clear(lambda: input_hijack.update({\"state\": False, \"value\": [\"\", \"\"]}), None, None)\n    single_image_checkbox.change(lambda x: params.update({\"add_all_images_to_prompt\": x}), single_image_checkbox, None)\n    shared.gradio['Generate'].click(lambda: None, None, picture_select)\n    shared.gradio['textbox'].submit(lambda: None, None, picture_select)\n", "extensions/multimodal/pipelines/llava/llava.py": "import time\nfrom abc import abstractmethod\nfrom typing import List, Tuple\n\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nfrom transformers import CLIPImageProcessor, CLIPVisionModel\n\nfrom extensions.multimodal.abstract_pipeline import AbstractMultimodalPipeline\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.text_generation import encode\n\n\ndef expand2square(pil_img: Image.Image, background_color: Tuple[int]) -> Image.Image:\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\nclass LLaVA_v0_Pipeline(AbstractMultimodalPipeline):\n    CLIP_REPO = \"openai/clip-vit-large-patch14\"\n\n    def __init__(self, params: dict) -> None:\n        super().__init__()\n        self.clip_device = self._get_device(\"vision_device\", params)\n        self.clip_dtype = self._get_dtype(\"vision_bits\", params)\n        self.projector_device = self._get_device(\"projector_device\", params)\n        self.projector_dtype = self._get_dtype(\"projector_bits\", params)\n        self.image_processor, self.vision_tower, self.mm_projector = self._load_models()\n\n    def _load_models(self):\n        start_ts = time.time()\n\n        logger.info(f\"LLaVA - Loading CLIP from {self.CLIP_REPO} as {self.clip_dtype} on {self.clip_device}...\")\n        image_processor = CLIPImageProcessor.from_pretrained(self.CLIP_REPO, torch_dtype=self.clip_dtype)\n        vision_tower = CLIPVisionModel.from_pretrained(self.CLIP_REPO, torch_dtype=self.clip_dtype).to(self.clip_device)\n\n        logger.info(f\"LLaVA - Loading projector from {self.llava_projector_repo()} as {self.projector_dtype} on {self.projector_device}...\")\n        projector_path = hf_hub_download(self.llava_projector_repo(), self.llava_projector_filename())\n        mm_projector = self.build_mm_projector()\n        projector_data = torch.load(projector_path)\n        projector_data = {k[19:]: v for k, v in projector_data.items() if k.startswith('model.mm_projector.')}\n        mm_projector.load_state_dict(projector_data)\n        mm_projector = mm_projector.to(self.projector_device)\n\n        logger.info(f\"LLaVA supporting models loaded, took {time.time() - start_ts:.2f} seconds\")\n        return image_processor, vision_tower, mm_projector\n\n    def build_mm_projector(self) -> torch.nn.Module:\n        projector_shape = self.llava_projector_shape()\n        if len(projector_shape) == 2:\n            return torch.nn.Linear(*projector_shape)\n        else:\n            modules = []\n            modules.append(torch.nn.Linear(projector_shape[0], projector_shape[1]))\n            for i in range(2, len(projector_shape)):\n                modules.append(torch.nn.GELU())\n                modules.append(torch.nn.Linear(projector_shape[i-1], projector_shape[i]))\n            return torch.nn.Sequential(*modules)\n\n    @staticmethod\n    def image_start() -> str:\n        return \"<im_start>\"\n\n    @staticmethod\n    def image_end() -> str:\n        return \"<im_end>\"\n\n    @staticmethod\n    def num_image_embeds() -> int:\n        return 256\n\n    @staticmethod\n    def embed_tokens(input_ids: torch.Tensor) -> torch.Tensor:\n        for attr in ['', 'model', 'model.model', 'model.model.model']:\n            tmp = getattr(shared.model, attr, None) if attr != '' else shared.model\n            if tmp is not None and hasattr(tmp, 'embed_tokens'):\n                func = tmp.embed_tokens\n                break\n        else:\n            raise ValueError('The embed_tokens method has not been found for this loader.')\n\n        return func(input_ids).to(shared.model.device, dtype=shared.model.dtype)\n\n    @staticmethod\n    def placeholder_embeddings() -> torch.Tensor:\n        return LLaVA_v0_Pipeline.embed_tokens(encode(\"<im_patch>\"*256, add_bos_token=False)[0])\n\n    def embed_images(self, images: List[Image.Image]) -> torch.Tensor:\n        images = self.image_processor(images, return_tensors='pt')['pixel_values']\n        images = images.to(self.clip_device, dtype=self.clip_dtype)\n\n        with torch.no_grad():\n            image_forward_outs = self.vision_tower(images, output_hidden_states=True)\n            select_hidden_state_layer = -2\n            select_hidden_state = image_forward_outs.hidden_states[select_hidden_state_layer]\n            image_features = select_hidden_state[:, 1:].to(self.projector_device, dtype=self.projector_dtype)\n            image_features = self.mm_projector(image_features)\n        return image_features.to(shared.model.device, dtype=shared.model.dtype)\n\n    @staticmethod\n    @abstractmethod\n    def llava_projector_repo() -> str:\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def llava_projector_filename() -> str:\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def llava_projector_shape() -> Tuple[int, int]:\n        pass\n\n\nclass LLaVA_v0_13B_Pipeline(LLaVA_v0_Pipeline):\n    def __init__(self, params: dict) -> None:\n        super().__init__(params)\n\n    @staticmethod\n    def name() -> str:\n        return \"llava-13b\"\n\n    @staticmethod\n    def placeholder_token_id() -> int:\n        return 32000\n\n    @staticmethod\n    def llava_projector_shape() -> Tuple[int, int]:\n        return (1024, 5120)\n\n    @staticmethod\n    def llava_projector_filename() -> str:\n        return \"mm_projector.bin\"\n\n    @staticmethod\n    def llava_projector_repo() -> str:\n        return \"liuhaotian/LLaVA-13b-delta-v0\"\n\n\nclass LLaVA_v0_7B_Pipeline(LLaVA_v0_Pipeline):\n    def __init__(self, params: dict) -> None:\n        super().__init__(params)\n\n    @staticmethod\n    def name() -> str:\n        return \"llava-7b\"\n\n    @staticmethod\n    def placeholder_token_id() -> int:\n        return 32001\n\n    @staticmethod\n    def llava_projector_shape() -> Tuple[int, int]:\n        return (1024, 4096)\n\n    @staticmethod\n    def llava_projector_filename() -> str:\n        return \"mm_projector.bin\"\n\n    @staticmethod\n    def llava_projector_repo() -> str:\n        return \"liuhaotian/LLaVA-7b-delta-v0\"\n\n\nclass LLaVA_LLaMA_2_13B_Pipeline(LLaVA_v0_13B_Pipeline):\n    def __init__(self, params: dict) -> None:\n        super().__init__(params)\n\n    @staticmethod\n    def name() -> str:\n        return \"llava-llama-2-13b\"\n\n    @staticmethod\n    def placeholder_token_id() -> int:\n        return 0\n\n    @staticmethod\n    def llava_projector_repo() -> str:\n        return \"liuhaotian/llava-llama-2-13b-chat-lightning-preview\"\n\n    @staticmethod\n    def image_start() -> str:\n        return \"\"\n\n    @staticmethod\n    def image_end() -> str:\n        return \"\"\n\n    @staticmethod\n    def placeholder_embeddings() -> torch.Tensor:\n        return LLaVA_v0_Pipeline.embed_tokens(encode(\"<unk>\"*256, add_bos_token=False)[0])\n\n\nclass LLaVA_v1_5_13B_Pipeline(LLaVA_v0_13B_Pipeline):\n    CLIP_REPO = \"openai/clip-vit-large-patch14-336\"\n\n    def __init__(self, params: dict) -> None:\n        super().__init__(params)\n\n    @staticmethod\n    def name() -> str:\n        return \"llava-v1.5-13b\"\n\n    @staticmethod\n    def llava_projector_shape() -> Tuple[int, int]:\n        return (1024, 5120, 5120)\n\n    @staticmethod\n    def placeholder_token_id() -> int:\n        return 0\n\n    @staticmethod\n    def llava_projector_repo() -> str:\n        return \"liuhaotian/llava-v1.5-13b\"\n\n    @staticmethod\n    def image_start() -> str:\n        return \"\"\n\n    @staticmethod\n    def image_end() -> str:\n        return \"\"\n\n    @staticmethod\n    def num_image_embeds() -> int:\n        return 576\n\n    def embed_images(self, images: List[Image.Image]) -> torch.Tensor:\n        # pad it to square first\n        images = [\n            expand2square(image, tuple(int(x*255) for x in self.image_processor.image_mean))\n            for image in images\n        ]\n        return super().embed_images(images)\n\n    @staticmethod\n    def placeholder_embeddings() -> torch.Tensor:\n        return LLaVA_v0_Pipeline.embed_tokens(encode(\"<unk>\"*576, add_bos_token=False)[0])\n\nclass LLaVA_v1_5_7B_Pipeline(LLaVA_v1_5_13B_Pipeline):\n    @staticmethod\n    def name() -> str:\n        return \"llava-v1.5-7b\"\n\n    @staticmethod\n    def llava_projector_shape() -> Tuple[int, int]:\n        return (1024, 4096, 4096)\n    @staticmethod\n    def llava_projector_repo() -> str:\n        return \"liuhaotian/llava-v1.5-7b\"", "extensions/multimodal/pipelines/llava/pipelines.py": "from typing import Optional\n\nfrom extensions.multimodal.abstract_pipeline import AbstractMultimodalPipeline\n\navailable_pipelines = ['llava-7b', 'llava-13b', 'llava-llama-2-13b', 'llava-v1.5-13b', 'llava-v1.5-7b']\n\n\ndef get_pipeline(name: str, params: dict) -> Optional[AbstractMultimodalPipeline]:\n    if name == 'llava-7b':\n        from .llava import LLaVA_v0_7B_Pipeline\n        return LLaVA_v0_7B_Pipeline(params)\n    if name == 'llava-13b':\n        from .llava import LLaVA_v0_13B_Pipeline\n        return LLaVA_v0_13B_Pipeline(params)\n    if name == 'llava-llama-2-13b':\n        from .llava import LLaVA_LLaMA_2_13B_Pipeline\n        return LLaVA_LLaMA_2_13B_Pipeline(params)\n    if name == 'llava-v1.5-7b':\n        from .llava import LLaVA_v1_5_7B_Pipeline\n        return LLaVA_v1_5_7B_Pipeline(params)\n    if name == 'llava-v1.5-13b':\n        from .llava import LLaVA_v1_5_13B_Pipeline\n        return LLaVA_v1_5_13B_Pipeline(params)\n    return None\n\n\ndef get_pipeline_from_model_name(model_name: str, params: dict) -> Optional[AbstractMultimodalPipeline]:\n    if 'llava' not in model_name.lower():\n        return None\n    if 'llama-2' in model_name.lower():\n        if '13b' in model_name.lower():\n            from .llava import LLaVA_LLaMA_2_13B_Pipeline\n            return LLaVA_LLaMA_2_13B_Pipeline(params)\n    elif 'llava-v1.5' in model_name.lower():\n        if '13b' in model_name.lower():\n            from .llava import LLaVA_v1_5_13B_Pipeline\n            return LLaVA_v1_5_13B_Pipeline(params)\n        if '7b' in model_name.lower():\n            from .llava import LLaVA_v1_5_7B_Pipeline\n            return LLaVA_v1_5_7B_Pipeline(params)\n    else:\n        if '7b' in model_name.lower():\n            from .llava import LLaVA_v0_7B_Pipeline\n            return LLaVA_v0_7B_Pipeline(params)\n        if '13b' in model_name.lower():\n            from .llava import LLaVA_v0_13B_Pipeline\n            return LLaVA_v0_13B_Pipeline(params)\n    return None\n", "modules/evaluate.py": "import datetime\nfrom pathlib import Path\n\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.models import clear_torch_cache, load_model, unload_model\nfrom modules.models_settings import get_model_metadata, update_model_parameters\nfrom modules.text_generation import encode\n\n\ndef load_past_evaluations():\n    if Path('logs/evaluations.csv').exists():\n        df = pd.read_csv(Path('logs/evaluations.csv'), dtype=str)\n        df['Perplexity'] = pd.to_numeric(df['Perplexity'])\n        return df\n    else:\n        return pd.DataFrame(columns=['Model', 'LoRAs', 'Dataset', 'Perplexity', 'stride', 'max_length', 'Date', 'Comment'])\n\n\npast_evaluations = load_past_evaluations()\n\n\ndef save_past_evaluations(df):\n    global past_evaluations\n    past_evaluations = df\n    filepath = Path('logs/evaluations.csv')\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n    df.to_csv(filepath, index=False)\n\n\ndef calculate_perplexity(models, input_dataset, stride, _max_length):\n    '''\n    Based on:\n    https://huggingface.co/docs/transformers/perplexity#calculating-ppl-with-fixedlength-models\n    '''\n\n    if shared.args.loader == \"llama.cpp\":\n        logger.error(\"llamacpp_HF is required for perplexity evaluation with GGUF models. Please reload the model with llamacpp_HF instead of llama.cpp.\")\n        raise ValueError\n\n    if shared.args.loader == \"ExLlamav2\":\n        logger.error(\"ExLlamav2_HF is required for perplexity evaluation with EXL2 models. Please reload the model with ExLlamav2_HF instead of ExLlamav2.\")\n        raise ValueError\n\n    if shared.args.loader == \"llamacpp_HF\" and not shared.args.logits_all:\n        logger.error(\"--logits_all is required for perplexity evaluation with GGUF models. Please reload the model with that option set/checked.\")\n        raise ValueError\n\n    if not shared.args.no_use_fast:\n        logger.warning(\"--no_use_fast is not set. If tokenizing the input dataset takes a long time, try reloading the model with that option set/checked.\")\n\n    global past_evaluations\n    cumulative_log = ''\n    cumulative_log += \"Loading the input dataset...\\n\\n\"\n    yield cumulative_log\n\n    # Copied from https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/triton/utils/datautils.py\n    if input_dataset == 'wikitext':\n        data = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n        text = \"\\n\\n\".join(data['text'])\n    elif input_dataset == 'ptb':\n        data = load_dataset('ptb_text_only', 'penn_treebank', split='validation')\n        text = \"\\n\\n\".join(data['sentence'])\n    elif input_dataset == 'ptb_new':\n        data = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n        text = \" \".join(data['sentence'])\n    else:\n        with open(Path(f'training/datasets/{input_dataset}.txt'), 'r', encoding='utf-8') as f:\n            text = f.read()\n\n    for model in models:\n        if is_in_past_evaluations(model, input_dataset, stride, _max_length):\n            cumulative_log += f\"`{model}` has already been tested. Ignoring.\\n\\n\"\n            yield cumulative_log\n            continue\n\n        if model != 'current model':\n            try:\n                yield cumulative_log + f\"Loading `{model}`...\\n\\n\"\n                model_settings = get_model_metadata(model)\n                shared.settings.update({k: v for k, v in model_settings.items() if k in shared.settings})  # hijacking the interface defaults\n                update_model_parameters(model_settings)  # hijacking the command-line arguments\n                unload_model()\n                shared.model, shared.tokenizer = load_model(model)\n            except:\n                cumulative_log += f\"Failed to load `{model}`. Moving on.\\n\\n\"\n                yield cumulative_log\n                continue\n\n        cumulative_log += f\"Processing `{shared.model_name}`...\\n\\n\"\n        yield cumulative_log + \"Tokenizing the input dataset...\\n\\n\"\n        encodings = encode(text, add_special_tokens=False)\n        seq_len = encodings.shape[1]\n        if _max_length:\n            max_length = _max_length\n        elif hasattr(shared.model.config, 'max_position_embeddings'):\n            max_length = shared.model.config.max_position_embeddings\n        else:\n            max_length = 2048\n\n        nlls = []\n        prev_end_loc = 0\n        for begin_loc in tqdm(range(0, seq_len, stride)):\n            yield cumulative_log + f\"Evaluating... {100*begin_loc/seq_len:.2f}%\"\n            end_loc = min(begin_loc + max_length, seq_len)\n            trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n            input_ids = encodings[:, begin_loc:end_loc]\n            target_ids = input_ids.clone()\n            target_ids[:, :-trg_len] = -100\n            clear_torch_cache()\n            with torch.no_grad():\n                outputs = shared.model(input_ids=input_ids, labels=target_ids)\n\n                # loss is calculated using CrossEntropyLoss which averages over valid labels\n                # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n                # to the left by 1.\n                neg_log_likelihood = outputs.loss\n\n            nlls.append(neg_log_likelihood)\n            prev_end_loc = end_loc\n            if end_loc == seq_len:\n                break\n\n        ppl = torch.exp(torch.stack(nlls).mean())\n\n        add_entry_to_past_evaluations(float(ppl), shared.model_name, input_dataset, stride, _max_length)\n        save_past_evaluations(past_evaluations)\n\n        message = f\"The perplexity for `{shared.model_name}` is: {float(ppl)}\"\n        logger.info(message)\n\n        cumulative_log += f\"{message}\\n\\n\"\n        yield cumulative_log\n\n\ndef add_entry_to_past_evaluations(perplexity, model, dataset, stride, max_length):\n    global past_evaluations\n    entry = {\n        'Model': model,\n        'LoRAs': ', '.join(shared.lora_names) or '-',\n        'Dataset': dataset,\n        'Perplexity': perplexity,\n        'stride': str(stride),\n        'max_length': str(max_length),\n        'Date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'Comment': ''\n    }\n    past_evaluations = pd.concat([past_evaluations, pd.DataFrame([entry])], ignore_index=True)\n\n\ndef is_in_past_evaluations(model, dataset, stride, max_length):\n    entries = past_evaluations[(past_evaluations['Model'] == model) &\n                               (past_evaluations['Dataset'] == dataset) &\n                               (past_evaluations['max_length'] == str(max_length)) &\n                               (past_evaluations['stride'] == str(stride))]\n\n    if entries.shape[0] > 0:\n        return True\n    else:\n        return False\n\n\ndef generate_markdown_table():\n    sorted_df = past_evaluations.sort_values(by=['Dataset', 'stride', 'Perplexity', 'Date'])\n    return sorted_df\n", "modules/training.py": "import os\n\nos.environ[\"WANDB_MODE\"] = \"offline\"\n# os.environ[\"WANDB_DISABLED\"] = \"true\"\n\nimport json\nimport math\nimport random\nimport shutil\nimport sys\nimport threading\nimport time\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport gradio as gr\nimport torch\nimport transformers\nfrom datasets import Dataset, load_dataset\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    set_peft_model_state_dict\n)\nfrom peft.utils.other import \\\n    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING as model_to_lora_modules\nfrom transformers import is_torch_xpu_available\nfrom transformers.models.auto.modeling_auto import (\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n)\n\nfrom modules import shared, ui, utils\nfrom modules.evaluate import (\n    calculate_perplexity,\n    generate_markdown_table,\n    save_past_evaluations\n)\nfrom modules.logging_colors import logger\nfrom modules.models import reload_model\nfrom modules.utils import natural_keys\n\nMODEL_CLASSES = {v[1]: v[0] for v in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.items()}\nPARAMETERS = [\"lora_name\", \"always_override\", \"q_proj_en\", \"v_proj_en\", \"k_proj_en\", \"o_proj_en\", \"gate_proj_en\", \"down_proj_en\", \"up_proj_en\", \"save_steps\", \"micro_batch_size\", \"batch_size\", \"epochs\", \"learning_rate\", \"lr_scheduler_type\", \"lora_rank\", \"lora_alpha\", \"lora_dropout\", \"cutoff_len\", \"dataset\", \"eval_dataset\", \"format\", \"eval_steps\", \"raw_text_file\", \"overlap_len\", \"newline_favor_len\", \"higher_rank_limit\", \"warmup_steps\", \"optimizer\", \"hard_cut_string\", \"train_only_after\", \"stop_at_loss\", \"add_eos_token\", \"min_chars\", \"report_to\"]\nWANT_INTERRUPT = False\n\ntrain_log = {}\ntrain_template = {}\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n    with gr.Tab(\"Training\", elem_id=\"training-tab\"):\n        with gr.Tab('Train LoRA', elem_id='lora-train-tab'):\n            tmp = gr.State('')\n            with gr.Row():\n                with gr.Column():\n                    gr.Markdown(\"[Tutorial](https://github.com/oobabooga/text-generation-webui/wiki/05-%E2%80%90-Training-Tab)\")\n\n                    with gr.Row():\n                        copy_from = gr.Dropdown(label='Copy parameters from', value='None', choices=utils.get_available_loras(), elem_classes=['slim-dropdown'], interactive=not mu)\n                        ui.create_refresh_button(copy_from, lambda: None, lambda: {'choices': utils.get_available_loras()}, 'refresh-button', interactive=not mu)\n\n                    with gr.Row():\n                        with gr.Column(scale=5):\n                            lora_name = gr.Textbox(label='Name', info='The name of your new LoRA file')\n                        with gr.Column():\n                            always_override = gr.Checkbox(label='Override Existing Files', value=False, info='If the name is the same, checking will replace the existing file, and unchecking will load and continue from it (the rank must be the same).', elem_classes=['no-background'])\n\n                    with gr.Accordion(label='Target Modules', open=False):\n                        gr.Markdown(\"Selects which modules to target in training. Targeting more modules is closer to a full fine-tune at the cost of increased VRAM requirements and adapter size.\\nNOTE: Only works for model_id='llama', other types will retain default training behavior and not use these settings.\")\n                        with gr.Row():\n                            with gr.Column():\n                                q_proj_en = gr.Checkbox(label='Enable q_proj', value=True)\n                            with gr.Column():\n                                v_proj_en = gr.Checkbox(label='Enable v_proj', value=True)\n                            with gr.Column():\n                                k_proj_en = gr.Checkbox(label='Enable k_proj', value=False)\n                            with gr.Column():\n                                o_proj_en = gr.Checkbox(label='Enable o_proj', value=False)\n                            with gr.Column():\n                                gate_proj_en = gr.Checkbox(label='Enable gate_proj', value=False)\n                            with gr.Column():\n                                down_proj_en = gr.Checkbox(label='Enable down_proj', value=False)\n                            with gr.Column():\n                                up_proj_en = gr.Checkbox(label='Enable up_proj', value=False)\n\n                    with gr.Row():\n                        with gr.Column():\n                            lora_rank = gr.Slider(label='LoRA Rank', value=32, minimum=0, maximum=1024, step=4, info='Also called dimension count. Higher values = larger file, more content control. Smaller values = smaller file, less control. Use 4 or 8 for style, 128 or 256 to teach, 1024+ for fine-detail on big data. More VRAM is needed for higher ranks.')\n                            lora_alpha = gr.Slider(label='LoRA Alpha', value=64, minimum=0, maximum=2048, step=4, info='This divided by the rank becomes the scaling of the LoRA. Higher means stronger. A good standard value is twice your Rank.')\n                            batch_size = gr.Slider(label='Batch Size', value=128, minimum=0, maximum=1024, step=4, info='Global batch size. The two batch sizes together determine gradient accumulation (gradientAccum = batch / microBatch). Higher gradient accum values lead to better quality training.')\n                            micro_batch_size = gr.Slider(label='Micro Batch Size', value=4, minimum=1, maximum=128, step=1, info='Per-device batch size (NOTE: multiple devices not yet implemented). Increasing this will increase VRAM usage.')\n                            cutoff_len = gr.Slider(label='Cutoff Length', minimum=0, maximum=4096, value=256, step=32, info='Cutoff length for text input. Essentially, how long of a line of text to feed in at a time. Higher values require drastically more VRAM.')\n\n                        with gr.Column():\n                            save_steps = gr.Number(label='Save every n steps', value=0, info='If above 0, a checkpoint of the LoRA will be saved every time this many steps pass.')\n\n                            epochs = gr.Number(label='Epochs', value=3, info='Number of times every entry in the dataset should be fed into training. So 1 means feed each item in once, 5 means feed it in five times, etc.')\n                            learning_rate = gr.Textbox(label='Learning Rate', value='3e-4', info='In scientific notation. 3e-4 is a good starting base point. 1e-2 is extremely high, 1e-6 is extremely low.')\n                            with gr.Row():\n                                lr_scheduler_type = gr.Dropdown(label='LR Scheduler', value='linear', choices=['linear', 'constant', 'constant_with_warmup', 'cosine', 'cosine_with_restarts', 'polynomial', 'inverse_sqrt'], info='Learning rate scheduler - defines how the learning rate changes over time. \"Constant\" means never change, \"linear\" means to go in a straight line from the learning rate down to 0, cosine follows a curve, etc.', elem_classes=['slim-dropdown'])\n\n                    with gr.Accordion(label='Advanced Options', open=False):\n                        with gr.Row():\n                            with gr.Column():\n                                lora_dropout = gr.Slider(label='LoRA Dropout', minimum=0.0, maximum=1.0, step=0.025, value=0.05, info='Percentage probability for dropout of LoRA layers. This can help reduce overfitting. Most users should leave at default.')\n                                stop_at_loss = gr.Slider(label='Stop at loss', minimum=0.0, maximum=3.0, step=0.1, value=0.00, info='The process will automatically stop once the desired loss value is reached. (reasonable numbers are 1.5-1.8)')\n                                with gr.Row():\n                                    optimizer = gr.Dropdown(label='Optimizer', value='adamw_torch', choices=['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'adamw_anyprecision', 'sgd', 'adagrad'], info='Different optimizer implementation options, for advanced users. Effects of different options are not well documented yet.', elem_classes=['slim-dropdown'])\n\n                            with gr.Column():\n                                warmup_steps = gr.Number(label='Warmup Steps', value=100, info='For this many steps at the start, the learning rate will be lower than normal. This helps the trainer prepare the model and precompute statistics to improve the quality of training after the start.')\n                                train_only_after = gr.Textbox(label='Train Only After', value='', info='Only consider text *after* this string in any given chunk for training. For Alpaca datasets, use \"### Response:\" to only train the response and ignore the input.')\n\n                                add_eos_token = gr.Checkbox(label='Add EOS token', value=False, info=\"Adds EOS token for each dataset item. In case of raw text, the EOS will be added at the Hard Cut\")\n\n                                higher_rank_limit = gr.Checkbox(label='Enable higher ranks', value=False, info='If checked, changes Rank/Alpha slider above to go much higher. This will not work without a datacenter-class GPU.')\n                                report_to = gr.Radio(label=\"Save detailed logs with\", value=\"None\", choices=[\"None\", \"wandb\", \"tensorboard\"], interactive=True)\n\n                with gr.Column():\n                    with gr.Tab(label='Formatted Dataset'):\n                        with gr.Row():\n                            format = gr.Dropdown(choices=utils.get_datasets('training/formats', 'json'), value='None', label='Data Format', info='The format file used to decide how to format the dataset input.', elem_classes=['slim-dropdown'], interactive=not mu)\n                            ui.create_refresh_button(format, lambda: None, lambda: {'choices': utils.get_datasets('training/formats', 'json')}, 'refresh-button', interactive=not mu)\n\n                        with gr.Row():\n                            dataset = gr.Dropdown(choices=utils.get_datasets('training/datasets', 'json'), value='None', label='Dataset', info='The dataset file to use for training.', elem_classes=['slim-dropdown'], interactive=not mu)\n                            ui.create_refresh_button(dataset, lambda: None, lambda: {'choices': utils.get_datasets('training/datasets', 'json')}, 'refresh-button', interactive=not mu)\n\n                        with gr.Row():\n                            eval_dataset = gr.Dropdown(choices=utils.get_datasets('training/datasets', 'json'), value='None', label='Evaluation Dataset', info='The (optional) dataset file used to evaluate the model after training.', elem_classes=['slim-dropdown'], interactive=not mu)\n                            ui.create_refresh_button(eval_dataset, lambda: None, lambda: {'choices': utils.get_datasets('training/datasets', 'json')}, 'refresh-button', interactive=not mu)\n\n                        eval_steps = gr.Number(label='Evaluate every n steps', value=100, info='If an evaluation dataset is given, test it every time this many steps pass.')\n\n                    with gr.Tab(label=\"Raw text file\"):\n                        with gr.Row():\n                            raw_text_file = gr.Dropdown(choices=utils.get_datasets('training/datasets', 'txt'), value='None', label='Text file', info='The raw text file to use for training.', elem_classes=['slim-dropdown'], interactive=not mu)\n                            ui.create_refresh_button(raw_text_file, lambda: None, lambda: {'choices': utils.get_datasets('training/datasets', 'txt')}, 'refresh-button', interactive=not mu)\n\n                        with gr.Row():\n                            with gr.Column():\n                                overlap_len = gr.Slider(label='Overlap Length', minimum=0, maximum=512, value=128, step=16, info='How many tokens from the prior chunk of text to include into the next chunk. (The chunks themselves will be of a size determined by Cutoff Length). Setting overlap to exactly half the cutoff length may be ideal.')\n                                newline_favor_len = gr.Slider(label='Prefer Newline Cut Length', minimum=0, maximum=512, value=128, step=16, info='Length (in characters, not tokens) of the maximum distance to shift an overlap cut by to ensure chunks cut at newlines. If too low, cuts may occur in the middle of lines.')\n\n                            with gr.Column():\n                                hard_cut_string = gr.Textbox(label='Hard Cut String', value='\\\\n\\\\n\\\\n', info='String that indicates a hard cut between text parts. Helps prevent unwanted overlap.')\n                                min_chars = gr.Number(label='Ignore small blocks', value=0, info='Ignore Hard Cut blocks that have less or equal characters than this number')\n\n                    with gr.Row():\n                        start_button = gr.Button(\"Start LoRA Training\", variant='primary', interactive=not mu)\n                        stop_button = gr.Button(\"Interrupt\", interactive=not mu)\n\n                    output = gr.Markdown(value=\"Ready\")\n\n        with gr.Tab('Perplexity evaluation', elem_id='evaluate-tab'):\n            with gr.Row():\n                with gr.Column():\n                    models = gr.Dropdown(utils.get_available_models(), label='Models', multiselect=True, interactive=not mu)\n                    evaluate_text_file = gr.Dropdown(choices=['wikitext', 'ptb', 'ptb_new'] + utils.get_datasets('training/datasets', 'txt')[1:], value='wikitext', label='Input dataset', info='The raw text file on which the model will be evaluated. The first options are automatically downloaded: wikitext, ptb, and ptb_new. The next options are your local text files under training/datasets.', interactive=not mu)\n                    with gr.Row():\n                        with gr.Column():\n                            stride_length = gr.Slider(label='Stride', minimum=0, maximum=32768, value=512, step=256, info='Used to make the evaluation faster at the cost of accuracy. 1 = slowest but most accurate. 512 is a common value.')\n\n                        with gr.Column():\n                            max_length = gr.Slider(label='max_length', minimum=0, maximum=shared.settings['truncation_length_max'], value=0, step=256, info='The context for each evaluation. If set to 0, the maximum context length for the model will be used.')\n\n                    with gr.Row():\n                        start_current_evaluation = gr.Button(\"Evaluate loaded model\", interactive=not mu)\n                        start_evaluation = gr.Button(\"Evaluate selected models\", interactive=not mu)\n                        stop_evaluation = gr.Button(\"Interrupt\", interactive=not mu)\n\n                with gr.Column():\n                    evaluation_log = gr.Markdown(value='')\n\n            evaluation_table = gr.Dataframe(value=generate_markdown_table(), interactive=True)\n            with gr.Row():\n                save_comments = gr.Button('Save comments', elem_classes=\"small-button\", interactive=not mu)\n                refresh_table = gr.Button('Refresh the table', elem_classes=\"small-button\", interactive=not mu)\n\n    # Training events\n    all_params = [lora_name, always_override, q_proj_en, v_proj_en, k_proj_en, o_proj_en, gate_proj_en, down_proj_en, up_proj_en, save_steps, micro_batch_size, batch_size, epochs, learning_rate, lr_scheduler_type, lora_rank, lora_alpha, lora_dropout, cutoff_len, dataset, eval_dataset, format, eval_steps, raw_text_file, overlap_len, newline_favor_len, higher_rank_limit, warmup_steps, optimizer, hard_cut_string, train_only_after, stop_at_loss, add_eos_token, min_chars, report_to]\n\n    copy_from.change(do_copy_params, [copy_from] + all_params, all_params)\n    start_button.click(do_train, all_params, output)\n    stop_button.click(do_interrupt, None, None, queue=False)\n    higher_rank_limit.change(change_rank_limit, [higher_rank_limit], [lora_rank, lora_alpha])\n\n    # Evaluation events. For some reason, the interrupt event\n    # doesn't work with the .then() syntax, so I write them one\n    # by one in this ugly but functional way.\n    ev = start_evaluation.click(calculate_perplexity, [models, evaluate_text_file, stride_length, max_length], evaluation_log, show_progress=False)\n    ev.then(generate_markdown_table, None, evaluation_table, show_progress=False)\n\n    ev_cur = start_current_evaluation.click(\n        lambda: ['current model'], None, tmp).then(\n        calculate_perplexity, [tmp, evaluate_text_file, stride_length, max_length], evaluation_log, show_progress=False)\n\n    ev_cur.then(generate_markdown_table, None, evaluation_table, show_progress=False)\n\n    stop_evaluation.click(None, None, None, cancels=[ev, ev_cur], queue=False)\n    refresh_table.click(generate_markdown_table, None, evaluation_table, show_progress=True)\n    save_comments.click(\n        save_past_evaluations, evaluation_table, None).then(\n        lambda: \"Comments saved.\", None, evaluation_log, show_progress=False)\n\n\ndef do_interrupt():\n    global WANT_INTERRUPT\n    WANT_INTERRUPT = True\n\n\ndef do_copy_params(lora_name: str, *args):\n    f_name = f\"{shared.args.lora_dir}/{clean_path(None, lora_name)}/training_parameters.json\"\n    if Path(f_name).is_file():\n        with open(f_name, 'r', encoding='utf-8') as format_file:\n            params: dict[str, str] = json.load(format_file)\n    else:\n        params = {}\n\n    result = list()\n    for i in range(0, len(PARAMETERS)):\n        key = PARAMETERS[i]\n        if key in params:\n            result.append(params[key])\n        else:\n            result.append(args[i])\n\n    return result\n\n\ndef change_rank_limit(use_higher_ranks: bool):\n    mult = 2 if use_higher_ranks else 1\n    return {\"maximum\": 1024 * mult, \"__type__\": \"update\"}, {\"maximum\": 2048 * mult, \"__type__\": \"update\"}\n\n\ndef clean_path(base_path: str, path: str):\n    \"\"\"Strips unusual symbols and forcibly builds a path as relative to the intended directory.\"\"\"\n    path = path.replace('\\\\', '/').replace('..', '_')\n    if base_path is None:\n        return path\n\n    return f'{Path(base_path).absolute()}/{path}'\n\n\ndef backup_adapter(input_folder):\n    # Get the creation date of the file adapter_model.bin\n    try:\n        adapter_file = Path(f\"{input_folder}/adapter_model.bin\")\n        if adapter_file.is_file():\n\n            logger.info(\"Backing up existing LoRA adapter\")\n            creation_date = datetime.fromtimestamp(adapter_file.stat().st_ctime)\n            creation_date_str = creation_date.strftime(\"Backup-%Y-%m-%d\")\n\n            # Create the new subfolder\n            subfolder_path = Path(f\"{input_folder}/{creation_date_str}\")\n            subfolder_path.mkdir(parents=True, exist_ok=True)\n\n            # Check if the file already exists in the subfolder\n            backup_adapter_file = Path(f\"{input_folder}/{creation_date_str}/adapter_model.bin\")\n            if backup_adapter_file.is_file():\n                print(\" - Backup already exists. Skipping backup process.\")\n                return\n\n            # Copy existing files to the new subfolder\n            existing_files = Path(input_folder).iterdir()\n            for file in existing_files:\n                if file.is_file():\n                    shutil.copy2(file, subfolder_path)\n    except Exception as e:\n        print(\"An error occurred in backup_adapter:\", str(e))\n\n\ndef calc_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        num_params = param.numel()\n        # if using DS Zero 3 and the weights are initialized empty\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    return trainable_params, all_param\n\n\ndef do_train(lora_name: str, always_override: bool, q_proj_en: bool, v_proj_en: bool, k_proj_en: bool, o_proj_en: bool, gate_proj_en: bool, down_proj_en: bool, up_proj_en: bool, save_steps: int, micro_batch_size: int, batch_size: int, epochs: int, learning_rate: str, lr_scheduler_type: str, lora_rank: int, lora_alpha: int, lora_dropout: float, cutoff_len: int, dataset: str, eval_dataset: str, format: str, eval_steps: int, raw_text_file: str, overlap_len: int, newline_favor_len: int, higher_rank_limit: bool, warmup_steps: int, optimizer: str, hard_cut_string: str, train_only_after: str, stop_at_loss: float, add_eos_token: bool, min_chars: int, report_to: str):\n\n    global WANT_INTERRUPT\n    WANT_INTERRUPT = False\n\n    # == Input validation / processing ==\n    yield \"Preparing the input...\"\n    lora_file_path = clean_path(None, lora_name)\n    if lora_file_path.strip() == '':\n        yield \"Missing or invalid LoRA file name input.\"\n        return\n\n    lora_file_path = f\"{Path(shared.args.lora_dir)}/{lora_file_path}\"\n    actual_lr = float(learning_rate)\n    model_type = type(shared.model).__name__\n\n    if model_type in MODEL_CLASSES:\n        model_id = MODEL_CLASSES[model_type]\n    else:\n        model_id = \"llama\"\n        if model_type == \"PeftModelForCausalLM\":\n            if len(shared.lora_names) > 0:\n                yield \"You are trying to train a LoRA while you already have another LoRA loaded. This will work, but may have unexpected effects. *(Will continue anyway in 5 seconds, press `Interrupt` to stop.)*\"\n                logger.warning(\"Training LoRA over top of another LoRA. May have unexpected effects.\")\n            else:\n                yield \"Model ID not matched due to LoRA loading. Consider reloading base model. *(Will continue anyway in 5 seconds, press `Interrupt` to stop.)*\"\n                logger.warning(\"Model ID not matched due to LoRA loading. Consider reloading base model.\")\n        else:\n            yield \"LoRA training has only currently been validated for LLaMA, OPT, GPT-J, and GPT-NeoX models. Unexpected errors may follow. *(Will continue anyway in 5 seconds, press `Interrupt` to stop.)*\"\n            logger.warning(f\"LoRA training has only currently been validated for LLaMA, OPT, GPT-J, and GPT-NeoX models. (Found model type: {model_type})\")\n\n        time.sleep(5)\n\n    if cutoff_len <= 0 or micro_batch_size <= 0 or batch_size <= 0 or actual_lr <= 0 or lora_rank <= 0 or lora_alpha <= 0:\n        yield \"Cannot input zeroes.\"\n        return\n\n    gradient_accumulation_steps = batch_size // micro_batch_size\n    shared.tokenizer.pad_token_id = 0\n    shared.tokenizer.padding_side = \"left\"\n\n    # Populate target_modules list with chosen X_proj modules. Llama-based models only atm, non-llama will revert to default behavior.\n    def list_target_modules(model_id):\n        if model_id != \"llama\" and model_id != \"mistral\":\n            return model_to_lora_modules[model_id]\n\n        available_modules = {\n            \"gate\": gate_proj_en,\n            \"down\": down_proj_en,\n            \"up\": up_proj_en,\n            \"q\": q_proj_en,\n            \"v\": v_proj_en,\n            \"k\": k_proj_en,\n            \"o\": o_proj_en,\n        }\n        target_mods = [f\"{name}_proj\" for name, enabled in available_modules.items() if enabled]\n        return target_mods\n\n    def encode(text, add_bos_token):\n        result = shared.tokenizer.encode(text, truncation=True, max_length=cutoff_len)\n        # Check if the first two tokens are BOS\n        if len(result) >= 2 and result[:2] == [shared.tokenizer.bos_token_id, shared.tokenizer.bos_token_id]:\n            result = result[1:]\n\n        if not add_bos_token and result[0] == shared.tokenizer.bos_token_id:\n            result = result[1:]\n        return result\n\n    def tokenize(prompt, append_eos_token=False):\n\n        if train_only_after == '' or train_only_after not in prompt:\n            input_ids = encode(prompt, True)\n\n            if append_eos_token and input_ids[-1] != shared.tokenizer.eos_token_id and len(input_ids) < cutoff_len:\n                input_ids.append(shared.tokenizer.eos_token_id)\n\n            input_ids = [shared.tokenizer.pad_token_id] * (cutoff_len - len(input_ids)) + input_ids\n            labels = [1] * len(input_ids)\n\n        else:\n            ind = prompt.index(train_only_after) + len(train_only_after)\n            before_tokens = encode(prompt[:ind], True)\n            after_tokens = encode(prompt[ind:], False)\n\n            if append_eos_token and after_tokens[-1] != shared.tokenizer.eos_token_id:\n                after_tokens.append(shared.tokenizer.eos_token_id)\n\n            full_length = len(after_tokens) + len(before_tokens)\n            if full_length > cutoff_len:\n                after_tokens = after_tokens[:cutoff_len - len(before_tokens)]\n            else:\n                before_tokens = [shared.tokenizer.pad_token_id] * (cutoff_len - full_length) + before_tokens\n\n            input_ids = before_tokens + after_tokens\n            labels = [-100] * len(before_tokens) + [1] * len(after_tokens)\n\n        input_ids = torch.tensor(input_ids)\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n            \"attention_mask\": input_ids.ne(shared.tokenizer.pad_token_id),\n        }\n\n    train_template.clear()\n\n    # == Prep the dataset, format, etc ==\n    if raw_text_file not in ['None', '']:\n        train_template[\"template_type\"] = \"raw_text\"\n        logger.info(\"Loading raw text file dataset\")\n        fullpath = clean_path('training/datasets', f'{raw_text_file}')\n        fullpath = Path(fullpath)\n        if fullpath.is_dir():\n            logger.info('Training path directory {}'.format(raw_text_file))\n            raw_text = \"\"\n            file_paths = sorted(fullpath.glob('*.txt'), key=lambda path: natural_keys(path.name))\n            for file_path in file_paths:\n                if file_path.is_file():\n                    with file_path.open('r', encoding='utf-8') as file:\n                        raw_text += file.read().replace('\\r', '')\n\n                    logger.info(f\"Loaded training file: {file_path.name}\")\n        else:\n            with open(clean_path('training/datasets', f'{raw_text_file}.txt'), 'r', encoding='utf-8') as file:\n                raw_text = file.read().replace('\\r', '')\n\n        cut_string = hard_cut_string.replace('\\\\n', '\\n')\n        eos_added = 0\n        out_tokens = []\n        for text_part in raw_text.split(cut_string):\n            if len(text_part.strip()) <= min_chars:\n                continue\n\n            tokens = shared.tokenizer.encode(text_part)\n            if add_eos_token:\n                tokens.append(shared.tokenizer.eos_token_id)\n                eos_added += 1\n\n            step = cutoff_len - overlap_len\n            if step <= 0:\n                yield f\"Error: overlap_len ({overlap_len}) cannot be greater than or equal to cutoff_len ({cutoff_len})\"\n                return\n\n            out_tokens.extend(split_chunks(tokens, cutoff_len, step))\n\n        if eos_added > 0:\n            print(f\"EOS added to {eos_added} text blocks\")\n\n        del raw_text  # Note: could be a gig for a large dataset, so delete redundant data as we go to be safe on RAM\n        text_chunks = [shared.tokenizer.decode(x) for x in out_tokens]\n        del out_tokens\n        if newline_favor_len > 0:\n            text_chunks = [cut_chunk_for_newline(x, newline_favor_len) for x in text_chunks]\n\n        train_data = Dataset.from_list([tokenize(x) for x in text_chunks])\n        del text_chunks\n        eval_data = None\n    else:\n        if dataset in ['None', '']:\n            yield \"Missing dataset choice input, cannot continue.\"\n            return\n\n        if format in ['None', '']:\n            yield \"Missing format choice input, cannot continue.\"\n            return\n\n        train_template[\"template_type\"] = \"dataset\"\n\n        with open(clean_path('training/formats', f'{format}.json'), 'r', encoding='utf-8-sig') as formatFile:\n            format_data: dict[str, str] = json.load(formatFile)\n\n        # == store training prompt ==\n        for _, value in format_data.items():\n            prompt_key = f\"template_{len(train_template)}\"\n            train_template[prompt_key] = value\n\n        def generate_prompt(data_point: dict[str, str]):\n            for options, data in format_data.items():\n                if set(options.split(',')) == set(x[0] for x in data_point.items() if (type(x[1]) is str and len(x[1].strip()) > 0)):\n                    for key, val in data_point.items():\n                        if type(val) is str:\n                            data = data.replace(f'%{key}%', val)\n                    return data\n            raise RuntimeError(f'Data-point \"{data_point}\" has no keyset match within format \"{list(format_data.keys())}\"')\n\n        def generate_and_tokenize_prompt(data_point):\n            prompt = generate_prompt(data_point)\n            return tokenize(prompt, add_eos_token)\n\n        logger.info(\"Loading JSON datasets\")\n        data = load_dataset(\"json\", data_files=clean_path('training/datasets', f'{dataset}.json'))\n        train_data = data['train'].map(generate_and_tokenize_prompt, new_fingerprint='%030x' % random.randrange(16**30))\n\n        if eval_dataset == 'None':\n            eval_data = None\n        else:\n            eval_data = load_dataset(\"json\", data_files=clean_path('training/datasets', f'{eval_dataset}.json'))\n            eval_data = eval_data['train'].map(generate_and_tokenize_prompt, new_fingerprint='%030x' % random.randrange(16**30))\n\n    # == We MUST reload model if it went through any previous training, even failed one ==\n    if shared.model_dirty_from_training:\n        selected_model = shared.model_name\n        if selected_model:\n            print(\"\\033[1;31;1m(Model has been modified by previous training, it needs to be reloaded...)\\033[0;37;0m\")\n            try:\n                yield f\"Reloading {selected_model}...\"\n                reload_model()\n                if shared.model is not None:\n                    print(\"Model reloaded OK, continue with training.\")\n                else:\n                    return f\"Failed to load {selected_model}.\"\n            except:\n                exc = traceback.format_exc()\n                logger.error('Failed to reload the model.')\n                print(exc)\n                return exc.replace('\\n', '\\n\\n')\n\n    # == Start prepping the model itself ==\n    if not hasattr(shared.model, 'lm_head') or hasattr(shared.model.lm_head, 'weight'):\n        logger.info(\"Getting model ready\")\n        if 'quantization_config' in shared.model.config.to_dict():\n            prepare_model_for_kbit_training(shared.model)\n\n    # base model is now frozen and should not be reused for any other LoRA training than this one\n    shared.model_dirty_from_training = True\n\n    logger.info(\"Preparing for training\")\n    config = LoraConfig(\n        r=lora_rank,\n        lora_alpha=lora_alpha,\n        target_modules=list_target_modules(model_id),\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n    # == Backup the existing adapter ==\n    if not always_override:\n        backup_adapter(lora_file_path)\n\n    # == get model trainable params\n    model_trainable_params, model_all_params = calc_trainable_parameters(shared.model)\n\n    try:\n        logger.info(\"Creating LoRA model\")\n        lora_model = get_peft_model(shared.model, config)\n        if not always_override and Path(f\"{lora_file_path}/adapter_model.bin\").is_file():\n            logger.info(\"Loading existing LoRA data\")\n            state_dict_peft = torch.load(f\"{lora_file_path}/adapter_model.bin\", weights_only=True)\n            set_peft_model_state_dict(lora_model, state_dict_peft)\n    except:\n        yield traceback.format_exc().replace('\\n', '\\n\\n')\n        return\n\n    class Tracked():\n        def __init__(self):\n            self.current_steps = 0\n            self.max_steps = 0\n            self.did_save = False\n\n    tracked = Tracked()\n    actual_save_steps = math.ceil(save_steps / gradient_accumulation_steps)\n\n    class Callbacks(transformers.TrainerCallback):\n        def on_step_begin(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs):\n            tracked.current_steps = state.global_step * gradient_accumulation_steps\n            tracked.max_steps = state.max_steps * gradient_accumulation_steps\n            if WANT_INTERRUPT:\n                control.should_epoch_stop = True\n                control.should_training_stop = True\n            elif state.global_step > 0 and actual_save_steps > 0 and state.global_step % actual_save_steps == 0:\n                lora_model.save_pretrained(f\"{lora_file_path}/checkpoint-{tracked.current_steps}/\")\n                # Save log\n                with open(f\"{lora_file_path}/checkpoint-{tracked.current_steps}/training_log.json\", 'w', encoding='utf-8') as file:\n                    json.dump(train_log, file, indent=2)\n                # == Save training prompt ==\n                with open(f\"{lora_file_path}/checkpoint-{tracked.current_steps}/training_prompt.json\", 'w', encoding='utf-8') as file:\n                    json.dump(train_template, file, indent=2)\n\n        def on_substep_end(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs):\n            tracked.current_steps += 1\n            if WANT_INTERRUPT:\n                control.should_epoch_stop = True\n                control.should_training_stop = True\n\n        def on_log(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, logs, **kwargs):\n            train_log.update(logs)\n            train_log.update({\"current_steps\": tracked.current_steps})\n            if WANT_INTERRUPT:\n                print(\"\\033[1;31;1mInterrupted by user\\033[0;37;0m\")\n\n            print(f\"\\033[1;30;40mStep: {tracked.current_steps} \\033[0;37;0m\", end='')\n            if 'loss' in logs:\n                loss = float(logs['loss'])\n                if loss <= stop_at_loss:\n                    control.should_epoch_stop = True\n                    control.should_training_stop = True\n                    print(f\"\\033[1;31;1mStop Loss {stop_at_loss} reached.\\033[0;37;0m\")\n\n    # Fix training for mixed precision models\n    for param in shared.model.parameters():\n        if param.requires_grad:\n            param.data = param.data.float()\n\n    trainer = transformers.Trainer(\n        model=lora_model,\n        train_dataset=train_data,\n        eval_dataset=eval_data,\n        args=transformers.TrainingArguments(\n            report_to=report_to if report_to != \"None\" else None,\n            per_device_train_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_steps=math.ceil(warmup_steps / gradient_accumulation_steps),\n            num_train_epochs=epochs,\n            learning_rate=actual_lr,\n            fp16=False if shared.args.cpu or shared.args.bf16 else True,\n            bf16=shared.args.bf16,\n            optim=optimizer,\n            logging_steps=2 if stop_at_loss > 0 else 5,\n            evaluation_strategy=\"steps\" if eval_data is not None else \"no\",\n            eval_steps=math.ceil(eval_steps / gradient_accumulation_steps) if eval_data is not None else None,\n            save_strategy=\"steps\" if eval_data is not None else \"no\",\n            output_dir=lora_file_path,\n            lr_scheduler_type=lr_scheduler_type,\n            load_best_model_at_end=eval_data is not None,\n            # TODO: Enable multi-device support\n            ddp_find_unused_parameters=None,\n            no_cuda=shared.args.cpu,\n            use_ipex=True if is_torch_xpu_available() and not shared.args.cpu else False\n        ),\n        data_collator=transformers.DataCollatorForLanguageModeling(shared.tokenizer, mlm=False),\n        callbacks=list([Callbacks()])\n    )\n\n    lora_model.config.use_cache = False\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        lora_model = torch.compile(lora_model)\n\n    # == Save parameters for reuse ==\n    with open(f\"{lora_file_path}/training_parameters.json\", 'w', encoding='utf-8') as file:\n        vars = locals()\n        json.dump({x: vars[x] for x in PARAMETERS}, file, indent=2)\n\n    # == Save training prompt ==\n    with open(f\"{lora_file_path}/training_prompt.json\", 'w', encoding='utf-8') as file:\n        json.dump(train_template, file, indent=2)\n\n    # == Main run and monitor loop ==\n    logger.info(\"Starting training\")\n    yield \"Starting...\"\n\n    lora_trainable_param, lora_all_param = calc_trainable_parameters(lora_model)\n\n    projections_string = \", \".join([projection.replace(\"_proj\", \"\") for projection in list_target_modules(model_id)])\n\n    print(f\"Training '{model_id}' model using ({projections_string}) projections\")\n\n    if lora_all_param > 0:\n        print(f\"Trainable params: {lora_trainable_param:,d} ({100 * lora_trainable_param / lora_all_param:.4f} %), All params: {lora_all_param:,d} (Model: {model_all_params:,d})\")\n\n    train_log.update({\"base_model_name\": shared.model_name})\n    train_log.update({\"base_model_class\": shared.model.__class__.__name__})\n    train_log.update({\"base_loaded_in_4bit\": getattr(lora_model, \"is_loaded_in_4bit\", False)})\n    train_log.update({\"base_loaded_in_8bit\": getattr(lora_model, \"is_loaded_in_8bit\", False)})\n    train_log.update({\"projections\": projections_string})\n\n    if stop_at_loss > 0:\n        print(f\"Monitoring loss \\033[1;31;1m(Auto-Stop at: {stop_at_loss})\\033[0;37;0m\")\n\n    if WANT_INTERRUPT:\n        yield \"Interrupted before start.\"\n        return\n\n    def log_train_dataset(trainer):\n        decoded_entries = []\n        # Try to decode the entries and write the log file\n        try:\n            # Iterate over the first 10 elements in the dataset (or fewer if there are less than 10)\n            for i in range(min(10, len(trainer.train_dataset))):\n                decoded_text = shared.tokenizer.decode(trainer.train_dataset[i]['input_ids'])\n                decoded_entries.append({\"value\": decoded_text})\n\n            # Write the log file\n            Path('logs').mkdir(exist_ok=True)\n            with open(Path('logs/train_dataset_sample.json'), 'w') as json_file:\n                json.dump(decoded_entries, json_file, indent=4)\n\n            logger.info(\"Log file 'train_dataset_sample.json' created in the 'logs' directory.\")\n        except Exception as e:\n            logger.error(f\"Failed to create log file due to error: {e}\")\n\n    def threaded_run():\n        log_train_dataset(trainer)\n        trainer.train()\n        # Note: save in the thread in case the gradio thread breaks (eg browser closed)\n        lora_model.save_pretrained(lora_file_path)\n        logger.info(\"LoRA training run is completed and saved.\")\n        # Save log\n        with open(f\"{lora_file_path}/training_log.json\", 'w', encoding='utf-8') as file:\n            json.dump(train_log, file, indent=2)\n\n    thread = threading.Thread(target=threaded_run)\n    thread.start()\n    last_step = 0\n    start_time = time.perf_counter()\n\n    while thread.is_alive():\n        time.sleep(0.5)\n        if WANT_INTERRUPT:\n            yield \"Interrupting, please wait... *(Run will stop after the current training step completes.)*\"\n\n        elif tracked.current_steps != last_step:\n            last_step = tracked.current_steps\n            time_elapsed = time.perf_counter() - start_time\n            if time_elapsed <= 0:\n                timer_info = \"\"\n                total_time_estimate = 999\n            else:\n                its = tracked.current_steps / time_elapsed\n                if its > 1:\n                    timer_info = f\"`{its:.2f}` it/s\"\n                else:\n                    timer_info = f\"`{1.0/its:.2f}` s/it\"\n\n                total_time_estimate = (1.0 / its) * (tracked.max_steps)\n\n            yield f\"Running... **{tracked.current_steps}** / **{tracked.max_steps}** ... {timer_info}, {format_time(time_elapsed)} / {format_time(total_time_estimate)} ... {format_time(total_time_estimate - time_elapsed)} remaining\"\n\n    # Saving in the train thread might fail if an error occurs, so save here if so.\n    if not tracked.did_save:\n        logger.info(\"Training complete, saving\")\n        lora_model.save_pretrained(lora_file_path)\n\n    if WANT_INTERRUPT:\n        logger.info(\"Training interrupted.\")\n        yield f\"Interrupted. Incomplete LoRA saved to `{lora_file_path}`.\"\n    else:\n        logger.info(\"Training complete!\")\n        yield f\"Done! LoRA saved to `{lora_file_path}`.\\n\\nBefore testing your new LoRA, make sure to first reload the model, as it is currently dirty from training.\"\n\n\ndef split_chunks(arr, size, step):\n    for i in range(0, len(arr), step):\n        yield arr[i:i + size]\n\n\ndef cut_chunk_for_newline(chunk: str, max_length: int):\n    if '\\n' not in chunk:\n        return chunk\n\n    first_newline = chunk.index('\\n')\n    if first_newline < max_length:\n        chunk = chunk[first_newline + 1:]\n\n    if '\\n' not in chunk:\n        return chunk\n\n    last_newline = chunk.rindex('\\n')\n    if len(chunk) - last_newline < max_length:\n        chunk = chunk[:last_newline]\n\n    return chunk\n\n\ndef format_time(seconds: float):\n    if seconds < 120:\n        return f\"`{seconds:.0f}` seconds\"\n\n    minutes = seconds / 60\n    if minutes < 120:\n        return f\"`{minutes:.0f}` minutes\"\n\n    hours = minutes / 60\n    return f\"`{hours:.0f}` hours\"\n", "modules/deepspeed_parameters.py": "def generate_ds_config(ds_bf16, train_batch_size, nvme_offload_dir):\n    '''\n    DeepSpeed configuration\n    https://huggingface.co/docs/transformers/main_classes/deepspeed\n    '''\n\n    if nvme_offload_dir:\n        ds_config = {\n            \"fp16\": {\n                \"enabled\": not ds_bf16,\n            },\n            \"bf16\": {\n                \"enabled\": ds_bf16,\n            },\n            \"zero_optimization\": {\n                \"stage\": 3,\n                \"offload_param\": {\n                    \"device\": \"nvme\",\n                    \"nvme_path\": nvme_offload_dir,\n                    \"pin_memory\": True,\n                    \"buffer_count\": 5,\n                    \"buffer_size\": 1e9,\n                    \"max_in_cpu\": 1e9\n                },\n                \"overlap_comm\": True,\n                \"reduce_bucket_size\": \"auto\",\n                \"contiguous_gradients\": True,\n                \"sub_group_size\": 1e8,\n                \"stage3_prefetch_bucket_size\": \"auto\",\n                \"stage3_param_persistence_threshold\": \"auto\",\n                \"stage3_max_live_parameters\": \"auto\",\n                \"stage3_max_reuse_distance\": \"auto\",\n            },\n            \"aio\": {\n                \"block_size\": 262144,\n                \"queue_depth\": 32,\n                \"thread_count\": 1,\n                \"single_submit\": False,\n                \"overlap_events\": True\n            },\n            \"steps_per_print\": 2000,\n            \"train_batch_size\": train_batch_size,\n            \"train_micro_batch_size_per_gpu\": 1,\n            \"wall_clock_breakdown\": False\n        }\n    else:\n        ds_config = {\n            \"fp16\": {\n                \"enabled\": not ds_bf16,\n            },\n            \"bf16\": {\n                \"enabled\": ds_bf16,\n            },\n            \"zero_optimization\": {\n                \"stage\": 3,\n                \"offload_param\": {\n                    \"device\": \"cpu\",\n                    \"pin_memory\": True\n                },\n                \"overlap_comm\": True,\n                \"contiguous_gradients\": True,\n                \"reduce_bucket_size\": \"auto\",\n                \"stage3_prefetch_bucket_size\": \"auto\",\n                \"stage3_param_persistence_threshold\": \"auto\",\n                \"stage3_max_live_parameters\": \"auto\",\n                \"stage3_max_reuse_distance\": \"auto\",\n            },\n            \"steps_per_print\": 2000,\n            \"train_batch_size\": train_batch_size,\n            \"train_micro_batch_size_per_gpu\": 1,\n            \"wall_clock_breakdown\": False\n        }\n\n    return ds_config\n", "modules/llamacpp_hf.py": "import os\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom modules import RoPE, llama_cpp_python_hijack, shared\nfrom modules.logging_colors import logger\n\ntry:\n    import llama_cpp\nexcept:\n    llama_cpp = None\n\ntry:\n    import llama_cpp_cuda\nexcept:\n    llama_cpp_cuda = None\n\ntry:\n    import llama_cpp_cuda_tensorcores\nexcept:\n    llama_cpp_cuda_tensorcores = None\n\n\ndef llama_cpp_lib():\n    if shared.args.cpu and llama_cpp is not None:\n        return llama_cpp\n    elif shared.args.tensorcores and llama_cpp_cuda_tensorcores is not None:\n        return llama_cpp_cuda_tensorcores\n    elif llama_cpp_cuda is not None:\n        return llama_cpp_cuda\n    else:\n        return llama_cpp\n\n\nclass LlamacppHF(PreTrainedModel):\n    def __init__(self, model, path):\n        super().__init__(PretrainedConfig())\n        self.model = model\n        self.generation_config = GenerationConfig()\n\n        self.past_seq = None\n        self.llamacpp_cache = {\n            'n_tokens': self.model.n_tokens,\n            'input_ids': self.model.input_ids,\n            'scores': self.model.scores,\n            'ctx': self.model._ctx.ctx\n        }\n\n        if shared.args.cfg_cache:\n            self.past_seq_negative = None\n            self.llamacpp_cache_negative = {\n                'n_tokens': self.model.n_tokens,\n                'input_ids': self.model.input_ids.copy(),\n                'scores': self.model.scores.copy(),\n                'ctx': llama_cpp_lib().llama_new_context_with_model(model.model, model.context_params)\n            }\n\n    def _validate_model_class(self):\n        pass\n\n    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n        pass\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {'input_ids': input_ids, **kwargs}\n\n    def save_cache(self):\n        self.llamacpp_cache.update({\n            'n_tokens': self.model.n_tokens,\n            'input_ids': self.model.input_ids,\n            'scores': self.model.scores,\n            'ctx': self.model._ctx.ctx\n        })\n\n    def save_negative_cache(self):\n        self.llamacpp_cache_negative.update({\n            'n_tokens': self.model.n_tokens,\n            'input_ids': self.model.input_ids,\n            'scores': self.model.scores,\n            'ctx': self.model._ctx.ctx\n        })\n\n    def load_cache(self):\n        self.model.n_tokens = self.llamacpp_cache['n_tokens']\n        self.model.input_ids = self.llamacpp_cache['input_ids']\n        self.model.scores = self.llamacpp_cache['scores']\n        self.model._ctx.ctx = self.llamacpp_cache['ctx']\n\n    def load_negative_cache(self):\n        self.model.n_tokens = self.llamacpp_cache_negative['n_tokens']\n        self.model.input_ids = self.llamacpp_cache_negative['input_ids']\n        self.model.scores = self.llamacpp_cache_negative['scores']\n        self.model._ctx.ctx = self.llamacpp_cache_negative['ctx']\n\n    @property\n    def device(self) -> torch.device:\n        return torch.device(0)\n\n    def __call__(self, *args, **kwargs):\n        use_cache = kwargs.get('use_cache', True)\n        labels = kwargs.get('labels', None)\n        past_key_values = kwargs.get('past_key_values', None)\n\n        if len(args) > 0:\n            if not shared.args.cfg_cache:\n                logger.error(\"Please enable the cfg-cache option to use CFG with llamacpp_HF.\")\n                return\n\n            input_ids = args[0]\n            is_negative = True\n            past_seq = self.past_seq_negative\n            self.load_negative_cache()\n        else:\n            input_ids = kwargs['input_ids']\n            is_negative = False\n            past_seq = self.past_seq\n            self.load_cache()\n\n        seq = input_ids[0].tolist()\n        if is_negative and past_key_values is not None:\n            seq = past_key_values + seq\n\n        seq_tensor = torch.tensor(seq)\n        reset = True\n\n        # Make the forward call. The prefix-match code has been adapted from\n        # https://github.com/abetlen/llama-cpp-python/commit/f4090a0bb2a2a25acfe28d31c82cc1aa273bedee\n        if labels is None:\n            if past_seq is not None:\n                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n                if len(indices) > 0:\n                    longest_prefix = indices[0].item()\n                else:\n                    longest_prefix = min_length\n\n                if longest_prefix > 0:\n                    reset = False\n                    self.model.n_tokens = longest_prefix\n                    if len(seq_tensor) - longest_prefix > 0:\n                        self.model.eval(seq[longest_prefix:])\n                    else:\n                        self.model.n_tokens -= 1\n                        self.model.eval([seq[-1]])\n\n            if reset:\n                self.model.reset()\n                self.model.eval(seq)\n\n            logits = torch.tensor(self.model.scores[self.model.n_tokens - 1, :]).view(1, 1, -1).to(input_ids.device)\n        else:\n            self.model.reset()\n            self.model.eval(seq)\n            logits = torch.tensor(self.model.eval_logits)\n            logits = logits.view(1, logits.shape[0], logits.shape[1]).to(input_ids.device)\n\n        if is_negative:\n            self.save_negative_cache()\n            self.past_seq_negative = seq_tensor\n        else:\n            self.save_cache()\n            self.past_seq = seq_tensor\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, logits.shape[-1])\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n\n        path = Path(f'{shared.args.model_dir}') / Path(pretrained_model_name_or_path)\n        if path.is_file():\n            model_file = path\n        else:\n            model_file = sorted(path.glob('*.gguf'))[0]\n\n        logger.info(f\"llama.cpp weights detected: {model_file}\\n\")\n\n        if shared.args.tensor_split is None or shared.args.tensor_split.strip() == '':\n            tensor_split_list = None\n        else:\n            tensor_split_list = [float(x) for x in shared.args.tensor_split.strip().split(\",\")]\n\n        params = {\n            'model_path': str(model_file),\n            'n_ctx': shared.args.n_ctx,\n            'n_threads': shared.args.threads or None,\n            'n_threads_batch': shared.args.threads_batch or None,\n            'n_batch': shared.args.n_batch,\n            'use_mmap': not shared.args.no_mmap,\n            'use_mlock': shared.args.mlock,\n            'mul_mat_q': not shared.args.no_mul_mat_q,\n            'numa': shared.args.numa,\n            'n_gpu_layers': shared.args.n_gpu_layers,\n            'rope_freq_base': RoPE.get_rope_freq_base(shared.args.alpha_value, shared.args.rope_freq_base),\n            'tensor_split': tensor_split_list,\n            'rope_freq_scale': 1.0 / shared.args.compress_pos_emb,\n            'logits_all': shared.args.logits_all,\n            'offload_kqv': not shared.args.no_offload_kqv,\n            'split_mode': 1 if not shared.args.row_split else 2,\n            'flash_attn': shared.args.flash_attn\n        }\n\n        Llama = llama_cpp_lib().Llama\n        model = Llama(**params)\n\n        return LlamacppHF(model, model_file)\n", "modules/ui_notebook.py": "import gradio as gr\n\nfrom modules import logits, shared, ui, utils\nfrom modules.prompts import count_tokens, load_prompt\nfrom modules.text_generation import (\n    generate_reply_wrapper,\n    get_token_ids,\n    stop_everything_event\n)\nfrom modules.utils import gradio\n\ninputs = ('textbox-notebook', 'interface_state')\noutputs = ('textbox-notebook', 'html-notebook')\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n    with gr.Tab('Notebook', elem_id='notebook-tab'):\n        shared.gradio['last_input-notebook'] = gr.State('')\n        with gr.Row():\n            with gr.Column(scale=4):\n                with gr.Tab('Raw'):\n                    with gr.Row():\n                        shared.gradio['textbox-notebook'] = gr.Textbox(value='', lines=27, elem_id='textbox-notebook', elem_classes=['textbox', 'add_scrollbar'])\n                        shared.gradio['token-counter-notebook'] = gr.HTML(value=\"<span>0</span>\", elem_classes=[\"token-counter\"])\n\n                with gr.Tab('Markdown'):\n                    shared.gradio['markdown_render-notebook'] = gr.Button('Render')\n                    shared.gradio['markdown-notebook'] = gr.Markdown()\n\n                with gr.Tab('HTML'):\n                    shared.gradio['html-notebook'] = gr.HTML()\n\n                with gr.Tab('Logits'):\n                    with gr.Row():\n                        with gr.Column(scale=10):\n                            shared.gradio['get_logits-notebook'] = gr.Button('Get next token probabilities')\n                        with gr.Column(scale=1):\n                            shared.gradio['use_samplers-notebook'] = gr.Checkbox(label='Use samplers', value=True, elem_classes=['no-background'])\n\n                    with gr.Row():\n                        shared.gradio['logits-notebook'] = gr.Textbox(lines=23, label='Output', elem_classes=['textbox_logits_notebook', 'add_scrollbar'])\n                        shared.gradio['logits-notebook-previous'] = gr.Textbox(lines=23, label='Previous output', elem_classes=['textbox_logits_notebook', 'add_scrollbar'])\n\n                with gr.Tab('Tokens'):\n                    shared.gradio['get_tokens-notebook'] = gr.Button('Get token IDs for the input')\n                    shared.gradio['tokens-notebook'] = gr.Textbox(lines=23, label='Tokens', elem_classes=['textbox_logits_notebook', 'add_scrollbar', 'monospace'])\n\n                with gr.Row():\n                    shared.gradio['Generate-notebook'] = gr.Button('Generate', variant='primary', elem_classes='small-button')\n                    shared.gradio['Stop-notebook'] = gr.Button('Stop', elem_classes='small-button', elem_id='stop')\n                    shared.gradio['Undo'] = gr.Button('Undo', elem_classes='small-button')\n                    shared.gradio['Regenerate-notebook'] = gr.Button('Regenerate', elem_classes='small-button')\n\n            with gr.Column(scale=1):\n                gr.HTML('<div style=\"padding-bottom: 13px\"></div>')\n                with gr.Row():\n                    shared.gradio['prompt_menu-notebook'] = gr.Dropdown(choices=utils.get_available_prompts(), value='None', label='Prompt', elem_classes='slim-dropdown')\n                    ui.create_refresh_button(shared.gradio['prompt_menu-notebook'], lambda: None, lambda: {'choices': utils.get_available_prompts()}, ['refresh-button', 'refresh-button-small'], interactive=not mu)\n                    shared.gradio['save_prompt-notebook'] = gr.Button('\ud83d\udcbe', elem_classes=['refresh-button', 'refresh-button-small'], interactive=not mu)\n                    shared.gradio['delete_prompt-notebook'] = gr.Button('\ud83d\uddd1\ufe0f', elem_classes=['refresh-button', 'refresh-button-small'], interactive=not mu)\n\n\ndef create_event_handlers():\n    shared.gradio['Generate-notebook'].click(\n        lambda x: x, gradio('textbox-notebook'), gradio('last_input-notebook')).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        generate_reply_wrapper, gradio(inputs), gradio(outputs), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['textbox-notebook'].submit(\n        lambda x: x, gradio('textbox-notebook'), gradio('last_input-notebook')).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        generate_reply_wrapper, gradio(inputs), gradio(outputs), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Undo'].click(lambda x: x, gradio('last_input-notebook'), gradio('textbox-notebook'), show_progress=False)\n    shared.gradio['markdown_render-notebook'].click(lambda x: x, gradio('textbox-notebook'), gradio('markdown-notebook'), queue=False)\n    shared.gradio['Regenerate-notebook'].click(\n        lambda x: x, gradio('last_input-notebook'), gradio('textbox-notebook'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        generate_reply_wrapper, gradio(inputs), gradio(outputs), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Stop-notebook'].click(stop_everything_event, None, None, queue=False)\n    shared.gradio['prompt_menu-notebook'].change(load_prompt, gradio('prompt_menu-notebook'), gradio('textbox-notebook'), show_progress=False)\n    shared.gradio['save_prompt-notebook'].click(\n        lambda x: x, gradio('textbox-notebook'), gradio('save_contents')).then(\n        lambda: 'prompts/', None, gradio('save_root')).then(\n        lambda: utils.current_time() + '.txt', None, gradio('save_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('file_saver'))\n\n    shared.gradio['delete_prompt-notebook'].click(\n        lambda: 'prompts/', None, gradio('delete_root')).then(\n        lambda x: x + '.txt', gradio('prompt_menu-notebook'), gradio('delete_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('file_deleter'))\n\n    shared.gradio['textbox-notebook'].input(lambda x: f\"<span>{count_tokens(x)}</span>\", gradio('textbox-notebook'), gradio('token-counter-notebook'), show_progress=False)\n    shared.gradio['get_logits-notebook'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        logits.get_next_logits, gradio('textbox-notebook', 'interface_state', 'use_samplers-notebook', 'logits-notebook'), gradio('logits-notebook', 'logits-notebook-previous'), show_progress=False)\n\n    shared.gradio['get_tokens-notebook'].click(get_token_ids, gradio('textbox-notebook'), gradio('tokens-notebook'), show_progress=False)\n", "modules/logits.py": "import time\nimport traceback\n\nimport torch\nfrom transformers import is_torch_npu_available, is_torch_xpu_available\n\nfrom modules import models, sampler_hijack, shared\nfrom modules.logging_colors import logger\nfrom modules.models import load_model\nfrom modules.text_generation import generate_reply\n\nglobal_scores = None\n\n\ndef get_next_logits(*args, **kwargs):\n    if shared.args.idle_timeout > 0 and shared.model is None and shared.previous_model_name not in [None, 'None']:\n        shared.model, shared.tokenizer = load_model(shared.previous_model_name)\n\n    shared.generation_lock.acquire()\n    try:\n        result = _get_next_logits(*args, **kwargs)\n    except Exception:\n        traceback.print_exc()\n        result = None\n\n    models.last_generation_time = time.time()\n    shared.generation_lock.release()\n    return result\n\n\ndef _get_next_logits(prompt, state, use_samplers, previous, top_logits=25, return_dict=False):\n    if shared.model is None:\n        logger.error(\"No model is loaded! Select one in the Model tab.\")\n        return 'Error: No model is loaded1 Select one in the Model tab.', previous\n\n    is_non_hf_exllamav2 = shared.model.__class__.__name__ == 'Exllamav2Model'\n    is_non_hf_llamacpp = shared.model.__class__.__name__ == 'LlamaCppModel'\n\n    if use_samplers:\n        if any([is_non_hf_exllamav2, is_non_hf_llamacpp]):\n            logger.error(\"Sampler hijacking is not supported non-Huggingface loaders.\")\n            # sampling is all done in c for exllama, so it is really hard to hijack\n            # it should be possible to hijack llamacpp sampler by hijacking all their sampling methods,\n            # but it is not implemented yet\n            return 'Error: Sampler hijacking is not supported non-Huggingface loaders. Please disable the \"Use samplers\" option.', previous\n\n        state['max_new_tokens'] = 1\n        state['auto_max_new_tokens'] = False\n        for _ in generate_reply(prompt, state):\n            pass\n\n        scores = sampler_hijack.global_scores[-1]\n    else:\n        if is_non_hf_exllamav2:\n            if is_torch_xpu_available():\n                tokens = shared.tokenizer.encode(prompt).to(\"xpu:0\")\n            elif is_torch_npu_available():\n                tokens = shared.tokenizer.encode(prompt).to(\"npu:0\")\n            else:\n                tokens = shared.tokenizer.encode(prompt).cuda()\n            scores = shared.model.get_logits(tokens)[-1][-1]\n        elif is_non_hf_llamacpp:\n            tokens = shared.tokenizer.encode(prompt)\n            scores = shared.model.get_logits(tokens)[-1][-1]\n        else:\n            if is_torch_xpu_available():\n                tokens = shared.tokenizer.encode(prompt, return_tensors='pt').to(\"xpu:0\")\n            elif is_torch_npu_available():\n                tokens = shared.tokenizer.encode(prompt, return_tensors='pt').to(\"npu:0\")\n            else:\n                tokens = shared.tokenizer.encode(prompt, return_tensors='pt').cuda()\n            output = shared.model(input_ids=tokens)\n            scores = output['logits'][-1][-1]\n\n    probs = torch.softmax(scores, dim=-1, dtype=torch.float)\n    topk_values, topk_indices = torch.topk(probs, k=top_logits, largest=True, sorted=True)\n    if is_non_hf_llamacpp:\n        topk_indices = [i.expand((1, 1)) for i in topk_indices]\n\n    if hasattr(shared.tokenizer, 'convert_ids_to_tokens'):\n        tokens = [shared.tokenizer.convert_ids_to_tokens(int(i)) for i in topk_indices]\n    else:\n        tokens = [shared.tokenizer.decode(i) for i in topk_indices]\n\n    if return_dict:\n        topk_values = [float(i) for i in topk_values]\n        output = {}\n        for row in list(zip(topk_values, tokens)):\n            key = row[1]\n            if isinstance(key, bytes):\n                try:\n                    key = key.decode()\n                except:\n                    key = key.decode('latin')\n\n            output[key] = row[0]\n\n        return output\n    else:\n        topk_values = [f\"{float(i):.5f}\" for i in topk_values]\n        output = ''\n        for row in list(zip(topk_values, tokens)):\n            output += f\"{row[0]}  -  {repr(row[1])}\\n\"\n\n        return output, previous\n", "modules/LoRA.py": "from pathlib import Path\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import is_torch_xpu_available\n\nimport modules.shared as shared\nfrom modules.logging_colors import logger\nfrom modules.models import reload_model\n\n\ndef add_lora_to_model(lora_names):\n    if 'GPTQForCausalLM' in shared.model.__class__.__name__ or shared.args.loader == 'AutoGPTQ':\n        add_lora_autogptq(lora_names)\n    elif shared.model.__class__.__name__ in ['Exllamav2Model', 'Exllamav2HF'] or shared.args.loader in ['ExLlamav2', 'ExLlamav2_HF']:\n        add_lora_exllamav2(lora_names)\n    else:\n        add_lora_transformers(lora_names)\n\n\ndef get_lora_path(lora_name):\n    p = Path(lora_name)\n    if p.exists():\n        lora_name = p.parts[-1]\n\n    return Path(f\"{shared.args.lora_dir}/{lora_name}\")\n\n\ndef add_lora_exllamav2(lora_names):\n\n    from exllamav2 import ExLlamaV2Lora\n\n    if isinstance(shared.model.loras, list):\n        for lora in shared.model.loras:\n            lora.unload()\n\n    if len(lora_names) > 0:\n        logger.info(\"Applying the following LoRAs to {}: {}\".format(shared.model_name, ', '.join(lora_names)))\n        shared.model.loras = []\n        for lora_name in lora_names:\n            lora_path = get_lora_path(lora_name)\n            if shared.model.__class__.__name__ == 'Exllamav2Model':\n                lora = ExLlamaV2Lora.from_directory(shared.model.model, str(lora_path))\n            else:\n                lora = ExLlamaV2Lora.from_directory(shared.model.ex_model, str(lora_path))\n\n            shared.model.loras.append(lora)\n\n        shared.lora_names = lora_names\n    else:\n        shared.lora_names = []\n        shared.model.loras = None\n\n\ndef add_lora_autogptq(lora_names):\n    '''\n    Adapted from https://github.com/Ph0rk0z/text-generation-webui-testing\n    '''\n\n    try:\n        from auto_gptq import get_gptq_peft_model\n        from auto_gptq.utils.peft_utils import GPTQLoraConfig\n    except:\n        logger.error(\"This version of AutoGPTQ does not support LoRA. You need to install from source or wait for a new release.\")\n        return\n\n    if len(lora_names) == 0:\n        reload_model()\n\n        shared.lora_names = []\n        return\n    else:\n        if len(lora_names) > 1:\n            logger.warning('AutoGPTQ can only work with 1 LoRA at the moment. Only the first one in the list will be loaded.')\n        if not shared.args.no_inject_fused_attention:\n            logger.warning('Fused Atttention + AutoGPTQ may break Lora loading. Disable it.')\n\n        peft_config = GPTQLoraConfig(\n            inference_mode=True,\n        )\n\n        lora_path = get_lora_path(lora_names[0])\n        logger.info(\"Applying the following LoRAs to {}: {}\".format(shared.model_name, ', '.join([lora_names[0]])))\n        shared.model = get_gptq_peft_model(shared.model, peft_config, lora_path)\n        shared.lora_names = [lora_names[0]]\n        return\n\n\ndef add_lora_transformers(lora_names):\n    prior_set = set(shared.lora_names)\n    added_set = set(lora_names) - prior_set\n    removed_set = prior_set - set(lora_names)\n\n    # If no LoRA needs to be added or removed, exit\n    if len(added_set) == 0 and len(removed_set) == 0:\n        return\n\n    # Add a LoRA when another LoRA is already present\n    if len(removed_set) == 0 and len(prior_set) > 0 and \"__merged\" not in shared.model.peft_config.keys():\n        logger.info(f\"Adding the LoRA(s) named {added_set} to the model\")\n        for lora in added_set:\n            shared.model.load_adapter(get_lora_path(lora), lora)\n\n        if len(lora_names) > 1:\n            merge_loras()\n\n        shared.lora_names = lora_names\n        return\n\n    # If any LoRA needs to be removed, start over\n    if len(removed_set) > 0:\n        shared.model = shared.model.unload()\n\n    if len(lora_names) > 0:\n        params = {}\n        if not shared.args.cpu:\n            if shared.args.load_in_4bit or shared.args.load_in_8bit:\n                params['peft_type'] = shared.model.dtype\n            else:\n                params['dtype'] = shared.model.dtype\n                if hasattr(shared.model, \"hf_device_map\"):\n                    params['device_map'] = {\"base_model.model.\" + k: v for k, v in shared.model.hf_device_map.items()}\n\n        logger.info(\"Applying the following LoRAs to {}: {}\".format(shared.model_name, ', '.join(lora_names)))\n        shared.model = PeftModel.from_pretrained(shared.model, get_lora_path(lora_names[0]), adapter_name=lora_names[0], **params)\n        for lora in lora_names[1:]:\n            shared.model.load_adapter(get_lora_path(lora), lora)\n\n        if len(lora_names) > 1:\n            merge_loras()\n\n        if not shared.args.load_in_8bit and not shared.args.cpu:\n            shared.model.half()\n            if not hasattr(shared.model, \"hf_device_map\"):\n                if torch.backends.mps.is_available():\n                    device = torch.device('mps')\n                    shared.model = shared.model.to(device)\n                elif is_torch_xpu_available():\n                    device = torch.device(\"xpu:0\")\n                    shared.model = shared.model.to(device)\n                else:\n                    shared.model = shared.model.cuda()\n\n    shared.lora_names = lora_names\n\n\ndef merge_loras():\n    if len(list({shared.model.peft_config[adapter].r for adapter in shared.model.peft_config.keys()})) > 1:\n        logger.warning(\"The loaded LoRAs cannot be merged, as they have dissimilar ranks. Only the first one will be active.\")\n        return\n\n    shared.model.add_weighted_adapter(shared.lora_names, [1] * len(shared.lora_names), \"__merged\")\n    shared.model.set_adapter(\"__merged\")\n", "modules/ui_session.py": "import gradio as gr\n\nfrom modules import shared, ui, utils\nfrom modules.github import clone_or_pull_repository\nfrom modules.utils import gradio\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n    with gr.Tab(\"Session\", elem_id=\"session-tab\"):\n        with gr.Row():\n            with gr.Column():\n                shared.gradio['reset_interface'] = gr.Button(\"Apply flags/extensions and restart\", interactive=not mu)\n                with gr.Row():\n                    shared.gradio['toggle_dark_mode'] = gr.Button('Toggle \ud83d\udca1')\n                    shared.gradio['save_settings'] = gr.Button('Save UI defaults to settings.yaml', interactive=not mu)\n\n                with gr.Row():\n                    with gr.Column():\n                        shared.gradio['extensions_menu'] = gr.CheckboxGroup(choices=utils.get_available_extensions(), value=shared.args.extensions, label=\"Available extensions\", info='Note that some of these extensions may require manually installing Python requirements through the command: pip install -r extensions/extension_name/requirements.txt', elem_classes='checkboxgroup-table')\n\n                    with gr.Column():\n                        shared.gradio['bool_menu'] = gr.CheckboxGroup(choices=get_boolean_arguments(), value=get_boolean_arguments(active=True), label=\"Boolean command-line flags\", elem_classes='checkboxgroup-table')\n\n            with gr.Column():\n                extension_name = gr.Textbox(lines=1, label='Install or update an extension', info='Enter the GitHub URL below and press Enter. For a list of extensions, see: https://github.com/oobabooga/text-generation-webui-extensions \u26a0\ufe0f  WARNING \u26a0\ufe0f : extensions can execute arbitrary code. Make sure to inspect their source code before activating them.', interactive=not mu)\n                extension_status = gr.Markdown()\n\n        shared.gradio['theme_state'] = gr.Textbox(visible=False, value='dark' if shared.settings['dark_theme'] else 'light')\n        extension_name.submit(clone_or_pull_repository, extension_name, extension_status, show_progress=False)\n\n        # Reset interface event\n        shared.gradio['reset_interface'].click(\n            set_interface_arguments, gradio('extensions_menu', 'bool_menu'), None).then(\n            lambda: None, None, None, js='() => {document.body.innerHTML=\\'<h1 style=\"font-family:monospace;padding-top:20%;margin:0;height:100vh;color:lightgray;text-align:center;background:var(--body-background-fill)\">Reloading...</h1>\\'; setTimeout(function(){location.reload()},2500); return []}')\n\n        shared.gradio['toggle_dark_mode'].click(\n            lambda: None, None, None, js='() => {document.getElementsByTagName(\"body\")[0].classList.toggle(\"dark\")}').then(\n            lambda x: 'dark' if x == 'light' else 'light', gradio('theme_state'), gradio('theme_state'))\n\n        shared.gradio['save_settings'].click(\n            ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n            ui.save_settings, gradio('interface_state', 'preset_menu', 'extensions_menu', 'show_controls', 'theme_state'), gradio('save_contents')).then(\n            lambda: './', None, gradio('save_root')).then(\n            lambda: 'settings.yaml', None, gradio('save_filename')).then(\n            lambda: gr.update(visible=True), None, gradio('file_saver'))\n\n\ndef set_interface_arguments(extensions, bool_active):\n    shared.args.extensions = extensions\n\n    bool_list = get_boolean_arguments()\n\n    for k in bool_list:\n        setattr(shared.args, k, False)\n    for k in bool_active:\n        setattr(shared.args, k, True)\n        if k == 'api':\n            shared.add_extension('openai', last=True)\n\n    shared.need_restart = True\n\n\ndef get_boolean_arguments(active=False):\n    exclude = shared.deprecated_args\n\n    cmd_list = vars(shared.args)\n    bool_list = sorted([k for k in cmd_list if type(cmd_list[k]) is bool and k not in exclude + ui.list_model_elements()])\n    bool_active = [k for k in bool_list if vars(shared.args)[k]]\n\n    if active:\n        return bool_active\n    else:\n        return bool_list\n", "modules/exllamav2.py": "import traceback\nfrom pathlib import Path\n\nimport torch\nfrom exllamav2 import (\n    ExLlamaV2,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Config,\n    ExLlamaV2Tokenizer\n)\nfrom exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n\nfrom modules import shared\nfrom modules.logging_colors import logger\nfrom modules.text_generation import get_max_prompt_length\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        'You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage '\n        'to be a lot higher than it could be.\\n'\n        'Try installing flash-attention following the instructions here: '\n        'https://github.com/Dao-AILab/flash-attention#installation-and-features'\n    )\n    pass\nexcept Exception:\n    logger.warning('Failed to load flash-attention due to the following error:\\n')\n    traceback.print_exc()\n\n\nclass Exllamav2Model:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_pretrained(self, path_to_model):\n\n        path_to_model = Path(f'{shared.args.model_dir}') / Path(path_to_model)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(path_to_model)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        model = ExLlamaV2(config)\n\n        if not shared.args.autosplit:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n\n            model.load(split)\n\n        if shared.args.cache_8bit:\n            cache = ExLlamaV2Cache_8bit(model, lazy=shared.args.autosplit)\n        elif shared.args.cache_4bit:\n            cache = ExLlamaV2Cache_Q4(model, lazy=shared.args.autosplit)\n        else:\n            cache = ExLlamaV2Cache(model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit:\n            model.load_autosplit(cache)\n\n        tokenizer = ExLlamaV2Tokenizer(config)\n        generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n\n        result = self()\n        result.model = model\n        result.cache = cache\n        result.tokenizer = tokenizer\n        result.generator = generator\n        result.loras = None\n        return result, result\n\n    def encode(self, string, **kwargs):\n        return self.tokenizer.encode(string, add_bos=True, encode_special_tokens=True)\n\n    def decode(self, ids, **kwargs):\n        if isinstance(ids, list):\n            ids = torch.tensor([ids])\n        elif isinstance(ids, torch.Tensor) and ids.numel() == 1:\n            ids = ids.view(1, -1)\n\n        return self.tokenizer.decode(ids, decode_special_tokens=True)[0]\n\n    def get_logits(self, token_ids, **kwargs):\n        self.cache.current_seq_len = 0\n        if token_ids.shape[-1] > 1:\n            self.model.forward(token_ids[:, :-1], self.cache, input_mask=None, preprocess_only=True, loras=self.loras)\n\n        return self.model.forward(token_ids[:, -1:], self.cache, input_mask=None, loras=self.loras, **kwargs).float().cpu()\n\n    def generate_with_streaming(self, prompt, state):\n        settings = ExLlamaV2Sampler.Settings()\n\n        settings.token_repetition_penalty = state['repetition_penalty']\n        settings.token_repetition_range = -1 if state['repetition_penalty_range'] <= 0 else state['repetition_penalty_range']\n\n        settings.token_frequency_penalty = state['frequency_penalty']\n        settings.token_presence_penalty = state['presence_penalty']\n\n        settings.temperature = state['temperature']\n        settings.top_k = state['top_k']\n        settings.top_p = state['top_p']\n        settings.top_a = state['top_a']\n        settings.min_p = state['min_p']\n        settings.tfs = state['tfs']\n        settings.typical = state['typical_p']\n\n        settings.temperature_last = state['temperature_last']\n\n        settings.mirostat = state['mirostat_mode'] == 2\n        settings.mirostat_tau = state['mirostat_tau']\n        settings.mirostat_eta = state['mirostat_eta']\n\n        if state['ban_eos_token']:\n            settings.disallow_tokens(self.tokenizer, [self.tokenizer.eos_token_id])\n\n        if state['custom_token_bans']:\n            to_ban = [int(x) for x in state['custom_token_bans'].split(',')]\n            if len(to_ban) > 0:\n                settings.disallow_tokens(self.tokenizer, to_ban)\n\n        ids = self.tokenizer.encode(prompt, add_bos=state['add_bos_token'], encode_special_tokens=True)\n        ids = ids[:, -get_max_prompt_length(state):]\n\n        if state['auto_max_new_tokens']:\n            max_new_tokens = state['truncation_length'] - ids.shape[-1]\n        else:\n            max_new_tokens = state['max_new_tokens']\n\n        self.generator.begin_stream(ids, settings, loras=self.loras)\n\n        decoded_text = ''\n        for i in range(max_new_tokens):\n            chunk, eos, _ = self.generator.stream()\n            if eos or shared.stop_everything:\n                break\n\n            decoded_text += chunk\n            yield decoded_text\n\n    def generate(self, prompt, state):\n        output = ''\n        for output in self.generate_with_streaming(prompt, state):\n            pass\n\n        return output\n", "modules/gradio_hijack.py": "'''\nCopied from: https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/14184\n'''\n\nimport inspect\nimport warnings\nfrom functools import wraps\n\nimport gradio as gr\n\n\nclass GradioDeprecationWarning(DeprecationWarning):\n    pass\n\n\ndef repair(grclass):\n    if not getattr(grclass, 'EVENTS', None):\n        return\n\n    @wraps(grclass.__init__)\n    def __repaired_init__(self, *args, tooltip=None, source=None, original=grclass.__init__, **kwargs):\n        if source:\n            kwargs[\"sources\"] = [source]\n\n        allowed_kwargs = inspect.signature(original).parameters\n        fixed_kwargs = {}\n        for k, v in kwargs.items():\n            if k in allowed_kwargs:\n                fixed_kwargs[k] = v\n            else:\n                warnings.warn(f\"unexpected argument for {grclass.__name__}: {k}\", GradioDeprecationWarning, stacklevel=2)\n\n        original(self, *args, **fixed_kwargs)\n\n        self.webui_tooltip = tooltip\n\n        for event in self.EVENTS:\n            replaced_event = getattr(self, str(event))\n\n            def fun(*xargs, _js=None, replaced_event=replaced_event, **xkwargs):\n                if _js:\n                    xkwargs['js'] = _js\n\n                return replaced_event(*xargs, **xkwargs)\n\n            setattr(self, str(event), fun)\n\n    grclass.__init__ = __repaired_init__\n    grclass.update = gr.update\n\n\nfor component in set(gr.components.__all__ + gr.layouts.__all__):\n    repair(getattr(gr, component, None))\n\n\nclass Dependency(gr.events.Dependency):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        def then(*xargs, _js=None, **xkwargs):\n            if _js:\n                xkwargs['js'] = _js\n\n            return original_then(*xargs, **xkwargs)\n\n        original_then = self.then\n        self.then = then\n\n\ngr.events.Dependency = Dependency\n\ngr.Box = gr.Group\n", "modules/cache_utils.py": "import torch\nfrom numba import njit\n\nfrom modules import shared\n\n\ndef process_llamacpp_cache(model, new_sequence, past_sequence):\n    if len(past_sequence) == 0 or len(new_sequence) == 0:\n        return past_sequence\n\n    i1, i2, j1, j2 = find_longest_common_substring_indices(past_sequence, new_sequence)\n    overlap_length = i2 - i1 + 1\n\n    # Do StreamingLLM if i1 > 0 (ie the longest common subsequence is not a prefix)\n    # and the overlap length is sufficiently long.\n    if i1 > 0 and overlap_length > 0.2 * len(new_sequence):\n\n        new_sequence = torch.tensor(new_sequence)\n        past_sequence = torch.tensor(past_sequence)\n\n        prefix_length = find_prefix_length(past_sequence[:i1], new_sequence[:j1])\n        sink_length = max(prefix_length, shared.args.attention_sink_size)\n        removed_length = i1 - sink_length\n\n        if removed_length <= 0:\n            return past_sequence.tolist()\n\n        matching_prefix = past_sequence[:prefix_length]\n        removed_chunk = past_sequence[sink_length:i1]\n        overlapping_sequence = new_sequence[j1:j2 + 1]\n        added_chunk = new_sequence[j2 + 1:]\n\n        # print(past_sequence.tolist())\n        # print(new_sequence.tolist())\n\n        print()\n        print('MATCHING PREFIX=', repr(shared.tokenizer.decode(matching_prefix)))\n        print('ADDED CHUNK=', repr(shared.tokenizer.decode(added_chunk)))\n        print('REMOVED CHUNK=', repr(shared.tokenizer.decode(removed_chunk)))\n        print('REMOVED LENGTH=', removed_length)\n        print()\n\n        # Remove interval [sink_length, sink_length + removed_length) from the context\n        # Update model.n_tokens\n        model._ctx.kv_cache_seq_rm(0, sink_length, sink_length + removed_length)\n        model._ctx.kv_cache_seq_shift(0, sink_length + removed_length, -1, -removed_length)\n\n        new_sequence = new_sequence.tolist()\n        model.input_ids[:j2 + 1] = new_sequence[:j2 + 1]\n        model.n_tokens = j2 + 1\n\n        return new_sequence[:j2 + 1]\n    else:\n        return past_sequence\n\n\ndef find_prefix_length(past_seq, seq_tensor):\n    '''\n    Given two torch tensors, finds the length of the longest\n    common prefix between the two.\n    '''\n    min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n    indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n    if len(indices) > 0:\n        prefix_length = indices[0].item()\n    else:\n        prefix_length = min_length\n\n    return prefix_length\n\n\n@njit\ndef find_longest_common_substring_indices(list1, list2):\n    '''\n    Given two lists, solves the Longest Common Substring problem.\n\n    It returns the indices where the substring starts and ends in\n    s1 and s2.\n\n    Example:\n\n    ir, jr, ir2, jr2 = find_longest_common_substring_indices(s1, s2)\n    print(s1[ir:jr + 1])\n    print(s2[ir2:jr2 + 1])\n\n    Adapted from\n    https://rosettacode.org/wiki/Longest_common_substring#Python\n    '''\n\n    len_list1, len_list2 = len(list1), len(list2)\n    start_index_list1, end_index_list1 = 0, -1\n    start_index_list2, end_index_list2 = 0, -1\n\n    # for index1 in tqdm(range(0, len_list1), desc=\"StreamingLLM prompt comparison\", leave=False):\n    for index1 in range(0, len_list1):\n        try:\n            index2 = list2.index(list1[index1])\n        except:\n            continue\n\n        while index2 >= 0:\n            temp_index1, temp_index2 = index1, index2\n            while temp_index1 < len_list1 and temp_index2 < len_list2 and list2[temp_index2] == list1[temp_index1]:\n                if temp_index1 - index1 >= end_index_list1 - start_index_list1:\n                    start_index_list1, end_index_list1 = index1, temp_index1\n                    start_index_list2, end_index_list2 = index2, temp_index2\n\n                temp_index1 += 1\n                temp_index2 += 1\n            try:\n                index2 = list2.index(list1[index1], index2 + 1)\n            except:\n                break\n\n    return start_index_list1, end_index_list1, start_index_list2, end_index_list2\n", "modules/ui_chat.py": "import json\nfrom functools import partial\nfrom pathlib import Path\n\nimport gradio as gr\nfrom PIL import Image\n\nfrom modules import chat, shared, ui, utils\nfrom modules.html_generator import chat_html_wrapper\nfrom modules.text_generation import stop_everything_event\nfrom modules.utils import gradio\n\ninputs = ('Chat input', 'interface_state')\nreload_arr = ('history', 'name1', 'name2', 'mode', 'chat_style', 'character_menu')\nclear_arr = ('delete_chat-confirm', 'delete_chat', 'delete_chat-cancel')\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n\n    shared.gradio['Chat input'] = gr.State()\n    shared.gradio['history'] = gr.State({'internal': [], 'visible': []})\n\n    with gr.Tab('Chat', elem_id='chat-tab', elem_classes=(\"old-ui\" if shared.args.chat_buttons else None)):\n        with gr.Row():\n            with gr.Column(elem_id='chat-col'):\n                shared.gradio['display'] = gr.HTML(value=chat_html_wrapper({'internal': [], 'visible': []}, '', '', 'chat', 'cai-chat', ''))\n\n                with gr.Row(elem_id=\"chat-input-row\"):\n                    with gr.Column(scale=1, elem_id='gr-hover-container'):\n                        gr.HTML(value='<div class=\"hover-element\" onclick=\"void(0)\"><span style=\"width: 100px; display: block\" id=\"hover-element-button\">&#9776;</span><div class=\"hover-menu\" id=\"hover-menu\"></div>', elem_id='gr-hover')\n\n                    with gr.Column(scale=10, elem_id='chat-input-container'):\n                        shared.gradio['textbox'] = gr.Textbox(label='', placeholder='Send a message', elem_id='chat-input', elem_classes=['add_scrollbar'])\n                        shared.gradio['show_controls'] = gr.Checkbox(value=shared.settings['show_controls'], label='Show controls (Ctrl+S)', elem_id='show-controls')\n                        shared.gradio['typing-dots'] = gr.HTML(value='<div class=\"typing\"><span></span><span class=\"dot1\"></span><span class=\"dot2\"></span></div>', label='typing', elem_id='typing-container')\n\n                    with gr.Column(scale=1, elem_id='generate-stop-container'):\n                        with gr.Row():\n                            shared.gradio['Stop'] = gr.Button('Stop', elem_id='stop', visible=False)\n                            shared.gradio['Generate'] = gr.Button('Generate', elem_id='Generate', variant='primary')\n\n        # Hover menu buttons\n        with gr.Column(elem_id='chat-buttons'):\n            with gr.Row():\n                shared.gradio['Regenerate'] = gr.Button('Regenerate (Ctrl + Enter)', elem_id='Regenerate')\n                shared.gradio['Continue'] = gr.Button('Continue (Alt + Enter)', elem_id='Continue')\n                shared.gradio['Remove last'] = gr.Button('Remove last reply (Ctrl + Shift + Backspace)', elem_id='Remove-last')\n\n            with gr.Row():\n                shared.gradio['Replace last reply'] = gr.Button('Replace last reply (Ctrl + Shift + L)', elem_id='Replace-last')\n                shared.gradio['Copy last reply'] = gr.Button('Copy last reply (Ctrl + Shift + K)', elem_id='Copy-last')\n                shared.gradio['Impersonate'] = gr.Button('Impersonate (Ctrl + Shift + M)', elem_id='Impersonate')\n\n            with gr.Row():\n                shared.gradio['Send dummy message'] = gr.Button('Send dummy message')\n                shared.gradio['Send dummy reply'] = gr.Button('Send dummy reply')\n\n            with gr.Row():\n                shared.gradio['send-chat-to-default'] = gr.Button('Send to default')\n                shared.gradio['send-chat-to-notebook'] = gr.Button('Send to notebook')\n\n        with gr.Row(elem_id='past-chats-row', elem_classes=['pretty_scrollbar']):\n            with gr.Column():\n                with gr.Row():\n                    shared.gradio['unique_id'] = gr.Dropdown(label='Past chats', elem_classes=['slim-dropdown'], interactive=not mu)\n\n                with gr.Row():\n                    shared.gradio['rename_chat'] = gr.Button('Rename', elem_classes='refresh-button', interactive=not mu)\n                    shared.gradio['delete_chat'] = gr.Button('\ud83d\uddd1\ufe0f', elem_classes='refresh-button', interactive=not mu)\n                    shared.gradio['delete_chat-confirm'] = gr.Button('Confirm', variant='stop', visible=False, elem_classes=['refresh-button', 'focus-on-chat-input'])\n                    shared.gradio['delete_chat-cancel'] = gr.Button('Cancel', visible=False, elem_classes=['refresh-button', 'focus-on-chat-input'])\n                    shared.gradio['Start new chat'] = gr.Button('New chat', elem_classes=['refresh-button', 'focus-on-chat-input'])\n\n                with gr.Row(elem_id='rename-row'):\n                    shared.gradio['rename_to'] = gr.Textbox(label='Rename to:', placeholder='New name', visible=False, elem_classes=['no-background'])\n                    shared.gradio['rename_to-confirm'] = gr.Button('Confirm', visible=False, elem_classes=['refresh-button', 'focus-on-chat-input'])\n                    shared.gradio['rename_to-cancel'] = gr.Button('Cancel', visible=False, elem_classes=['refresh-button', 'focus-on-chat-input'])\n\n        with gr.Row(elem_id='chat-controls', elem_classes=['pretty_scrollbar']):\n            with gr.Column():\n                with gr.Row():\n                    shared.gradio['start_with'] = gr.Textbox(label='Start reply with', placeholder='Sure thing!', value=shared.settings['start_with'], elem_classes=['add_scrollbar'])\n\n                with gr.Row():\n                    shared.gradio['mode'] = gr.Radio(choices=['chat', 'chat-instruct', 'instruct'], value='chat', label='Mode', info='Defines how the chat prompt is generated. In instruct and chat-instruct modes, the instruction template selected under Parameters > Instruction template must match the current model.', elem_id='chat-mode')\n\n                with gr.Row():\n                    shared.gradio['chat_style'] = gr.Dropdown(choices=utils.get_available_chat_styles(), label='Chat style', value=shared.settings['chat_style'], visible=shared.settings['mode'] != 'instruct')\n\n                with gr.Row():\n                    shared.gradio['chat-instruct_command'] = gr.Textbox(value=shared.settings['chat-instruct_command'], lines=16, label='Command for chat-instruct mode', info='<|character|> and <|prompt|> get replaced with the bot name and the regular chat prompt respectively.', visible=False, elem_classes=['add_scrollbar'])\n\n\ndef create_chat_settings_ui():\n    mu = shared.args.multi_user\n    with gr.Tab('Chat'):\n        with gr.Row():\n            with gr.Column(scale=8):\n                with gr.Tab(\"Character\"):\n                    with gr.Row():\n                        shared.gradio['character_menu'] = gr.Dropdown(value=None, choices=utils.get_available_characters(), label='Character', elem_id='character-menu', info='Used in chat and chat-instruct modes.', elem_classes='slim-dropdown')\n                        ui.create_refresh_button(shared.gradio['character_menu'], lambda: None, lambda: {'choices': utils.get_available_characters()}, 'refresh-button', interactive=not mu)\n                        shared.gradio['save_character'] = gr.Button('\ud83d\udcbe', elem_classes='refresh-button', interactive=not mu)\n                        shared.gradio['delete_character'] = gr.Button('\ud83d\uddd1\ufe0f', elem_classes='refresh-button', interactive=not mu)\n\n                    shared.gradio['name2'] = gr.Textbox(value='', lines=1, label='Character\\'s name')\n                    shared.gradio['context'] = gr.Textbox(value='', lines=10, label='Context', elem_classes=['add_scrollbar'])\n                    shared.gradio['greeting'] = gr.Textbox(value='', lines=5, label='Greeting', elem_classes=['add_scrollbar'])\n\n                with gr.Tab(\"User\"):\n                    shared.gradio['name1'] = gr.Textbox(value=shared.settings['name1'], lines=1, label='Name')\n                    shared.gradio['user_bio'] = gr.Textbox(value=shared.settings['user_bio'], lines=10, label='Description', info='Here you can optionally write a description of yourself.', placeholder='{{user}}\\'s personality: ...', elem_classes=['add_scrollbar'])\n\n                with gr.Tab('Chat history'):\n                    with gr.Row():\n                        with gr.Column():\n                            shared.gradio['save_chat_history'] = gr.Button(value='Save history')\n\n                        with gr.Column():\n                            shared.gradio['load_chat_history'] = gr.File(type='binary', file_types=['.json', '.txt'], label='Upload History JSON')\n\n                with gr.Tab('Upload character'):\n                    with gr.Tab('YAML or JSON'):\n                        with gr.Row():\n                            shared.gradio['upload_json'] = gr.File(type='binary', file_types=['.json', '.yaml'], label='JSON or YAML File', interactive=not mu)\n                            shared.gradio['upload_img_bot'] = gr.Image(type='pil', label='Profile Picture (optional)', interactive=not mu)\n\n                        shared.gradio['Submit character'] = gr.Button(value='Submit', interactive=False)\n\n                    with gr.Tab('TavernAI PNG'):\n                        with gr.Row():\n                            with gr.Column():\n                                shared.gradio['upload_img_tavern'] = gr.Image(type='pil', label='TavernAI PNG File', elem_id='upload_img_tavern', interactive=not mu)\n                                shared.gradio['tavern_json'] = gr.State()\n                            with gr.Column():\n                                shared.gradio['tavern_name'] = gr.Textbox(value='', lines=1, label='Name', interactive=False)\n                                shared.gradio['tavern_desc'] = gr.Textbox(value='', lines=4, max_lines=4, label='Description', interactive=False)\n\n                        shared.gradio['Submit tavern character'] = gr.Button(value='Submit', interactive=False)\n\n            with gr.Column(scale=1):\n                shared.gradio['character_picture'] = gr.Image(label='Character picture', type='pil', interactive=not mu)\n                shared.gradio['your_picture'] = gr.Image(label='Your picture', type='pil', value=Image.open(Path('cache/pfp_me.png')) if Path('cache/pfp_me.png').exists() else None, interactive=not mu)\n\n    with gr.Tab('Instruction template'):\n        with gr.Row():\n            with gr.Column():\n                with gr.Row():\n                    shared.gradio['instruction_template'] = gr.Dropdown(choices=utils.get_available_instruction_templates(), label='Saved instruction templates', info=\"After selecting the template, click on \\\"Load\\\" to load and apply it.\", value='None', elem_classes='slim-dropdown')\n                    ui.create_refresh_button(shared.gradio['instruction_template'], lambda: None, lambda: {'choices': utils.get_available_instruction_templates()}, 'refresh-button', interactive=not mu)\n                    shared.gradio['load_template'] = gr.Button(\"Load\", elem_classes='refresh-button')\n                    shared.gradio['save_template'] = gr.Button('\ud83d\udcbe', elem_classes='refresh-button', interactive=not mu)\n                    shared.gradio['delete_template'] = gr.Button('\ud83d\uddd1\ufe0f ', elem_classes='refresh-button', interactive=not mu)\n\n            with gr.Column():\n                pass\n\n        with gr.Row():\n            with gr.Column():\n                shared.gradio['custom_system_message'] = gr.Textbox(value=shared.settings['custom_system_message'], lines=2, label='Custom system message', info='If not empty, will be used instead of the default one.', elem_classes=['add_scrollbar'])\n                shared.gradio['instruction_template_str'] = gr.Textbox(value='', label='Instruction template', lines=24, info='Change this according to the model/LoRA that you are using. Used in instruct and chat-instruct modes.', elem_classes=['add_scrollbar', 'monospace'])\n                with gr.Row():\n                    shared.gradio['send_instruction_to_default'] = gr.Button('Send to default', elem_classes=['small-button'])\n                    shared.gradio['send_instruction_to_notebook'] = gr.Button('Send to notebook', elem_classes=['small-button'])\n                    shared.gradio['send_instruction_to_negative_prompt'] = gr.Button('Send to negative prompt', elem_classes=['small-button'])\n\n            with gr.Column():\n                shared.gradio['chat_template_str'] = gr.Textbox(value=shared.settings['chat_template_str'], label='Chat template', lines=22, elem_classes=['add_scrollbar', 'monospace'])\n\n\ndef create_event_handlers():\n\n    # Obsolete variables, kept for compatibility with old extensions\n    shared.input_params = gradio(inputs)\n    shared.reload_inputs = gradio(reload_arr)\n\n    shared.gradio['Generate'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: (x, ''), gradio('textbox'), gradio('Chat input', 'textbox'), show_progress=False).then(\n        chat.generate_chat_reply_wrapper, gradio(inputs), gradio('display', 'history'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['textbox'].submit(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: (x, ''), gradio('textbox'), gradio('Chat input', 'textbox'), show_progress=False).then(\n        chat.generate_chat_reply_wrapper, gradio(inputs), gradio('display', 'history'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Regenerate'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        partial(chat.generate_chat_reply_wrapper, regenerate=True), gradio(inputs), gradio('display', 'history'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Continue'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        partial(chat.generate_chat_reply_wrapper, _continue=True), gradio(inputs), gradio('display', 'history'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Impersonate'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: x, gradio('textbox'), gradio('Chat input'), show_progress=False).then(\n        chat.impersonate_wrapper, gradio(inputs), gradio('textbox', 'display'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Replace last reply'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.replace_last_reply, gradio('textbox', 'interface_state'), gradio('history')).then(\n        lambda: '', None, gradio('textbox'), show_progress=False).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None)\n\n    shared.gradio['Send dummy message'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.send_dummy_message, gradio('textbox', 'interface_state'), gradio('history')).then(\n        lambda: '', None, gradio('textbox'), show_progress=False).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None)\n\n    shared.gradio['Send dummy reply'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.send_dummy_reply, gradio('textbox', 'interface_state'), gradio('history')).then(\n        lambda: '', None, gradio('textbox'), show_progress=False).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None)\n\n    shared.gradio['Remove last'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.remove_last_message, gradio('history'), gradio('textbox', 'history'), show_progress=False).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None)\n\n    shared.gradio['Stop'].click(\n        stop_everything_event, None, None, queue=False).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display'))\n\n    if not shared.args.multi_user:\n        shared.gradio['unique_id'].select(\n            chat.load_history, gradio('unique_id', 'character_menu', 'mode'), gradio('history')).then(\n            chat.redraw_html, gradio(reload_arr), gradio('display'))\n\n    shared.gradio['Start new chat'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.start_new_chat, gradio('interface_state'), gradio('history')).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        lambda x: gr.update(choices=(histories := chat.find_all_histories(x)), value=histories[0]), gradio('interface_state'), gradio('unique_id'))\n\n    shared.gradio['delete_chat'].click(lambda: [gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)], None, gradio(clear_arr))\n    shared.gradio['delete_chat-cancel'].click(lambda: [gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)], None, gradio(clear_arr))\n    shared.gradio['delete_chat-confirm'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x, y: str(chat.find_all_histories(x).index(y)), gradio('interface_state', 'unique_id'), gradio('temporary_text')).then(\n        chat.delete_history, gradio('unique_id', 'character_menu', 'mode'), None).then(\n        chat.load_history_after_deletion, gradio('interface_state', 'temporary_text'), gradio('history', 'unique_id')).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        lambda: [gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)], None, gradio(clear_arr))\n\n    shared.gradio['rename_chat'].click(\n        lambda x: x, gradio('unique_id'), gradio('rename_to')).then(\n        lambda: [gr.update(visible=True)] * 3, None, gradio('rename_to', 'rename_to-confirm', 'rename_to-cancel'), show_progress=False)\n\n    shared.gradio['rename_to-cancel'].click(\n        lambda: [gr.update(visible=False)] * 3, None, gradio('rename_to', 'rename_to-confirm', 'rename_to-cancel'), show_progress=False)\n\n    shared.gradio['rename_to-confirm'].click(\n        chat.rename_history, gradio('unique_id', 'rename_to', 'character_menu', 'mode'), None).then(\n        lambda: [gr.update(visible=False)] * 3, None, gradio('rename_to', 'rename_to-confirm', 'rename_to-cancel'), show_progress=False).then(\n        lambda x, y: gr.update(choices=chat.find_all_histories(x), value=y), gradio('interface_state', 'rename_to'), gradio('unique_id'))\n\n    shared.gradio['rename_to'].submit(\n        chat.rename_history, gradio('unique_id', 'rename_to', 'character_menu', 'mode'), None).then(\n        lambda: [gr.update(visible=False)] * 3, None, gradio('rename_to', 'rename_to-confirm', 'rename_to-cancel'), show_progress=False).then(\n        lambda x, y: gr.update(choices=chat.find_all_histories(x), value=y), gradio('interface_state', 'rename_to'), gradio('unique_id'))\n\n    shared.gradio['load_chat_history'].upload(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.start_new_chat, gradio('interface_state'), gradio('history')).then(\n        chat.load_history_json, gradio('load_chat_history', 'history'), gradio('history')).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        lambda x: gr.update(choices=(histories := chat.find_all_histories(x)), value=histories[0]), gradio('interface_state'), gradio('unique_id')).then(\n        chat.save_history, gradio('history', 'unique_id', 'character_menu', 'mode'), None).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_chat()}}')\n\n    shared.gradio['character_menu'].change(\n        chat.load_character, gradio('character_menu', 'name1', 'name2'), gradio('name1', 'name2', 'character_picture', 'greeting', 'context')).success(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.load_latest_history, gradio('interface_state'), gradio('history')).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        lambda x: gr.update(choices=(histories := chat.find_all_histories(x)), value=histories[0]), gradio('interface_state'), gradio('unique_id')).then(\n        lambda: None, None, None, js=f'() => {{{ui.update_big_picture_js}; updateBigPicture()}}')\n\n    shared.gradio['mode'].change(\n        lambda x: [gr.update(visible=x != 'instruct'), gr.update(visible=x == 'chat-instruct')], gradio('mode'), gradio('chat_style', 'chat-instruct_command'), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        chat.load_latest_history, gradio('interface_state'), gradio('history')).then(\n        chat.redraw_html, gradio(reload_arr), gradio('display')).then(\n        lambda x: gr.update(choices=(histories := chat.find_all_histories(x)), value=histories[0]), gradio('interface_state'), gradio('unique_id'))\n\n    shared.gradio['chat_style'].change(chat.redraw_html, gradio(reload_arr), gradio('display'))\n    shared.gradio['Copy last reply'].click(chat.send_last_reply_to_input, gradio('history'), gradio('textbox'), show_progress=False)\n\n    # Save/delete a character\n    shared.gradio['save_character'].click(\n        lambda x: x, gradio('name2'), gradio('save_character_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('character_saver'))\n\n    shared.gradio['delete_character'].click(lambda: gr.update(visible=True), None, gradio('character_deleter'))\n\n    shared.gradio['load_template'].click(\n        chat.load_instruction_template, gradio('instruction_template'), gradio('instruction_template_str')).then(\n        lambda: \"Select template to load...\", None, gradio('instruction_template'))\n\n    shared.gradio['save_template'].click(\n        lambda: 'My Template.yaml', None, gradio('save_filename')).then(\n        lambda: 'instruction-templates/', None, gradio('save_root')).then(\n        chat.generate_instruction_template_yaml, gradio('instruction_template_str'), gradio('save_contents')).then(\n        lambda: gr.update(visible=True), None, gradio('file_saver'))\n\n    shared.gradio['delete_template'].click(\n        lambda x: f'{x}.yaml', gradio('instruction_template'), gradio('delete_filename')).then(\n        lambda: 'instruction-templates/', None, gradio('delete_root')).then(\n        lambda: gr.update(visible=True), None, gradio('file_deleter'))\n\n    shared.gradio['save_chat_history'].click(\n        lambda x: json.dumps(x, indent=4), gradio('history'), gradio('temporary_text')).then(\n        None, gradio('temporary_text', 'character_menu', 'mode'), None, js=f'(hist, char, mode) => {{{ui.save_files_js}; saveHistory(hist, char, mode)}}')\n\n    shared.gradio['Submit character'].click(\n        chat.upload_character, gradio('upload_json', 'upload_img_bot'), gradio('character_menu')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_character()}}')\n\n    shared.gradio['Submit tavern character'].click(\n        chat.upload_tavern_character, gradio('upload_img_tavern', 'tavern_json'), gradio('character_menu')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_character()}}')\n\n    shared.gradio['upload_json'].upload(lambda: gr.update(interactive=True), None, gradio('Submit character'))\n    shared.gradio['upload_json'].clear(lambda: gr.update(interactive=False), None, gradio('Submit character'))\n    shared.gradio['upload_img_tavern'].upload(chat.check_tavern_character, gradio('upload_img_tavern'), gradio('tavern_name', 'tavern_desc', 'tavern_json', 'Submit tavern character'), show_progress=False)\n    shared.gradio['upload_img_tavern'].clear(lambda: (None, None, None, gr.update(interactive=False)), None, gradio('tavern_name', 'tavern_desc', 'tavern_json', 'Submit tavern character'), show_progress=False)\n    shared.gradio['your_picture'].change(\n        chat.upload_your_profile_picture, gradio('your_picture'), None).then(\n        partial(chat.redraw_html, reset_cache=True), gradio(reload_arr), gradio('display'))\n\n    shared.gradio['send_instruction_to_default'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: x.update({'mode': 'instruct', 'history': {'internal': [], 'visible': []}}), gradio('interface_state'), None).then(\n        partial(chat.generate_chat_prompt, 'Input'), gradio('interface_state'), gradio('textbox-default')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_default()}}')\n\n    shared.gradio['send_instruction_to_notebook'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: x.update({'mode': 'instruct', 'history': {'internal': [], 'visible': []}}), gradio('interface_state'), None).then(\n        partial(chat.generate_chat_prompt, 'Input'), gradio('interface_state'), gradio('textbox-notebook')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_notebook()}}')\n\n    shared.gradio['send_instruction_to_negative_prompt'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: x.update({'mode': 'instruct', 'history': {'internal': [], 'visible': []}}), gradio('interface_state'), None).then(\n        partial(chat.generate_chat_prompt, 'Input'), gradio('interface_state'), gradio('negative_prompt')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_generation_parameters()}}')\n\n    shared.gradio['send-chat-to-default'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        partial(chat.generate_chat_prompt, '', _continue=True), gradio('interface_state'), gradio('textbox-default')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_default()}}')\n\n    shared.gradio['send-chat-to-notebook'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        partial(chat.generate_chat_prompt, '', _continue=True), gradio('interface_state'), gradio('textbox-notebook')).then(\n        lambda: None, None, None, js=f'() => {{{ui.switch_tabs_js}; switch_to_notebook()}}')\n\n    shared.gradio['show_controls'].change(lambda x: None, gradio('show_controls'), None, js=f'(x) => {{{ui.show_controls_js}; toggle_controls(x)}}')\n", "modules/models_settings.py": "import json\nimport re\nfrom pathlib import Path\n\nimport yaml\n\nfrom modules import chat, loaders, metadata_gguf, shared, ui\n\n\ndef get_fallback_settings():\n    return {\n        'wbits': 'None',\n        'groupsize': 'None',\n        'desc_act': False,\n        'max_seq_len': 2048,\n        'n_ctx': 2048,\n        'rope_freq_base': 0,\n        'compress_pos_emb': 1,\n        'truncation_length': shared.settings['truncation_length'],\n        'skip_special_tokens': shared.settings['skip_special_tokens'],\n        'custom_stopping_strings': shared.settings['custom_stopping_strings'],\n    }\n\n\ndef get_model_metadata(model):\n    model_settings = {}\n\n    # Get settings from models/config.yaml and models/config-user.yaml\n    settings = shared.model_config\n    for pat in settings:\n        if re.match(pat.lower(), model.lower()):\n            for k in settings[pat]:\n                model_settings[k] = settings[pat][k]\n\n    path = Path(f'{shared.args.model_dir}/{model}/config.json')\n    if path.exists():\n        hf_metadata = json.loads(open(path, 'r', encoding='utf-8').read())\n    else:\n        hf_metadata = None\n\n    if 'loader' not in model_settings:\n        model_settings['loader'] = infer_loader(model, model_settings)\n\n    # GGUF metadata\n    if model_settings['loader'] in ['llama.cpp', 'llamacpp_HF']:\n        path = Path(f'{shared.args.model_dir}/{model}')\n        if path.is_file():\n            model_file = path\n        else:\n            model_file = list(path.glob('*.gguf'))[0]\n\n        metadata = metadata_gguf.load_metadata(model_file)\n\n        for k in metadata:\n            if k.endswith('context_length'):\n                model_settings['n_ctx'] = metadata[k]\n            elif k.endswith('rope.freq_base'):\n                model_settings['rope_freq_base'] = metadata[k]\n            elif k.endswith('rope.scale_linear'):\n                model_settings['compress_pos_emb'] = metadata[k]\n            elif k.endswith('block_count'):\n                model_settings['n_gpu_layers'] = metadata[k] + 1\n\n        if 'tokenizer.chat_template' in metadata:\n            template = metadata['tokenizer.chat_template']\n            eos_token = metadata['tokenizer.ggml.tokens'][metadata['tokenizer.ggml.eos_token_id']]\n            bos_token = metadata['tokenizer.ggml.tokens'][metadata['tokenizer.ggml.bos_token_id']]\n            template = template.replace('eos_token', \"'{}'\".format(eos_token))\n            template = template.replace('bos_token', \"'{}'\".format(bos_token))\n\n            template = re.sub(r'raise_exception\\([^)]*\\)', \"''\", template)\n            template = re.sub(r'{% if add_generation_prompt %}.*', '', template, flags=re.DOTALL)\n            model_settings['instruction_template'] = 'Custom (obtained from model metadata)'\n            model_settings['instruction_template_str'] = template\n\n    else:\n        # Transformers metadata\n        if hf_metadata is not None:\n            metadata = json.loads(open(path, 'r', encoding='utf-8').read())\n            for k in ['max_position_embeddings', 'model_max_length', 'max_seq_len']:\n                if k in metadata:\n                    model_settings['truncation_length'] = metadata[k]\n                    model_settings['max_seq_len'] = metadata[k]\n\n            if 'rope_theta' in metadata:\n                model_settings['rope_freq_base'] = metadata['rope_theta']\n            elif 'attn_config' in metadata and 'rope_theta' in metadata['attn_config']:\n                model_settings['rope_freq_base'] = metadata['attn_config']['rope_theta']\n\n            if 'rope_scaling' in metadata and type(metadata['rope_scaling']) is dict and all(key in metadata['rope_scaling'] for key in ('type', 'factor')):\n                if metadata['rope_scaling']['type'] == 'linear':\n                    model_settings['compress_pos_emb'] = metadata['rope_scaling']['factor']\n\n            # Read GPTQ metadata for old GPTQ loaders\n            if 'quantization_config' in metadata and metadata['quantization_config'].get('quant_method', '') != 'exl2':\n                if 'bits' in metadata['quantization_config']:\n                    model_settings['wbits'] = metadata['quantization_config']['bits']\n                if 'group_size' in metadata['quantization_config']:\n                    model_settings['groupsize'] = metadata['quantization_config']['group_size']\n                if 'desc_act' in metadata['quantization_config']:\n                    model_settings['desc_act'] = metadata['quantization_config']['desc_act']\n\n        # Read AutoGPTQ metadata\n        path = Path(f'{shared.args.model_dir}/{model}/quantize_config.json')\n        if path.exists():\n            metadata = json.loads(open(path, 'r', encoding='utf-8').read())\n            if 'bits' in metadata:\n                model_settings['wbits'] = metadata['bits']\n            if 'group_size' in metadata:\n                model_settings['groupsize'] = metadata['group_size']\n            if 'desc_act' in metadata:\n                model_settings['desc_act'] = metadata['desc_act']\n\n    # Try to find the Jinja instruct template\n    path = Path(f'{shared.args.model_dir}/{model}') / 'tokenizer_config.json'\n    if path.exists():\n        metadata = json.loads(open(path, 'r', encoding='utf-8').read())\n        if 'chat_template' in metadata:\n            template = metadata['chat_template']\n            if isinstance(template, list):\n                template = template[0]['template']\n\n            for k in ['eos_token', 'bos_token']:\n                if k in metadata:\n                    value = metadata[k]\n                    if type(value) is dict:\n                        value = value['content']\n\n                    template = template.replace(k, \"'{}'\".format(value))\n\n            template = re.sub(r'raise_exception\\([^)]*\\)', \"''\", template)\n            template = re.sub(r'{% if add_generation_prompt %}.*', '', template, flags=re.DOTALL)\n            model_settings['instruction_template'] = 'Custom (obtained from model metadata)'\n            model_settings['instruction_template_str'] = template\n\n    if 'instruction_template' not in model_settings:\n        model_settings['instruction_template'] = 'Alpaca'\n\n    # Ignore rope_freq_base if set to the default value\n    if 'rope_freq_base' in model_settings and model_settings['rope_freq_base'] == 10000:\n        model_settings.pop('rope_freq_base')\n\n    # Apply user settings from models/config-user.yaml\n    settings = shared.user_config\n    for pat in settings:\n        if re.match(pat.lower(), model.lower()):\n            for k in settings[pat]:\n                model_settings[k] = settings[pat][k]\n\n    # Load instruction template if defined by name rather than by value\n    if model_settings['instruction_template'] != 'Custom (obtained from model metadata)':\n        model_settings['instruction_template_str'] = chat.load_instruction_template(model_settings['instruction_template'])\n\n    return model_settings\n\n\ndef infer_loader(model_name, model_settings):\n    path_to_model = Path(f'{shared.args.model_dir}/{model_name}')\n    if not path_to_model.exists():\n        loader = None\n    elif (path_to_model / 'quantize_config.json').exists() or ('wbits' in model_settings and type(model_settings['wbits']) is int and model_settings['wbits'] > 0):\n        loader = 'ExLlamav2_HF'\n    elif (path_to_model / 'quant_config.json').exists() or re.match(r'.*-awq', model_name.lower()):\n        loader = 'AutoAWQ'\n    elif len(list(path_to_model.glob('*.gguf'))) > 0 and path_to_model.is_dir() and (path_to_model / 'tokenizer_config.json').exists():\n        loader = 'llamacpp_HF'\n    elif len(list(path_to_model.glob('*.gguf'))) > 0:\n        loader = 'llama.cpp'\n    elif re.match(r'.*\\.gguf', model_name.lower()):\n        loader = 'llama.cpp'\n    elif re.match(r'.*exl2', model_name.lower()):\n        loader = 'ExLlamav2_HF'\n    elif re.match(r'.*-hqq', model_name.lower()):\n        return 'HQQ'\n    else:\n        loader = 'Transformers'\n\n    return loader\n\n\ndef update_model_parameters(state, initial=False):\n    '''\n    UI: update the command-line arguments based on the interface values\n    '''\n    elements = ui.list_model_elements()  # the names of the parameters\n    gpu_memories = []\n\n    for i, element in enumerate(elements):\n        if element not in state:\n            continue\n\n        value = state[element]\n        if element.startswith('gpu_memory'):\n            gpu_memories.append(value)\n            continue\n\n        if initial and element in shared.provided_arguments:\n            continue\n\n        # Setting null defaults\n        if element in ['wbits', 'groupsize'] and value == 'None':\n            value = vars(shared.args_defaults)[element]\n        elif element in ['cpu_memory'] and value == 0:\n            value = vars(shared.args_defaults)[element]\n\n        # Making some simple conversions\n        if element in ['wbits', 'groupsize']:\n            value = int(value)\n        elif element == 'cpu_memory' and value is not None:\n            value = f\"{value}MiB\"\n\n        setattr(shared.args, element, value)\n\n    found_positive = False\n    for i in gpu_memories:\n        if i > 0:\n            found_positive = True\n            break\n\n    if not (initial and vars(shared.args)['gpu_memory'] != vars(shared.args_defaults)['gpu_memory']):\n        if found_positive:\n            shared.args.gpu_memory = [f\"{i}MiB\" for i in gpu_memories]\n        else:\n            shared.args.gpu_memory = None\n\n\ndef apply_model_settings_to_state(model, state):\n    '''\n    UI: update the state variable with the model settings\n    '''\n    model_settings = get_model_metadata(model)\n    if 'loader' in model_settings:\n        loader = model_settings.pop('loader')\n\n        # If the user is using an alternative loader for the same model type, let them keep using it\n        if not (loader == 'ExLlamav2_HF' and state['loader'] in ['ExLlamav2', 'AutoGPTQ']):\n            state['loader'] = loader\n\n    for k in model_settings:\n        if k in state:\n            if k in ['wbits', 'groupsize']:\n                state[k] = str(model_settings[k])\n            else:\n                state[k] = model_settings[k]\n\n    return state\n\n\ndef save_model_settings(model, state):\n    '''\n    Save the settings for this model to models/config-user.yaml\n    '''\n    if model == 'None':\n        yield (\"Not saving the settings because no model is selected in the menu.\")\n        return\n\n    user_config = shared.load_user_config()\n    model_regex = model + '$'  # For exact matches\n    if model_regex not in user_config:\n        user_config[model_regex] = {}\n\n    for k in ui.list_model_elements():\n        if k == 'loader' or k in loaders.loaders_and_params[state['loader']]:\n            user_config[model_regex][k] = state[k]\n\n    shared.user_config = user_config\n\n    output = yaml.dump(user_config, sort_keys=False)\n    p = Path(f'{shared.args.model_dir}/config-user.yaml')\n    with open(p, 'w') as f:\n        f.write(output)\n\n    yield (f\"Settings for `{model}` saved to `{p}`.\")\n\n\ndef save_instruction_template(model, template):\n    '''\n    Similar to the function above, but it saves only the instruction template.\n    '''\n    if model == 'None':\n        yield (\"Not saving the template because no model is selected in the menu.\")\n        return\n\n    user_config = shared.load_user_config()\n    model_regex = model + '$'  # For exact matches\n    if model_regex not in user_config:\n        user_config[model_regex] = {}\n\n    if template == 'None':\n        user_config[model_regex].pop('instruction_template', None)\n    else:\n        user_config[model_regex]['instruction_template'] = template\n\n    shared.user_config = user_config\n\n    output = yaml.dump(user_config, sort_keys=False)\n    p = Path(f'{shared.args.model_dir}/config-user.yaml')\n    with open(p, 'w') as f:\n        f.write(output)\n\n    if template == 'None':\n        yield (f\"Instruction template for `{model}` unset in `{p}`, as the value for template was `{template}`.\")\n    else:\n        yield (f\"Instruction template for `{model}` saved to `{p}` as `{template}`.\")\n", "modules/text_generation.py": "import ast\nimport copy\nimport html\nimport pprint\nimport random\nimport time\nimport traceback\n\nimport numpy as np\nimport torch\nimport transformers\nfrom transformers import (\n    LogitsProcessorList,\n    is_torch_npu_available,\n    is_torch_xpu_available\n)\n\nimport modules.shared as shared\nfrom modules import models\nfrom modules.cache_utils import process_llamacpp_cache\nfrom modules.callbacks import (\n    Iteratorize,\n    Stream,\n    _StopEverythingStoppingCriteria\n)\nfrom modules.extensions import apply_extensions\nfrom modules.grammar.grammar_utils import initialize_grammar\nfrom modules.grammar.logits_process import GrammarConstrainedLogitsProcessor\nfrom modules.html_generator import generate_basic_html\nfrom modules.logging_colors import logger\nfrom modules.models import clear_torch_cache, load_model\n\n\ndef generate_reply(*args, **kwargs):\n    if shared.args.idle_timeout > 0 and shared.model is None and shared.previous_model_name not in [None, 'None']:\n        shared.model, shared.tokenizer = load_model(shared.previous_model_name)\n\n    shared.generation_lock.acquire()\n    try:\n        for result in _generate_reply(*args, **kwargs):\n            yield result\n    finally:\n        models.last_generation_time = time.time()\n        shared.generation_lock.release()\n\n\ndef _generate_reply(question, state, stopping_strings=None, is_chat=False, escape_html=False, for_ui=False):\n\n    # Find the appropriate generation function\n    generate_func = apply_extensions('custom_generate_reply')\n    if generate_func is None:\n        if shared.model_name == 'None' or shared.model is None:\n            logger.error(\"No model is loaded! Select one in the Model tab.\")\n            yield ''\n            return\n\n        if shared.model.__class__.__name__ in ['LlamaCppModel', 'Exllamav2Model']:\n            generate_func = generate_reply_custom\n        else:\n            generate_func = generate_reply_HF\n\n    if generate_func != generate_reply_HF and shared.args.verbose:\n        logger.info(\"PROMPT=\")\n        print_prompt(question)\n\n    # Prepare the input\n    original_question = question\n    if not is_chat:\n        state = apply_extensions('state', state)\n        question = apply_extensions('input', question, state)\n\n    # Find the stopping strings\n    all_stop_strings = []\n    for st in (stopping_strings, state['custom_stopping_strings']):\n        if type(st) is str:\n            st = ast.literal_eval(f\"[{st}]\")\n\n        if type(st) is list and len(st) > 0:\n            all_stop_strings += st\n\n    shared.stop_everything = False\n    clear_torch_cache()\n    seed = set_manual_seed(state['seed'])\n    last_update = -1\n    reply = ''\n    is_stream = state['stream']\n    if len(all_stop_strings) > 0 and not state['stream']:\n        state = copy.deepcopy(state)\n        state['stream'] = True\n\n    min_update_interval = 0\n    if state.get('max_updates_second', 0) > 0:\n        min_update_interval = 1 / state['max_updates_second']\n\n    # Generate\n    for reply in generate_func(question, original_question, seed, state, stopping_strings, is_chat=is_chat):\n        reply, stop_found = apply_stopping_strings(reply, all_stop_strings)\n        if escape_html:\n            reply = html.escape(reply)\n\n        if is_stream:\n            cur_time = time.time()\n\n            # Limit number of tokens/second to make text readable in real time\n            if state['max_tokens_second'] > 0:\n                diff = 1 / state['max_tokens_second'] - (cur_time - last_update)\n                if diff > 0:\n                    time.sleep(diff)\n\n                last_update = time.time()\n                yield reply\n\n            # Limit updates to avoid lag in the Gradio UI\n            # API updates are not limited\n            else:\n                if cur_time - last_update > min_update_interval:\n                    last_update = cur_time\n                    yield reply\n\n                yield reply\n\n        if stop_found or (state['max_tokens_second'] > 0 and shared.stop_everything):\n            break\n\n    if not is_chat:\n        reply = apply_extensions('output', reply, state)\n\n    yield reply\n\n\ndef encode(prompt, add_special_tokens=True, add_bos_token=True, truncation_length=None):\n    if shared.tokenizer is None:\n        raise ValueError('No tokenizer is loaded')\n\n    if shared.model.__class__.__name__ in ['LlamaCppModel', 'Exllamav2Model']:\n        input_ids = shared.tokenizer.encode(str(prompt))\n        if shared.model.__class__.__name__ not in ['Exllamav2Model']:\n            input_ids = np.array(input_ids).reshape(1, len(input_ids))\n    else:\n        input_ids = shared.tokenizer.encode(str(prompt), return_tensors='pt', add_special_tokens=add_special_tokens)\n\n        if hasattr(shared.tokenizer, 'bos_token_id') and shared.tokenizer.bos_token_id is not None:\n            if add_bos_token:\n                if (len(input_ids[0]) > 0 and input_ids[0][0] != shared.tokenizer.bos_token_id) or len(input_ids[0]) == 0:\n                    # Add a missing bos token (it may not have been added due to faulty model metadata)\n                    bos_tensor = torch.tensor([[shared.tokenizer.bos_token_id]])\n                    input_ids = torch.cat((bos_tensor, input_ids), 1)\n\n                # Prevent double bos token due to jinja templates with <s> somewhere\n                while len(input_ids[0]) > 1 and input_ids[0][0] == shared.tokenizer.bos_token_id and input_ids[0][1] == shared.tokenizer.bos_token_id:\n                    input_ids = input_ids[:, 1:]\n            else:\n                # Remove any bos token that may have been added\n                while len(input_ids[0]) > 0 and input_ids[0][0] == shared.tokenizer.bos_token_id:\n                    input_ids = input_ids[:, 1:]\n\n    # Handling truncation\n    if truncation_length is not None:\n        input_ids = input_ids[:, -truncation_length:]\n\n    if shared.model.__class__.__name__ in ['LlamaCppModel', 'Exllamav2Model'] or shared.args.cpu:\n        return input_ids\n    elif shared.args.deepspeed:\n        import deepspeed\n        return input_ids.to(deepspeed.get_accelerator().current_device_name())\n    elif torch.backends.mps.is_available():\n        device = torch.device('mps')\n        return input_ids.to(device)\n    elif is_torch_xpu_available():\n        return input_ids.to(\"xpu:0\")\n    elif is_torch_npu_available():\n        return input_ids.to(\"npu:0\")\n    else:\n        return input_ids.cuda()\n\n\ndef decode(output_ids, skip_special_tokens=True):\n    if shared.tokenizer is None:\n        raise ValueError('No tokenizer is loaded')\n\n    return shared.tokenizer.decode(output_ids, skip_special_tokens=skip_special_tokens)\n\n\ndef get_encoded_length(prompt):\n    length_after_extensions = apply_extensions('tokenized_length', prompt)\n    if length_after_extensions is not None:\n        return length_after_extensions\n\n    return len(encode(prompt)[0])\n\n\ndef get_token_ids(prompt):\n    tokens = encode(prompt)[0]\n    decoded_tokens = [shared.tokenizer.decode([i]) for i in tokens]\n\n    output = ''\n    for row in list(zip(tokens, decoded_tokens)):\n        output += f\"{str(int(row[0])).ljust(5)}  -  {repr(row[1])}\\n\"\n\n    return output\n\n\ndef get_max_prompt_length(state):\n    return state['truncation_length'] - state['max_new_tokens']\n\n\ndef generate_reply_wrapper(question, state, stopping_strings=None):\n    \"\"\"\n    Returns formatted outputs for the UI\n    \"\"\"\n    reply = question if not shared.is_seq2seq else ''\n    yield formatted_outputs(reply, shared.model_name)\n\n    for reply in generate_reply(question, state, stopping_strings, is_chat=False, escape_html=True, for_ui=True):\n        if not shared.is_seq2seq:\n            reply = question + reply\n\n        yield formatted_outputs(reply, shared.model_name)\n\n\ndef formatted_outputs(reply, model_name):\n    return html.unescape(reply), generate_basic_html(reply)\n\n\ndef set_manual_seed(seed):\n    seed = int(seed)\n    if seed == -1:\n        seed = random.randint(1, 2**31)\n\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    elif is_torch_xpu_available():\n        torch.xpu.manual_seed_all(seed)\n    elif is_torch_npu_available():\n        torch.npu.manual_seed_all(seed)\n\n    return seed\n\n\ndef stop_everything_event():\n    shared.stop_everything = True\n\n\ndef apply_stopping_strings(reply, all_stop_strings):\n    stop_found = False\n    for string in all_stop_strings:\n        idx = reply.find(string)\n        if idx != -1:\n            reply = reply[:idx]\n            stop_found = True\n            break\n\n    if not stop_found:\n        # If something like \"\\nYo\" is generated just before \"\\nYou:\"\n        # is completed, trim it\n        for string in all_stop_strings:\n            for j in range(len(string) - 1, 0, -1):\n                if reply[-j:] == string[:j]:\n                    reply = reply[:-j]\n                    break\n            else:\n                continue\n\n            break\n\n    return reply, stop_found\n\n\ndef get_reply_from_output_ids(output_ids, state=None, starting_from=0):\n    reply = decode(output_ids[starting_from:], state['skip_special_tokens'] if state else True)\n\n    # Handle tokenizers that do not add the leading space for the first token\n    if (hasattr(shared.tokenizer, 'convert_ids_to_tokens') and len(output_ids) > starting_from) and not reply.startswith(' '):\n        first_token = shared.tokenizer.convert_ids_to_tokens(int(output_ids[starting_from]))\n        if isinstance(first_token, (bytes,)):\n            first_token = first_token.decode('utf8')\n\n        if first_token.startswith('\u2581'):\n            reply = ' ' + reply\n\n    return reply\n\n\ndef generate_reply_HF(question, original_question, seed, state, stopping_strings=None, is_chat=False):\n    generate_params = {}\n    for k in ['max_new_tokens', 'temperature', 'temperature_last', 'dynamic_temperature', 'dynatemp_low', 'dynatemp_high', 'dynatemp_exponent', 'smoothing_factor', 'smoothing_curve', 'top_p', 'min_p', 'top_k', 'repetition_penalty', 'presence_penalty', 'frequency_penalty', 'repetition_penalty_range', 'typical_p', 'tfs', 'top_a', 'guidance_scale', 'penalty_alpha', 'mirostat_mode', 'mirostat_tau', 'mirostat_eta', 'do_sample', 'encoder_repetition_penalty', 'no_repeat_ngram_size', 'dry_multiplier', 'dry_base', 'dry_allowed_length', 'dry_sequence_breakers']:\n        if k in state:\n            generate_params[k] = state[k]\n\n    if isinstance(state['sampler_priority'], list) and len(state['sampler_priority']) > 0:\n        generate_params['sampler_priority'] = state['sampler_priority']\n    elif isinstance(state['sampler_priority'], str) and state['sampler_priority'].strip() != '':\n        generate_params['sampler_priority'] = [x.strip() for x in state['sampler_priority'].replace('\\n', ',').split(',') if x.strip()]\n\n    if state['negative_prompt'] != '':\n        generate_params['negative_prompt_ids'] = encode(state['negative_prompt'])\n\n    if state['prompt_lookup_num_tokens'] > 0:\n        generate_params['prompt_lookup_num_tokens'] = state['prompt_lookup_num_tokens']\n\n    for k in ['epsilon_cutoff', 'eta_cutoff']:\n        if state[k] > 0:\n            generate_params[k] = state[k] * 1e-4\n\n    if state['ban_eos_token']:\n        generate_params['suppress_tokens'] = [shared.tokenizer.eos_token_id]\n\n    if state['custom_token_bans']:\n        to_ban = [int(x) for x in state['custom_token_bans'].split(',')]\n        if len(to_ban) > 0:\n            if generate_params.get('suppress_tokens', None):\n                generate_params['suppress_tokens'] += to_ban\n            else:\n                generate_params['suppress_tokens'] = to_ban\n\n    generate_params.update({'use_cache': not shared.args.no_cache})\n    if shared.args.deepspeed:\n        generate_params.update({'synced_gpus': True})\n\n    # Encode the input\n    input_ids = encode(question, add_bos_token=state['add_bos_token'], truncation_length=get_max_prompt_length(state))\n    output = input_ids[0]\n    cuda = not any((shared.args.cpu, shared.args.deepspeed))\n    if state['auto_max_new_tokens']:\n        generate_params['max_new_tokens'] = state['truncation_length'] - input_ids.shape[-1]\n\n    # Add the encoded tokens to generate_params\n    question, input_ids, inputs_embeds = apply_extensions('tokenizer', state, question, input_ids, None)\n    original_input_ids = input_ids\n    generate_params.update({'inputs': input_ids})\n    if inputs_embeds is not None:\n        generate_params.update({'inputs_embeds': inputs_embeds})\n\n    # Stopping criteria / eos token\n    eos_token_ids = [shared.tokenizer.eos_token_id] if shared.tokenizer.eos_token_id is not None else []\n    generate_params['eos_token_id'] = eos_token_ids\n    generate_params['stopping_criteria'] = transformers.StoppingCriteriaList()\n    generate_params['stopping_criteria'].append(_StopEverythingStoppingCriteria())\n\n    # Logits processor\n    processor = state.get('logits_processor', LogitsProcessorList([]))\n    if not isinstance(processor, LogitsProcessorList):\n        processor = LogitsProcessorList([processor])\n\n    # Grammar\n    if state['grammar_string'].strip() != '':\n        grammar = initialize_grammar(state['grammar_string'])\n        grammar_processor = GrammarConstrainedLogitsProcessor(grammar)\n        processor.append(grammar_processor)\n\n    apply_extensions('logits_processor', processor, input_ids)\n    generate_params['logits_processor'] = processor\n\n    if shared.args.verbose:\n        logger.info(\"GENERATE_PARAMS=\")\n        filtered_params = {key: value for key, value in generate_params.items() if not isinstance(value, torch.Tensor)}\n        pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint(filtered_params)\n        print()\n\n        logger.info(\"PROMPT=\")\n        print_prompt(decode(input_ids[0], skip_special_tokens=False))\n\n    # Handle StreamingLLM for llamacpp_HF\n    if shared.model.__class__.__name__ == 'LlamacppHF' and shared.args.streaming_llm:\n        tmp = process_llamacpp_cache(shared.model.model, input_ids[-1].tolist(), shared.model.model._input_ids.tolist())\n        shared.model.past_seq = torch.tensor(tmp)\n        shared.model.save_cache()\n\n    t0 = time.time()\n    try:\n        if not is_chat and not shared.is_seq2seq:\n            yield ''\n\n        # Generate the entire reply at once.\n        if not state['stream']:\n            with torch.no_grad():\n                output = shared.model.generate(**generate_params)[0]\n                if cuda:\n                    output = output.cuda()\n\n            starting_from = 0 if shared.is_seq2seq else len(input_ids[0])\n            yield get_reply_from_output_ids(output, state, starting_from=starting_from)\n\n        # Stream the reply 1 token at a time.\n        # This is based on the trick of using 'stopping_criteria' to create an iterator.\n        else:\n\n            def generate_with_callback(callback=None, *args, **kwargs):\n                kwargs['stopping_criteria'].append(Stream(callback_func=callback))\n                clear_torch_cache()\n                with torch.no_grad():\n                    shared.model.generate(**kwargs)\n\n            def generate_with_streaming(**kwargs):\n                return Iteratorize(generate_with_callback, [], kwargs, callback=None)\n\n            with generate_with_streaming(**generate_params) as generator:\n                cumulative_reply = ''\n                starting_from = 0 if shared.is_seq2seq else len(input_ids[0])\n                for output in generator:\n                    if output[-1] in eos_token_ids:\n                        break\n\n                    new_content = get_reply_from_output_ids(output, state, starting_from=starting_from)\n                    # check the partial unicode character\n                    if chr(0xfffd) in new_content:\n                        continue\n\n                    cumulative_reply += new_content\n                    starting_from = len(output)\n                    yield cumulative_reply\n\n    except Exception:\n        traceback.print_exc()\n    finally:\n        t1 = time.time()\n        original_tokens = len(original_input_ids[0])\n        new_tokens = len(output) - (original_tokens if not shared.is_seq2seq else 0)\n        print(f'Output generated in {(t1-t0):.2f} seconds ({new_tokens/(t1-t0):.2f} tokens/s, {new_tokens} tokens, context {original_tokens}, seed {seed})')\n        return\n\n\ndef generate_reply_custom(question, original_question, seed, state, stopping_strings=None, is_chat=False):\n    \"\"\"\n    For models that do not use the transformers library for sampling\n    \"\"\"\n    seed = set_manual_seed(state['seed'])\n\n    t0 = time.time()\n    reply = ''\n    try:\n        if not is_chat:\n            yield ''\n\n        if not state['stream']:\n            reply = shared.model.generate(question, state)\n            yield reply\n        else:\n            for reply in shared.model.generate_with_streaming(question, state):\n                yield reply\n\n    except Exception:\n        traceback.print_exc()\n    finally:\n        t1 = time.time()\n        original_tokens = len(encode(original_question)[0])\n        new_tokens = len(encode(original_question + reply)[0]) - original_tokens\n        print(f'Output generated in {(t1-t0):.2f} seconds ({new_tokens/(t1-t0):.2f} tokens/s, {new_tokens} tokens, context {original_tokens}, seed {seed})')\n        return\n\n\ndef print_prompt(prompt, max_chars=2000):\n    DARK_YELLOW = \"\\033[38;5;3m\"\n    RESET = \"\\033[0m\"\n\n    if len(prompt) > max_chars:\n        half_chars = max_chars // 2\n        hidden_len = len(prompt[half_chars:-half_chars])\n        hidden_msg = f\"{DARK_YELLOW}[...{hidden_len} characters hidden...]{RESET}\"\n        print(prompt[:half_chars] + hidden_msg + prompt[-half_chars:])\n    else:\n        print(prompt)\n\n    print()\n", "modules/relative_imports.py": "import sys\nfrom pathlib import Path\n\n\nclass RelativeImport:\n    def __init__(self, path):\n        self.import_path = Path(path)\n\n    def __enter__(self):\n        sys.path.insert(0, str(self.import_path))\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        sys.path.remove(str(self.import_path))\n", "modules/one_click_installer_check.py": "from pathlib import Path\n\nfrom modules.logging_colors import logger\n\nif Path('../webui.py').exists():\n    logger.warning('\\nIt looks like you are running an outdated version of '\n                   'the one-click-installers.\\n'\n                   'Please migrate your installation following the instructions here:\\n'\n                   'https://github.com/oobabooga/text-generation-webui/wiki/Migrating-an-old-one%E2%80%90click-install')\n", "modules/ui_parameters.py": "from pathlib import Path\n\nimport gradio as gr\n\nfrom modules import loaders, presets, shared, ui, ui_chat, utils\nfrom modules.utils import gradio\n\n\ndef create_ui(default_preset):\n    mu = shared.args.multi_user\n    generate_params = presets.load_preset(default_preset)\n    with gr.Tab(\"Parameters\", elem_id=\"parameters\"):\n        with gr.Tab(\"Generation\"):\n            with gr.Row():\n                with gr.Column():\n                    with gr.Row():\n                        shared.gradio['preset_menu'] = gr.Dropdown(choices=utils.get_available_presets(), value=default_preset, label='Preset', elem_classes='slim-dropdown')\n                        ui.create_refresh_button(shared.gradio['preset_menu'], lambda: None, lambda: {'choices': utils.get_available_presets()}, 'refresh-button', interactive=not mu)\n                        shared.gradio['save_preset'] = gr.Button('\ud83d\udcbe', elem_classes='refresh-button', interactive=not mu)\n                        shared.gradio['delete_preset'] = gr.Button('\ud83d\uddd1\ufe0f', elem_classes='refresh-button', interactive=not mu)\n                        shared.gradio['random_preset'] = gr.Button('\ud83c\udfb2', elem_classes='refresh-button')\n\n                with gr.Column():\n                    shared.gradio['filter_by_loader'] = gr.Dropdown(label=\"Filter by loader\", choices=[\"All\"] + list(loaders.loaders_and_params.keys()), value=\"All\", elem_classes='slim-dropdown')\n\n            with gr.Row():\n                with gr.Column():\n                    with gr.Row():\n                        with gr.Column():\n                            shared.gradio['max_new_tokens'] = gr.Slider(minimum=shared.settings['max_new_tokens_min'], maximum=shared.settings['max_new_tokens_max'], step=1, label='max_new_tokens', value=shared.settings['max_new_tokens'])\n                            shared.gradio['temperature'] = gr.Slider(0.01, 5, value=generate_params['temperature'], step=0.01, label='temperature')\n                            shared.gradio['top_p'] = gr.Slider(0.0, 1.0, value=generate_params['top_p'], step=0.01, label='top_p')\n                            shared.gradio['top_k'] = gr.Slider(0, 200, value=generate_params['top_k'], step=1, label='top_k')\n                            shared.gradio['typical_p'] = gr.Slider(0.0, 1.0, value=generate_params['typical_p'], step=0.01, label='typical_p')\n                            shared.gradio['min_p'] = gr.Slider(0.0, 1.0, value=generate_params['min_p'], step=0.01, label='min_p')\n                            shared.gradio['repetition_penalty'] = gr.Slider(1.0, 1.5, value=generate_params['repetition_penalty'], step=0.01, label='repetition_penalty')\n                            shared.gradio['frequency_penalty'] = gr.Slider(0, 2, value=generate_params['frequency_penalty'], step=0.05, label='frequency_penalty')\n                            shared.gradio['presence_penalty'] = gr.Slider(0, 2, value=generate_params['presence_penalty'], step=0.05, label='presence_penalty')\n                            shared.gradio['repetition_penalty_range'] = gr.Slider(0, 4096, step=64, value=generate_params['repetition_penalty_range'], label='repetition_penalty_range')\n                            shared.gradio['do_sample'] = gr.Checkbox(value=generate_params['do_sample'], label='do_sample')\n\n                            with gr.Blocks():\n                                gr.Markdown(\"[DRY sequence repetition penalty](https://github.com/oobabooga/text-generation-webui/pull/5677)\")\n                                shared.gradio['dry_multiplier'] = gr.Slider(0, 5, value=generate_params['dry_multiplier'], step=0.01, label='dry_multiplier', info='Set to value > 0 to enable DRY. Controls the magnitude of the penalty for the shortest penalized sequences.')\n                                shared.gradio['dry_base'] = gr.Slider(1, 4, value=generate_params['dry_base'], step=0.01, label='dry_base', info='Controls how fast the penalty grows with increasing sequence length.')\n                                shared.gradio['dry_allowed_length'] = gr.Slider(1, 20, value=generate_params['dry_allowed_length'], step=1, label='dry_allowed_length', info='Longest sequence that can be repeated without being penalized.')\n                                shared.gradio['dry_sequence_breakers'] = gr.Textbox(value=generate_params['dry_sequence_breakers'], label='dry_sequence_breakers', info='Tokens across which sequence matching is not continued. Specified as a comma-separated list of quoted strings.')\n\n                            gr.Markdown(\"[Learn more](https://github.com/oobabooga/text-generation-webui/wiki/03-%E2%80%90-Parameters-Tab)\")\n\n                        with gr.Column():\n                            with gr.Group():\n                                shared.gradio['auto_max_new_tokens'] = gr.Checkbox(value=shared.settings['auto_max_new_tokens'], label='auto_max_new_tokens', info='Expand max_new_tokens to the available context length.')\n                                shared.gradio['ban_eos_token'] = gr.Checkbox(value=shared.settings['ban_eos_token'], label='Ban the eos_token', info='Forces the model to never end the generation prematurely.')\n                                shared.gradio['add_bos_token'] = gr.Checkbox(value=shared.settings['add_bos_token'], label='Add the bos_token to the beginning of prompts', info='Disabling this can make the replies more creative.')\n                                shared.gradio['custom_stopping_strings'] = gr.Textbox(lines=2, value=shared.settings[\"custom_stopping_strings\"] or None, label='Custom stopping strings', info='Written between \"\" and separated by commas.', placeholder='\"\\\\n\", \"\\\\nYou:\"')\n                                shared.gradio['custom_token_bans'] = gr.Textbox(value=shared.settings['custom_token_bans'] or None, label='Token bans', info='Token IDs to ban, separated by commas. The IDs can be found in the Default or Notebook tab.')\n\n                            shared.gradio['penalty_alpha'] = gr.Slider(0, 5, value=generate_params['penalty_alpha'], label='penalty_alpha', info='For Contrastive Search. do_sample must be unchecked.')\n                            shared.gradio['guidance_scale'] = gr.Slider(-0.5, 2.5, step=0.05, value=generate_params['guidance_scale'], label='guidance_scale', info='For CFG. 1.5 is a good value.')\n                            shared.gradio['negative_prompt'] = gr.Textbox(value=shared.settings['negative_prompt'], label='Negative prompt', lines=3, elem_classes=['add_scrollbar'])\n                            shared.gradio['mirostat_mode'] = gr.Slider(0, 2, step=1, value=generate_params['mirostat_mode'], label='mirostat_mode', info='mode=1 is for llama.cpp only.')\n                            shared.gradio['mirostat_tau'] = gr.Slider(0, 10, step=0.01, value=generate_params['mirostat_tau'], label='mirostat_tau')\n                            shared.gradio['mirostat_eta'] = gr.Slider(0, 1, step=0.01, value=generate_params['mirostat_eta'], label='mirostat_eta')\n                            shared.gradio['epsilon_cutoff'] = gr.Slider(0, 9, value=generate_params['epsilon_cutoff'], step=0.01, label='epsilon_cutoff')\n                            shared.gradio['eta_cutoff'] = gr.Slider(0, 20, value=generate_params['eta_cutoff'], step=0.01, label='eta_cutoff')\n                            shared.gradio['encoder_repetition_penalty'] = gr.Slider(0.8, 1.5, value=generate_params['encoder_repetition_penalty'], step=0.01, label='encoder_repetition_penalty')\n                            shared.gradio['no_repeat_ngram_size'] = gr.Slider(0, 20, step=1, value=generate_params['no_repeat_ngram_size'], label='no_repeat_ngram_size')\n\n                with gr.Column():\n                    with gr.Row() as shared.gradio['grammar_file_row']:\n                        shared.gradio['grammar_file'] = gr.Dropdown(value='None', choices=utils.get_available_grammars(), label='Load grammar from file (.gbnf)', elem_classes='slim-dropdown')\n                        ui.create_refresh_button(shared.gradio['grammar_file'], lambda: None, lambda: {'choices': utils.get_available_grammars()}, 'refresh-button', interactive=not mu)\n                        shared.gradio['save_grammar'] = gr.Button('\ud83d\udcbe', elem_classes='refresh-button', interactive=not mu)\n                        shared.gradio['delete_grammar'] = gr.Button('\ud83d\uddd1\ufe0f ', elem_classes='refresh-button', interactive=not mu)\n\n                    shared.gradio['grammar_string'] = gr.Textbox(value='', label='Grammar', lines=16, elem_classes=['add_scrollbar', 'monospace'])\n\n                    with gr.Row():\n                        with gr.Column():\n                            shared.gradio['tfs'] = gr.Slider(0.0, 1.0, value=generate_params['tfs'], step=0.01, label='tfs')\n                            shared.gradio['top_a'] = gr.Slider(0.0, 1.0, value=generate_params['top_a'], step=0.01, label='top_a')\n                            shared.gradio['smoothing_factor'] = gr.Slider(0.0, 10.0, value=generate_params['smoothing_factor'], step=0.01, label='smoothing_factor', info='Activates Quadratic Sampling.')\n                            shared.gradio['smoothing_curve'] = gr.Slider(1.0, 10.0, value=generate_params['smoothing_curve'], step=0.01, label='smoothing_curve', info='Adjusts the dropoff curve of Quadratic Sampling.')\n                            shared.gradio['dynamic_temperature'] = gr.Checkbox(value=generate_params['dynamic_temperature'], label='dynamic_temperature')\n                            shared.gradio['dynatemp_low'] = gr.Slider(0.01, 5, value=generate_params['dynatemp_low'], step=0.01, label='dynatemp_low', visible=generate_params['dynamic_temperature'])\n                            shared.gradio['dynatemp_high'] = gr.Slider(0.01, 5, value=generate_params['dynatemp_high'], step=0.01, label='dynatemp_high', visible=generate_params['dynamic_temperature'])\n                            shared.gradio['dynatemp_exponent'] = gr.Slider(0.01, 5, value=generate_params['dynatemp_exponent'], step=0.01, label='dynatemp_exponent', visible=generate_params['dynamic_temperature'])\n                            shared.gradio['temperature_last'] = gr.Checkbox(value=generate_params['temperature_last'], label='temperature_last', info='Moves temperature/dynamic temperature/quadratic sampling to the end of the sampler stack, ignoring their positions in \"Sampler priority\".')\n                            shared.gradio['sampler_priority'] = gr.Textbox(value=generate_params['sampler_priority'], lines=12, label='Sampler priority', info='Parameter names separated by new lines or commas.')\n\n                        with gr.Column():\n                            shared.gradio['truncation_length'] = gr.Slider(value=get_truncation_length(), minimum=shared.settings['truncation_length_min'], maximum=shared.settings['truncation_length_max'], step=256, label='Truncate the prompt up to this length', info='The leftmost tokens are removed if the prompt exceeds this length. Most models require this to be at most 2048.')\n                            shared.gradio['prompt_lookup_num_tokens'] = gr.Slider(value=shared.settings['prompt_lookup_num_tokens'], minimum=0, maximum=10, step=1, label='prompt_lookup_num_tokens', info='Activates Prompt Lookup Decoding.')\n                            shared.gradio['max_tokens_second'] = gr.Slider(value=shared.settings['max_tokens_second'], minimum=0, maximum=20, step=1, label='Maximum tokens/second', info='To make text readable in real time.')\n                            shared.gradio['max_updates_second'] = gr.Slider(value=shared.settings['max_updates_second'], minimum=0, maximum=24, step=1, label='Maximum UI updates/second', info='Set this if you experience lag in the UI during streaming.')\n                            shared.gradio['seed'] = gr.Number(value=shared.settings['seed'], label='Seed (-1 for random)')\n                            shared.gradio['skip_special_tokens'] = gr.Checkbox(value=shared.settings['skip_special_tokens'], label='Skip special tokens', info='Some specific models need this unset.')\n                            shared.gradio['stream'] = gr.Checkbox(value=shared.settings['stream'], label='Activate text streaming')\n\n        ui_chat.create_chat_settings_ui()\n\n\ndef create_event_handlers():\n    shared.gradio['filter_by_loader'].change(loaders.blacklist_samplers, gradio('filter_by_loader', 'dynamic_temperature'), gradio(loaders.list_all_samplers()), show_progress=False)\n    shared.gradio['preset_menu'].change(presets.load_preset_for_ui, gradio('preset_menu', 'interface_state'), gradio('interface_state') + gradio(presets.presets_params()))\n    shared.gradio['random_preset'].click(presets.random_preset, gradio('interface_state'), gradio('interface_state') + gradio(presets.presets_params()))\n    shared.gradio['grammar_file'].change(load_grammar, gradio('grammar_file'), gradio('grammar_string'))\n    shared.gradio['dynamic_temperature'].change(lambda x: [gr.update(visible=x)] * 3, gradio('dynamic_temperature'), gradio('dynatemp_low', 'dynatemp_high', 'dynatemp_exponent'))\n\n\ndef get_truncation_length():\n    if 'max_seq_len' in shared.provided_arguments or shared.args.max_seq_len != shared.args_defaults.max_seq_len:\n        return shared.args.max_seq_len\n    elif 'n_ctx' in shared.provided_arguments or shared.args.n_ctx != shared.args_defaults.n_ctx:\n        return shared.args.n_ctx\n    else:\n        return shared.settings['truncation_length']\n\n\ndef load_grammar(name):\n    p = Path(f'grammars/{name}')\n    if p.exists():\n        return open(p, 'r', encoding='utf-8').read()\n    else:\n        return ''\n", "modules/utils.py": "import os\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom modules import github, shared\nfrom modules.logging_colors import logger\n\n\n# Helper function to get multiple values from shared.gradio\ndef gradio(*keys):\n    if len(keys) == 1 and type(keys[0]) in [list, tuple]:\n        keys = keys[0]\n\n    return [shared.gradio[k] for k in keys]\n\n\ndef save_file(fname, contents):\n    if fname == '':\n        logger.error('File name is empty!')\n        return\n\n    root_folder = Path(__file__).resolve().parent.parent\n    abs_path_str = os.path.abspath(fname)\n    rel_path_str = os.path.relpath(abs_path_str, root_folder)\n    rel_path = Path(rel_path_str)\n    if rel_path.parts[0] == '..':\n        logger.error(f'Invalid file path: \\\"{fname}\\\"')\n        return\n\n    with open(abs_path_str, 'w', encoding='utf-8') as f:\n        f.write(contents)\n\n    logger.info(f'Saved \\\"{abs_path_str}\\\".')\n\n\ndef delete_file(fname):\n    if fname == '':\n        logger.error('File name is empty!')\n        return\n\n    root_folder = Path(__file__).resolve().parent.parent\n    abs_path_str = os.path.abspath(fname)\n    rel_path_str = os.path.relpath(abs_path_str, root_folder)\n    rel_path = Path(rel_path_str)\n    if rel_path.parts[0] == '..':\n        logger.error(f'Invalid file path: \\\"{fname}\\\"')\n        return\n\n    if rel_path.exists():\n        rel_path.unlink()\n        logger.info(f'Deleted \\\"{fname}\\\".')\n\n\ndef current_time():\n    return f\"{datetime.now().strftime('%Y-%m-%d-%H%M%S')}\"\n\n\ndef atoi(text):\n    return int(text) if text.isdigit() else text.lower()\n\n\n# Replace multiple string pairs in a string\ndef replace_all(text, dic):\n    for i, j in dic.items():\n        text = text.replace(i, j)\n\n    return text\n\n\ndef natural_keys(text):\n    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n\n\ndef get_available_models():\n    model_list = []\n    for item in list(Path(f'{shared.args.model_dir}/').glob('*')):\n        if not item.name.endswith(('.txt', '-np', '.pt', '.json', '.yaml', '.py')) and 'llama-tokenizer' not in item.name:\n            model_list.append(item.name)\n\n    return ['None'] + sorted(model_list, key=natural_keys)\n\n\ndef get_available_ggufs():\n    model_list = []\n    for item in Path(f'{shared.args.model_dir}/').glob('*'):\n        if item.is_file() and item.name.lower().endswith(\".gguf\"):\n            model_list.append(item.name)\n\n    return ['None'] + sorted(model_list, key=natural_keys)\n\n\ndef get_available_presets():\n    return sorted(set((k.stem for k in Path('presets').glob('*.yaml'))), key=natural_keys)\n\n\ndef get_available_prompts():\n    prompts = []\n    files = set((k.stem for k in Path('prompts').glob('*.txt')))\n    prompts += sorted([k for k in files if re.match('^[0-9]', k)], key=natural_keys, reverse=True)\n    prompts += sorted([k for k in files if re.match('^[^0-9]', k)], key=natural_keys)\n    prompts += ['None']\n    return prompts\n\n\ndef get_available_characters():\n    paths = (x for x in Path('characters').iterdir() if x.suffix in ('.json', '.yaml', '.yml'))\n    return sorted(set((k.stem for k in paths)), key=natural_keys)\n\n\ndef get_available_instruction_templates():\n    path = \"instruction-templates\"\n    paths = []\n    if os.path.exists(path):\n        paths = (x for x in Path(path).iterdir() if x.suffix in ('.json', '.yaml', '.yml'))\n\n    return ['None'] + sorted(set((k.stem for k in paths)), key=natural_keys)\n\n\ndef get_available_extensions():\n    extensions = sorted(set(map(lambda x: x.parts[1], Path('extensions').glob('*/script.py'))), key=natural_keys)\n    extensions = [v for v in extensions if v not in github.new_extensions]\n    return extensions\n\n\ndef get_available_loras():\n    return ['None'] + sorted([item.name for item in list(Path(shared.args.lora_dir).glob('*')) if not item.name.endswith(('.txt', '-np', '.pt', '.json'))], key=natural_keys)\n\n\ndef get_datasets(path: str, ext: str):\n    # include subdirectories for raw txt files to allow training from a subdirectory of txt files\n    if ext == \"txt\":\n        return ['None'] + sorted(set([k.stem for k in list(Path(path).glob('*.txt')) + list(Path(path).glob('*/')) if k.stem != 'put-trainer-datasets-here']), key=natural_keys)\n\n    return ['None'] + sorted(set([k.stem for k in Path(path).glob(f'*.{ext}') if k.stem != 'put-trainer-datasets-here']), key=natural_keys)\n\n\ndef get_available_chat_styles():\n    return sorted(set(('-'.join(k.stem.split('-')[1:]) for k in Path('css').glob('chat_style*.css'))), key=natural_keys)\n\n\ndef get_available_grammars():\n    return ['None'] + sorted([item.name for item in list(Path('grammars').glob('*.gbnf'))], key=natural_keys)\n", "modules/ui_file_saving.py": "import gradio as gr\n\nfrom modules import chat, presets, shared, ui, utils\nfrom modules.utils import gradio\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n\n    # Text file saver\n    with gr.Group(visible=False, elem_classes='file-saver') as shared.gradio['file_saver']:\n        shared.gradio['save_filename'] = gr.Textbox(lines=1, label='File name')\n        shared.gradio['save_root'] = gr.Textbox(lines=1, label='File folder', info='For reference. Unchangeable.', interactive=False)\n        shared.gradio['save_contents'] = gr.Textbox(lines=10, label='File contents')\n        with gr.Row():\n            shared.gradio['save_cancel'] = gr.Button('Cancel', elem_classes=\"small-button\")\n            shared.gradio['save_confirm'] = gr.Button('Save', elem_classes=\"small-button\", variant='primary', interactive=not mu)\n\n    # Text file deleter\n    with gr.Group(visible=False, elem_classes='file-saver') as shared.gradio['file_deleter']:\n        shared.gradio['delete_filename'] = gr.Textbox(lines=1, label='File name')\n        shared.gradio['delete_root'] = gr.Textbox(lines=1, label='File folder', info='For reference. Unchangeable.', interactive=False)\n        with gr.Row():\n            shared.gradio['delete_cancel'] = gr.Button('Cancel', elem_classes=\"small-button\")\n            shared.gradio['delete_confirm'] = gr.Button('Delete', elem_classes=\"small-button\", variant='stop', interactive=not mu)\n\n    # Character saver/deleter\n    with gr.Group(visible=False, elem_classes='file-saver') as shared.gradio['character_saver']:\n        shared.gradio['save_character_filename'] = gr.Textbox(lines=1, label='File name', info='The character will be saved to your characters/ folder with this base filename.')\n        with gr.Row():\n            shared.gradio['save_character_cancel'] = gr.Button('Cancel', elem_classes=\"small-button\")\n            shared.gradio['save_character_confirm'] = gr.Button('Save', elem_classes=\"small-button\", variant='primary', interactive=not mu)\n\n    with gr.Group(visible=False, elem_classes='file-saver') as shared.gradio['character_deleter']:\n        gr.Markdown('Confirm the character deletion?')\n        with gr.Row():\n            shared.gradio['delete_character_cancel'] = gr.Button('Cancel', elem_classes=\"small-button\")\n            shared.gradio['delete_character_confirm'] = gr.Button('Delete', elem_classes=\"small-button\", variant='stop', interactive=not mu)\n\n    # Preset saver\n    with gr.Group(visible=False, elem_classes='file-saver') as shared.gradio['preset_saver']:\n        shared.gradio['save_preset_filename'] = gr.Textbox(lines=1, label='File name', info='The preset will be saved to your presets/ folder with this base filename.')\n        shared.gradio['save_preset_contents'] = gr.Textbox(lines=10, label='File contents')\n        with gr.Row():\n            shared.gradio['save_preset_cancel'] = gr.Button('Cancel', elem_classes=\"small-button\")\n            shared.gradio['save_preset_confirm'] = gr.Button('Save', elem_classes=\"small-button\", variant='primary', interactive=not mu)\n\n\ndef create_event_handlers():\n    shared.gradio['save_confirm'].click(\n        lambda x, y, z: utils.save_file(x + y, z), gradio('save_root', 'save_filename', 'save_contents'), None).then(\n        lambda: gr.update(visible=False), None, gradio('file_saver'))\n\n    shared.gradio['delete_confirm'].click(\n        lambda x, y: utils.delete_file(x + y), gradio('delete_root', 'delete_filename'), None).then(\n        lambda: gr.update(visible=False), None, gradio('file_deleter'))\n\n    shared.gradio['delete_cancel'].click(lambda: gr.update(visible=False), None, gradio('file_deleter'))\n    shared.gradio['save_cancel'].click(lambda: gr.update(visible=False), None, gradio('file_saver'))\n\n    shared.gradio['save_character_confirm'].click(\n        chat.save_character, gradio('name2', 'greeting', 'context', 'character_picture', 'save_character_filename'), None).then(\n        lambda: gr.update(visible=False), None, gradio('character_saver')).then(\n        lambda x: gr.update(choices=utils.get_available_characters(), value=x), gradio('save_character_filename'), gradio('character_menu'))\n\n    shared.gradio['delete_character_confirm'].click(\n        lambda x: str(utils.get_available_characters().index(x)), gradio('character_menu'), gradio('temporary_text')).then(\n        chat.delete_character, gradio('character_menu'), None).then(\n        chat.update_character_menu_after_deletion, gradio('temporary_text'), gradio('character_menu')).then(\n        lambda: gr.update(visible=False), None, gradio('character_deleter'))\n\n    shared.gradio['save_character_cancel'].click(lambda: gr.update(visible=False), None, gradio('character_saver'))\n    shared.gradio['delete_character_cancel'].click(lambda: gr.update(visible=False), None, gradio('character_deleter'))\n\n    shared.gradio['save_preset'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        presets.generate_preset_yaml, gradio('interface_state'), gradio('save_preset_contents')).then(\n        lambda: 'My Preset', None, gradio('save_preset_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('preset_saver'))\n\n    shared.gradio['save_preset_confirm'].click(\n        lambda x, y: utils.save_file(f'presets/{x}.yaml', y), gradio('save_preset_filename', 'save_preset_contents'), None).then(\n        lambda: gr.update(visible=False), None, gradio('preset_saver')).then(\n        lambda x: gr.update(choices=utils.get_available_presets(), value=x), gradio('save_preset_filename'), gradio('preset_menu'))\n\n    shared.gradio['save_preset_cancel'].click(lambda: gr.update(visible=False), None, gradio('preset_saver'))\n\n    shared.gradio['delete_preset'].click(\n        lambda x: f'{x}.yaml', gradio('preset_menu'), gradio('delete_filename')).then(\n        lambda: 'presets/', None, gradio('delete_root')).then(\n        lambda: gr.update(visible=True), None, gradio('file_deleter'))\n\n    shared.gradio['save_grammar'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda x: x, gradio('grammar_string'), gradio('save_contents')).then(\n        lambda: 'grammars/', None, gradio('save_root')).then(\n        lambda: 'My Fancy Grammar.gbnf', None, gradio('save_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('file_saver'))\n\n    shared.gradio['delete_grammar'].click(\n        lambda x: x, gradio('grammar_file'), gradio('delete_filename')).then(\n        lambda: 'grammars/', None, gradio('delete_root')).then(\n        lambda: gr.update(visible=True), None, gradio('file_deleter'))\n", "modules/presets.py": "import functools\nimport pprint\nimport random\nfrom pathlib import Path\n\nimport yaml\n\nfrom modules import shared\nfrom modules.loaders import loaders_samplers\nfrom modules.logging_colors import logger\n\n\ndef default_preset():\n    return {\n        'temperature': 1,\n        'temperature_last': False,\n        'dynamic_temperature': False,\n        'dynatemp_low': 1,\n        'dynatemp_high': 1,\n        'dynatemp_exponent': 1,\n        'smoothing_factor': 0,\n        'smoothing_curve': 1,\n        'top_p': 1,\n        'min_p': 0,\n        'top_k': 0,\n        'repetition_penalty': 1,\n        'presence_penalty': 0,\n        'frequency_penalty': 0,\n        'repetition_penalty_range': 1024,\n        'typical_p': 1,\n        'tfs': 1,\n        'top_a': 0,\n        'epsilon_cutoff': 0,\n        'eta_cutoff': 0,\n        'guidance_scale': 1,\n        'penalty_alpha': 0,\n        'mirostat_mode': 0,\n        'mirostat_tau': 5,\n        'mirostat_eta': 0.1,\n        'do_sample': True,\n        'encoder_repetition_penalty': 1,\n        'no_repeat_ngram_size': 0,\n        'dry_multiplier': 0,\n        'dry_base': 1.75,\n        'dry_allowed_length': 2,\n        'dry_sequence_breakers': '\"\\\\n\", \":\", \"\\\\\"\", \"*\"',\n        'sampler_priority': 'temperature\\ndynamic_temperature\\nquadratic_sampling\\ntop_k\\ntop_p\\ntypical_p\\nepsilon_cutoff\\neta_cutoff\\ntfs\\ntop_a\\nmin_p\\nmirostat'\n    }\n\n\ndef presets_params():\n    return [k for k in default_preset()]\n\n\ndef load_preset(name, verbose=False):\n    generate_params = default_preset()\n    if name not in ['None', None, '']:\n        path = Path(f'presets/{name}.yaml')\n        if path.exists():\n            with open(path, 'r') as infile:\n                preset = yaml.safe_load(infile)\n\n            for k in preset:\n                generate_params[k] = preset[k]\n        else:\n            logger.error(f\"The preset \\\"{name}\\\" does not exist under \\\"{path}\\\". Using the default parameters.\")\n\n    if verbose:\n        logger.info(f\"\\\"{name}\\\" preset:\")\n        pprint.PrettyPrinter(indent=4, width=1, sort_dicts=False).pprint(remove_defaults(generate_params))\n\n    return generate_params\n\n\n@functools.cache\ndef load_preset_memoized(name):\n    return load_preset(name)\n\n\ndef load_preset_for_ui(name, state):\n    generate_params = load_preset(name, verbose=True)\n    state.update(generate_params)\n    return state, *[generate_params[k] for k in presets_params()]\n\n\ndef random_preset(state):\n    params_and_values = {\n        'remove_tail_tokens': {\n            'top_p': [0.5, 0.8, 0.9, 0.95, 0.99],\n            'min_p': [0.5, 0.2, 0.1, 0.05, 0.01],\n            'top_k': [3, 5, 10, 20, 30, 40],\n            'typical_p': [0.2, 0.575, 0.95],\n            'tfs': [0.5, 0.8, 0.9, 0.95, 0.99],\n            'top_a': [0.5, 0.2, 0.1, 0.05, 0.01],\n            'epsilon_cutoff': [1, 3, 5, 7, 9],\n            'eta_cutoff': [3, 6, 9, 12, 15, 18],\n        },\n        'flatten_distribution': {\n            'temperature': [0.1, 0.5, 0.7, 0.8, 1, 1.2, 1.5, 2.0, 5.0],\n            'dynamic_temperature': [\n                [0.1, 1],\n                [0.1, 1.5],\n                [0.1, 2],\n                [0.1, 5],\n                [0.5, 1],\n                [0.5, 1.5],\n                [0.5, 2],\n                [0.5, 5],\n                [0.8, 1],\n                [0.8, 1.5],\n                [0.8, 2],\n                [0.8, 5],\n                [1, 1.5],\n                [1, 2],\n                [1, 5]\n            ],\n            'smoothing_factor': [0.2, 0.3, 0.6, 1.2],\n        },\n        'repetition': {\n            'repetition_penalty': [1, 1.05, 1.1, 1.15, 1.20, 1.25],\n            'presence_penalty': [0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0],\n            'frequency_penalty': [0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0],\n        },\n        'other': {\n            'temperature_last': [True, False],\n        }\n    }\n\n    generate_params = default_preset()\n    for cat in params_and_values:\n        choices = list(params_and_values[cat].keys())\n        if shared.args.loader is not None:\n            choices = [x for x in choices if loader_contains(x)]\n\n        if len(choices) > 0:\n            choice = random.choice(choices)\n            value = random.choice(params_and_values[cat][choice])\n            if choice == 'dynamic_temperature':\n                generate_params['dynamic_temperature'] = True\n                generate_params['dynatemp_low'] = value[0]\n                generate_params['dynatemp_high'] = value[1]\n            else:\n                generate_params[choice] = value\n\n    state.update(generate_params)\n    logger.info(\"GENERATED_PRESET=\")\n    pprint.PrettyPrinter(indent=4, width=1, sort_dicts=False).pprint(remove_defaults(state))\n    return state, *[generate_params[k] for k in presets_params()]\n\n\ndef loader_contains(sampler):\n    if sampler == 'dynamic_temperature' and 'dynatemp_low' in loaders_samplers[shared.args.loader]:\n        return True\n    else:\n        return sampler in loaders_samplers[shared.args.loader]\n\n\ndef remove_defaults(state):\n    defaults = default_preset()\n    data = {k: state[k] for k in presets_params()}\n\n    for k in list(data.keys()):\n        if data[k] == defaults[k]:\n            del data[k]\n\n    return data\n\n\ndef generate_preset_yaml(state):\n    data = remove_defaults(state)\n    return yaml.dump(data, sort_keys=False)\n", "modules/prompts.py": "from pathlib import Path\n\nfrom modules.text_generation import get_encoded_length\n\n\ndef load_prompt(fname):\n    if fname in ['None', '']:\n        return ''\n    else:\n        file_path = Path(f'prompts/{fname}.txt')\n        if not file_path.exists():\n            return ''\n\n        with open(file_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n            if text[-1] == '\\n':\n                text = text[:-1]\n\n            return text\n\n\ndef count_tokens(text):\n    try:\n        tokens = get_encoded_length(text)\n        return str(tokens)\n    except:\n        return '0'\n", "modules/extensions.py": "import importlib\nimport traceback\nfrom functools import partial\nfrom inspect import signature\n\nimport gradio as gr\n\nimport extensions\nimport modules.shared as shared\nfrom modules.logging_colors import logger\n\nstate = {}\navailable_extensions = []\nsetup_called = set()\n\n\ndef apply_settings(extension, name):\n    if not hasattr(extension, 'params'):\n        return\n\n    for param in extension.params:\n        _id = f\"{name}-{param}\"\n        shared.default_settings[_id] = extension.params[param]\n        if _id in shared.settings:\n            extension.params[param] = shared.settings[_id]\n\n\ndef load_extensions():\n    global state, setup_called\n    state = {}\n    for i, name in enumerate(shared.args.extensions):\n        if name in available_extensions:\n            if name != 'api':\n                logger.info(f'Loading the extension \"{name}\"')\n            try:\n                try:\n                    extension = importlib.import_module(f\"extensions.{name}.script\")\n                except ModuleNotFoundError:\n                    logger.error(f\"Could not import the requirements for '{name}'. Make sure to install the requirements for the extension.\\n\\n* To install requirements for all available extensions, launch the\\n  update_wizard script for your OS and choose the B option.\\n\\n* To install the requirements for this extension alone, launch the\\n  cmd script for your OS and paste the following command in the\\n  terminal window that appears:\\n\\nLinux / Mac:\\n\\npip install -r extensions/{name}/requirements.txt --upgrade\\n\\nWindows:\\n\\npip install -r extensions\\\\{name}\\\\requirements.txt --upgrade\\n\")\n                    raise\n\n                # Only run setup() and apply settings from settings.yaml once\n                if extension not in setup_called:\n                    apply_settings(extension, name)\n                    if hasattr(extension, \"setup\"):\n                        extension.setup()\n\n                    setup_called.add(extension)\n\n                state[name] = [True, i]\n            except:\n                logger.error(f'Failed to load the extension \"{name}\".')\n                traceback.print_exc()\n\n\n# This iterator returns the extensions in the order specified in the command-line\ndef iterator():\n    for name in sorted(state, key=lambda x: state[x][1]):\n        if state[name][0]:\n            yield getattr(extensions, name).script, name\n\n\n# Extension functions that map string -> string\ndef _apply_string_extensions(function_name, text, state, is_chat=False):\n    for extension, _ in iterator():\n        if hasattr(extension, function_name):\n            func = getattr(extension, function_name)\n\n            # Handle old extensions without the 'state' arg or\n            # the 'is_chat' kwarg\n            count = 0\n            has_chat = False\n            for k in signature(func).parameters:\n                if k == 'is_chat':\n                    has_chat = True\n                else:\n                    count += 1\n\n            if count == 2:\n                args = [text, state]\n            else:\n                args = [text]\n\n            if has_chat:\n                kwargs = {'is_chat': is_chat}\n            else:\n                kwargs = {}\n\n            text = func(*args, **kwargs)\n\n    return text\n\n\n# Extension functions that map string -> string\ndef _apply_chat_input_extensions(text, visible_text, state):\n    for extension, _ in iterator():\n        if hasattr(extension, 'chat_input_modifier'):\n            text, visible_text = extension.chat_input_modifier(text, visible_text, state)\n\n    return text, visible_text\n\n\n# custom_generate_chat_prompt handling - currently only the first one will work\ndef _apply_custom_generate_chat_prompt(text, state, **kwargs):\n    for extension, _ in iterator():\n        if hasattr(extension, 'custom_generate_chat_prompt'):\n            return extension.custom_generate_chat_prompt(text, state, **kwargs)\n\n    return None\n\n\n# Extension that modifies the input parameters before they are used\ndef _apply_state_modifier_extensions(state):\n    for extension, _ in iterator():\n        if hasattr(extension, \"state_modifier\"):\n            state = getattr(extension, \"state_modifier\")(state)\n\n    return state\n\n\n# Extension that modifies the chat history before it is used\ndef _apply_history_modifier_extensions(history):\n    for extension, _ in iterator():\n        if hasattr(extension, \"history_modifier\"):\n            history = getattr(extension, \"history_modifier\")(history)\n\n    return history\n\n\n# Extension functions that override the default tokenizer output - The order of execution is not defined\ndef _apply_tokenizer_extensions(function_name, state, prompt, input_ids, input_embeds):\n    for extension, _ in iterator():\n        if hasattr(extension, function_name):\n            prompt, input_ids, input_embeds = getattr(extension, function_name)(state, prompt, input_ids, input_embeds)\n\n    return prompt, input_ids, input_embeds\n\n\n# Allow extensions to add their own logits processors to the stack being run.\n# Each extension would call `processor_list.append({their LogitsProcessor}())`.\ndef _apply_logits_processor_extensions(function_name, processor_list, input_ids):\n    for extension, _ in iterator():\n        if hasattr(extension, function_name):\n            result = getattr(extension, function_name)(processor_list, input_ids)\n            if type(result) is list:\n                processor_list = result\n\n    return processor_list\n\n\n# Get prompt length in tokens after applying extension functions which override the default tokenizer output\n# currently only the first one will work\ndef _apply_custom_tokenized_length(prompt):\n    for extension, _ in iterator():\n        if hasattr(extension, 'custom_tokenized_length'):\n            return getattr(extension, 'custom_tokenized_length')(prompt)\n\n    return None\n\n\n# Custom generate reply handling - currently only the first one will work\ndef _apply_custom_generate_reply():\n    for extension, _ in iterator():\n        if hasattr(extension, 'custom_generate_reply'):\n            return getattr(extension, 'custom_generate_reply')\n\n    return None\n\n\ndef _apply_custom_css():\n    all_css = ''\n    for extension, _ in iterator():\n        if hasattr(extension, 'custom_css'):\n            all_css += getattr(extension, 'custom_css')()\n\n    return all_css\n\n\ndef _apply_custom_js():\n    all_js = ''\n    for extension, _ in iterator():\n        if hasattr(extension, 'custom_js'):\n            all_js += getattr(extension, 'custom_js')()\n\n    return all_js\n\n\ndef create_extensions_block():\n    to_display = []\n    for extension, name in iterator():\n        if hasattr(extension, \"ui\") and not (hasattr(extension, 'params') and extension.params.get('is_tab', False)):\n            to_display.append((extension, name))\n\n    # Creating the extension ui elements\n    if len(to_display) > 0:\n        with gr.Column(elem_id=\"extensions\"):\n            for row in to_display:\n                extension, _ = row\n                extension.ui()\n\n\ndef create_extensions_tabs():\n    for extension, name in iterator():\n        if hasattr(extension, \"ui\") and (hasattr(extension, 'params') and extension.params.get('is_tab', False)):\n            display_name = getattr(extension, 'params', {}).get('display_name', name)\n            with gr.Tab(display_name, elem_classes=\"extension-tab\"):\n                extension.ui()\n\n\nEXTENSION_MAP = {\n    \"input\": partial(_apply_string_extensions, \"input_modifier\"),\n    \"output\": partial(_apply_string_extensions, \"output_modifier\"),\n    \"chat_input\": _apply_chat_input_extensions,\n    \"state\": _apply_state_modifier_extensions,\n    \"history\": _apply_history_modifier_extensions,\n    \"bot_prefix\": partial(_apply_string_extensions, \"bot_prefix_modifier\"),\n    \"tokenizer\": partial(_apply_tokenizer_extensions, \"tokenizer_modifier\"),\n    'logits_processor': partial(_apply_logits_processor_extensions, 'logits_processor_modifier'),\n    \"custom_generate_chat_prompt\": _apply_custom_generate_chat_prompt,\n    \"custom_generate_reply\": _apply_custom_generate_reply,\n    \"tokenized_length\": _apply_custom_tokenized_length,\n    \"css\": _apply_custom_css,\n    \"js\": _apply_custom_js\n}\n\n\ndef apply_extensions(typ, *args, **kwargs):\n    if typ not in EXTENSION_MAP:\n        raise ValueError(f\"Invalid extension type {typ}\")\n\n    return EXTENSION_MAP[typ](*args, **kwargs)\n", "modules/callbacks.py": "import gc\nimport traceback\nfrom queue import Queue\nfrom threading import Thread\n\nimport torch\nimport transformers\nfrom transformers import is_torch_npu_available, is_torch_xpu_available\n\nimport modules.shared as shared\n\n\nclass StopNowException(Exception):\n    pass\n\n\nclass _StopEverythingStoppingCriteria(transformers.StoppingCriteria):\n    def __init__(self):\n        transformers.StoppingCriteria.__init__(self)\n\n    def __call__(self, input_ids: torch.LongTensor, _scores: torch.FloatTensor) -> bool:\n        return shared.stop_everything\n\n\nclass Stream(transformers.StoppingCriteria):\n    def __init__(self, callback_func=None):\n        self.callback_func = callback_func\n\n    def __call__(self, input_ids, scores) -> bool:\n        if self.callback_func is not None:\n            self.callback_func(input_ids[0])\n\n        return False\n\n\nclass Iteratorize:\n\n    \"\"\"\n    Transforms a function that takes a callback\n    into a lazy iterator (generator).\n\n    Adapted from: https://stackoverflow.com/a/9969000\n    \"\"\"\n\n    def __init__(self, func, args=None, kwargs=None, callback=None):\n        self.mfunc = func\n        self.c_callback = callback\n        self.q = Queue()\n        self.sentinel = object()\n        self.args = args or []\n        self.kwargs = kwargs or {}\n        self.stop_now = False\n\n        def _callback(val):\n            if self.stop_now or shared.stop_everything:\n                raise StopNowException\n            self.q.put(val)\n\n        def gentask():\n            try:\n                ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n            except StopNowException:\n                pass\n            except:\n                traceback.print_exc()\n                pass\n\n            clear_torch_cache()\n            self.q.put(self.sentinel)\n            if self.c_callback:\n                self.c_callback(ret)\n\n        self.thread = Thread(target=gentask)\n        self.thread.start()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        obj = self.q.get(True, None)\n        if obj is self.sentinel:\n            raise StopIteration\n        else:\n            return obj\n\n    def __del__(self):\n        clear_torch_cache()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.stop_now = True\n        clear_torch_cache()\n\n\ndef clear_torch_cache():\n    gc.collect()\n    if not shared.args.cpu:\n        if is_torch_xpu_available():\n            torch.xpu.empty_cache()\n        elif is_torch_npu_available():\n            torch.npu.empty_cache()\n        else:\n            torch.cuda.empty_cache()\n", "modules/ui_model_menu.py": "import importlib\nimport math\nimport re\nimport traceback\nfrom functools import partial\nfrom pathlib import Path\n\nimport gradio as gr\nimport psutil\nimport torch\nfrom transformers import is_torch_npu_available, is_torch_xpu_available\n\nfrom modules import loaders, shared, ui, utils\nfrom modules.logging_colors import logger\nfrom modules.LoRA import add_lora_to_model\nfrom modules.models import load_model, unload_model\nfrom modules.models_settings import (\n    apply_model_settings_to_state,\n    get_model_metadata,\n    save_instruction_template,\n    save_model_settings,\n    update_model_parameters\n)\nfrom modules.utils import gradio\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n\n    # Finding the default values for the GPU and CPU memories\n    total_mem = []\n    if is_torch_xpu_available():\n        for i in range(torch.xpu.device_count()):\n            total_mem.append(math.floor(torch.xpu.get_device_properties(i).total_memory / (1024 * 1024)))\n    elif is_torch_npu_available():\n        for i in range(torch.npu.device_count()):\n            total_mem.append(math.floor(torch.npu.get_device_properties(i).total_memory / (1024 * 1024)))\n    else:\n        for i in range(torch.cuda.device_count()):\n            total_mem.append(math.floor(torch.cuda.get_device_properties(i).total_memory / (1024 * 1024)))\n\n    default_gpu_mem = []\n    if shared.args.gpu_memory is not None and len(shared.args.gpu_memory) > 0:\n        for i in shared.args.gpu_memory:\n            if 'mib' in i.lower():\n                default_gpu_mem.append(int(re.sub('[a-zA-Z ]', '', i)))\n            else:\n                default_gpu_mem.append(int(re.sub('[a-zA-Z ]', '', i)) * 1000)\n\n    while len(default_gpu_mem) < len(total_mem):\n        default_gpu_mem.append(0)\n\n    total_cpu_mem = math.floor(psutil.virtual_memory().total / (1024 * 1024))\n    if shared.args.cpu_memory is not None:\n        default_cpu_mem = re.sub('[a-zA-Z ]', '', shared.args.cpu_memory)\n    else:\n        default_cpu_mem = 0\n\n    with gr.Tab(\"Model\", elem_id=\"model-tab\"):\n        with gr.Row():\n            with gr.Column():\n                with gr.Row():\n                    with gr.Column():\n                        with gr.Row():\n                            shared.gradio['model_menu'] = gr.Dropdown(choices=utils.get_available_models(), value=lambda: shared.model_name, label='Model', elem_classes='slim-dropdown', interactive=not mu)\n                            ui.create_refresh_button(shared.gradio['model_menu'], lambda: None, lambda: {'choices': utils.get_available_models()}, 'refresh-button', interactive=not mu)\n                            shared.gradio['load_model'] = gr.Button(\"Load\", visible=not shared.settings['autoload_model'], elem_classes='refresh-button', interactive=not mu)\n                            shared.gradio['unload_model'] = gr.Button(\"Unload\", elem_classes='refresh-button', interactive=not mu)\n                            shared.gradio['reload_model'] = gr.Button(\"Reload\", elem_classes='refresh-button', interactive=not mu)\n                            shared.gradio['save_model_settings'] = gr.Button(\"Save settings\", elem_classes='refresh-button', interactive=not mu)\n\n                    with gr.Column():\n                        with gr.Row():\n                            shared.gradio['lora_menu'] = gr.Dropdown(multiselect=True, choices=utils.get_available_loras(), value=shared.lora_names, label='LoRA(s)', elem_classes='slim-dropdown', interactive=not mu)\n                            ui.create_refresh_button(shared.gradio['lora_menu'], lambda: None, lambda: {'choices': utils.get_available_loras(), 'value': shared.lora_names}, 'refresh-button', interactive=not mu)\n                            shared.gradio['lora_menu_apply'] = gr.Button(value='Apply LoRAs', elem_classes='refresh-button', interactive=not mu)\n\n        with gr.Row():\n            with gr.Column():\n                shared.gradio['loader'] = gr.Dropdown(label=\"Model loader\", choices=loaders.loaders_and_params.keys(), value=None)\n                with gr.Blocks():\n                    with gr.Row():\n                        with gr.Column():\n                            with gr.Blocks():\n                                for i in range(len(total_mem)):\n                                    shared.gradio[f'gpu_memory_{i}'] = gr.Slider(label=f\"gpu-memory in MiB for device :{i}\", maximum=total_mem[i], value=default_gpu_mem[i])\n\n                                shared.gradio['cpu_memory'] = gr.Slider(label=\"cpu-memory in MiB\", maximum=total_cpu_mem, value=default_cpu_mem)\n\n                            with gr.Blocks():\n                                shared.gradio['transformers_info'] = gr.Markdown('load-in-4bit params:')\n                                shared.gradio['compute_dtype'] = gr.Dropdown(label=\"compute_dtype\", choices=[\"bfloat16\", \"float16\", \"float32\"], value=shared.args.compute_dtype)\n                                shared.gradio['quant_type'] = gr.Dropdown(label=\"quant_type\", choices=[\"nf4\", \"fp4\"], value=shared.args.quant_type)\n\n                            shared.gradio['hqq_backend'] = gr.Dropdown(label=\"hqq_backend\", choices=[\"PYTORCH\", \"PYTORCH_COMPILE\", \"ATEN\"], value=shared.args.hqq_backend)\n                            shared.gradio['n_gpu_layers'] = gr.Slider(label=\"n-gpu-layers\", minimum=0, maximum=256, value=shared.args.n_gpu_layers, info='Must be set to more than 0 for your GPU to be used.')\n                            shared.gradio['n_ctx'] = gr.Slider(minimum=0, maximum=shared.settings['truncation_length_max'], step=256, label=\"n_ctx\", value=shared.args.n_ctx, info='Context length. Try lowering this if you run out of memory while loading the model.')\n                            shared.gradio['tensor_split'] = gr.Textbox(label='tensor_split', info='List of proportions to split the model across multiple GPUs. Example: 18,17')\n                            shared.gradio['n_batch'] = gr.Slider(label=\"n_batch\", minimum=1, maximum=2048, step=1, value=shared.args.n_batch)\n                            shared.gradio['threads'] = gr.Slider(label=\"threads\", minimum=0, step=1, maximum=256, value=shared.args.threads)\n                            shared.gradio['threads_batch'] = gr.Slider(label=\"threads_batch\", minimum=0, step=1, maximum=256, value=shared.args.threads_batch)\n                            shared.gradio['wbits'] = gr.Dropdown(label=\"wbits\", choices=[\"None\", 1, 2, 3, 4, 8], value=shared.args.wbits if shared.args.wbits > 0 else \"None\")\n                            shared.gradio['groupsize'] = gr.Dropdown(label=\"groupsize\", choices=[\"None\", 32, 64, 128, 1024], value=shared.args.groupsize if shared.args.groupsize > 0 else \"None\")\n                            shared.gradio['gpu_split'] = gr.Textbox(label='gpu-split', info='Comma-separated list of VRAM (in GB) to use per GPU. Example: 20,7,7')\n                            shared.gradio['max_seq_len'] = gr.Slider(label='max_seq_len', minimum=0, maximum=shared.settings['truncation_length_max'], step=256, info='Context length. Try lowering this if you run out of memory while loading the model.', value=shared.args.max_seq_len)\n                            with gr.Blocks():\n                                shared.gradio['alpha_value'] = gr.Slider(label='alpha_value', minimum=1, maximum=8, step=0.05, info='Positional embeddings alpha factor for NTK RoPE scaling. Recommended values (NTKv1): 1.75 for 1.5x context, 2.5 for 2x context. Use either this or compress_pos_emb, not both.', value=shared.args.alpha_value)\n                                shared.gradio['rope_freq_base'] = gr.Slider(label='rope_freq_base', minimum=0, maximum=1000000, step=1000, info='If greater than 0, will be used instead of alpha_value. Those two are related by rope_freq_base = 10000 * alpha_value ^ (64 / 63)', value=shared.args.rope_freq_base)\n                                shared.gradio['compress_pos_emb'] = gr.Slider(label='compress_pos_emb', minimum=1, maximum=8, step=1, info='Positional embeddings compression factor. Should be set to (context length) / (model\\'s original context length). Equal to 1/rope_freq_scale.', value=shared.args.compress_pos_emb)\n\n                            shared.gradio['autogptq_info'] = gr.Markdown('ExLlamav2_HF is recommended over AutoGPTQ for models derived from Llama.')\n\n                        with gr.Column():\n                            shared.gradio['load_in_8bit'] = gr.Checkbox(label=\"load-in-8bit\", value=shared.args.load_in_8bit)\n                            shared.gradio['load_in_4bit'] = gr.Checkbox(label=\"load-in-4bit\", value=shared.args.load_in_4bit)\n                            shared.gradio['use_double_quant'] = gr.Checkbox(label=\"use_double_quant\", value=shared.args.use_double_quant)\n                            shared.gradio['use_flash_attention_2'] = gr.Checkbox(label=\"use_flash_attention_2\", value=shared.args.use_flash_attention_2, info='Set use_flash_attention_2=True while loading the model.')\n                            shared.gradio['flash_attn'] = gr.Checkbox(label=\"flash_attn\", value=shared.args.flash_attn, info='Use flash-attention.')\n                            shared.gradio['auto_devices'] = gr.Checkbox(label=\"auto-devices\", value=shared.args.auto_devices)\n                            shared.gradio['tensorcores'] = gr.Checkbox(label=\"tensorcores\", value=shared.args.tensorcores, info='NVIDIA only: use llama-cpp-python compiled with tensor cores support. This increases performance on RTX cards.')\n                            shared.gradio['streaming_llm'] = gr.Checkbox(label=\"streaming_llm\", value=shared.args.streaming_llm, info='(experimental) Activate StreamingLLM to avoid re-evaluating the entire prompt when old messages are removed.')\n                            shared.gradio['attention_sink_size'] = gr.Number(label=\"attention_sink_size\", value=shared.args.attention_sink_size, precision=0, info='StreamingLLM: number of sink tokens. Only used if the trimmed prompt doesn\\'t share a prefix with the old prompt.')\n                            shared.gradio['cpu'] = gr.Checkbox(label=\"cpu\", value=shared.args.cpu, info='llama.cpp: Use llama-cpp-python compiled without GPU acceleration. Transformers: use PyTorch in CPU mode.')\n                            shared.gradio['row_split'] = gr.Checkbox(label=\"row_split\", value=shared.args.row_split, info='Split the model by rows across GPUs. This may improve multi-gpu performance.')\n                            shared.gradio['no_offload_kqv'] = gr.Checkbox(label=\"no_offload_kqv\", value=shared.args.no_offload_kqv, info='Do not offload the  K, Q, V to the GPU. This saves VRAM but reduces the performance.')\n                            shared.gradio['no_mul_mat_q'] = gr.Checkbox(label=\"no_mul_mat_q\", value=shared.args.no_mul_mat_q, info='Disable the mulmat kernels.')\n                            shared.gradio['triton'] = gr.Checkbox(label=\"triton\", value=shared.args.triton)\n                            shared.gradio['no_inject_fused_attention'] = gr.Checkbox(label=\"no_inject_fused_attention\", value=shared.args.no_inject_fused_attention, info='Disable fused attention. Fused attention improves inference performance but uses more VRAM. Fuses layers for AutoAWQ. Disable if running low on VRAM.')\n                            shared.gradio['no_inject_fused_mlp'] = gr.Checkbox(label=\"no_inject_fused_mlp\", value=shared.args.no_inject_fused_mlp, info='Affects Triton only. Disable fused MLP. Fused MLP improves performance but uses more VRAM. Disable if running low on VRAM.')\n                            shared.gradio['no_use_cuda_fp16'] = gr.Checkbox(label=\"no_use_cuda_fp16\", value=shared.args.no_use_cuda_fp16, info='This can make models faster on some systems.')\n                            shared.gradio['desc_act'] = gr.Checkbox(label=\"desc_act\", value=shared.args.desc_act, info='\\'desc_act\\', \\'wbits\\', and \\'groupsize\\' are used for old models without a quantize_config.json.')\n                            shared.gradio['no_mmap'] = gr.Checkbox(label=\"no-mmap\", value=shared.args.no_mmap)\n                            shared.gradio['mlock'] = gr.Checkbox(label=\"mlock\", value=shared.args.mlock)\n                            shared.gradio['numa'] = gr.Checkbox(label=\"numa\", value=shared.args.numa, info='NUMA support can help on some systems with non-uniform memory access.')\n                            shared.gradio['disk'] = gr.Checkbox(label=\"disk\", value=shared.args.disk)\n                            shared.gradio['bf16'] = gr.Checkbox(label=\"bf16\", value=shared.args.bf16)\n                            shared.gradio['cache_8bit'] = gr.Checkbox(label=\"cache_8bit\", value=shared.args.cache_8bit, info='Use 8-bit cache to save VRAM.')\n                            shared.gradio['cache_4bit'] = gr.Checkbox(label=\"cache_4bit\", value=shared.args.cache_4bit, info='Use Q4 cache to save VRAM.')\n                            shared.gradio['autosplit'] = gr.Checkbox(label=\"autosplit\", value=shared.args.autosplit, info='Automatically split the model tensors across the available GPUs.')\n                            shared.gradio['no_flash_attn'] = gr.Checkbox(label=\"no_flash_attn\", value=shared.args.no_flash_attn, info='Force flash-attention to not be used.')\n                            shared.gradio['cfg_cache'] = gr.Checkbox(label=\"cfg-cache\", value=shared.args.cfg_cache, info='Necessary to use CFG with this loader.')\n                            shared.gradio['num_experts_per_token'] = gr.Number(label=\"Number of experts per token\", value=shared.args.num_experts_per_token, info='Only applies to MoE models like Mixtral.')\n                            with gr.Blocks():\n                                shared.gradio['trust_remote_code'] = gr.Checkbox(label=\"trust-remote-code\", value=shared.args.trust_remote_code, info='Set trust_remote_code=True while loading the tokenizer/model. To enable this option, start the web UI with the --trust-remote-code flag.', interactive=shared.args.trust_remote_code)\n                                shared.gradio['no_use_fast'] = gr.Checkbox(label=\"no_use_fast\", value=shared.args.no_use_fast, info='Set use_fast=False while loading the tokenizer.')\n                                shared.gradio['logits_all'] = gr.Checkbox(label=\"logits_all\", value=shared.args.logits_all, info='Needs to be set for perplexity evaluation to work with this loader. Otherwise, ignore it, as it makes prompt processing slower.')\n\n                            shared.gradio['disable_exllama'] = gr.Checkbox(label=\"disable_exllama\", value=shared.args.disable_exllama, info='Disable ExLlama kernel for GPTQ models.')\n                            shared.gradio['disable_exllamav2'] = gr.Checkbox(label=\"disable_exllamav2\", value=shared.args.disable_exllamav2, info='Disable ExLlamav2 kernel for GPTQ models.')\n                            shared.gradio['gptq_for_llama_info'] = gr.Markdown('Legacy loader for compatibility with older GPUs. ExLlamav2_HF or AutoGPTQ are preferred for GPTQ models when supported.')\n                            shared.gradio['exllamav2_info'] = gr.Markdown(\"ExLlamav2_HF is recommended over ExLlamav2 for better integration with extensions and more consistent sampling behavior across loaders.\")\n                            shared.gradio['llamacpp_HF_info'] = gr.Markdown(\"llamacpp_HF loads llama.cpp as a Transformers model. To use it, you need to place your GGUF in a subfolder of models/ with the necessary tokenizer files.\\n\\nYou can use the \\\"llamacpp_HF creator\\\" menu to do that automatically.\")\n\n            with gr.Column():\n                with gr.Row():\n                    shared.gradio['autoload_model'] = gr.Checkbox(value=shared.settings['autoload_model'], label='Autoload the model', info='Whether to load the model as soon as it is selected in the Model dropdown.', interactive=not mu)\n\n                with gr.Tab(\"Download\"):\n                    shared.gradio['custom_model_menu'] = gr.Textbox(label=\"Download model or LoRA\", info=\"Enter the Hugging Face username/model path, for instance: facebook/galactica-125m. To specify a branch, add it at the end after a \\\":\\\" character like this: facebook/galactica-125m:main. To download a single file, enter its name in the second box.\", interactive=not mu)\n                    shared.gradio['download_specific_file'] = gr.Textbox(placeholder=\"File name (for GGUF models)\", show_label=False, max_lines=1, interactive=not mu)\n                    with gr.Row():\n                        shared.gradio['download_model_button'] = gr.Button(\"Download\", variant='primary', interactive=not mu)\n                        shared.gradio['get_file_list'] = gr.Button(\"Get file list\", interactive=not mu)\n\n                with gr.Tab(\"llamacpp_HF creator\"):\n                    with gr.Row():\n                        shared.gradio['gguf_menu'] = gr.Dropdown(choices=utils.get_available_ggufs(), value=lambda: shared.model_name, label='Choose your GGUF', elem_classes='slim-dropdown', interactive=not mu)\n                        ui.create_refresh_button(shared.gradio['gguf_menu'], lambda: None, lambda: {'choices': utils.get_available_ggufs()}, 'refresh-button', interactive=not mu)\n\n                    shared.gradio['unquantized_url'] = gr.Textbox(label=\"Enter the URL for the original (unquantized) model\", info=\"Example: https://huggingface.co/lmsys/vicuna-13b-v1.5\", max_lines=1)\n                    shared.gradio['create_llamacpp_hf_button'] = gr.Button(\"Submit\", variant=\"primary\", interactive=not mu)\n                    gr.Markdown(\"This will move your gguf file into a subfolder of `models` along with the necessary tokenizer files.\")\n\n                with gr.Tab(\"Customize instruction template\"):\n                    with gr.Row():\n                        shared.gradio['customized_template'] = gr.Dropdown(choices=utils.get_available_instruction_templates(), value='None', label='Select the desired instruction template', elem_classes='slim-dropdown')\n                        ui.create_refresh_button(shared.gradio['customized_template'], lambda: None, lambda: {'choices': utils.get_available_instruction_templates()}, 'refresh-button', interactive=not mu)\n\n                    shared.gradio['customized_template_submit'] = gr.Button(\"Submit\", variant=\"primary\", interactive=not mu)\n                    gr.Markdown(\"This allows you to set a customized template for the model currently selected in the \\\"Model loader\\\" menu. Whenever the model gets loaded, this template will be used in place of the template specified in the model's medatada, which sometimes is wrong.\")\n\n                with gr.Row():\n                    shared.gradio['model_status'] = gr.Markdown('No model is loaded' if shared.model_name == 'None' else 'Ready')\n\n\ndef create_event_handlers():\n    shared.gradio['loader'].change(loaders.make_loader_params_visible, gradio('loader'), gradio(loaders.get_all_params()))\n\n    # In this event handler, the interface state is read and updated\n    # with the model defaults (if any), and then the model is loaded\n    # unless \"autoload_model\" is unchecked\n    shared.gradio['model_menu'].change(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        apply_model_settings_to_state, gradio('model_menu', 'interface_state'), gradio('interface_state')).then(\n        ui.apply_interface_values, gradio('interface_state'), gradio(ui.list_interface_input_elements()), show_progress=False).then(\n        update_model_parameters, gradio('interface_state'), None).then(\n        load_model_wrapper, gradio('model_menu', 'loader', 'autoload_model'), gradio('model_status'), show_progress=False).success(\n        update_truncation_length, gradio('truncation_length', 'interface_state'), gradio('truncation_length')).then(\n        lambda x: x, gradio('loader'), gradio('filter_by_loader'))\n\n    shared.gradio['load_model'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        update_model_parameters, gradio('interface_state'), None).then(\n        partial(load_model_wrapper, autoload=True), gradio('model_menu', 'loader'), gradio('model_status'), show_progress=False).success(\n        update_truncation_length, gradio('truncation_length', 'interface_state'), gradio('truncation_length')).then(\n        lambda x: x, gradio('loader'), gradio('filter_by_loader'))\n\n    shared.gradio['reload_model'].click(\n        unload_model, None, None).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        update_model_parameters, gradio('interface_state'), None).then(\n        partial(load_model_wrapper, autoload=True), gradio('model_menu', 'loader'), gradio('model_status'), show_progress=False).success(\n        update_truncation_length, gradio('truncation_length', 'interface_state'), gradio('truncation_length')).then(\n        lambda x: x, gradio('loader'), gradio('filter_by_loader'))\n\n    shared.gradio['unload_model'].click(\n        unload_model, None, None).then(\n        lambda: \"Model unloaded\", None, gradio('model_status'))\n\n    shared.gradio['save_model_settings'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        save_model_settings, gradio('model_menu', 'interface_state'), gradio('model_status'), show_progress=False)\n\n    shared.gradio['lora_menu_apply'].click(load_lora_wrapper, gradio('lora_menu'), gradio('model_status'), show_progress=False)\n    shared.gradio['download_model_button'].click(download_model_wrapper, gradio('custom_model_menu', 'download_specific_file'), gradio('model_status'), show_progress=True)\n    shared.gradio['get_file_list'].click(partial(download_model_wrapper, return_links=True), gradio('custom_model_menu', 'download_specific_file'), gradio('model_status'), show_progress=True)\n    shared.gradio['autoload_model'].change(lambda x: gr.update(visible=not x), gradio('autoload_model'), gradio('load_model'))\n    shared.gradio['create_llamacpp_hf_button'].click(create_llamacpp_hf, gradio('gguf_menu', 'unquantized_url'), gradio('model_status'), show_progress=True)\n    shared.gradio['customized_template_submit'].click(save_instruction_template, gradio('model_menu', 'customized_template'), gradio('model_status'), show_progress=True)\n\n\ndef load_model_wrapper(selected_model, loader, autoload=False):\n    if not autoload:\n        yield f\"The settings for `{selected_model}` have been updated.\\n\\nClick on \\\"Load\\\" to load it.\"\n        return\n\n    if selected_model == 'None':\n        yield \"No model selected\"\n    else:\n        try:\n            yield f\"Loading `{selected_model}`...\"\n            unload_model()\n            if selected_model != '':\n                shared.model, shared.tokenizer = load_model(selected_model, loader)\n\n            if shared.model is not None:\n                output = f\"Successfully loaded `{selected_model}`.\"\n\n                settings = get_model_metadata(selected_model)\n                if 'instruction_template' in settings:\n                    output += '\\n\\nIt seems to be an instruction-following model with template \"{}\". In the chat tab, instruct or chat-instruct modes should be used.'.format(settings['instruction_template'])\n\n                yield output\n            else:\n                yield f\"Failed to load `{selected_model}`.\"\n        except:\n            exc = traceback.format_exc()\n            logger.error('Failed to load the model.')\n            print(exc)\n            yield exc.replace('\\n', '\\n\\n')\n\n\ndef load_lora_wrapper(selected_loras):\n    yield (\"Applying the following LoRAs to {}:\\n\\n{}\".format(shared.model_name, '\\n'.join(selected_loras)))\n    add_lora_to_model(selected_loras)\n    yield (\"Successfuly applied the LoRAs\")\n\n\ndef download_model_wrapper(repo_id, specific_file, progress=gr.Progress(), return_links=False, check=False):\n    try:\n        if repo_id == \"\":\n            yield (\"Please enter a model path\")\n            return\n\n        downloader = importlib.import_module(\"download-model\").ModelDownloader()\n\n        progress(0.0)\n        model, branch = downloader.sanitize_model_and_branch_names(repo_id, None)\n\n        yield (\"Getting the download links from Hugging Face\")\n        links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(model, branch, text_only=False, specific_file=specific_file)\n        if return_links:\n            output = \"```\\n\"\n            for link in links:\n                output += f\"{Path(link).name}\" + \"\\n\"\n\n            output += \"```\"\n            yield output\n            return\n\n        yield (\"Getting the output folder\")\n        output_folder = downloader.get_output_folder(\n            model,\n            branch,\n            is_lora,\n            is_llamacpp=is_llamacpp,\n            model_dir=shared.args.model_dir if shared.args.model_dir != shared.args_defaults.model_dir else None\n        )\n\n        if output_folder == Path(\"models\"):\n            output_folder = Path(shared.args.model_dir)\n        elif output_folder == Path(\"loras\"):\n            output_folder = Path(shared.args.lora_dir)\n\n        if check:\n            progress(0.5)\n\n            yield (\"Checking previously downloaded files\")\n            downloader.check_model_files(model, branch, links, sha256, output_folder)\n            progress(1.0)\n        else:\n            yield (f\"Downloading file{'s' if len(links) > 1 else ''} to `{output_folder}`\")\n            downloader.download_model_files(model, branch, links, sha256, output_folder, progress_bar=progress, threads=4, is_llamacpp=is_llamacpp)\n\n            yield (f\"Model successfully saved to `{output_folder}/`.\")\n    except:\n        progress(1.0)\n        yield traceback.format_exc().replace('\\n', '\\n\\n')\n\n\ndef create_llamacpp_hf(gguf_name, unquantized_url, progress=gr.Progress()):\n    try:\n        downloader = importlib.import_module(\"download-model\").ModelDownloader()\n\n        progress(0.0)\n        model, branch = downloader.sanitize_model_and_branch_names(unquantized_url, None)\n\n        yield (\"Getting the tokenizer files links from Hugging Face\")\n        links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(model, branch, text_only=True)\n        output_folder = Path(shared.args.model_dir) / (re.sub(r'(?i)\\.gguf$', '', gguf_name) + \"-HF\")\n\n        yield (f\"Downloading tokenizer to `{output_folder}`\")\n        downloader.download_model_files(model, branch, links, sha256, output_folder, progress_bar=progress, threads=4, is_llamacpp=False)\n\n        # Move the GGUF\n        (Path(shared.args.model_dir) / gguf_name).rename(output_folder / gguf_name)\n\n        yield (f\"Model saved to `{output_folder}/`.\\n\\nYou can now load it using llamacpp_HF.\")\n    except:\n        progress(1.0)\n        yield traceback.format_exc().replace('\\n', '\\n\\n')\n\n\ndef update_truncation_length(current_length, state):\n    if 'loader' in state:\n        if state['loader'].lower().startswith('exllama'):\n            return state['max_seq_len']\n        elif state['loader'] in ['llama.cpp', 'llamacpp_HF']:\n            return state['n_ctx']\n\n    return current_length\n", "modules/github.py": "import subprocess\nfrom pathlib import Path\n\nnew_extensions = set()\n\n\ndef clone_or_pull_repository(github_url):\n    global new_extensions\n\n    repository_folder = Path(\"extensions\")\n    repo_name = github_url.rstrip(\"/\").split(\"/\")[-1].split(\".\")[0]\n\n    # Check if the repository folder exists\n    if not repository_folder.exists():\n        repository_folder.mkdir(parents=True)\n\n    repo_path = repository_folder / repo_name\n\n    # Check if the repository is already cloned\n    if repo_path.exists():\n        yield f\"Updating {github_url}...\"\n        # Perform a 'git pull' to update the repository\n        try:\n            pull_output = subprocess.check_output([\"git\", \"-C\", repo_path, \"pull\"], stderr=subprocess.STDOUT)\n            yield \"Done.\"\n            return pull_output.decode()\n        except subprocess.CalledProcessError as e:\n            return str(e)\n\n    # Clone the repository\n    try:\n        yield f\"Cloning {github_url}...\"\n        clone_output = subprocess.check_output([\"git\", \"clone\", github_url, repo_path], stderr=subprocess.STDOUT)\n        new_extensions.add(repo_name)\n        yield f\"The extension `{repo_name}` has been downloaded.\\n\\nPlease close the the web UI completely and launch it again to be able to load it.\"\n        return clone_output.decode()\n    except subprocess.CalledProcessError as e:\n        return str(e)\n", "modules/models.py": "import gc\nimport os\nimport pprint\nimport re\nimport time\nimport traceback\nfrom pathlib import Path\n\nimport torch\nimport transformers\nfrom accelerate import infer_auto_device_map, init_empty_weights\nfrom accelerate.utils import (\n    is_ccl_available,\n    is_npu_available,\n    is_xpu_available\n)\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    GPTQConfig\n)\n\nimport modules.shared as shared\nfrom modules import RoPE, sampler_hijack\nfrom modules.logging_colors import logger\nfrom modules.models_settings import get_model_metadata\n\ntransformers.logging.set_verbosity_error()\n\nlocal_rank = None\nif shared.args.deepspeed:\n    import deepspeed\n    from transformers.deepspeed import (\n        HfDeepSpeedConfig,\n        is_deepspeed_zero3_enabled\n    )\n\n    from modules.deepspeed_parameters import generate_ds_config\n\n    # Distributed setup\n    local_rank = shared.args.local_rank if shared.args.local_rank is not None else int(os.getenv(\"LOCAL_RANK\", \"0\"))\n    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n    if is_xpu_available() and is_ccl_available():\n        torch.xpu.set_device(local_rank)\n        deepspeed.init_distributed(backend=\"ccl\")\n    elif is_npu_available():\n        torch.npu.set_device(local_rank)\n        deepspeed.init_distributed(dist_backend=\"hccl\")\n    else:\n        torch.cuda.set_device(local_rank)\n        deepspeed.init_distributed()\n    ds_config = generate_ds_config(shared.args.bf16, 1 * world_size, shared.args.nvme_offload_dir)\n    dschf = HfDeepSpeedConfig(ds_config)  # Keep this object alive for the Transformers integration\n\nsampler_hijack.hijack_samplers()\n\n\nlast_generation_time = time.time()\n\n\ndef load_model(model_name, loader=None):\n    logger.info(f\"Loading \\\"{model_name}\\\"\")\n    t0 = time.time()\n\n    shared.is_seq2seq = False\n    shared.model_name = model_name\n    load_func_map = {\n        'Transformers': huggingface_loader,\n        'AutoGPTQ': AutoGPTQ_loader,\n        'llama.cpp': llamacpp_loader,\n        'llamacpp_HF': llamacpp_HF_loader,\n        'ExLlamav2': ExLlamav2_loader,\n        'ExLlamav2_HF': ExLlamav2_HF_loader,\n        'AutoAWQ': AutoAWQ_loader,\n        'HQQ': HQQ_loader,\n    }\n\n    metadata = get_model_metadata(model_name)\n    if loader is None:\n        if shared.args.loader is not None:\n            loader = shared.args.loader\n        else:\n            loader = metadata['loader']\n            if loader is None:\n                logger.error('The path to the model does not exist. Exiting.')\n                raise ValueError\n\n    shared.args.loader = loader\n    output = load_func_map[loader](model_name)\n    if type(output) is tuple:\n        model, tokenizer = output\n    else:\n        model = output\n        if model is None:\n            return None, None\n        else:\n            tokenizer = load_tokenizer(model_name, model)\n\n    shared.settings.update({k: v for k, v in metadata.items() if k in shared.settings})\n    if loader.lower().startswith('exllama'):\n        shared.settings['truncation_length'] = shared.args.max_seq_len\n    elif loader in ['llama.cpp', 'llamacpp_HF']:\n        shared.settings['truncation_length'] = shared.args.n_ctx\n\n    logger.info(f\"Loaded \\\"{model_name}\\\" in {(time.time()-t0):.2f} seconds.\")\n    logger.info(f\"LOADER: \\\"{loader}\\\"\")\n    logger.info(f\"TRUNCATION LENGTH: {shared.settings['truncation_length']}\")\n    logger.info(f\"INSTRUCTION TEMPLATE: \\\"{metadata['instruction_template']}\\\"\")\n    return model, tokenizer\n\n\ndef load_tokenizer(model_name, model):\n    tokenizer = None\n    path_to_model = Path(f\"{shared.args.model_dir}/{model_name}/\")\n    if path_to_model.exists():\n        if shared.args.no_use_fast:\n            logger.info('Loading the tokenizer with use_fast=False.')\n\n        tokenizer = AutoTokenizer.from_pretrained(\n            path_to_model,\n            trust_remote_code=shared.args.trust_remote_code,\n            use_fast=not shared.args.no_use_fast\n        )\n\n    return tokenizer\n\n\ndef huggingface_loader(model_name):\n    path_to_model = Path(f'{shared.args.model_dir}/{model_name}')\n    params = {\n        'low_cpu_mem_usage': True,\n        'torch_dtype': torch.bfloat16 if shared.args.bf16 else torch.float16,\n    }\n\n    if shared.args.trust_remote_code:\n        params['trust_remote_code'] = True\n\n    if shared.args.use_flash_attention_2:\n        params['use_flash_attention_2'] = True\n\n    if shared.args.force_safetensors:\n        params['force_safetensors'] = True\n\n    config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=shared.args.trust_remote_code)\n\n    if 'chatglm' in model_name.lower():\n        LoaderClass = AutoModel\n    else:\n        if config.to_dict().get('is_encoder_decoder', False):\n            LoaderClass = AutoModelForSeq2SeqLM\n            shared.is_seq2seq = True\n        else:\n            LoaderClass = AutoModelForCausalLM\n\n    # Load the model without any special settings\n    if not any([shared.args.cpu, shared.args.load_in_8bit, shared.args.load_in_4bit, shared.args.auto_devices, shared.args.disk, shared.args.deepspeed, shared.args.gpu_memory is not None, shared.args.cpu_memory is not None, shared.args.compress_pos_emb > 1, shared.args.alpha_value > 1, shared.args.disable_exllama, shared.args.disable_exllamav2]):\n        logger.info(\"TRANSFORMERS_PARAMS=\")\n        pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint(params)\n        print()\n\n        model = LoaderClass.from_pretrained(path_to_model, **params)\n        if not (hasattr(model, 'is_loaded_in_4bit') and model.is_loaded_in_4bit):\n            if torch.backends.mps.is_available():\n                device = torch.device('mps')\n                model = model.to(device)\n            elif is_xpu_available():\n                device = torch.device(\"xpu\")\n                model = model.to(device)\n            elif is_npu_available():\n                device = torch.device(\"npu\")\n                model = model.to(device)\n            else:\n                model = model.cuda()\n\n    # DeepSpeed ZeRO-3\n    elif shared.args.deepspeed:\n        model = LoaderClass.from_pretrained(path_to_model, torch_dtype=params['torch_dtype'], trust_remote_code=params.get('trust_remote_code'))\n        model = deepspeed.initialize(model=model, config_params=ds_config, model_parameters=None, optimizer=None, lr_scheduler=None)[0]\n        model.module.eval()  # Inference\n        logger.info(f'DeepSpeed ZeRO-3 is enabled: {is_deepspeed_zero3_enabled()}')\n\n    # Load with quantization and/or offloading\n    else:\n        if not any((shared.args.cpu, torch.cuda.is_available(), is_xpu_available(), torch.backends.mps.is_available())):\n            logger.warning('torch.cuda.is_available() and is_xpu_available() returned False. This means that no GPU has been detected. Falling back to CPU mode.')\n            shared.args.cpu = True\n\n        if shared.args.cpu:\n            params['torch_dtype'] = torch.float32\n        else:\n            params['device_map'] = 'auto'\n            if x := get_max_memory_dict():\n                params['max_memory'] = x\n\n            if shared.args.load_in_4bit:\n                # See https://github.com/huggingface/transformers/pull/23479/files\n                # and https://huggingface.co/blog/4bit-transformers-bitsandbytes\n                quantization_config_params = {\n                    'load_in_4bit': True,\n                    'bnb_4bit_compute_dtype': eval(\"torch.{}\".format(shared.args.compute_dtype)) if shared.args.compute_dtype in [\"bfloat16\", \"float16\", \"float32\"] else None,\n                    'bnb_4bit_quant_type': shared.args.quant_type,\n                    'bnb_4bit_use_double_quant': shared.args.use_double_quant,\n                    'llm_int8_enable_fp32_cpu_offload': True\n                }\n\n                params['quantization_config'] = BitsAndBytesConfig(**quantization_config_params)\n\n            elif shared.args.load_in_8bit:\n                if any((shared.args.auto_devices, shared.args.gpu_memory)):\n                    params['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n                else:\n                    params['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n\n                if params.get('max_memory') is not None:\n                    with init_empty_weights():\n                        model = LoaderClass.from_config(config, trust_remote_code=params.get('trust_remote_code'))\n\n                    model.tie_weights()\n                    params['device_map'] = infer_auto_device_map(\n                        model,\n                        dtype=torch.int8,\n                        max_memory=params.get('max_memory'),\n                        no_split_module_classes=model._no_split_modules\n                    )\n\n            if shared.args.disk:\n                params['offload_folder'] = shared.args.disk_cache_dir\n\n        if shared.args.disable_exllama or shared.args.disable_exllamav2:\n            try:\n                gptq_config = GPTQConfig(\n                    bits=config.quantization_config.get('bits', 4),\n                    disable_exllama=shared.args.disable_exllama,\n                    disable_exllamav2=shared.args.disable_exllamav2,\n                )\n\n                params['quantization_config'] = gptq_config\n                logger.info(f'Loading with disable_exllama={shared.args.disable_exllama} and disable_exllamav2={shared.args.disable_exllamav2}.')\n            except:\n                exc = traceback.format_exc()\n                logger.error('Failed to disable exllama. Does the config.json for this model contain the necessary quantization info?')\n                print(exc)\n\n        if shared.args.compress_pos_emb > 1:\n            params['rope_scaling'] = {'type': 'linear', 'factor': shared.args.compress_pos_emb}\n        elif shared.args.alpha_value > 1:\n            params['rope_scaling'] = {'type': 'dynamic', 'factor': RoPE.get_alpha_value(shared.args.alpha_value, shared.args.rope_freq_base)}\n\n        logger.info(\"TRANSFORMERS_PARAMS=\")\n        pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint(params)\n        print()\n        model = LoaderClass.from_pretrained(path_to_model, **params)\n\n    return model\n\n\ndef llamacpp_loader(model_name):\n    from modules.llamacpp_model import LlamaCppModel\n\n    path = Path(f'{shared.args.model_dir}/{model_name}')\n    if path.is_file():\n        model_file = path\n    else:\n        model_file = sorted(Path(f'{shared.args.model_dir}/{model_name}').glob('*.gguf'))[0]\n\n    logger.info(f\"llama.cpp weights detected: \\\"{model_file}\\\"\")\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n    return model, tokenizer\n\n\ndef llamacpp_HF_loader(model_name):\n    from modules.llamacpp_hf import LlamacppHF\n\n    path = Path(f'{shared.args.model_dir}/{model_name}')\n\n    # Check if a HF tokenizer is available for the model\n    if all((path / file).exists() for file in ['tokenizer_config.json']):\n        logger.info(f'Using tokenizer from: \\\"{path}\\\"')\n    else:\n        logger.error(\"Could not load the model because a tokenizer in Transformers format was not found.\")\n        return None, None\n\n    model = LlamacppHF.from_pretrained(model_name)\n    return model\n\n\ndef AutoAWQ_loader(model_name):\n    from awq import AutoAWQForCausalLM\n\n    model_dir = Path(f'{shared.args.model_dir}/{model_name}')\n\n    model = AutoAWQForCausalLM.from_quantized(\n        quant_path=model_dir,\n        max_new_tokens=shared.args.max_seq_len,\n        trust_remote_code=shared.args.trust_remote_code,\n        fuse_layers=not shared.args.no_inject_fused_attention,\n        max_memory=get_max_memory_dict(),\n        batch_size=1,\n        safetensors=any(model_dir.glob('*.safetensors')),\n    )\n\n    return model\n\n\ndef AutoGPTQ_loader(model_name):\n    import modules.AutoGPTQ_loader\n\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\n\n\ndef ExLlamav2_loader(model_name):\n    from modules.exllamav2 import Exllamav2Model\n\n    model, tokenizer = Exllamav2Model.from_pretrained(model_name)\n    return model, tokenizer\n\n\ndef ExLlamav2_HF_loader(model_name):\n    from modules.exllamav2_hf import Exllamav2HF\n\n    return Exllamav2HF.from_pretrained(model_name)\n\n\ndef HQQ_loader(model_name):\n    from hqq.core.quantize import HQQBackend, HQQLinear\n    from hqq.models.hf.base import AutoHQQHFModel\n\n    logger.info(f\"Loading HQQ model with backend: \\\"{shared.args.hqq_backend}\\\"\")\n\n    model_dir = Path(f'{shared.args.model_dir}/{model_name}')\n    model = AutoHQQHFModel.from_quantized(str(model_dir))\n    HQQLinear.set_backend(getattr(HQQBackend, shared.args.hqq_backend))\n    return model\n\n\ndef get_max_memory_dict():\n    max_memory = {}\n    max_cpu_memory = shared.args.cpu_memory.strip() if shared.args.cpu_memory is not None else '99GiB'\n    if shared.args.gpu_memory:\n        memory_map = list(map(lambda x: x.strip(), shared.args.gpu_memory))\n        for i in range(len(memory_map)):\n            max_memory[i] = f'{memory_map[i]}GiB' if not re.match('.*ib$', memory_map[i].lower()) else memory_map[i]\n\n        max_memory['cpu'] = f'{max_cpu_memory}GiB' if not re.match('.*ib$', max_cpu_memory.lower()) else max_cpu_memory\n\n    # If --auto-devices is provided standalone, try to get a reasonable value\n    # for the maximum memory of device :0\n    elif shared.args.auto_devices:\n        if is_xpu_available():\n            total_mem = (torch.xpu.get_device_properties(0).total_memory / (1024 * 1024))\n        else:\n            total_mem = (torch.cuda.get_device_properties(0).total_memory / (1024 * 1024))\n\n        suggestion = round((total_mem - 1000) / 1000) * 1000\n        if total_mem - suggestion < 800:\n            suggestion -= 1000\n\n        suggestion = int(round(suggestion / 1000))\n        logger.warning(f\"Auto-assiging --gpu-memory {suggestion} for your GPU to try to prevent out-of-memory errors. You can manually set other values.\")\n        max_memory[0] = f'{suggestion}GiB'\n        max_memory['cpu'] = f'{max_cpu_memory}GiB' if not re.match('.*ib$', max_cpu_memory.lower()) else max_cpu_memory\n\n    return max_memory if len(max_memory) > 0 else None\n\n\ndef clear_torch_cache():\n    gc.collect()\n    if not shared.args.cpu:\n        if is_xpu_available():\n            torch.xpu.empty_cache()\n        else:\n            torch.cuda.empty_cache()\n\n\ndef unload_model():\n    shared.model = shared.tokenizer = None\n    shared.previous_model_name = shared.model_name\n    shared.model_name = 'None'\n    shared.lora_names = []\n    shared.model_dirty_from_training = False\n    clear_torch_cache()\n\n\ndef reload_model():\n    unload_model()\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\n\ndef unload_model_if_idle():\n    global last_generation_time\n\n    logger.info(f\"Setting a timeout of {shared.args.idle_timeout} minutes to unload the model in case of inactivity.\")\n\n    while True:\n        shared.generation_lock.acquire()\n        try:\n            if time.time() - last_generation_time > shared.args.idle_timeout * 60:\n                if shared.model is not None:\n                    logger.info(\"Unloading the model for inactivity.\")\n                    unload_model()\n        finally:\n            shared.generation_lock.release()\n\n        time.sleep(60)\n", "modules/ui.py": "import copy\nfrom pathlib import Path\n\nimport gradio as gr\nimport torch\nimport yaml\nfrom transformers import is_torch_xpu_available\n\nimport extensions\nfrom modules import shared\n\nwith open(Path(__file__).resolve().parent / '../css/NotoSans/stylesheet.css', 'r') as f:\n    css = f.read()\nwith open(Path(__file__).resolve().parent / '../css/main.css', 'r') as f:\n    css += f.read()\nwith open(Path(__file__).resolve().parent / '../css/katex/katex.min.css', 'r') as f:\n    css += f.read()\nwith open(Path(__file__).resolve().parent / '../css/highlightjs/github-dark.min.css', 'r') as f:\n    css += f.read()\nwith open(Path(__file__).resolve().parent / '../css/highlightjs/highlightjs-copy.min.css', 'r') as f:\n    css += f.read()\nwith open(Path(__file__).resolve().parent / '../js/main.js', 'r') as f:\n    js = f.read()\nwith open(Path(__file__).resolve().parent / '../js/save_files.js', 'r') as f:\n    save_files_js = f.read()\nwith open(Path(__file__).resolve().parent / '../js/switch_tabs.js', 'r') as f:\n    switch_tabs_js = f.read()\nwith open(Path(__file__).resolve().parent / '../js/show_controls.js', 'r') as f:\n    show_controls_js = f.read()\nwith open(Path(__file__).resolve().parent / '../js/update_big_picture.js', 'r') as f:\n    update_big_picture_js = f.read()\n\nrefresh_symbol = '\ud83d\udd04'\ndelete_symbol = '\ud83d\uddd1\ufe0f'\nsave_symbol = '\ud83d\udcbe'\n\ntheme = gr.themes.Default(\n    font=['Noto Sans', 'Helvetica', 'ui-sans-serif', 'system-ui', 'sans-serif'],\n    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],\n).set(\n    border_color_primary='#c5c5d2',\n    button_large_padding='6px 12px',\n    body_text_color_subdued='#484848',\n    background_fill_secondary='#eaeaea',\n    background_fill_primary='var(--neutral-50)',\n)\n\nif Path(\"notification.mp3\").exists():\n    audio_notification_js = \"document.querySelector('#audio_notification audio')?.play();\"\nelse:\n    audio_notification_js = \"\"\n\n\ndef list_model_elements():\n    elements = [\n        'loader',\n        'filter_by_loader',\n        'cpu_memory',\n        'auto_devices',\n        'disk',\n        'cpu',\n        'bf16',\n        'load_in_8bit',\n        'trust_remote_code',\n        'no_use_fast',\n        'use_flash_attention_2',\n        'load_in_4bit',\n        'compute_dtype',\n        'quant_type',\n        'use_double_quant',\n        'wbits',\n        'groupsize',\n        'triton',\n        'desc_act',\n        'no_inject_fused_attention',\n        'no_inject_fused_mlp',\n        'no_use_cuda_fp16',\n        'disable_exllama',\n        'disable_exllamav2',\n        'cfg_cache',\n        'no_flash_attn',\n        'num_experts_per_token',\n        'cache_8bit',\n        'cache_4bit',\n        'autosplit',\n        'threads',\n        'threads_batch',\n        'n_batch',\n        'no_mmap',\n        'mlock',\n        'no_mul_mat_q',\n        'n_gpu_layers',\n        'tensor_split',\n        'n_ctx',\n        'gpu_split',\n        'max_seq_len',\n        'compress_pos_emb',\n        'alpha_value',\n        'rope_freq_base',\n        'numa',\n        'logits_all',\n        'no_offload_kqv',\n        'row_split',\n        'tensorcores',\n        'flash_attn',\n        'streaming_llm',\n        'attention_sink_size',\n        'hqq_backend',\n    ]\n    if is_torch_xpu_available():\n        for i in range(torch.xpu.device_count()):\n            elements.append(f'gpu_memory_{i}')\n    else:\n        for i in range(torch.cuda.device_count()):\n            elements.append(f'gpu_memory_{i}')\n\n    return elements\n\n\ndef list_interface_input_elements():\n    elements = [\n        'max_new_tokens',\n        'auto_max_new_tokens',\n        'max_tokens_second',\n        'max_updates_second',\n        'prompt_lookup_num_tokens',\n        'seed',\n        'temperature',\n        'temperature_last',\n        'dynamic_temperature',\n        'dynatemp_low',\n        'dynatemp_high',\n        'dynatemp_exponent',\n        'smoothing_factor',\n        'smoothing_curve',\n        'top_p',\n        'min_p',\n        'top_k',\n        'typical_p',\n        'epsilon_cutoff',\n        'eta_cutoff',\n        'repetition_penalty',\n        'presence_penalty',\n        'frequency_penalty',\n        'repetition_penalty_range',\n        'encoder_repetition_penalty',\n        'no_repeat_ngram_size',\n        'dry_multiplier',\n        'dry_base',\n        'dry_allowed_length',\n        'dry_sequence_breakers',\n        'do_sample',\n        'penalty_alpha',\n        'mirostat_mode',\n        'mirostat_tau',\n        'mirostat_eta',\n        'grammar_string',\n        'negative_prompt',\n        'guidance_scale',\n        'add_bos_token',\n        'ban_eos_token',\n        'custom_token_bans',\n        'sampler_priority',\n        'truncation_length',\n        'custom_stopping_strings',\n        'skip_special_tokens',\n        'stream',\n        'tfs',\n        'top_a',\n    ]\n\n    # Chat elements\n    elements += [\n        'textbox',\n        'start_with',\n        'character_menu',\n        'history',\n        'name1',\n        'user_bio',\n        'name2',\n        'greeting',\n        'context',\n        'mode',\n        'custom_system_message',\n        'instruction_template_str',\n        'chat_template_str',\n        'chat_style',\n        'chat-instruct_command',\n    ]\n\n    # Notebook/default elements\n    elements += [\n        'textbox-notebook',\n        'textbox-default',\n        'output_textbox',\n        'prompt_menu-default',\n        'prompt_menu-notebook',\n    ]\n\n    # Model elements\n    elements += list_model_elements()\n\n    return elements\n\n\ndef gather_interface_values(*args):\n    output = {}\n    for i, element in enumerate(list_interface_input_elements()):\n        output[element] = args[i]\n\n    if not shared.args.multi_user:\n        shared.persistent_interface_state = output\n\n    return output\n\n\ndef apply_interface_values(state, use_persistent=False):\n    if use_persistent:\n        state = shared.persistent_interface_state\n\n    elements = list_interface_input_elements()\n    if len(state) == 0:\n        return [gr.update() for k in elements]  # Dummy, do nothing\n    else:\n        return [state[k] if k in state else gr.update() for k in elements]\n\n\ndef save_settings(state, preset, extensions_list, show_controls, theme_state):\n    output = copy.deepcopy(shared.settings)\n    exclude = ['name2', 'greeting', 'context', 'turn_template', 'truncation_length']\n    for k in state:\n        if k in shared.settings and k not in exclude:\n            output[k] = state[k]\n\n    output['preset'] = preset\n    output['prompt-default'] = state['prompt_menu-default']\n    output['prompt-notebook'] = state['prompt_menu-notebook']\n    output['character'] = state['character_menu']\n    output['default_extensions'] = extensions_list\n    output['seed'] = int(output['seed'])\n    output['show_controls'] = show_controls\n    output['dark_theme'] = True if theme_state == 'dark' else False\n\n    # Save extension values in the UI\n    for extension_name in extensions_list:\n        extension = getattr(extensions, extension_name, None)\n        if extension:\n            extension = extension.script\n            if hasattr(extension, 'params'):\n                params = getattr(extension, 'params')\n                for param in params:\n                    _id = f\"{extension_name}-{param}\"\n                    # Only save if different from default value\n                    if param not in shared.default_settings or params[param] != shared.default_settings[param]:\n                        output[_id] = params[param]\n\n    # Do not save unchanged settings\n    for key in list(output.keys()):\n        if key in shared.default_settings and output[key] == shared.default_settings[key]:\n            output.pop(key)\n\n    return yaml.dump(output, sort_keys=False, width=float(\"inf\"))\n\n\ndef create_refresh_button(refresh_component, refresh_method, refreshed_args, elem_class, interactive=True):\n    \"\"\"\n    Copied from https://github.com/AUTOMATIC1111/stable-diffusion-webui\n    \"\"\"\n    def refresh():\n        refresh_method()\n        args = refreshed_args() if callable(refreshed_args) else refreshed_args\n\n        return gr.update(**(args or {}))\n\n    refresh_button = gr.Button(refresh_symbol, elem_classes=elem_class, interactive=interactive)\n    refresh_button.click(\n        fn=lambda: {k: tuple(v) if type(k) is list else v for k, v in refresh().items()},\n        inputs=[],\n        outputs=[refresh_component]\n    )\n\n    return refresh_button\n", "modules/llama_cpp_python_hijack.py": "from typing import Sequence\n\nfrom tqdm import tqdm\n\nfrom modules import shared\nfrom modules.cache_utils import process_llamacpp_cache\n\ntry:\n    import llama_cpp\nexcept:\n    llama_cpp = None\n\ntry:\n    import llama_cpp_cuda\nexcept:\n    llama_cpp_cuda = None\n\ntry:\n    import llama_cpp_cuda_tensorcores\nexcept:\n    llama_cpp_cuda_tensorcores = None\n\n\ndef eval_with_progress(self, tokens: Sequence[int]):\n    \"\"\"\n    A copy of\n\n    https://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama.py\n\n    with tqdm to show prompt processing progress.\n    \"\"\"\n    assert self._ctx.ctx is not None\n    assert self._batch.batch is not None\n    self._ctx.kv_cache_seq_rm(-1, self.n_tokens, -1)\n\n    if len(tokens) > 1:\n        progress_bar = tqdm(range(0, len(tokens), self.n_batch), desc=\"Prompt evaluation\", leave=False)\n    else:\n        progress_bar = range(0, len(tokens), self.n_batch)\n\n    for i in progress_bar:\n        batch = tokens[i : min(len(tokens), i + self.n_batch)]\n        n_past = self.n_tokens\n        n_tokens = len(batch)\n        self._batch.set_batch(\n            batch=batch, n_past=n_past, logits_all=self.context_params.logits_all\n        )\n        self._ctx.decode(self._batch)\n        # Save tokens\n        self.input_ids[n_past : n_past + n_tokens] = batch\n        # Save logits\n        if self.context_params.logits_all:\n            rows = n_tokens\n            cols = self._n_vocab\n            logits = self._ctx.get_logits()[: rows * cols]\n            self.scores[n_past : n_past + n_tokens, :].reshape(-1)[: :] = logits\n        else:\n            rows = 1\n            cols = self._n_vocab\n            logits = self._ctx.get_logits()[: rows * cols]\n            self.scores[n_past + n_tokens - 1, :].reshape(-1)[: :] = logits\n        # Update n_tokens\n        self.n_tokens += n_tokens\n\n\ndef monkey_patch_generate(lib):\n\n    def my_generate(self, *args, **kwargs):\n\n        if shared.args.streaming_llm:\n            new_sequence = args[0]\n            past_sequence = self._input_ids\n\n            # Do the cache trimming for StreamingLLM\n            process_llamacpp_cache(self, new_sequence, past_sequence)\n\n        for output in self.original_generate(*args, **kwargs):\n            yield output\n\n    lib.Llama.original_generate = lib.Llama.generate\n    lib.Llama.generate = my_generate\n\n\nfor lib in [llama_cpp, llama_cpp_cuda, llama_cpp_cuda_tensorcores]:\n    if lib is not None:\n        lib.Llama.eval = eval_with_progress\n        monkey_patch_generate(lib)\n", "modules/chat.py": "import base64\nimport copy\nimport functools\nimport html\nimport json\nimport re\nfrom datetime import datetime\nfrom functools import partial\nfrom pathlib import Path\n\nimport gradio as gr\nimport yaml\nfrom jinja2.sandbox import ImmutableSandboxedEnvironment\nfrom PIL import Image\n\nimport modules.shared as shared\nfrom modules import utils\nfrom modules.extensions import apply_extensions\nfrom modules.html_generator import chat_html_wrapper, make_thumbnail\nfrom modules.logging_colors import logger\nfrom modules.text_generation import (\n    generate_reply,\n    get_encoded_length,\n    get_max_prompt_length\n)\nfrom modules.utils import delete_file, get_available_characters, save_file\n\n# Copied from the Transformers library\njinja_env = ImmutableSandboxedEnvironment(trim_blocks=True, lstrip_blocks=True)\n\n\ndef str_presenter(dumper, data):\n    \"\"\"\n    Copied from https://github.com/yaml/pyyaml/issues/240\n    Makes pyyaml output prettier multiline strings.\n    \"\"\"\n\n    if data.count('\\n') > 0:\n        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n\n    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n\n\nyaml.add_representer(str, str_presenter)\nyaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n\n\ndef get_generation_prompt(renderer, impersonate=False, strip_trailing_spaces=True):\n    '''\n    Given a Jinja template, reverse-engineers the prefix and the suffix for\n    an assistant message (if impersonate=False) or an user message\n    (if impersonate=True)\n    '''\n\n    if impersonate:\n        messages = [\n            {\"role\": \"user\", \"content\": \"<<|user-message-1|>>\"},\n            {\"role\": \"user\", \"content\": \"<<|user-message-2|>>\"},\n        ]\n    else:\n        messages = [\n            {\"role\": \"assistant\", \"content\": \"<<|user-message-1|>>\"},\n            {\"role\": \"assistant\", \"content\": \"<<|user-message-2|>>\"},\n        ]\n\n    prompt = renderer(messages=messages)\n\n    suffix_plus_prefix = prompt.split(\"<<|user-message-1|>>\")[1].split(\"<<|user-message-2|>>\")[0]\n    suffix = prompt.split(\"<<|user-message-2|>>\")[1]\n    prefix = suffix_plus_prefix[len(suffix):]\n\n    if strip_trailing_spaces:\n        prefix = prefix.rstrip(' ')\n\n    return prefix, suffix\n\n\ndef generate_chat_prompt(user_input, state, **kwargs):\n    impersonate = kwargs.get('impersonate', False)\n    _continue = kwargs.get('_continue', False)\n    also_return_rows = kwargs.get('also_return_rows', False)\n    history = kwargs.get('history', state['history'])['internal']\n\n    # Templates\n    chat_template_str = state['chat_template_str']\n    if state['mode'] != 'instruct':\n        chat_template_str = replace_character_names(chat_template_str, state['name1'], state['name2'])\n\n    instruction_template = jinja_env.from_string(state['instruction_template_str'])\n    instruct_renderer = partial(instruction_template.render, add_generation_prompt=False)\n    chat_template = jinja_env.from_string(chat_template_str)\n    chat_renderer = partial(\n        chat_template.render,\n        add_generation_prompt=False,\n        name1=state['name1'],\n        name2=state['name2'],\n        user_bio=replace_character_names(state['user_bio'], state['name1'], state['name2']),\n    )\n\n    messages = []\n\n    if state['mode'] == 'instruct':\n        renderer = instruct_renderer\n        if state['custom_system_message'].strip() != '':\n            messages.append({\"role\": \"system\", \"content\": state['custom_system_message']})\n    else:\n        renderer = chat_renderer\n        if state['context'].strip() != '' or state['user_bio'].strip() != '':\n            context = replace_character_names(state['context'], state['name1'], state['name2'])\n            messages.append({\"role\": \"system\", \"content\": context})\n\n    insert_pos = len(messages)\n    for user_msg, assistant_msg in reversed(history):\n        user_msg = user_msg.strip()\n        assistant_msg = assistant_msg.strip()\n\n        if assistant_msg:\n            messages.insert(insert_pos, {\"role\": \"assistant\", \"content\": assistant_msg})\n\n        if user_msg not in ['', '<|BEGIN-VISIBLE-CHAT|>']:\n            messages.insert(insert_pos, {\"role\": \"user\", \"content\": user_msg})\n\n    user_input = user_input.strip()\n    if user_input and not impersonate and not _continue:\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n    def remove_extra_bos(prompt):\n        for bos_token in ['<s>', '<|startoftext|>', '<BOS_TOKEN>', '<|endoftext|>']:\n            while prompt.startswith(bos_token):\n                prompt = prompt[len(bos_token):]\n\n        return prompt\n\n    def make_prompt(messages):\n        if state['mode'] == 'chat-instruct' and _continue:\n            prompt = renderer(messages=messages[:-1])\n        else:\n            prompt = renderer(messages=messages)\n\n        if state['mode'] == 'chat-instruct':\n            outer_messages = []\n            if state['custom_system_message'].strip() != '':\n                outer_messages.append({\"role\": \"system\", \"content\": state['custom_system_message']})\n\n            prompt = remove_extra_bos(prompt)\n            command = state['chat-instruct_command']\n            command = command.replace('<|character|>', state['name2'] if not impersonate else state['name1'])\n            command = command.replace('<|prompt|>', prompt)\n            command = replace_character_names(command, state['name1'], state['name2'])\n\n            if _continue:\n                prefix = get_generation_prompt(renderer, impersonate=impersonate, strip_trailing_spaces=False)[0]\n                prefix += messages[-1][\"content\"]\n            else:\n                prefix = get_generation_prompt(renderer, impersonate=impersonate)[0]\n                if not impersonate:\n                    prefix = apply_extensions('bot_prefix', prefix, state)\n\n            outer_messages.append({\"role\": \"user\", \"content\": command})\n            outer_messages.append({\"role\": \"assistant\", \"content\": prefix})\n\n            prompt = instruction_template.render(messages=outer_messages)\n            suffix = get_generation_prompt(instruct_renderer, impersonate=False)[1]\n            if len(suffix) > 0:\n                prompt = prompt[:-len(suffix)]\n\n        else:\n            if _continue:\n                suffix = get_generation_prompt(renderer, impersonate=impersonate)[1]\n                if len(suffix) > 0:\n                    prompt = prompt[:-len(suffix)]\n            else:\n                prefix = get_generation_prompt(renderer, impersonate=impersonate)[0]\n                if state['mode'] == 'chat' and not impersonate:\n                    prefix = apply_extensions('bot_prefix', prefix, state)\n\n                prompt += prefix\n\n        prompt = remove_extra_bos(prompt)\n        return prompt\n\n    prompt = make_prompt(messages)\n\n    # Handle truncation\n    if shared.tokenizer is not None:\n        max_length = get_max_prompt_length(state)\n        encoded_length = get_encoded_length(prompt)\n        while len(messages) > 0 and encoded_length > max_length:\n\n            # Remove old message, save system message\n            if len(messages) > 2 and messages[0]['role'] == 'system':\n                messages.pop(1)\n\n            # Remove old message when no system message is present\n            elif len(messages) > 1 and messages[0]['role'] != 'system':\n                messages.pop(0)\n\n            # Resort to truncating the user input\n            else:\n\n                user_message = messages[-1]['content']\n\n                # Bisect the truncation point\n                left, right = 0, len(user_message) - 1\n\n                while right - left > 1:\n                    mid = (left + right) // 2\n\n                    messages[-1]['content'] = user_message[:mid]\n                    prompt = make_prompt(messages)\n                    encoded_length = get_encoded_length(prompt)\n\n                    if encoded_length <= max_length:\n                        left = mid\n                    else:\n                        right = mid\n\n                messages[-1]['content'] = user_message[:left]\n                prompt = make_prompt(messages)\n                encoded_length = get_encoded_length(prompt)\n                if encoded_length > max_length:\n                    logger.error(f\"Failed to build the chat prompt. The input is too long for the available context length.\\n\\nTruncation length: {state['truncation_length']}\\nmax_new_tokens: {state['max_new_tokens']} (is it too high?)\\nAvailable context length: {max_length}\\n\")\n                    raise ValueError\n                else:\n                    logger.warning(f\"The input has been truncated. Context length: {state['truncation_length']}, max_new_tokens: {state['max_new_tokens']}, available context length: {max_length}.\")\n                    break\n\n            prompt = make_prompt(messages)\n            encoded_length = get_encoded_length(prompt)\n\n    if also_return_rows:\n        return prompt, [message['content'] for message in messages]\n    else:\n        return prompt\n\n\ndef get_stopping_strings(state):\n    stopping_strings = []\n    renderers = []\n\n    if state['mode'] in ['instruct', 'chat-instruct']:\n        template = jinja_env.from_string(state['instruction_template_str'])\n        renderer = partial(template.render, add_generation_prompt=False)\n        renderers.append(renderer)\n\n    if state['mode'] in ['chat', 'chat-instruct']:\n        template = jinja_env.from_string(state['chat_template_str'])\n        renderer = partial(template.render, add_generation_prompt=False, name1=state['name1'], name2=state['name2'])\n        renderers.append(renderer)\n\n    for renderer in renderers:\n        prefix_bot, suffix_bot = get_generation_prompt(renderer, impersonate=False)\n        prefix_user, suffix_user = get_generation_prompt(renderer, impersonate=True)\n\n        stopping_strings += [\n            suffix_user + prefix_bot,\n            suffix_user + prefix_user,\n            suffix_bot + prefix_bot,\n            suffix_bot + prefix_user,\n        ]\n\n    if 'stopping_strings' in state and isinstance(state['stopping_strings'], list):\n        stopping_strings += state.pop('stopping_strings')\n\n    return list(set(stopping_strings))\n\n\ndef chatbot_wrapper(text, state, regenerate=False, _continue=False, loading_message=True, for_ui=False):\n    history = state['history']\n    output = copy.deepcopy(history)\n    output = apply_extensions('history', output)\n    state = apply_extensions('state', state)\n\n    visible_text = None\n    stopping_strings = get_stopping_strings(state)\n    is_stream = state['stream']\n\n    # Prepare the input\n    if not (regenerate or _continue):\n        visible_text = html.escape(text)\n\n        # Apply extensions\n        text, visible_text = apply_extensions('chat_input', text, visible_text, state)\n        text = apply_extensions('input', text, state, is_chat=True)\n\n        output['internal'].append([text, ''])\n        output['visible'].append([visible_text, ''])\n\n        # *Is typing...*\n        if loading_message:\n            yield {\n                'visible': output['visible'][:-1] + [[output['visible'][-1][0], shared.processing_message]],\n                'internal': output['internal']\n            }\n    else:\n        text, visible_text = output['internal'][-1][0], output['visible'][-1][0]\n        if regenerate:\n            if loading_message:\n                yield {\n                    'visible': output['visible'][:-1] + [[visible_text, shared.processing_message]],\n                    'internal': output['internal'][:-1] + [[text, '']]\n                }\n        elif _continue:\n            last_reply = [output['internal'][-1][1], output['visible'][-1][1]]\n            if loading_message:\n                yield {\n                    'visible': output['visible'][:-1] + [[visible_text, last_reply[1] + '...']],\n                    'internal': output['internal']\n                }\n\n    # Generate the prompt\n    kwargs = {\n        '_continue': _continue,\n        'history': output if _continue else {k: v[:-1] for k, v in output.items()}\n    }\n    prompt = apply_extensions('custom_generate_chat_prompt', text, state, **kwargs)\n    if prompt is None:\n        prompt = generate_chat_prompt(text, state, **kwargs)\n\n    # Generate\n    reply = None\n    for j, reply in enumerate(generate_reply(prompt, state, stopping_strings=stopping_strings, is_chat=True, for_ui=for_ui)):\n\n        # Extract the reply\n        visible_reply = reply\n        if state['mode'] in ['chat', 'chat-instruct']:\n            visible_reply = re.sub(\"(<USER>|<user>|{{user}})\", state['name1'], reply)\n\n        visible_reply = html.escape(visible_reply)\n\n        if shared.stop_everything:\n            output['visible'][-1][1] = apply_extensions('output', output['visible'][-1][1], state, is_chat=True)\n            yield output\n            return\n\n        if _continue:\n            output['internal'][-1] = [text, last_reply[0] + reply]\n            output['visible'][-1] = [visible_text, last_reply[1] + visible_reply]\n            if is_stream:\n                yield output\n        elif not (j == 0 and visible_reply.strip() == ''):\n            output['internal'][-1] = [text, reply.lstrip(' ')]\n            output['visible'][-1] = [visible_text, visible_reply.lstrip(' ')]\n            if is_stream:\n                yield output\n\n    output['visible'][-1][1] = apply_extensions('output', output['visible'][-1][1], state, is_chat=True)\n    yield output\n\n\ndef impersonate_wrapper(text, state):\n\n    static_output = chat_html_wrapper(state['history'], state['name1'], state['name2'], state['mode'], state['chat_style'], state['character_menu'])\n\n    prompt = generate_chat_prompt('', state, impersonate=True)\n    stopping_strings = get_stopping_strings(state)\n\n    yield text + '...', static_output\n    reply = None\n    for reply in generate_reply(prompt + text, state, stopping_strings=stopping_strings, is_chat=True):\n        yield (text + reply).lstrip(' '), static_output\n        if shared.stop_everything:\n            return\n\n\ndef generate_chat_reply(text, state, regenerate=False, _continue=False, loading_message=True, for_ui=False):\n    history = state['history']\n    if regenerate or _continue:\n        text = ''\n        if (len(history['visible']) == 1 and not history['visible'][0][0]) or len(history['internal']) == 0:\n            yield history\n            return\n\n    for history in chatbot_wrapper(text, state, regenerate=regenerate, _continue=_continue, loading_message=loading_message, for_ui=for_ui):\n        yield history\n\n\ndef character_is_loaded(state, raise_exception=False):\n    if state['mode'] in ['chat', 'chat-instruct'] and state['name2'] == '':\n        logger.error('It looks like no character is loaded. Please load one under Parameters > Character.')\n        if raise_exception:\n            raise ValueError\n\n        return False\n    else:\n        return True\n\n\ndef generate_chat_reply_wrapper(text, state, regenerate=False, _continue=False):\n    '''\n    Same as above but returns HTML for the UI\n    '''\n\n    if not character_is_loaded(state):\n        return\n\n    if state['start_with'] != '' and not _continue:\n        if regenerate:\n            text, state['history'] = remove_last_message(state['history'])\n            regenerate = False\n\n        _continue = True\n        send_dummy_message(text, state)\n        send_dummy_reply(state['start_with'], state)\n\n    for i, history in enumerate(generate_chat_reply(text, state, regenerate, _continue, loading_message=True, for_ui=True)):\n        yield chat_html_wrapper(history, state['name1'], state['name2'], state['mode'], state['chat_style'], state['character_menu']), history\n\n\ndef remove_last_message(history):\n    if len(history['visible']) > 0 and history['internal'][-1][0] != '<|BEGIN-VISIBLE-CHAT|>':\n        last = history['visible'].pop()\n        history['internal'].pop()\n    else:\n        last = ['', '']\n\n    return html.unescape(last[0]), history\n\n\ndef send_last_reply_to_input(history):\n    if len(history['visible']) > 0:\n        return html.unescape(history['visible'][-1][1])\n    else:\n        return ''\n\n\ndef replace_last_reply(text, state):\n    history = state['history']\n\n    if len(text.strip()) == 0:\n        return history\n    elif len(history['visible']) > 0:\n        history['visible'][-1][1] = html.escape(text)\n        history['internal'][-1][1] = apply_extensions('input', text, state, is_chat=True)\n\n    return history\n\n\ndef send_dummy_message(text, state):\n    history = state['history']\n    history['visible'].append([html.escape(text), ''])\n    history['internal'].append([apply_extensions('input', text, state, is_chat=True), ''])\n    return history\n\n\ndef send_dummy_reply(text, state):\n    history = state['history']\n    if len(history['visible']) > 0 and not history['visible'][-1][1] == '':\n        history['visible'].append(['', ''])\n        history['internal'].append(['', ''])\n\n    history['visible'][-1][1] = html.escape(text)\n    history['internal'][-1][1] = apply_extensions('input', text, state, is_chat=True)\n    return history\n\n\ndef redraw_html(history, name1, name2, mode, style, character, reset_cache=False):\n    return chat_html_wrapper(history, name1, name2, mode, style, character, reset_cache=reset_cache)\n\n\ndef start_new_chat(state):\n    mode = state['mode']\n    history = {'internal': [], 'visible': []}\n\n    if mode != 'instruct':\n        greeting = replace_character_names(state['greeting'], state['name1'], state['name2'])\n        if greeting != '':\n            history['internal'] += [['<|BEGIN-VISIBLE-CHAT|>', greeting]]\n            history['visible'] += [['', apply_extensions('output', greeting, state, is_chat=True)]]\n\n    unique_id = datetime.now().strftime('%Y%m%d-%H-%M-%S')\n    save_history(history, unique_id, state['character_menu'], state['mode'])\n\n    return history\n\n\ndef get_history_file_path(unique_id, character, mode):\n    if mode == 'instruct':\n        p = Path(f'logs/instruct/{unique_id}.json')\n    else:\n        p = Path(f'logs/chat/{character}/{unique_id}.json')\n\n    return p\n\n\ndef save_history(history, unique_id, character, mode):\n    if shared.args.multi_user:\n        return\n\n    p = get_history_file_path(unique_id, character, mode)\n    if not p.parent.is_dir():\n        p.parent.mkdir(parents=True)\n\n    with open(p, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(history, indent=4))\n\n\ndef rename_history(old_id, new_id, character, mode):\n    if shared.args.multi_user:\n        return\n\n    old_p = get_history_file_path(old_id, character, mode)\n    new_p = get_history_file_path(new_id, character, mode)\n    if new_p.parent != old_p.parent:\n        logger.error(f\"The following path is not allowed: \\\"{new_p}\\\".\")\n    elif new_p == old_p:\n        logger.info(\"The provided path is identical to the old one.\")\n    else:\n        logger.info(f\"Renaming \\\"{old_p}\\\" to \\\"{new_p}\\\"\")\n        old_p.rename(new_p)\n\n\ndef find_all_histories(state):\n    if shared.args.multi_user:\n        return ['']\n\n    if state['mode'] == 'instruct':\n        paths = Path('logs/instruct').glob('*.json')\n    else:\n        character = state['character_menu']\n\n        # Handle obsolete filenames and paths\n        old_p = Path(f'logs/{character}_persistent.json')\n        new_p = Path(f'logs/persistent_{character}.json')\n        if old_p.exists():\n            logger.warning(f\"Renaming \\\"{old_p}\\\" to \\\"{new_p}\\\"\")\n            old_p.rename(new_p)\n\n        if new_p.exists():\n            unique_id = datetime.now().strftime('%Y%m%d-%H-%M-%S')\n            p = get_history_file_path(unique_id, character, state['mode'])\n            logger.warning(f\"Moving \\\"{new_p}\\\" to \\\"{p}\\\"\")\n            p.parent.mkdir(exist_ok=True)\n            new_p.rename(p)\n\n        paths = Path(f'logs/chat/{character}').glob('*.json')\n\n    histories = sorted(paths, key=lambda x: x.stat().st_mtime, reverse=True)\n    histories = [path.stem for path in histories]\n\n    return histories\n\n\ndef load_latest_history(state):\n    '''\n    Loads the latest history for the given character in chat or chat-instruct\n    mode, or the latest instruct history for instruct mode.\n    '''\n\n    if shared.args.multi_user:\n        return start_new_chat(state)\n\n    histories = find_all_histories(state)\n\n    if len(histories) > 0:\n        history = load_history(histories[0], state['character_menu'], state['mode'])\n    else:\n        history = start_new_chat(state)\n\n    return history\n\n\ndef load_history_after_deletion(state, idx):\n    '''\n    Loads the latest history for the given character in chat or chat-instruct\n    mode, or the latest instruct history for instruct mode.\n    '''\n\n    if shared.args.multi_user:\n        return start_new_chat(state)\n\n    histories = find_all_histories(state)\n    idx = min(int(idx), len(histories) - 1)\n    idx = max(0, idx)\n\n    if len(histories) > 0:\n        history = load_history(histories[idx], state['character_menu'], state['mode'])\n    else:\n        history = start_new_chat(state)\n        histories = find_all_histories(state)\n\n    return history, gr.update(choices=histories, value=histories[idx])\n\n\ndef update_character_menu_after_deletion(idx):\n    characters = utils.get_available_characters()\n    idx = min(int(idx), len(characters) - 1)\n    idx = max(0, idx)\n    return gr.update(choices=characters, value=characters[idx])\n\n\ndef load_history(unique_id, character, mode):\n    p = get_history_file_path(unique_id, character, mode)\n\n    f = json.loads(open(p, 'rb').read())\n    if 'internal' in f and 'visible' in f:\n        history = f\n    else:\n        history = {\n            'internal': f['data'],\n            'visible': f['data_visible']\n        }\n\n    return history\n\n\ndef load_history_json(file, history):\n    try:\n        file = file.decode('utf-8')\n        f = json.loads(file)\n        if 'internal' in f and 'visible' in f:\n            history = f\n        else:\n            history = {\n                'internal': f['data'],\n                'visible': f['data_visible']\n            }\n\n        return history\n    except:\n        return history\n\n\ndef delete_history(unique_id, character, mode):\n    p = get_history_file_path(unique_id, character, mode)\n    delete_file(p)\n\n\ndef replace_character_names(text, name1, name2):\n    text = text.replace('{{user}}', name1).replace('{{char}}', name2)\n    return text.replace('<USER>', name1).replace('<BOT>', name2)\n\n\ndef generate_pfp_cache(character):\n    cache_folder = Path(shared.args.disk_cache_dir)\n    if not cache_folder.exists():\n        cache_folder.mkdir()\n\n    for path in [Path(f\"characters/{character}.{extension}\") for extension in ['png', 'jpg', 'jpeg']]:\n        if path.exists():\n            original_img = Image.open(path)\n            original_img.save(Path(f'{cache_folder}/pfp_character.png'), format='PNG')\n\n            thumb = make_thumbnail(original_img)\n            thumb.save(Path(f'{cache_folder}/pfp_character_thumb.png'), format='PNG')\n\n            return thumb\n\n    return None\n\n\ndef load_character(character, name1, name2):\n    context = greeting = \"\"\n    greeting_field = 'greeting'\n    picture = None\n\n    filepath = None\n    for extension in [\"yml\", \"yaml\", \"json\"]:\n        filepath = Path(f'characters/{character}.{extension}')\n        if filepath.exists():\n            break\n\n    if filepath is None or not filepath.exists():\n        logger.error(f\"Could not find the character \\\"{character}\\\" inside characters/. No character has been loaded.\")\n        raise ValueError\n\n    file_contents = open(filepath, 'r', encoding='utf-8').read()\n    data = json.loads(file_contents) if extension == \"json\" else yaml.safe_load(file_contents)\n    cache_folder = Path(shared.args.disk_cache_dir)\n\n    for path in [Path(f\"{cache_folder}/pfp_character.png\"), Path(f\"{cache_folder}/pfp_character_thumb.png\")]:\n        if path.exists():\n            path.unlink()\n\n    picture = generate_pfp_cache(character)\n\n    # Finding the bot's name\n    for k in ['name', 'bot', '<|bot|>', 'char_name']:\n        if k in data and data[k] != '':\n            name2 = data[k]\n            break\n\n    # Find the user name (if any)\n    for k in ['your_name', 'user', '<|user|>']:\n        if k in data and data[k] != '':\n            name1 = data[k]\n            break\n\n    if 'context' in data:\n        context = data['context'].strip()\n    elif \"char_persona\" in data:\n        context = build_pygmalion_style_context(data)\n        greeting_field = 'char_greeting'\n\n    greeting = data.get(greeting_field, greeting)\n    return name1, name2, picture, greeting, context\n\n\ndef load_instruction_template(template):\n    if template == 'None':\n        return ''\n\n    for filepath in [Path(f'instruction-templates/{template}.yaml'), Path('instruction-templates/Alpaca.yaml')]:\n        if filepath.exists():\n            break\n    else:\n        return ''\n\n    file_contents = open(filepath, 'r', encoding='utf-8').read()\n    data = yaml.safe_load(file_contents)\n    if 'instruction_template' in data:\n        return data['instruction_template']\n    else:\n        return jinja_template_from_old_format(data)\n\n\n@functools.cache\ndef load_character_memoized(character, name1, name2):\n    return load_character(character, name1, name2)\n\n\n@functools.cache\ndef load_instruction_template_memoized(template):\n    return load_instruction_template(template)\n\n\ndef upload_character(file, img, tavern=False):\n    decoded_file = file if isinstance(file, str) else file.decode('utf-8')\n    try:\n        data = json.loads(decoded_file)\n    except:\n        data = yaml.safe_load(decoded_file)\n\n    if 'char_name' in data:\n        name = data['char_name']\n        greeting = data['char_greeting']\n        context = build_pygmalion_style_context(data)\n        yaml_data = generate_character_yaml(name, greeting, context)\n    else:\n        name = data['name']\n        yaml_data = generate_character_yaml(data['name'], data['greeting'], data['context'])\n\n    outfile_name = name\n    i = 1\n    while Path(f'characters/{outfile_name}.yaml').exists():\n        outfile_name = f'{name}_{i:03d}'\n        i += 1\n\n    with open(Path(f'characters/{outfile_name}.yaml'), 'w', encoding='utf-8') as f:\n        f.write(yaml_data)\n\n    if img is not None:\n        img.save(Path(f'characters/{outfile_name}.png'))\n\n    logger.info(f'New character saved to \"characters/{outfile_name}.yaml\".')\n    return gr.update(value=outfile_name, choices=get_available_characters())\n\n\ndef build_pygmalion_style_context(data):\n    context = \"\"\n    if 'char_persona' in data and data['char_persona'] != '':\n        context += f\"{data['char_name']}'s Persona: {data['char_persona']}\\n\"\n\n    if 'world_scenario' in data and data['world_scenario'] != '':\n        context += f\"Scenario: {data['world_scenario']}\\n\"\n\n    if 'example_dialogue' in data and data['example_dialogue'] != '':\n        context += f\"{data['example_dialogue'].strip()}\\n\"\n\n    context = f\"{context.strip()}\\n\"\n    return context\n\n\ndef upload_tavern_character(img, _json):\n    _json = {'char_name': _json['name'], 'char_persona': _json['description'], 'char_greeting': _json['first_mes'], 'example_dialogue': _json['mes_example'], 'world_scenario': _json['scenario']}\n    return upload_character(json.dumps(_json), img, tavern=True)\n\n\ndef check_tavern_character(img):\n    if \"chara\" not in img.info:\n        return \"Not a TavernAI card\", None, None, gr.update(interactive=False)\n\n    decoded_string = base64.b64decode(img.info['chara']).replace(b'\\\\r\\\\n', b'\\\\n')\n    _json = json.loads(decoded_string)\n    if \"data\" in _json:\n        _json = _json[\"data\"]\n\n    return _json['name'], _json['description'], _json, gr.update(interactive=True)\n\n\ndef upload_your_profile_picture(img):\n    cache_folder = Path(shared.args.disk_cache_dir)\n    if not cache_folder.exists():\n        cache_folder.mkdir()\n\n    if img is None:\n        if Path(f\"{cache_folder}/pfp_me.png\").exists():\n            Path(f\"{cache_folder}/pfp_me.png\").unlink()\n    else:\n        img = make_thumbnail(img)\n        img.save(Path(f'{cache_folder}/pfp_me.png'))\n        logger.info(f'Profile picture saved to \"{cache_folder}/pfp_me.png\"')\n\n\ndef generate_character_yaml(name, greeting, context):\n    data = {\n        'name': name,\n        'greeting': greeting,\n        'context': context,\n    }\n\n    data = {k: v for k, v in data.items() if v}  # Strip falsy\n    return yaml.dump(data, sort_keys=False, width=float(\"inf\"))\n\n\ndef generate_instruction_template_yaml(instruction_template):\n    data = {\n        'instruction_template': instruction_template\n    }\n\n    return my_yaml_output(data)\n\n\ndef save_character(name, greeting, context, picture, filename):\n    if filename == \"\":\n        logger.error(\"The filename is empty, so the character will not be saved.\")\n        return\n\n    data = generate_character_yaml(name, greeting, context)\n    filepath = Path(f'characters/{filename}.yaml')\n    save_file(filepath, data)\n    path_to_img = Path(f'characters/{filename}.png')\n    if picture is not None:\n        picture.save(path_to_img)\n        logger.info(f'Saved {path_to_img}.')\n\n\ndef delete_character(name, instruct=False):\n    for extension in [\"yml\", \"yaml\", \"json\"]:\n        delete_file(Path(f'characters/{name}.{extension}'))\n\n    delete_file(Path(f'characters/{name}.png'))\n\n\ndef jinja_template_from_old_format(params, verbose=False):\n    MASTER_TEMPLATE = \"\"\"\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if not ns.found -%}\n    {{- '<|PRE-SYSTEM|>' + '<|SYSTEM-MESSAGE|>' + '<|POST-SYSTEM|>' -}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'system' -%}\n        {{- '<|PRE-SYSTEM|>' + message['content'] + '<|POST-SYSTEM|>' -}}\n    {%- else -%}\n        {%- if message['role'] == 'user' -%}\n            {{-'<|PRE-USER|>' + message['content'] + '<|POST-USER|>'-}}\n        {%- else -%}\n            {{-'<|PRE-ASSISTANT|>' + message['content'] + '<|POST-ASSISTANT|>' -}}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{-'<|PRE-ASSISTANT-GENERATE|>'-}}\n{%- endif -%}\n\"\"\"\n\n    if 'context' in params and '<|system-message|>' in params['context']:\n        pre_system = params['context'].split('<|system-message|>')[0]\n        post_system = params['context'].split('<|system-message|>')[1]\n    else:\n        pre_system = ''\n        post_system = ''\n\n    pre_user = params['turn_template'].split('<|user-message|>')[0].replace('<|user|>', params['user'])\n    post_user = params['turn_template'].split('<|user-message|>')[1].split('<|bot|>')[0]\n\n    pre_assistant = '<|bot|>' + params['turn_template'].split('<|bot-message|>')[0].split('<|bot|>')[1]\n    pre_assistant = pre_assistant.replace('<|bot|>', params['bot'])\n    post_assistant = params['turn_template'].split('<|bot-message|>')[1]\n\n    def preprocess(string):\n        return string.replace('\\n', '\\\\n').replace('\\'', '\\\\\\'')\n\n    pre_system = preprocess(pre_system)\n    post_system = preprocess(post_system)\n    pre_user = preprocess(pre_user)\n    post_user = preprocess(post_user)\n    pre_assistant = preprocess(pre_assistant)\n    post_assistant = preprocess(post_assistant)\n\n    if verbose:\n        print(\n            '\\n',\n            repr(pre_system) + '\\n',\n            repr(post_system) + '\\n',\n            repr(pre_user) + '\\n',\n            repr(post_user) + '\\n',\n            repr(pre_assistant) + '\\n',\n            repr(post_assistant) + '\\n',\n        )\n\n    result = MASTER_TEMPLATE\n    if 'system_message' in params:\n        result = result.replace('<|SYSTEM-MESSAGE|>', preprocess(params['system_message']))\n    else:\n        result = result.replace('<|SYSTEM-MESSAGE|>', '')\n\n    result = result.replace('<|PRE-SYSTEM|>', pre_system)\n    result = result.replace('<|POST-SYSTEM|>', post_system)\n    result = result.replace('<|PRE-USER|>', pre_user)\n    result = result.replace('<|POST-USER|>', post_user)\n    result = result.replace('<|PRE-ASSISTANT|>', pre_assistant)\n    result = result.replace('<|PRE-ASSISTANT-GENERATE|>', pre_assistant.rstrip(' '))\n    result = result.replace('<|POST-ASSISTANT|>', post_assistant)\n\n    result = result.strip()\n\n    return result\n\n\ndef my_yaml_output(data):\n    '''\n    pyyaml is very inconsistent with multiline strings.\n    for simple instruction template outputs, this is enough.\n    '''\n    result = \"\"\n    for k in data:\n        result += k + \": |-\\n\"\n        for line in data[k].splitlines():\n            result += \"  \" + line.rstrip(' ') + \"\\n\"\n\n    return result\n", "modules/RoPE.py": "def get_alpha_value(alpha, base):\n    '''\n    Gets alpha_value from alpha_value and rope_freq_base\n    '''\n    if base > 0:\n        return (base / 10000.) ** (63 / 64.)\n    else:\n        return alpha\n\n\ndef get_rope_freq_base(alpha, base):\n    '''\n    Gets rope_freq_base from alpha_value and rope_freq_base\n    '''\n    if base > 0:\n        return base\n    else:\n        return 10000 * alpha ** (64 / 63.)\n", "modules/html_generator.py": "import functools\nimport html\nimport os\nimport re\nimport time\nfrom pathlib import Path\n\nimport markdown\nfrom PIL import Image, ImageOps\n\nfrom modules import shared\nfrom modules.utils import get_available_chat_styles\n\n# This is to store the paths to the thumbnails of the profile pictures\nimage_cache = {}\n\nwith open(Path(__file__).resolve().parent / '../css/html_readable_style.css', 'r') as f:\n    readable_css = f.read()\nwith open(Path(__file__).resolve().parent / '../css/html_instruct_style.css', 'r') as f:\n    instruct_css = f.read()\n\n# Custom chat styles\nchat_styles = {}\nfor k in get_available_chat_styles():\n    chat_styles[k] = open(Path(f'css/chat_style-{k}.css'), 'r').read()\n\n# Handle styles that derive from other styles\nfor k in chat_styles:\n    lines = chat_styles[k].split('\\n')\n    input_string = lines[0]\n    match = re.search(r'chat_style-([a-z\\-]*)\\.css', input_string)\n\n    if match:\n        style = match.group(1)\n        chat_styles[k] = chat_styles.get(style, '') + '\\n\\n' + '\\n'.join(lines[1:])\n\n\ndef fix_newlines(string):\n    string = string.replace('\\n', '\\n\\n')\n    string = re.sub(r\"\\n{3,}\", \"\\n\\n\", string)\n    string = string.strip()\n    return string\n\n\ndef replace_blockquote(m):\n    return m.group().replace('\\n', '\\n> ').replace('\\\\begin{blockquote}', '').replace('\\\\end{blockquote}', '')\n\n\n@functools.lru_cache(maxsize=4096)\ndef convert_to_markdown(string):\n\n    # Blockquote\n    string = re.sub(r'(^|[\\n])&gt;', r'\\1>', string)\n    pattern = re.compile(r'\\\\begin{blockquote}(.*?)\\\\end{blockquote}', re.DOTALL)\n    string = pattern.sub(replace_blockquote, string)\n\n    # Code\n    string = string.replace('\\\\begin{code}', '```')\n    string = string.replace('\\\\end{code}', '```')\n    string = string.replace('\\\\begin{align*}', '$$')\n    string = string.replace('\\\\end{align*}', '$$')\n    string = string.replace('\\\\begin{align}', '$$')\n    string = string.replace('\\\\end{align}', '$$')\n    string = string.replace('\\\\begin{equation}', '$$')\n    string = string.replace('\\\\end{equation}', '$$')\n    string = string.replace('\\\\begin{equation*}', '$$')\n    string = string.replace('\\\\end{equation*}', '$$')\n    string = re.sub(r\"(.)```\", r\"\\1\\n```\", string)\n\n    result = ''\n    is_code = False\n    for line in string.split('\\n'):\n        if line.lstrip(' ').startswith('```'):\n            is_code = not is_code\n\n        result += line\n        if is_code or line.startswith('|'):  # Don't add an extra \\n for tables or code\n            result += '\\n'\n        else:\n            result += '\\n\\n'\n\n    result = result.strip()\n    if is_code:\n        result += '\\n```'  # Unfinished code block\n\n    # Unfinished list, like \"\\n1.\". A |delete| string is added and then\n    # removed to force a <ol> or <ul> to be generated instead of a <p>.\n    if re.search(r'(\\n\\d+\\.?|\\n\\*\\s*)$', result):\n        delete_str = '|delete|'\n\n        if re.search(r'(\\d+\\.?)$', result) and not result.endswith('.'):\n            result += '.'\n\n        result = re.sub(r'(\\n\\d+\\.?|\\n\\*\\s*)$', r'\\g<1> ' + delete_str, result)\n\n        html_output = markdown.markdown(result, extensions=['fenced_code', 'tables'])\n        pos = html_output.rfind(delete_str)\n        if pos > -1:\n            html_output = html_output[:pos] + html_output[pos + len(delete_str):]\n    else:\n        html_output = markdown.markdown(result, extensions=['fenced_code', 'tables'])\n\n    # Unescape code blocks\n    pattern = re.compile(r'<code[^>]*>(.*?)</code>', re.DOTALL)\n    html_output = pattern.sub(lambda x: html.unescape(x.group()), html_output)\n\n    return html_output\n\n\ndef convert_to_markdown_wrapped(string, use_cache=True):\n    '''\n    Used to avoid caching convert_to_markdown calls during streaming.\n    '''\n\n    if use_cache:\n        return convert_to_markdown(string)\n\n    return convert_to_markdown.__wrapped__(string)\n\n\ndef generate_basic_html(string):\n    string = convert_to_markdown(string)\n    string = f'<style>{readable_css}</style><div class=\"readable-container\">{string}</div>'\n    return string\n\n\ndef make_thumbnail(image):\n    image = image.resize((350, round(image.size[1] / image.size[0] * 350)), Image.Resampling.LANCZOS)\n    if image.size[1] > 470:\n        image = ImageOps.fit(image, (350, 470), Image.LANCZOS)\n\n    return image\n\n\ndef get_image_cache(path):\n    cache_folder = Path(shared.args.disk_cache_dir)\n    if not cache_folder.exists():\n        cache_folder.mkdir()\n\n    mtime = os.stat(path).st_mtime\n    if (path in image_cache and mtime != image_cache[path][0]) or (path not in image_cache):\n        img = make_thumbnail(Image.open(path))\n\n        old_p = Path(f'{cache_folder}/{path.name}_cache.png')\n        p = Path(f'{cache_folder}/cache_{path.name}.png')\n        if old_p.exists():\n            old_p.rename(p)\n\n        output_file = p\n        img.convert('RGBA').save(output_file, format='PNG')\n        image_cache[path] = [mtime, output_file.as_posix()]\n\n    return image_cache[path][1]\n\n\ndef generate_instruct_html(history):\n    output = f'<style>{instruct_css}</style><div class=\"chat\" id=\"chat\"><div class=\"messages\">'\n    for i, _row in enumerate(history):\n        row = [convert_to_markdown_wrapped(entry, use_cache=i != len(history) - 1) for entry in _row]\n\n        if row[0]:  # don't display empty user messages\n            output += f\"\"\"\n                  <div class=\"user-message\">\n                    <div class=\"text\">\n                      <div class=\"message-body\">\n                        {row[0]}\n                      </div>\n                    </div>\n                  </div>\n                \"\"\"\n\n        output += f\"\"\"\n              <div class=\"assistant-message\">\n                <div class=\"text\">\n                  <div class=\"message-body\">\n                    {row[1]}\n                  </div>\n                </div>\n              </div>\n            \"\"\"\n\n    output += \"</div></div>\"\n\n    return output\n\n\ndef generate_cai_chat_html(history, name1, name2, style, character, reset_cache=False):\n    output = f'<style>{chat_styles[style]}</style><div class=\"chat\" id=\"chat\"><div class=\"messages\">'\n\n    # We use ?character and ?time.time() to force the browser to reset caches\n    img_bot = f'<img src=\"file/cache/pfp_character_thumb.png?{character}\" class=\"pfp_character\">' if Path(\"cache/pfp_character_thumb.png\").exists() else ''\n    img_me = f'<img src=\"file/cache/pfp_me.png?{time.time() if reset_cache else \"\"}\">' if Path(\"cache/pfp_me.png\").exists() else ''\n\n    for i, _row in enumerate(history):\n        row = [convert_to_markdown_wrapped(entry, use_cache=i != len(history) - 1) for entry in _row]\n\n        if row[0]:  # don't display empty user messages\n            output += f\"\"\"\n                  <div class=\"message\">\n                    <div class=\"circle-you\">\n                      {img_me}\n                    </div>\n                    <div class=\"text\">\n                      <div class=\"username\">\n                        {name1}\n                      </div>\n                      <div class=\"message-body\">\n                        {row[0]}\n                      </div>\n                    </div>\n                  </div>\n                \"\"\"\n\n        output += f\"\"\"\n              <div class=\"message\">\n                <div class=\"circle-bot\">\n                  {img_bot}\n                </div>\n                <div class=\"text\">\n                  <div class=\"username\">\n                    {name2}\n                  </div>\n                  <div class=\"message-body\">\n                    {row[1]}\n                  </div>\n                </div>\n              </div>\n            \"\"\"\n\n    output += \"</div></div>\"\n    return output\n\n\ndef generate_chat_html(history, name1, name2, reset_cache=False):\n    output = f'<style>{chat_styles[\"wpp\"]}</style><div class=\"chat\" id=\"chat\"><div class=\"messages\">'\n\n    for i, _row in enumerate(history):\n        row = [convert_to_markdown_wrapped(entry, use_cache=i != len(history) - 1) for entry in _row]\n\n        if row[0]:  # don't display empty user messages\n            output += f\"\"\"\n              <div class=\"message\">\n                <div class=\"text-you\">\n                  <div class=\"message-body\">\n                    {row[0]}\n                  </div>\n                </div>\n              </div>\n            \"\"\"\n\n        output += f\"\"\"\n          <div class=\"message\">\n            <div class=\"text-bot\">\n              <div class=\"message-body\">\n                {row[1]}\n              </div>\n            </div>\n          </div>\n        \"\"\"\n\n    output += \"</div></div>\"\n    return output\n\n\ndef chat_html_wrapper(history, name1, name2, mode, style, character, reset_cache=False):\n    if mode == 'instruct':\n        return generate_instruct_html(history['visible'])\n    elif style == 'wpp':\n        return generate_chat_html(history['visible'], name1, name2)\n    else:\n        return generate_cai_chat_html(history['visible'], name1, name2, style, character, reset_cache)\n", "modules/AutoGPTQ_loader.py": "from pathlib import Path\n\nfrom accelerate.utils import is_xpu_available\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nimport modules.shared as shared\nfrom modules.logging_colors import logger\nfrom modules.models import get_max_memory_dict\n\n\ndef load_quantized(model_name):\n    path_to_model = Path(f'{shared.args.model_dir}/{model_name}')\n    pt_path = None\n\n    # Find the model checkpoint\n    if shared.args.checkpoint:\n        pt_path = Path(shared.args.checkpoint)\n    else:\n        for ext in ['.safetensors', '.pt', '.bin']:\n            found = list(path_to_model.glob(f\"*{ext}\"))\n            if len(found) > 0:\n                if len(found) > 1:\n                    logger.warning(f'More than one {ext} model has been found. The last one will be selected. It could be wrong.')\n\n                pt_path = found[-1]\n                break\n\n    if pt_path is None:\n        logger.error(\"The model could not be loaded because its checkpoint file in .bin/.pt/.safetensors format could not be located.\")\n        return\n\n    use_safetensors = pt_path.suffix == '.safetensors'\n    if not (path_to_model / \"quantize_config.json\").exists():\n        quantize_config = BaseQuantizeConfig(\n            bits=bits if (bits := shared.args.wbits) > 0 else 4,\n            group_size=gs if (gs := shared.args.groupsize) > 0 else -1,\n            desc_act=shared.args.desc_act\n        )\n    else:\n        quantize_config = None\n\n    # Define the params for AutoGPTQForCausalLM.from_quantized\n    params = {\n        'model_basename': pt_path.stem,\n        'device': \"xpu:0\" if is_xpu_available() else \"cuda:0\" if not shared.args.cpu else \"cpu\",\n        'use_triton': shared.args.triton,\n        'inject_fused_attention': False,\n        'inject_fused_mlp': not shared.args.no_inject_fused_mlp,\n        'use_safetensors': use_safetensors,\n        'trust_remote_code': shared.args.trust_remote_code,\n        'max_memory': get_max_memory_dict(),\n        'quantize_config': quantize_config,\n        'use_cuda_fp16': not shared.args.no_use_cuda_fp16,\n        'disable_exllama': shared.args.disable_exllama,\n        'disable_exllamav2': shared.args.disable_exllamav2,\n    }\n\n    logger.info(f\"The AutoGPTQ params are: {params}\")\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params)\n\n    # These lines fix the multimodal extension when used with AutoGPTQ\n    if hasattr(model, 'model'):\n        if not hasattr(model, 'dtype'):\n            if hasattr(model.model, 'dtype'):\n                model.dtype = model.model.dtype\n\n        if hasattr(model.model, 'model') and hasattr(model.model.model, 'embed_tokens'):\n            if not hasattr(model, 'embed_tokens'):\n                model.embed_tokens = model.model.model.embed_tokens\n\n            if not hasattr(model.model, 'embed_tokens'):\n                model.model.embed_tokens = model.model.model.embed_tokens\n\n    return model\n", "modules/logging_colors.py": "import logging\n\nlogger = logging.getLogger('text-generation-webui')\n\n\ndef setup_logging():\n    '''\n    Copied from: https://github.com/vladmandic/automatic\n\n    All credits to vladmandic.\n    '''\n\n    class RingBuffer(logging.StreamHandler):\n        def __init__(self, capacity):\n            super().__init__()\n            self.capacity = capacity\n            self.buffer = []\n            self.formatter = logging.Formatter('{ \"asctime\":\"%(asctime)s\", \"created\":%(created)f, \"facility\":\"%(name)s\", \"pid\":%(process)d, \"tid\":%(thread)d, \"level\":\"%(levelname)s\", \"module\":\"%(module)s\", \"func\":\"%(funcName)s\", \"msg\":\"%(message)s\" }')\n\n        def emit(self, record):\n            msg = self.format(record)\n            # self.buffer.append(json.loads(msg))\n            self.buffer.append(msg)\n            if len(self.buffer) > self.capacity:\n                self.buffer.pop(0)\n\n        def get(self):\n            return self.buffer\n\n    from rich.console import Console\n    from rich.logging import RichHandler\n    from rich.pretty import install as pretty_install\n    from rich.theme import Theme\n    from rich.traceback import install as traceback_install\n\n    level = logging.DEBUG\n    logger.setLevel(logging.DEBUG)  # log to file is always at level debug for facility `sd`\n    console = Console(log_time=True, log_time_format='%H:%M:%S-%f', theme=Theme({\n        \"traceback.border\": \"black\",\n        \"traceback.border.syntax_error\": \"black\",\n        \"inspect.value.border\": \"black\",\n    }))\n    logging.basicConfig(level=logging.ERROR, format='%(asctime)s | %(name)s | %(levelname)s | %(module)s | %(message)s', handlers=[logging.NullHandler()])  # redirect default logger to null\n    pretty_install(console=console)\n    traceback_install(console=console, extra_lines=1, max_frames=10, width=console.width, word_wrap=False, indent_guides=False, suppress=[])\n    while logger.hasHandlers() and len(logger.handlers) > 0:\n        logger.removeHandler(logger.handlers[0])\n\n    # handlers\n    rh = RichHandler(show_time=True, omit_repeated_times=False, show_level=True, show_path=False, markup=False, rich_tracebacks=True, log_time_format='%H:%M:%S-%f', level=level, console=console)\n    rh.setLevel(level)\n    logger.addHandler(rh)\n\n    rb = RingBuffer(100)  # 100 entries default in log ring buffer\n    rb.setLevel(level)\n    logger.addHandler(rb)\n    logger.buffer = rb.buffer\n\n    # overrides\n    logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n    logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n    logging.getLogger(\"diffusers\").setLevel(logging.ERROR)\n    logging.getLogger(\"torch\").setLevel(logging.ERROR)\n    logging.getLogger(\"lycoris\").handlers = logger.handlers\n\n\nsetup_logging()\n", "modules/ui_default.py": "import gradio as gr\n\nfrom modules import logits, shared, ui, utils\nfrom modules.prompts import count_tokens, load_prompt\nfrom modules.text_generation import (\n    generate_reply_wrapper,\n    get_token_ids,\n    stop_everything_event\n)\nfrom modules.utils import gradio\n\ninputs = ('textbox-default', 'interface_state')\noutputs = ('output_textbox', 'html-default')\n\n\ndef create_ui():\n    mu = shared.args.multi_user\n    with gr.Tab('Default', elem_id='default-tab'):\n        shared.gradio['last_input-default'] = gr.State('')\n        with gr.Row():\n            with gr.Column():\n                with gr.Row():\n                    shared.gradio['textbox-default'] = gr.Textbox(value='', lines=27, label='Input', elem_classes=['textbox_default', 'add_scrollbar'])\n                    shared.gradio['token-counter-default'] = gr.HTML(value=\"<span>0</span>\", elem_classes=[\"token-counter\", \"default-token-counter\"])\n\n                with gr.Row():\n                    shared.gradio['Generate-default'] = gr.Button('Generate', variant='primary')\n                    shared.gradio['Stop-default'] = gr.Button('Stop', elem_id='stop')\n                    shared.gradio['Continue-default'] = gr.Button('Continue')\n\n                with gr.Row():\n                    shared.gradio['prompt_menu-default'] = gr.Dropdown(choices=utils.get_available_prompts(), value='None', label='Prompt', elem_classes='slim-dropdown')\n                    ui.create_refresh_button(shared.gradio['prompt_menu-default'], lambda: None, lambda: {'choices': utils.get_available_prompts()}, 'refresh-button', interactive=not mu)\n                    shared.gradio['save_prompt-default'] = gr.Button('\ud83d\udcbe', elem_classes='refresh-button', interactive=not mu)\n                    shared.gradio['delete_prompt-default'] = gr.Button('\ud83d\uddd1\ufe0f', elem_classes='refresh-button', interactive=not mu)\n\n            with gr.Column():\n                with gr.Tab('Raw'):\n                    shared.gradio['output_textbox'] = gr.Textbox(lines=27, label='Output', elem_id='textbox-default', elem_classes=['textbox_default_output', 'add_scrollbar'])\n\n                with gr.Tab('Markdown'):\n                    shared.gradio['markdown_render-default'] = gr.Button('Render')\n                    shared.gradio['markdown-default'] = gr.Markdown()\n\n                with gr.Tab('HTML'):\n                    shared.gradio['html-default'] = gr.HTML()\n\n                with gr.Tab('Logits'):\n                    with gr.Row():\n                        with gr.Column(scale=10):\n                            shared.gradio['get_logits-default'] = gr.Button('Get next token probabilities')\n                        with gr.Column(scale=1):\n                            shared.gradio['use_samplers-default'] = gr.Checkbox(label='Use samplers', value=True, elem_classes=['no-background'])\n\n                    with gr.Row():\n                        shared.gradio['logits-default'] = gr.Textbox(lines=23, label='Output', elem_classes=['textbox_logits', 'add_scrollbar'])\n                        shared.gradio['logits-default-previous'] = gr.Textbox(lines=23, label='Previous output', elem_classes=['textbox_logits', 'add_scrollbar'])\n\n                with gr.Tab('Tokens'):\n                    shared.gradio['get_tokens-default'] = gr.Button('Get token IDs for the input')\n                    shared.gradio['tokens-default'] = gr.Textbox(lines=23, label='Tokens', elem_classes=['textbox_logits', 'add_scrollbar', 'monospace'])\n\n\ndef create_event_handlers():\n    shared.gradio['Generate-default'].click(\n        lambda x: x, gradio('textbox-default'), gradio('last_input-default')).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        generate_reply_wrapper, gradio(inputs), gradio(outputs), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['textbox-default'].submit(\n        lambda x: x, gradio('textbox-default'), gradio('last_input-default')).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        generate_reply_wrapper, gradio(inputs), gradio(outputs), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['markdown_render-default'].click(lambda x: x, gradio('output_textbox'), gradio('markdown-default'), queue=False)\n    shared.gradio['Continue-default'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        generate_reply_wrapper, [shared.gradio['output_textbox']] + gradio(inputs)[1:], gradio(outputs), show_progress=False).then(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        lambda: None, None, None, js=f'() => {{{ui.audio_notification_js}}}')\n\n    shared.gradio['Stop-default'].click(stop_everything_event, None, None, queue=False)\n    shared.gradio['prompt_menu-default'].change(load_prompt, gradio('prompt_menu-default'), gradio('textbox-default'), show_progress=False)\n    shared.gradio['save_prompt-default'].click(\n        lambda x: x, gradio('textbox-default'), gradio('save_contents')).then(\n        lambda: 'prompts/', None, gradio('save_root')).then(\n        lambda: utils.current_time() + '.txt', None, gradio('save_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('file_saver'))\n\n    shared.gradio['delete_prompt-default'].click(\n        lambda: 'prompts/', None, gradio('delete_root')).then(\n        lambda x: x + '.txt', gradio('prompt_menu-default'), gradio('delete_filename')).then(\n        lambda: gr.update(visible=True), None, gradio('file_deleter'))\n\n    shared.gradio['textbox-default'].change(lambda x: f\"<span>{count_tokens(x)}</span>\", gradio('textbox-default'), gradio('token-counter-default'), show_progress=False)\n    shared.gradio['get_logits-default'].click(\n        ui.gather_interface_values, gradio(shared.input_elements), gradio('interface_state')).then(\n        logits.get_next_logits, gradio('textbox-default', 'interface_state', 'use_samplers-default', 'logits-default'), gradio('logits-default', 'logits-default-previous'), show_progress=False)\n\n    shared.gradio['get_tokens-default'].click(get_token_ids, gradio('textbox-default'), gradio('tokens-default'), show_progress=False)\n", "modules/block_requests.py": "import builtins\nimport io\n\nimport requests\n\nfrom modules.logging_colors import logger\n\noriginal_open = open\noriginal_get = requests.get\noriginal_print = print\n\n\nclass RequestBlocker:\n\n    def __enter__(self):\n        requests.get = my_get\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        requests.get = original_get\n\n\nclass OpenMonkeyPatch:\n\n    def __enter__(self):\n        builtins.open = my_open\n        builtins.print = my_print\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        builtins.open = original_open\n        builtins.print = original_print\n\n\ndef my_get(url, **kwargs):\n    logger.info('Unwanted HTTP request redirected to localhost :)')\n    kwargs.setdefault('allow_redirects', True)\n    return requests.api.request('get', 'http://127.0.0.1/', **kwargs)\n\n\n# Kindly provided by our friend WizardLM-30B\ndef my_open(*args, **kwargs):\n    filename = str(args[0])\n    if filename.endswith('index.html'):\n        with original_open(*args, **kwargs) as f:\n            file_contents = f.read()\n\n        file_contents = file_contents.replace(b'\\t\\t<script\\n\\t\\t\\tsrc=\"https://cdnjs.cloudflare.com/ajax/libs/iframe-resizer/4.3.9/iframeResizer.contentWindow.min.js\"\\n\\t\\t\\tasync\\n\\t\\t></script>', b'')\n        file_contents = file_contents.replace(b'cdnjs.cloudflare.com', b'127.0.0.1')\n        file_contents = file_contents.replace(\n            b'</head>',\n            b'\\n    <script src=\"file/js/katex/katex.min.js\"></script>'\n            b'\\n    <script src=\"file/js/katex/auto-render.min.js\"></script>'\n            b'\\n    <script src=\"file/js/highlightjs/highlight.min.js\"></script>'\n            b'\\n    <script src=\"file/js/highlightjs/highlightjs-copy.min.js\"></script>'\n            b'\\n    <script>hljs.addPlugin(new CopyButtonPlugin());</script>'\n            b'\\n  </head>'\n        )\n\n        return io.BytesIO(file_contents)\n    else:\n        return original_open(*args, **kwargs)\n\n\ndef my_print(*args, **kwargs):\n    if len(args) > 0 and 'To create a public link, set `share=True`' in args[0]:\n        return\n    else:\n        if len(args) > 0 and 'Running on local URL' in args[0]:\n            args = list(args)\n            args[0] = f\"\\n{args[0].strip()}\\n\"\n            args = tuple(args)\n\n        original_print(*args, **kwargs)\n", "modules/exllamav2_hf.py": "import os\nimport traceback\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport torch\nfrom exllamav2 import (\n    ExLlamaV2,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Config\n)\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom modules import shared\nfrom modules.logging_colors import logger\n\ntry:\n    import flash_attn\nexcept ModuleNotFoundError:\n    logger.warning(\n        'You are running ExLlamaV2 without flash-attention. This will cause the VRAM usage '\n        'to be a lot higher than it could be.\\n'\n        'Try installing flash-attention following the instructions here: '\n        'https://github.com/Dao-AILab/flash-attention#installation-and-features'\n    )\n    pass\nexcept Exception:\n    logger.warning('Failed to load flash-attention due to the following error:\\n')\n    traceback.print_exc()\n\n\nclass Exllamav2HF(PreTrainedModel):\n    def __init__(self, config: ExLlamaV2Config):\n        super().__init__(PretrainedConfig())\n        self.ex_config = config\n        self.loras = None\n        self.generation_config = GenerationConfig()\n\n        self.ex_model = ExLlamaV2(config)\n\n        if not shared.args.autosplit:\n            split = None\n            if shared.args.gpu_split:\n                split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n\n            self.ex_model.load(split)\n\n        if shared.args.cache_8bit:\n            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model, lazy=shared.args.autosplit)\n        elif shared.args.cache_4bit:\n            self.ex_cache = ExLlamaV2Cache_Q4(self.ex_model, lazy=shared.args.autosplit)\n        else:\n            self.ex_cache = ExLlamaV2Cache(self.ex_model, lazy=shared.args.autosplit)\n\n        if shared.args.autosplit:\n            self.ex_model.load_autosplit(self.ex_cache)\n\n        self.past_seq = None\n        if shared.args.cfg_cache:\n            if shared.args.cache_8bit:\n                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n            elif shared.args.cache_4bit:\n                self.ex_cache_negative = ExLlamaV2Cache_Q4(self.ex_model)\n            else:\n                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n\n            self.past_seq_negative = None\n\n    def _validate_model_class(self):\n        pass\n\n    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n        pass\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {'input_ids': input_ids, **kwargs}\n\n    @property\n    def device(self) -> torch.device:\n        return torch.device(0)\n\n    def __call__(self, *args, **kwargs):\n        use_cache = kwargs.get('use_cache', True)\n        labels = kwargs.get('labels', None)\n        past_key_values = kwargs.get('past_key_values', None)\n\n        if len(args) > 0:\n            if not shared.args.cfg_cache:\n                logger.error(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n                return\n\n            input_ids = args[0]\n            is_negative = True\n            past_seq = self.past_seq_negative\n            ex_cache = self.ex_cache_negative\n        else:\n            input_ids = kwargs['input_ids']\n            is_negative = False\n            past_seq = self.past_seq\n            ex_cache = self.ex_cache\n\n        seq = input_ids[0].tolist()\n        if is_negative and past_key_values is not None:\n            seq = past_key_values + seq\n\n        seq_tensor = torch.tensor(seq)\n        reset = True\n\n        # Make the forward call\n        if labels is None:\n            if past_seq is not None:\n                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n                if len(indices) > 0:\n                    longest_prefix = indices[0].item()\n                else:\n                    longest_prefix = min_length\n\n                if longest_prefix > 0:\n                    reset = False\n                    ex_cache.current_seq_len = longest_prefix\n                    if len(seq_tensor) - longest_prefix > 1:\n                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n                    elif len(seq_tensor) == longest_prefix:\n                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n                        ex_cache.current_seq_len -= 1\n\n            if reset:\n                ex_cache.current_seq_len = 0\n                if len(seq_tensor) > 1:\n                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n\n            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n        else:\n            ex_cache.current_seq_len = 0\n            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n\n        if is_negative:\n            self.past_seq_negative = seq_tensor\n        else:\n            self.past_seq = seq_tensor\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, logits.shape[-1])\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n\n        pretrained_model_name_or_path = Path(f'{shared.args.model_dir}') / Path(pretrained_model_name_or_path)\n\n        config = ExLlamaV2Config()\n        config.model_dir = str(pretrained_model_name_or_path)\n        config.prepare()\n\n        config.max_seq_len = shared.args.max_seq_len\n        config.scale_pos_emb = shared.args.compress_pos_emb\n        config.scale_alpha_value = shared.args.alpha_value\n        config.no_flash_attn = shared.args.no_flash_attn\n        config.num_experts_per_token = int(shared.args.num_experts_per_token)\n\n        return Exllamav2HF(config)\n", "modules/llamacpp_model.py": "import re\nfrom functools import partial\n\nimport numpy as np\nimport torch\n\nfrom modules import RoPE, llama_cpp_python_hijack, shared\nfrom modules.callbacks import Iteratorize\nfrom modules.logging_colors import logger\nfrom modules.text_generation import get_max_prompt_length\n\ntry:\n    import llama_cpp\nexcept:\n    llama_cpp = None\n\ntry:\n    import llama_cpp_cuda\nexcept:\n    llama_cpp_cuda = None\n\ntry:\n    import llama_cpp_cuda_tensorcores\nexcept:\n    llama_cpp_cuda_tensorcores = None\n\n\ndef llama_cpp_lib():\n    if shared.args.cpu and llama_cpp is not None:\n        return llama_cpp\n    elif shared.args.tensorcores and llama_cpp_cuda_tensorcores is not None:\n        return llama_cpp_cuda_tensorcores\n    elif llama_cpp_cuda is not None:\n        return llama_cpp_cuda\n    else:\n        return llama_cpp\n\n\ndef ban_eos_logits_processor(eos_token, input_ids, logits):\n    logits[eos_token] = -float('inf')\n    return logits\n\n\ndef custom_token_ban_logits_processor(token_ids, input_ids, logits):\n    for token_id in token_ids:\n        logits[token_id] = -float('inf')\n\n    return logits\n\n\nclass LlamaCppModel:\n    def __init__(self):\n        self.initialized = False\n        self.grammar_string = ''\n        self.grammar = None\n\n    def __del__(self):\n        del self.model\n\n    @classmethod\n    def from_pretrained(self, path):\n\n        Llama = llama_cpp_lib().Llama\n        LlamaCache = llama_cpp_lib().LlamaCache\n\n        result = self()\n        cache_capacity = 0\n        if shared.args.cache_capacity is not None:\n            if 'GiB' in shared.args.cache_capacity:\n                cache_capacity = int(re.sub('[a-zA-Z]', '', shared.args.cache_capacity)) * 1000 * 1000 * 1000\n            elif 'MiB' in shared.args.cache_capacity:\n                cache_capacity = int(re.sub('[a-zA-Z]', '', shared.args.cache_capacity)) * 1000 * 1000\n            else:\n                cache_capacity = int(shared.args.cache_capacity)\n\n        if cache_capacity > 0:\n            logger.info(\"Cache capacity is \" + str(cache_capacity) + \" bytes\")\n\n        if shared.args.tensor_split is None or shared.args.tensor_split.strip() == '':\n            tensor_split_list = None\n        else:\n            tensor_split_list = [float(x) for x in shared.args.tensor_split.strip().split(\",\")]\n\n        params = {\n            'model_path': str(path),\n            'n_ctx': shared.args.n_ctx,\n            'n_threads': shared.args.threads or None,\n            'n_threads_batch': shared.args.threads_batch or None,\n            'n_batch': shared.args.n_batch,\n            'use_mmap': not shared.args.no_mmap,\n            'use_mlock': shared.args.mlock,\n            'mul_mat_q': not shared.args.no_mul_mat_q,\n            'numa': shared.args.numa,\n            'n_gpu_layers': shared.args.n_gpu_layers,\n            'rope_freq_base': RoPE.get_rope_freq_base(shared.args.alpha_value, shared.args.rope_freq_base),\n            'tensor_split': tensor_split_list,\n            'rope_freq_scale': 1.0 / shared.args.compress_pos_emb,\n            'offload_kqv': not shared.args.no_offload_kqv,\n            'split_mode': 1 if not shared.args.row_split else 2,\n            'flash_attn': shared.args.flash_attn\n        }\n\n        result.model = Llama(**params)\n        if cache_capacity > 0:\n            result.model.set_cache(LlamaCache(capacity_bytes=cache_capacity))\n\n        # This is ugly, but the model and the tokenizer are the same object in this library.\n        return result, result\n\n    def encode(self, string):\n        if type(string) is str:\n            string = string.encode()\n\n        return self.model.tokenize(string)\n\n    def decode(self, ids, **kwargs):\n        return self.model.detokenize(ids).decode('utf-8')\n\n    def get_logits(self, tokens):\n        self.model.reset()\n        self.model.eval(tokens)\n        logits = self.model._scores\n        logits = np.expand_dims(logits, 0)  # batch dim is expected\n        return torch.tensor(logits, dtype=torch.float32)\n\n    def load_grammar(self, string):\n        if string != self.grammar_string:\n            self.grammar_string = string\n            if string.strip() != '':\n                self.grammar = llama_cpp_lib().LlamaGrammar.from_string(string)\n            else:\n                self.grammar = None\n\n    def generate(self, prompt, state, callback=None):\n        LogitsProcessorList = llama_cpp_lib().LogitsProcessorList\n        prompt = prompt if type(prompt) is str else prompt.decode()\n\n        # Handle truncation\n        prompt = self.encode(prompt)\n        prompt = prompt[-get_max_prompt_length(state):]\n        prompt = self.decode(prompt)\n\n        self.load_grammar(state['grammar_string'])\n        logit_processors = LogitsProcessorList()\n        if state['ban_eos_token']:\n            logit_processors.append(partial(ban_eos_logits_processor, self.model.token_eos()))\n\n        if state['custom_token_bans']:\n            to_ban = [int(x) for x in state['custom_token_bans'].split(',')]\n            if len(to_ban) > 0:\n                logit_processors.append(partial(custom_token_ban_logits_processor, to_ban))\n\n        completion_chunks = self.model.create_completion(\n            prompt=prompt,\n            max_tokens=state['max_new_tokens'],\n            temperature=state['temperature'],\n            top_p=state['top_p'],\n            min_p=state['min_p'],\n            typical_p=state['typical_p'],\n            frequency_penalty=state['frequency_penalty'],\n            presence_penalty=state['presence_penalty'],\n            repeat_penalty=state['repetition_penalty'],\n            top_k=state['top_k'],\n            stream=True,\n            seed=int(state['seed']) if state['seed'] != -1 else None,\n            tfs_z=state['tfs'],\n            mirostat_mode=int(state['mirostat_mode']),\n            mirostat_tau=state['mirostat_tau'],\n            mirostat_eta=state['mirostat_eta'],\n            logits_processor=logit_processors,\n            grammar=self.grammar\n        )\n\n        output = \"\"\n        for completion_chunk in completion_chunks:\n            if shared.stop_everything:\n                break\n\n            text = completion_chunk['choices'][0]['text']\n            output += text\n            if callback:\n                callback(text)\n\n        return output\n\n    def generate_with_streaming(self, *args, **kwargs):\n        with Iteratorize(self.generate, args, kwargs, callback=None) as generator:\n            reply = ''\n            for token in generator:\n                reply += token\n                yield reply\n", "modules/sampler_hijack.py": "import json\nimport math\nimport pprint\n\nimport torch\nimport transformers\nfrom transformers import LogitsWarper, is_torch_xpu_available\nfrom transformers.generation.logits_process import (\n    LogitNormalization,\n    LogitsProcessor,\n    LogitsProcessorList\n)\n\nfrom modules import shared\nfrom modules.logging_colors import logger\n\nglobal_scores = None\n\n\nclass TemperatureLogitsWarperCustom(LogitsWarper):\n    '''\n    A copy of the original Transformers temperature logits warper.\n    '''\n\n    def __init__(self, temperature: float):\n        if not isinstance(temperature, float) or not (temperature > 0):\n            except_msg = (\n                f\"`temperature` (={temperature}) has to be a strictly positive float, otherwise your next token \"\n                \"scores will be invalid.\"\n            )\n            if isinstance(temperature, float) and temperature == 0.0:\n                except_msg += \" If you're looking for greedy decoding strategies, set `do_sample=False`.\"\n\n            raise ValueError(except_msg)\n\n        self.temperature = temperature\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        scores = scores / self.temperature\n        return scores\n\n\nclass DynamicTemperatureLogitsWarper(LogitsWarper):\n    '''\n    Dynamic temperature.\n    '''\n\n    def __init__(self, dynatemp_low: float, dynatemp_high: float, dynatemp_exponent: float):\n        self.dynatemp_low = dynatemp_low\n        self.dynatemp_high = dynatemp_high\n        self.dynatemp_exponent = dynatemp_exponent\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        min_temp = self.dynatemp_low\n        max_temp = self.dynatemp_high\n        exponent_val = self.dynatemp_exponent\n\n        # Convert logits to probabilities\n        probs = torch.softmax(scores, dim=-1)\n\n        # Calculate entropy of the softmax probabilities\n        entropy = -1.0 * torch.where(probs > 0, probs * torch.log(probs), torch.zeros_like(probs)).sum()\n\n        # Guard against future possible division by zero\n        entropy = max(entropy, torch.tensor(1e-10))  # Ensures entropy is slightly greater than 0\n\n        # Any logits which are not -Infinity will be considered for calculating max entropy.\n        num_valid_tokens = torch.sum(scores > -float('inf')).item()\n\n        # Now, calculate the max entropy by using only the valid tokens' count\n        max_entropy = math.log(num_valid_tokens)\n\n        # Guard against future possible division by zero\n        max_entropy = max_entropy if max_entropy > 0.0 else 1e-10\n\n        # Normalize the entropy\n        normalized_entropy = entropy / max_entropy\n\n        # Map the normalized entropy to the desired temperature range using the power function\n        dyn_temp = min_temp + (max_temp - min_temp) * (normalized_entropy.pow(exponent_val))\n\n        # Apply the dynamically calculated temperature scaling\n        scores = scores / dyn_temp\n\n        # print(\"----------------------\\nTemperature from generation_config:\", self.temperature)\n        # print(\"min_temp:\", min_temp)\n        # print(\"max_temp:\", max_temp)\n        # print(\"Entropy:\", entropy.item())\n        # print(\"Max Possible Entropy considering valid tokens only:\", max_entropy)\n        # print(\"Normalized Entropy:\", normalized_entropy.item())\n        # print(\"Dynamic Temperature (dyn_temp):\", dyn_temp.item())\n        # print(\"----------------------\")\n\n        # max_prob_token_id = torch.argmax(scores, dim=-1)  # Get the token ID with the highest probability\n        # max_prob_token = shared.tokenizer.convert_ids_to_tokens(int(max_prob_token_id))  # Convert ID to token\n        # print(\"--- T=\", float(dyn_temp), \"token=\", max_prob_token, \"min=\", min_temp, \"max=\", max_temp, \"exponent=\", exponent_val)\n\n        return scores\n\n\nclass QuadraticSamplingLogitsWarper(LogitsWarper):\n    '''\n    Quadratic sampling with smoothing factor and smoothing curve parameters.\n    '''\n\n    def __init__(self, smoothing_factor, smoothing_curve):\n        self.smoothing_factor = smoothing_factor\n        self.smoothing_curve = smoothing_curve\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n\n        # Compute necessary values\n        max_logit = scores.max()\n        diff = scores - max_logit\n        k = (3 - self.smoothing_curve) / 2\n        s = (self.smoothing_curve - 1) / 2\n\n        # Apply transformation to non-negative infinity values\n        transformed_logits = torch.where(\n            scores != float('-inf'),\n            -(k * self.smoothing_factor * diff**2) + (s * self.smoothing_factor * diff**3) + max_logit,\n            scores\n        )\n\n        return transformed_logits\n\n\nclass TailFreeLogitsWarper(LogitsWarper):\n    def __init__(self, tfs: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        tfs = float(tfs)\n        if tfs < 0 or tfs > 1.0:\n            raise ValueError(f\"`tfs` has to be a float >= 0 and <= 1, but is {tfs}\")\n        self.tfs = tfs\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n        probs = sorted_logits.softmax(dim=-1)\n\n        # Compute second derivative normalized CDF\n        d2 = probs.diff().diff().abs()\n        normalized_d2 = d2 / d2.sum(dim=-1, keepdim=True)\n        normalized_d2_cdf = normalized_d2.cumsum(dim=-1)\n\n        # Remove tokens with CDF value above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = normalized_d2_cdf > self.tfs\n\n        # Centre the distribution around the cutoff as in the original implementation of the algorithm\n        sorted_indices_to_remove = torch.cat(\n            (\n                torch.zeros(scores.shape[0], 1, dtype=torch.bool, device=scores.device),\n                sorted_indices_to_remove,\n                torch.ones(scores.shape[0], 1, dtype=torch.bool, device=scores.device),\n            ),\n            dim=-1,\n        )\n\n        if self.min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep\n            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n\n\nclass TopALogitsWarper(LogitsWarper):\n    def __init__(self, top_a: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        top_a = float(top_a)\n        if top_a < 0 or top_a > 1.0:\n            raise ValueError(f\"`top_a` has to be a float >= 0 and <= 1, but is {top_a}\")\n        self.top_a = top_a\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n        probs = sorted_logits.softmax(dim=-1)\n\n        # Remove tokens with probability less than top_a*(max(probs))^2 (token with 0 are kept)\n        probs_max = probs[..., 0, None]\n        sorted_indices_to_remove = probs < probs_max * probs_max * self.top_a\n\n        if self.min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep\n            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n\n\nclass DRYLogitsProcessor(LogitsProcessor):\n    def __init__(self, multiplier: float, base: float, allowed_length: int, sequence_breakers: set[int], _range: int):\n        self.multiplier = multiplier\n        self.base = base\n        self.allowed_length = allowed_length\n        self.sequence_breakers = sequence_breakers\n        self._range = _range\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if self._range > 0:\n            input_ids = input_ids[:, -self._range:]\n\n        for input_ids_row, scores_row in zip(input_ids, scores):\n            # Use normal Python data types for improved performance\n            input_ids = input_ids_row.tolist()\n\n            last_token = input_ids[-1]\n            if last_token in self.sequence_breakers:\n                continue\n\n            # Exclude the last token as it always matches.\n            match_indices = []\n            for idx, val in enumerate(input_ids[:-1]):\n                if val == last_token:\n                    match_indices.append(idx)\n\n            # Stores the maximum matching sequence length\n            # for each token immediately following the sequence in the input.\n            match_lengths = {}\n\n            for i in match_indices:\n                next_token = input_ids[i + 1]\n\n                if next_token in self.sequence_breakers:\n                    continue\n\n                # We have already found that `last_token` matches at this index,\n                # so the match is at least of length 1.\n                match_length = 1\n\n                # Extend the match backwards (at most to 50 to prevent exponent overflow at penalty calculation) (this cap also improves performance on worst case)\n                while match_length < 50:\n                    j = i - match_length\n                    if j < 0:\n                        # Start of input reached.\n                        break\n\n                    previous_token = input_ids[-(match_length + 1)]\n                    if input_ids[j] != previous_token:\n                        # Start of match reached.\n                        break\n\n                    if previous_token in self.sequence_breakers:\n                        # Sequence-breaking token reached.\n                        break\n\n                    match_length += 1\n\n                if next_token in match_lengths:\n                    match_lengths[next_token] = max(match_length, match_lengths[next_token])\n                else:\n                    match_lengths[next_token] = match_length\n\n            # Apply penalties.\n            for token, match_length in match_lengths.items():\n                if match_length >= self.allowed_length:\n                    penalty = self.multiplier * self.base ** (match_length - self.allowed_length)\n                    scores_row[token] -= penalty\n\n        return scores\n\n\nclass MirostatLogitsWarper(LogitsWarper):\n    def __init__(self, mirostat_mode: int, mirostat_tau: float, mirostat_eta: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        if mirostat_mode not in [2]:\n            raise ValueError(f\"`mirostat` has to be a an integer 2, but is {mirostat_mode}\")\n\n        self.mirostat_mode = mirostat_mode\n        self.mirostat_eta = mirostat_eta\n        self.mirostat_tau = mirostat_tau\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n        self.mu = 2 * self.mirostat_tau\n        self.e = 0\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        logits = scores[0]\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        prob_original = torch.softmax(sorted_logits, dim=-1).tolist()  # candidates\n\n        # Truncate the words with surprise values greater than mu\n        for i, candidate in enumerate(prob_original):\n            if candidate > 0 and -math.log2(candidate) > self.mu:\n                if (i == 0):\n                    sorted_logits = sorted_logits[:1]\n                else:\n                    sorted_logits = sorted_logits[:i]\n                break\n\n        # Normalize the probabilities of the remaining words\n        if is_torch_xpu_available():\n            prob_topk = torch.softmax(sorted_logits, dim=0).to(\"xpu\")\n            prev_i = torch.multinomial(prob_topk, num_samples=1, replacement=True).to(\"xpu\")\n        else:\n            prob_topk = torch.softmax(sorted_logits, dim=0).to('cuda')\n            prev_i = torch.multinomial(prob_topk, num_samples=1, replacement=True).to('cuda')\n\n        observed_surprise = -math.log2(prob_topk[prev_i])\n        self.e = observed_surprise - self.mirostat_tau\n\n        # Update mu using the learning rate and error\n        self.mu -= self.mirostat_eta * self.e\n\n        sorted_indices_to_remove = torch.ones_like(scores[0], dtype=torch.bool)\n        sorted_indices_to_remove[prev_i] = False\n\n        indices_to_remove = sorted_indices_to_remove.unsqueeze(0).scatter(1, sorted_indices.unsqueeze(0), sorted_indices_to_remove.unsqueeze(0))\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n\n\nclass SpyLogitsWarper(LogitsWarper):\n    def __init__(self):\n        pass\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        global global_scores\n        global_scores = scores\n        return scores\n\n\nclass RepetitionPenaltyLogitsProcessorWithRange(LogitsProcessor):\n    '''\n    Copied from the transformers library\n    '''\n\n    def __init__(self, penalty: float, presence_penalty: float, frequency_penalty: float, _range: int):\n        if not (penalty > 0):\n            raise ValueError(f\"`penalty` has to be strictly positive, but is {penalty}\")\n\n        self.penalty = penalty\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self._range = _range\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        input_ids = input_ids[:, -self._range:]\n\n        # We loop here because torch.unique() needs to process each row separately in the\n        # case that batch_size > 1.\n        for input_ids_row, scores_row in zip(input_ids, scores):\n            unique_ids, counts = torch.unique(input_ids_row, return_counts=True)\n            score = torch.gather(scores_row, 0, unique_ids)\n\n            # multiplicative repetition penalty\n            # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability\n            score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n            scores_row.scatter_(0, unique_ids, score)\n\n            # presence_penalty and frequency_penalty\n            raw_presence_penalty = (counts > 0).to(scores.dtype)\n            raw_frequency_penalty = counts.to(scores.dtype)\n            additive_penalty = raw_presence_penalty * self.presence_penalty + raw_frequency_penalty * self.frequency_penalty\n            scores_row.scatter_add_(0, unique_ids, -additive_penalty)\n\n        return scores\n\n\ndef get_logits_warper_patch(self, generation_config):\n\n    # Parameter sanitization\n    if isinstance(generation_config.temperature, int):\n        generation_config.temperature = float(generation_config.temperature)  # Must be float\n\n    # Get the original warpers\n    warpers = self._get_logits_warper_old(generation_config)\n\n    # Replace temperature with our modified class.\n    # Currently, it behaves identically to the original.\n    for i in range(len(warpers)):\n        if warpers[i].__class__.__name__ == 'TemperatureLogitsWarper':\n            warpers[i] = TemperatureLogitsWarperCustom(\n                generation_config.temperature,\n            )\n\n    # Add custom warpers\n    warpers_to_add = LogitsProcessorList()\n    min_tokens_to_keep = 2 if generation_config.num_beams > 1 else 1\n    if generation_config.tfs is not None and 0.0 <= generation_config.tfs < 1.0:\n        warpers_to_add.append(\n            TailFreeLogitsWarper(\n                tfs=generation_config.tfs,\n                min_tokens_to_keep=min_tokens_to_keep\n            )\n        )\n\n    if generation_config.top_a is not None and 0.0 < generation_config.top_a <= 1.0:\n        warpers_to_add.append(\n            TopALogitsWarper(\n                top_a=generation_config.top_a,\n                min_tokens_to_keep=min_tokens_to_keep\n            )\n        )\n\n    if generation_config.dynamic_temperature:\n        warpers_to_add.append(\n            DynamicTemperatureLogitsWarper(\n                dynatemp_low=generation_config.dynatemp_low,\n                dynatemp_high=generation_config.dynatemp_high,\n                dynatemp_exponent=generation_config.dynatemp_exponent,\n            )\n        )\n\n    if generation_config.smoothing_factor > 0:\n        warpers_to_add.append(\n            QuadraticSamplingLogitsWarper(\n                smoothing_factor=generation_config.smoothing_factor,\n                smoothing_curve=generation_config.smoothing_curve\n            )\n        )\n\n    if generation_config.mirostat_mode is not None and generation_config.mirostat_mode == 2:\n        warpers_to_add.append(\n            MirostatLogitsWarper(\n                mirostat_mode=generation_config.mirostat_mode,\n                mirostat_eta=generation_config.mirostat_eta,\n                mirostat_tau=generation_config.mirostat_tau,\n                min_tokens_to_keep=min_tokens_to_keep\n            )\n        )\n\n    if len(warpers) > 0 and isinstance(warpers[-1], LogitNormalization):\n        normalize = warpers.pop(-1)\n    else:\n        normalize = None\n\n    warpers += warpers_to_add\n\n    # Sort the samplers.\n    sampler_priority = generation_config.sampler_priority\n\n    # Handle temperature_last\n    if generation_config.temperature_last:\n        for param_name in ['temperature', 'dynamic_temperature', 'quadratic_sampling']:\n            if param_name in sampler_priority:\n                if param_name in sampler_priority:\n                    index = sampler_priority.index(param_name)\n                    sampler_priority.append(sampler_priority.pop(index))\n                else:\n                    sampler_priority.append(param_name)\n\n    class_name_to_nickname = {\n        'DynamicTemperatureLogitsWarper': 'dynamic_temperature',\n        'EpsilonLogitsWarper': 'epsilon_cutoff',\n        'EtaLogitsWarper': 'eta_cutoff',\n        'MinPLogitsWarper': 'min_p',\n        'MirostatLogitsWarper': 'mirostat',\n        'QuadraticSamplingLogitsWarper': 'quadratic_sampling',\n        'TailFreeLogitsWarper': 'tfs',\n        'TemperatureLogitsWarperCustom': 'temperature',\n        'TopALogitsWarper': 'top_a',\n        'TopKLogitsWarper': 'top_k',\n        'TopPLogitsWarper': 'top_p',\n        'TypicalLogitsWarper': 'typical_p'\n    }\n\n    def custom_sort_key(obj):\n        class_name = obj.__class__.__name__\n\n        # Return a large value if class name is not mapped or if the mapped nickname is not in priority\n        if class_name not in class_name_to_nickname or class_name_to_nickname[class_name] not in sampler_priority:\n            return float('inf')\n\n        # Return the index of the nickname in the priority list for sorting\n        return sampler_priority.index(class_name_to_nickname[class_name])\n\n    # Sort the list using the custom key function\n    warpers = sorted(warpers, key=custom_sort_key)\n    if shared.args.verbose:\n        logger.info(\"WARPERS=\")\n        pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint([x.__class__.__name__ for x in warpers])\n        print()\n\n    if normalize is not None:\n        warpers.append(normalize)\n\n    warpers.append(SpyLogitsWarper())\n    warpers = LogitsProcessorList(warpers)\n    return warpers\n\n\ndef get_logits_processor_patch(self, **kwargs):\n    generation_config = kwargs['generation_config']\n\n    do_rep_pen_hijack = (generation_config.repetition_penalty > 1) or (generation_config.presence_penalty != 0) or (generation_config.frequency_penalty != 0)\n    if do_rep_pen_hijack:\n        generation_config.repetition_penalty = 1.1  # Set to value > 1 to ensure RepetitionPenaltyLogitsProcessor is created\n\n    result = self._get_logits_processor_old(**kwargs)\n\n    if do_rep_pen_hijack:\n        for i in range(len(result)):\n            if result[i].__class__.__name__ == 'RepetitionPenaltyLogitsProcessor':\n                result[i] = RepetitionPenaltyLogitsProcessorWithRange(\n                    generation_config.repetition_penalty,\n                    generation_config.presence_penalty,\n                    generation_config.frequency_penalty,\n                    generation_config.repetition_penalty_range\n                )\n\n    if generation_config.dry_multiplier is not None and generation_config.dry_multiplier > 0.0:\n        dry_sequence_breakers = generation_config.dry_sequence_breakers\n\n        # Support both JSON array notation and comma-separated strings.\n        if not dry_sequence_breakers.startswith(\"[\"):\n            dry_sequence_breakers = \"[\" + dry_sequence_breakers + \"]\"\n\n        sequence_breaker_strings = json.loads(dry_sequence_breakers)\n        # Prefix with 'a' to get the correct encoding of the token at the end of a text.\n        sequence_breakers = {shared.tokenizer.encode(f'a{s}')[-1] for s in sequence_breaker_strings}\n\n        result.append(\n            DRYLogitsProcessor(\n                multiplier=generation_config.dry_multiplier,\n                base=generation_config.dry_base,\n                allowed_length=generation_config.dry_allowed_length,\n                sequence_breakers=sequence_breakers,\n                _range=generation_config.repetition_penalty_range,\n            )\n        )\n\n    return result\n\n\ndef generation_config_init_patch(self, **kwargs):\n    self.__init___old(**kwargs)\n    self.min_p = kwargs.pop(\"min_p\", 0.0)\n    self.dynamic_temperature = kwargs.pop(\"dynamic_temperature\", False)\n    self.dynatemp_low = kwargs.pop(\"dynatemp_low\", 1)\n    self.dynatemp_high = kwargs.pop(\"dynatemp_high\", 1)\n    self.dynatemp_exponent = kwargs.pop(\"dynatemp_exponent\", 1)\n    self.smoothing_factor = kwargs.pop(\"smoothing_factor\", 0.0)\n    self.smoothing_curve = kwargs.pop(\"smoothing_curve\", 1.0)\n    self.tfs = kwargs.pop(\"tfs\", 1.0)\n    self.top_a = kwargs.pop(\"top_a\", 0.0)\n    self.mirostat_mode = kwargs.pop(\"mirostat_mode\", 0)\n    self.mirostat_eta = kwargs.pop(\"mirostat_eta\", 0.1)\n    self.mirostat_tau = kwargs.pop(\"mirostat_tau\", 5)\n    self.repetition_penalty_range = kwargs.pop(\"repetition_penalty_range\", 0)\n    self.presence_penalty = kwargs.pop(\"presence_penalty\", 0)\n    self.frequency_penalty = kwargs.pop(\"frequency_penalty\", 0)\n    self.dry_multiplier = kwargs.pop(\"dry_multiplier\", 0.0)\n    self.dry_base = kwargs.pop(\"dry_base\", 1.75)\n    self.dry_allowed_length = kwargs.pop(\"dry_allowed_length\", 2)\n    self.dry_sequence_breakers = kwargs.pop(\"dry_sequence_breakers\", '\"\\\\n\", \":\", \"\\\\\"\", \"*\"')\n    self.temperature_last = kwargs.pop(\"temperature_last\", False)\n    self.sampler_priority = kwargs.pop(\"sampler_priority\", ['temperature', 'dynamic_temperature', 'quadratic_sampling', 'top_k', 'top_p', 'typical_p', 'epsilon_cutoff', 'eta_cutoff', 'tfs', 'top_a', 'min_p', 'mirostat'])\n\n\ndef hijack_samplers():\n    transformers.GenerationMixin._get_logits_warper_old = transformers.GenerationMixin._get_logits_warper\n    transformers.GenerationMixin._get_logits_warper = get_logits_warper_patch\n\n    transformers.GenerationMixin._get_logits_processor_old = transformers.GenerationMixin._get_logits_processor\n    transformers.GenerationMixin._get_logits_processor = get_logits_processor_patch\n\n    transformers.GenerationConfig.__init___old = transformers.GenerationConfig.__init__\n    transformers.GenerationConfig.__init__ = generation_config_init_patch\n", "modules/shared.py": "import argparse\nimport copy\nimport os\nimport sys\nfrom collections import OrderedDict\nfrom pathlib import Path\n\nimport yaml\n\nfrom modules.logging_colors import logger\n\n# Model variables\nmodel = None\ntokenizer = None\nmodel_name = 'None'\nprevious_model_name = 'None'\nis_seq2seq = False\nmodel_dirty_from_training = False\nlora_names = []\n\n# Generation variables\nstop_everything = False\ngeneration_lock = None\nprocessing_message = '*Is typing...*'\n\n# UI variables\ngradio = {}\npersistent_interface_state = {}\nneed_restart = False\n\n# UI defaults\nsettings = {\n    'dark_theme': True,\n    'show_controls': True,\n    'start_with': '',\n    'mode': 'chat',\n    'chat_style': 'cai-chat',\n    'prompt-default': 'QA',\n    'prompt-notebook': 'QA',\n    'preset': 'min_p',\n    'max_new_tokens': 512,\n    'max_new_tokens_min': 1,\n    'max_new_tokens_max': 4096,\n    'negative_prompt': '',\n    'seed': -1,\n    'truncation_length': 2048,\n    'truncation_length_min': 0,\n    'truncation_length_max': 200000,\n    'max_tokens_second': 0,\n    'max_updates_second': 0,\n    'prompt_lookup_num_tokens': 0,\n    'custom_stopping_strings': '',\n    'custom_token_bans': '',\n    'auto_max_new_tokens': False,\n    'ban_eos_token': False,\n    'add_bos_token': True,\n    'skip_special_tokens': True,\n    'stream': True,\n    'character': 'Assistant',\n    'name1': 'You',\n    'user_bio': '',\n    'custom_system_message': '',\n    'instruction_template_str': \"{%- set ns = namespace(found=false) -%}\\n{%- for message in messages -%}\\n    {%- if message['role'] == 'system' -%}\\n        {%- set ns.found = true -%}\\n    {%- endif -%}\\n{%- endfor -%}\\n{%- if not ns.found -%}\\n    {{- '' + 'Below is an instruction that describes a task. Write a response that appropriately completes the request.' + '\\\\n\\\\n' -}}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' -%}\\n        {{- '' + message['content'] + '\\\\n\\\\n' -}}\\n    {%- else -%}\\n        {%- if message['role'] == 'user' -%}\\n            {{-'### Instruction:\\\\n' + message['content'] + '\\\\n\\\\n'-}}\\n        {%- else -%}\\n            {{-'### Response:\\\\n' + message['content'] + '\\\\n\\\\n' -}}\\n        {%- endif -%}\\n    {%- endif -%}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{-'### Response:\\\\n'-}}\\n{%- endif -%}\",\n    'chat_template_str': \"{%- for message in messages %}\\n    {%- if message['role'] == 'system' -%}\\n        {%- if message['content'] -%}\\n            {{- message['content'] + '\\\\n\\\\n' -}}\\n        {%- endif -%}\\n        {%- if user_bio -%}\\n            {{- user_bio + '\\\\n\\\\n' -}}\\n        {%- endif -%}\\n    {%- else -%}\\n        {%- if message['role'] == 'user' -%}\\n            {{- name1 + ': ' + message['content'] + '\\\\n'-}}\\n        {%- else -%}\\n            {{- name2 + ': ' + message['content'] + '\\\\n' -}}\\n        {%- endif -%}\\n    {%- endif -%}\\n{%- endfor -%}\",\n    'chat-instruct_command': 'Continue the chat dialogue below. Write a single reply for the character \"<|character|>\".\\n\\n<|prompt|>',\n    'autoload_model': False,\n    'default_extensions': [],\n}\n\ndefault_settings = copy.deepcopy(settings)\n\n# Parser copied from https://github.com/vladmandic/automatic\nparser = argparse.ArgumentParser(description=\"Text generation web UI\", conflict_handler='resolve', add_help=True, formatter_class=lambda prog: argparse.HelpFormatter(prog, max_help_position=55, indent_increment=2, width=200))\n\n# Basic settings\ngroup = parser.add_argument_group('Basic settings')\ngroup.add_argument('--multi-user', action='store_true', help='Multi-user mode. Chat histories are not saved or automatically loaded. Warning: this is likely not safe for sharing publicly.')\ngroup.add_argument('--character', type=str, help='The name of the character to load in chat mode by default.')\ngroup.add_argument('--model', type=str, help='Name of the model to load by default.')\ngroup.add_argument('--lora', type=str, nargs='+', help='The list of LoRAs to load. If you want to load more than one LoRA, write the names separated by spaces.')\ngroup.add_argument('--model-dir', type=str, default='models/', help='Path to directory with all the models.')\ngroup.add_argument('--lora-dir', type=str, default='loras/', help='Path to directory with all the loras.')\ngroup.add_argument('--model-menu', action='store_true', help='Show a model menu in the terminal when the web UI is first launched.')\ngroup.add_argument('--settings', type=str, help='Load the default interface settings from this yaml file. See settings-template.yaml for an example. If you create a file called settings.yaml, this file will be loaded by default without the need to use the --settings flag.')\ngroup.add_argument('--extensions', type=str, nargs='+', help='The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.')\ngroup.add_argument('--verbose', action='store_true', help='Print the prompts to the terminal.')\ngroup.add_argument('--chat-buttons', action='store_true', help='Show buttons on the chat tab instead of a hover menu.')\ngroup.add_argument('--idle-timeout', type=int, default=0, help='Unload model after this many minutes of inactivity. It will be automatically reloaded when you try to use it again.')\n\n# Model loader\ngroup = parser.add_argument_group('Model loader')\ngroup.add_argument('--loader', type=str, help='Choose the model loader manually, otherwise, it will get autodetected. Valid options: Transformers, llama.cpp, llamacpp_HF, ExLlamav2_HF, ExLlamav2, AutoGPTQ, AutoAWQ.')\n\n# Transformers/Accelerate\ngroup = parser.add_argument_group('Transformers/Accelerate')\ngroup.add_argument('--cpu', action='store_true', help='Use the CPU to generate text. Warning: Training on CPU is extremely slow.')\ngroup.add_argument('--auto-devices', action='store_true', help='Automatically split the model across the available GPU(s) and CPU.')\ngroup.add_argument('--gpu-memory', type=str, nargs='+', help='Maximum GPU memory in GiB to be allocated per GPU. Example: --gpu-memory 10 for a single GPU, --gpu-memory 10 5 for two GPUs. You can also set values in MiB like --gpu-memory 3500MiB.')\ngroup.add_argument('--cpu-memory', type=str, help='Maximum CPU memory in GiB to allocate for offloaded weights. Same as above.')\ngroup.add_argument('--disk', action='store_true', help='If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.')\ngroup.add_argument('--disk-cache-dir', type=str, default='cache', help='Directory to save the disk cache to. Defaults to \"cache\".')\ngroup.add_argument('--load-in-8bit', action='store_true', help='Load the model with 8-bit precision (using bitsandbytes).')\ngroup.add_argument('--bf16', action='store_true', help='Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.')\ngroup.add_argument('--no-cache', action='store_true', help='Set use_cache to False while generating text. This reduces VRAM usage slightly, but it comes at a performance cost.')\ngroup.add_argument('--trust-remote-code', action='store_true', help='Set trust_remote_code=True while loading the model. Necessary for some models.')\ngroup.add_argument('--force-safetensors', action='store_true', help='Set use_safetensors=True while loading the model. This prevents arbitrary code execution.')\ngroup.add_argument('--no_use_fast', action='store_true', help='Set use_fast=False while loading the tokenizer (it\\'s True by default). Use this if you have any problems related to use_fast.')\ngroup.add_argument('--use_flash_attention_2', action='store_true', help='Set use_flash_attention_2=True while loading the model.')\n\n# bitsandbytes 4-bit\ngroup = parser.add_argument_group('bitsandbytes 4-bit')\ngroup.add_argument('--load-in-4bit', action='store_true', help='Load the model with 4-bit precision (using bitsandbytes).')\ngroup.add_argument('--use_double_quant', action='store_true', help='use_double_quant for 4-bit.')\ngroup.add_argument('--compute_dtype', type=str, default='float16', help='compute dtype for 4-bit. Valid options: bfloat16, float16, float32.')\ngroup.add_argument('--quant_type', type=str, default='nf4', help='quant_type for 4-bit. Valid options: nf4, fp4.')\n\n# llama.cpp\ngroup = parser.add_argument_group('llama.cpp')\ngroup.add_argument('--flash-attn', action='store_true', help='Use flash-attention.')\ngroup.add_argument('--tensorcores', action='store_true', help='Use llama-cpp-python compiled with tensor cores support. This increases performance on RTX cards. NVIDIA only.')\ngroup.add_argument('--n_ctx', type=int, default=2048, help='Size of the prompt context.')\ngroup.add_argument('--threads', type=int, default=0, help='Number of threads to use.')\ngroup.add_argument('--threads-batch', type=int, default=0, help='Number of threads to use for batches/prompt processing.')\ngroup.add_argument('--no_mul_mat_q', action='store_true', help='Disable the mulmat kernels.')\ngroup.add_argument('--n_batch', type=int, default=512, help='Maximum number of prompt tokens to batch together when calling llama_eval.')\ngroup.add_argument('--no-mmap', action='store_true', help='Prevent mmap from being used.')\ngroup.add_argument('--mlock', action='store_true', help='Force the system to keep the model in RAM.')\ngroup.add_argument('--n-gpu-layers', type=int, default=0, help='Number of layers to offload to the GPU.')\ngroup.add_argument('--tensor_split', type=str, default=None, help='Split the model across multiple GPUs. Comma-separated list of proportions. Example: 18,17.')\ngroup.add_argument('--numa', action='store_true', help='Activate NUMA task allocation for llama.cpp.')\ngroup.add_argument('--logits_all', action='store_true', help='Needs to be set for perplexity evaluation to work. Otherwise, ignore it, as it makes prompt processing slower.')\ngroup.add_argument('--no_offload_kqv', action='store_true', help='Do not offload the  K, Q, V to the GPU. This saves VRAM but reduces the performance.')\ngroup.add_argument('--cache-capacity', type=str, help='Maximum cache capacity (llama-cpp-python). Examples: 2000MiB, 2GiB. When provided without units, bytes will be assumed.')\ngroup.add_argument('--row_split', action='store_true', help='Split the model by rows across GPUs. This may improve multi-gpu performance.')\ngroup.add_argument('--streaming-llm', action='store_true', help='Activate StreamingLLM to avoid re-evaluating the entire prompt when old messages are removed.')\ngroup.add_argument('--attention-sink-size', type=int, default=5, help='StreamingLLM: number of sink tokens. Only used if the trimmed prompt does not share a prefix with the old prompt.')\n\n# ExLlamaV2\ngroup = parser.add_argument_group('ExLlamaV2')\ngroup.add_argument('--gpu-split', type=str, help='Comma-separated list of VRAM (in GB) to use per GPU device for model layers. Example: 20,7,7.')\ngroup.add_argument('--autosplit', action='store_true', help='Autosplit the model tensors across the available GPUs. This causes --gpu-split to be ignored.')\ngroup.add_argument('--max_seq_len', type=int, default=2048, help='Maximum sequence length.')\ngroup.add_argument('--cfg-cache', action='store_true', help='ExLlamav2_HF: Create an additional cache for CFG negative prompts. Necessary to use CFG with that loader.')\ngroup.add_argument('--no_flash_attn', action='store_true', help='Force flash-attention to not be used.')\ngroup.add_argument('--cache_8bit', action='store_true', help='Use 8-bit cache to save VRAM.')\ngroup.add_argument('--cache_4bit', action='store_true', help='Use Q4 cache to save VRAM.')\ngroup.add_argument('--num_experts_per_token', type=int, default=2, help='Number of experts to use for generation. Applies to MoE models like Mixtral.')\n\n# AutoGPTQ\ngroup = parser.add_argument_group('AutoGPTQ')\ngroup.add_argument('--triton', action='store_true', help='Use triton.')\ngroup.add_argument('--no_inject_fused_mlp', action='store_true', help='Triton mode only: disable the use of fused MLP, which will use less VRAM at the cost of slower inference.')\ngroup.add_argument('--no_use_cuda_fp16', action='store_true', help='This can make models faster on some systems.')\ngroup.add_argument('--desc_act', action='store_true', help='For models that do not have a quantize_config.json, this parameter is used to define whether to set desc_act or not in BaseQuantizeConfig.')\ngroup.add_argument('--disable_exllama', action='store_true', help='Disable ExLlama kernel, which can improve inference speed on some systems.')\ngroup.add_argument('--disable_exllamav2', action='store_true', help='Disable ExLlamav2 kernel.')\ngroup.add_argument('--wbits', type=int, default=0, help='Load a pre-quantized model with specified precision in bits. 2, 3, 4 and 8 are supported.')\ngroup.add_argument('--groupsize', type=int, default=-1, help='Group size.')\n\n# AutoAWQ\ngroup = parser.add_argument_group('AutoAWQ')\ngroup.add_argument('--no_inject_fused_attention', action='store_true', help='Disable the use of fused attention, which will use less VRAM at the cost of slower inference.')\n\n# HQQ\ngroup = parser.add_argument_group('HQQ')\ngroup.add_argument('--hqq-backend', type=str, default='PYTORCH_COMPILE', help='Backend for the HQQ loader. Valid options: PYTORCH, PYTORCH_COMPILE, ATEN.')\n\n# DeepSpeed\ngroup = parser.add_argument_group('DeepSpeed')\ngroup.add_argument('--deepspeed', action='store_true', help='Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.')\ngroup.add_argument('--nvme-offload-dir', type=str, help='DeepSpeed: Directory to use for ZeRO-3 NVME offloading.')\ngroup.add_argument('--local_rank', type=int, default=0, help='DeepSpeed: Optional argument for distributed setups.')\n\n# RoPE\ngroup = parser.add_argument_group('RoPE')\ngroup.add_argument('--alpha_value', type=float, default=1, help='Positional embeddings alpha factor for NTK RoPE scaling. Use either this or compress_pos_emb, not both.')\ngroup.add_argument('--rope_freq_base', type=int, default=0, help='If greater than 0, will be used instead of alpha_value. Those two are related by rope_freq_base = 10000 * alpha_value ^ (64 / 63).')\ngroup.add_argument('--compress_pos_emb', type=int, default=1, help=\"Positional embeddings compression factor. Should be set to (context length) / (model\\'s original context length). Equal to 1/rope_freq_scale.\")\n\n# Gradio\ngroup = parser.add_argument_group('Gradio')\ngroup.add_argument('--listen', action='store_true', help='Make the web UI reachable from your local network.')\ngroup.add_argument('--listen-port', type=int, help='The listening port that the server will use.')\ngroup.add_argument('--listen-host', type=str, help='The hostname that the server will use.')\ngroup.add_argument('--share', action='store_true', help='Create a public URL. This is useful for running the web UI on Google Colab or similar.')\ngroup.add_argument('--auto-launch', action='store_true', default=False, help='Open the web UI in the default browser upon launch.')\ngroup.add_argument('--gradio-auth', type=str, help='Set Gradio authentication password in the format \"username:password\". Multiple credentials can also be supplied with \"u1:p1,u2:p2,u3:p3\".', default=None)\ngroup.add_argument('--gradio-auth-path', type=str, help='Set the Gradio authentication file path. The file should contain one or more user:password pairs in the same format as above.', default=None)\ngroup.add_argument('--ssl-keyfile', type=str, help='The path to the SSL certificate key file.', default=None)\ngroup.add_argument('--ssl-certfile', type=str, help='The path to the SSL certificate cert file.', default=None)\n\n# API\ngroup = parser.add_argument_group('API')\ngroup.add_argument('--api', action='store_true', help='Enable the API extension.')\ngroup.add_argument('--public-api', action='store_true', help='Create a public URL for the API using Cloudfare.')\ngroup.add_argument('--public-api-id', type=str, help='Tunnel ID for named Cloudflare Tunnel. Use together with public-api option.', default=None)\ngroup.add_argument('--api-port', type=int, default=5000, help='The listening port for the API.')\ngroup.add_argument('--api-key', type=str, default='', help='API authentication key.')\ngroup.add_argument('--admin-key', type=str, default='', help='API authentication key for admin tasks like loading and unloading models. If not set, will be the same as --api-key.')\ngroup.add_argument('--nowebui', action='store_true', help='Do not launch the Gradio UI. Useful for launching the API in standalone mode.')\n\n# Multimodal\ngroup = parser.add_argument_group('Multimodal')\ngroup.add_argument('--multimodal-pipeline', type=str, default=None, help='The multimodal pipeline to use. Examples: llava-7b, llava-13b.')\n\n# Deprecated parameters\ngroup = parser.add_argument_group('Deprecated')\ngroup.add_argument('--model_type', type=str, help='DEPRECATED')\ngroup.add_argument('--pre_layer', type=int, nargs='+', help='DEPRECATED')\ngroup.add_argument('--checkpoint', type=str, help='DEPRECATED')\ngroup.add_argument('--monkey-patch', action='store_true', help='DEPRECATED')\n\nargs = parser.parse_args()\nargs_defaults = parser.parse_args([])\nprovided_arguments = []\nfor arg in sys.argv[1:]:\n    arg = arg.lstrip('-').replace('-', '_')\n    if hasattr(args, arg):\n        provided_arguments.append(arg)\n\ndeprecated_args = []\n\n\ndef do_cmd_flags_warnings():\n\n    # Deprecation warnings\n    for k in deprecated_args:\n        if getattr(args, k):\n            logger.warning(f'The --{k} flag has been deprecated and will be removed soon. Please remove that flag.')\n\n    # Security warnings\n    if args.trust_remote_code:\n        logger.warning('trust_remote_code is enabled. This is dangerous.')\n    if 'COLAB_GPU' not in os.environ and not args.nowebui:\n        if args.share:\n            logger.warning(\"The gradio \\\"share link\\\" feature uses a proprietary executable to create a reverse tunnel. Use it with care.\")\n        if any((args.listen, args.share)) and not any((args.gradio_auth, args.gradio_auth_path)):\n            logger.warning(\"\\nYou are potentially exposing the web UI to the entire internet without any access password.\\nYou can create one with the \\\"--gradio-auth\\\" flag like this:\\n\\n--gradio-auth username:password\\n\\nMake sure to replace username:password with your own.\")\n            if args.multi_user:\n                logger.warning('\\nThe multi-user mode is highly experimental and should not be shared publicly.')\n\n\ndef fix_loader_name(name):\n    if not name:\n        return name\n\n    name = name.lower()\n    if name in ['llamacpp', 'llama.cpp', 'llama-cpp', 'llama cpp']:\n        return 'llama.cpp'\n    if name in ['llamacpp_hf', 'llama.cpp_hf', 'llama-cpp-hf', 'llamacpp-hf', 'llama.cpp-hf']:\n        return 'llamacpp_HF'\n    elif name in ['transformers', 'huggingface', 'hf', 'hugging_face', 'hugging face']:\n        return 'Transformers'\n    elif name in ['autogptq', 'auto-gptq', 'auto_gptq', 'auto gptq']:\n        return 'AutoGPTQ'\n    elif name in ['exllama', 'ex-llama', 'ex_llama', 'exlama']:\n        return 'ExLlama'\n    elif name in ['exllamav2', 'exllama-v2', 'ex_llama-v2', 'exlamav2', 'exlama-v2', 'exllama2', 'exllama-2']:\n        return 'ExLlamav2'\n    elif name in ['exllamav2-hf', 'exllamav2_hf', 'exllama-v2-hf', 'exllama_v2_hf', 'exllama-v2_hf', 'exllama2-hf', 'exllama2_hf', 'exllama-2-hf', 'exllama_2_hf', 'exllama-2_hf']:\n        return 'ExLlamav2_HF'\n    elif name in ['autoawq', 'awq', 'auto-awq']:\n        return 'AutoAWQ'\n    elif name in ['hqq']:\n        return 'HQQ'\n\n\ndef add_extension(name, last=False):\n    if args.extensions is None:\n        args.extensions = [name]\n    elif last:\n        args.extensions = [x for x in args.extensions if x != name]\n        args.extensions.append(name)\n    elif name not in args.extensions:\n        args.extensions.append(name)\n\n\ndef is_chat():\n    return True\n\n\ndef load_user_config():\n    '''\n    Loads custom model-specific settings\n    '''\n    if Path(f'{args.model_dir}/config-user.yaml').exists():\n        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()\n\n        if file_content:\n            user_config = yaml.safe_load(file_content)\n        else:\n            user_config = {}\n    else:\n        user_config = {}\n\n    return user_config\n\n\nargs.loader = fix_loader_name(args.loader)\n\n# Activate the multimodal extension\nif args.multimodal_pipeline is not None:\n    add_extension('multimodal')\n\n# Activate the API extension\nif args.api or args.public_api:\n    add_extension('openai', last=True)\n\n# Load model-specific settings\nwith Path(f'{args.model_dir}/config.yaml') as p:\n    if p.exists():\n        model_config = yaml.safe_load(open(p, 'r').read())\n    else:\n        model_config = {}\n\n# Load custom model-specific settings\nuser_config = load_user_config()\n\nmodel_config = OrderedDict(model_config)\nuser_config = OrderedDict(user_config)\n", "modules/loaders.py": "import functools\nfrom collections import OrderedDict\n\nimport gradio as gr\n\nfrom modules import shared\n\nloaders_and_params = OrderedDict({\n    'Transformers': [\n        'cpu_memory',\n        'gpu_memory',\n        'load_in_8bit',\n        'bf16',\n        'cpu',\n        'disk',\n        'auto_devices',\n        'load_in_4bit',\n        'use_double_quant',\n        'quant_type',\n        'compute_dtype',\n        'trust_remote_code',\n        'no_use_fast',\n        'use_flash_attention_2',\n        'alpha_value',\n        'rope_freq_base',\n        'compress_pos_emb',\n        'disable_exllama',\n        'disable_exllamav2',\n        'transformers_info',\n    ],\n    'llama.cpp': [\n        'n_ctx',\n        'n_gpu_layers',\n        'tensor_split',\n        'n_batch',\n        'threads',\n        'threads_batch',\n        'no_mmap',\n        'mlock',\n        'no_mul_mat_q',\n        'alpha_value',\n        'rope_freq_base',\n        'compress_pos_emb',\n        'cpu',\n        'numa',\n        'no_offload_kqv',\n        'row_split',\n        'tensorcores',\n        'flash_attn',\n        'streaming_llm',\n        'attention_sink_size',\n    ],\n    'llamacpp_HF': [\n        'n_ctx',\n        'n_gpu_layers',\n        'tensor_split',\n        'n_batch',\n        'threads',\n        'threads_batch',\n        'no_mmap',\n        'mlock',\n        'no_mul_mat_q',\n        'alpha_value',\n        'rope_freq_base',\n        'compress_pos_emb',\n        'cpu',\n        'numa',\n        'cfg_cache',\n        'trust_remote_code',\n        'no_use_fast',\n        'logits_all',\n        'no_offload_kqv',\n        'row_split',\n        'tensorcores',\n        'flash_attn',\n        'streaming_llm',\n        'attention_sink_size',\n        'llamacpp_HF_info',\n    ],\n    'ExLlamav2_HF': [\n        'gpu_split',\n        'max_seq_len',\n        'cfg_cache',\n        'no_flash_attn',\n        'num_experts_per_token',\n        'cache_8bit',\n        'cache_4bit',\n        'autosplit',\n        'alpha_value',\n        'compress_pos_emb',\n        'trust_remote_code',\n        'no_use_fast',\n    ],\n    'ExLlamav2': [\n        'gpu_split',\n        'max_seq_len',\n        'no_flash_attn',\n        'num_experts_per_token',\n        'cache_8bit',\n        'cache_4bit',\n        'autosplit',\n        'alpha_value',\n        'compress_pos_emb',\n        'exllamav2_info',\n    ],\n    'AutoGPTQ': [\n        'triton',\n        'no_inject_fused_mlp',\n        'no_use_cuda_fp16',\n        'wbits',\n        'groupsize',\n        'desc_act',\n        'disable_exllama',\n        'disable_exllamav2',\n        'gpu_memory',\n        'cpu_memory',\n        'cpu',\n        'disk',\n        'auto_devices',\n        'trust_remote_code',\n        'no_use_fast',\n        'autogptq_info',\n    ],\n    'AutoAWQ': [\n        'cpu_memory',\n        'gpu_memory',\n        'auto_devices',\n        'max_seq_len',\n        'no_inject_fused_attention',\n        'trust_remote_code',\n        'no_use_fast',\n    ],\n    'HQQ': [\n        'hqq_backend',\n        'trust_remote_code',\n        'no_use_fast',\n    ]\n})\n\n\ndef transformers_samplers():\n    return {\n        'temperature',\n        'temperature_last',\n        'dynamic_temperature',\n        'dynatemp_low',\n        'dynatemp_high',\n        'dynatemp_exponent',\n        'smoothing_factor',\n        'smoothing_curve',\n        'top_p',\n        'min_p',\n        'top_k',\n        'typical_p',\n        'epsilon_cutoff',\n        'eta_cutoff',\n        'tfs',\n        'top_a',\n        'repetition_penalty',\n        'presence_penalty',\n        'frequency_penalty',\n        'repetition_penalty_range',\n        'encoder_repetition_penalty',\n        'no_repeat_ngram_size',\n        'dry_multiplier',\n        'dry_base',\n        'dry_allowed_length',\n        'dry_sequence_breakers',\n        'seed',\n        'do_sample',\n        'penalty_alpha',\n        'mirostat_mode',\n        'mirostat_tau',\n        'mirostat_eta',\n        'grammar_file_row',\n        'grammar_string',\n        'guidance_scale',\n        'negative_prompt',\n        'ban_eos_token',\n        'custom_token_bans',\n        'sampler_priority',\n        'add_bos_token',\n        'skip_special_tokens',\n        'auto_max_new_tokens',\n        'prompt_lookup_num_tokens'\n    }\n\n\nloaders_samplers = {\n    'Transformers': transformers_samplers(),\n    'AutoGPTQ': transformers_samplers(),\n    'AutoAWQ': transformers_samplers(),\n    'HQQ': transformers_samplers(),\n    'ExLlamav2': {\n        'temperature',\n        'temperature_last',\n        'top_p',\n        'min_p',\n        'top_k',\n        'typical_p',\n        'tfs',\n        'top_a',\n        'repetition_penalty',\n        'presence_penalty',\n        'frequency_penalty',\n        'repetition_penalty_range',\n        'seed',\n        'mirostat_mode',\n        'mirostat_tau',\n        'mirostat_eta',\n        'ban_eos_token',\n        'add_bos_token',\n        'custom_token_bans',\n        'skip_special_tokens',\n        'auto_max_new_tokens',\n    },\n    'ExLlamav2_HF': {\n        'temperature',\n        'temperature_last',\n        'dynamic_temperature',\n        'dynatemp_low',\n        'dynatemp_high',\n        'dynatemp_exponent',\n        'smoothing_factor',\n        'smoothing_curve',\n        'top_p',\n        'min_p',\n        'top_k',\n        'typical_p',\n        'epsilon_cutoff',\n        'eta_cutoff',\n        'tfs',\n        'top_a',\n        'repetition_penalty',\n        'presence_penalty',\n        'frequency_penalty',\n        'repetition_penalty_range',\n        'encoder_repetition_penalty',\n        'no_repeat_ngram_size',\n        'dry_multiplier',\n        'dry_base',\n        'dry_allowed_length',\n        'dry_sequence_breakers',\n        'seed',\n        'do_sample',\n        'mirostat_mode',\n        'mirostat_tau',\n        'mirostat_eta',\n        'grammar_file_row',\n        'grammar_string',\n        'guidance_scale',\n        'negative_prompt',\n        'ban_eos_token',\n        'custom_token_bans',\n        'sampler_priority',\n        'add_bos_token',\n        'skip_special_tokens',\n        'auto_max_new_tokens',\n    },\n    'llama.cpp': {\n        'temperature',\n        'top_p',\n        'min_p',\n        'top_k',\n        'typical_p',\n        'tfs',\n        'repetition_penalty',\n        'presence_penalty',\n        'frequency_penalty',\n        'seed',\n        'mirostat_mode',\n        'mirostat_tau',\n        'mirostat_eta',\n        'grammar_file_row',\n        'grammar_string',\n        'ban_eos_token',\n        'custom_token_bans',\n    },\n    'llamacpp_HF': {\n        'temperature',\n        'temperature_last',\n        'dynamic_temperature',\n        'dynatemp_low',\n        'dynatemp_high',\n        'dynatemp_exponent',\n        'smoothing_factor',\n        'smoothing_curve',\n        'top_p',\n        'min_p',\n        'top_k',\n        'typical_p',\n        'epsilon_cutoff',\n        'eta_cutoff',\n        'tfs',\n        'top_a',\n        'repetition_penalty',\n        'presence_penalty',\n        'frequency_penalty',\n        'repetition_penalty_range',\n        'encoder_repetition_penalty',\n        'no_repeat_ngram_size',\n        'dry_multiplier',\n        'dry_base',\n        'dry_allowed_length',\n        'dry_sequence_breakers',\n        'seed',\n        'do_sample',\n        'mirostat_mode',\n        'mirostat_tau',\n        'mirostat_eta',\n        'grammar_file_row',\n        'grammar_string',\n        'guidance_scale',\n        'negative_prompt',\n        'ban_eos_token',\n        'custom_token_bans',\n        'sampler_priority',\n        'add_bos_token',\n        'skip_special_tokens',\n        'auto_max_new_tokens',\n    },\n}\n\n\n@functools.cache\ndef list_all_samplers():\n    all_samplers = set()\n    for k in loaders_samplers:\n        for sampler in loaders_samplers[k]:\n            all_samplers.add(sampler)\n\n    return sorted(all_samplers)\n\n\ndef blacklist_samplers(loader, dynamic_temperature):\n    all_samplers = list_all_samplers()\n    output = []\n\n    for sampler in all_samplers:\n        if loader == 'All' or sampler in loaders_samplers[loader]:\n            if sampler.startswith('dynatemp'):\n                output.append(gr.update(visible=dynamic_temperature))\n            else:\n                output.append(gr.update(visible=True))\n        else:\n            output.append(gr.update(visible=False))\n\n    return output\n\n\ndef get_gpu_memory_keys():\n    return [k for k in shared.gradio if k.startswith('gpu_memory')]\n\n\n@functools.cache\ndef get_all_params():\n    all_params = set()\n    for k in loaders_and_params:\n        for el in loaders_and_params[k]:\n            all_params.add(el)\n\n    if 'gpu_memory' in all_params:\n        all_params.remove('gpu_memory')\n        for k in get_gpu_memory_keys():\n            all_params.add(k)\n\n    return sorted(all_params)\n\n\ndef make_loader_params_visible(loader):\n    params = []\n    all_params = get_all_params()\n    if loader in loaders_and_params:\n        params = loaders_and_params[loader]\n\n        if 'gpu_memory' in params:\n            params.remove('gpu_memory')\n            params += get_gpu_memory_keys()\n\n    return [gr.update(visible=True) if k in params else gr.update(visible=False) for k in all_params]\n", "modules/grammar/logits_process.py": "'''\nThis file has been 100% copied from this PR to the Transformers library:\nhttps://github.com/huggingface/transformers/pull/27557\n\nAuthor: Saibo-creator\nAuthor GitHub: https://github.com/Saibo-creator\n\nAll credits go to the author.\n'''\n\nimport math\n\nimport torch\nfrom transformers.generation.logits_process import LogitsProcessor\nfrom transformers.utils import add_start_docstrings\n\nLOGITS_PROCESSOR_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n        scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n            search or log softmax for each vocabulary token when using beam search\n\n    Return:\n        `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n\n\"\"\"\n\n\nclass GrammarConstrainedLogitsProcessor(LogitsProcessor):\n    def __init__(self, grammar_constraint):\n        self.last_size = None\n        self.grammar_constraint = grammar_constraint\n        self.batch_stacks = None\n\n    def filter_logits(self, logits, device):\n        # resolve each stack to a tensor of True/False for each token\n        # indicating acceptance\n        # acceptance = self.grammar_acceptor.filter_vocab(self.stacks, device)\n        acceptance = self.grammar_constraint.batch_filter_vocab(self.batch_stacks, device)\n        # logger.debug(acceptance)\n        # Logits to -inf where False\n        logits[~acceptance] = -math.inf\n\n    # TODO: batching\n    def process_logits(self, input_ids, scores, parse_start_index=None):\n        \"\"\"\n        :param input_ids:\n        :param scores:\n        :param parse_start_index: default None, which means generate from scratch. Set to 0 to parse all input_ids\n        :return:\n        \"\"\"\n        # we dynamically create stacks at the first call, so that we know the batch size and beam size\n        if self.batch_stacks is None:\n            self.batch_stacks = [self.grammar_constraint.init_stacks() for _ in range(len(input_ids))]\n\n        # if self.last_size is not set (which would be the case when processing the first token).\n        # In this case, do nothing.\n        if self.last_size is None:\n            prefix_to_parse = [\n                single_input_ids[parse_start_index:] if parse_start_index is not None else []\n                for single_input_ids in input_ids\n            ]\n            # self.grammar_acceptor.accept_token_ids(prefix_to_parse, self.stacks)\n            self.batch_stacks = [\n                self.grammar_constraint.accept_token_ids(prefix, stack)\n                for prefix, stack in zip(prefix_to_parse, self.batch_stacks)\n            ]\n        #  if the length of the current input IDs (input_ids[0]) is exactly one more than self.last_size.\n        #  This is expected in a scenario where inputs are processed incrementally, one token at a time.\n        elif len(input_ids[0]) == self.last_size + 1:\n            # self.stacks = self.grammar_acceptor.accept_token_id(input_ids[0][-1], self.stacks)\n            self.batch_stacks = [\n                self.grammar_constraint.accept_token_id(single_input_ids[-1], stack)\n                for single_input_ids, stack in zip(input_ids, self.batch_stacks)\n            ]\n        #  ensure that the input size is consistent with the expected incremental processing\n        #  (i.e., one token at a time).\n        else:\n            # here we check if the input_ids are one token longer than the last time we processed\n            # but we don't check if input_ids are actually valid.\n            # Imagine a scenario where we generate 10 tokens, then we replace the 10 generated tokens with 10 new tokens.\n            # In this case, the input_ids will be consistent with the last_size, but the input_ids are not valid.\n            # However, should we really check if the input_ids are valid here?\n            # If we do, then we need to reparse the whole input_ids at each call, which is not efficient.\n            # Maybe we should just trust the user to provide valid input_ids?\n            # The conclusion is that, we assume the input_ids are valid, and our generation will be correct.\n            # If the input_ids are not valid, then the generation result will be wrong and we don't take responsibility for that.\n            raise RuntimeError(\n                \"Input ID's length is inconsistent with the current state of \"\n                \"the GrammarConstrainedLogitsProcessor. If you want to process \"\n                \"another input sequence, please instantiate a new \"\n                \"GrammarConstrainedLogitsProcessor.\"\n            )\n\n        self.filter_logits(scores, scores.device)\n\n        self.last_size = len(input_ids[0])\n        return scores\n\n    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        return self.process_logits(input_ids, scores)\n", "modules/grammar/grammar_utils.py": "'''\nThis file has been 100% copied from this PR to the Transformers library:\nhttps://github.com/huggingface/transformers/pull/27557\n\nAuthor: Saibo-creator\nAuthor GitHub: https://github.com/Saibo-creator\n\nAll credits go to the author.\n'''\n\nimport logging\nimport re\nimport time\nfrom abc import ABC\nfrom functools import lru_cache\nfrom typing import Dict, List\n\nimport torch\n\nfrom modules import shared\n\nlogger = logging.getLogger(__name__)\n\n\n########################\n# EBNF Grammar Parsing #\n########################\n\nEND_OF_ALTERNATE_MARKER = 0\nEND_OF_RULE_MARKER = 0\nTO_BE_FILLED_MARKER = 0\nREF_RULE_MARKER = 1\nLITERAL_MARKER = 2\n\n\nclass ParseState:\n    def __init__(self):\n        self.symbol_ids = {}\n        self.grammar_encoding = []  # old name: out_grammar\n\n\ndef get_symbol_id(state, src):\n    if src not in state.symbol_ids:\n        state.symbol_ids[src] = len(state.symbol_ids)\n    return state.symbol_ids[src]\n\n\ndef generate_symbol_id(state, base_name):\n    next_id = len(state.symbol_ids)\n    state.symbol_ids[base_name + \"_\" + str(next_id)] = next_id\n    return next_id\n\n\ndef is_word_char(c):\n    return c.isalnum() or c == \"-\" or c == \"_\"\n\n\ndef hex_to_int(c):\n    if c.isdigit():\n        return int(c)\n    elif \"a\" <= c.lower() <= \"f\":\n        return ord(c.lower()) - ord(\"a\") + 10\n    raise RuntimeError(\"unknown hex char \" + c)\n\n\ndef remove_leading_white_space(src, newline_ok):\n    \"\"\"\n    Skips over whitespace and comments in the input string.\n    This function processes the input string, skipping over any spaces, tabs,\n    and content following a '#' character, which denotes a comment. The parsing\n    of a comment continues until the end of the line (denoted by newline characters\n    '\\r' or '\\n'). If the 'newline_ok' parameter is set to False, the function\n    will stop processing and return the remaining string upon encountering a\n    newline character, otherwise it will skip over newline characters as well.\n    Parameters:\n    src (str): The input string to be processed.\n    newline_ok (bool): A flag indicating whether encountering a newline character\n                       should stop the parsing (False) or if it should be skipped (True).\n    Returns:\n    str: The remaining portion of the input string after skipping whitespace and comments.\n    \"\"\"\n    pos = 0\n    while pos < len(src) and (src[pos].isspace() or src[pos] == \"#\"):\n        if src[pos] == \"#\":\n            while pos < len(src) and src[pos] not in (\"\\r\", \"\\n\"):\n                pos += 1\n        else:\n            if not newline_ok and src[pos] in (\"\\r\", \"\\n\"):\n                break\n            pos += 1\n    return src[pos:]\n\n\ndef parse_name(src):\n    pos = 0\n    while pos < len(src) and is_word_char(src[pos]):\n        pos += 1\n    if pos == 0:\n        raise RuntimeError(\"expecting name at \" + src)\n    return src[:pos], src[pos:]\n\n\ndef read_hex(s):\n    val = 0\n    for c in s:\n        val = (val << 4) + hex_to_int(c)\n    return chr(val)\n\n\ndef parse_char(src):\n    \"\"\"\n    parse the leading char from the input string\n    :param src:\n    :return: char, remaining_src\n    \"\"\"\n\n    # if we have a backslash, it's maybe an escape\n    if src[0] == \"\\\\\":\n        esc = src[1]\n        if esc == \"x\":\n            return read_hex(src[2:4]), src[4:]\n        elif esc == \"u\":\n            return read_hex(src[2:6]), src[6:]\n        elif esc == \"U\":\n            return read_hex(src[2:10]), src[10:]\n        elif esc in ('\"', \"[\", \"]\", \"\\\\\", \"-\"):\n            return esc, src[2:]\n        elif esc == \"r\":\n            return \"\\r\", src[2:]\n        elif esc == \"n\":\n            return \"\\n\", src[2:]\n        elif esc == \"t\":\n            return \"\\t\", src[2:]\n        elif esc == \"\\\\\":\n            return \"\\\\\", src[2:]\n        raise RuntimeError(\"unknown escape at \" + src)\n    elif src:\n        return src[0], src[1:]\n    raise RuntimeError(\"unexpected end of input\")\n\n\ndef parse_sequence(state, src, rule_name, outbuf, is_nested):\n    out_start_pos = len(outbuf)\n\n    # sequence size, will be replaced at end when known\n    outbuf.append(TO_BE_FILLED_MARKER)\n\n    last_sym_start = len(outbuf)\n    remaining_src = src\n    while remaining_src:\n        if remaining_src[0] == '\"':  # literal string\n            remaining_src = remaining_src[1:]\n            last_sym_start = len(outbuf)\n            while remaining_src[0] != '\"':\n                char, remaining_src = parse_char(remaining_src)\n\n                # each char of a literal is encoded as a \"range\" of char - char\n                outbuf.append(LITERAL_MARKER)\n                outbuf.append(ord(char))\n                outbuf.append(ord(char))\n            remaining_src = remove_leading_white_space(remaining_src[1:], is_nested)\n        elif remaining_src[0] == \"[\":  # char range(s)\n            remaining_src = remaining_src[1:]\n            last_sym_start = len(outbuf)\n            # num chars in range - replaced at end of loop\n            outbuf.append(TO_BE_FILLED_MARKER)\n            while remaining_src[0] != \"]\":\n                char, remaining_src = parse_char(remaining_src)\n\n                outbuf.append(ord(char))\n                if remaining_src[0] == \"-\" and remaining_src[1] != \"]\":\n                    endchar_pair, remaining_src = parse_char(remaining_src[1:])\n                    outbuf.append(ord(endchar_pair))\n                else:\n                    # chars that aren't part of a c1-c2 range are just doubled (i.e., c-c)\n                    outbuf.append(ord(char))\n            # replace num chars with actual\n            outbuf[last_sym_start] = len(outbuf) - last_sym_start - 1\n            remaining_src = remove_leading_white_space(remaining_src[1:], is_nested)\n        elif is_word_char(remaining_src[0]):  # rule reference\n            name, remaining_src = parse_name(remaining_src)\n            ref_rule_id = get_symbol_id(state, name)\n            remaining_src = remove_leading_white_space(remaining_src, is_nested)\n            last_sym_start = len(outbuf)\n            outbuf.append(REF_RULE_MARKER)\n            outbuf.append(ref_rule_id)\n        elif remaining_src[0] == \"(\":  # grouping\n            # parse nested alternates into synthesized rule\n            remaining_src = remove_leading_white_space(remaining_src[1:], True)\n            sub_rule_id = generate_symbol_id(state, rule_name)\n            remaining_src = parse_alternates(state, remaining_src, rule_name, sub_rule_id, True)\n            last_sym_start = len(outbuf)\n            # output reference to synthesized rule\n            outbuf.append(REF_RULE_MARKER)\n            outbuf.append(sub_rule_id)\n            if remaining_src[0] != \")\":\n                raise RuntimeError(\"expecting ')' at \" + remaining_src)\n            remaining_src = remove_leading_white_space(remaining_src[1:], is_nested)\n        elif remaining_src[0] in (\"*\", \"+\", \"?\"):  # repetition operator\n            if len(outbuf) - out_start_pos - 1 == 0:\n                raise RuntimeError(\"expecting preceeding item to */+/? at \" + remaining_src)\n            out_grammar = state.grammar_encoding\n\n            # apply transformation to previous symbol (last_sym_start -\n            # end) according to rewrite rules:\n            # S* --> S' ::= S S' |\n            # S+ --> S' ::= S S' | S\n            # S? --> S' ::= S |\n            sub_rule_id = generate_symbol_id(state, rule_name)\n            out_grammar.append(sub_rule_id)\n            sub_rule_start = len(out_grammar)\n            # placeholder for size of 1st alternate\n            out_grammar.append(TO_BE_FILLED_MARKER)\n            # add preceding symbol to generated rule\n            out_grammar.extend(outbuf[last_sym_start:])\n            if remaining_src[0] in (\"*\", \"+\"):\n                # cause generated rule to recurse\n                out_grammar.append(REF_RULE_MARKER)\n                out_grammar.append(sub_rule_id)\n            # apply actual size\n            out_grammar[sub_rule_start] = len(out_grammar) - sub_rule_start\n            # mark end of 1st alternate\n            out_grammar.append(END_OF_ALTERNATE_MARKER)\n            sub_rule_start = len(out_grammar)\n            # placeholder for size of 2nd alternate\n            out_grammar.append(TO_BE_FILLED_MARKER)\n            if remaining_src[0] == \"+\":\n                # add preceding symbol as alternate only for '+'\n                out_grammar.extend(outbuf[last_sym_start:])\n            # apply actual size of 2nd alternate\n            out_grammar[sub_rule_start] = len(out_grammar) - sub_rule_start\n            # mark end of 2nd alternate, then end of rule\n            out_grammar.append(END_OF_ALTERNATE_MARKER)\n            out_grammar.append(END_OF_RULE_MARKER)\n\n            # in original rule, replace previous symbol with reference to generated rule\n            outbuf[last_sym_start:] = [1, sub_rule_id]\n\n            remaining_src = remove_leading_white_space(remaining_src[1:], is_nested)\n        else:\n            break\n    # apply actual size of this alternate sequence\n    outbuf[out_start_pos] = len(outbuf) - out_start_pos\n    # mark end of alternate\n    outbuf.append(END_OF_ALTERNATE_MARKER)\n    return remaining_src\n\n\ndef parse_alternates(state, src, rule_name, rule_id, is_nested):\n    outbuf = []\n    remaining_src = parse_sequence(state, src, rule_name, outbuf, is_nested)\n    while remaining_src and remaining_src[0] == \"|\":\n        remaining_src = remove_leading_white_space(remaining_src[1:], True)\n        remaining_src = parse_sequence(state, remaining_src, rule_name, outbuf, is_nested)\n\n    state.grammar_encoding.append(rule_id)\n    state.grammar_encoding.extend(outbuf)\n    state.grammar_encoding.append(0)\n    return remaining_src\n\n\ndef parse_rule(state, src):\n    name, remaining_src = parse_name(src)\n    remaining_src = remove_leading_white_space(remaining_src, False)\n    rule_id = get_symbol_id(state, name)\n\n    if remaining_src[:3] != \"::=\":\n        raise RuntimeError(\"expecting ::= at \" + remaining_src)\n    remaining_src = remove_leading_white_space(remaining_src[3:], True)\n\n    remaining_src = parse_alternates(state, remaining_src, name, rule_id, False)\n\n    if remaining_src and remaining_src[0] == \"\\r\":\n        remaining_src = remaining_src[2:] if remaining_src[1] == \"\\n\" else remaining_src[1:]\n    elif remaining_src and remaining_src[0] == \"\\n\":\n        remaining_src = remaining_src[1:]\n    elif remaining_src:\n        raise RuntimeError(\"expecting newline or end at \" + remaining_src)\n    return remove_leading_white_space(remaining_src, True)\n\n\ndef parse_ebnf(src):\n    try:\n        state = ParseState()\n        grammar_repr = remove_leading_white_space(src, True)\n        last_grammar_repr = \"\"\n        while grammar_repr:\n            if last_grammar_repr:\n                last_parsed_rule_len = len(last_grammar_repr) - len(grammar_repr)\n                logger.debug(f\"last_parsed_rule: {last_grammar_repr[:last_parsed_rule_len]}\")\n            last_grammar_repr = grammar_repr\n            grammar_repr = parse_rule(state, grammar_repr)\n        state.grammar_encoding.append(0xFFFF)\n        return state\n    except RuntimeError as err:\n        logger.warning(\"error parsing grammar:\", err)\n        return ParseState()\n\n\ndef print_rule(file, grammar_encoding, index, symbol_id_names):\n    rule_id = grammar_encoding[index]\n    print(f\"<{index}>{symbol_id_names[rule_id]} ::=\", end=\" \", file=file)\n    pos = index + 1\n    while grammar_encoding[pos]:\n        if pos - 1 > index:\n            print(\"|\", end=\" \", file=file)\n        pos += 1  # sequence size, not needed here\n        while grammar_encoding[pos]:\n            if grammar_encoding[pos] == REF_RULE_MARKER:\n                ref_rule_id = grammar_encoding[pos + 1]\n                print(\n                    f\"<{pos}>{symbol_id_names[ref_rule_id]}\",\n                    end=\" \",\n                    file=file,\n                )\n                pos += 2\n            else:\n                print(\"<{}>[\".format(pos), end=\"\", file=file)\n                num_chars = grammar_encoding[pos]\n                pos += 1\n\n                for i in range(0, num_chars, 2):\n                    print(\"{}-\".format(chr(grammar_encoding[pos + i])), end=\"\", file=file)\n                    if i + 1 < num_chars:\n                        print(\"{}\".format(chr(grammar_encoding[pos + i + 1])), end=\"\", file=file)\n                print(\"]\", end=\" \", file=file)\n                pos += num_chars\n        pos += 1\n    print(file=file)\n    return pos + 1\n\n\ndef print_grammar(file, state):\n    pos = 0\n    symbol_id_names = {v: k for k, v in state.symbol_ids.items()}\n    print(\"Grammar Rules:\", file=file)\n\n    while state.grammar_encoding[pos] != 0xFFFF:\n        pos = print_rule(file, state.grammar_encoding, pos, symbol_id_names)\n    pos = 0\n    print(\"\\nBinary representation:\", file=file)\n    while state.grammar_encoding[pos] != 0xFFFF:\n        print(f\"{state.grammar_encoding[pos]:04x}\", end=\" \", file=file)\n        pos += 1\n    print(\"ffff\\n\")\n\n\n###################################\n# EBNF Grammar Parsing ends here  #\n###################################\n\n\nclass GrammarConstraint(ABC):\n    def __init__(self, grammar_str, start_rule_name, tokenizer):\n        self.tt = 0\n        self.nt = 0\n        state = parse_ebnf(grammar_str)\n        grammar_encoding = state.grammar_encoding\n        self.start_rule_id = state.symbol_ids.get(start_rule_name)\n\n        self.eos_token_id = tokenizer.eos_token_id\n        self.token_trie = TokenTrie(tokenizer)\n        self.tokenizer = tokenizer\n        self.grammar_encoding = grammar_encoding\n\n        pos = 0\n        rules: Dict[int, int] = {}\n\n        while grammar_encoding[pos] != 0xFFFF:\n            rule_id = grammar_encoding[pos]\n\n            # Store the current position in the 'rules' list at the index corresponding to rule_id.\n            # This effectively maps each rule_id to its position in the grammar encoding.\n            rules[rule_id] = pos\n            pos += 1\n\n            # Continue to the next rule in the encoding.\n            # The loop advances by the size indicated at the current position (grammar_encoding[pos])\n            # plus one for the size field itself.\n            while grammar_encoding[pos]:\n                pos += 1 + grammar_encoding[pos]\n            # Now we're at the end of the rule,\n            # so advance to the next rule by skipping the 0, which means 'end of rule'.\n            pos += 1\n\n        self.start_rule_pos = rules[self.start_rule_id]\n        self.rules_pos_dict: Dict[int, int] = rules\n\n    def init_stacks(self):\n        # suppose the start rule position is 0, then grammar_encoding[0] = rule_id\n        # grammar_encoding[1] = rule_size\n        # grammar_encoding[2] = rule_type\n        # this is why we need to add 2 to the start rule position\n        stack = [self.start_rule_pos + 2]\n        # convert to tuple for caching(immutable)\n        return self.advance_stack(tuple(stack))\n\n    # For each stack, resolve rules to find the actual characters that are\n    # accepted by this stack (not the set of sub-rules).\n    # This is where the parsing happens.\n    # The parsing is a top-down, left-to-right, depth-first traversal of the\n    # grammar.\n    @lru_cache(maxsize=32768)\n    def advance_stack(self, stack):\n        stack = list(stack)\n        # If the stack is empty, we're done. Because no more tokens should be accepted.\n        if len(stack) == 0:\n            return [stack]\n\n        # Get the top of the stack.\n        pos = stack[-1]\n\n        # If the stack head is a terminal(literal), we can resolve it immediately.\n        # literal is marked with 2 in the grammar encoding.\n        if self.grammar_encoding[pos] > 1:\n            return [stack]\n\n        # The stack head is a nonterminal (a rule reference, 1 in the grammar encoding).\n        # Resolving this rule gives a set of one or more possible positions\n        # (e.g. two in `a ::= b | c`)\n        # We pop the current rule off the stack and, for each option, push:\n        # - the symbol following this symbol in the current rule; then\n        # - the first symbol of the resolved rule.\n        referenced_rule_id = self.grammar_encoding[pos + 1]\n\n        # subpos should points to the size of the subrule\n        subpos = self.rules_pos_dict[referenced_rule_id] + 1\n        stacks: List[List[int]] = []\n\n        # do depth-first search to find all possible rules and check the next terminal\n        # When this value is non-zero, it indicates that subpos is not yet at the end of the rule, so we can continue.\n        # here subpos is a pointer, and the value in the rule encoding can never be 0 except for the end of the rule.\n        while self.grammar_encoding[subpos]:\n            new_stack = stack[:-1]\n            if self.grammar_encoding[pos + 2]:\n                # check if there is a next symbol in the current rule, e.g. `a ::= b c | d`\n                # if yes, push the pos to rule_size to the stack\n                new_stack.append(pos + 2)\n\n            # if the type of the next symbol is not \"empty\", push the first symbol of the resolved rule to the stack\n            if self.grammar_encoding[subpos + 1]:\n                new_stack.append(subpos + 1)\n            stacks.extend(self.advance_stack(tuple(new_stack)))\n            # The increment subpos += self.grammar_encoding[subpos] + 1\n            # moves subpos forward in the grammar encoding array to the next alternative in the current rule.\n            subpos += self.grammar_encoding[subpos] + 1\n        return stacks\n\n    def accept_char(self, *args, **kwargs):\n        \"\"\"Process a byte according to the grammar rules.\"\"\"\n        raise NotImplementedError\n\n    def accept_token_id(self, *args, **kwargs):\n        \"\"\"Process a token according to the grammar rules.\"\"\"\n        raise NotImplementedError\n\n    def filter_vocab(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass IncrementalGrammarConstraint(GrammarConstraint):\n    def __init__(self, grammar_str, start_rule_name, tokenizer):\n        super().__init__(grammar_str, start_rule_name, tokenizer)\n\n    def accept_char(self, char, stacks):\n        byte = ord(char)\n        new_stacks = []\n        for stack in stacks:\n            # stack is empty\n            if not stack:\n                continue\n\n            pos = stack[-1]\n            num_chars = self.grammar_encoding[pos]\n\n            # to make pos point to the size of the char range rule\n            pos += 1\n            found = False\n            for i in range(0, num_chars, 2):\n                if self.grammar_encoding[pos + i] <= byte and byte <= self.grammar_encoding[pos + i + 1]:\n                    found = True\n                    break\n                if self.grammar_encoding[pos + i] >= byte and byte >= self.grammar_encoding[pos + i + 1]:\n                    found = True\n                    break\n            if not found:\n                continue\n\n            pos += num_chars\n            new_stack = stack[:-1]\n            if self.grammar_encoding[pos]:\n                new_stack.append(pos)\n            new_stacks.extend(self.advance_stack(tuple(new_stack)))\n\n        return new_stacks\n\n    def accept_string(self, string: str, stacks: List[List[int]]):\n        for char in string:\n            stacks = self.accept_char(char, stacks)\n        return stacks\n\n    def accept_token_id(self, token_id: int, stacks: List[List[int]]):\n        if token_id == self.eos_token_id:\n            if stacks and all(len(stack) != 0 for stack in stacks):\n                raise Exception(\n                    f\"At least one of the stack should be empty when EOS is reached. However, \"\n                    f\"the stacks are {stacks}\"\n                )\n            return []\n\n        for byte in self.token_trie.id2str(token_id):\n            stacks = self.accept_char(byte, stacks)\n            # check updated stacks\n            # TODO, I commented this out because it will fail when the stack is empty\n            # empty stack means the end of the grammar\n            # assert stacks != []\n\n        return stacks\n\n    def accept_token_ids(self, token_ids: List[int], stacks: List[List[int]], as_string=True):\n        if as_string:\n            string = self.tokenizer.decode(token_ids)\n            stacks = self.accept_string(string, stacks)\n        else:\n            for token_id in token_ids:\n                stacks = self.accept_token_id(token_id, stacks)\n        return stacks\n\n    def batch_filter_vocab(self, batch_stacks, device):\n        batch_acceptance = []\n        for stacks in batch_stacks:\n            batch_acceptance.append(self.filter_vocab(stacks, device))\n        return torch.stack(batch_acceptance)\n\n    def filter_vocab(self, stacks, device):\n        if not stacks:  # Check if stacks is empty\n            # Handle the empty case: for example, return a tensor of False\n            # The size of the tensor should match the size of your vocabulary\n            vocab_size = len(self.token_trie)\n            logger.debug(f\"sum of acceptance: {0}\")\n            return torch.zeros(vocab_size, dtype=torch.bool, device=device)\n\n        acceptance_matrix = torch.cat([self.token_acceptance_for_stack(tuple(stack), device) for stack in stacks])\n        # Merge stacks: any True => True\n        acceptance = acceptance_matrix.reshape(len(stacks), -1).any(dim=0)\n        logger.debug(f\"sum of acceptance: {acceptance.sum()}\")\n        return acceptance\n\n    # For each sub-rule in the grammar, cache whether each byte is accepted.\n    @lru_cache(maxsize=None)\n    def pos_char_acceptance(self, pos, char):\n        byte = ord(char)\n        num_chars = self.grammar_encoding[pos]\n        pos += 1\n        for i in range(0, num_chars, 2):\n            start = self.grammar_encoding[pos + i]\n            end = self.grammar_encoding[pos + i + 1]\n            if byte >= start and byte <= end:\n                return True\n            if byte <= start and byte >= end:\n                return True\n        return False\n\n    # Probably this should be configurable. If the grammar has an exceedingly\n    # large number of states, the correct setting is a tradeoff between GPU\n    # RAM usage and recomputation time.\n    #\n    # The main variable that pushes usage up here is number of states in the\n    # grammar.\n    @lru_cache(maxsize=32768)\n    def token_acceptance_for_stack(self, stack, device):\n        st = time.time()\n        stack = list(stack)  # needs to come in as a tuple for lru_cache\n\n        accepts = [False] * len(self.token_trie)\n        accepts[self.eos_token_id] = len(stack) == 0\n        if len(stack) == 0:\n            logger.debug(\"empty stack\")\n\n        def traverse_trie(trie, stacks):\n            for byte, next_trie in trie.items():\n                if byte == LEAF:\n                    token_id = next_trie\n                    if token_id != self.eos_token_id:\n                        accepts[token_id] = bool(stacks)\n                    continue\n\n                new_stacks = []\n                for stk in stacks:\n                    if not stk:\n                        continue\n\n                    pos = stk[-1]\n                    num_chars = self.grammar_encoding[pos]\n\n                    if not self.pos_char_acceptance(pos, byte):\n                        continue\n\n                    pos += num_chars + 1\n                    new_stack = stk[:-1]\n                    if self.grammar_encoding[pos]:\n                        new_stack.append(pos)\n                    new_stacks.extend(self.advance_stack(tuple(new_stack)))\n\n                if new_stacks:\n                    traverse_trie(next_trie, new_stacks)\n\n        traverse_trie(self.token_trie.trie, [stack])\n\n        et = time.time() - st\n        x = torch.tensor(accepts, dtype=torch.bool, device=device)\n        self.tt += et\n        self.nt += 1\n        return x\n\n\nclass StaticGrammarConstraint(GrammarConstraint):\n    def __init__(self, grammar_str, start_rule_name, tokenizer):\n        super().__init__(grammar_str, start_rule_name, tokenizer)\n\n    def accept_char(self):\n        raise NotImplementedError\n\n\n#################\n# DATA STRUCTURES\n#################\n\n\nLEAF = -1\n\n\nclass TokenTrie:\n    def __init__(self, tokenizer):\n        self.eos_token_id = tokenizer.eos_token_id\n        self.tokens = []\n        self.trie = {}\n        self.load_tokens(tokenizer)\n\n    def id2str(self, token_id):\n        return self.tokens[token_id]\n\n    def __len__(self):\n        return len(self.tokens)\n\n    def load_tokens(self, tokenizer):\n        def replace_hex(match):\n            hex_value = match.group(1)\n            return chr(int(hex_value, 16))\n\n        if \"gpt2\" in tokenizer.__class__.__name__.lower():\n            special = tokenizer.additional_special_tokens_ids\n\n            # Here, the decoder does a string replace on a bunch of sequences\n            # like ' .' for '.'. This interferes with our assumptions, where a\n            # token should always have exactly one representation.\n            # Fortunately(?) text-generation-inference doesn't seem to run this\n            # cleanup, so we get extraneous spaces. So, in order to generate\n            # the right token set for TGI, we have to skip the space trimming.\n            # See:\n            # https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L3588-L3600\n            def fmt_token(id):\n                if id in special:\n                    return None\n                return bytes(tokenizer.decode([id], clean_up_tokenization_spaces=False), \"utf-8\")\n\n        elif \"llama\" in tokenizer.__class__.__name__.lower():\n\n            def fmt_token(id):\n                token = tokenizer.convert_ids_to_tokens(id)\n                token = re.sub(r\"<0x([0-9a-fA-F]{2})>\", replace_hex, token)\n                token = token.replace(\"\u2581\", \" \")\n                return token\n\n        else:\n            print(\"Warning: unrecognized tokenizer: using default token formatting\")\n\n            def fmt_token(id):\n                token = tokenizer.convert_ids_to_tokens(id)\n                return token\n\n        # note: vocab_size doesn't work here because there are also\n        # get_added_vocab() tokens\n        self.tokens = [fmt_token(i) for i in range(len(tokenizer.get_vocab()))]\n        for token_id, token_bytes in enumerate(self.tokens):\n            if token_bytes is not None:\n                self.insert_into_trie(self.trie, token_bytes, token_id)\n\n    def insert_into_trie(self, trie, token_bytes, token_id):\n        current = trie\n        for byte in token_bytes:\n            if byte not in current:\n                current[byte] = {}\n            current = current[byte]\n        current[LEAF] = token_id\n\n\n@lru_cache(maxsize=5)\ndef initialize_grammar(grammar_string):\n    return IncrementalGrammarConstraint(grammar_string.strip(), start_rule_name=\"root\", tokenizer=shared.tokenizer)\n"}