{"setup.py": "from pathlib import Path\n\nfrom setuptools import find_packages, setup\n\nversion = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n\n\ninstall_requires = [\n    \"Twisted>=18.9.0\",\n    \"cryptography>=36.0.0\",\n    \"cssselect>=0.9.1\",\n    \"itemloaders>=1.0.1\",\n    \"parsel>=1.5.0\",\n    \"pyOpenSSL>=21.0.0\",\n    \"queuelib>=1.4.2\",\n    \"service_identity>=18.1.0\",\n    \"w3lib>=1.17.0\",\n    \"zope.interface>=5.1.0\",\n    \"protego>=0.1.15\",\n    \"itemadapter>=0.1.0\",\n    \"setuptools\",\n    \"packaging\",\n    \"tldextract\",\n    \"lxml>=4.4.1\",\n    \"defusedxml>=0.7.1\",\n]\nextras_require = {\n    ':platform_python_implementation == \"CPython\"': [\"PyDispatcher>=2.0.5\"],\n    ':platform_python_implementation == \"PyPy\"': [\"PyPyDispatcher>=2.1.0\"],\n}\n\n\nsetup(\n    name=\"Scrapy\",\n    version=version,\n    url=\"https://scrapy.org\",\n    project_urls={\n        \"Documentation\": \"https://docs.scrapy.org/\",\n        \"Source\": \"https://github.com/scrapy/scrapy\",\n        \"Tracker\": \"https://github.com/scrapy/scrapy/issues\",\n    },\n    description=\"A high-level Web Crawling and Web Scraping framework\",\n    long_description=open(\"README.rst\", encoding=\"utf-8\").read(),\n    author=\"Scrapy developers\",\n    author_email=\"pablo@pablohoffman.com\",\n    maintainer=\"Pablo Hoffman\",\n    maintainer_email=\"pablo@pablohoffman.com\",\n    license=\"BSD\",\n    packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n    include_package_data=True,\n    zip_safe=False,\n    entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n    classifiers=[\n        \"Framework :: Scrapy\",\n        \"Development Status :: 5 - Production/Stable\",\n        \"Environment :: Console\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=install_requires,\n    extras_require=extras_require,\n)\n", "extras/qps-bench-server.py": "#!/usr/bin/env python\nfrom collections import deque\nfrom time import time\n\nfrom twisted.internet import reactor\nfrom twisted.web.resource import Resource\nfrom twisted.web.server import NOT_DONE_YET, Site\n\n\nclass Root(Resource):\n    def __init__(self):\n        Resource.__init__(self)\n        self.concurrent = 0\n        self.tail = deque(maxlen=100)\n        self._reset_stats()\n\n    def _reset_stats(self):\n        self.tail.clear()\n        self.start = self.lastmark = self.lasttime = time()\n\n    def getChild(self, request, name):\n        return self\n\n    def render(self, request):\n        now = time()\n        delta = now - self.lasttime\n\n        # reset stats on high iter-request times caused by client restarts\n        if delta > 3:  # seconds\n            self._reset_stats()\n            return \"\"\n\n        self.tail.appendleft(delta)\n        self.lasttime = now\n        self.concurrent += 1\n\n        if now - self.lastmark >= 3:\n            self.lastmark = now\n            qps = len(self.tail) / sum(self.tail)\n            print(\n                f\"samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}\"\n            )\n\n        if \"latency\" in request.args:\n            latency = float(request.args[\"latency\"][0])\n            reactor.callLater(latency, self._finish, request)\n            return NOT_DONE_YET\n\n        self.concurrent -= 1\n        return \"\"\n\n    def _finish(self, request):\n        self.concurrent -= 1\n        if not request.finished and not request._disconnected:\n            request.finish()\n\n\nroot = Root()\nfactory = Site(root)\nreactor.listenTCP(8880, factory)\nreactor.run()\n", "extras/qpsclient.py": "\"\"\"\nA spider that generate light requests to measure QPS throughput\n\nusage:\n\n    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0\n     --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n\n\"\"\"\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\n\n\nclass QPSSpider(Spider):\n    name = \"qps\"\n    benchurl = \"http://localhost:8880/\"\n\n    # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n    max_concurrent_requests = 8\n    # Requests per second goal\n    qps = None  # same as: 1 / download_delay\n    download_delay = None\n    # time in seconds to delay server responses\n    latency = None\n    # number of slots to create\n    slots = 1\n\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        if self.qps is not None:\n            self.qps = float(self.qps)\n            self.download_delay = 1 / self.qps\n        elif self.download_delay is not None:\n            self.download_delay = float(self.download_delay)\n\n    def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += f\"?latency={self.latency}\"\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace(\"localhost\", f\"127.0.0.{x + 1}\") for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1\n\n    def parse(self, response):\n        pass\n", "scrapy/shell.py": "\"\"\"Scrapy Shell\n\nSee documentation in docs/topics/shell.rst\n\n\"\"\"\n\nimport os\nimport signal\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nfrom itemadapter import is_item\nfrom twisted.internet import defer, threads\nfrom twisted.python import threadable\nfrom w3lib.url import any_to_uri\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.conf import get_config\nfrom scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\nfrom scrapy.utils.response import open_in_browser\n\n\nclass Shell:\n    relevant_classes: Tuple[type, ...] = (Crawler, Spider, Request, Response, Settings)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        update_vars: Optional[Callable[[Dict[str, Any]], None]] = None,\n        code: Optional[str] = None,\n    ):\n        self.crawler: Crawler = crawler\n        self.update_vars: Callable[[Dict[str, Any]], None] = update_vars or (\n            lambda x: None\n        )\n        self.item_class: type = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n        self.spider: Optional[Spider] = None\n        self.inthread: bool = not threadable.isInIOThread()\n        self.code: Optional[str] = code\n        self.vars: Dict[str, Any] = {}\n\n    def start(\n        self,\n        url: Optional[str] = None,\n        request: Optional[Request] = None,\n        response: Optional[Response] = None,\n        spider: Optional[Spider] = None,\n        redirect: bool = True,\n    ) -> None:\n        # disable accidental Ctrl-C key press from shutting down the engine\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        if url:\n            self.fetch(url, spider, redirect=redirect)\n        elif request:\n            self.fetch(request, spider)\n        elif response:\n            request = response.request\n            self.populate_vars(response, request, spider)\n        else:\n            self.populate_vars()\n        if self.code:\n            print(eval(self.code, globals(), self.vars))  # nosec\n        else:\n            \"\"\"\n            Detect interactive shell setting in scrapy.cfg\n            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n            [settings]\n            # shell can be one of ipython, bpython or python;\n            # to be used as the interactive python console, if available.\n            # (default is ipython, fallbacks in the order listed above)\n            shell = python\n            \"\"\"\n            cfg = get_config()\n            section, option = \"settings\", \"shell\"\n            env = os.environ.get(\"SCRAPY_PYTHON_SHELL\")\n            shells = []\n            if env:\n                shells += env.strip().lower().split(\",\")\n            elif cfg.has_option(section, option):\n                shells += [cfg.get(section, option).strip().lower()]\n            else:  # try all by default\n                shells += DEFAULT_PYTHON_SHELLS.keys()\n            # always add standard shell as fallback\n            shells += [\"python\"]\n            start_python_console(\n                self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n            )\n\n    def _schedule(self, request: Request, spider: Optional[Spider]) -> defer.Deferred:\n        if is_asyncio_reactor_installed():\n            # set the asyncio event loop for the current thread\n            event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n            set_asyncio_event_loop(event_loop_path)\n        spider = self._open_spider(request, spider)\n        d = _request_deferred(request)\n        d.addCallback(lambda x: (x, spider))\n        assert self.crawler.engine\n        self.crawler.engine.crawl(request)\n        return d\n\n    def _open_spider(self, request: Request, spider: Optional[Spider]) -> Spider:\n        if self.spider:\n            return self.spider\n\n        if spider is None:\n            spider = self.crawler.spider or self.crawler._create_spider()\n\n        self.crawler.spider = spider\n        assert self.crawler.engine\n        self.crawler.engine.open_spider(spider, close_if_idle=False)\n        self.spider = spider\n        return spider\n\n    def fetch(\n        self,\n        request_or_url: Union[Request, str],\n        spider: Optional[Spider] = None,\n        redirect: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        from twisted.internet import reactor\n\n        if isinstance(request_or_url, Request):\n            request = request_or_url\n        else:\n            url = any_to_uri(request_or_url)\n            request = Request(url, dont_filter=True, **kwargs)\n            if redirect:\n                request.meta[\"handle_httpstatus_list\"] = SequenceExclude(\n                    range(300, 400)\n                )\n            else:\n                request.meta[\"handle_httpstatus_all\"] = True\n        response = None\n        try:\n            response, spider = threads.blockingCallFromThread(\n                reactor, self._schedule, request, spider\n            )\n        except IgnoreRequest:\n            pass\n        self.populate_vars(response, request, spider)\n\n    def populate_vars(\n        self,\n        response: Optional[Response] = None,\n        request: Optional[Request] = None,\n        spider: Optional[Spider] = None,\n    ) -> None:\n        import scrapy\n\n        self.vars[\"scrapy\"] = scrapy\n        self.vars[\"crawler\"] = self.crawler\n        self.vars[\"item\"] = self.item_class()\n        self.vars[\"settings\"] = self.crawler.settings\n        self.vars[\"spider\"] = spider\n        self.vars[\"request\"] = request\n        self.vars[\"response\"] = response\n        if self.inthread:\n            self.vars[\"fetch\"] = self.fetch\n        self.vars[\"view\"] = open_in_browser\n        self.vars[\"shelp\"] = self.print_help\n        self.update_vars(self.vars)\n        if not self.code:\n            self.vars[\"banner\"] = self.get_help()\n\n    def print_help(self) -> None:\n        print(self.get_help())\n\n    def get_help(self) -> str:\n        b = []\n        b.append(\"Available Scrapy objects:\")\n        b.append(\n            \"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\"\n        )\n        for k, v in sorted(self.vars.items()):\n            if self._is_relevant(v):\n                b.append(f\"  {k:<10} {v}\")\n        b.append(\"Useful shortcuts:\")\n        if self.inthread:\n            b.append(\n                \"  fetch(url[, redirect=True]) \"\n                \"Fetch URL and update local objects (by default, redirects are followed)\"\n            )\n            b.append(\n                \"  fetch(req)                  \"\n                \"Fetch a scrapy.Request and update local objects \"\n            )\n        b.append(\"  shelp()           Shell help (print this help)\")\n        b.append(\"  view(response)    View response in a browser\")\n\n        return \"\\n\".join(f\"[s] {line}\" for line in b)\n\n    def _is_relevant(self, value: Any) -> bool:\n        return isinstance(value, self.relevant_classes) or is_item(value)\n\n\ndef inspect_response(response: Response, spider: Spider) -> None:\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    # Shell.start removes the SIGINT handler, so save it and re-add it after\n    # the shell has closed\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    Shell(spider.crawler).start(response=response, spider=spider)\n    signal.signal(signal.SIGINT, sigint_handler)\n\n\ndef _request_deferred(request: Request) -> defer.Deferred:\n    \"\"\"Wrap a request inside a Deferred.\n\n    This function is harmful, do not use it until you know what you are doing.\n\n    This returns a Deferred whose first pair of callbacks are the request\n    callback and errback. The Deferred also triggers when the request\n    callback/errback is executed (i.e. when the request is downloaded)\n\n    WARNING: Do not call request.replace() until after the deferred is called.\n    \"\"\"\n    request_callback = request.callback\n    request_errback = request.errback\n\n    def _restore_callbacks(result: Any) -> Any:\n        request.callback = request_callback\n        request.errback = request_errback\n        return result\n\n    d: defer.Deferred = defer.Deferred()\n    d.addBoth(_restore_callbacks)\n    if request.callback:\n        d.addCallback(request.callback)\n    if request.errback:\n        d.addErrback(request.errback)\n\n    request.callback, request.errback = d.callback, d.errback\n    return d\n", "scrapy/dupefilters.py": "from __future__ import annotations\n\nimport logging\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Optional, Set\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.http.request import Request\nfrom scrapy.settings import BaseSettings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.request import (\n    RequestFingerprinter,\n    RequestFingerprinterProtocol,\n    referer_str,\n)\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nclass BaseDupeFilter:\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls()\n\n    def request_seen(self, request: Request) -> bool:\n        return False\n\n    def open(self) -> Optional[Deferred]:\n        pass\n\n    def close(self, reason: str) -> Optional[Deferred]:\n        pass\n\n    def log(self, request: Request, spider: Spider) -> None:\n        \"\"\"Log that a request has been filtered\"\"\"\n        pass\n\n\nclass RFPDupeFilter(BaseDupeFilter):\n    \"\"\"Request Fingerprint duplicates filter\"\"\"\n\n    def __init__(\n        self,\n        path: Optional[str] = None,\n        debug: bool = False,\n        *,\n        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n    ) -> None:\n        self.file = None\n        self.fingerprinter: RequestFingerprinterProtocol = (\n            fingerprinter or RequestFingerprinter()\n        )\n        self.fingerprints: Set[str] = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = Path(path, \"requests.seen\").open(\"a+\", encoding=\"utf-8\")\n            self.file.seek(0)\n            self.fingerprints.update(x.rstrip() for x in self.file)\n\n    @classmethod\n    def from_settings(\n        cls,\n        settings: BaseSettings,\n        *,\n        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n    ) -> Self:\n        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.request_fingerprinter\n        return cls.from_settings(\n            crawler.settings,\n            fingerprinter=crawler.request_fingerprinter,\n        )\n\n    def request_seen(self, request: Request) -> bool:\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + \"\\n\")\n        return False\n\n    def request_fingerprint(self, request: Request) -> str:\n        return self.fingerprinter.fingerprint(request).hex()\n\n    def close(self, reason: str) -> None:\n        if self.file:\n            self.file.close()\n\n    def log(self, request: Request, spider: Spider) -> None:\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n            args = {\"request\": request, \"referer\": referer_str(request)}\n            self.logger.debug(msg, args, extra={\"spider\": spider})\n        elif self.logdupes:\n            msg = (\n                \"Filtered duplicate request: %(request)s\"\n                \" - no more duplicates will be shown\"\n                \" (see DUPEFILTER_DEBUG to show all duplicates)\"\n            )\n            self.logger.debug(msg, {\"request\": request}, extra={\"spider\": spider})\n            self.logdupes = False\n\n        assert spider.crawler.stats\n        spider.crawler.stats.inc_value(\"dupefilter/filtered\", spider=spider)\n", "scrapy/spiderloader.py": "from __future__ import annotations\n\nimport traceback\nimport warnings\nfrom collections import defaultdict\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type\n\nfrom zope.interface import implementer\n\nfrom scrapy import Request, Spider\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import walk_modules\nfrom scrapy.utils.spider import iter_spider_classes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n@implementer(ISpiderLoader)\nclass SpiderLoader:\n    \"\"\"\n    SpiderLoader is a class which locates and loads spiders\n    in a Scrapy project.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        self.spider_modules: List[str] = settings.getlist(\"SPIDER_MODULES\")\n        self.warn_only: bool = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n        self._spiders: Dict[str, Type[Spider]] = {}\n        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n        self._load_all_spiders()\n\n    def _check_name_duplicates(self) -> None:\n        dupes = []\n        for name, locations in self._found.items():\n            dupes.extend(\n                [\n                    f\"  {cls} named {name!r} (in {mod})\"\n                    for mod, cls in locations\n                    if len(locations) > 1\n                ]\n            )\n\n        if dupes:\n            dupes_string = \"\\n\\n\".join(dupes)\n            warnings.warn(\n                \"There are several spiders with the same name:\\n\\n\"\n                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                category=UserWarning,\n            )\n\n    def _load_spiders(self, module: ModuleType) -> None:\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls\n\n    def _load_all_spiders(self) -> None:\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError:\n                if self.warn_only:\n                    warnings.warn(\n                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n                        f\"from module '{name}'. \"\n                        \"See above traceback for details.\",\n                        category=RuntimeWarning,\n                    )\n                else:\n                    raise\n        self._check_name_duplicates()\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(settings)\n\n    def load(self, spider_name: str) -> Type[Spider]:\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(f\"Spider not found: {spider_name}\")\n\n    def find_by_request(self, request: Request) -> List[str]:\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [\n            name for name, cls in self._spiders.items() if cls.handles_request(request)\n        ]\n\n    def list(self) -> List[str]:\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())\n", "scrapy/exceptions.py": "\"\"\"\nScrapy core exceptions\n\nThese exceptions are documented in docs/topics/exceptions.rst. Please don't add\nnew exceptions here without documenting them there.\n\"\"\"\n\nfrom typing import Any\n\n# Internal\n\n\nclass NotConfigured(Exception):\n    \"\"\"Indicates a missing configuration situation\"\"\"\n\n    pass\n\n\nclass _InvalidOutput(TypeError):\n    \"\"\"\n    Indicates an invalid value has been returned by a middleware's processing method.\n    Internal and undocumented, it should not be raised or caught by user code.\n    \"\"\"\n\n    pass\n\n\n# HTTP and crawling\n\n\nclass IgnoreRequest(Exception):\n    \"\"\"Indicates a decision was made not to process a request\"\"\"\n\n\nclass DontCloseSpider(Exception):\n    \"\"\"Request the spider not to be closed yet\"\"\"\n\n    pass\n\n\nclass CloseSpider(Exception):\n    \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n\n    def __init__(self, reason: str = \"cancelled\"):\n        super().__init__()\n        self.reason = reason\n\n\nclass StopDownload(Exception):\n    \"\"\"\n    Stop the download of the body for a given response.\n    The 'fail' boolean parameter indicates whether or not the resulting partial response\n    should be handled by the request errback. Note that 'fail' is a keyword-only argument.\n    \"\"\"\n\n    def __init__(self, *, fail: bool = True):\n        super().__init__()\n        self.fail = fail\n\n\n# Items\n\n\nclass DropItem(Exception):\n    \"\"\"Drop item from the item pipeline\"\"\"\n\n    pass\n\n\nclass NotSupported(Exception):\n    \"\"\"Indicates a feature or method is not supported\"\"\"\n\n    pass\n\n\n# Commands\n\n\nclass UsageError(Exception):\n    \"\"\"To indicate a command-line usage error\"\"\"\n\n    def __init__(self, *a: Any, **kw: Any):\n        self.print_help = kw.pop(\"print_help\", True)\n        super().__init__(*a, **kw)\n\n\nclass ScrapyDeprecationWarning(Warning):\n    \"\"\"Warning category for deprecated features, since the default\n    DeprecationWarning is silenced on Python 2.7+\n    \"\"\"\n\n    pass\n\n\nclass ContractFail(AssertionError):\n    \"\"\"Error raised in case of a failing contract\"\"\"\n\n    pass\n", "scrapy/interfaces.py": "from zope.interface import Interface\n\n\nclass ISpiderLoader(Interface):\n    def from_settings(settings):\n        \"\"\"Return an instance of the class for the given settings\"\"\"\n\n    def load(spider_name):\n        \"\"\"Return the Spider class for the given spider name. If the spider\n        name is not found, it must raise a KeyError.\"\"\"\n\n    def list():\n        \"\"\"Return a list with the names of all spiders available in the\n        project\"\"\"\n\n    def find_by_request(request):\n        \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n", "scrapy/statscollectors.py": "\"\"\"\nScrapy extension for collecting scraping stats\n\"\"\"\n\nimport logging\nimport pprint\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nfrom scrapy import Spider\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nStatsT = Dict[str, Any]\n\n\nclass StatsCollector:\n    def __init__(self, crawler: \"Crawler\"):\n        self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n        self._stats: StatsT = {}\n\n    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return self._stats.get(key, default)\n\n    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:\n        return self._stats\n\n    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = value\n\n    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        self._stats = stats\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count\n\n    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = max(self._stats.setdefault(key, value), value)\n\n    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = min(self._stats.setdefault(key, value), value)\n\n    def clear_stats(self, spider: Optional[Spider] = None) -> None:\n        self._stats.clear()\n\n    def open_spider(self, spider: Spider) -> None:\n        pass\n\n    def close_spider(self, spider: Spider, reason: str) -> None:\n        if self._dump:\n            logger.info(\n                \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                extra={\"spider\": spider},\n            )\n        self._persist_stats(self._stats, spider)\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        pass\n\n\nclass MemoryStatsCollector(StatsCollector):\n    def __init__(self, crawler: \"Crawler\"):\n        super().__init__(crawler)\n        self.spider_stats: Dict[str, StatsT] = {}\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        self.spider_stats[spider.name] = stats\n\n\nclass DummyStatsCollector(StatsCollector):\n    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return default\n\n    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        pass\n\n    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n", "scrapy/resolver.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, List, Optional, Sequence, Type\n\nfrom twisted.internet import defer\nfrom twisted.internet.base import ReactorBase, ThreadedResolver\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostnameResolver,\n    IHostResolution,\n    IResolutionReceiver,\n    IResolverSimple,\n)\nfrom zope.interface.declarations import implementer, provider\n\nfrom scrapy.utils.datatypes import LocalCache\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n# TODO: cache misses\ndnscache: LocalCache[str, Any] = LocalCache(10000)\n\n\n@implementer(IResolverSimple)\nclass CachingThreadedResolver(ThreadedResolver):\n    \"\"\"\n    Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.\n    \"\"\"\n\n    def __init__(self, reactor: ReactorBase, cache_size: int, timeout: float):\n        super().__init__(reactor)\n        dnscache.limit = cache_size\n        self.timeout = timeout\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size, crawler.settings.getfloat(\"DNS_TIMEOUT\"))\n\n    def install_on_reactor(self) -> None:\n        self.reactor.installResolver(self)\n\n    def getHostByName(self, name: str, timeout: Sequence[int] = ()) -> Deferred[str]:\n        if name in dnscache:\n            return defer.succeed(dnscache[name])\n        # in Twisted<=16.6, getHostByName() is always called with\n        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n        # so the input argument above is simply overridden\n        # to enforce Scrapy's DNS_TIMEOUT setting's value\n        # The timeout arg is typed as Sequence[int] but supports floats.\n        timeout = (self.timeout,)  # type: ignore[assignment]\n        d = super().getHostByName(name, timeout)\n        if dnscache.limit:\n            d.addCallback(self._cache_result, name)\n        return d\n\n    def _cache_result(self, result: Any, name: str) -> Any:\n        dnscache[name] = result\n        return result\n\n\n@implementer(IHostResolution)\nclass HostResolution:\n    def __init__(self, name: str):\n        self.name: str = name\n\n    def cancel(self) -> None:\n        raise NotImplementedError()\n\n\n@provider(IResolutionReceiver)\nclass _CachingResolutionReceiver:\n    def __init__(self, resolutionReceiver: IResolutionReceiver, hostName: str):\n        self.resolutionReceiver: IResolutionReceiver = resolutionReceiver\n        self.hostName: str = hostName\n        self.addresses: List[IAddress] = []\n\n    def resolutionBegan(self, resolution: IHostResolution) -> None:\n        self.resolutionReceiver.resolutionBegan(resolution)\n        self.resolution = resolution\n\n    def addressResolved(self, address: IAddress) -> None:\n        self.resolutionReceiver.addressResolved(address)\n        self.addresses.append(address)\n\n    def resolutionComplete(self) -> None:\n        self.resolutionReceiver.resolutionComplete()\n        if self.addresses:\n            dnscache[self.hostName] = self.addresses\n\n\n@implementer(IHostnameResolver)\nclass CachingHostnameResolver:\n    \"\"\"\n    Experimental caching resolver. Resolves IPv4 and IPv6 addresses,\n    does not support setting a timeout value for DNS requests.\n    \"\"\"\n\n    def __init__(self, reactor: ReactorBase, cache_size: int):\n        self.reactor: ReactorBase = reactor\n        self.original_resolver: IHostnameResolver = reactor.nameResolver\n        dnscache.limit = cache_size\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size)\n\n    def install_on_reactor(self) -> None:\n        self.reactor.installNameResolver(self)\n\n    def resolveHostName(\n        self,\n        resolutionReceiver: IResolutionReceiver,\n        hostName: str,\n        portNumber: int = 0,\n        addressTypes: Optional[Sequence[Type[IAddress]]] = None,\n        transportSemantics: str = \"TCP\",\n    ) -> IHostResolution:\n        try:\n            addresses = dnscache[hostName]\n        except KeyError:\n            return self.original_resolver.resolveHostName(\n                _CachingResolutionReceiver(resolutionReceiver, hostName),\n                hostName,\n                portNumber,\n                addressTypes,\n                transportSemantics,\n            )\n        else:\n            resolutionReceiver.resolutionBegan(HostResolution(hostName))\n            for addr in addresses:\n                resolutionReceiver.addressResolved(addr)\n            resolutionReceiver.resolutionComplete()\n            return resolutionReceiver\n", "scrapy/extension.py": "\"\"\"\nThe Extension Manager\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom typing import Any, List\n\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import build_component_list\n\n\nclass ExtensionManager(MiddlewareManager):\n    component_name = \"extension\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"EXTENSIONS\"))\n", "scrapy/logformatter.py": "from __future__ import annotations\n\nimport logging\nimport os\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, TypedDict, Union\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import Response\nfrom scrapy.utils.request import referer_str\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nSCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\nDROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\nCRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\nITEMERRORMSG = \"Error processing %(item)s\"\nSPIDERERRORMSG = \"Spider error processing %(request)s (referer: %(referer)s)\"\nDOWNLOADERRORMSG_SHORT = \"Error downloading %(request)s\"\nDOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n\n\nclass LogFormatterResult(TypedDict):\n    level: int\n    msg: str\n    args: Union[Dict[str, Any], Tuple[Any, ...]]\n\n\nclass LogFormatter:\n    \"\"\"Class for generating log messages for different actions.\n\n    All methods must return a dictionary listing the parameters ``level``, ``msg``\n    and ``args`` which are going to be used for constructing the log message when\n    calling ``logging.log``.\n\n    Dictionary keys for the method outputs:\n\n    *   ``level`` is the log level for that action, you can use those from the\n        `python logging library <https://docs.python.org/3/library/logging.html>`_ :\n        ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR``\n        and ``logging.CRITICAL``.\n    *   ``msg`` should be a string that can contain different formatting placeholders.\n        This string, formatted with the provided ``args``, is going to be the long message\n        for that action.\n    *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.\n        The final log message is computed as ``msg % args``.\n\n    Users can define their own ``LogFormatter`` class if they want to customize how\n    each action is logged or if they want to omit it entirely. In order to omit\n    logging an action the method must return ``None``.\n\n    Here is an example on how to create a custom log formatter to lower the severity level of\n    the log message when an item is dropped from the pipeline::\n\n            class PoliteLogFormatter(logformatter.LogFormatter):\n                def dropped(self, item, exception, response, spider):\n                    return {\n                        'level': logging.INFO, # lowering the level from logging.WARNING\n                        'msg': \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\",\n                        'args': {\n                            'exception': exception,\n                            'item': item,\n                        }\n                    }\n    \"\"\"\n\n    def crawled(\n        self, request: Request, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n        request_flags = f\" {str(request.flags)}\" if request.flags else \"\"\n        response_flags = f\" {str(response.flags)}\" if response.flags else \"\"\n        return {\n            \"level\": logging.DEBUG,\n            \"msg\": CRAWLEDMSG,\n            \"args\": {\n                \"status\": response.status,\n                \"request\": request,\n                \"request_flags\": request_flags,\n                \"referer\": referer_str(request),\n                \"response_flags\": response_flags,\n                # backward compatibility with Scrapy logformatter below 1.4 version\n                \"flags\": response_flags,\n            },\n        }\n\n    def scraped(\n        self, item: Any, response: Union[Response, Failure], spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n        src: Any\n        if isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            \"level\": logging.DEBUG,\n            \"msg\": SCRAPEDMSG,\n            \"args\": {\n                \"src\": src,\n                \"item\": item,\n            },\n        }\n\n    def dropped(\n        self, item: Any, exception: BaseException, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n        return {\n            \"level\": logging.WARNING,\n            \"msg\": DROPPEDMSG,\n            \"args\": {\n                \"exception\": exception,\n                \"item\": item,\n            },\n        }\n\n    def item_error(\n        self, item: Any, exception: BaseException, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item causes an error while it is passing\n        through the item pipeline.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": ITEMERRORMSG,\n            \"args\": {\n                \"item\": item,\n            },\n        }\n\n    def spider_error(\n        self,\n        failure: Failure,\n        request: Request,\n        response: Union[Response, Failure],\n        spider: Spider,\n    ) -> LogFormatterResult:\n        \"\"\"Logs an error message from a spider.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": SPIDERERRORMSG,\n            \"args\": {\n                \"request\": request,\n                \"referer\": referer_str(request),\n            },\n        }\n\n    def download_error(\n        self,\n        failure: Failure,\n        request: Request,\n        spider: Spider,\n        errmsg: Optional[str] = None,\n    ) -> LogFormatterResult:\n        \"\"\"Logs a download error message from a spider (typically coming from\n        the engine).\n\n        .. versionadded:: 2.0\n        \"\"\"\n        args: Dict[str, Any] = {\"request\": request}\n        if errmsg:\n            msg = DOWNLOADERRORMSG_LONG\n            args[\"errmsg\"] = errmsg\n        else:\n            msg = DOWNLOADERRORMSG_SHORT\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": msg,\n            \"args\": args,\n        }\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls()\n", "scrapy/signals.py": "\"\"\"\nScrapy signals\n\nThese signals are documented in docs/topics/signals.rst. Please don't add new\nsignals here without documenting them there.\n\"\"\"\n\nengine_started = object()\nengine_stopped = object()\nspider_opened = object()\nspider_idle = object()\nspider_closed = object()\nspider_error = object()\nrequest_scheduled = object()\nrequest_dropped = object()\nrequest_reached_downloader = object()\nrequest_left_downloader = object()\nresponse_received = object()\nresponse_downloaded = object()\nheaders_received = object()\nbytes_received = object()\nitem_scraped = object()\nitem_dropped = object()\nitem_error = object()\nfeed_slot_closed = object()\nfeed_exporter_closed = object()\n\n# for backward compatibility\nstats_spider_opened = spider_opened\nstats_spider_closing = spider_closed\nstats_spider_closed = spider_closed\n\nitem_passed = item_scraped\n\nrequest_received = request_scheduled\n", "scrapy/cmdline.py": "from __future__ import annotations\n\nimport argparse\nimport cProfile\nimport inspect\nimport os\nimport sys\nfrom importlib.metadata import entry_points\nfrom typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Type\n\nimport scrapy\nfrom scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.exceptions import UsageError\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.misc import walk_modules\nfrom scrapy.utils.project import get_project_settings, inside_project\nfrom scrapy.utils.python import garbage_collect\n\nif TYPE_CHECKING:\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n\nclass ScrapyArgumentParser(argparse.ArgumentParser):\n    def _parse_optional(\n        self, arg_string: str\n    ) -> Optional[Tuple[Optional[argparse.Action], str, Optional[str]]]:\n        # if starts with -: it means that is a parameter not a argument\n        if arg_string[:2] == \"-:\":\n            return None\n\n        return super()._parse_optional(arg_string)\n\n\ndef _iter_command_classes(module_name: str) -> Iterable[Type[ScrapyCommand]]:\n    # TODO: add `name` attribute to commands and merge this function with\n    # scrapy.utils.spider.iter_spider_classes\n    for module in walk_modules(module_name):\n        for obj in vars(module).values():\n            if (\n                inspect.isclass(obj)\n                and issubclass(obj, ScrapyCommand)\n                and obj.__module__ == module.__name__\n                and obj not in (ScrapyCommand, BaseRunSpiderCommand)\n            ):\n                yield obj\n\n\ndef _get_commands_from_module(module: str, inproject: bool) -> Dict[str, ScrapyCommand]:\n    d: Dict[str, ScrapyCommand] = {}\n    for cmd in _iter_command_classes(module):\n        if inproject or not cmd.requires_project:\n            cmdname = cmd.__module__.split(\".\")[-1]\n            d[cmdname] = cmd()\n    return d\n\n\ndef _get_commands_from_entry_points(\n    inproject: bool, group: str = \"scrapy.commands\"\n) -> Dict[str, ScrapyCommand]:\n    cmds: Dict[str, ScrapyCommand] = {}\n    if sys.version_info >= (3, 10):\n        eps = entry_points(group=group)\n    else:\n        eps = entry_points().get(group, ())\n    for entry_point in eps:\n        obj = entry_point.load()\n        if inspect.isclass(obj):\n            cmds[entry_point.name] = obj()\n        else:\n            raise Exception(f\"Invalid entry point {entry_point.name}\")\n    return cmds\n\n\ndef _get_commands_dict(\n    settings: BaseSettings, inproject: bool\n) -> Dict[str, ScrapyCommand]:\n    cmds = _get_commands_from_module(\"scrapy.commands\", inproject)\n    cmds.update(_get_commands_from_entry_points(inproject))\n    cmds_module = settings[\"COMMANDS_MODULE\"]\n    if cmds_module:\n        cmds.update(_get_commands_from_module(cmds_module, inproject))\n    return cmds\n\n\ndef _pop_command_name(argv: List[str]) -> Optional[str]:\n    i = 0\n    for arg in argv[1:]:\n        if not arg.startswith(\"-\"):\n            del argv[i]\n            return arg\n        i += 1\n    return None\n\n\ndef _print_header(settings: BaseSettings, inproject: bool) -> None:\n    version = scrapy.__version__\n    if inproject:\n        print(f\"Scrapy {version} - active project: {settings['BOT_NAME']}\\n\")\n\n    else:\n        print(f\"Scrapy {version} - no active project\\n\")\n\n\ndef _print_commands(settings: BaseSettings, inproject: bool) -> None:\n    _print_header(settings, inproject)\n    print(\"Usage:\")\n    print(\"  scrapy <command> [options] [args]\\n\")\n    print(\"Available commands:\")\n    cmds = _get_commands_dict(settings, inproject)\n    for cmdname, cmdclass in sorted(cmds.items()):\n        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n    if not inproject:\n        print()\n        print(\"  [ more ]      More commands available when run from project directory\")\n    print()\n    print('Use \"scrapy <command> -h\" to see more info about a command')\n\n\ndef _print_unknown_command(\n    settings: BaseSettings, cmdname: str, inproject: bool\n) -> None:\n    _print_header(settings, inproject)\n    print(f\"Unknown command: {cmdname}\\n\")\n    print('Use \"scrapy\" to see available commands')\n\n\ndef _run_print_help(\n    parser: argparse.ArgumentParser,\n    func: Callable[_P, None],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> None:\n    try:\n        func(*a, **kw)\n    except UsageError as e:\n        if str(e):\n            parser.error(str(e))\n        if e.print_help:\n            parser.print_help()\n        sys.exit(2)\n\n\ndef execute(\n    argv: Optional[List[str]] = None, settings: Optional[Settings] = None\n) -> None:\n    if argv is None:\n        argv = sys.argv\n\n    if settings is None:\n        settings = get_project_settings()\n        # set EDITOR from environment if available\n        try:\n            editor = os.environ[\"EDITOR\"]\n        except KeyError:\n            pass\n        else:\n            settings[\"EDITOR\"] = editor\n\n    inproject = inside_project()\n    cmds = _get_commands_dict(settings, inproject)\n    cmdname = _pop_command_name(argv)\n    if not cmdname:\n        _print_commands(settings, inproject)\n        sys.exit(0)\n    elif cmdname not in cmds:\n        _print_unknown_command(settings, cmdname, inproject)\n        sys.exit(2)\n\n    cmd = cmds[cmdname]\n    parser = ScrapyArgumentParser(\n        formatter_class=ScrapyHelpFormatter,\n        usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n        conflict_handler=\"resolve\",\n        description=cmd.long_desc(),\n    )\n    settings.setdict(cmd.default_settings, priority=\"command\")\n    cmd.settings = settings\n    cmd.add_options(parser)\n    opts, args = parser.parse_known_args(args=argv[1:])\n    _run_print_help(parser, cmd.process_options, args, opts)\n\n    cmd.crawler_process = CrawlerProcess(settings)\n    _run_print_help(parser, _run_command, cmd, args, opts)\n    sys.exit(cmd.exitcode)\n\n\ndef _run_command(cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace) -> None:\n    if opts.profile:\n        _run_command_profiled(cmd, args, opts)\n    else:\n        cmd.run(args, opts)\n\n\ndef _run_command_profiled(\n    cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace\n) -> None:\n    if opts.profile:\n        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n    loc = locals()\n    p = cProfile.Profile()\n    p.runctx(\"cmd.run(args, opts)\", globals(), loc)\n    if opts.profile:\n        p.dump_stats(opts.profile)\n\n\nif __name__ == \"__main__\":\n    try:\n        execute()\n    finally:\n        # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect() on exit:\n        # http://doc.pypy.org/en/latest/cpython_differences.html\n        # ?highlight=gc.collect#differences-related-to-garbage-collection-strategies\n        garbage_collect()\n", "scrapy/mail.py": "\"\"\"\nMail sending helpers\n\nSee documentation in docs/topics/email.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom email import encoders as Encoders\nfrom email.mime.base import MIMEBase\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.nonmultipart import MIMENonMultipart\nfrom email.mime.text import MIMEText\nfrom email.utils import formatdate\nfrom io import BytesIO\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nfrom twisted import version as twisted_version\nfrom twisted.internet import ssl\nfrom twisted.internet.defer import Deferred\nfrom twisted.python.failure import Failure\nfrom twisted.python.versions import Version\n\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # imports twisted.internet.reactor\n    from twisted.mail.smtp import ESMTPSenderFactory\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\n# Defined in the email.utils module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/email/utils.py#L42\nCOMMASPACE = \", \"\n\n\ndef _to_bytes_or_none(text: Union[str, bytes, None]) -> Optional[bytes]:\n    if text is None:\n        return None\n    return to_bytes(text)\n\n\nclass MailSender:\n    def __init__(\n        self,\n        smtphost: str = \"localhost\",\n        mailfrom: str = \"scrapy@localhost\",\n        smtpuser: Optional[str] = None,\n        smtppass: Optional[str] = None,\n        smtpport: int = 25,\n        smtptls: bool = False,\n        smtpssl: bool = False,\n        debug: bool = False,\n    ):\n        self.smtphost: str = smtphost\n        self.smtpport: int = smtpport\n        self.smtpuser: Optional[bytes] = _to_bytes_or_none(smtpuser)\n        self.smtppass: Optional[bytes] = _to_bytes_or_none(smtppass)\n        self.smtptls: bool = smtptls\n        self.smtpssl: bool = smtpssl\n        self.mailfrom: str = mailfrom\n        self.debug: bool = debug\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(\n            smtphost=settings[\"MAIL_HOST\"],\n            mailfrom=settings[\"MAIL_FROM\"],\n            smtpuser=settings[\"MAIL_USER\"],\n            smtppass=settings[\"MAIL_PASS\"],\n            smtpport=settings.getint(\"MAIL_PORT\"),\n            smtptls=settings.getbool(\"MAIL_TLS\"),\n            smtpssl=settings.getbool(\"MAIL_SSL\"),\n        )\n\n    def send(\n        self,\n        to: Union[str, List[str]],\n        subject: str,\n        body: str,\n        cc: Union[str, List[str], None] = None,\n        attachs: Sequence[Tuple[str, str, IO[Any]]] = (),\n        mimetype: str = \"text/plain\",\n        charset: Optional[str] = None,\n        _callback: Optional[Callable[..., None]] = None,\n    ) -> Optional[Deferred]:\n        from twisted.internet import reactor\n\n        msg: MIMEBase\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split(\"/\", 1))\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg[\"From\"] = self.mailfrom\n        msg[\"To\"] = COMMASPACE.join(to)\n        msg[\"Date\"] = formatdate(localtime=True)\n        msg[\"Subject\"] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg[\"Cc\"] = COMMASPACE.join(cc)\n\n        if attachs:\n            if charset:\n                msg.set_charset(charset)\n            msg.attach(MIMEText(body, \"plain\", charset or \"us-ascii\"))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split(\"/\"))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header(\n                    \"Content-Disposition\", \"attachment\", filename=attach_name\n                )\n                msg.attach(part)\n        else:\n            msg.set_payload(body, charset)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug(\n                \"Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n                'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                {\n                    \"mailto\": to,\n                    \"mailcc\": cc,\n                    \"mailsubject\": subject,\n                    \"mailattachs\": len(attachs),\n                },\n            )\n            return None\n\n        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or \"utf-8\"))\n        dfd.addCallback(self._sent_ok, to, cc, subject, len(attachs))\n        dfd.addErrback(self._sent_failed, to, cc, subject, len(attachs))\n        reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n        return dfd\n\n    def _sent_ok(\n        self, result: Any, to: List[str], cc: List[str], subject: str, nattachs: int\n    ) -> None:\n        logger.info(\n            \"Mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n            'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n            {\n                \"mailto\": to,\n                \"mailcc\": cc,\n                \"mailsubject\": subject,\n                \"mailattachs\": nattachs,\n            },\n        )\n\n    def _sent_failed(\n        self,\n        failure: Failure,\n        to: List[str],\n        cc: List[str],\n        subject: str,\n        nattachs: int,\n    ) -> Failure:\n        errstr = str(failure.value)\n        logger.error(\n            \"Unable to send mail: To=%(mailto)s Cc=%(mailcc)s \"\n            'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n            \"- %(mailerr)s\",\n            {\n                \"mailto\": to,\n                \"mailcc\": cc,\n                \"mailsubject\": subject,\n                \"mailattachs\": nattachs,\n                \"mailerr\": errstr,\n            },\n        )\n        return failure\n\n    def _sendmail(self, to_addrs: List[str], msg: bytes) -> Deferred:\n        from twisted.internet import reactor\n\n        msg_io = BytesIO(msg)\n        d: Deferred = Deferred()\n\n        factory = self._create_sender_factory(to_addrs, msg_io, d)\n\n        if self.smtpssl:\n            reactor.connectSSL(\n                self.smtphost, self.smtpport, factory, ssl.ClientContextFactory()\n            )\n        else:\n            reactor.connectTCP(self.smtphost, self.smtpport, factory)\n\n        return d\n\n    def _create_sender_factory(\n        self, to_addrs: List[str], msg: IO[bytes], d: Deferred\n    ) -> ESMTPSenderFactory:\n        from twisted.mail.smtp import ESMTPSenderFactory\n\n        factory_keywords: Dict[str, Any] = {\n            \"heloFallback\": True,\n            \"requireAuthentication\": False,\n            \"requireTransportSecurity\": self.smtptls,\n        }\n\n        # Newer versions of twisted require the hostname to use STARTTLS\n        if twisted_version >= Version(\"twisted\", 21, 2, 0):\n            factory_keywords[\"hostname\"] = self.smtphost\n\n        factory = ESMTPSenderFactory(\n            self.smtpuser,\n            self.smtppass,\n            self.mailfrom,\n            to_addrs,\n            msg,\n            d,\n            **factory_keywords,\n        )\n        factory.noisy = False\n        return factory\n", "scrapy/signalmanager.py": "from typing import Any, List, Tuple\n\nfrom pydispatch import dispatcher\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils import signal as _signal\n\n\nclass SignalManager:\n    def __init__(self, sender: Any = dispatcher.Anonymous):\n        self.sender: Any = sender\n\n    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.connect(receiver, signal, **kwargs)\n\n    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.disconnect(receiver, signal, **kwargs)\n\n    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log(signal, **kwargs)\n\n    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning\n        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)\n\n    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        _signal.disconnect_all(signal, **kwargs)\n", "scrapy/addons.py": "import logging\nfrom typing import TYPE_CHECKING, Any, List\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nclass AddonManager:\n    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n\n    def __init__(self, crawler: \"Crawler\") -> None:\n        self.crawler: \"Crawler\" = crawler\n        self.addons: List[Any] = []\n\n    def load_settings(self, settings: Settings) -> None:\n        \"\"\"Load add-ons and configurations from a settings object and apply them.\n\n        This will load the add-on for every add-on path in the\n        ``ADDONS`` setting and execute their ``update_settings`` methods.\n\n        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n            which to read the add-on configuration\n        :type settings: :class:`~scrapy.settings.Settings`\n        \"\"\"\n        for clspath in build_component_list(settings[\"ADDONS\"]):\n            try:\n                addoncls = load_object(clspath)\n                addon = build_from_crawler(addoncls, self.crawler)\n                addon.update_settings(settings)\n                self.addons.append(addon)\n            except NotConfigured as e:\n                if e.args:\n                    logger.warning(\n                        \"Disabled %(clspath)s: %(eargs)s\",\n                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                        extra={\"crawler\": self.crawler},\n                    )\n        logger.info(\n            \"Enabled addons:\\n%(addons)s\",\n            {\n                \"addons\": self.addons,\n            },\n            extra={\"crawler\": self.crawler},\n        )\n", "scrapy/exporters.py": "\"\"\"\nItem Exporters are used to export/serialize items into different formats.\n\"\"\"\n\nimport csv\nimport marshal\nimport pickle  # nosec\nimport pprint\nfrom io import BytesIO, TextIOWrapper\nfrom json import JSONEncoder\nfrom typing import Any, Callable, Dict, Iterable, Mapping, Optional, Tuple, Union\nfrom xml.sax.saxutils import XMLGenerator  # nosec\nfrom xml.sax.xmlreader import AttributesImpl  # nosec\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.python import is_listlike, to_bytes, to_unicode\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\n__all__ = [\n    \"BaseItemExporter\",\n    \"PprintItemExporter\",\n    \"PickleItemExporter\",\n    \"CsvItemExporter\",\n    \"XmlItemExporter\",\n    \"JsonLinesItemExporter\",\n    \"JsonItemExporter\",\n    \"MarshalItemExporter\",\n]\n\n\nclass BaseItemExporter:\n    def __init__(self, *, dont_fail: bool = False, **kwargs: Any):\n        self._kwargs: Dict[str, Any] = kwargs\n        self._configure(kwargs, dont_fail=dont_fail)\n\n    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n        \"\"\"Configure the exporter by popping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding: Optional[str] = options.pop(\"encoding\", None)\n        self.fields_to_export: Union[Mapping[str, str], Iterable[str], None] = (\n            options.pop(\"fields_to_export\", None)\n        )\n        self.export_empty_fields: bool = options.pop(\"export_empty_fields\", False)\n        self.indent: Optional[int] = options.pop(\"indent\", None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n\n    def export_item(self, item: Any) -> None:\n        raise NotImplementedError\n\n    def serialize_field(\n        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\"serializer\", lambda x: x)\n        return serializer(value)\n\n    def start_exporting(self) -> None:\n        pass\n\n    def finish_exporting(self) -> None:\n        pass\n\n    def _get_serialized_fields(\n        self, item: Any, default_value: Any = None, include_empty: Optional[bool] = None\n    ) -> Iterable[Tuple[str, Any]]:\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            if include_empty:\n                field_iter = item.field_names()\n            else:\n                field_iter = item.keys()\n        elif isinstance(self.fields_to_export, Mapping):\n            if include_empty:\n                field_iter = self.fields_to_export.items()\n            else:\n                field_iter = (\n                    (x, y) for x, y in self.fields_to_export.items() if x in item\n                )\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if isinstance(field_name, str):\n                item_field, output_field = field_name, field_name\n            else:\n                item_field, output_field = field_name\n            if item_field in item:\n                field_meta = item.get_field_meta(item_field)\n                value = self.serialize_field(field_meta, output_field, item[item_field])\n            else:\n                value = default_value\n\n            yield output_field, value\n\n\nclass JsonLinesItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file: BytesIO = file\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder: JSONEncoder = ScrapyJSONEncoder(**self._kwargs)\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + \"\\n\"\n        self.file.write(to_bytes(data, self.encoding))\n\n\nclass JsonItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file: BytesIO = file\n        # there is a small difference between the behaviour or JsonItemExporter.indent\n        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n        # the addition of newlines everywhere\n        json_indent = (\n            self.indent if self.indent is not None and self.indent > 0 else None\n        )\n        self._kwargs.setdefault(\"indent\", json_indent)\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n        self.first_item = True\n\n    def _beautify_newline(self) -> None:\n        if self.indent is not None:\n            self.file.write(b\"\\n\")\n\n    def _add_comma_after_first(self) -> None:\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b\",\")\n            self._beautify_newline()\n\n    def start_exporting(self) -> None:\n        self.file.write(b\"[\")\n        self._beautify_newline()\n\n    def finish_exporting(self) -> None:\n        self._beautify_newline()\n        self.file.write(b\"]\")\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n        self._add_comma_after_first()\n        self.file.write(data)\n\n\nclass XmlItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        self.item_element = kwargs.pop(\"item_element\", \"item\")\n        self.root_element = kwargs.pop(\"root_element\", \"items\")\n        super().__init__(**kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.xg = XMLGenerator(file, encoding=self.encoding)\n\n    def _beautify_newline(self, new_item: bool = False) -> None:\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters(\"\\n\")\n\n    def _beautify_indent(self, depth: int = 1) -> None:\n        if self.indent:\n            self.xg.characters(\" \" * self.indent * depth)\n\n    def start_exporting(self) -> None:\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, AttributesImpl({}))\n        self._beautify_newline(new_item=True)\n\n    def export_item(self, item: Any) -> None:\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, AttributesImpl({}))\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=\"\"):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)\n\n    def finish_exporting(self) -> None:\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()\n\n    def _export_xml_field(self, name: str, serialized_value: Any, depth: int) -> None:\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, AttributesImpl({}))\n        if hasattr(serialized_value, \"items\"):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field(\"value\", value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()\n\n\nclass CsvItemExporter(BaseItemExporter):\n    def __init__(\n        self,\n        file: BytesIO,\n        include_headers_line: bool = True,\n        join_multivalued: str = \",\",\n        errors: Optional[str] = None,\n        **kwargs: Any,\n    ):\n        super().__init__(dont_fail=True, **kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.include_headers_line = include_headers_line\n        self.stream = TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding,\n            newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n            errors=errors,\n        )\n        self.csv_writer = csv.writer(self.stream, **self._kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued\n\n    def serialize_field(\n        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\"serializer\", self._join_if_needed)\n        return serializer(value)\n\n    def _join_if_needed(self, value: Any) -> Any:\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value\n\n    def export_item(self, item: Any) -> None:\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)\n\n    def finish_exporting(self) -> None:\n        self.stream.detach()  # Avoid closing the wrapped file.\n\n    def _build_row(self, values: Iterable[Any]) -> Iterable[Any]:\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s\n\n    def _write_headers_and_set_fields_to_export(self, item: Any) -> None:\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            fields: Iterable[str]\n            if isinstance(self.fields_to_export, Mapping):\n                fields = self.fields_to_export.values()\n            else:\n                assert self.fields_to_export\n                fields = self.fields_to_export\n            row = list(self._build_row(fields))\n            self.csv_writer.writerow(row)\n\n\nclass PickleItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, protocol: int = 4, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n        self.protocol: int = protocol\n\n    def export_item(self, item: Any) -> None:\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)\n\n\nclass MarshalItemExporter(BaseItemExporter):\n    \"\"\"Exports items in a Python-specific binary format (see\n    :mod:`marshal`).\n\n    :param file: The file-like object to use for exporting the data. Its\n                 ``write`` method should accept :class:`bytes` (a disk file\n                 opened in binary mode, a :class:`~io.BytesIO` object, etc)\n    \"\"\"\n\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n\n    def export_item(self, item: Any) -> None:\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n\n\nclass PprintItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n\n\nclass PythonItemExporter(BaseItemExporter):\n    \"\"\"This is a base class for item exporters that extends\n    :class:`BaseItemExporter` with support for nested items.\n\n    It serializes items to built-in Python types, so that any serialization\n    library (e.g. :mod:`json` or msgpack_) can be used on top of it.\n\n    .. _msgpack: https://pypi.org/project/msgpack/\n    \"\"\"\n\n    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n        super()._configure(options, dont_fail)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n\n    def serialize_field(\n        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\n            \"serializer\", self._serialize_value\n        )\n        return serializer(value)\n\n    def _serialize_value(self, value: Any) -> Any:\n        if isinstance(value, Item):\n            return self.export_item(value)\n        if is_item(value):\n            return dict(self._serialize_item(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        if isinstance(value, (str, bytes)):\n            return to_unicode(value, encoding=self.encoding)\n        return value\n\n    def _serialize_item(self, item: Any) -> Iterable[Tuple[Union[str, bytes], Any]]:\n        for key, value in ItemAdapter(item).items():\n            yield key, self._serialize_value(value)\n\n    def export_item(self, item: Any) -> Dict[Union[str, bytes], Any]:  # type: ignore[override]\n        result: Dict[Union[str, bytes], Any] = dict(self._get_serialized_fields(item))\n        return result\n", "scrapy/squeues.py": "\"\"\"\nScheduler queues\n\"\"\"\n\nfrom __future__ import annotations\n\nimport marshal\nimport pickle  # nosec\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, Optional, Type, Union\n\nfrom queuelib import queue\n\nfrom scrapy import Request\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.request import request_from_dict\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\ndef _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n    class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n        def __init__(self, path: Union[str, PathLike], *args: Any, **kwargs: Any):\n            dirname = Path(path).parent\n            if not dirname.exists():\n                dirname.mkdir(parents=True, exist_ok=True)\n            super().__init__(path, *args, **kwargs)\n\n    return DirectoriesCreated\n\n\ndef _serializable_queue(\n    queue_class: Type[queue.BaseQueue],\n    serialize: Callable[[Any], bytes],\n    deserialize: Callable[[bytes], Any],\n) -> Type[queue.BaseQueue]:\n    class SerializableQueue(queue_class):  # type: ignore[valid-type,misc]\n        def push(self, obj: Any) -> None:\n            s = serialize(obj)\n            super().push(s)\n\n        def pop(self) -> Optional[Any]:\n            s = super().pop()\n            if s:\n                return deserialize(s)\n            return None\n\n        def peek(self) -> Optional[Any]:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            try:\n                s = super().peek()\n            except AttributeError as ex:\n                raise NotImplementedError(\n                    \"The underlying queue class does not implement 'peek'\"\n                ) from ex\n            if s:\n                return deserialize(s)\n            return None\n\n    return SerializableQueue\n\n\ndef _scrapy_serialization_queue(\n    queue_class: Type[queue.BaseQueue],\n) -> Type[queue.BaseQueue]:\n    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n        def __init__(self, crawler: Crawler, key: str):\n            self.spider = crawler.spider\n            super().__init__(key)\n\n        @classmethod\n        def from_crawler(\n            cls, crawler: Crawler, key: str, *args: Any, **kwargs: Any\n        ) -> Self:\n            return cls(crawler, key)\n\n        def push(self, request: Request) -> None:\n            request_dict = request.to_dict(spider=self.spider)\n            super().push(request_dict)\n\n        def pop(self) -> Optional[Request]:\n            request = super().pop()\n            if not request:\n                return None\n            return request_from_dict(request, spider=self.spider)\n\n        def peek(self) -> Optional[Request]:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            request = super().peek()\n            if not request:\n                return None\n            return request_from_dict(request, spider=self.spider)\n\n    return ScrapyRequestQueue\n\n\ndef _scrapy_non_serialization_queue(\n    queue_class: Type[queue.BaseQueue],\n) -> Type[queue.BaseQueue]:\n    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n        @classmethod\n        def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n            return cls()\n\n        def peek(self) -> Optional[Any]:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            try:\n                s = super().peek()\n            except AttributeError as ex:\n                raise NotImplementedError(\n                    \"The underlying queue class does not implement 'peek'\"\n                ) from ex\n            return s\n\n    return ScrapyRequestQueue\n\n\ndef _pickle_serialize(obj: Any) -> bytes:\n    try:\n        return pickle.dumps(obj, protocol=4)\n    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n    # TypeError is raised from parsel.Selector\n    except (pickle.PicklingError, AttributeError, TypeError) as e:\n        raise ValueError(str(e)) from e\n\n\n# queue.*Queue aren't subclasses of queue.BaseQueue\n_PickleFifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n)\n_PickleLifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n)\n_MarshalFifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n)\n_MarshalLifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n)\n\n# public queue classes\nPickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQueue)\nPickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)\nMarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)\nMarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)\nFifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)  # type: ignore[arg-type]\nLifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)  # type: ignore[arg-type]\n", "scrapy/middleware.py": "from __future__ import annotations\n\nimport logging\nimport pprint\nfrom collections import defaultdict, deque\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Deque,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.defer import process_chain, process_parallel\nfrom scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MiddlewareManager:\n    \"\"\"Base class for implementing middleware managers\"\"\"\n\n    component_name = \"foo middleware\"\n\n    def __init__(self, *middlewares: Any) -> None:\n        self.middlewares = middlewares\n        # Only process_spider_output and process_spider_exception can be None.\n        # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n        self.methods: Dict[\n            str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]\n        ] = defaultdict(deque)\n        for mw in middlewares:\n            self._add_middleware(mw)\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n        raise NotImplementedError\n\n    @classmethod\n    def from_settings(\n        cls, settings: Settings, crawler: Optional[Crawler] = None\n    ) -> Self:\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                if crawler is not None:\n                    mw = build_from_crawler(mwcls, crawler)\n                else:\n                    mw = build_from_settings(mwcls, settings)\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    logger.warning(\n                        \"Disabled %(clspath)s: %(eargs)s\",\n                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                        extra={\"crawler\": crawler},\n                    )\n\n        logger.info(\n            \"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n            {\n                \"componentname\": cls.component_name,\n                \"enabledlist\": pprint.pformat(enabled),\n            },\n            extra={\"crawler\": crawler},\n        )\n        return cls(*middlewares)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls.from_settings(crawler.settings, crawler)\n\n    def _add_middleware(self, mw: Any) -> None:\n        if hasattr(mw, \"open_spider\"):\n            self.methods[\"open_spider\"].append(mw.open_spider)\n        if hasattr(mw, \"close_spider\"):\n            self.methods[\"close_spider\"].appendleft(mw.close_spider)\n\n    def _process_parallel(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n        methods = cast(Iterable[Callable], self.methods[methodname])\n        return process_parallel(methods, obj, *args)\n\n    def _process_chain(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n        methods = cast(Iterable[Callable], self.methods[methodname])\n        return process_chain(methods, obj, *args)\n\n    def open_spider(self, spider: Spider) -> Deferred:\n        return self._process_parallel(\"open_spider\", spider)\n\n    def close_spider(self, spider: Spider) -> Deferred:\n        return self._process_parallel(\"close_spider\", spider)\n", "scrapy/robotstxt.py": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom typing import TYPE_CHECKING, Optional, Union\nfrom warnings import warn\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef decode_robotstxt(\n    robotstxt_body: bytes, spider: Optional[Spider], to_native_str_type: bool = False\n) -> str:\n    try:\n        if to_native_str_type:\n            body_decoded = to_unicode(robotstxt_body)\n        else:\n            body_decoded = robotstxt_body.decode(\"utf-8\", errors=\"ignore\")\n    except UnicodeDecodeError:\n        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n        # Switch to 'allow all' state.\n        logger.warning(\n            \"Failure while parsing robots.txt. File either contains garbage or \"\n            \"is in an encoding other than UTF-8, treating it as an empty file.\",\n            exc_info=sys.exc_info(),\n            extra={\"spider\": spider},\n        )\n        body_decoded = \"\"\n    return body_decoded\n\n\nclass RobotParser(metaclass=ABCMeta):\n    @classmethod\n    @abstractmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n        It must return a new instance of the parser backend.\n\n        :param crawler: crawler which made the request\n        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n\n        :param robotstxt_body: content of a robots.txt_ file.\n        :type robotstxt_body: bytes\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n\n        :param url: Absolute URL\n        :type url: str or bytes\n\n        :param user_agent: User agent\n        :type user_agent: str or bytes\n        \"\"\"\n        pass\n\n\nclass PythonRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        from urllib.robotparser import RobotFileParser\n\n        self.spider: Optional[Spider] = spider\n        body_decoded = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n        self.rp: RobotFileParser = RobotFileParser()\n        self.rp.parse(body_decoded.splitlines())\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(user_agent, url)\n\n\nclass ReppyRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        warn(\"ReppyRobotParser is deprecated.\", ScrapyDeprecationWarning, stacklevel=2)\n        from reppy.robots import Robots\n\n        self.spider: Optional[Spider] = spider\n        self.rp = Robots.parse(\"\", robotstxt_body)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        return self.rp.allowed(url, user_agent)\n\n\nclass RerpRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        from robotexclusionrulesparser import RobotExclusionRulesParser\n\n        self.spider: Optional[Spider] = spider\n        self.rp: RobotExclusionRulesParser = RobotExclusionRulesParser()\n        body_decoded = decode_robotstxt(robotstxt_body, spider)\n        self.rp.parse(body_decoded)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.is_allowed(user_agent, url)\n\n\nclass ProtegoRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        from protego import Protego\n\n        self.spider: Optional[Spider] = spider\n        body_decoded = decode_robotstxt(robotstxt_body, spider)\n        self.rp = Protego.parse(body_decoded)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(url, user_agent)\n", "scrapy/__main__.py": "from scrapy.cmdline import execute\n\nif __name__ == \"__main__\":\n    execute()\n", "scrapy/pqueues.py": "from __future__ import annotations\n\nimport hashlib\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    cast,\n)\n\nfrom scrapy import Request\nfrom scrapy.core.downloader import Downloader\nfrom scrapy.utils.misc import build_from_crawler\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef _path_safe(text: str) -> str:\n    \"\"\"\n    Return a filesystem-safe version of a string ``text``\n\n    >>> _path_safe('simple.org').startswith('simple.org')\n    True\n    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n    True\n    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n    True\n    \"\"\"\n    pathable_slot = \"\".join([c if c.isalnum() or c in \"-._\" else \"_\" for c in text])\n    # as we replace some letters we can get collision for different slots\n    # add we add unique part\n    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # nosec\n    return \"-\".join([pathable_slot, unique_slot])\n\n\nclass QueueProtocol(Protocol):\n    \"\"\"Protocol for downstream queues of ``ScrapyPriorityQueue``.\"\"\"\n\n    def push(self, request: Request) -> None: ...\n\n    def pop(self) -> Optional[Request]: ...\n\n    def close(self) -> None: ...\n\n    def __len__(self) -> int: ...\n\n\nclass ScrapyPriorityQueue:\n    \"\"\"A priority queue implemented using multiple internal queues (typically,\n    FIFO queues). It uses one internal queue for each priority value. The internal\n    queue must implement the following methods:\n\n        * push(obj)\n        * pop()\n        * close()\n        * __len__()\n\n    Optionally, the queue could provide a ``peek`` method, that should return the\n    next object to be returned by ``pop``, but without removing it from the queue.\n\n    ``__init__`` method of ScrapyPriorityQueue receives a downstream_queue_cls\n    argument, which is a class used to instantiate a new (internal) queue when\n    a new priority is allocated.\n\n    Only integer priorities should be used. Lower numbers are higher\n    priorities.\n\n    startprios is a sequence of priorities to start with. If the queue was\n    previously closed leaving some priority buckets non-empty, those priorities\n    should be passed in startprios.\n\n    \"\"\"\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        startprios: Iterable[int] = (),\n    ) -> Self:\n        return cls(crawler, downstream_queue_cls, key, startprios)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        startprios: Iterable[int] = (),\n    ):\n        self.crawler: Crawler = crawler\n        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n        self.key: str = key\n        self.queues: Dict[int, QueueProtocol] = {}\n        self.curprio: Optional[int] = None\n        self.init_prios(startprios)\n\n    def init_prios(self, startprios: Iterable[int]) -> None:\n        if not startprios:\n            return\n\n        for priority in startprios:\n            self.queues[priority] = self.qfactory(priority)\n\n        self.curprio = min(startprios)\n\n    def qfactory(self, key: int) -> QueueProtocol:\n        return build_from_crawler(\n            self.downstream_queue_cls,\n            self.crawler,\n            self.key + \"/\" + str(key),\n        )\n\n    def priority(self, request: Request) -> int:\n        return -request.priority\n\n    def push(self, request: Request) -> None:\n        priority = self.priority(request)\n        if priority not in self.queues:\n            self.queues[priority] = self.qfactory(priority)\n        q = self.queues[priority]\n        q.push(request)  # this may fail (eg. serialization error)\n        if self.curprio is None or priority < self.curprio:\n            self.curprio = priority\n\n    def pop(self) -> Optional[Request]:\n        if self.curprio is None:\n            return None\n        q = self.queues[self.curprio]\n        m = q.pop()\n        if not q:\n            del self.queues[self.curprio]\n            q.close()\n            prios = [p for p, q in self.queues.items() if q]\n            self.curprio = min(prios) if prios else None\n        return m\n\n    def peek(self) -> Optional[Request]:\n        \"\"\"Returns the next object to be returned by :meth:`pop`,\n        but without removing it from the queue.\n\n        Raises :exc:`NotImplementedError` if the underlying queue class does\n        not implement a ``peek`` method, which is optional for queues.\n        \"\"\"\n        if self.curprio is None:\n            return None\n        queue = self.queues[self.curprio]\n        # Protocols can't declare optional members\n        return cast(Request, queue.peek())  # type: ignore[attr-defined]\n\n    def close(self) -> List[int]:\n        active: List[int] = []\n        for p, q in self.queues.items():\n            active.append(p)\n            q.close()\n        return active\n\n    def __len__(self) -> int:\n        return sum(len(x) for x in self.queues.values()) if self.queues else 0\n\n\nclass DownloaderInterface:\n    def __init__(self, crawler: Crawler):\n        assert crawler.engine\n        self.downloader: Downloader = crawler.engine.downloader\n\n    def stats(self, possible_slots: Iterable[str]) -> List[Tuple[int, str]]:\n        return [(self._active_downloads(slot), slot) for slot in possible_slots]\n\n    def get_slot_key(self, request: Request) -> str:\n        return self.downloader.get_slot_key(request)\n\n    def _active_downloads(self, slot: str) -> int:\n        \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n        if slot not in self.downloader.slots:\n            return 0\n        return len(self.downloader.slots[slot].active)\n\n\nclass DownloaderAwarePriorityQueue:\n    \"\"\"PriorityQueue which takes Downloader activity into account:\n    domains (slots) with the least amount of active downloads are dequeued\n    first.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        startprios: Optional[Dict[str, Iterable[int]]] = None,\n    ) -> Self:\n        return cls(crawler, downstream_queue_cls, key, startprios)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        slot_startprios: Optional[Dict[str, Iterable[int]]] = None,\n    ):\n        if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n            raise ValueError(\n                f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP'\n            )\n\n        if slot_startprios and not isinstance(slot_startprios, dict):\n            raise ValueError(\n                \"DownloaderAwarePriorityQueue accepts \"\n                \"``slot_startprios`` as a dict; \"\n                f\"{slot_startprios.__class__!r} instance \"\n                \"is passed. Most likely, it means the state is\"\n                \"created by an incompatible priority queue. \"\n                \"Only a crawl started with the same priority \"\n                \"queue class can be resumed.\"\n            )\n\n        self._downloader_interface: DownloaderInterface = DownloaderInterface(crawler)\n        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n        self.key: str = key\n        self.crawler: Crawler = crawler\n\n        self.pqueues: Dict[str, ScrapyPriorityQueue] = {}  # slot -> priority queue\n        for slot, startprios in (slot_startprios or {}).items():\n            self.pqueues[slot] = self.pqfactory(slot, startprios)\n\n    def pqfactory(\n        self, slot: str, startprios: Iterable[int] = ()\n    ) -> ScrapyPriorityQueue:\n        return ScrapyPriorityQueue(\n            self.crawler,\n            self.downstream_queue_cls,\n            self.key + \"/\" + _path_safe(slot),\n            startprios,\n        )\n\n    def pop(self) -> Optional[Request]:\n        stats = self._downloader_interface.stats(self.pqueues)\n\n        if not stats:\n            return None\n\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        request = queue.pop()\n        if len(queue) == 0:\n            del self.pqueues[slot]\n        return request\n\n    def push(self, request: Request) -> None:\n        slot = self._downloader_interface.get_slot_key(request)\n        if slot not in self.pqueues:\n            self.pqueues[slot] = self.pqfactory(slot)\n        queue = self.pqueues[slot]\n        queue.push(request)\n\n    def peek(self) -> Optional[Request]:\n        \"\"\"Returns the next object to be returned by :meth:`pop`,\n        but without removing it from the queue.\n\n        Raises :exc:`NotImplementedError` if the underlying queue class does\n        not implement a ``peek`` method, which is optional for queues.\n        \"\"\"\n        stats = self._downloader_interface.stats(self.pqueues)\n        if not stats:\n            return None\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        return queue.peek()\n\n    def close(self) -> Dict[str, List[int]]:\n        active = {slot: queue.close() for slot, queue in self.pqueues.items()}\n        self.pqueues.clear()\n        return active\n\n    def __len__(self) -> int:\n        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n\n    def __contains__(self, slot: str) -> bool:\n        return slot in self.pqueues\n", "scrapy/link.py": "\"\"\"\nThis module defines the Link object used in Link extractors.\n\nFor actual link extractors implementation see scrapy.linkextractors, or\nits documentation in: docs/topics/link-extractors.rst\n\"\"\"\n\nfrom typing import Any\n\n\nclass Link:\n    \"\"\"Link objects represent an extracted link by the LinkExtractor.\n\n    Using the anchor tag sample below to illustrate the parameters::\n\n            <a href=\"https://example.com/nofollow.html#foo\" rel=\"nofollow\">Dont follow this one</a>\n\n    :param url: the absolute url being linked to in the anchor tag.\n                From the sample, this is ``https://example.com/nofollow.html``.\n\n    :param text: the text in the anchor tag. From the sample, this is ``Dont follow this one``.\n\n    :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.\n\n    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute\n                    of the anchor tag.\n    \"\"\"\n\n    __slots__ = [\"url\", \"text\", \"fragment\", \"nofollow\"]\n\n    def __init__(\n        self, url: str, text: str = \"\", fragment: str = \"\", nofollow: bool = False\n    ):\n        if not isinstance(url, str):\n            got = url.__class__.__name__\n            raise TypeError(f\"Link urls must be str objects, got {got}\")\n        self.url: str = url\n        self.text: str = text\n        self.fragment: str = fragment\n        self.nofollow: bool = nofollow\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Link):\n            raise NotImplementedError\n        return (\n            self.url == other.url\n            and self.text == other.text\n            and self.fragment == other.fragment\n            and self.nofollow == other.nofollow\n        )\n\n    def __hash__(self) -> int:\n        return (\n            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n        )\n\n    def __repr__(self) -> str:\n        return (\n            f\"Link(url={self.url!r}, text={self.text!r}, \"\n            f\"fragment={self.fragment!r}, nofollow={self.nofollow!r})\"\n        )\n", "scrapy/__init__.py": "\"\"\"\nScrapy - a web crawling and web scraping framework written for Python\n\"\"\"\n\nimport pkgutil\nimport sys\nimport warnings\n\nfrom twisted import version as _txv\n\n# Declare top-level shortcuts\nfrom scrapy.http import FormRequest, Request\nfrom scrapy.item import Field, Item\nfrom scrapy.selector import Selector\nfrom scrapy.spiders import Spider\n\n__all__ = [\n    \"__version__\",\n    \"version_info\",\n    \"twisted_version\",\n    \"Spider\",\n    \"Request\",\n    \"FormRequest\",\n    \"Selector\",\n    \"Item\",\n    \"Field\",\n]\n\n\n# Scrapy and Twisted versions\n__version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\nversion_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\ntwisted_version = (_txv.major, _txv.minor, _txv.micro)\n\n\n# Ignore noisy twisted deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\n\n\ndel pkgutil\ndel sys\ndel warnings\n", "scrapy/crawler.py": "from __future__ import annotations\n\nimport logging\nimport pprint\nimport signal\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union, cast\n\nfrom twisted.internet.defer import (\n    Deferred,\n    DeferredList,\n    inlineCallbacks,\n    maybeDeferred,\n)\n\ntry:\n    # zope >= 5.0 only supports MultipleInvalid\n    from zope.interface.exceptions import MultipleInvalid\nexcept ImportError:\n    MultipleInvalid = None\n\nfrom zope.interface.verify import verifyClass\n\nfrom scrapy import Spider, signals\nfrom scrapy.addons import AddonManager\nfrom scrapy.core.engine import ExecutionEngine\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.extension import ExtensionManager\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.settings import BaseSettings, Settings, overridden_settings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.log import (\n    LogCounterHandler,\n    configure_logging,\n    get_scrapy_root_handler,\n    install_scrapy_root_handler,\n    log_reactor_info,\n    log_scrapy_info,\n)\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.ossignal import install_shutdown_handlers, signal_names\nfrom scrapy.utils.reactor import (\n    install_reactor,\n    is_asyncio_reactor_installed,\n    verify_installed_asyncio_event_loop,\n    verify_installed_reactor,\n)\n\nif TYPE_CHECKING:\n    from scrapy.utils.request import RequestFingerprinter\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Crawler:\n    def __init__(\n        self,\n        spidercls: Type[Spider],\n        settings: Union[None, Dict[str, Any], Settings] = None,\n        init_reactor: bool = False,\n    ):\n        if isinstance(spidercls, Spider):\n            raise ValueError(\"The spidercls argument must be a class, not an object\")\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        self.spidercls: Type[Spider] = spidercls\n        self.settings: Settings = settings.copy()\n        self.spidercls.update_settings(self.settings)\n        self._update_root_log_handler()\n\n        self.addons: AddonManager = AddonManager(self)\n        self.signals: SignalManager = SignalManager(self)\n\n        self._init_reactor: bool = init_reactor\n        self.crawling: bool = False\n        self._started: bool = False\n\n        self.extensions: Optional[ExtensionManager] = None\n        self.stats: Optional[StatsCollector] = None\n        self.logformatter: Optional[LogFormatter] = None\n        self.request_fingerprinter: Optional[RequestFingerprinter] = None\n        self.spider: Optional[Spider] = None\n        self.engine: Optional[ExecutionEngine] = None\n\n    def _update_root_log_handler(self) -> None:\n        if get_scrapy_root_handler() is not None:\n            # scrapy root handler already installed: update it with new settings\n            install_scrapy_root_handler(self.settings)\n\n    def _apply_settings(self) -> None:\n        if self.settings.frozen:\n            return\n\n        self.addons.load_settings(self.settings)\n        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n\n        handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n        logging.root.addHandler(handler)\n        # lambda is assigned to Crawler attribute because this way it is not\n        # garbage collected after leaving the scope\n        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n\n        lf_cls: Type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n        self.logformatter = lf_cls.from_crawler(self)\n\n        self.request_fingerprinter = build_from_crawler(\n            load_object(self.settings[\"REQUEST_FINGERPRINTER_CLASS\"]),\n            self,\n        )\n\n        reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n        event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n        if self._init_reactor:\n            # this needs to be done after the spider settings are merged,\n            # but before something imports twisted.internet.reactor\n            if reactor_class:\n                install_reactor(reactor_class, event_loop)\n            else:\n                from twisted.internet import reactor  # noqa: F401\n            log_reactor_info()\n        if reactor_class:\n            verify_installed_reactor(reactor_class)\n            if is_asyncio_reactor_installed() and event_loop:\n                verify_installed_asyncio_event_loop(event_loop)\n\n            log_reactor_info()\n\n        self.extensions = ExtensionManager.from_crawler(self)\n        self.settings.freeze()\n\n        d = dict(overridden_settings(self.settings))\n        logger.info(\n            \"Overridden settings:\\n%(settings)s\", {\"settings\": pprint.pformat(d)}\n        )\n\n    @inlineCallbacks\n    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n        if self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        if self._started:\n            warnings.warn(\n                \"Running Crawler.crawl() more than once is deprecated.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n        self.crawling = self._started = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self._apply_settings()\n            self._update_root_log_handler()\n            self.engine = self._create_engine()\n            start_requests = iter(self.spider.start_requests())\n            yield self.engine.open_spider(self.spider, start_requests)\n            yield maybeDeferred(self.engine.start)\n        except Exception:\n            self.crawling = False\n            if self.engine is not None:\n                yield self.engine.close()\n            raise\n\n    def _create_spider(self, *args: Any, **kwargs: Any) -> Spider:\n        return self.spidercls.from_crawler(self, *args, **kwargs)\n\n    def _create_engine(self) -> ExecutionEngine:\n        return ExecutionEngine(self, lambda _: self.stop())\n\n    @inlineCallbacks\n    def stop(self) -> Generator[Deferred, Any, None]:\n        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n        fired when the crawler is stopped.\"\"\"\n        if self.crawling:\n            self.crawling = False\n            assert self.engine\n            yield maybeDeferred(self.engine.stop)\n\n    @staticmethod\n    def _get_component(component_class, components):\n        for component in components:\n            if isinstance(component, component_class):\n                return component\n        return None\n\n    def get_addon(self, cls):\n        return self._get_component(cls, self.addons.addons)\n\n    def get_downloader_middleware(self, cls):\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_downloader_middleware() can only be called after \"\n                \"the crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n\n    def get_extension(self, cls):\n        if not self.extensions:\n            raise RuntimeError(\n                \"Crawler.get_extension() can only be called after the \"\n                \"extension manager has been created.\"\n            )\n        return self._get_component(cls, self.extensions.middlewares)\n\n    def get_item_pipeline(self, cls):\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_item_pipeline() can only be called after the \"\n                \"crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n\n    def get_spider_middleware(self, cls):\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_spider_middleware() can only be called after the \"\n                \"crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.scraper.spidermw.middlewares)\n\n\nclass CrawlerRunner:\n    \"\"\"\n    This is a convenient helper class that keeps track of, manages and runs\n    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n\n    The CrawlerRunner object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    \"\"\"\n\n    crawlers = property(\n        lambda self: self._crawlers,\n        doc=\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by \"\n        \":meth:`crawl` and managed by this class.\",\n    )\n\n    @staticmethod\n    def _get_spider_loader(settings: BaseSettings):\n        \"\"\"Get SpiderLoader instance from settings\"\"\"\n        cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n        loader_cls = load_object(cls_path)\n        verifyClass(ISpiderLoader, loader_cls)\n        return loader_cls.from_settings(settings.frozencopy())\n\n    def __init__(self, settings: Union[Dict[str, Any], Settings, None] = None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        self.settings = settings\n        self.spider_loader = self._get_spider_loader(settings)\n        self._crawlers: Set[Crawler] = set()\n        self._active: Set[Deferred] = set()\n        self.bootstrap_failed = False\n\n    def crawl(\n        self,\n        crawler_or_spidercls: Union[Type[Spider], str, Crawler],\n        *args: Any,\n        **kwargs: Any,\n    ) -> Deferred:\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param args: arguments to initialize the spider\n\n        :param kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)\n\n    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> Deferred:\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result: Any) -> Any:\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n            return result\n\n        return d.addBoth(_done)\n\n    def create_crawler(\n        self, crawler_or_spidercls: Union[Type[Spider], str, Crawler]\n    ) -> Crawler:\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If ``crawler_or_spidercls`` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)\n\n    def _create_crawler(self, spidercls: Union[str, Type[Spider]]) -> Crawler:\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        # temporary cast until self.spider_loader is typed\n        return Crawler(cast(Type[Spider], spidercls), self.settings)\n\n    def stop(self) -> Deferred:\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return DeferredList([c.stop() for c in list(self.crawlers)])\n\n    @inlineCallbacks\n    def join(self) -> Generator[Deferred, Any, None]:\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield DeferredList(self._active)\n\n\nclass CrawlerProcess(CrawlerRunner):\n    \"\"\"\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n    signals, like the keyboard interrupt command Ctrl-C. It also configures\n    top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n    :mod:`~twisted.internet.reactor` within your application.\n\n    The CrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: Union[Dict[str, Any], Settings, None] = None,\n        install_root_handler: bool = True,\n    ):\n        super().__init__(settings)\n        configure_logging(self.settings, install_root_handler)\n        log_scrapy_info(self.settings)\n        self._initialized_reactor = False\n\n    def _signal_shutdown(self, signum: int, _: Any) -> None:\n        from twisted.internet import reactor\n\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\n            \"Received %(signame)s, shutting down gracefully. Send again to force \",\n            {\"signame\": signame},\n        )\n        reactor.callFromThread(self._graceful_stop_reactor)\n\n    def _signal_kill(self, signum: int, _: Any) -> None:\n        from twisted.internet import reactor\n\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info(\n            \"Received %(signame)s twice, forcing unclean shutdown\", {\"signame\": signame}\n        )\n        reactor.callFromThread(self._stop_reactor)\n\n    def _create_crawler(self, spidercls: Union[Type[Spider], str]) -> Crawler:\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        init_reactor = not self._initialized_reactor\n        self._initialized_reactor = True\n        # temporary cast until self.spider_loader is typed\n        return Crawler(\n            cast(Type[Spider], spidercls), self.settings, init_reactor=init_reactor\n        )\n\n    def start(\n        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n    ) -> None:\n        \"\"\"\n        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param bool stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n\n        :param bool install_signal_handlers: whether to install the OS signal\n            handlers from Twisted and Scrapy (default: True)\n        \"\"\"\n        from twisted.internet import reactor\n\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(self._stop_reactor)\n\n        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n        # We pass self, which is CrawlerProcess, instead of Crawler here,\n        # which works because the default resolvers only use crawler.settings.\n        resolver = build_from_crawler(resolver_class, self, reactor=reactor)  # type: ignore[arg-type]\n        resolver.install_on_reactor()\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n        reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n        if install_signal_handlers:\n            reactor.addSystemEventTrigger(\n                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n            )\n        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n\n    def _graceful_stop_reactor(self) -> Deferred:\n        d = self.stop()\n        d.addBoth(self._stop_reactor)\n        return d\n\n    def _stop_reactor(self, _: Any = None) -> None:\n        from twisted.internet import reactor\n\n        try:\n            reactor.stop()\n        except RuntimeError:  # raised if already stopped or in shutdown stage\n            pass\n", "scrapy/responsetypes.py": "\"\"\"\nThis module implements a class which returns the appropriate Response class\nbased on different criteria.\n\"\"\"\n\nfrom io import StringIO\nfrom mimetypes import MimeTypes\nfrom pkgutil import get_data\nfrom typing import Dict, Mapping, Optional, Type, Union\n\nfrom scrapy.http import Response\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n\n\nclass ResponseTypes:\n    CLASSES = {\n        \"text/html\": \"scrapy.http.HtmlResponse\",\n        \"application/atom+xml\": \"scrapy.http.XmlResponse\",\n        \"application/rdf+xml\": \"scrapy.http.XmlResponse\",\n        \"application/rss+xml\": \"scrapy.http.XmlResponse\",\n        \"application/xhtml+xml\": \"scrapy.http.HtmlResponse\",\n        \"application/vnd.wap.xhtml+xml\": \"scrapy.http.HtmlResponse\",\n        \"application/xml\": \"scrapy.http.XmlResponse\",\n        \"application/json\": \"scrapy.http.JsonResponse\",\n        \"application/x-json\": \"scrapy.http.JsonResponse\",\n        \"application/json-amazonui-streaming\": \"scrapy.http.JsonResponse\",\n        \"application/javascript\": \"scrapy.http.TextResponse\",\n        \"application/x-javascript\": \"scrapy.http.TextResponse\",\n        \"text/xml\": \"scrapy.http.XmlResponse\",\n        \"text/*\": \"scrapy.http.TextResponse\",\n    }\n\n    def __init__(self) -> None:\n        self.classes: Dict[str, Type[Response]] = {}\n        self.mimetypes: MimeTypes = MimeTypes()\n        mimedata = get_data(\"scrapy\", \"mime.types\")\n        if not mimedata:\n            raise ValueError(\n                \"The mime.types file is not found in the Scrapy installation\"\n            )\n        self.mimetypes.readfp(StringIO(mimedata.decode(\"utf8\")))\n        for mimetype, cls in self.CLASSES.items():\n            self.classes[mimetype] = load_object(cls)\n\n    def from_mimetype(self, mimetype: str) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        if mimetype in self.classes:\n            return self.classes[mimetype]\n        basetype = f\"{mimetype.split('/')[0]}/*\"\n        return self.classes.get(basetype, Response)\n\n    def from_content_type(\n        self, content_type: Union[str, bytes], content_encoding: Optional[bytes] = None\n    ) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header\"\"\"\n        if content_encoding:\n            return Response\n        mimetype = (\n            to_unicode(content_type, encoding=\"latin-1\").split(\";\")[0].strip().lower()\n        )\n        return self.from_mimetype(mimetype)\n\n    def from_content_disposition(\n        self, content_disposition: Union[str, bytes]\n    ) -> Type[Response]:\n        try:\n            filename = (\n                to_unicode(content_disposition, encoding=\"latin-1\", errors=\"replace\")\n                .split(\";\")[1]\n                .split(\"=\")[1]\n                .strip(\"\\\"'\")\n            )\n            return self.from_filename(filename)\n        except IndexError:\n            return Response\n\n    def from_headers(self, headers: Mapping[bytes, bytes]) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b\"Content-Type\" in headers:\n            cls = self.from_content_type(\n                content_type=headers[b\"Content-Type\"],\n                content_encoding=headers.get(b\"Content-Encoding\"),\n            )\n        if cls is Response and b\"Content-Disposition\" in headers:\n            cls = self.from_content_disposition(headers[b\"Content-Disposition\"])\n        return cls\n\n    def from_filename(self, filename: str) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        return Response\n\n    def from_body(self, body: bytes) -> Type[Response]:\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype(\"application/octet-stream\")\n        lowercase_chunk = chunk.lower()\n        if b\"<html>\" in lowercase_chunk:\n            return self.from_mimetype(\"text/html\")\n        if b\"<?xml\" in lowercase_chunk:\n            return self.from_mimetype(\"text/xml\")\n        if b\"<!doctype html>\" in lowercase_chunk:\n            return self.from_mimetype(\"text/html\")\n        return self.from_mimetype(\"text\")\n\n    def from_args(\n        self,\n        headers: Optional[Mapping[bytes, bytes]] = None,\n        url: Optional[str] = None,\n        filename: Optional[str] = None,\n        body: Optional[bytes] = None,\n    ) -> Type[Response]:\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls\n\n\nresponsetypes = ResponseTypes()\n", "scrapy/item.py": "\"\"\"\nScrapy Item\n\nSee documentation in docs/topics/item.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABCMeta\nfrom copy import deepcopy\nfrom pprint import pformat\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    KeysView,\n    MutableMapping,\n    NoReturn,\n    Tuple,\n)\n\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass Field(Dict[str, Any]):\n    \"\"\"Container of field metadata\"\"\"\n\n\nclass ItemMeta(ABCMeta):\n    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n\n    .. _metaclass: https://realpython.com/python-metaclasses\n    \"\"\"\n\n    def __new__(\n        mcs, class_name: str, bases: Tuple[type, ...], attrs: Dict[str, Any]\n    ) -> ItemMeta:\n        classcell = attrs.pop(\"__classcell__\", None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n        _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n\n        fields = getattr(_class, \"fields\", {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs[\"fields\"] = fields\n        new_attrs[\"_class\"] = _class\n        if classcell is not None:\n            new_attrs[\"__classcell__\"] = classcell\n        return super().__new__(mcs, class_name, bases, new_attrs)\n\n\nclass Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n    \"\"\"\n    Base class for scraped items.\n\n    In Scrapy, an object is considered an ``item`` if it is an instance of either\n    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n    spider callback is evaluated, only instances of :class:`Item` or\n    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n\n    If you need instances of a custom class to be considered items by Scrapy,\n    you must inherit from either :class:`Item` or :class:`dict`.\n\n    Items must declare :class:`Field` attributes, which are processed and stored\n    in the ``fields`` attribute. This restricts the set of allowed field names\n    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n    Additionally, fields can be used to define metadata and control the way\n    data is processed internally. Please refer to the :ref:`documentation\n    about fields <topics-items-fields>` for additional information.\n\n    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n    \"\"\"\n\n    fields: Dict[str, Field]\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        self._values: Dict[str, Any] = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in dict(*args, **kwargs).items():\n                self[k] = v\n\n    def __getitem__(self, key: str) -> Any:\n        return self._values[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n\n    def __delitem__(self, key: str) -> None:\n        del self._values[key]\n\n    def __getattr__(self, name: str) -> NoReturn:\n        if name in self.fields:\n            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n        raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        if not name.startswith(\"_\"):\n            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n        super().__setattr__(name, value)\n\n    def __len__(self) -> int:\n        return len(self._values)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._values)\n\n    __hash__ = object_ref.__hash__\n\n    def keys(self) -> KeysView[str]:\n        return self._values.keys()\n\n    def __repr__(self) -> str:\n        return pformat(dict(self))\n\n    def copy(self) -> Self:\n        return self.__class__(self)\n\n    def deepcopy(self) -> Self:\n        \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n        return deepcopy(self)\n", "scrapy/selector/unified.py": "\"\"\"\nXPath selectors based on lxml\n\"\"\"\n\nfrom typing import Any, Optional, Type, Union\n\nfrom parsel import Selector as _ParselSelector\n\nfrom scrapy.http import HtmlResponse, TextResponse, XmlResponse\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.response import get_base_url\nfrom scrapy.utils.trackref import object_ref\n\n__all__ = [\"Selector\", \"SelectorList\"]\n\n_NOT_SET = object()\n\n\ndef _st(response: Optional[TextResponse], st: Optional[str]) -> str:\n    if st is None:\n        return \"xml\" if isinstance(response, XmlResponse) else \"html\"\n    return st\n\n\ndef _response_from_text(text: Union[str, bytes], st: Optional[str]) -> TextResponse:\n    rt: Type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n    return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n\n\nclass SelectorList(_ParselSelector.selectorlist_cls, object_ref):\n    \"\"\"\n    The :class:`SelectorList` class is a subclass of the builtin ``list``\n    class, which provides a few additional methods.\n    \"\"\"\n\n\nclass Selector(_ParselSelector, object_ref):\n    \"\"\"\n    An instance of :class:`Selector` is a wrapper over response to select\n    certain parts of its content.\n\n    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an\n    :class:`~scrapy.http.XmlResponse` object that will be used for selecting\n    and extracting data.\n\n    ``text`` is a unicode string or utf-8 encoded text for cases when a\n    ``response`` isn't available. Using ``text`` and ``response`` together is\n    undefined behavior.\n\n    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``, ``\"json\"``\n    or ``None`` (default).\n\n    If ``type`` is ``None``, the selector automatically chooses the best type\n    based on ``response`` type (see below), or defaults to ``\"html\"`` in case it\n    is used together with ``text``.\n\n    If ``type`` is ``None`` and a ``response`` is passed, the selector type is\n    inferred from the response type as follows:\n\n    * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n    * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n    * ``\"json\"`` for :class:`~scrapy.http.TextResponse` type\n    * ``\"html\"`` for anything else\n\n    Otherwise, if ``type`` is set, the selector type will be forced and no\n    detection will occur.\n    \"\"\"\n\n    __slots__ = [\"response\"]\n    selectorlist_cls = SelectorList\n\n    def __init__(\n        self,\n        response: Optional[TextResponse] = None,\n        text: Optional[str] = None,\n        type: Optional[str] = None,\n        root: Optional[Any] = _NOT_SET,\n        **kwargs: Any,\n    ):\n        if response is not None and text is not None:\n            raise ValueError(\n                f\"{self.__class__.__name__}.__init__() received \"\n                \"both response and text\"\n            )\n\n        st = _st(response, type)\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            text = response.text\n            kwargs.setdefault(\"base_url\", get_base_url(response))\n\n        self.response = response\n\n        if root is not _NOT_SET:\n            kwargs[\"root\"] = root\n\n        super().__init__(text=text, type=st, **kwargs)\n", "scrapy/selector/__init__.py": "\"\"\"\nSelectors\n\"\"\"\n\n# top-level imports\nfrom scrapy.selector.unified import Selector, SelectorList\n", "scrapy/utils/response.py": "\"\"\"\nThis module provides some useful functions for working with\nscrapy.http.Response objects\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport tempfile\nimport webbrowser\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable, Tuple, Union\nfrom weakref import WeakKeyDictionary\n\nfrom twisted.web import http\nfrom w3lib import html\n\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    from scrapy.http import Response, TextResponse\n\n_baseurl_cache: WeakKeyDictionary[Response, str] = WeakKeyDictionary()\n\n\ndef get_base_url(response: TextResponse) -> str:\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(\n            text, response.url, response.encoding\n        )\n    return _baseurl_cache[response]\n\n\n_metaref_cache: WeakKeyDictionary[\n    Response, Union[Tuple[None, None], Tuple[float, str]]\n] = WeakKeyDictionary()\n\n\ndef get_meta_refresh(\n    response: TextResponse,\n    ignore_tags: Iterable[str] = (\"script\", \"noscript\"),\n) -> Union[Tuple[None, None], Tuple[float, str]]:\n    \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(\n            text, response.url, response.encoding, ignore_tags=ignore_tags\n        )\n    return _metaref_cache[response]\n\n\ndef response_status_message(status: Union[bytes, float, int, str]) -> str:\n    \"\"\"Return status code plus status text descriptive message\"\"\"\n    status_int = int(status)\n    message = http.RESPONSES.get(status_int, \"Unknown Status\")\n    return f\"{status_int} {to_unicode(message)}\"\n\n\ndef _remove_html_comments(body: bytes) -> bytes:\n    start = body.find(b\"<!--\")\n    while start != -1:\n        end = body.find(b\"-->\", start + 1)\n        if end == -1:\n            return body[:start]\n        else:\n            body = body[:start] + body[end + 3 :]\n            start = body.find(b\"<!--\")\n    return body\n\n\ndef open_in_browser(\n    response: TextResponse,\n    _openfunc: Callable[[str], Any] = webbrowser.open,\n) -> Any:\n    \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for\n    external links to work, e.g. so that images and styles are displayed.\n\n    .. _base tag: https://www.w3schools.com/tags/tag_base.asp\n\n    For example:\n\n    .. code-block:: python\n\n        from scrapy.utils.response import open_in_browser\n\n\n        def parse_details(self, response):\n            if \"item name\" not in response.body:\n                open_in_browser(response)\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b\"<base\" not in body:\n            _remove_html_comments(body)\n            repl = rf'\\0<base href=\"{response.url}\">'\n            body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)\n        ext = \".html\"\n    elif isinstance(response, TextResponse):\n        ext = \".txt\"\n    else:\n        raise TypeError(\"Unsupported response type: \" f\"{response.__class__.__name__}\")\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(f\"file://{fname}\")\n", "scrapy/utils/engine.py": "\"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n\nfrom __future__ import annotations\n\n# used in global tests code\nfrom time import time  # noqa: F401\nfrom typing import Any, List, Tuple\n\nfrom scrapy.core.engine import ExecutionEngine\n\n\ndef get_engine_status(engine: ExecutionEngine) -> List[Tuple[str, Any]]:\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle()\",\n        \"engine.slot.closing\",\n        \"len(engine.slot.inprogress)\",\n        \"len(engine.slot.scheduler.dqs or [])\",\n        \"len(engine.slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks: List[Tuple[str, Any]] = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]  # nosec\n        except Exception as e:\n            checks += [(test, f\"{type(e).__name__} (exception)\")]\n\n    return checks\n\n\ndef format_engine_status(engine: ExecutionEngine) -> str:\n    checks = get_engine_status(engine)\n    s = \"Execution engine status\\n\\n\"\n    for test, result in checks:\n        s += f\"{test:<47} : {result}\\n\"\n    s += \"\\n\"\n\n    return s\n\n\ndef print_engine_status(engine: ExecutionEngine) -> None:\n    print(format_engine_status(engine))\n", "scrapy/utils/display.py": "\"\"\"\npprint and pformat wrappers with colorization support\n\"\"\"\n\nimport ctypes\nimport platform\nimport sys\nfrom pprint import pformat as pformat_\nfrom typing import Any\n\nfrom packaging.version import Version as parse_version\n\n\ndef _enable_windows_terminal_processing() -> bool:\n    # https://stackoverflow.com/a/36760881\n    kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]\n    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))\n\n\ndef _tty_supports_color() -> bool:\n    if sys.platform != \"win32\":\n        return True\n\n    if parse_version(platform.version()) < parse_version(\"10.0.14393\"):\n        return True\n\n    # Windows >= 10.0.14393 interprets ANSI escape sequences providing terminal\n    # processing is enabled.\n    return _enable_windows_terminal_processing()\n\n\ndef _colorize(text: str, colorize: bool = True) -> str:\n    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n        return text\n    try:\n        from pygments import highlight\n    except ImportError:\n        return text\n    else:\n        from pygments.formatters import TerminalFormatter\n        from pygments.lexers import PythonLexer\n\n        return highlight(text, PythonLexer(), TerminalFormatter())\n\n\ndef pformat(obj: Any, *args: Any, **kwargs: Any) -> str:\n    return _colorize(pformat_(obj), kwargs.pop(\"colorize\", True))\n\n\ndef pprint(obj: Any, *args: Any, **kwargs: Any) -> None:\n    print(pformat(obj, *args, **kwargs))\n", "scrapy/utils/trackref.py": "\"\"\"This module provides some functions and classes to record and report\nreferences to live object instances.\n\nIf you want live objects for a particular class to be tracked, you only have to\nsubclass from object_ref (instead of object).\n\nAbout performance: This library has a minimal performance impact when enabled,\nand no performance penalty at all when disabled (as object_ref becomes just an\nalias to object in that case).\n\"\"\"\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Iterable\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nNoneType = type(None)\nlive_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n\n\nclass object_ref:\n    \"\"\"Inherit from this class to a keep a record of live instances\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> \"Self\":\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj\n\n\n# using Any as it's hard to type type(None)\ndef format_live_refs(ignore: Any = NoneType) -> str:\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(wdict.values())\n        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n    return s\n\n\ndef print_live_refs(*a: Any, **kw: Any) -> None:\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))\n\n\ndef get_oldest(class_name: str) -> Any:\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(wdict.items(), key=itemgetter(1))[0]\n\n\ndef iter_all(class_name: str) -> Iterable[Any]:\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            return wdict.keys()\n    return []\n", "scrapy/utils/log.py": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom logging.config import dictConfig\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    List,\n    MutableMapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nfrom twisted.python import log as twisted_log\nfrom twisted.python.failure import Failure\n\nimport scrapy\nfrom scrapy.logformatter import LogFormatterResult\nfrom scrapy.settings import Settings, _SettingsKeyT\nfrom scrapy.utils.versions import scrapy_components_versions\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef failure_to_exc_info(\n    failure: Failure,\n) -> Optional[Tuple[Type[BaseException], BaseException, Optional[TracebackType]]]:\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        assert failure.type\n        assert failure.value\n        return (\n            failure.type,\n            failure.value,\n            cast(Optional[TracebackType], failure.getTracebackObject()),\n        )\n    return None\n\n\nclass TopLevelFormatter(logging.Filter):\n    \"\"\"Keep only top level loggers's name (direct children from root) from\n    records.\n\n    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics\n    the old Scrapy log behaviour and helps shortening long names.\n\n    Since it can't be set for just one logger (it won't propagate for its\n    children), it's going to be set in the root handler, with a parametrized\n    ``loggers`` list where it should act.\n    \"\"\"\n\n    def __init__(self, loggers: Optional[List[str]] = None):\n        self.loggers: List[str] = loggers or []\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        if any(record.name.startswith(logger + \".\") for logger in self.loggers):\n            record.name = record.name.split(\".\", 1)[0]\n        return True\n\n\nDEFAULT_LOGGING = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"loggers\": {\n        \"filelock\": {\n            \"level\": \"ERROR\",\n        },\n        \"hpack\": {\n            \"level\": \"ERROR\",\n        },\n        \"scrapy\": {\n            \"level\": \"DEBUG\",\n        },\n        \"twisted\": {\n            \"level\": \"ERROR\",\n        },\n    },\n}\n\n\ndef configure_logging(\n    settings: Union[Settings, Dict[_SettingsKeyT, Any], None] = None,\n    install_root_handler: bool = True,\n) -> None:\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver(\"twisted\")\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool(\"LOG_STDOUT\"):\n        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))  # type: ignore[assignment]\n\n    if install_root_handler:\n        install_scrapy_root_handler(settings)\n\n\n_scrapy_root_handler: Optional[logging.Handler] = None\n\n\ndef install_scrapy_root_handler(settings: Settings) -> None:\n    global _scrapy_root_handler\n\n    if (\n        _scrapy_root_handler is not None\n        and _scrapy_root_handler in logging.root.handlers\n    ):\n        logging.root.removeHandler(_scrapy_root_handler)\n    logging.root.setLevel(logging.NOTSET)\n    _scrapy_root_handler = _get_handler(settings)\n    logging.root.addHandler(_scrapy_root_handler)\n\n\ndef get_scrapy_root_handler() -> Optional[logging.Handler]:\n    return _scrapy_root_handler\n\n\ndef _get_handler(settings: Settings) -> logging.Handler:\n    \"\"\"Return a log handler object according to settings\"\"\"\n    filename = settings.get(\"LOG_FILE\")\n    handler: logging.Handler\n    if filename:\n        mode = \"a\" if settings.getbool(\"LOG_FILE_APPEND\") else \"w\"\n        encoding = settings.get(\"LOG_ENCODING\")\n        handler = logging.FileHandler(filename, mode=mode, encoding=encoding)\n    elif settings.getbool(\"LOG_ENABLED\"):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get(\"LOG_FORMAT\"), datefmt=settings.get(\"LOG_DATEFORMAT\")\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get(\"LOG_LEVEL\"))\n    if settings.getbool(\"LOG_SHORT_NAMES\"):\n        handler.addFilter(TopLevelFormatter([\"scrapy\"]))\n    return handler\n\n\ndef log_scrapy_info(settings: Settings) -> None:\n    logger.info(\n        \"Scrapy %(version)s started (bot: %(bot)s)\",\n        {\"version\": scrapy.__version__, \"bot\": settings[\"BOT_NAME\"]},\n    )\n    versions = [\n        f\"{name} {version}\"\n        for name, version in scrapy_components_versions()\n        if name != \"Scrapy\"\n    ]\n    logger.info(\"Versions: %(versions)s\", {\"versions\": \", \".join(versions)})\n\n\ndef log_reactor_info() -> None:\n    from twisted.internet import reactor\n\n    logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n    from twisted.internet import asyncioreactor\n\n    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n        logger.debug(\n            \"Using asyncio event loop: %s.%s\",\n            reactor._asyncioEventloop.__module__,\n            reactor._asyncioEventloop.__class__.__name__,\n        )\n\n\nclass StreamLogger:\n    \"\"\"Fake file-like stream object that redirects writes to a logger instance\n\n    Taken from:\n        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n    \"\"\"\n\n    def __init__(self, logger: logging.Logger, log_level: int = logging.INFO):\n        self.logger: logging.Logger = logger\n        self.log_level: int = log_level\n        self.linebuf: str = \"\"\n\n    def write(self, buf: str) -> None:\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())\n\n    def flush(self) -> None:\n        for h in self.logger.handlers:\n            h.flush()\n\n\nclass LogCounterHandler(logging.Handler):\n    \"\"\"Record log levels count into a crawler stats\"\"\"\n\n    def __init__(self, crawler: Crawler, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.crawler: Crawler = crawler\n\n    def emit(self, record: logging.LogRecord) -> None:\n        sname = f\"log_count/{record.levelname}\"\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(sname)\n\n\ndef logformatter_adapter(\n    logkws: LogFormatterResult,\n) -> Tuple[int, str, Union[Dict[str, Any], Tuple[Any, ...]]]:\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n\n    level = logkws.get(\"level\", logging.INFO)\n    message = logkws.get(\"msg\") or \"\"\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = cast(Dict[str, Any], logkws) if not logkws.get(\"args\") else logkws[\"args\"]\n\n    return (level, message, args)\n\n\nclass SpiderLoggerAdapter(logging.LoggerAdapter):\n    def process(\n        self, msg: str, kwargs: MutableMapping[str, Any]\n    ) -> Tuple[str, MutableMapping[str, Any]]:\n        \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n        if isinstance(kwargs.get(\"extra\"), MutableMapping):\n            kwargs[\"extra\"].update(self.extra)\n        else:\n            kwargs[\"extra\"] = self.extra\n\n        return msg, kwargs\n", "scrapy/utils/boto.py": "\"\"\"Boto/botocore helpers\"\"\"\n\n\ndef is_botocore_available() -> bool:\n    try:\n        import botocore  # noqa: F401\n\n        return True\n    except ImportError:\n        return False\n", "scrapy/utils/spider.py": "from __future__ import annotations\n\nimport inspect\nimport logging\nfrom types import CoroutineType, ModuleType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Iterable,\n    Literal,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import deferred_from_coro\nfrom scrapy.utils.misc import arg_to_iter\n\nif TYPE_CHECKING:\n    from scrapy.spiderloader import SpiderLoader\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\n# https://stackoverflow.com/questions/60222982\n@overload\ndef iterate_spider_output(result: AsyncGenerator[_T, None]) -> AsyncGenerator[_T, None]: ...  # type: ignore[overload-overlap]\n\n\n@overload\ndef iterate_spider_output(result: CoroutineType[Any, Any, _T]) -> Deferred[_T]: ...\n\n\n@overload\ndef iterate_spider_output(result: _T) -> Iterable[Any]: ...\n\n\ndef iterate_spider_output(\n    result: Any,\n) -> Union[Iterable[Any], AsyncGenerator[_T, None], Deferred[_T]]:\n    if inspect.isasyncgen(result):\n        return result\n    if inspect.iscoroutine(result):\n        d = deferred_from_coro(result)\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(deferred_from_coro(result))\n\n\ndef iter_spider_classes(module: ModuleType) -> Iterable[Type[Spider]]:\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (i.e. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in vars(module).values():\n        if (\n            inspect.isclass(obj)\n            and issubclass(obj, Spider)\n            and obj.__module__ == module.__name__\n            and getattr(obj, \"name\", None)\n        ):\n            yield obj\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    default_spidercls: Type[Spider],\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> Type[Spider]: ...\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    default_spidercls: Literal[None],\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> Optional[Type[Spider]]: ...\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    *,\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> Optional[Type[Spider]]: ...\n\n\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    default_spidercls: Optional[Type[Spider]] = None,\n    log_none: bool = False,\n    log_multiple: bool = False,\n) -> Optional[Type[Spider]]:\n    \"\"\"Return a spider class that handles the given Request.\n\n    This will look for the spiders that can handle the given request (using\n    the spider loader) and return a Spider class if (and only if) there is\n    only one Spider able to handle the Request.\n\n    If multiple spiders (or no spider) are found, it will return the\n    default_spidercls passed. It can optionally log if multiple or no spiders\n    are found.\n    \"\"\"\n    snames = spider_loader.find_by_request(request)\n    if len(snames) == 1:\n        return spider_loader.load(snames[0])\n\n    if len(snames) > 1 and log_multiple:\n        logger.error(\n            \"More than one spider can handle: %(request)s - %(snames)s\",\n            {\"request\": request, \"snames\": \", \".join(snames)},\n        )\n\n    if len(snames) == 0 and log_none:\n        logger.error(\n            \"Unable to find spider that handles: %(request)s\", {\"request\": request}\n        )\n\n    return default_spidercls\n\n\nclass DefaultSpider(Spider):\n    name = \"default\"\n", "scrapy/utils/serialize.py": "import datetime\nimport decimal\nimport json\nfrom typing import Any\n\nfrom itemadapter import ItemAdapter, is_item\nfrom twisted.internet import defer\n\nfrom scrapy.http import Request, Response\n\n\nclass ScrapyJSONEncoder(json.JSONEncoder):\n    DATE_FORMAT = \"%Y-%m-%d\"\n    TIME_FORMAT = \"%H:%M:%S\"\n\n    def default(self, o: Any) -> Any:\n        if isinstance(o, set):\n            return list(o)\n        if isinstance(o, datetime.datetime):\n            return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n        if isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        if isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        if isinstance(o, decimal.Decimal):\n            return str(o)\n        if isinstance(o, defer.Deferred):\n            return str(o)\n        if is_item(o):\n            return ItemAdapter(o).asdict()\n        if isinstance(o, Request):\n            return f\"<{type(o).__name__} {o.method} {o.url}>\"\n        if isinstance(o, Response):\n            return f\"<{type(o).__name__} {o.status} {o.url}>\"\n        return super().default(o)\n\n\nclass ScrapyJSONDecoder(json.JSONDecoder):\n    pass\n", "scrapy/utils/reactor.py": "from __future__ import annotations\n\nimport asyncio\nimport sys\nfrom asyncio import AbstractEventLoop, AbstractEventLoopPolicy\nfrom contextlib import suppress\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n)\nfrom warnings import catch_warnings, filterwarnings, warn\n\nfrom twisted.internet import asyncioreactor, error\nfrom twisted.internet.base import DelayedCall\nfrom twisted.internet.protocol import ServerFactory\nfrom twisted.internet.tcp import Port\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n\n\ndef listen_tcp(portrange: List[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    from twisted.internet import reactor\n\n    if len(portrange) > 2:\n        raise ValueError(f\"invalid portrange: {portrange}\")\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1] + 1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise\n\n\nclass CallLaterOnce(Generic[_T]):\n    \"\"\"Schedule a function to be called in the next reactor loop, but only if\n    it hasn't been already scheduled since the last time it ran.\n    \"\"\"\n\n    def __init__(self, func: Callable[_P, _T], *a: _P.args, **kw: _P.kwargs):\n        self._func: Callable[_P, _T] = func\n        self._a: Tuple[Any, ...] = a\n        self._kw: Dict[str, Any] = kw\n        self._call: Optional[DelayedCall] = None\n\n    def schedule(self, delay: float = 0) -> None:\n        from twisted.internet import reactor\n\n        if self._call is None:\n            self._call = reactor.callLater(delay, self)\n\n    def cancel(self) -> None:\n        if self._call:\n            self._call.cancel()\n\n    def __call__(self) -> _T:\n        self._call = None\n        return self._func(*self._a, **self._kw)\n\n\ndef set_asyncio_event_loop_policy() -> None:\n    \"\"\"The policy functions from asyncio often behave unexpectedly,\n    so we restrict their use to the absolutely essential case.\n    This should only be used to install the reactor.\n    \"\"\"\n    _get_asyncio_event_loop_policy()\n\n\ndef get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n    warn(\n        \"Call to deprecated function \"\n        \"scrapy.utils.reactor.get_asyncio_event_loop_policy().\\n\"\n        \"\\n\"\n        \"Please use get_event_loop, new_event_loop and set_event_loop\"\n        \" from asyncio instead, as the corresponding policy methods may lead\"\n        \" to unexpected behaviour.\\n\"\n        \"This function is replaced by set_asyncio_event_loop_policy and\"\n        \" is meant to be used only when the reactor is being installed.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    return _get_asyncio_event_loop_policy()\n\n\ndef _get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n    policy = asyncio.get_event_loop_policy()\n    if (\n        sys.version_info >= (3, 8)\n        and sys.platform == \"win32\"\n        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\n    ):\n        policy = asyncio.WindowsSelectorEventLoopPolicy()\n        asyncio.set_event_loop_policy(policy)\n    return policy\n\n\ndef install_reactor(reactor_path: str, event_loop_path: Optional[str] = None) -> None:\n    \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n    import path. Also installs the asyncio event loop with the specified import\n    path if the asyncio reactor is enabled\"\"\"\n    reactor_class = load_object(reactor_path)\n    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n        set_asyncio_event_loop_policy()\n        with suppress(error.ReactorAlreadyInstalledError):\n            event_loop = set_asyncio_event_loop(event_loop_path)\n            asyncioreactor.install(eventloop=event_loop)\n    else:\n        *module, _ = reactor_path.split(\".\")\n        installer_path = module + [\"install\"]\n        installer = load_object(\".\".join(installer_path))\n        with suppress(error.ReactorAlreadyInstalledError):\n            installer()\n\n\ndef _get_asyncio_event_loop() -> AbstractEventLoop:\n    return set_asyncio_event_loop(None)\n\n\ndef set_asyncio_event_loop(event_loop_path: Optional[str]) -> AbstractEventLoop:\n    \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n    if event_loop_path is not None:\n        event_loop_class: Type[AbstractEventLoop] = load_object(event_loop_path)\n        event_loop = event_loop_class()\n        asyncio.set_event_loop(event_loop)\n    else:\n        try:\n            with catch_warnings():\n                # In Python 3.10.9, 3.11.1, 3.12 and 3.13, a DeprecationWarning\n                # is emitted about the lack of a current event loop, because in\n                # Python 3.14 and later `get_event_loop` will raise a\n                # RuntimeError in that event. Because our code is already\n                # prepared for that future behavior, we ignore the deprecation\n                # warning.\n                filterwarnings(\n                    \"ignore\",\n                    message=\"There is no current event loop\",\n                    category=DeprecationWarning,\n                )\n                event_loop = asyncio.get_event_loop()\n        except RuntimeError:\n            # `get_event_loop` raises RuntimeError when called with no asyncio\n            # event loop yet installed in the following scenarios:\n            # - Previsibly on Python 3.14 and later.\n            #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902\n            event_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(event_loop)\n    return event_loop\n\n\ndef verify_installed_reactor(reactor_path: str) -> None:\n    \"\"\"Raises :exc:`Exception` if the installed\n    :mod:`~twisted.internet.reactor` does not match the specified import\n    path.\"\"\"\n    from twisted.internet import reactor\n\n    reactor_class = load_object(reactor_path)\n    if not reactor.__class__ == reactor_class:\n        msg = (\n            \"The installed reactor \"\n            f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n            f\"match the requested one ({reactor_path})\"\n        )\n        raise Exception(msg)\n\n\ndef verify_installed_asyncio_event_loop(loop_path: str) -> None:\n    from twisted.internet import reactor\n\n    loop_class = load_object(loop_path)\n    if isinstance(reactor._asyncioEventloop, loop_class):\n        return\n    installed = (\n        f\"{reactor._asyncioEventloop.__class__.__module__}\"\n        f\".{reactor._asyncioEventloop.__class__.__qualname__}\"\n    )\n    specified = f\"{loop_class.__module__}.{loop_class.__qualname__}\"\n    raise Exception(\n        \"Scrapy found an asyncio Twisted reactor already \"\n        f\"installed, and its event loop class ({installed}) does \"\n        \"not match the one specified in the ASYNCIO_EVENT_LOOP \"\n        f\"setting ({specified})\"\n    )\n\n\ndef is_asyncio_reactor_installed() -> bool:\n    from twisted.internet import reactor\n\n    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n", "scrapy/utils/signal.py": "\"\"\"Helper functions for working with signals\"\"\"\n\nimport collections.abc\nimport logging\nfrom typing import Any as TypingAny\nfrom typing import List, Tuple\n\nfrom pydispatch.dispatcher import (\n    Anonymous,\n    Any,\n    disconnect,\n    getAllReceivers,\n    liveReceivers,\n)\nfrom pydispatch.robustapply import robustApply\nfrom twisted.internet.defer import Deferred, DeferredList\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.utils.defer import maybeDeferred_coro\nfrom scrapy.utils.log import failure_to_exc_info\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_catch_log(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny\n) -> List[Tuple[TypingAny, TypingAny]]:\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop(\"dont_log\", ())\n    dont_log = (\n        tuple(dont_log)\n        if isinstance(dont_log, collections.abc.Sequence)\n        else (dont_log,)\n    )\n    dont_log += (StopDownload,)\n    spider = named.get(\"spider\", None)\n    responses: List[Tuple[TypingAny, TypingAny]] = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        result: TypingAny\n        try:\n            response = robustApply(\n                receiver, signal=signal, sender=sender, *arguments, **named\n            )\n            if isinstance(response, Deferred):\n                logger.error(\n                    \"Cannot return deferreds from signal handler: %(receiver)s\",\n                    {\"receiver\": receiver},\n                    extra={\"spider\": spider},\n                )\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\n                \"Error caught on signal handler: %(receiver)s\",\n                {\"receiver\": receiver},\n                exc_info=True,\n                extra={\"spider\": spider},\n            )\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses\n\n\ndef send_catch_log_deferred(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny\n) -> Deferred:\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n\n    def logerror(failure: Failure, recv: Any) -> Failure:\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\n                \"Error caught on signal handler: %(receiver)s\",\n                {\"receiver\": recv},\n                exc_info=failure_to_exc_info(failure),\n                extra={\"spider\": spider},\n            )\n        return failure\n\n    dont_log = named.pop(\"dont_log\", None)\n    spider = named.get(\"spider\", None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred_coro(\n            robustApply, receiver, signal=signal, sender=sender, *arguments, **named\n        )\n        d.addErrback(logerror, receiver)\n        # TODO https://pylint.readthedocs.io/en/latest/user_guide/messages/warning/cell-var-from-loop.html\n        d.addBoth(\n            lambda result: (\n                receiver,  # pylint: disable=cell-var-from-loop  # noqa: B023\n                result,\n            )\n        )\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d\n\n\ndef disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)\n", "scrapy/utils/sitemap.py": "\"\"\"\nModule for processing Sitemaps.\n\nNote: The main purpose of this module is to provide support for the\nSitemapSpider, its API is subject to change without notice.\n\"\"\"\n\nfrom typing import Any, Dict, Iterable, Iterator, Optional, Union\nfrom urllib.parse import urljoin\n\nimport lxml.etree  # nosec\n\n\nclass Sitemap:\n    \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n    (type=sitemapindex) files\"\"\"\n\n    def __init__(self, xmltext: Union[str, bytes]):\n        xmlp = lxml.etree.XMLParser(\n            recover=True, remove_comments=True, resolve_entities=False\n        )\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # nosec\n        rt = self._root.tag\n        self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n\n    def __iter__(self) -> Iterator[Dict[str, Any]]:\n        for elem in self._root.getchildren():\n            d: Dict[str, Any] = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n\n                if name == \"link\":\n                    if \"href\" in el.attrib:\n                        d.setdefault(\"alternate\", []).append(el.get(\"href\"))\n                else:\n                    d[name] = el.text.strip() if el.text else \"\"\n\n            if \"loc\" in d:\n                yield d\n\n\ndef sitemap_urls_from_robots(\n    robots_text: str, base_url: Optional[str] = None\n) -> Iterable[str]:\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith(\"sitemap:\"):\n            url = line.split(\":\", 1)[1].strip()\n            yield urljoin(base_url or \"\", url)\n", "scrapy/utils/gz.py": "import struct\nfrom gzip import GzipFile\nfrom io import BytesIO\n\nfrom scrapy.http import Response\n\nfrom ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\n\n\ndef gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output_stream = BytesIO()\n    chunk = b\".\"\n    decompressed_size = 0\n    while chunk:\n        try:\n            chunk = f.read1(_CHUNK_SIZE)\n        except (OSError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output_stream is empty\n            if output_stream.getbuffer().nbytes > 0:\n                break\n            raise\n        decompressed_size += len(chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef gzip_magic_number(response: Response) -> bool:\n    return response.body[:3] == b\"\\x1f\\x8b\\x08\"\n", "scrapy/utils/ossignal.py": "import signal\nfrom types import FrameType\nfrom typing import Any, Callable, Dict, Optional, Union\n\n# copy of _HANDLER from typeshed/stdlib/signal.pyi\nSignalHandlerT = Union[\n    Callable[[int, Optional[FrameType]], Any], int, signal.Handlers, None\n]\n\nsignal_names: Dict[int, str] = {}\nfor signame in dir(signal):\n    if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n        signum = getattr(signal, signame)\n        if isinstance(signum, int):\n            signal_names[signum] = signame\n\n\ndef install_shutdown_handlers(\n    function: SignalHandlerT, override_sigint: bool = True\n) -> None:\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n    SIGINT handler won't be installed if there is already a handler in place\n    (e.g. Pdb)\n    \"\"\"\n    signal.signal(signal.SIGTERM, function)\n    if (\n        signal.getsignal(signal.SIGINT)  # pylint: disable=comparison-with-callable\n        == signal.default_int_handler\n        or override_sigint\n    ):\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, \"SIGBREAK\"):\n        signal.signal(signal.SIGBREAK, function)\n", "scrapy/utils/request.py": "\"\"\"\nThis module provides some useful functions for working with\nscrapy.http.Request objects\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    Union,\n)\nfrom urllib.parse import urlunparse\nfrom weakref import WeakKeyDictionary\n\nfrom w3lib.http import basic_auth_header\nfrom w3lib.url import canonicalize_url\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\ndef _serialize_headers(headers: Iterable[bytes], request: Request) -> Iterable[bytes]:\n    for header in headers:\n        if header in request.headers:\n            yield header\n            yield from request.headers.getlist(header)\n\n\n_fingerprint_cache: WeakKeyDictionary[\n    Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]\n]\n_fingerprint_cache = WeakKeyDictionary()\n\n\ndef fingerprint(\n    request: Request,\n    *,\n    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n    keep_fragments: bool = False,\n) -> bytes:\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (i.e. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accessible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lots of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingerprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    Also, servers usually ignore fragments in urls when handling requests,\n    so they are also ignored by default when calculating the fingerprint.\n    If you want to include them, set the keep_fragments argument to True\n    (for instance when handling requests with a headless browser).\n    \"\"\"\n    processed_include_headers: Optional[Tuple[bytes, ...]] = None\n    if include_headers:\n        processed_include_headers = tuple(\n            to_bytes(h.lower()) for h in sorted(include_headers)\n        )\n    cache = _fingerprint_cache.setdefault(request, {})\n    cache_key = (processed_include_headers, keep_fragments)\n    if cache_key not in cache:\n        # To decode bytes reliably (JSON does not support bytes), regardless of\n        # character encoding, we use bytes.hex()\n        headers: Dict[str, List[str]] = {}\n        if processed_include_headers:\n            for header in processed_include_headers:\n                if header in request.headers:\n                    headers[header.hex()] = [\n                        header_value.hex()\n                        for header_value in request.headers.getlist(header)\n                    ]\n        fingerprint_data = {\n            \"method\": to_unicode(request.method),\n            \"url\": canonicalize_url(request.url, keep_fragments=keep_fragments),\n            \"body\": (request.body or b\"\").hex(),\n            \"headers\": headers,\n        }\n        fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()  # nosec\n    return cache[cache_key]\n\n\nclass RequestFingerprinterProtocol(Protocol):\n    def fingerprint(self, request: Request) -> bytes: ...\n\n\nclass RequestFingerprinter:\n    \"\"\"Default fingerprinter.\n\n    It takes into account a canonical version\n    (:func:`w3lib.url.canonicalize_url`) of :attr:`request.url\n    <scrapy.http.Request.url>` and the values of :attr:`request.method\n    <scrapy.http.Request.method>` and :attr:`request.body\n    <scrapy.http.Request.body>`. It then generates an `SHA1\n    <https://en.wikipedia.org/wiki/SHA-1>`_ hash.\n\n    .. seealso:: :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION`.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def __init__(self, crawler: Optional[Crawler] = None):\n        if crawler:\n            implementation = crawler.settings.get(\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n            )\n        else:\n            implementation = \"SENTINEL\"\n\n        if implementation != \"SENTINEL\":\n            message = (\n                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n                \"And it will be removed in future version of Scrapy.\"\n            )\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n        self._fingerprint = fingerprint\n\n    def fingerprint(self, request: Request) -> bytes:\n        return self._fingerprint(request)\n\n\ndef request_authenticate(\n    request: Request,\n    username: str,\n    password: str,\n) -> None:\n    \"\"\"Authenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers[\"Authorization\"] = basic_auth_header(username, password)\n\n\ndef request_httprepr(request: Request) -> bytes:\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b\"\") + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s\n\n\ndef referer_str(request: Request) -> Optional[str]:\n    \"\"\"Return Referer HTTP header suitable for logging.\"\"\"\n    referrer = request.headers.get(\"Referer\")\n    if referrer is None:\n        return referrer\n    return to_unicode(referrer, errors=\"replace\")\n\n\ndef request_from_dict(d: Dict[str, Any], *, spider: Optional[Spider] = None) -> Request:\n    \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    request_cls: Type[Request] = load_object(d[\"_class\"]) if \"_class\" in d else Request\n    kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}\n    if d.get(\"callback\") and spider:\n        kwargs[\"callback\"] = _get_method(spider, d[\"callback\"])\n    if d.get(\"errback\") and spider:\n        kwargs[\"errback\"] = _get_method(spider, d[\"errback\"])\n    return request_cls(**kwargs)\n\n\ndef _get_method(obj: Any, name: Any) -> Any:\n    \"\"\"Helper function for request_from_dict\"\"\"\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(f\"Method {name!r} not found in: {obj}\")\n\n\ndef request_to_curl(request: Request) -> str:\n    \"\"\"\n    Converts a :class:`~scrapy.Request` object to a curl command.\n\n    :param :class:`~scrapy.Request`: Request object to be converted\n    :return: string containing the curl command\n    \"\"\"\n    method = request.method\n\n    data = f\"--data-raw '{request.body.decode('utf-8')}'\" if request.body else \"\"\n\n    headers = \" \".join(\n        f\"-H '{k.decode()}: {v[0].decode()}'\" for k, v in request.headers.items()\n    )\n\n    url = request.url\n    cookies = \"\"\n    if request.cookies:\n        if isinstance(request.cookies, dict):\n            cookie = \"; \".join(f\"{k}={v}\" for k, v in request.cookies.items())\n            cookies = f\"--cookie '{cookie}'\"\n        elif isinstance(request.cookies, list):\n            cookie = \"; \".join(\n                f\"{list(c.keys())[0]}={list(c.values())[0]}\" for c in request.cookies\n            )\n            cookies = f\"--cookie '{cookie}'\"\n\n    curl_cmd = f\"curl -X {method} {url} {data} {headers} {cookies}\".strip()\n    return \" \".join(curl_cmd.split())\n", "scrapy/utils/iterators.py": "import csv\nimport logging\nimport re\nfrom io import StringIO\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Union,\n    cast,\n    overload,\n)\nfrom warnings import warn\n\nfrom lxml import etree  # nosec\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import re_rsearch\n\nlogger = logging.getLogger(__name__)\n\n\ndef xmliter(obj: Union[Response, str, bytes], nodename: str) -> Iterator[Selector]:\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    warn(\n        (\n            \"xmliter is deprecated and its use strongly discouraged because \"\n            \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"\n            \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    nodename_patt = re.escape(nodename)\n\n    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)\n    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)\n    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n    text = _body_or_str(obj)\n\n    document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n    document_header = (\n        document_header_match.group().strip() if document_header_match else \"\"\n    )\n    header_end_idx = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"\n    namespaces: Dict[str, str] = {}\n    if header_end:\n        for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n            assert header_end_idx\n            tag = re.search(\n                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n            )\n            if tag:\n                for x in re.findall(NAMESPACE_RE, tag.group()):\n                    namespaces[x[1]] = x[0]\n\n    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = (\n            document_header\n            + match.group().replace(\n                nodename, f'{nodename} {\" \".join(namespaces.values())}', 1\n            )\n            + header_end\n        )\n        yield Selector(text=nodetext, type=\"xml\")\n\n\ndef xmliter_lxml(\n    obj: Union[Response, str, bytes],\n    nodename: str,\n    namespace: Optional[str] = None,\n    prefix: str = \"x\",\n) -> Iterator[Selector]:\n    reader = _StreamReader(obj)\n    tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n    iterable = etree.iterparse(\n        reader,\n        encoding=reader.encoding,\n        events=(\"end\", \"start-ns\"),\n        resolve_entities=False,\n        huge_tree=True,\n    )\n    selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n    needs_namespace_resolution = not namespace and \":\" in nodename\n    if needs_namespace_resolution:\n        prefix, nodename = nodename.split(\":\", maxsplit=1)\n    for event, data in iterable:\n        if event == \"start-ns\":\n            assert isinstance(data, tuple)\n            if needs_namespace_resolution:\n                _prefix, _namespace = data\n                if _prefix != prefix:\n                    continue\n                namespace = _namespace\n                needs_namespace_resolution = False\n                selxpath = f\"//{prefix}:{nodename}\"\n                tag = f\"{{{namespace}}}{nodename}\"\n            continue\n        assert isinstance(data, etree._Element)\n        node = data\n        if node.tag != tag:\n            continue\n        nodetext = etree.tostring(node, encoding=\"unicode\")\n        node.clear()\n        xs = Selector(text=nodetext, type=\"xml\")\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]\n\n\nclass _StreamReader:\n    def __init__(self, obj: Union[Response, str, bytes]):\n        self._ptr: int = 0\n        self._text: Union[str, bytes]\n        if isinstance(obj, TextResponse):\n            self._text, self.encoding = obj.body, obj.encoding\n        elif isinstance(obj, Response):\n            self._text, self.encoding = obj.body, \"utf-8\"\n        else:\n            self._text, self.encoding = obj, \"utf-8\"\n        self._is_unicode: bool = isinstance(self._text, str)\n        self._is_first_read: bool = True\n\n    def read(self, n: int = 65535) -> bytes:\n        method: Callable[[int], bytes] = (\n            self._read_unicode if self._is_unicode else self._read_string\n        )\n        result = method(n)\n        if self._is_first_read:\n            self._is_first_read = False\n            result = result.lstrip()\n        return result\n\n    def _read_string(self, n: int = 65535) -> bytes:\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return cast(bytes, self._text)[s:e]\n\n    def _read_unicode(self, n: int = 65535) -> bytes:\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return cast(str, self._text)[s:e].encode(\"utf-8\")\n\n\ndef csviter(\n    obj: Union[Response, str, bytes],\n    delimiter: Optional[str] = None,\n    headers: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n    quotechar: Optional[str] = None,\n) -> Iterator[Dict[str, str]]:\n    \"\"\"Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    if encoding is not None:\n        warn(\n            \"The encoding argument of csviter() is ignored and will be removed\"\n            \" in a future Scrapy version.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n    lines = StringIO(_body_or_str(obj, unicode=True))\n\n    kwargs: Dict[str, Any] = {}\n    if delimiter:\n        kwargs[\"delimiter\"] = delimiter\n    if quotechar:\n        kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            headers = next(csv_r)\n        except StopIteration:\n            return\n\n    for row in csv_r:\n        if len(row) != len(headers):\n            logger.warning(\n                \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                \"should be: %(csvheader)d)\",\n                {\n                    \"csvlnum\": csv_r.line_num,\n                    \"csvrow\": len(row),\n                    \"csvheader\": len(headers),\n                },\n            )\n            continue\n        yield dict(zip(headers, row))\n\n\n@overload\ndef _body_or_str(obj: Union[Response, str, bytes]) -> str: ...\n\n\n@overload\ndef _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str: ...\n\n\n@overload\ndef _body_or_str(\n    obj: Union[Response, str, bytes], unicode: Literal[False]\n) -> bytes: ...\n\n\ndef _body_or_str(\n    obj: Union[Response, str, bytes], unicode: bool = True\n) -> Union[str, bytes]:\n    expected_types = (Response, str, bytes)\n    if not isinstance(obj, expected_types):\n        expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n        raise TypeError(\n            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"\n        )\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        if isinstance(obj, TextResponse):\n            return obj.text\n        return obj.body.decode(\"utf-8\")\n    if isinstance(obj, str):\n        return obj if unicode else obj.encode(\"utf-8\")\n    return obj.decode(\"utf-8\") if unicode else obj\n", "scrapy/utils/versions.py": "import platform\nimport sys\nfrom typing import List, Tuple\n\nimport cryptography\nimport cssselect\nimport lxml.etree  # nosec\nimport parsel\nimport twisted\nimport w3lib\n\nimport scrapy\nfrom scrapy.utils.ssl import get_openssl_version\n\n\ndef scrapy_components_versions() -> List[Tuple[str, str]]:\n    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n\n    return [\n        (\"Scrapy\", scrapy.__version__),\n        (\"lxml\", lxml_version),\n        (\"libxml2\", libxml2_version),\n        (\"cssselect\", cssselect.__version__),\n        (\"parsel\", parsel.__version__),\n        (\"w3lib\", w3lib.__version__),\n        (\"Twisted\", twisted.version.short()),\n        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n        (\"pyOpenSSL\", get_openssl_version()),\n        (\"cryptography\", cryptography.__version__),\n        (\"Platform\", platform.platform()),\n    ]\n", "scrapy/utils/httpobj.py": "\"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n\nfrom typing import Union\nfrom urllib.parse import ParseResult, urlparse\nfrom weakref import WeakKeyDictionary\n\nfrom scrapy.http import Request, Response\n\n_urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = (\n    WeakKeyDictionary()\n)\n\n\ndef urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]\n", "scrapy/utils/ftp.py": "import posixpath\nfrom ftplib import FTP, error_perm\nfrom posixpath import dirname\nfrom typing import IO\n\n\ndef ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool = True) -> None:\n    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n    argument (as a ftplib.FTP object), creating all parent directories if they\n    don't exist. The ftplib.FTP object must be already connected and logged in.\n    \"\"\"\n    try:\n        ftp.cwd(path)\n    except error_perm:\n        ftp_makedirs_cwd(ftp, dirname(path), False)\n        ftp.mkd(path)\n        if first_call:\n            ftp.cwd(path)\n\n\ndef ftp_store_file(\n    *,\n    path: str,\n    file: IO[bytes],\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n    overwrite: bool = True,\n) -> None:\n    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n    to the directory extracted from given path, then uploads the file to server\n    \"\"\"\n    with FTP() as ftp:\n        ftp.connect(host, port)\n        ftp.login(username, password)\n        if use_active_mode:\n            ftp.set_pasv(False)\n        file.seek(0)\n        dirname, filename = posixpath.split(path)\n        ftp_makedirs_cwd(ftp, dirname)\n        command = \"STOR\" if overwrite else \"APPE\"\n        ftp.storbinary(f\"{command} {filename}\", file)\n        file.close()\n", "scrapy/utils/job.py": "from pathlib import Path\nfrom typing import Optional\n\nfrom scrapy.settings import BaseSettings\n\n\ndef job_dir(settings: BaseSettings) -> Optional[str]:\n    path: Optional[str] = settings[\"JOBDIR\"]\n    if not path:\n        return None\n    if not Path(path).exists():\n        Path(path).mkdir(parents=True)\n    return path\n", "scrapy/utils/conf.py": "import numbers\nimport os\nimport sys\nimport warnings\nfrom configparser import ConfigParser\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Collection,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning, UsageError\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.deprecate import update_classpath\nfrom scrapy.utils.python import without_none_values\n\n\ndef build_component_list(\n    compdict: MutableMapping[Any, Any],\n    custom: Any = None,\n    convert: Callable[[Any], Any] = update_classpath,\n) -> List[Any]:\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist: Collection[Any]) -> None:\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(\n                f\"Some paths in {complist!r} convert to the same object, \"\n                \"please update your settings\"\n            )\n\n    def _map_keys(compdict: Mapping[Any, Any]) -> Union[BaseSettings, Dict[Any, Any]]:\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                assert prio is not None\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(\n                        f\"Some paths in {list(compdict.keys())!r} \"\n                        \"convert to the same \"\n                        \"object, please update your settings\"\n                    )\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        _check_components(compdict)\n        return {convert(k): v for k, v in compdict.items()}\n\n    def _validate_values(compdict: Mapping[Any, Any]) -> None:\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(\n                    f\"Invalid value {value} for component {name}, \"\n                    \"please provide a real number or None instead\"\n                )\n\n    if custom is not None:\n        warnings.warn(\n            \"The 'custom' attribute of build_component_list() is deprecated. \"\n            \"Please merge its value into 'compdict' manually or change your \"\n            \"code to use Settings.getwithbase().\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        if isinstance(custom, (list, tuple)):\n            _check_components(custom)\n            return type(custom)(convert(c) for c in custom)  # type: ignore[return-value]\n        compdict.update(custom)\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n\n\ndef arglist_to_dict(arglist: List[str]) -> Dict[str, str]:\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split(\"=\", 1) for x in arglist)\n\n\ndef closest_scrapy_cfg(\n    path: Union[str, os.PathLike] = \".\",\n    prevpath: Optional[Union[str, os.PathLike]] = None,\n) -> str:\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if prevpath is not None and str(path) == str(prevpath):\n        return \"\"\n    path = Path(path).resolve()\n    cfgfile = path / \"scrapy.cfg\"\n    if cfgfile.exists():\n        return str(cfgfile)\n    return closest_scrapy_cfg(path.parent, path)\n\n\ndef init_env(project: str = \"default\", set_syspath: bool = True) -> None:\n    \"\"\"Initialize environment to use command-line tool from inside a project\n    dir. This sets the Scrapy settings module and modifies the Python path to\n    be able to locate the project module.\n    \"\"\"\n    cfg = get_config()\n    if cfg.has_option(\"settings\", project):\n        os.environ[\"SCRAPY_SETTINGS_MODULE\"] = cfg.get(\"settings\", project)\n    closest = closest_scrapy_cfg()\n    if closest:\n        projdir = str(Path(closest).parent)\n        if set_syspath and projdir not in sys.path:\n            sys.path.append(projdir)\n\n\ndef get_config(use_closest: bool = True) -> ConfigParser:\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg\n\n\ndef get_sources(use_closest: bool = True) -> List[str]:\n    xdg_config_home = (\n        os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n    )\n    sources = [\n        \"/etc/scrapy.cfg\",\n        r\"c:\\scrapy\\scrapy.cfg\",\n        str(Path(xdg_config_home) / \"scrapy.cfg\"),\n        str(Path(\"~/.scrapy.cfg\").expanduser()),\n    ]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources\n\n\ndef feed_complete_default_values_from_settings(\n    feed: Dict[str, Any], settings: BaseSettings\n) -> Dict[str, Any]:\n    out = feed.copy()\n    out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n    out.setdefault(\"item_export_kwargs\", {})\n    if settings[\"FEED_EXPORT_INDENT\"] is None:\n        out.setdefault(\"indent\", None)\n    else:\n        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n    return out\n\n\ndef feed_process_params_from_cli(\n    settings: BaseSettings,\n    output: List[str],\n    output_format: Optional[str] = None,\n    overwrite_output: Optional[List[str]] = None,\n) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Receives feed export params (from the 'crawl' or 'runspider' commands),\n    checks for inconsistencies in their quantities and returns a dictionary\n    suitable to be used as the FEEDS setting.\n    \"\"\"\n    valid_output_formats: Iterable[str] = without_none_values(\n        cast(Dict[str, str], settings.getwithbase(\"FEED_EXPORTERS\"))\n    ).keys()\n\n    def check_valid_format(output_format: str) -> None:\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )\n\n    overwrite = False\n    if overwrite_output:\n        if output:\n            raise UsageError(\n                \"Please use only one of -o/--output and -O/--overwrite-output\"\n            )\n        if output_format:\n            raise UsageError(\n                \"-t/--output-format is a deprecated command line option\"\n                \" and does not work in combination with -O/--overwrite-output.\"\n                \" To specify a format please specify it after a colon at the end of the\"\n                \" output URI (i.e. -O <URI>:<FORMAT>).\"\n                \" Example working in the tutorial: \"\n                \"scrapy crawl quotes -O quotes.json:json\"\n            )\n        output = overwrite_output\n        overwrite = True\n\n    if output_format:\n        if len(output) == 1:\n            check_valid_format(output_format)\n            message = (\n                \"The -t/--output-format command line option is deprecated in favor of \"\n                \"specifying the output format within the output URI using the -o/--output or the\"\n                \" -O/--overwrite-output option (i.e. -o/-O <URI>:<FORMAT>). See the documentation\"\n                \" of the -o or -O option or the following examples for more information. \"\n                \"Examples working in the tutorial: \"\n                \"scrapy crawl quotes -o quotes.csv:csv   or   \"\n                \"scrapy crawl quotes -O quotes.json:json\"\n            )\n            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n            return {output[0]: {\"format\": output_format}}\n        raise UsageError(\n            \"The -t command-line option cannot be used if multiple output \"\n            \"URIs are specified\"\n        )\n\n    result: Dict[str, Dict[str, Any]] = {}\n    for element in output:\n        try:\n            feed_uri, feed_format = element.rsplit(\":\", 1)\n            check_valid_format(feed_format)\n        except (ValueError, UsageError):\n            feed_uri = element\n            feed_format = Path(element).suffix.replace(\".\", \"\")\n        else:\n            if feed_uri == \"-\":\n                feed_uri = \"stdout:\"\n        check_valid_format(feed_format)\n        result[feed_uri] = {\"format\": feed_format}\n        if overwrite:\n            result[feed_uri][\"overwrite\"] = True\n\n    # FEEDS setting should take precedence over the matching CLI options\n    result.update(settings.getdict(\"FEEDS\"))\n\n    return result\n", "scrapy/utils/_compression.py": "import zlib\nfrom io import BytesIO\nfrom warnings import warn\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\ntry:\n    try:\n        import brotli\n    except ImportError:\n        import brotlicffi as brotli\nexcept ImportError:\n    pass\nelse:\n    try:\n        brotli.Decompressor.process\n    except AttributeError:\n        warn(\n            (\n                \"You have brotlipy installed, and Scrapy will use it, but \"\n                \"Scrapy support for brotlipy is deprecated and will stop \"\n                \"working in a future version of Scrapy. brotlipy itself is \"\n                \"deprecated, it has been superseded by brotlicffi. Please, \"\n                \"uninstall brotlipy and install brotli or brotlicffi instead. \"\n                \"brotlipy has the same import name as brotli, so keeping both \"\n                \"installed is strongly discouraged.\"\n            ),\n            ScrapyDeprecationWarning,\n        )\n\n        def _brotli_decompress(decompressor, data):\n            return decompressor.decompress(data)\n\n    else:\n\n        def _brotli_decompress(decompressor, data):\n            return decompressor.process(data)\n\n\ntry:\n    import zstandard\nexcept ImportError:\n    pass\n\n\n_CHUNK_SIZE = 65536  # 64 KiB\n\n\nclass _DecompressionMaxSizeExceeded(ValueError):\n    pass\n\n\ndef _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = zlib.decompressobj()\n    raw_decompressor = zlib.decompressobj(wbits=-15)\n    input_stream = BytesIO(data)\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        input_chunk = input_stream.read(_CHUNK_SIZE)\n        try:\n            output_chunk = decompressor.decompress(input_chunk)\n        except zlib.error:\n            if decompressor != raw_decompressor:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                decompressor = raw_decompressor\n                output_chunk = decompressor.decompress(input_chunk)\n            else:\n                raise\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = brotli.Decompressor()\n    input_stream = BytesIO(data)\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        input_chunk = input_stream.read(_CHUNK_SIZE)\n        output_chunk = _brotli_decompress(decompressor, input_chunk)\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = zstandard.ZstdDecompressor()\n    stream_reader = decompressor.stream_reader(BytesIO(data))\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        output_chunk = stream_reader.read(_CHUNK_SIZE)\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n", "scrapy/utils/python.py": "\"\"\"\nThis module contains essential stuff that should've come with Python itself ;)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport collections.abc\nimport gc\nimport inspect\nimport re\nimport sys\nimport weakref\nfrom functools import partial, wraps\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Pattern,\n    Tuple,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom scrapy.utils.asyncgen import as_async_generator\n\nif TYPE_CHECKING:\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\ndef flatten(x: Iterable[Any]) -> List[Any]:\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))\n\n\ndef iflatten(x: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            yield from iflatten(el)\n        else:\n            yield el\n\n\ndef is_listlike(x: Any) -> bool:\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n\n\ndef unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> List[_T]:\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result: List[_T] = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result\n\n\ndef to_unicode(\n    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n) -> str:\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError(\n            \"to_unicode must receive a bytes or str \"\n            f\"object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.decode(encoding, errors)\n\n\ndef to_bytes(\n    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n) -> bytes:\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError(\n            \"to_bytes must receive a str or bytes \" f\"object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.encode(encoding, errors)\n\n\ndef re_rsearch(\n    pattern: Union[str, Pattern[str]], text: str, chunk_size: int = 1024\n) -> Optional[Tuple[int, int]]:\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter() -> Iterable[Tuple[str, int]]:\n        offset = len(text)\n        while True:\n            offset -= chunk_size * 1024\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = list(pattern.finditer(chunk))\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None\n\n\n_SelfT = TypeVar(\"_SelfT\")\n\n\ndef memoizemethod_noargs(\n    method: Callable[Concatenate[_SelfT, _P], _T]\n) -> Callable[Concatenate[_SelfT, _P], _T]:\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache: weakref.WeakKeyDictionary[_SelfT, _T] = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self: _SelfT, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method\n\n\n_BINARYCHARS = {\n    i for i in range(32) if to_bytes(chr(i)) not in {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n}\n\n\ndef binary_is_text(data: bytes) -> bool:\n    \"\"\"Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)\n\n\ndef get_func_args(func: Callable[..., Any], stripself: bool = False) -> List[str]:\n    \"\"\"Return the argument name list of a callable object\"\"\"\n    if not callable(func):\n        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n\n    args: List[str] = []\n    try:\n        sig = inspect.signature(func)\n    except ValueError:\n        return args\n\n    if isinstance(func, partial):\n        partial_args = func.args\n        partial_kw = func.keywords\n\n        for name, param in sig.parameters.items():\n            if param.name in partial_args:\n                continue\n            if partial_kw and param.name in partial_kw:\n                continue\n            args.append(name)\n    else:\n        for name in sig.parameters.keys():\n            args.append(name)\n\n    if stripself and args and args[0] == \"self\":\n        args = args[1:]\n    return args\n\n\ndef get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getfullargspec(func)\n    elif hasattr(func, \"__call__\"):  # noqa: B004\n        spec = inspect.getfullargspec(func.__call__)\n    else:\n        raise TypeError(f\"{type(func)} is not callable\")\n\n    defaults: Tuple[Any, ...] = spec.defaults or ()\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs\n\n\ndef equal_attributes(\n    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable[[Any], Any]]]]\n) -> bool:\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True\n\n\n@overload\ndef without_none_values(iterable: Mapping[_KT, _VT]) -> Dict[_KT, _VT]: ...\n\n\n@overload\ndef without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n\n\ndef without_none_values(\n    iterable: Union[Mapping[_KT, _VT], Iterable[_KT]]\n) -> Union[Dict[_KT, _VT], Iterable[_KT]]:\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    if isinstance(iterable, collections.abc.Mapping):\n        return {k: v for k, v in iterable.items() if v is not None}\n    else:\n        # the iterable __init__ must take another iterable\n        return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n\n\ndef global_object_name(obj: Any) -> str:\n    \"\"\"\n    Return full name of a global object.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__qualname__}\"\n\n\nif hasattr(sys, \"pypy_version_info\"):\n\n    def garbage_collect() -> None:\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()\n\nelse:\n\n    def garbage_collect() -> None:\n        gc.collect()\n\n\nclass MutableChain(Iterable[_T]):\n    \"\"\"\n    Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T]):\n        self.data: Iterator[_T] = chain.from_iterable(args)\n\n    def extend(self, *iterables: Iterable[_T]) -> None:\n        self.data = chain(self.data, chain.from_iterable(iterables))\n\n    def __iter__(self) -> Iterator[_T]:\n        return self\n\n    def __next__(self) -> _T:\n        return next(self.data)\n\n\nasync def _async_chain(\n    *iterables: Union[Iterable[_T], AsyncIterable[_T]]\n) -> AsyncIterator[_T]:\n    for it in iterables:\n        async for o in as_async_generator(it):\n            yield o\n\n\nclass MutableAsyncChain(AsyncIterable[_T]):\n    \"\"\"\n    Similar to MutableChain but for async iterables\n    \"\"\"\n\n    def __init__(self, *args: Union[Iterable[_T], AsyncIterable[_T]]):\n        self.data: AsyncIterator[_T] = _async_chain(*args)\n\n    def extend(self, *iterables: Union[Iterable[_T], AsyncIterable[_T]]) -> None:\n        self.data = _async_chain(self.data, _async_chain(*iterables))\n\n    def __aiter__(self) -> AsyncIterator[_T]:\n        return self\n\n    async def __anext__(self) -> _T:\n        return await self.data.__anext__()\n", "scrapy/utils/template.py": "\"\"\"Helper functions for working with templates\"\"\"\n\nimport re\nimport string\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Any, Union\n\n\ndef render_templatefile(path: Union[str, PathLike], **kwargs: Any) -> None:\n    path_obj = Path(path)\n    raw = path_obj.read_text(\"utf8\")\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path_obj.with_suffix(\"\") if path_obj.suffix == \".tmpl\" else path_obj\n\n    if path_obj.suffix == \".tmpl\":\n        path_obj.rename(render_path)\n\n    render_path.write_text(content, \"utf8\")\n\n\nCAMELCASE_INVALID_CHARS = re.compile(r\"[^a-zA-Z\\d]\")\n\n\ndef string_camelcase(string: str) -> str:\n    \"\"\"Convert a word  to its CamelCase version and remove invalid chars\n\n    >>> string_camelcase('lost-pound')\n    'LostPound'\n\n    >>> string_camelcase('missing_images')\n    'MissingImages'\n\n    \"\"\"\n    return CAMELCASE_INVALID_CHARS.sub(\"\", string.title())\n", "scrapy/utils/misc.py": "\"\"\"Helper functions which don't fit anywhere else\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport hashlib\nimport inspect\nimport os\nimport re\nimport warnings\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom importlib import import_module\nfrom pkgutil import iter_modules\nfrom types import ModuleType\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Deque,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.item import Item\nfrom scrapy.utils.datatypes import LocalWeakReferencedCache\n\nif TYPE_CHECKING:\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n_ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\nT = TypeVar(\"T\")\n\n\ndef arg_to_iter(arg: Any) -> Iterable[Any]:\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, \"__iter__\"):\n        return cast(Iterable[Any], arg)\n    return [arg]\n\n\ndef load_object(path: Union[str, Callable[..., Any]]) -> Any:\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    The object can be the import path of a class, function, variable or an\n    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n\n    If ``path`` is not a string, but is a callable object, such as a class or\n    a function, then return it as is.\n    \"\"\"\n\n    if not isinstance(path, str):\n        if callable(path):\n            return path\n        raise TypeError(\n            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n        )\n\n    try:\n        dot = path.rindex(\".\")\n    except ValueError:\n        raise ValueError(f\"Error loading object '{path}': not a full path\")\n\n    module, name = path[:dot], path[dot + 1 :]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n\n    return obj\n\n\ndef walk_modules(path: str) -> List[ModuleType]:\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods: List[ModuleType] = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, \"__path__\"):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + \".\" + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods\n\n\ndef md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    warnings.warn(\n        (\n            \"The scrapy.utils.misc.md5sum function is deprecated, and will be \"\n            \"removed in a future version of Scrapy.\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    m = hashlib.md5()  # nosec\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\ndef rel_has_nofollow(rel: Optional[str]) -> bool:\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n\n\ndef create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n\n    .. versionchanged:: 2.2\n       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n       extension has not been implemented correctly).\n    \"\"\"\n    warnings.warn(\n        \"The create_instance() function is deprecated. \"\n        \"Please use build_from_crawler() or build_from_settings() instead.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specify at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return instance\n\n\ndef build_from_crawler(\n    objcls: Type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n) -> T:\n    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n\n    Raises ``TypeError`` if the resulting instance is ``None``.\n    \"\"\"\n    if hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return cast(T, instance)\n\n\ndef build_from_settings(\n    objcls: Type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n) -> T:\n    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n\n    Raises ``TypeError`` if the resulting instance is ``None``.\n    \"\"\"\n    if hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return cast(T, instance)\n\n\n@contextmanager\ndef set_environ(**kwargs: str) -> Iterator[None]:\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v\n\n\ndef walk_callable(node: ast.AST) -> Iterable[ast.AST]:\n    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n    functions defined within the node.\n    \"\"\"\n    todo: Deque[ast.AST] = deque([node])\n    walked_func_def = False\n    while todo:\n        node = todo.popleft()\n        if isinstance(node, ast.FunctionDef):\n            if walked_func_def:\n                continue\n            walked_func_def = True\n        todo.extend(ast.iter_child_nodes(node))\n        yield node\n\n\n_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n\n\ndef is_generator_with_return_value(callable: Callable[..., Any]) -> bool:\n    \"\"\"\n    Returns True if a callable is a generator function which includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n    if callable in _generator_callbacks_cache:\n        return bool(_generator_callbacks_cache[callable])\n\n    def returns_none(return_node: ast.Return) -> bool:\n        value = return_node.value\n        return (\n            value is None or isinstance(value, ast.NameConstant) and value.value is None\n        )\n\n    if inspect.isgeneratorfunction(callable):\n        func = callable\n        while isinstance(func, partial):\n            func = func.func\n\n        src = inspect.getsource(func)\n        pattern = re.compile(r\"(^[\\t ]+)\")\n        code = pattern.sub(\"\", src)\n\n        match = pattern.match(src)  # finds indentation\n        if match:\n            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n\n        tree = ast.parse(code)\n        for node in walk_callable(tree):\n            if isinstance(node, ast.Return) and not returns_none(node):\n                _generator_callbacks_cache[callable] = True\n                return bool(_generator_callbacks_cache[callable])\n\n    _generator_callbacks_cache[callable] = False\n    return bool(_generator_callbacks_cache[callable])\n\n\ndef warn_on_generator_with_return_value(\n    spider: Spider, callable: Callable[..., Any]\n) -> None:\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    try:\n        if is_generator_with_return_value(callable):\n            warnings.warn(\n                f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                'a generator and includes a \"return\" statement with a value '\n                \"different than None. This could lead to unexpected behaviour. Please see \"\n                \"https://docs.python.org/3/reference/simple_stmts.html#the-return-statement \"\n                'for details about the semantics of the \"return\" statement within generators',\n                stacklevel=2,\n            )\n    except IndentationError:\n        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n        warnings.warn(\n            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n            \"This will not prevent your code from working, but it prevents Scrapy from detecting \"\n            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n            \"Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), \"\n            f'including the code of \"{callable_name}\"',\n            stacklevel=2,\n        )\n", "scrapy/utils/ssl.py": "from typing import Any, Optional\n\nimport OpenSSL._util as pyOpenSSLutil\nimport OpenSSL.SSL\nimport OpenSSL.version\nfrom OpenSSL.crypto import X509Name\n\nfrom scrapy.utils.python import to_unicode\n\n\ndef ffi_buf_to_string(buf: Any) -> str:\n    return to_unicode(pyOpenSSLutil.ffi.string(buf))\n\n\ndef x509name_to_string(x509name: X509Name) -> str:\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer: Any = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(\n        x509name._name, result_buffer, len(result_buffer)  # type: ignore[attr-defined]\n    )\n\n    return ffi_buf_to_string(result_buffer)\n\n\ndef get_temp_key_info(ssl_object: Any) -> Optional[str]:\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    if not hasattr(pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"):\n        # removed in cryptography 40.0.0\n        return None\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append(\"RSA\")\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append(\"DH\")\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append(\"ECDH\")\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(\n            pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key)\n        )\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append(f\"{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits\")\n    return \", \".join(key_info)\n\n\ndef get_openssl_version() -> str:\n    system_openssl_bytes = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n    system_openssl = system_openssl_bytes.decode(\"ascii\", errors=\"replace\")\n    return f\"{OpenSSL.version.__version__} ({system_openssl})\"\n", "scrapy/utils/decorators.py": "from __future__ import annotations\n\nimport warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar\n\nfrom twisted.internet import defer, threads\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n\n_T = TypeVar(\"_T\")\n\n\ndef deprecated(\n    use_instead: Any = None,\n) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    def deco(func: Callable[_P, _T]) -> Callable[_P, _T]:\n        @wraps(func)\n        def wrapped(*args: _P.args, **kwargs: _P.kwargs) -> Any:\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return wrapped\n\n    if callable(use_instead):\n        deco = deco(use_instead)\n        use_instead = None\n    return deco\n\n\ndef defers(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n\n    @wraps(func)\n    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n        return defer.maybeDeferred(func, *a, **kw)\n\n    return wrapped\n\n\ndef inthread(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Decorator to call a function in a thread and return a deferred with the\n    result\n    \"\"\"\n\n    @wraps(func)\n    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n        return threads.deferToThread(func, *a, **kw)\n\n    return wrapped\n", "scrapy/utils/benchserver.py": "import random\nfrom typing import Any\nfrom urllib.parse import urlencode\n\nfrom twisted.web.resource import Resource\nfrom twisted.web.server import Request, Site\n\n\nclass Root(Resource):\n    isLeaf = True\n\n    def getChild(self, name: str, request: Request) -> Resource:\n        return self\n\n    def render(self, request: Request) -> bytes:\n        total = _getarg(request, b\"total\", 100, int)\n        show = _getarg(request, b\"show\", 10, int)\n        nlist = [random.randint(1, total) for _ in range(show)]  # nosec\n        request.write(b\"<html><head></head><body>\")\n        assert request.args is not None\n        args = request.args.copy()\n        for nl in nlist:\n            args[\"n\"] = nl\n            argstr = urlencode(args, doseq=True)\n            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\".encode())\n        request.write(b\"</body></html>\")\n        return b\"\"\n\n\ndef _getarg(request, name: bytes, default: Any = None, type=str):\n    return type(request.args[name][0]) if name in request.args else default\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor\n\n    root = Root()\n    factory = Site(root)\n    httpPort = reactor.listenTCP(8998, Site(root))\n\n    def _print_listening() -> None:\n        httpHost = httpPort.getHost()\n        print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n\n    reactor.callWhenRunning(_print_listening)\n    reactor.run()\n", "scrapy/utils/url.py": "\"\"\"\nThis module contains general purpose URL functions not found in the standard\nlibrary.\n\nSome of the functions that used to be imported from this module have been moved\nto the w3lib.url module. Always import those from there instead.\n\"\"\"\n\nimport re\nfrom typing import TYPE_CHECKING, Iterable, Optional, Type, Union, cast\nfrom urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n\n# scrapy.utils.url was moved to w3lib.url and import * ensures this\n# move doesn't break old code\nfrom w3lib.url import *\nfrom w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from scrapy import Spider\n\n\nUrlT = Union[str, bytes, ParseResult]\n\n\ndef url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith(f\".{d}\")) for d in domains)\n\n\ndef url_is_from_spider(url: UrlT, spider: Type[\"Spider\"]) -> bool:\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(\n        url, [spider.name] + list(getattr(spider, \"allowed_domains\", []))\n    )\n\n\ndef url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n    \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n    lowercase_path = parse_url(url).path.lower()\n    return any(lowercase_path.endswith(ext) for ext in extensions)\n\n\ndef parse_url(url: UrlT, encoding: Optional[str] = None) -> ParseResult:\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return cast(ParseResult, urlparse(to_unicode(url, encoding)))\n\n\ndef escape_ajax(url: str) -> str:\n    \"\"\"\n    Return the crawlable url according to:\n    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith(\"!\"):\n        return url\n    return add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n\n\ndef add_http_if_no_scheme(url: str) -> str:\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.I)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url\n\n\ndef _is_posix_path(string: str) -> bool:\n    return bool(\n        re.match(\n            r\"\"\"\n            ^                   # start with...\n            (\n                \\.              # ...a single dot,\n                (\n                    \\. | [^/\\.]+  # optionally followed by\n                )?                # either a second dot or some characters\n                |\n                ~   # $HOME\n            )?      # optional match of \".\", \"..\" or \".blabla\"\n            /       # at least one \"/\" for a file path,\n            .       # and something after the \"/\"\n            \"\"\",\n            string,\n            flags=re.VERBOSE,\n        )\n    )\n\n\ndef _is_windows_path(string: str) -> bool:\n    return bool(\n        re.match(\n            r\"\"\"\n            ^\n            (\n                [a-z]:\\\\\n                | \\\\\\\\\n            )\n            \"\"\",\n            string,\n            flags=re.IGNORECASE | re.VERBOSE,\n        )\n    )\n\n\ndef _is_filesystem_path(string: str) -> bool:\n    return _is_posix_path(string) or _is_windows_path(string)\n\n\ndef guess_scheme(url: str) -> str:\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n    http:// otherwise.\"\"\"\n    if _is_filesystem_path(url):\n        return any_to_uri(url)\n    return add_http_if_no_scheme(url)\n\n\ndef strip_url(\n    url: str,\n    strip_credentials: bool = True,\n    strip_default_port: bool = True,\n    origin_only: bool = False,\n    strip_fragment: bool = True,\n) -> str:\n    \"\"\"Strip URL string from some of its components:\n\n    - ``strip_credentials`` removes \"user:password@\"\n    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n      from http:// (resp. https://, ftp://) URLs\n    - ``origin_only`` replaces path component with \"/\", also dropping\n      query and fragment components ; it also strips credentials\n    - ``strip_fragment`` drops any #fragment component\n    \"\"\"\n\n    parsed_url = urlparse(url)\n    netloc = parsed_url.netloc\n    if (strip_credentials or origin_only) and (\n        parsed_url.username or parsed_url.password\n    ):\n        netloc = netloc.split(\"@\")[-1]\n    if strip_default_port and parsed_url.port:\n        if (parsed_url.scheme, parsed_url.port) in (\n            (\"http\", 80),\n            (\"https\", 443),\n            (\"ftp\", 21),\n        ):\n            netloc = netloc.replace(f\":{parsed_url.port}\", \"\")\n    return urlunparse(\n        (\n            parsed_url.scheme,\n            netloc,\n            \"/\" if origin_only else parsed_url.path,\n            \"\" if origin_only else parsed_url.params,\n            \"\" if origin_only else parsed_url.query,\n            \"\" if strip_fragment else parsed_url.fragment,\n        )\n    )\n", "scrapy/utils/curl.py": "import argparse\nimport warnings\nfrom http.cookies import SimpleCookie\nfrom shlex import split\nfrom typing import Any, Dict, List, NoReturn, Optional, Sequence, Tuple, Union\nfrom urllib.parse import urlparse\n\nfrom w3lib.http import basic_auth_header\n\n\nclass DataAction(argparse.Action):\n    def __call__(\n        self,\n        parser: argparse.ArgumentParser,\n        namespace: argparse.Namespace,\n        values: Union[str, Sequence[Any], None],\n        option_string: Optional[str] = None,\n    ) -> None:\n        value = str(values)\n        if value.startswith(\"$\"):\n            value = value[1:]\n        setattr(namespace, self.dest, value)\n\n\nclass CurlParser(argparse.ArgumentParser):\n    def error(self, message: str) -> NoReturn:\n        error_msg = f\"There was an error parsing the curl command: {message}\"\n        raise ValueError(error_msg)\n\n\ncurl_parser = CurlParser()\ncurl_parser.add_argument(\"url\")\ncurl_parser.add_argument(\"-H\", \"--header\", dest=\"headers\", action=\"append\")\ncurl_parser.add_argument(\"-X\", \"--request\", dest=\"method\")\ncurl_parser.add_argument(\"-d\", \"--data\", \"--data-raw\", dest=\"data\", action=DataAction)\ncurl_parser.add_argument(\"-u\", \"--user\", dest=\"auth\")\n\n\nsafe_to_ignore_arguments = [\n    [\"--compressed\"],\n    # `--compressed` argument is not safe to ignore, but it's included here\n    # because the `HttpCompressionMiddleware` is enabled by default\n    [\"-s\", \"--silent\"],\n    [\"-v\", \"--verbose\"],\n    [\"-#\", \"--progress-bar\"],\n]\n\nfor argument in safe_to_ignore_arguments:\n    curl_parser.add_argument(*argument, action=\"store_true\")\n\n\ndef _parse_headers_and_cookies(\n    parsed_args: argparse.Namespace,\n) -> Tuple[List[Tuple[str, bytes]], Dict[str, str]]:\n    headers: List[Tuple[str, bytes]] = []\n    cookies: Dict[str, str] = {}\n    for header in parsed_args.headers or ():\n        name, val = header.split(\":\", 1)\n        name = name.strip()\n        val = val.strip()\n        if name.title() == \"Cookie\":\n            for name, morsel in SimpleCookie(val).items():\n                cookies[name] = morsel.value\n        else:\n            headers.append((name, val))\n\n    if parsed_args.auth:\n        user, password = parsed_args.auth.split(\":\", 1)\n        headers.append((\"Authorization\", basic_auth_header(user, password)))\n\n    return headers, cookies\n\n\ndef curl_to_request_kwargs(\n    curl_command: str, ignore_unknown_options: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Convert a cURL command syntax to Request kwargs.\n\n    :param str curl_command: string containing the curl command\n    :param bool ignore_unknown_options: If true, only a warning is emitted when\n                                        cURL options are unknown. Otherwise\n                                        raises an error. (default: True)\n    :return: dictionary of Request kwargs\n    \"\"\"\n\n    curl_args = split(curl_command)\n\n    if curl_args[0] != \"curl\":\n        raise ValueError('A curl command must start with \"curl\"')\n\n    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n\n    if argv:\n        msg = f'Unrecognized options: {\", \".join(argv)}'\n        if ignore_unknown_options:\n            warnings.warn(msg)\n        else:\n            raise ValueError(msg)\n\n    url = parsed_args.url\n\n    # curl automatically prepends 'http' if the scheme is missing, but Request\n    # needs the scheme to work\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme:\n        url = \"http://\" + url\n\n    method = parsed_args.method or \"GET\"\n\n    result: Dict[str, Any] = {\"method\": method.upper(), \"url\": url}\n\n    headers, cookies = _parse_headers_and_cookies(parsed_args)\n\n    if headers:\n        result[\"headers\"] = headers\n    if cookies:\n        result[\"cookies\"] = cookies\n    if parsed_args.data:\n        result[\"body\"] = parsed_args.data\n        if not parsed_args.method:\n            # if the \"data\" is specified but the \"method\" is not specified,\n            # the default method is 'POST'\n            result[\"method\"] = \"POST\"\n\n    return result\n", "scrapy/utils/console.py": "from functools import wraps\nfrom typing import Any, Callable, Dict, Iterable, Optional\n\nEmbedFuncT = Callable[..., None]\nKnownShellsT = Dict[str, Callable[..., EmbedFuncT]]\n\n\ndef _embed_ipython_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start an IPython Shell\"\"\"\n    try:\n        from IPython.terminal.embed import InteractiveShellEmbed  # noqa: T100\n        from IPython.terminal.ipapp import load_default_config\n    except ImportError:\n        from IPython.frontend.terminal.embed import (  # type: ignore[no-redef]  # noqa: T100\n            InteractiveShellEmbed,\n        )\n        from IPython.frontend.terminal.ipapp import (  # type: ignore[no-redef]\n            load_default_config,\n        )\n\n    @wraps(_embed_ipython_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        config = load_default_config()\n        # Always use .instance() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config\n        )\n        shell()\n\n    return wrapper\n\n\ndef _embed_bpython_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a bpython shell\"\"\"\n    import bpython\n\n    @wraps(_embed_bpython_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        bpython.embed(locals_=namespace, banner=banner)\n\n    return wrapper\n\n\ndef _embed_ptpython_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a ptpython shell\"\"\"\n    import ptpython.repl\n\n    @wraps(_embed_ptpython_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        print(banner)\n        ptpython.repl.embed(locals=namespace)\n\n    return wrapper\n\n\ndef _embed_standard_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a standard python shell\"\"\"\n    import code\n\n    try:  # readline module is only available on unix systems\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter  # noqa: F401\n\n        readline.parse_and_bind(\"tab:complete\")\n\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        code.interact(banner=banner, local=namespace)\n\n    return wrapper\n\n\nDEFAULT_PYTHON_SHELLS: KnownShellsT = {\n    \"ptpython\": _embed_ptpython_shell,\n    \"ipython\": _embed_ipython_shell,\n    \"bpython\": _embed_bpython_shell,\n    \"python\": _embed_standard_shell,\n}\n\n\ndef get_shell_embed_func(\n    shells: Optional[Iterable[str]] = None, known_shells: Optional[KnownShellsT] = None\n) -> Any:\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None:  # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None:  # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue\n\n\ndef start_python_console(\n    namespace: Optional[Dict[str, Any]] = None,\n    banner: str = \"\",\n    shells: Optional[Iterable[str]] = None,\n) -> None:\n    \"\"\"Start Python console bound to the given namespace.\n    Readline support and tab completion will be used on Unix, if available.\n    \"\"\"\n    if namespace is None:\n        namespace = {}\n\n    try:\n        shell = get_shell_embed_func(shells)\n        if shell is not None:\n            shell(namespace=namespace, banner=banner)\n    except SystemExit:  # raised when using exit() in python code.interact\n        pass\n", "scrapy/utils/__init__.py": "", "scrapy/utils/asyncgen.py": "from typing import AsyncGenerator, AsyncIterable, Iterable, List, TypeVar, Union\n\n_T = TypeVar(\"_T\")\n\n\nasync def collect_asyncgen(result: AsyncIterable[_T]) -> List[_T]:\n    results = []\n    async for x in result:\n        results.append(x)\n    return results\n\n\nasync def as_async_generator(\n    it: Union[Iterable[_T], AsyncIterable[_T]]\n) -> AsyncGenerator[_T, None]:\n    \"\"\"Wraps an iterable (sync or async) into an async generator.\"\"\"\n    if isinstance(it, AsyncIterable):\n        async for r in it:\n            yield r\n    else:\n        for r in it:\n            yield r\n", "scrapy/utils/project.py": "import os\nimport warnings\nfrom importlib import import_module\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Union\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n\nENVVAR = \"SCRAPY_SETTINGS_MODULE\"\nDATADIR_CFG_SECTION = \"datadir\"\n\n\ndef inside_project() -> bool:\n    scrapy_module = os.environ.get(ENVVAR)\n    if scrapy_module:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(\n                f\"Cannot import scrapy settings module {scrapy_module}: {exc}\"\n            )\n        else:\n            return True\n    return bool(closest_scrapy_cfg())\n\n\ndef project_data_dir(project: str = \"default\") -> str:\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = Path(cfg.get(DATADIR_CFG_SECTION, project))\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\n                \"Unable to find scrapy.cfg file to infer project data dir\"\n            )\n        d = (Path(scrapy_cfg).parent / \".scrapy\").resolve()\n    if not d.exists():\n        d.mkdir(parents=True)\n    return str(d)\n\n\ndef data_path(path: Union[str, PathLike], createdir: bool = False) -> str:\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    path_obj = Path(path)\n    if not path_obj.is_absolute():\n        if inside_project():\n            path_obj = Path(project_data_dir(), path)\n        else:\n            path_obj = Path(\".scrapy\", path)\n    if createdir and not path_obj.exists():\n        path_obj.mkdir(parents=True)\n    return str(path_obj)\n\n\ndef get_project_settings() -> Settings:\n    if ENVVAR not in os.environ:\n        project = os.environ.get(\"SCRAPY_PROJECT\", \"default\")\n        init_env(project)\n\n    settings = Settings()\n    settings_module_path = os.environ.get(ENVVAR)\n    if settings_module_path:\n        settings.setmodule(settings_module_path, priority=\"project\")\n\n    valid_envvars = {\n        \"CHECK\",\n        \"PROJECT\",\n        \"PYTHON_SHELL\",\n        \"SETTINGS_MODULE\",\n    }\n\n    scrapy_envvars = {\n        k[7:]: v\n        for k, v in os.environ.items()\n        if k.startswith(\"SCRAPY_\") and k.replace(\"SCRAPY_\", \"\") in valid_envvars\n    }\n\n    settings.setdict(scrapy_envvars, priority=\"project\")\n\n    return settings\n", "scrapy/utils/defer.py": "\"\"\"\nHelper functions for dealing with Twisted deferreds\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport warnings\nfrom asyncio import Future\nfrom functools import wraps\nfrom types import CoroutineType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Coroutine,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred, DeferredList, ensureDeferred\nfrom twisted.internet.task import Cooperator\nfrom twisted.python import failure\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\nfrom scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n\nif TYPE_CHECKING:\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_T2 = TypeVar(\"_T2\")\n\n# copied from twisted.internet.defer\n_SelfResultT = TypeVar(\"_SelfResultT\")\n_DeferredListResultItemT = Tuple[bool, _SelfResultT]\nDeferredListResultListT = List[_DeferredListResultItemT[_SelfResultT]]\n\n\ndef defer_fail(_failure: Failure) -> Deferred:\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n\n    d: Deferred = Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d\n\n\ndef defer_succeed(result: _T) -> Deferred[_T]:\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n\n    d: Deferred = Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d\n\n\ndef defer_result(result: Any) -> Deferred:\n    if isinstance(result, Deferred):\n        return result\n    if isinstance(result, failure.Failure):\n        return defer_fail(result)\n    return defer_succeed(result)\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, Deferred[_T]], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[_T]: ...\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, Coroutine[Deferred[Any], Any, _T]],\n    *args: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[_T]: ...\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, _T], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[_T]: ...\n\n\ndef mustbe_deferred(\n    f: Callable[_P, Union[Deferred[_T], Coroutine[Deferred[Any], Any, _T], _T]],\n    *args: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[_T]:\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except Exception:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)\n\n\ndef parallel(\n    iterable: Iterable[_T],\n    count: int,\n    callable: Callable[Concatenate[_T, _P], _T2],\n    *args: _P.args,\n    **named: _P.kwargs,\n) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: https://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = Cooperator()\n    work = (callable(elem, *args, **named) for elem in iterable)\n    return DeferredList([coop.coiterate(work) for _ in range(count)])\n\n\nclass _AsyncCooperatorAdapter(Iterator[Deferred]):\n    \"\"\"A class that wraps an async iterable into a normal iterator suitable\n    for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n    it calls the callable directly in the callback, instead of providing a more\n    generic interface.\n\n    On the outside, this class behaves as an iterator that yields Deferreds.\n    Each Deferred is fired with the result of the callable which was called on\n    the next result from aiterator. It raises StopIteration when aiterator is\n    exhausted, as expected.\n\n    Cooperator calls __next__() multiple times and waits on the Deferreds\n    returned from it. As async generators (since Python 3.8) don't support\n    awaiting on __anext__() several times in parallel, we need to serialize\n    this. It's done by storing the Deferreds returned from __next__() and\n    firing the oldest one when a result from __anext__() is available.\n\n    The workflow:\n    1. When __next__() is called for the first time, it creates a Deferred, stores it\n    in self.waiting_deferreds and returns it. It also makes a Deferred that will wait\n    for self.aiterator.__anext__() and puts it into self.anext_deferred.\n    2. If __next__() is called again before self.anext_deferred fires, more Deferreds\n    are added to self.waiting_deferreds.\n    3. When self.anext_deferred fires, it either calls _callback() or _errback(). Both\n    clear self.anext_deferred.\n    3.1. _callback() calls the callable passing the result value that it takes, pops a\n    Deferred from self.waiting_deferreds, and if the callable result was a Deferred, it\n    chains those Deferreds so that the waiting Deferred will fire when the result\n    Deferred does, otherwise it fires it directly. This causes one awaiting task to\n    receive a result. If self.waiting_deferreds is still not empty, new __anext__() is\n    called and self.anext_deferred is populated.\n    3.2. _errback() checks the exception class. If it's StopAsyncIteration it means\n    self.aiterator is exhausted and so it sets self.finished and fires all\n    self.waiting_deferreds. Other exceptions are propagated.\n    4. If __next__() is called after __anext__() was handled, then if self.finished is\n    True, it raises StopIteration, otherwise it acts like in step 2, but if\n    self.anext_deferred is now empty is also populates it with a new __anext__().\n\n    Note that CooperativeTask ignores the value returned from the Deferred that it waits\n    for, so we fire them with None when needed.\n\n    It may be possible to write an async iterator-aware replacement for\n    Cooperator/CooperativeTask and use it instead of this adapter to achieve the same\n    goal.\n    \"\"\"\n\n    def __init__(\n        self,\n        aiterable: AsyncIterable[_T],\n        callable: Callable[Concatenate[_T, _P], _T2],\n        *callable_args: _P.args,\n        **callable_kwargs: _P.kwargs,\n    ):\n        self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n        self.callable: Callable[Concatenate[_T, _P], _T2] = callable\n        self.callable_args: Tuple[Any, ...] = callable_args\n        self.callable_kwargs: Dict[str, Any] = callable_kwargs\n        self.finished: bool = False\n        self.waiting_deferreds: List[Deferred] = []\n        self.anext_deferred: Optional[Deferred[_T]] = None\n\n    def _callback(self, result: _T) -> None:\n        # This gets called when the result from aiterator.__anext__() is available.\n        # It calls the callable on it and sends the result to the oldest waiting Deferred\n        # (by chaining if the result is a Deferred too or by firing if not).\n        self.anext_deferred = None\n        callable_result = self.callable(\n            result, *self.callable_args, **self.callable_kwargs\n        )\n        d = self.waiting_deferreds.pop(0)\n        if isinstance(callable_result, Deferred):\n            callable_result.chainDeferred(d)\n        else:\n            d.callback(None)\n        if self.waiting_deferreds:\n            self._call_anext()\n\n    def _errback(self, failure: Failure) -> None:\n        # This gets called on any exceptions in aiterator.__anext__().\n        # It handles StopAsyncIteration by stopping the iteration and reraises all others.\n        self.anext_deferred = None\n        failure.trap(StopAsyncIteration)\n        self.finished = True\n        for d in self.waiting_deferreds:\n            d.callback(None)\n\n    def _call_anext(self) -> None:\n        # This starts waiting for the next result from aiterator.\n        # If aiterator is exhausted, _errback will be called.\n        self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n        self.anext_deferred.addCallbacks(self._callback, self._errback)\n\n    def __next__(self) -> Deferred:\n        # This puts a new Deferred into self.waiting_deferreds and returns it.\n        # It also calls __anext__() if needed.\n        if self.finished:\n            raise StopIteration\n        d: Deferred = Deferred()\n        self.waiting_deferreds.append(d)\n        if not self.anext_deferred:\n            self._call_anext()\n        return d\n\n\ndef parallel_async(\n    async_iterable: AsyncIterable[_T],\n    count: int,\n    callable: Callable[Concatenate[_T, _P], _T2],\n    *args: _P.args,\n    **named: _P.kwargs,\n) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n    \"\"\"Like parallel but for async iterators\"\"\"\n    coop = Cooperator()\n    work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n    dl: Deferred = DeferredList([coop.coiterate(work) for _ in range(count)])\n    return dl\n\n\ndef process_chain(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    input: Any,\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d: Deferred = Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d\n\n\ndef process_chain_both(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    errbacks: Iterable[Callable[Concatenate[Failure, _P], Any]],\n    input: Any,\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    warnings.warn(\n        \"process_chain_both() is deprecated and will be removed in a future\"\n        \" Scrapy version.\",\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    d: Deferred = Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallback(cb, *a, **kw)\n        d.addErrback(eb, *a, **kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d\n\n\ndef process_parallel(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    input: Any,\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d: Deferred = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n    d.addCallback(lambda r: [x[1] for x in r])\n    d.addErrback(lambda f: f.value.subFailure)\n    return d\n\n\ndef iter_errback(\n    iterable: Iterable[_T],\n    errback: Callable[Concatenate[Failure, _P], Any],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Iterable[_T]:\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\nasync def aiter_errback(\n    aiterable: AsyncIterable[_T],\n    errback: Callable[Concatenate[Failure, _P], Any],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> AsyncIterable[_T]:\n    \"\"\"Wraps an async iterable calling an errback if an error is caught while\n    iterating it. Similar to scrapy.utils.defer.iter_errback()\n    \"\"\"\n    it = aiterable.__aiter__()\n    while True:\n        try:\n            yield await it.__anext__()\n        except StopAsyncIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\n_CT = TypeVar(\"_CT\", bound=Union[Awaitable, CoroutineType, Future])\n\n\n@overload\ndef deferred_from_coro(o: _CT) -> Deferred: ...\n\n\n@overload\ndef deferred_from_coro(o: _T) -> _T: ...\n\n\ndef deferred_from_coro(o: _T) -> Union[Deferred, _T]:\n    \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n    if isinstance(o, Deferred):\n        return o\n    if asyncio.isfuture(o) or inspect.isawaitable(o):\n        if not is_asyncio_reactor_installed():\n            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n            return ensureDeferred(cast(Coroutine[Deferred, Any, Any], o))\n        # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n        event_loop = _get_asyncio_event_loop()\n        return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n    return o\n\n\ndef deferred_f_from_coro_f(\n    coro_f: Callable[_P, Coroutine[Any, Any, _T]]\n) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Converts a coroutine function into a function that returns a Deferred.\n\n    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n    This is useful for callback chains, as callback functions are called with the previous callback result.\n    \"\"\"\n\n    @wraps(coro_f)\n    def f(*coro_args: _P.args, **coro_kwargs: _P.kwargs) -> Any:\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n\n    return f\n\n\ndef maybeDeferred_coro(\n    f: Callable[_P, Any], *args: _P.args, **kw: _P.kwargs\n) -> Deferred:\n    \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n    try:\n        result = f(*args, **kw)\n    except:  # noqa: E722,B001\n        return defer.fail(failure.Failure(captureVars=Deferred.debug))\n\n    if isinstance(result, Deferred):\n        return result\n    if asyncio.isfuture(result) or inspect.isawaitable(result):\n        return deferred_from_coro(result)\n    if isinstance(result, failure.Failure):\n        return defer.fail(result)\n    return defer.succeed(result)\n\n\ndef deferred_to_future(d: Deferred) -> Future:\n    \"\"\"\n    .. versionadded:: 2.6.0\n\n    Return an :class:`asyncio.Future` object that wraps *d*.\n\n    When :ref:`using the asyncio reactor <install-asyncio>`, you cannot await\n    on :class:`~twisted.internet.defer.Deferred` objects from :ref:`Scrapy\n    callables defined as coroutines <coroutine-support>`, you can only await on\n    ``Future`` objects. Wrapping ``Deferred`` objects into ``Future`` objects\n    allows you to wait on them::\n\n        class MySpider(Spider):\n            ...\n            async def parse(self, response):\n                additional_request = scrapy.Request('https://example.org/price')\n                deferred = self.crawler.engine.download(additional_request)\n                additional_response = await deferred_to_future(deferred)\n    \"\"\"\n    return d.asFuture(_get_asyncio_event_loop())\n\n\ndef maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n    \"\"\"\n    .. versionadded:: 2.6.0\n\n    Return *d* as an object that can be awaited from a :ref:`Scrapy callable\n    defined as a coroutine <coroutine-support>`.\n\n    What you can await in Scrapy callables defined as coroutines depends on the\n    value of :setting:`TWISTED_REACTOR`:\n\n    -   When not using the asyncio reactor, you can only await on\n        :class:`~twisted.internet.defer.Deferred` objects.\n\n    -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only\n        await on :class:`asyncio.Future` objects.\n\n    If you want to write code that uses ``Deferred`` objects but works with any\n    reactor, use this function on all ``Deferred`` objects::\n\n        class MySpider(Spider):\n            ...\n            async def parse(self, response):\n                additional_request = scrapy.Request('https://example.org/price')\n                deferred = self.crawler.engine.download(additional_request)\n                additional_response = await maybe_deferred_to_future(deferred)\n    \"\"\"\n    if not is_asyncio_reactor_installed():\n        return d\n    return deferred_to_future(d)\n", "scrapy/utils/deprecate.py": "\"\"\"Some helpers for deprecation messages\"\"\"\n\nimport inspect\nimport warnings\nfrom typing import Any, Dict, List, Optional, Tuple, Type, overload\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\n\ndef attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> None:\n    cname = obj.__class__.__name__\n    warnings.warn(\n        f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n        f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n        ScrapyDeprecationWarning,\n        stacklevel=3,\n    )\n\n\ndef create_deprecated_class(\n    name: str,\n    new_class: type,\n    clsdict: Optional[Dict[str, Any]] = None,\n    warn_category: Type[Warning] = ScrapyDeprecationWarning,\n    warn_once: bool = True,\n    old_class_path: Optional[str] = None,\n    new_class_path: Optional[str] = None,\n    subclass_warn_message: str = \"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n    instance_warn_message: str = \"{cls} is deprecated, instantiate {new} instead.\",\n) -> type:\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    # https://github.com/python/mypy/issues/4177\n    class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]\n        deprecated_class: Optional[type] = None\n        warned_on_subclass: bool = False\n\n        def __new__(\n            metacls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]\n        ) -> type:\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(\n                    cls=_clspath(cls),\n                    old=_clspath(old, old_class_path),\n                    new=_clspath(new_class, new_class_path),\n                )\n                if warn_once:\n                    msg += \" (warning only on first subclass, there may be others)\"\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super().__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst: Any) -> bool:\n            return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))\n\n        def __subclasscheck__(cls, sub: type) -> bool:\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, \"__mro__\", ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args: Any, **kwargs: Any) -> Any:\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(\n                    cls=_clspath(cls, old_class_path),\n                    new=_clspath(new_class, new_class_path),\n                )\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(f\"Error detecting parent module: {e!r}\")\n\n    return deprecated_cls\n\n\ndef _clspath(cls: type, forced: Optional[str] = None) -> str:\n    if forced is not None:\n        return forced\n    return f\"{cls.__module__}.{cls.__name__}\"\n\n\nDEPRECATION_RULES: List[Tuple[str, str]] = []\n\n\n@overload\ndef update_classpath(path: str) -> str: ...\n\n\n@overload\ndef update_classpath(path: Any) -> Any: ...\n\n\ndef update_classpath(path: Any) -> Any:\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if isinstance(path, str) and path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\n                f\"`{path}` class is deprecated, use `{new_path}` instead\",\n                ScrapyDeprecationWarning,\n            )\n            return new_path\n    return path\n\n\ndef method_is_overridden(subclass: type, base_class: type, method_name: str) -> bool:\n    \"\"\"\n    Return True if a method named ``method_name`` of a ``base_class``\n    is overridden in a ``subclass``.\n\n    >>> class Base:\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub1(Base):\n    ...     pass\n    >>> class Sub2(Base):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub3(Sub1):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub4(Sub2):\n    ...     pass\n    >>> method_is_overridden(Sub1, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub2, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub3, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub4, Base, 'foo')\n    True\n    \"\"\"\n    base_method = getattr(base_class, method_name)\n    sub_method = getattr(subclass, method_name)\n    return base_method.__code__ is not sub_method.__code__\n", "scrapy/commands/shell.py": "\"\"\"\nScrapy Shell\n\nSee documentation in docs/topics/shell.rst\n\"\"\"\n\nfrom argparse import ArgumentParser, Namespace\nfrom threading import Thread\nfrom typing import Any, Dict, List, Type\n\nfrom scrapy import Spider\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.http import Request\nfrom scrapy.shell import Shell\nfrom scrapy.utils.spider import DefaultSpider, spidercls_for_request\nfrom scrapy.utils.url import guess_scheme\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\n        \"KEEP_ALIVE\": True,\n        \"LOGSTATS_INTERVAL\": 0,\n        \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n    }\n\n    def syntax(self) -> str:\n        return \"[url|file]\"\n\n    def short_desc(self) -> str:\n        return \"Interactive scraping console\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Interactive console for scraping the given url or file. \"\n            \"Use ./file.html syntax or full path for local file.\"\n        )\n\n    def add_options(self, parser: ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-c\",\n            dest=\"code\",\n            help=\"evaluate the code in the shell, print the result and exit\",\n        )\n        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_argument(\n            \"--no-redirect\",\n            dest=\"no_redirect\",\n            action=\"store_true\",\n            default=False,\n            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n        )\n\n    def update_vars(self, vars: Dict[str, Any]) -> None:\n        \"\"\"You can use this function to update the Scrapy objects that will be\n        available in the shell\n        \"\"\"\n        pass\n\n    def run(self, args: List[str], opts: Namespace) -> None:\n        url = args[0] if args else None\n        if url:\n            # first argument may be a local file\n            url = guess_scheme(url)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        spidercls: Type[Spider] = DefaultSpider\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        elif url:\n            spidercls = spidercls_for_request(\n                spider_loader, Request(url), spidercls, log_multiple=True\n            )\n\n        # The crawler is created this way since the Shell manually handles the\n        # crawling engine, so the set up in the crawl method won't work\n        crawler = self.crawler_process._create_crawler(spidercls)\n        crawler._apply_settings()\n        # The Shell class needs a persistent engine in the crawler\n        crawler.engine = crawler._create_engine()\n        crawler.engine.start()\n\n        self._start_crawler_thread()\n\n        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n        shell.start(url=url, redirect=not opts.no_redirect)\n\n    def _start_crawler_thread(self) -> None:\n        assert self.crawler_process\n        t = Thread(\n            target=self.crawler_process.start,\n            kwargs={\"stop_after_crawl\": False, \"install_signal_handlers\": False},\n        )\n        t.daemon = True\n        t.start()\n", "scrapy/commands/fetch.py": "import sys\nfrom argparse import ArgumentParser, Namespace\nfrom typing import Dict, List, Type\n\nfrom w3lib.url import is_url\n\nfrom scrapy import Spider\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.spider import DefaultSpider, spidercls_for_request\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n\n    def syntax(self) -> str:\n        return \"[options] <url>\"\n\n    def short_desc(self) -> str:\n        return \"Fetch a URL using the Scrapy downloader\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Fetch a URL using the Scrapy downloader and print its content\"\n            \" to stdout. You may want to use --nolog to disable logging\"\n        )\n\n    def add_options(self, parser: ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_argument(\n            \"--headers\",\n            dest=\"headers\",\n            action=\"store_true\",\n            help=\"print response HTTP headers instead of body\",\n        )\n        parser.add_argument(\n            \"--no-redirect\",\n            dest=\"no_redirect\",\n            action=\"store_true\",\n            default=False,\n            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n        )\n\n    def _print_headers(self, headers: Dict[bytes, List[bytes]], prefix: bytes) -> None:\n        for key, values in headers.items():\n            for value in values:\n                self._print_bytes(prefix + b\" \" + key + b\": \" + value)\n\n    def _print_response(self, response: Response, opts: Namespace) -> None:\n        if opts.headers:\n            assert response.request\n            self._print_headers(response.request.headers, b\">\")\n            print(\">\")\n            self._print_headers(response.headers, b\"<\")\n        else:\n            self._print_bytes(response.body)\n\n    def _print_bytes(self, bytes_: bytes) -> None:\n        sys.stdout.buffer.write(bytes_ + b\"\\n\")\n\n    def run(self, args: List[str], opts: Namespace) -> None:\n        if len(args) != 1 or not is_url(args[0]):\n            raise UsageError()\n        request = Request(\n            args[0],\n            callback=self._print_response,\n            cb_kwargs={\"opts\": opts},\n            dont_filter=True,\n        )\n        # by default, let the framework handle redirects,\n        # i.e. command handles all codes expect 3xx\n        if not opts.no_redirect:\n            request.meta[\"handle_httpstatus_list\"] = SequenceExclude(range(300, 400))\n        else:\n            request.meta[\"handle_httpstatus_all\"] = True\n\n        spidercls: Type[Spider] = DefaultSpider\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        else:\n            spidercls = spidercls_for_request(spider_loader, request, spidercls)\n        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n        self.crawler_process.start()\n", "scrapy/commands/view.py": "import argparse\nimport logging\n\nfrom scrapy.commands import fetch\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.utils.response import open_in_browser\n\nlogger = logging.getLogger(__name__)\n\n\nclass Command(fetch.Command):\n    def short_desc(self) -> str:\n        return \"Open URL in browser, as seen by Scrapy\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n        )\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\"--headers\", help=argparse.SUPPRESS)\n\n    def _print_response(self, response: Response, opts: argparse.Namespace) -> None:\n        if not isinstance(response, TextResponse):\n            logger.error(\"Cannot view a non-text response.\")\n            return\n        open_in_browser(response)\n", "scrapy/commands/runspider.py": "import argparse\nimport sys\nfrom importlib import import_module\nfrom os import PathLike\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import List, Union\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.spider import iter_spider_classes\n\n\ndef _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n    abspath = Path(filepath).resolve()\n    if abspath.suffix not in (\".py\", \".pyw\"):\n        raise ValueError(f\"Not a Python source file: {abspath}\")\n    dirname = str(abspath.parent)\n    sys.path = [dirname] + sys.path\n    try:\n        module = import_module(abspath.stem)\n    finally:\n        sys.path.pop(0)\n    return module\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = False\n    default_settings = {\"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"[options] <spider_file>\"\n\n    def short_desc(self) -> str:\n        return \"Run a self-contained spider (without creating a project)\"\n\n    def long_desc(self) -> str:\n        return \"Run the spider defined in the given file\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) != 1:\n            raise UsageError()\n        filename = Path(args[0])\n        if not filename.exists():\n            raise UsageError(f\"File not found: {filename}\\n\")\n        try:\n            module = _import_file(filename)\n        except (ImportError, ValueError) as e:\n            raise UsageError(f\"Unable to load {str(filename)!r}: {e}\\n\")\n        spclasses = list(iter_spider_classes(module))\n        if not spclasses:\n            raise UsageError(f\"No spider found in file: {filename}\\n\")\n        spidercls = spclasses.pop()\n\n        assert self.crawler_process\n        self.crawler_process.crawl(spidercls, **opts.spargs)\n        self.crawler_process.start()\n\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1\n", "scrapy/commands/parse.py": "from __future__ import annotations\n\nimport argparse\nimport functools\nimport inspect\nimport json\nimport logging\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Coroutine,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom itemadapter import ItemAdapter, is_item\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom twisted.python.failure import Failure\nfrom w3lib.url import is_url\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils import display\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.defer import aiter_errback, deferred_from_coro\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.spider import spidercls_for_request\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = True\n\n    spider = None\n    items: Dict[int, List[Any]] = {}\n    requests: Dict[int, List[Request]] = {}\n\n    first_response = None\n\n    def syntax(self) -> str:\n        return \"[options] <url>\"\n\n    def short_desc(self) -> str:\n        return \"Parse URL (using its spider) and print the results\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--spider\",\n            dest=\"spider\",\n            default=None,\n            help=\"use this spider without looking for one\",\n        )\n        parser.add_argument(\n            \"--pipelines\", action=\"store_true\", help=\"process items through pipelines\"\n        )\n        parser.add_argument(\n            \"--nolinks\",\n            dest=\"nolinks\",\n            action=\"store_true\",\n            help=\"don't show links to follow (extracted requests)\",\n        )\n        parser.add_argument(\n            \"--noitems\",\n            dest=\"noitems\",\n            action=\"store_true\",\n            help=\"don't show scraped items\",\n        )\n        parser.add_argument(\n            \"--nocolour\",\n            dest=\"nocolour\",\n            action=\"store_true\",\n            help=\"avoid using pygments to colorize the output\",\n        )\n        parser.add_argument(\n            \"-r\",\n            \"--rules\",\n            dest=\"rules\",\n            action=\"store_true\",\n            help=\"use CrawlSpider rules to discover the callback\",\n        )\n        parser.add_argument(\n            \"-c\",\n            \"--callback\",\n            dest=\"callback\",\n            help=\"use this callback for parsing, instead looking for a callback\",\n        )\n        parser.add_argument(\n            \"-m\",\n            \"--meta\",\n            dest=\"meta\",\n            help=\"inject extra meta into the Request, it must be a valid raw json string\",\n        )\n        parser.add_argument(\n            \"--cbkwargs\",\n            dest=\"cbkwargs\",\n            help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\",\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--depth\",\n            dest=\"depth\",\n            type=int,\n            default=1,\n            help=\"maximum depth for parsing requests [default: %(default)s]\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            action=\"store_true\",\n            help=\"print each depth level one by one\",\n        )\n\n    @property\n    def max_level(self) -> int:\n        max_items, max_requests = 0, 0\n        if self.items:\n            max_items = max(self.items)\n        if self.requests:\n            max_requests = max(self.requests)\n        return max(max_items, max_requests)\n\n    def handle_exception(self, _failure: Failure) -> None:\n        logger.error(\n            \"An error is caught while iterating the async iterable\",\n            exc_info=failure_to_exc_info(_failure),\n        )\n\n    @overload\n    def iterate_spider_output(\n        self, result: Union[AsyncGenerator[_T, None], Coroutine[Any, Any, _T]]\n    ) -> Deferred[_T]: ...\n\n    @overload\n    def iterate_spider_output(self, result: _T) -> Iterable[Any]: ...\n\n    def iterate_spider_output(self, result: Any) -> Union[Iterable[Any], Deferred]:\n        if inspect.isasyncgen(result):\n            d = deferred_from_coro(\n                collect_asyncgen(aiter_errback(result, self.handle_exception))\n            )\n            d.addCallback(self.iterate_spider_output)\n            return d\n        if inspect.iscoroutine(result):\n            d = deferred_from_coro(result)\n            d.addCallback(self.iterate_spider_output)\n            return d\n        return arg_to_iter(deferred_from_coro(result))\n\n    def add_items(self, lvl: int, new_items: List[Any]) -> None:\n        old_items = self.items.get(lvl, [])\n        self.items[lvl] = old_items + new_items\n\n    def add_requests(self, lvl: int, new_reqs: List[Request]) -> None:\n        old_reqs = self.requests.get(lvl, [])\n        self.requests[lvl] = old_reqs + new_reqs\n\n    def print_items(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n        if lvl is None:\n            items = [item for lst in self.items.values() for item in lst]\n        else:\n            items = self.items.get(lvl, [])\n\n        print(\"# Scraped Items \", \"-\" * 60)\n        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n\n    def print_requests(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n        if lvl is None:\n            if self.requests:\n                requests = self.requests[max(self.requests)]\n            else:\n                requests = []\n        else:\n            requests = self.requests.get(lvl, [])\n\n        print(\"# Requests \", \"-\" * 65)\n        display.pprint(requests, colorize=colour)\n\n    def print_results(self, opts: argparse.Namespace) -> None:\n        colour = not opts.nocolour\n\n        if opts.verbose:\n            for level in range(1, self.max_level + 1):\n                print(f\"\\n>>> DEPTH LEVEL: {level} <<<\")\n                if not opts.noitems:\n                    self.print_items(level, colour)\n                if not opts.nolinks:\n                    self.print_requests(level, colour)\n        else:\n            print(f\"\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<\")\n            if not opts.noitems:\n                self.print_items(colour=colour)\n            if not opts.nolinks:\n                self.print_requests(colour=colour)\n\n    def _get_items_and_requests(\n        self,\n        spider_output: Iterable[Any],\n        opts: argparse.Namespace,\n        depth: int,\n        spider: Spider,\n        callback: Callable,\n    ) -> Tuple[List[Any], List[Request], argparse.Namespace, int, Spider, Callable]:\n        items, requests = [], []\n        for x in spider_output:\n            if is_item(x):\n                items.append(x)\n            elif isinstance(x, Request):\n                requests.append(x)\n        return items, requests, opts, depth, spider, callback\n\n    def run_callback(\n        self,\n        response: Response,\n        callback: Callable,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Deferred:\n        cb_kwargs = cb_kwargs or {}\n        d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n        return d\n\n    def get_callback_from_rules(\n        self, spider: Spider, response: Response\n    ) -> Union[Callable, str, None]:\n        if getattr(spider, \"rules\", None):\n            for rule in spider.rules:  # type: ignore[attr-defined]\n                if rule.link_extractor.matches(response.url):\n                    return rule.callback or \"parse\"\n        else:\n            logger.error(\n                \"No CrawlSpider rules found in spider %(spider)r, \"\n                \"please specify a callback to use for parsing\",\n                {\"spider\": spider.name},\n            )\n        return None\n\n    def set_spidercls(self, url: str, opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            try:\n                self.spidercls = spider_loader.load(opts.spider)\n            except KeyError:\n                logger.error(\n                    \"Unable to find spider: %(spider)s\", {\"spider\": opts.spider}\n                )\n        else:\n            self.spidercls = spidercls_for_request(spider_loader, Request(url))\n            if not self.spidercls:\n                logger.error(\"Unable to find spider for: %(url)s\", {\"url\": url})\n\n        def _start_requests(spider: Spider) -> Iterable[Request]:\n            yield self.prepare_request(spider, Request(url), opts)\n\n        if self.spidercls:\n            self.spidercls.start_requests = _start_requests\n\n    def start_parsing(self, url: str, opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        self.crawler_process.crawl(self.spidercls, **opts.spargs)\n        self.pcrawler = list(self.crawler_process.crawlers)[0]\n        self.crawler_process.start()\n\n        if not self.first_response:\n            logger.error(\"No response downloaded for: %(url)s\", {\"url\": url})\n\n    def scraped_data(\n        self,\n        args: Tuple[\n            List[Any], List[Request], argparse.Namespace, int, Spider, Callable\n        ],\n    ) -> List[Any]:\n        items, requests, opts, depth, spider, callback = args\n        if opts.pipelines:\n            itemproc = self.pcrawler.engine.scraper.itemproc\n            for item in items:\n                itemproc.process_item(item, spider)\n        self.add_items(depth, items)\n        self.add_requests(depth, requests)\n\n        scraped_data = items if opts.output else []\n        if depth < opts.depth:\n            for req in requests:\n                req.meta[\"_depth\"] = depth + 1\n                req.meta[\"_callback\"] = req.callback\n                req.callback = callback\n            scraped_data += requests\n\n        return scraped_data\n\n    def _get_callback(\n        self,\n        *,\n        spider: Spider,\n        opts: argparse.Namespace,\n        response: Optional[Response] = None,\n    ) -> Callable:\n        cb: Union[str, Callable, None] = None\n        if response:\n            cb = response.meta[\"_callback\"]\n        if not cb:\n            if opts.callback:\n                cb = opts.callback\n            elif response and opts.rules and self.first_response == response:\n                cb = self.get_callback_from_rules(spider, response)\n                if not cb:\n                    raise ValueError(\n                        f\"Cannot find a rule that matches {response.url!r} in spider: \"\n                        f\"{spider.name}\"\n                    )\n            else:\n                cb = \"parse\"\n\n        if not callable(cb):\n            assert cb is not None\n            cb_method = getattr(spider, cb, None)\n            if callable(cb_method):\n                cb = cb_method\n            else:\n                raise ValueError(\n                    f\"Cannot find callback {cb!r} in spider: {spider.name}\"\n                )\n        assert callable(cb)\n        return cb\n\n    def prepare_request(\n        self, spider: Spider, request: Request, opts: argparse.Namespace\n    ) -> Request:\n        def callback(response: Response, **cb_kwargs: Any) -> Deferred:\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            cb = self._get_callback(spider=spider, opts=opts, response=response)\n\n            # parse items and requests\n            depth: int = response.meta[\"_depth\"]\n\n            d = self.run_callback(response, cb, cb_kwargs)\n            d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)\n            d.addCallback(self.scraped_data)\n            return d\n\n        # update request meta if any extra meta was passed through the --meta/-m opts.\n        if opts.meta:\n            request.meta.update(opts.meta)\n\n        # update cb_kwargs if any extra values were was passed through the --cbkwargs option.\n        if opts.cbkwargs:\n            request.cb_kwargs.update(opts.cbkwargs)\n\n        request.meta[\"_depth\"] = 1\n        request.meta[\"_callback\"] = request.callback\n        if not request.callback and not opts.rules:\n            cb = self._get_callback(spider=spider, opts=opts)\n            functools.update_wrapper(callback, cb)\n        request.callback = callback\n        return request\n\n    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n\n        self.process_request_meta(opts)\n        self.process_request_cb_kwargs(opts)\n\n    def process_request_meta(self, opts: argparse.Namespace) -> None:\n        if opts.meta:\n            try:\n                opts.meta = json.loads(opts.meta)\n            except ValueError:\n                raise UsageError(\n                    \"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n                    'Example: --meta=\\'{\"foo\" : \"bar\"}\\'',\n                    print_help=False,\n                )\n\n    def process_request_cb_kwargs(self, opts: argparse.Namespace) -> None:\n        if opts.cbkwargs:\n            try:\n                opts.cbkwargs = json.loads(opts.cbkwargs)\n            except ValueError:\n                raise UsageError(\n                    \"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n                    'Example: --cbkwargs=\\'{\"foo\" : \"bar\"}\\'',\n                    print_help=False,\n                )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        # parse arguments\n        if not len(args) == 1 or not is_url(args[0]):\n            raise UsageError()\n        else:\n            url = args[0]\n\n        # prepare spidercls\n        self.set_spidercls(url, opts)\n\n        if self.spidercls and opts.depth > 0:\n            self.start_parsing(url, opts)\n            self.print_results(opts)\n", "scrapy/commands/settings.py": "import argparse\nimport json\nfrom typing import List\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.settings import BaseSettings\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"[options]\"\n\n    def short_desc(self) -> str:\n        return \"Get settings values\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--get\", dest=\"get\", metavar=\"SETTING\", help=\"print raw setting value\"\n        )\n        parser.add_argument(\n            \"--getbool\",\n            dest=\"getbool\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a boolean\",\n        )\n        parser.add_argument(\n            \"--getint\",\n            dest=\"getint\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as an integer\",\n        )\n        parser.add_argument(\n            \"--getfloat\",\n            dest=\"getfloat\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a float\",\n        )\n        parser.add_argument(\n            \"--getlist\",\n            dest=\"getlist\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a list\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        settings = self.crawler_process.settings\n        if opts.get:\n            s = settings.get(opts.get)\n            if isinstance(s, BaseSettings):\n                print(json.dumps(s.copy_to_dict()))\n            else:\n                print(s)\n        elif opts.getbool:\n            print(settings.getbool(opts.getbool))\n        elif opts.getint:\n            print(settings.getint(opts.getint))\n        elif opts.getfloat:\n            print(settings.getfloat(opts.getfloat))\n        elif opts.getlist:\n            print(settings.getlist(opts.getlist))\n", "scrapy/commands/genspider.py": "import argparse\nimport os\nimport shutil\nimport string\nfrom importlib import import_module\nfrom pathlib import Path\nfrom typing import List, Optional, Union, cast\nfrom urllib.parse import urlparse\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.template import render_templatefile, string_camelcase\n\n\ndef sanitize_module_name(module_name: str) -> str:\n    \"\"\"Sanitize the given module name, by replacing dashes and points\n    with underscores and prefixing it with a letter if it doesn't start\n    with one\n    \"\"\"\n    module_name = module_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n    if module_name[0] not in string.ascii_letters:\n        module_name = \"a\" + module_name\n    return module_name\n\n\ndef extract_domain(url: str) -> str:\n    \"\"\"Extract domain name from URL string\"\"\"\n    o = urlparse(url)\n    if o.scheme == \"\" and o.netloc == \"\":\n        o = urlparse(\"//\" + url.lstrip(\"/\"))\n    return o.netloc\n\n\ndef verify_url_scheme(url: str) -> str:\n    \"\"\"Check url for scheme and insert https if none found.\"\"\"\n    parsed = urlparse(url)\n    if parsed.scheme == \"\" and parsed.netloc == \"\":\n        parsed = urlparse(\"//\" + url)._replace(scheme=\"https\")\n    return parsed.geturl()\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <name> <domain>\"\n\n    def short_desc(self) -> str:\n        return \"Generate new spider using pre-defined templates\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"List available templates\",\n        )\n        parser.add_argument(\n            \"-e\",\n            \"--edit\",\n            dest=\"edit\",\n            action=\"store_true\",\n            help=\"Edit spider after creating it\",\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--dump\",\n            dest=\"dump\",\n            metavar=\"TEMPLATE\",\n            help=\"Dump template to standard output\",\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--template\",\n            dest=\"template\",\n            default=\"basic\",\n            help=\"Uses a custom template.\",\n        )\n        parser.add_argument(\n            \"--force\",\n            dest=\"force\",\n            action=\"store_true\",\n            help=\"If the spider already exists, overwrite it with the template\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if opts.list:\n            self._list_templates()\n            return\n        if opts.dump:\n            template_file = self._find_template(opts.dump)\n            if template_file:\n                print(template_file.read_text(encoding=\"utf-8\"))\n            return\n        if len(args) != 2:\n            raise UsageError()\n\n        name, url = args[0:2]\n        url = verify_url_scheme(url)\n        module = sanitize_module_name(name)\n\n        if self.settings.get(\"BOT_NAME\") == module:\n            print(\"Cannot create a spider with the same name as your project\")\n            return\n\n        if not opts.force and self._spider_exists(name):\n            return\n\n        template_file = self._find_template(opts.template)\n        if template_file:\n            self._genspider(module, name, url, opts.template, template_file)\n            if opts.edit:\n                self.exitcode = os.system(f'scrapy edit \"{name}\"')  # nosec\n\n    def _genspider(\n        self,\n        module: str,\n        name: str,\n        url: str,\n        template_name: str,\n        template_file: Union[str, os.PathLike],\n    ) -> None:\n        \"\"\"Generate the spider module, based on the given template\"\"\"\n        capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n        domain = extract_domain(url)\n        tvars = {\n            \"project_name\": self.settings.get(\"BOT_NAME\"),\n            \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n            \"module\": module,\n            \"name\": name,\n            \"url\": url,\n            \"domain\": domain,\n            \"classname\": f\"{capitalized_module}Spider\",\n        }\n        if self.settings.get(\"NEWSPIDER_MODULE\"):\n            spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n            assert spiders_module.__file__\n            spiders_dir = Path(spiders_module.__file__).parent.resolve()\n        else:\n            spiders_module = None\n            spiders_dir = Path(\".\")\n        spider_file = f\"{spiders_dir / module}.py\"\n        shutil.copyfile(template_file, spider_file)\n        render_templatefile(spider_file, **tvars)\n        print(\n            f\"Created spider {name!r} using template {template_name!r} \",\n            end=(\"\" if spiders_module else \"\\n\"),\n        )\n        if spiders_module:\n            print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n\n    def _find_template(self, template: str) -> Optional[Path]:\n        template_file = Path(self.templates_dir, f\"{template}.tmpl\")\n        if template_file.exists():\n            return template_file\n        print(f\"Unable to find template: {template}\\n\")\n        print('Use \"scrapy genspider --list\" to see all available templates.')\n        return None\n\n    def _list_templates(self) -> None:\n        print(\"Available templates:\")\n        for file in sorted(Path(self.templates_dir).iterdir()):\n            if file.suffix == \".tmpl\":\n                print(f\"  {file.stem}\")\n\n    def _spider_exists(self, name: str) -> bool:\n        if not self.settings.get(\"NEWSPIDER_MODULE\"):\n            # if run as a standalone command and file with same filename already exists\n            path = Path(name + \".py\")\n            if path.exists():\n                print(f\"{path.resolve()} already exists\")\n                return True\n            return False\n\n        assert (\n            self.crawler_process is not None\n        ), \"crawler_process must be set before calling run\"\n\n        try:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n        else:\n            # if spider with same name exists\n            print(f\"Spider {name!r} already exists in module:\")\n            print(f\"  {spidercls.__module__}\")\n            return True\n\n        # a file with the same name exists in the target directory\n        spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n        spiders_dir = Path(cast(str, spiders_module.__file__)).parent\n        spiders_dir_abs = spiders_dir.resolve()\n        path = spiders_dir_abs / (name + \".py\")\n        if path.exists():\n            print(f\"{path} already exists\")\n            return True\n\n        return False\n\n    @property\n    def templates_dir(self) -> str:\n        return str(\n            Path(\n                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n                \"spiders\",\n            )\n        )\n", "scrapy/commands/version.py": "import argparse\nfrom typing import List\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.utils.versions import scrapy_components_versions\n\n\nclass Command(ScrapyCommand):\n    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"[-v]\"\n\n    def short_desc(self) -> str:\n        return \"Print Scrapy version\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--verbose\",\n            \"-v\",\n            dest=\"verbose\",\n            action=\"store_true\",\n            help=\"also display twisted/python/platform info (useful for bug reports)\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if opts.verbose:\n            versions = scrapy_components_versions()\n            width = max(len(n) for (n, _) in versions)\n            for name, version in versions:\n                print(f\"{name:<{width}} : {version}\")\n        else:\n            print(f\"Scrapy {scrapy.__version__}\")\n", "scrapy/commands/crawl.py": "import argparse\nfrom typing import List, cast\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = True\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Run a spider\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) < 1:\n            raise UsageError()\n        elif len(args) > 1:\n            raise UsageError(\n                \"running 'scrapy crawl' with more than one spider is not supported\"\n            )\n        spname = args[0]\n\n        assert self.crawler_process\n        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n        if getattr(crawl_defer, \"result\", None) is not None and issubclass(\n            cast(Failure, crawl_defer.result).type, Exception\n        ):\n            self.exitcode = 1\n        else:\n            self.crawler_process.start()\n\n            if (\n                self.crawler_process.bootstrap_failed\n                or hasattr(self.crawler_process, \"has_exception\")\n                and self.crawler_process.has_exception\n            ):\n                self.exitcode = 1\n", "scrapy/commands/bench.py": "import argparse\nimport subprocess  # nosec\nimport sys\nimport time\nfrom typing import Any, Iterable, List\nfrom urllib.parse import urlencode\n\nimport scrapy\nfrom scrapy import Request\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.linkextractors import LinkExtractor\n\n\nclass Command(ScrapyCommand):\n    default_settings = {\n        \"LOG_LEVEL\": \"INFO\",\n        \"LOGSTATS_INTERVAL\": 1,\n        \"CLOSESPIDER_TIMEOUT\": 10,\n    }\n\n    def short_desc(self) -> str:\n        return \"Run quick benchmark test\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        with _BenchServer():\n            assert self.crawler_process\n            self.crawler_process.crawl(_BenchSpider, total=100000)\n            self.crawler_process.start()\n\n\nclass _BenchServer:\n    def __enter__(self) -> None:\n        from scrapy.utils.test import get_testenv\n\n        pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n        self.proc = subprocess.Popen(\n            pargs, stdout=subprocess.PIPE, env=get_testenv()\n        )  # nosec\n        assert self.proc.stdout\n        self.proc.stdout.readline()\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:\n        self.proc.kill()\n        self.proc.wait()\n        time.sleep(0.2)\n\n\nclass _BenchSpider(scrapy.Spider):\n    \"\"\"A spider that follows all links\"\"\"\n\n    name = \"follow\"\n    total = 10000\n    show = 20\n    baseurl = \"http://localhost:8998\"\n    link_extractor = LinkExtractor()\n\n    def start_requests(self) -> Iterable[Request]:\n        qargs = {\"total\": self.total, \"show\": self.show}\n        url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n        return [scrapy.Request(url, dont_filter=True)]\n\n    def parse(self, response: Response) -> Any:\n        assert isinstance(Response, TextResponse)\n        for link in self.link_extractor.extract_links(response):\n            yield scrapy.Request(link.url, callback=self.parse)\n", "scrapy/commands/startproject.py": "import argparse\nimport os\nimport re\nimport string\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom shutil import copy2, copystat, ignore_patterns, move\nfrom stat import S_IWUSR as OWNER_WRITE_PERMISSION\nfrom typing import List, Tuple, Union\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.template import render_templatefile, string_camelcase\n\nTEMPLATES_TO_RENDER: Tuple[Tuple[str, ...], ...] = (\n    (\"scrapy.cfg\",),\n    (\"${project_name}\", \"settings.py.tmpl\"),\n    (\"${project_name}\", \"items.py.tmpl\"),\n    (\"${project_name}\", \"pipelines.py.tmpl\"),\n    (\"${project_name}\", \"middlewares.py.tmpl\"),\n)\n\nIGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n\n\ndef _make_writable(path: Union[str, os.PathLike]) -> None:\n    current_permissions = os.stat(path).st_mode\n    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"<project_name> [project_dir]\"\n\n    def short_desc(self) -> str:\n        return \"Create new project\"\n\n    def _is_valid_name(self, project_name: str) -> bool:\n        def _module_exists(module_name: str) -> bool:\n            spec = find_spec(module_name)\n            return spec is not None and spec.loader is not None\n\n        if not re.search(r\"^[_a-zA-Z]\\w*$\", project_name):\n            print(\n                \"Error: Project names must begin with a letter and contain\"\n                \" only\\nletters, numbers and underscores\"\n            )\n        elif _module_exists(project_name):\n            print(f\"Error: Module {project_name!r} already exists\")\n        else:\n            return True\n        return False\n\n    def _copytree(self, src: Path, dst: Path) -> None:\n        \"\"\"\n        Since the original function always creates the directory, to resolve\n        the issue a new function had to be created. It's a simple copy and\n        was reduced for this case.\n\n        More info at:\n        https://github.com/scrapy/scrapy/pull/2005\n        \"\"\"\n        ignore = IGNORE\n        names = [x.name for x in src.iterdir()]\n        ignored_names = ignore(src, names)\n\n        if not dst.exists():\n            dst.mkdir(parents=True)\n\n        for name in names:\n            if name in ignored_names:\n                continue\n\n            srcname = src / name\n            dstname = dst / name\n            if srcname.is_dir():\n                self._copytree(srcname, dstname)\n            else:\n                copy2(srcname, dstname)\n                _make_writable(dstname)\n\n        copystat(src, dst)\n        _make_writable(dst)\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) not in (1, 2):\n            raise UsageError()\n\n        project_name = args[0]\n\n        if len(args) == 2:\n            project_dir = Path(args[1])\n        else:\n            project_dir = Path(args[0])\n\n        if (project_dir / \"scrapy.cfg\").exists():\n            self.exitcode = 1\n            print(f\"Error: scrapy.cfg already exists in {project_dir.resolve()}\")\n            return\n\n        if not self._is_valid_name(project_name):\n            self.exitcode = 1\n            return\n\n        self._copytree(Path(self.templates_dir), project_dir.resolve())\n        # On 3.8 shutil.move doesn't fully support Path args, but it supports our use case\n        # See https://bugs.python.org/issue32689\n        move(project_dir / \"module\", project_dir / project_name)  # type: ignore[arg-type]\n        for paths in TEMPLATES_TO_RENDER:\n            tplfile = Path(\n                project_dir,\n                *(\n                    string.Template(s).substitute(project_name=project_name)\n                    for s in paths\n                ),\n            )\n            render_templatefile(\n                tplfile,\n                project_name=project_name,\n                ProjectName=string_camelcase(project_name),\n            )\n        print(\n            f\"New Scrapy project '{project_name}', using template directory \"\n            f\"'{self.templates_dir}', created in:\"\n        )\n        print(f\"    {project_dir.resolve()}\\n\")\n        print(\"You can start your first spider with:\")\n        print(f\"    cd {project_dir}\")\n        print(\"    scrapy genspider example example.com\")\n\n    @property\n    def templates_dir(self) -> str:\n        return str(\n            Path(\n                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n                \"project\",\n            )\n        )\n", "scrapy/commands/__init__.py": "\"\"\"\nBase class for Scrapy commands\n\"\"\"\n\nimport argparse\nimport builtins\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, List, Optional\n\nfrom twisted.python import failure\n\nfrom scrapy.crawler import Crawler, CrawlerProcess\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n\n\nclass ScrapyCommand:\n    requires_project: bool = False\n    crawler_process: Optional[CrawlerProcess] = None\n\n    # default settings to be used for this command instead of global defaults\n    default_settings: Dict[str, Any] = {}\n\n    exitcode: int = 0\n\n    def __init__(self) -> None:\n        self.settings: Any = None  # set in scrapy.cmdline\n\n    def set_crawler(self, crawler: Crawler) -> None:\n        if hasattr(self, \"_crawler\"):\n            raise RuntimeError(\"crawler already set\")\n        self._crawler: Crawler = crawler\n\n    def syntax(self) -> str:\n        \"\"\"\n        Command syntax (preferably one-line). Do not include command name.\n        \"\"\"\n        return \"\"\n\n    def short_desc(self) -> str:\n        \"\"\"\n        A short description of the command\n        \"\"\"\n        return \"\"\n\n    def long_desc(self) -> str:\n        \"\"\"A long description of the command. Return short description when not\n        available. It cannot contain newlines since contents will be formatted\n        by optparser which removes newlines and wraps text.\n        \"\"\"\n        return self.short_desc()\n\n    def help(self) -> str:\n        \"\"\"An extensive help for the command. It will be shown when using the\n        \"help\" command. It can contain newlines since no post-formatting will\n        be applied to its contents.\n        \"\"\"\n        return self.long_desc()\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        \"\"\"\n        Populate option parse with options available for this command\n        \"\"\"\n        group = parser.add_argument_group(title=\"Global Options\")\n        group.add_argument(\n            \"--logfile\", metavar=\"FILE\", help=\"log file. if omitted stderr will be used\"\n        )\n        group.add_argument(\n            \"-L\",\n            \"--loglevel\",\n            metavar=\"LEVEL\",\n            default=None,\n            help=f\"log level (default: {self.settings['LOG_LEVEL']})\",\n        )\n        group.add_argument(\n            \"--nolog\", action=\"store_true\", help=\"disable logging completely\"\n        )\n        group.add_argument(\n            \"--profile\",\n            metavar=\"FILE\",\n            default=None,\n            help=\"write python cProfile stats to FILE\",\n        )\n        group.add_argument(\"--pidfile\", metavar=\"FILE\", help=\"write process ID to FILE\")\n        group.add_argument(\n            \"-s\",\n            \"--set\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set/override setting (may be repeated)\",\n        )\n        group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n\n    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n        try:\n            self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n        except ValueError:\n            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n\n        if opts.logfile:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_FILE\", opts.logfile, priority=\"cmdline\")\n\n        if opts.loglevel:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_LEVEL\", opts.loglevel, priority=\"cmdline\")\n\n        if opts.nolog:\n            self.settings.set(\"LOG_ENABLED\", False, priority=\"cmdline\")\n\n        if opts.pidfile:\n            Path(opts.pidfile).write_text(\n                str(os.getpid()) + os.linesep, encoding=\"utf-8\"\n            )\n\n        if opts.pdb:\n            failure.startDebugMode()\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        \"\"\"\n        Entry point for running commands\n        \"\"\"\n        raise NotImplementedError\n\n\nclass BaseRunSpiderCommand(ScrapyCommand):\n    \"\"\"\n    Common class used to share functionality between the crawl, parse and runspider commands\n    \"\"\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-a\",\n            dest=\"spargs\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set spider argument (may be repeated)\",\n        )\n        parser.add_argument(\n            \"-o\",\n            \"--output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"append scraped items to the end of FILE (use - for stdout),\"\n            \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\",\n        )\n        parser.add_argument(\n            \"-O\",\n            \"--overwrite-output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"dump scraped items into FILE, overwriting any existing file,\"\n            \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\",\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--output-format\",\n            metavar=\"FORMAT\",\n            help=\"format to use for dumping items\",\n        )\n\n    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n        try:\n            opts.spargs = arglist_to_dict(opts.spargs)\n        except ValueError:\n            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n        if opts.output or opts.overwrite_output:\n            feeds = feed_process_params_from_cli(\n                self.settings,\n                opts.output,\n                opts.output_format,\n                opts.overwrite_output,\n            )\n            self.settings.set(\"FEEDS\", feeds, priority=\"cmdline\")\n\n\nclass ScrapyHelpFormatter(argparse.HelpFormatter):\n    \"\"\"\n    Help Formatter for scrapy command line help messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        prog: str,\n        indent_increment: int = 2,\n        max_help_position: int = 24,\n        width: Optional[int] = None,\n    ):\n        super().__init__(\n            prog,\n            indent_increment=indent_increment,\n            max_help_position=max_help_position,\n            width=width,\n        )\n\n    def _join_parts(self, part_strings: Iterable[str]) -> str:\n        # scrapy.commands.list shadows builtins.list\n        parts = self.format_part_strings(builtins.list(part_strings))\n        return super()._join_parts(parts)\n\n    def format_part_strings(self, part_strings: List[str]) -> List[str]:\n        \"\"\"\n        Underline and title case command line help message headers.\n        \"\"\"\n        if part_strings and part_strings[0].startswith(\"usage: \"):\n            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len(\"usage: \") :]\n        headings = [\n            i for i in range(len(part_strings)) if part_strings[i].endswith(\":\\n\")\n        ]\n        for index in headings[::-1]:\n            char = \"-\" if \"Global Options\" in part_strings[index] else \"=\"\n            part_strings[index] = part_strings[index][:-2].title()\n            underline = \"\".join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n            part_strings.insert(index + 1, underline)\n        return part_strings\n", "scrapy/commands/edit.py": "import argparse\nimport os\nimport sys\nfrom typing import List\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"<spider>\"\n\n    def short_desc(self) -> str:\n        return \"Edit spider\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Edit a spider using the editor defined in the EDITOR environment\"\n            \" variable or else the EDITOR setting\"\n        )\n\n    def _err(self, msg: str) -> None:\n        sys.stderr.write(msg + os.linesep)\n        self.exitcode = 1\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) != 1:\n            raise UsageError()\n\n        editor = self.settings[\"EDITOR\"]\n        assert self.crawler_process\n        try:\n            spidercls = self.crawler_process.spider_loader.load(args[0])\n        except KeyError:\n            return self._err(f\"Spider not found: {args[0]}\")\n\n        sfile = sys.modules[spidercls.__module__].__file__\n        assert sfile\n        sfile = sfile.replace(\".pyc\", \".py\")\n        self.exitcode = os.system(f'{editor} \"{sfile}\"')  # nosec\n", "scrapy/commands/check.py": "import argparse\nimport time\nfrom collections import defaultdict\nfrom typing import List\nfrom unittest import TextTestResult as _TextTestResult\nfrom unittest import TextTestRunner\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.contracts import ContractsManager\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import load_object, set_environ\n\n\nclass TextTestResult(_TextTestResult):\n    def printSummary(self, start: float, stop: float) -> None:\n        write = self.stream.write\n        # _WritelnDecorator isn't implemented in typeshed yet\n        writeln = self.stream.writeln  # type: ignore[attr-defined]\n\n        run = self.testsRun\n        plural = \"s\" if run != 1 else \"\"\n\n        writeln(self.separator2)\n        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n        writeln()\n\n        infos = []\n        if not self.wasSuccessful():\n            write(\"FAILED\")\n            failed, errored = map(len, (self.failures, self.errors))\n            if failed:\n                infos.append(f\"failures={failed}\")\n            if errored:\n                infos.append(f\"errors={errored}\")\n        else:\n            write(\"OK\")\n\n        if infos:\n            writeln(f\" ({', '.join(infos)})\")\n        else:\n            write(\"\\n\")\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Check spider contracts\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"only list contracts, without checking them\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            default=False,\n            action=\"store_true\",\n            help=\"print contract tests for all spiders\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        # load contracts\n        contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n        conman = ContractsManager(load_object(c) for c in contracts)\n        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n        # contract requests\n        contract_reqs = defaultdict(list)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        with set_environ(SCRAPY_CHECK=\"true\"):\n            for spidername in args or spider_loader.list():\n                spidercls = spider_loader.load(spidername)\n                spidercls.start_requests = lambda s: conman.from_spider(s, result)\n\n                tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                if opts.list:\n                    for method in tested_methods:\n                        contract_reqs[spidercls.name].append(method)\n                elif tested_methods:\n                    self.crawler_process.crawl(spidercls)\n\n            # start checks\n            if opts.list:\n                for spider, methods in sorted(contract_reqs.items()):\n                    if not methods and not opts.verbose:\n                        continue\n                    print(spider)\n                    for method in sorted(methods):\n                        print(f\"  * {method}\")\n            else:\n                start = time.time()\n                self.crawler_process.start()\n                stop = time.time()\n\n                result.printErrors()\n                result.printSummary(start, stop)\n                self.exitcode = int(not result.wasSuccessful())\n", "scrapy/commands/list.py": "import argparse\nfrom typing import List\n\nfrom scrapy.commands import ScrapyCommand\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def short_desc(self) -> str:\n        return \"List available spiders\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        for s in sorted(self.crawler_process.spider_loader.list()):\n            print(s)\n", "scrapy/contracts/__init__.py": "import re\nimport sys\nfrom functools import wraps\nfrom inspect import getmembers\nfrom types import CoroutineType\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\nfrom unittest import TestCase, TestResult\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.python import get_spec\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass Contract:\n    \"\"\"Abstract class for contracts\"\"\"\n\n    request_cls: Optional[Type[Request]] = None\n    name: str\n\n    def __init__(self, method: Callable, *args: Any):\n        self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n        self.testcase_post = _create_testcase(method, f\"@{self.name} post-hook\")\n        self.args: Tuple[Any, ...] = args\n\n    def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n        if hasattr(self, \"pre_process\"):\n            cb = request.callback\n            assert cb is not None\n\n            @wraps(cb)\n            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    cb_result = cb(response, **cb_kwargs)\n                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                        raise TypeError(\"Contracts don't support async callbacks\")\n                    return list(  # pylint: disable=return-in-finally\n                        iterate_spider_output(cb_result)\n                    )\n\n            request.callback = wrapper\n\n        return request\n\n    def add_post_hook(self, request: Request, results: TestResult) -> Request:\n        if hasattr(self, \"post_process\"):\n            cb = request.callback\n            assert cb is not None\n\n            @wraps(cb)\n            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n                cb_result = cb(response, **cb_kwargs)\n                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                    raise TypeError(\"Contracts don't support async callbacks\")\n                output = list(iterate_spider_output(cb_result))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output  # pylint: disable=return-in-finally\n\n            request.callback = wrapper\n\n        return request\n\n    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n        return args\n\n\nclass ContractsManager:\n    contracts: Dict[str, Type[Contract]] = {}\n\n    def __init__(self, contracts: Iterable[Type[Contract]]):\n        for contract in contracts:\n            self.contracts[contract.name] = contract\n\n    def tested_methods_from_spidercls(self, spidercls: Type[Spider]) -> List[str]:\n        is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n        methods = []\n        for key, value in getmembers(spidercls):\n            if callable(value) and value.__doc__ and is_method(value.__doc__):\n                methods.append(key)\n\n        return methods\n\n    def extract_contracts(self, method: Callable) -> List[Contract]:\n        contracts: List[Contract] = []\n        assert method.__doc__ is not None\n        for line in method.__doc__.split(\"\\n\"):\n            line = line.strip()\n\n            if line.startswith(\"@\"):\n                m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n                if m is None:\n                    continue\n                name, args = m.groups()\n                args = re.split(r\"\\s+\", args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts\n\n    def from_spider(\n        self, spider: Spider, results: TestResult\n    ) -> List[Optional[Request]]:\n        requests: List[Optional[Request]] = []\n        for method in self.tested_methods_from_spidercls(type(spider)):\n            bound_method = spider.__getattribute__(method)\n            try:\n                requests.append(self.from_method(bound_method, results))\n            except Exception:\n                case = _create_testcase(bound_method, \"contract\")\n                results.addError(case, sys.exc_info())\n\n        return requests\n\n    def from_method(self, method: Callable, results: TestResult) -> Optional[Request]:\n        contracts = self.extract_contracts(method)\n        if contracts:\n            request_cls = Request\n            for contract in contracts:\n                if contract.request_cls is not None:\n                    request_cls = contract.request_cls\n\n            # calculate request args\n            args, kwargs = get_spec(request_cls.__init__)\n\n            # Don't filter requests to allow\n            # testing different callbacks on the same URL.\n            kwargs[\"dont_filter\"] = True\n            kwargs[\"callback\"] = method\n\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            args.remove(\"self\")\n\n            # check if all positional arguments are defined in kwargs\n            if set(args).issubset(set(kwargs)):\n                request = request_cls(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request\n        return None\n\n    def _clean_req(\n        self, request: Request, method: Callable, results: TestResult\n    ) -> None:\n        \"\"\"stop the request from returning objects and records any errors\"\"\"\n\n        cb = request.callback\n        assert cb is not None\n\n        @wraps(cb)\n        def cb_wrapper(response: Response, **cb_kwargs: Any) -> None:\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, \"callback\")\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure: Failure) -> None:\n            case = _create_testcase(method, \"errback\")\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper\n\n\ndef _create_testcase(method: Callable, desc: str) -> TestCase:\n    spider = method.__self__.name  # type: ignore[attr-defined]\n\n    class ContractTestCase(TestCase):\n        def __str__(_self) -> str:\n            return f\"[{spider}] {method.__name__} ({desc})\"\n\n    name = f\"{spider}_{method.__name__}\"\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)\n", "scrapy/contracts/default.py": "import json\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.contracts import Contract\nfrom scrapy.exceptions import ContractFail\nfrom scrapy.http import Request\n\n\n# contracts\nclass UrlContract(Contract):\n    \"\"\"Contract to set the url of the request (mandatory)\n    @url http://scrapy.org\n    \"\"\"\n\n    name = \"url\"\n\n    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n        args[\"url\"] = self.args[0]\n        return args\n\n\nclass CallbackKeywordArgumentsContract(Contract):\n    \"\"\"Contract to set the keyword arguments for the request.\n    The value should be a JSON-encoded dictionary, e.g.:\n\n    @cb_kwargs {\"arg1\": \"some value\"}\n    \"\"\"\n\n    name = \"cb_kwargs\"\n\n    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n        args[\"cb_kwargs\"] = json.loads(\" \".join(self.args))\n        return args\n\n\nclass ReturnsContract(Contract):\n    \"\"\"Contract to check the output of a callback\n\n    general form:\n    @returns request(s)/item(s) [min=1 [max]]\n\n    e.g.:\n    @returns request\n    @returns request 2\n    @returns request 2 10\n    @returns request 0 10\n    \"\"\"\n\n    name = \"returns\"\n    object_type_verifiers: Dict[Optional[str], Callable[[Any], bool]] = {\n        \"request\": lambda x: isinstance(x, Request),\n        \"requests\": lambda x: isinstance(x, Request),\n        \"item\": is_item,\n        \"items\": is_item,\n    }\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n\n        if len(self.args) not in [1, 2, 3]:\n            raise ValueError(\n                f\"Incorrect argument quantity: expected 1, 2 or 3, got {len(self.args)}\"\n            )\n        self.obj_name = self.args[0] or None\n        self.obj_type_verifier = self.object_type_verifiers[self.obj_name]\n\n        try:\n            self.min_bound: float = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound: float = int(self.args[2])\n        except IndexError:\n            self.max_bound = float(\"inf\")\n\n    def post_process(self, output: List[Any]) -> None:\n        occurrences = 0\n        for x in output:\n            if self.obj_type_verifier(x):\n                occurrences += 1\n\n        assertion = self.min_bound <= occurrences <= self.max_bound\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = str(self.min_bound)\n            else:\n                expected = f\"{self.min_bound}..{self.max_bound}\"\n\n            raise ContractFail(\n                f\"Returned {occurrences} {self.obj_name}, expected {expected}\"\n            )\n\n\nclass ScrapesContract(Contract):\n    \"\"\"Contract to check presence of fields in scraped items\n    @scrapes page_name page_body\n    \"\"\"\n\n    name = \"scrapes\"\n\n    def post_process(self, output: List[Any]) -> None:\n        for x in output:\n            if is_item(x):\n                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                if missing:\n                    missing_fields = \", \".join(missing)\n                    raise ContractFail(f\"Missing fields: {missing_fields}\")\n", "scrapy/spiders/init.py": "from typing import Any, Iterable, Optional, cast\n\nfrom scrapy import Request\nfrom scrapy.http import Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass InitSpider(Spider):\n    \"\"\"Base Spider with initialization facilities\"\"\"\n\n    def start_requests(self) -> Iterable[Request]:\n        self._postinit_reqs: Iterable[Request] = super().start_requests()\n        return cast(Iterable[Request], iterate_spider_output(self.init_request()))\n\n    def initialized(self, response: Optional[Response] = None) -> Any:\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop(\"_postinit_reqs\")\n\n    def init_request(self) -> Any:\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()\n", "scrapy/spiders/feed.py": "\"\"\"\nThis module implements the XMLFeedSpider which is the recommended spider to use\nfor scraping from an XML feed.\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n\nfrom scrapy.exceptions import NotConfigured, NotSupported\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.selector import Selector\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.iterators import csviter, xmliter_lxml\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass XMLFeedSpider(Spider):\n    \"\"\"\n    This class intends to be the base class for spiders that scrape\n    from XML feeds.\n\n    You can choose whether to parse the file using the 'iternodes' iterator, an\n    'xml' selector, or an 'html' selector.  In most cases, it's convenient to\n    use iternodes, since it's a faster and cleaner.\n    \"\"\"\n\n    iterator: str = \"iternodes\"\n    itertag: str = \"item\"\n    namespaces: Sequence[Tuple[str, str]] = ()\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (items or requests).\n        \"\"\"\n        return results\n\n    def adapt_response(self, response: Response) -> Response:\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response\n\n    def parse_node(self, response: Response, selector: Selector) -> Any:\n        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n        if hasattr(self, \"parse_item\"):  # backward compatibility\n            return self.parse_item(response, selector)\n        raise NotImplementedError\n\n    def parse_nodes(self, response: Response, nodes: Iterable[Selector]) -> Any:\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either an item, a request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            yield from self.process_results(response, ret)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        if not hasattr(self, \"parse_node\"):\n            raise NotConfigured(\n                \"You must define parse_node method in order to scrape this XML feed\"\n            )\n\n        response = self.adapt_response(response)\n        nodes: Iterable[Selector]\n        if self.iterator == \"iternodes\":\n            nodes = self._iternodes(response)\n        elif self.iterator == \"xml\":\n            if not isinstance(response, TextResponse):\n                raise ValueError(\"Response content isn't text\")\n            selector = Selector(response, type=\"xml\")\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f\"//{self.itertag}\")\n        elif self.iterator == \"html\":\n            if not isinstance(response, TextResponse):\n                raise ValueError(\"Response content isn't text\")\n            selector = Selector(response, type=\"html\")\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f\"//{self.itertag}\")\n        else:\n            raise NotSupported(\"Unsupported node iterator\")\n\n        return self.parse_nodes(response, nodes)\n\n    def _iternodes(self, response: Response) -> Iterable[Selector]:\n        for node in xmliter_lxml(response, self.itertag):\n            self._register_namespaces(node)\n            yield node\n\n    def _register_namespaces(self, selector: Selector) -> None:\n        for prefix, uri in self.namespaces:\n            selector.register_namespace(prefix, uri)\n\n\nclass CSVFeedSpider(Spider):\n    \"\"\"Spider for parsing CSV feeds.\n    It receives a CSV file in a response; iterates through each of its rows,\n    and calls parse_row with a dict containing each field's data.\n\n    You can set some options regarding the CSV file, such as the delimiter, quotechar\n    and the file's headers.\n    \"\"\"\n\n    delimiter: Optional[str] = (\n        None  # When this is None, python's csv module's default delimiter is used\n    )\n    quotechar: Optional[str] = (\n        None  # When this is None, python's csv module's default quotechar is used\n    )\n    headers: Optional[List[str]] = None\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return results\n\n    def adapt_response(self, response: Response) -> Response:\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return response\n\n    def parse_row(self, response: Response, row: Dict[str, str]) -> Any:\n        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n        raise NotImplementedError\n\n    def parse_rows(self, response: Response) -> Any:\n        \"\"\"Receives a response and a dict (representing each row) with a key for\n        each provided (or detected) header of the CSV file.  This spider also\n        gives the opportunity to override adapt_response and\n        process_results methods for pre and post-processing purposes.\n        \"\"\"\n\n        for row in csviter(\n            response, self.delimiter, self.headers, quotechar=self.quotechar\n        ):\n            ret = iterate_spider_output(self.parse_row(response, row))\n            yield from self.process_results(response, ret)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        if not hasattr(self, \"parse_row\"):\n            raise NotConfigured(\n                \"You must define parse_row method in order to scrape this CSV feed\"\n            )\n        response = self.adapt_response(response)\n        return self.parse_rows(response)\n", "scrapy/spiders/sitemap.py": "from __future__ import annotations\n\nimport logging\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom scrapy.http import Request, Response, XmlResponse\nfrom scrapy.spiders import Spider\nfrom scrapy.utils._compression import _DecompressionMaxSizeExceeded\nfrom scrapy.utils.gz import gunzip, gzip_magic_number\nfrom scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nclass SitemapSpider(Spider):\n    sitemap_urls: Sequence[str] = ()\n    sitemap_rules: Sequence[\n        Tuple[Union[re.Pattern[str], str], Union[str, Callable]]\n    ] = [(\"\", \"parse\")]\n    sitemap_follow: Sequence[Union[re.Pattern[str], str]] = [\"\"]\n    sitemap_alternate_links: bool = False\n    _max_size: int\n    _warn_size: int\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._max_size = getattr(\n            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\n        )\n        spider._warn_size = getattr(\n            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\n        )\n        return spider\n\n    def __init__(self, *a: Any, **kw: Any):\n        super().__init__(*a, **kw)\n        self._cbs: List[Tuple[re.Pattern[str], Callable]] = []\n        for r, c in self.sitemap_rules:\n            if isinstance(c, str):\n                c = cast(Callable, getattr(self, c))\n            self._cbs.append((regex(r), c))\n        self._follow: List[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n\n    def start_requests(self) -> Iterable[Request]:\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)\n\n    def sitemap_filter(\n        self, entries: Iterable[Dict[str, Any]]\n    ) -> Iterable[Dict[str, Any]]:\n        \"\"\"This method can be used to filter sitemap entries by their\n        attributes, for example, you can filter locs with lastmod greater\n        than a given date (see docs).\n        \"\"\"\n        yield from entries\n\n    def _parse_sitemap(self, response: Response) -> Iterable[Request]:\n        if response.url.endswith(\"/robots.txt\"):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\n                    \"Ignoring invalid sitemap: %(response)s\",\n                    {\"response\": response},\n                    extra={\"spider\": self},\n                )\n                return\n\n            s = Sitemap(body)\n            it = self.sitemap_filter(s)\n\n            if s.type == \"sitemapindex\":\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == \"urlset\":\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break\n\n    def _get_sitemap_body(self, response: Response) -> Optional[bytes]:\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        if gzip_magic_number(response):\n            uncompressed_size = len(response.body)\n            max_size = response.meta.get(\"download_maxsize\", self._max_size)\n            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\n            try:\n                body = gunzip(response.body, max_size=max_size)\n            except _DecompressionMaxSizeExceeded:\n                return None\n            if uncompressed_size < warn_size <= len(body):\n                logger.warning(\n                    f\"{response} body size after decompression ({len(body)} B) \"\n                    f\"is larger than the download warning size ({warn_size} B).\"\n                )\n            return body\n        # actual gzipped sitemap files are decompressed above ;\n        # if we are here (response body is not gzipped)\n        # and have a response for .xml.gz,\n        # it usually means that it was already gunzipped\n        # by HttpCompression middleware,\n        # the HTTP response being sent with \"Content-Encoding: gzip\"\n        # without actually being a .xml.gz file in the first place,\n        # merely XML gzip-compressed on the fly,\n        # in other word, here, we have plain XML\n        if response.url.endswith(\".xml\") or response.url.endswith(\".xml.gz\"):\n            return response.body\n        return None\n\n\ndef regex(x: Union[re.Pattern[str], str]) -> re.Pattern[str]:\n    if isinstance(x, str):\n        return re.compile(x)\n    return x\n\n\ndef iterloc(it: Iterable[Dict[str, Any]], alt: bool = False) -> Iterable[str]:\n    for d in it:\n        yield d[\"loc\"]\n\n        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n        if alt and \"alternate\" in d:\n            yield from d[\"alternate\"]\n", "scrapy/spiders/crawl.py": "\"\"\"\nThis modules implements the CrawlSpider which is the recommended spider to use\nfor scraping typical web sites that requires crawling pages.\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.link import Link\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\n_T = TypeVar(\"_T\")\nProcessLinksT = Callable[[List[Link]], List[Link]]\nProcessRequestT = Callable[[Request, Response], Optional[Request]]\n\n\ndef _identity(x: _T) -> _T:\n    return x\n\n\ndef _identity_process_request(\n    request: Request, response: Response\n) -> Optional[Request]:\n    return request\n\n\ndef _get_method(\n    method: Union[Callable, str, None], spider: Spider\n) -> Optional[Callable]:\n    if callable(method):\n        return method\n    if isinstance(method, str):\n        return getattr(spider, method, None)\n    return None\n\n\n_default_link_extractor = LinkExtractor()\n\n\nclass Rule:\n    def __init__(\n        self,\n        link_extractor: Optional[LinkExtractor] = None,\n        callback: Union[Callable, str, None] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        follow: Optional[bool] = None,\n        process_links: Union[ProcessLinksT, str, None] = None,\n        process_request: Union[ProcessRequestT, str, None] = None,\n        errback: Union[Callable[[Failure], Any], str, None] = None,\n    ):\n        self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n        self.callback: Union[Callable, str, None] = callback\n        self.errback: Union[Callable[[Failure], Any], str, None] = errback\n        self.cb_kwargs: Dict[str, Any] = cb_kwargs or {}\n        self.process_links: Union[ProcessLinksT, str] = process_links or _identity\n        self.process_request: Union[ProcessRequestT, str] = (\n            process_request or _identity_process_request\n        )\n        self.follow: bool = follow if follow is not None else not callback\n\n    def _compile(self, spider: Spider) -> None:\n        # this replaces method names with methods and we can't express this in type hints\n        self.callback = _get_method(self.callback, spider)\n        self.errback = cast(Callable[[Failure], Any], _get_method(self.errback, spider))\n        self.process_links = cast(\n            ProcessLinksT, _get_method(self.process_links, spider)\n        )\n        self.process_request = cast(\n            ProcessRequestT, _get_method(self.process_request, spider)\n        )\n\n\nclass CrawlSpider(Spider):\n    rules: Sequence[Rule] = ()\n    _rules: List[Rule]\n    _follow_links: bool\n\n    def __init__(self, *a: Any, **kw: Any):\n        super().__init__(*a, **kw)\n        self._compile_rules()\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        return self._parse_response(\n            response=response,\n            callback=self.parse_start_url,\n            cb_kwargs=kwargs,\n            follow=True,\n        )\n\n    def parse_start_url(self, response: Response, **kwargs: Any) -> Any:\n        return []\n\n    def process_results(self, response: Response, results: Any) -> Any:\n        return results\n\n    def _build_request(self, rule_index: int, link: Link) -> Request:\n        return Request(\n            url=link.url,\n            callback=self._callback,\n            errback=self._errback,\n            meta={\"rule\": rule_index, \"link_text\": link.text},\n        )\n\n    def _requests_to_follow(self, response: Response) -> Iterable[Optional[Request]]:\n        if not isinstance(response, HtmlResponse):\n            return\n        seen: Set[Link] = set()\n        for rule_index, rule in enumerate(self._rules):\n            links: List[Link] = [\n                lnk\n                for lnk in rule.link_extractor.extract_links(response)\n                if lnk not in seen\n            ]\n            for link in cast(ProcessLinksT, rule.process_links)(links):\n                seen.add(link)\n                request = self._build_request(rule_index, link)\n                yield cast(ProcessRequestT, rule.process_request)(request, response)\n\n    def _callback(self, response: Response, **cb_kwargs: Any) -> Any:\n        rule = self._rules[cast(int, response.meta[\"rule\"])]\n        return self._parse_response(\n            response,\n            cast(Callable, rule.callback),\n            {**rule.cb_kwargs, **cb_kwargs},\n            rule.follow,\n        )\n\n    def _errback(self, failure: Failure) -> Iterable[Any]:\n        rule = self._rules[cast(int, failure.request.meta[\"rule\"])]  # type: ignore[attr-defined]\n        return self._handle_failure(\n            failure, cast(Callable[[Failure], Any], rule.errback)\n        )\n\n    async def _parse_response(\n        self,\n        response: Response,\n        callback: Optional[Callable],\n        cb_kwargs: Dict[str, Any],\n        follow: bool = True,\n    ) -> AsyncIterable[Any]:\n        if callback:\n            cb_res = callback(response, **cb_kwargs) or ()\n            if isinstance(cb_res, AsyncIterable):\n                cb_res = await collect_asyncgen(cb_res)\n            elif isinstance(cb_res, Awaitable):\n                cb_res = await cb_res\n            cb_res = self.process_results(response, cb_res)\n            for request_or_item in iterate_spider_output(cb_res):\n                yield request_or_item\n\n        if follow and self._follow_links:\n            for request_or_item in self._requests_to_follow(response):\n                yield request_or_item\n\n    def _handle_failure(\n        self, failure: Failure, errback: Optional[Callable[[Failure], Any]]\n    ) -> Iterable[Any]:\n        if errback:\n            results = errback(failure) or ()\n            yield from iterate_spider_output(results)\n\n    def _compile_rules(self) -> None:\n        self._rules = []\n        for rule in self.rules:\n            self._rules.append(copy.copy(rule))\n            self._rules[-1]._compile(self)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool(\n            \"CRAWLSPIDER_FOLLOW_LINKS\", True\n        )\n        return spider\n", "scrapy/spiders/__init__.py": "\"\"\"\nBase class for Scrapy spiders\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Union, cast\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import signals\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import url_is_from_spider\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n    # typing.Concatenate requires Python 3.10\n    # typing.Self requires Python 3.11\n    from typing_extensions import Concatenate, Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings, _SettingsKeyT\n    from scrapy.utils.log import SpiderLoggerAdapter\n\n    CallbackT = Callable[Concatenate[Response, ...], Any]\n\n\nclass Spider(object_ref):\n    \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n    class.\n    \"\"\"\n\n    name: str\n    custom_settings: Optional[Dict[_SettingsKeyT, Any]] = None\n\n    def __init__(self, name: Optional[str] = None, **kwargs: Any):\n        if name is not None:\n            self.name: str = name\n        elif not getattr(self, \"name\", None):\n            raise ValueError(f\"{type(self).__name__} must have a name\")\n        self.__dict__.update(kwargs)\n        if not hasattr(self, \"start_urls\"):\n            self.start_urls: List[str] = []\n\n    @property\n    def logger(self) -> SpiderLoggerAdapter:\n        from scrapy.utils.log import SpiderLoggerAdapter\n\n        logger = logging.getLogger(self.name)\n        return SpiderLoggerAdapter(logger, {\"spider\": self})\n\n    def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider\n\n    def _set_crawler(self, crawler: Crawler) -> None:\n        self.crawler: Crawler = crawler\n        self.settings: BaseSettings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)\n\n    def start_requests(self) -> Iterable[Request]:\n        if not self.start_urls and hasattr(self, \"start_url\"):\n            raise AttributeError(\n                \"Crawling could not start: 'start_urls' not found \"\n                \"or empty (but found 'start_url' attribute instead, \"\n                \"did you miss an 's'?)\"\n            )\n        for url in self.start_urls:\n            yield Request(url, dont_filter=True)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        return self.parse(response, **kwargs)\n\n    if TYPE_CHECKING:\n        parse: CallbackT\n    else:\n\n        def parse(self, response: Response, **kwargs: Any) -> Any:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__}.parse callback is not defined\"\n            )\n\n    @classmethod\n    def update_settings(cls, settings: BaseSettings) -> None:\n        settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n\n    @classmethod\n    def handles_request(cls, request: Request) -> bool:\n        return url_is_from_spider(request.url, cls)\n\n    @staticmethod\n    def close(spider: Spider, reason: str) -> Union[Deferred, None]:\n        closed = getattr(spider, \"closed\", None)\n        if callable(closed):\n            return cast(Union[Deferred, None], closed(reason))\n        return None\n\n    def __repr__(self) -> str:\n        return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"\n\n\n# Top-level imports\nfrom scrapy.spiders.crawl import CrawlSpider, Rule\nfrom scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\nfrom scrapy.spiders.sitemap import SitemapSpider\n", "scrapy/loader/__init__.py": "\"\"\"\nItem Loader\n\nSee documentation in docs/topics/loaders.rst\n\"\"\"\n\nfrom typing import Any, Optional\n\nimport itemloaders\n\nfrom scrapy.http import TextResponse\nfrom scrapy.item import Item\nfrom scrapy.selector import Selector\n\n\nclass ItemLoader(itemloaders.ItemLoader):\n    \"\"\"\n    A user-friendly abstraction to populate an :ref:`item <topics-items>` with data\n    by applying :ref:`field processors <topics-loaders-processors>` to scraped data.\n    When instantiated with a ``selector`` or a ``response`` it supports\n    data extraction from web pages using :ref:`selectors <topics-selectors>`.\n\n    :param item: The item instance to populate using subsequent calls to\n        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n        or :meth:`~ItemLoader.add_value`.\n    :type item: scrapy.item.Item\n\n    :param selector: The selector to extract data from, when using the\n        :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or\n        :meth:`replace_css` method.\n    :type selector: :class:`~scrapy.selector.Selector` object\n\n    :param response: The response used to construct the selector using the\n        :attr:`default_selector_class`, unless the selector argument is given,\n        in which case this argument is ignored.\n    :type response: :class:`~scrapy.http.Response` object\n\n    If no item is given, one is instantiated automatically using the class in\n    :attr:`default_item_class`.\n\n    The item, selector, response and remaining keyword arguments are\n    assigned to the Loader context (accessible through the :attr:`context` attribute).\n\n    .. attribute:: item\n\n        The item object being parsed by this Item Loader.\n        This is mostly used as a property so, when attempting to override this\n        value, you may want to check out :attr:`default_item_class` first.\n\n    .. attribute:: context\n\n        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n\n    .. attribute:: default_item_class\n\n        An :ref:`item <topics-items>` class (or factory), used to instantiate\n        items when not given in the ``__init__`` method.\n\n    .. attribute:: default_input_processor\n\n        The default input processor to use for those fields which don't specify\n        one.\n\n    .. attribute:: default_output_processor\n\n        The default output processor to use for those fields which don't specify\n        one.\n\n    .. attribute:: default_selector_class\n\n        The class used to construct the :attr:`selector` of this\n        :class:`ItemLoader`, if only a response is given in the ``__init__`` method.\n        If a selector is given in the ``__init__`` method this attribute is ignored.\n        This attribute is sometimes overridden in subclasses.\n\n    .. attribute:: selector\n\n        The :class:`~scrapy.selector.Selector` object to extract data from.\n        It's either the selector given in the ``__init__`` method or one created from\n        the response given in the ``__init__`` method using the\n        :attr:`default_selector_class`. This attribute is meant to be\n        read-only.\n    \"\"\"\n\n    default_item_class: type = Item\n    default_selector_class = Selector\n\n    def __init__(\n        self,\n        item: Any = None,\n        selector: Optional[Selector] = None,\n        response: Optional[TextResponse] = None,\n        parent: Optional[itemloaders.ItemLoader] = None,\n        **context: Any\n    ):\n        if selector is None and response is not None:\n            try:\n                selector = self.default_selector_class(response)\n            except AttributeError:\n                selector = None\n        context.update(response=response)\n        super().__init__(item=item, selector=selector, parent=parent, **context)\n", "scrapy/settings/__init__.py": "from __future__ import annotations\n\nimport copy\nimport json\nfrom importlib import import_module\nfrom pprint import pformat\nfrom types import ModuleType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom scrapy.settings import default_settings\n\n# The key types are restricted in BaseSettings._get_key() to ones supported by JSON,\n# see https://github.com/scrapy/scrapy/issues/5383.\n_SettingsKeyT = Union[bool, float, int, str, None]\n\nif TYPE_CHECKING:\n    # https://github.com/python/typing/issues/445#issuecomment-1131458824\n    from _typeshed import SupportsItems\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    _SettingsInputT = Union[SupportsItems[_SettingsKeyT, Any], str, None]\n\n\nSETTINGS_PRIORITIES: Dict[str, int] = {\n    \"default\": 0,\n    \"command\": 10,\n    \"addon\": 15,\n    \"project\": 20,\n    \"spider\": 30,\n    \"cmdline\": 40,\n}\n\n\ndef get_settings_priority(priority: Union[int, str]) -> int:\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, str):\n        return SETTINGS_PRIORITIES[priority]\n    return priority\n\n\nclass SettingsAttribute:\n    \"\"\"Class for storing data related to settings attributes.\n\n    This class is intended for internal usage, you should try Settings class\n    for settings configuration, not this one.\n    \"\"\"\n\n    def __init__(self, value: Any, priority: int):\n        self.value: Any = value\n        self.priority: int\n        if isinstance(self.value, BaseSettings):\n            self.priority = max(self.value.maxpriority(), priority)\n        else:\n            self.priority = priority\n\n    def set(self, value: Any, priority: int) -> None:\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority\n\n    def __repr__(self) -> str:\n        return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"\n\n\nclass BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n    \"\"\"\n    Instances of this class behave like dictionaries, but store priorities\n    along with their ``(key, value)`` pairs, and can be frozen (i.e. marked\n    immutable).\n\n    Key-value entries can be passed on initialization with the ``values``\n    argument, and they would take the ``priority`` level (unless ``values`` is\n    already an instance of :class:`~scrapy.settings.BaseSettings`, in which\n    case the existing priority levels will be kept).  If the ``priority``\n    argument is a string, the priority name will be looked up in\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer\n    should be provided.\n\n    Once the object is created, new settings can be loaded or updated with the\n    :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with\n    the square bracket notation of dictionaries, or with the\n    :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its\n    value conversion variants. When requesting a stored key, the value with the\n    highest priority will be retrieved.\n    \"\"\"\n\n    __default = object()\n\n    def __init__(\n        self, values: _SettingsInputT = None, priority: Union[int, str] = \"project\"\n    ):\n        self.frozen: bool = False\n        self.attributes: dict[_SettingsKeyT, SettingsAttribute] = {}\n        if values:\n            self.update(values, priority)\n\n    def __getitem__(self, opt_name: _SettingsKeyT) -> Any:\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value\n\n    def __contains__(self, name: Any) -> bool:\n        return name in self.attributes\n\n    def get(self, name: _SettingsKeyT, default: Any = None) -> Any:\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return self[name] if self[name] is not None else default\n\n    def getbool(self, name: _SettingsKeyT, default: bool = False) -> bool:\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\n                \"Supported values for boolean settings \"\n                \"are 0/1, True/False, '0'/'1', \"\n                \"'True'/'False' and 'true'/'false'\"\n            )\n\n    def getint(self, name: _SettingsKeyT, default: int = 0) -> int:\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return int(self.get(name, default))\n\n    def getfloat(self, name: _SettingsKeyT, default: float = 0.0) -> float:\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return float(self.get(name, default))\n\n    def getlist(\n        self, name: _SettingsKeyT, default: Optional[List[Any]] = None\n    ) -> List[Any]:\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list, a\n        copy of it will be returned. If it's a string it will be split by \",\".\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or [])\n        if isinstance(value, str):\n            value = value.split(\",\")\n        return list(value)\n\n    def getdict(\n        self, name: _SettingsKeyT, default: Optional[Dict[Any, Any]] = None\n    ) -> Dict[Any, Any]:\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, str):\n            value = json.loads(value)\n        return dict(value)\n\n    def getdictorlist(\n        self,\n        name: _SettingsKeyT,\n        default: Union[Dict[Any, Any], List[Any], Tuple[Any], None] = None,\n    ) -> Union[Dict[Any, Any], List[Any]]:\n        \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n\n        If the setting is already a dict or a list, a copy of it will be\n        returned.\n\n        If it is a string it will be evaluated as JSON, or as a comma-separated\n        list of strings as a fallback.\n\n        For example, settings populated from the command line will return:\n\n        -   ``{'key1': 'value1', 'key2': 'value2'}`` if set to\n            ``'{\"key1\": \"value1\", \"key2\": \"value2\"}'``\n\n        -   ``['one', 'two']`` if set to ``'[\"one\", \"two\"]'`` or ``'one,two'``\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default)\n        if value is None:\n            return {}\n        if isinstance(value, str):\n            try:\n                value_loaded = json.loads(value)\n                assert isinstance(value_loaded, (dict, list))\n                return value_loaded\n            except ValueError:\n                return value.split(\",\")\n        if isinstance(value, tuple):\n            return list(value)\n        assert isinstance(value, (dict, list))\n        return copy.deepcopy(value)\n\n    def getwithbase(self, name: _SettingsKeyT) -> BaseSettings:\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: str\n        \"\"\"\n        if not isinstance(name, str):\n            raise ValueError(f\"Base setting key must be a string, got {name}\")\n        compbs = BaseSettings()\n        compbs.update(self[name + \"_BASE\"])\n        compbs.update(self[name])\n        return compbs\n\n    def getpriority(self, name: _SettingsKeyT) -> Optional[int]:\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: str\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority\n\n    def maxpriority(self) -> int:\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(cast(int, self.getpriority(name)) for name in self)\n        return get_settings_priority(\"default\")\n\n    def __setitem__(self, name: _SettingsKeyT, value: Any) -> None:\n        self.set(name, value)\n\n    def set(\n        self, name: _SettingsKeyT, value: Any, priority: Union[int, str] = \"project\"\n    ) -> None:\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: str\n\n        :param value: the value to associate with the setting\n        :type value: object\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)\n\n    def setdefault(\n        self,\n        name: _SettingsKeyT,\n        default: Any = None,\n        priority: Union[int, str] = \"project\",\n    ) -> Any:\n        if name not in self:\n            self.set(name, default, priority)\n            return default\n\n        return self.attributes[name].value\n\n    def setdict(\n        self, values: _SettingsInputT, priority: Union[int, str] = \"project\"\n    ) -> None:\n        self.update(values, priority)\n\n    def setmodule(\n        self, module: Union[ModuleType, str], priority: Union[int, str] = \"project\"\n    ) -> None:\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: types.ModuleType or str\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, str):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)\n\n    # BaseSettings.update() doesn't support all inputs that MutableMapping.update() supports\n    def update(self, values: _SettingsInputT, priority: Union[int, str] = \"project\") -> None:  # type: ignore[override]\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, str):\n            values = cast(Dict[_SettingsKeyT, Any], json.loads(values))\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in values.items():\n                    self.set(name, value, cast(int, values.getpriority(name)))\n            else:\n                for name, value in values.items():\n                    self.set(name, value, priority)\n\n    def delete(\n        self, name: _SettingsKeyT, priority: Union[int, str] = \"project\"\n    ) -> None:\n        if name not in self:\n            raise KeyError(name)\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= cast(int, self.getpriority(name)):\n            del self.attributes[name]\n\n    def __delitem__(self, name: _SettingsKeyT) -> None:\n        self._assert_mutability()\n        del self.attributes[name]\n\n    def _assert_mutability(self) -> None:\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")\n\n    def copy(self) -> Self:\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)\n\n    def freeze(self) -> None:\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True\n\n    def frozencopy(self) -> Self:\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy\n\n    def __iter__(self) -> Iterator[_SettingsKeyT]:\n        return iter(self.attributes)\n\n    def __len__(self) -> int:\n        return len(self.attributes)\n\n    def _to_dict(self) -> Dict[_SettingsKeyT, Any]:\n        return {\n            self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n            for k, v in self.items()\n        }\n\n    def _get_key(self, key_value: Any) -> _SettingsKeyT:\n        return (\n            key_value\n            if isinstance(key_value, (bool, float, int, str, type(None)))\n            else str(key_value)\n        )\n\n    def copy_to_dict(self) -> Dict[_SettingsKeyT, Any]:\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()\n\n    # https://ipython.readthedocs.io/en/stable/config/integrating.html#pretty-printing\n    def _repr_pretty_(self, p: Any, cycle: bool) -> None:\n        if cycle:\n            p.text(repr(self))\n        else:\n            p.text(pformat(self.copy_to_dict()))\n\n    def pop(self, name: _SettingsKeyT, default: Any = __default) -> Any:\n        try:\n            value = self.attributes[name].value\n        except KeyError:\n            if default is self.__default:\n                raise\n\n            return default\n        else:\n            self.__delitem__(name)\n            return value\n\n\nclass Settings(BaseSettings):\n    \"\"\"\n    This object stores Scrapy settings for the configuration of internal\n    components, and can be used for any further customization.\n\n    It is a direct subclass and supports all methods of\n    :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation\n    of this class, the new object will have the global default settings\n    described on :ref:`topics-settings-ref` already populated.\n    \"\"\"\n\n    def __init__(\n        self, values: _SettingsInputT = None, priority: Union[int, str] = \"project\"\n    ):\n        # Do not pass kwarg values here. We don't want to promote user-defined\n        # dicts, and we want to update, not replace, default dicts with the\n        # values given by the user\n        super().__init__()\n        self.setmodule(default_settings, \"default\")\n        # Promote default dictionaries to BaseSettings instances for per-key\n        # priorities\n        for name, val in self.items():\n            if isinstance(val, dict):\n                self.set(name, BaseSettings(val, \"default\"), \"default\")\n        self.update(values, priority)\n\n\ndef iter_default_settings() -> Iterable[Tuple[str, Any]]:\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)\n\n\ndef overridden_settings(\n    settings: Mapping[_SettingsKeyT, Any]\n) -> Iterable[Tuple[str, Any]]:\n    \"\"\"Return an iterable of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value\n", "scrapy/settings/default_settings.py": "\"\"\"\nThis module contains the default values for all settings used by Scrapy.\n\nFor more information about these settings you can read the settings\ndocumentation in docs/topics/settings.rst\n\nScrapy developers, if you add a setting here remember to:\n\n* add it in alphabetical order\n* group similar settings without leaving blank lines\n* add its documentation to the available settings documentation\n  (docs/topics/settings.rst)\n\n\"\"\"\n\nimport sys\nfrom importlib import import_module\nfrom pathlib import Path\n\nADDONS = {}\n\nAJAXCRAWL_ENABLED = False\n\nASYNCIO_EVENT_LOOP = None\n\nAUTOTHROTTLE_ENABLED = False\nAUTOTHROTTLE_DEBUG = False\nAUTOTHROTTLE_MAX_DELAY = 60.0\nAUTOTHROTTLE_START_DELAY = 5.0\nAUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n\nBOT_NAME = \"scrapybot\"\n\nCLOSESPIDER_TIMEOUT = 0\nCLOSESPIDER_PAGECOUNT = 0\nCLOSESPIDER_ITEMCOUNT = 0\nCLOSESPIDER_ERRORCOUNT = 0\n\nCOMMANDS_MODULE = \"\"\n\nCOMPRESSION_ENABLED = True\n\nCONCURRENT_ITEMS = 100\n\nCONCURRENT_REQUESTS = 16\nCONCURRENT_REQUESTS_PER_DOMAIN = 8\nCONCURRENT_REQUESTS_PER_IP = 0\n\nCOOKIES_ENABLED = True\nCOOKIES_DEBUG = False\n\nDEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n\nDEFAULT_REQUEST_HEADERS = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"en\",\n}\n\nDEPTH_LIMIT = 0\nDEPTH_STATS_VERBOSE = False\nDEPTH_PRIORITY = 0\n\nDNSCACHE_ENABLED = True\nDNSCACHE_SIZE = 10000\nDNS_RESOLVER = \"scrapy.resolver.CachingThreadedResolver\"\nDNS_TIMEOUT = 60\n\nDOWNLOAD_DELAY = 0\n\nDOWNLOAD_HANDLERS = {}\nDOWNLOAD_HANDLERS_BASE = {\n    \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n    \"file\": \"scrapy.core.downloader.handlers.file.FileDownloadHandler\",\n    \"http\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"https\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"s3\": \"scrapy.core.downloader.handlers.s3.S3DownloadHandler\",\n    \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\",\n}\n\nDOWNLOAD_TIMEOUT = 180  # 3mins\n\nDOWNLOAD_MAXSIZE = 1024 * 1024 * 1024  # 1024m\nDOWNLOAD_WARNSIZE = 32 * 1024 * 1024  # 32m\n\nDOWNLOAD_FAIL_ON_DATALOSS = True\n\nDOWNLOADER = \"scrapy.core.downloader.Downloader\"\n\nDOWNLOADER_HTTPCLIENTFACTORY = (\n    \"scrapy.core.downloader.webclient.ScrapyHTTPClientFactory\"\n)\nDOWNLOADER_CLIENTCONTEXTFACTORY = (\n    \"scrapy.core.downloader.contextfactory.ScrapyClientContextFactory\"\n)\nDOWNLOADER_CLIENT_TLS_CIPHERS = \"DEFAULT\"\n# Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:\nDOWNLOADER_CLIENT_TLS_METHOD = \"TLS\"\nDOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n\nDOWNLOADER_MIDDLEWARES = {}\n\nDOWNLOADER_MIDDLEWARES_BASE = {\n    # Engine side\n    \"scrapy.downloadermiddlewares.offsite.OffsiteMiddleware\": 50,\n    \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n    \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n    \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n    \"scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\": 400,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": 500,\n    \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 550,\n    \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware\": 560,\n    \"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\": 580,\n    \"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\": 590,\n    \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 600,\n    \"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\": 700,\n    \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\": 750,\n    \"scrapy.downloadermiddlewares.stats.DownloaderStats\": 850,\n    \"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\": 900,\n    # Downloader side\n}\n\nDOWNLOADER_STATS = True\n\nDUPEFILTER_CLASS = \"scrapy.dupefilters.RFPDupeFilter\"\n\nEDITOR = \"vi\"\nif sys.platform == \"win32\":\n    EDITOR = \"%s -m idlelib.idle\"\n\nEXTENSIONS = {}\n\nEXTENSIONS_BASE = {\n    \"scrapy.extensions.corestats.CoreStats\": 0,\n    \"scrapy.extensions.telnet.TelnetConsole\": 0,\n    \"scrapy.extensions.memusage.MemoryUsage\": 0,\n    \"scrapy.extensions.memdebug.MemoryDebugger\": 0,\n    \"scrapy.extensions.closespider.CloseSpider\": 0,\n    \"scrapy.extensions.feedexport.FeedExporter\": 0,\n    \"scrapy.extensions.logstats.LogStats\": 0,\n    \"scrapy.extensions.spiderstate.SpiderState\": 0,\n    \"scrapy.extensions.throttle.AutoThrottle\": 0,\n}\n\nFEED_TEMPDIR = None\nFEEDS = {}\nFEED_URI_PARAMS = None  # a function to extend uri arguments\nFEED_STORE_EMPTY = True\nFEED_EXPORT_ENCODING = None\nFEED_EXPORT_FIELDS = None\nFEED_STORAGES = {}\nFEED_STORAGES_BASE = {\n    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n    \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n}\nFEED_EXPORT_BATCH_ITEM_COUNT = 0\nFEED_EXPORTERS = {}\nFEED_EXPORTERS_BASE = {\n    \"json\": \"scrapy.exporters.JsonItemExporter\",\n    \"jsonlines\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"jsonl\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"jl\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"csv\": \"scrapy.exporters.CsvItemExporter\",\n    \"xml\": \"scrapy.exporters.XmlItemExporter\",\n    \"marshal\": \"scrapy.exporters.MarshalItemExporter\",\n    \"pickle\": \"scrapy.exporters.PickleItemExporter\",\n}\nFEED_EXPORT_INDENT = 0\n\nFEED_STORAGE_FTP_ACTIVE = False\nFEED_STORAGE_GCS_ACL = \"\"\nFEED_STORAGE_S3_ACL = \"\"\n\nFILES_STORE_S3_ACL = \"private\"\nFILES_STORE_GCS_ACL = \"\"\n\nFTP_USER = \"anonymous\"\nFTP_PASSWORD = \"guest\"  # nosec\nFTP_PASSIVE_MODE = True\n\nGCS_PROJECT_ID = None\n\nHTTPCACHE_ENABLED = False\nHTTPCACHE_DIR = \"httpcache\"\nHTTPCACHE_IGNORE_MISSING = False\nHTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\nHTTPCACHE_EXPIRATION_SECS = 0\nHTTPCACHE_ALWAYS_STORE = False\nHTTPCACHE_IGNORE_HTTP_CODES = []\nHTTPCACHE_IGNORE_SCHEMES = [\"file\"]\nHTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\nHTTPCACHE_DBM_MODULE = \"dbm\"\nHTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\nHTTPCACHE_GZIP = False\n\nHTTPPROXY_ENABLED = True\nHTTPPROXY_AUTH_ENCODING = \"latin-1\"\n\nIMAGES_STORE_S3_ACL = \"private\"\nIMAGES_STORE_GCS_ACL = \"\"\n\nITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n\nITEM_PIPELINES = {}\nITEM_PIPELINES_BASE = {}\n\nJOBDIR = None\n\nLOG_ENABLED = True\nLOG_ENCODING = \"utf-8\"\nLOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\nLOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\"\nLOG_DATEFORMAT = \"%Y-%m-%d %H:%M:%S\"\nLOG_STDOUT = False\nLOG_LEVEL = \"DEBUG\"\nLOG_FILE = None\nLOG_FILE_APPEND = True\nLOG_SHORT_NAMES = False\n\nSCHEDULER_DEBUG = False\n\nLOGSTATS_INTERVAL = 60.0\n\nMAIL_HOST = \"localhost\"\nMAIL_PORT = 25\nMAIL_FROM = \"scrapy@localhost\"\nMAIL_PASS = None\nMAIL_USER = None\n\nMEMDEBUG_ENABLED = False  # enable memory debugging\nMEMDEBUG_NOTIFY = []  # send memory debugging report by mail at engine shutdown\n\nMEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\nMEMUSAGE_ENABLED = True\nMEMUSAGE_LIMIT_MB = 0\nMEMUSAGE_NOTIFY_MAIL = []\nMEMUSAGE_WARNING_MB = 0\n\nMETAREFRESH_ENABLED = True\nMETAREFRESH_IGNORE_TAGS = [\"noscript\"]\nMETAREFRESH_MAXDELAY = 100\n\nNEWSPIDER_MODULE = \"\"\n\nPERIODIC_LOG_DELTA = None\nPERIODIC_LOG_STATS = None\nPERIODIC_LOG_TIMING_ENABLED = False\n\nRANDOMIZE_DOWNLOAD_DELAY = True\n\nREACTOR_THREADPOOL_MAXSIZE = 10\n\nREDIRECT_ENABLED = True\nREDIRECT_MAX_TIMES = 20  # uses Firefox default setting\nREDIRECT_PRIORITY_ADJUST = +2\n\nREFERER_ENABLED = True\nREFERRER_POLICY = \"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"\n\nREQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\nREQUEST_FINGERPRINTER_IMPLEMENTATION = \"SENTINEL\"\n\nRETRY_ENABLED = True\nRETRY_TIMES = 2  # initial response + 2 retries = 3 requests\nRETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\nRETRY_PRIORITY_ADJUST = -1\nRETRY_EXCEPTIONS = [\n    \"twisted.internet.defer.TimeoutError\",\n    \"twisted.internet.error.TimeoutError\",\n    \"twisted.internet.error.DNSLookupError\",\n    \"twisted.internet.error.ConnectionRefusedError\",\n    \"twisted.internet.error.ConnectionDone\",\n    \"twisted.internet.error.ConnectError\",\n    \"twisted.internet.error.ConnectionLost\",\n    \"twisted.internet.error.TCPTimedOutError\",\n    \"twisted.web.client.ResponseFailed\",\n    # OSError is raised by the HttpCompression middleware when trying to\n    # decompress an empty response\n    OSError,\n    \"scrapy.core.downloader.handlers.http11.TunnelError\",\n]\n\nROBOTSTXT_OBEY = False\nROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\nROBOTSTXT_USER_AGENT = None\n\nSCHEDULER = \"scrapy.core.scheduler.Scheduler\"\nSCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleLifoDiskQueue\"\nSCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.LifoMemoryQueue\"\nSCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.ScrapyPriorityQueue\"\n\nSCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n\nSPIDER_LOADER_CLASS = \"scrapy.spiderloader.SpiderLoader\"\nSPIDER_LOADER_WARN_ONLY = False\n\nSPIDER_MIDDLEWARES = {}\n\nSPIDER_MIDDLEWARES_BASE = {\n    # Engine side\n    \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n    \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n    \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n    \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900,\n    # Spider side\n}\n\nSPIDER_MODULES = []\n\nSTATS_CLASS = \"scrapy.statscollectors.MemoryStatsCollector\"\nSTATS_DUMP = True\n\nSTATSMAILER_RCPTS = []\n\nTEMPLATES_DIR = str((Path(__file__).parent / \"..\" / \"templates\").resolve())\n\nURLLENGTH_LIMIT = 2083\n\nUSER_AGENT = f'Scrapy/{import_module(\"scrapy\").__version__} (+https://scrapy.org)'\n\nTELNETCONSOLE_ENABLED = 1\nTELNETCONSOLE_PORT = [6023, 6073]\nTELNETCONSOLE_HOST = \"127.0.0.1\"\nTELNETCONSOLE_USERNAME = \"scrapy\"\nTELNETCONSOLE_PASSWORD = None\n\nTWISTED_REACTOR = None\n\nSPIDER_CONTRACTS = {}\nSPIDER_CONTRACTS_BASE = {\n    \"scrapy.contracts.default.UrlContract\": 1,\n    \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n    \"scrapy.contracts.default.ReturnsContract\": 2,\n    \"scrapy.contracts.default.ScrapesContract\": 3,\n}\n", "scrapy/core/engine.py": "\"\"\"\nThis is the Scrapy engine which controls the Scheduler, Downloader and Spider.\n\nFor more information see docs/topics/architecture.rst\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom time import time\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generator,\n    Iterable,\n    Iterator,\n    Optional,\n    Set,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks, succeed\nfrom twisted.internet.task import LoopingCall\nfrom twisted.python.failure import Failure\n\nfrom scrapy import signals\nfrom scrapy.core.downloader import Downloader\nfrom scrapy.core.scraper import Scraper\nfrom scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.log import failure_to_exc_info, logformatter_adapter\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.reactor import CallLaterOnce\n\nif TYPE_CHECKING:\n    from scrapy.core.scheduler import BaseScheduler\n    from scrapy.core.scraper import _HandleOutputDeferred\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass Slot:\n    def __init__(\n        self,\n        start_requests: Iterable[Request],\n        close_if_idle: bool,\n        nextcall: CallLaterOnce[None],\n        scheduler: BaseScheduler,\n    ) -> None:\n        self.closing: Optional[Deferred[None]] = None\n        self.inprogress: Set[Request] = set()\n        self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n        self.close_if_idle: bool = close_if_idle\n        self.nextcall: CallLaterOnce[None] = nextcall\n        self.scheduler: BaseScheduler = scheduler\n        self.heartbeat: LoopingCall = LoopingCall(nextcall.schedule)\n\n    def add_request(self, request: Request) -> None:\n        self.inprogress.add(request)\n\n    def remove_request(self, request: Request) -> None:\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()\n\n    def close(self) -> Deferred[None]:\n        self.closing = Deferred()\n        self._maybe_fire_closing()\n        return self.closing\n\n    def _maybe_fire_closing(self) -> None:\n        if self.closing is not None and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)\n\n\nclass ExecutionEngine:\n    def __init__(\n        self,\n        crawler: Crawler,\n        spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]],\n    ) -> None:\n        self.crawler: Crawler = crawler\n        self.settings: Settings = crawler.settings\n        self.signals: SignalManager = crawler.signals\n        assert crawler.logformatter\n        self.logformatter: LogFormatter = crawler.logformatter\n        self.slot: Optional[Slot] = None\n        self.spider: Optional[Spider] = None\n        self.running: bool = False\n        self.paused: bool = False\n        self.scheduler_cls: Type[BaseScheduler] = self._get_scheduler_class(\n            crawler.settings\n        )\n        downloader_cls: Type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n        self.downloader: Downloader = downloader_cls(crawler)\n        self.scraper = Scraper(crawler)\n        self._spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]] = (\n            spider_closed_callback\n        )\n        self.start_time: Optional[float] = None\n\n    def _get_scheduler_class(self, settings: BaseSettings) -> Type[BaseScheduler]:\n        from scrapy.core.scheduler import BaseScheduler\n\n        scheduler_cls: Type[BaseScheduler] = load_object(settings[\"SCHEDULER\"])\n        if not issubclass(scheduler_cls, BaseScheduler):\n            raise TypeError(\n                f\"The provided scheduler class ({settings['SCHEDULER']})\"\n                \" does not fully implement the scheduler interface\"\n            )\n        return scheduler_cls\n\n    @inlineCallbacks\n    def start(self) -> Generator[Deferred[Any], Any, None]:\n        if self.running:\n            raise RuntimeError(\"Engine already running\")\n        self.start_time = time()\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n        self.running = True\n        self._closewait: Deferred[None] = Deferred()\n        yield self._closewait\n\n    def stop(self) -> Deferred[None]:\n        \"\"\"Gracefully stop the execution engine\"\"\"\n\n        @inlineCallbacks\n        def _finish_stopping_engine(_: Any) -> Generator[Deferred[Any], Any, None]:\n            yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n            self._closewait.callback(None)\n\n        if not self.running:\n            raise RuntimeError(\"Engine not running\")\n\n        self.running = False\n        dfd = (\n            self.close_spider(self.spider, reason=\"shutdown\")\n            if self.spider is not None\n            else succeed(None)\n        )\n        return dfd.addBoth(_finish_stopping_engine)\n\n    def close(self) -> Deferred[None]:\n        \"\"\"\n        Gracefully close the execution engine.\n        If it has already been started, stop it. In all cases, close the spider and the downloader.\n        \"\"\"\n        if self.running:\n            return self.stop()  # will also close spider and downloader\n        if self.spider is not None:\n            return self.close_spider(\n                self.spider, reason=\"shutdown\"\n            )  # will also close downloader\n        self.downloader.close()\n        return succeed(None)\n\n    def pause(self) -> None:\n        self.paused = True\n\n    def unpause(self) -> None:\n        self.paused = False\n\n    def _next_request(self) -> None:\n        if self.slot is None:\n            return\n\n        assert self.spider is not None  # typing\n\n        if self.paused:\n            return None\n\n        while (\n            not self._needs_backout()\n            and self._next_request_from_scheduler() is not None\n        ):\n            pass\n\n        if self.slot.start_requests is not None and not self._needs_backout():\n            try:\n                request = next(self.slot.start_requests)\n            except StopIteration:\n                self.slot.start_requests = None\n            except Exception:\n                self.slot.start_requests = None\n                logger.error(\n                    \"Error while obtaining start requests\",\n                    exc_info=True,\n                    extra={\"spider\": self.spider},\n                )\n            else:\n                self.crawl(request)\n\n        if self.spider_is_idle() and self.slot.close_if_idle:\n            self._spider_idle()\n\n    def _needs_backout(self) -> bool:\n        assert self.slot is not None  # typing\n        assert self.scraper.slot is not None  # typing\n        return (\n            not self.running\n            or bool(self.slot.closing)\n            or self.downloader.needs_backout()\n            or self.scraper.slot.needs_backout()\n        )\n\n    def _next_request_from_scheduler(self) -> Optional[Deferred[None]]:\n        assert self.slot is not None  # typing\n        assert self.spider is not None  # typing\n\n        request = self.slot.scheduler.next_request()\n        if request is None:\n            return None\n\n        d: Deferred[Union[Response, Request]] = self._download(request)\n        d.addBoth(self._handle_downloader_output, request)\n        d.addErrback(\n            lambda f: logger.info(\n                \"Error while handling downloader output\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n\n        def _remove_request(_: Any) -> None:\n            assert self.slot\n            self.slot.remove_request(request)\n\n        d2: Deferred[None] = d.addBoth(_remove_request)\n        d2.addErrback(\n            lambda f: logger.info(\n                \"Error while removing request from slot\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        slot = self.slot\n        d2.addBoth(lambda _: slot.nextcall.schedule())\n        d2.addErrback(\n            lambda f: logger.info(\n                \"Error while scheduling new request\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        return d2\n\n    def _handle_downloader_output(\n        self, result: Union[Request, Response, Failure], request: Request\n    ) -> Optional[_HandleOutputDeferred]:\n        assert self.spider is not None  # typing\n\n        if not isinstance(result, (Request, Response, Failure)):\n            raise TypeError(\n                f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\"\n            )\n\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(result, Request):\n            self.crawl(result)\n            return None\n\n        d = self.scraper.enqueue_scrape(result, request, self.spider)\n        d.addErrback(\n            lambda f: logger.error(\n                \"Error while enqueuing downloader output\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        return d\n\n    def spider_is_idle(self) -> bool:\n        if self.slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n        if not self.scraper.slot.is_idle():  # type: ignore[union-attr]\n            return False\n        if self.downloader.active:  # downloader has pending requests\n            return False\n        if self.slot.start_requests is not None:  # not all start requests are handled\n            return False\n        if self.slot.scheduler.has_pending_requests():\n            return False\n        return True\n\n    def crawl(self, request: Request) -> None:\n        \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        self._schedule_request(request, self.spider)\n        self.slot.nextcall.schedule()  # type: ignore[union-attr]\n\n    def _schedule_request(self, request: Request, spider: Spider) -> None:\n        request_scheduled_result = self.signals.send_catch_log(\n            signals.request_scheduled,\n            request=request,\n            spider=spider,\n            dont_log=IgnoreRequest,\n        )\n        for handler, result in request_scheduled_result:\n            if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n                logger.debug(\n                    f\"Signal handler {global_object_name(handler)} dropped \"\n                    f\"request {request} before it reached the scheduler.\"\n                )\n                return\n        if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n            self.signals.send_catch_log(\n                signals.request_dropped, request=request, spider=spider\n            )\n\n    def download(self, request: Request) -> Deferred[Response]:\n        \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        d: Deferred[Union[Response, Request]] = self._download(request)\n        # Deferred.addBoth() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n        d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[arg-type]\n        return d2\n\n    def _downloaded(\n        self, result: Union[Response, Request, Failure], request: Request\n    ) -> Union[Deferred[Response], Response, Failure]:\n        assert self.slot is not None  # typing\n        self.slot.remove_request(request)\n        return self.download(result) if isinstance(result, Request) else result\n\n    def _download(self, request: Request) -> Deferred[Union[Response, Request]]:\n        assert self.slot is not None  # typing\n\n        self.slot.add_request(request)\n\n        def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n            if not isinstance(result, (Response, Request)):\n                raise TypeError(\n                    f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\"\n                )\n            if isinstance(result, Response):\n                if result.request is None:\n                    result.request = request\n                assert self.spider is not None\n                logkws = self.logformatter.crawled(result.request, result, self.spider)\n                if logkws is not None:\n                    logger.log(\n                        *logformatter_adapter(logkws), extra={\"spider\": self.spider}\n                    )\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=result,\n                    request=result.request,\n                    spider=self.spider,\n                )\n            return result\n\n        def _on_complete(_: _T) -> _T:\n            assert self.slot is not None\n            self.slot.nextcall.schedule()\n            return _\n\n        assert self.spider is not None\n        dwld: Deferred[Union[Response, Request]] = self.downloader.fetch(\n            request, self.spider\n        )\n        dwld.addCallback(_on_success)\n        dwld.addBoth(_on_complete)\n        return dwld\n\n    @inlineCallbacks\n    def open_spider(\n        self,\n        spider: Spider,\n        start_requests: Iterable[Request] = (),\n        close_if_idle: bool = True,\n    ) -> Generator[Deferred[Any], Any, None]:\n        if self.slot is not None:\n            raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n        logger.info(\"Spider opened\", extra={\"spider\": spider})\n        nextcall = CallLaterOnce(self._next_request)\n        scheduler = build_from_crawler(self.scheduler_cls, self.crawler)\n        start_requests = yield self.scraper.spidermw.process_start_requests(\n            start_requests, spider\n        )\n        self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n        self.spider = spider\n        if hasattr(scheduler, \"open\"):\n            if d := scheduler.open(spider):\n                yield d\n        yield self.scraper.open_spider(spider)\n        assert self.crawler.stats\n        self.crawler.stats.open_spider(spider)\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n        self.slot.nextcall.schedule()\n        self.slot.heartbeat.start(5)\n\n    def _spider_idle(self) -> None:\n        \"\"\"\n        Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n        It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n        exception, the spider is not closed until the next loop and this function is guaranteed to be called\n        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.\n        \"\"\"\n        assert self.spider is not None  # typing\n        expected_ex = (DontCloseSpider, CloseSpider)\n        res = self.signals.send_catch_log(\n            signals.spider_idle, spider=self.spider, dont_log=expected_ex\n        )\n        detected_ex = {\n            ex: x.value\n            for _, x in res\n            for ex in expected_ex\n            if isinstance(x, Failure) and isinstance(x.value, ex)\n        }\n        if DontCloseSpider in detected_ex:\n            return None\n        if self.spider_is_idle():\n            ex = detected_ex.get(CloseSpider, CloseSpider(reason=\"finished\"))\n            assert isinstance(ex, CloseSpider)  # typing\n            self.close_spider(self.spider, reason=ex.reason)\n\n    def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred[None]:\n        \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n        if self.slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n\n        if self.slot.closing is not None:\n            return self.slot.closing\n\n        logger.info(\n            \"Closing spider (%(reason)s)\", {\"reason\": reason}, extra={\"spider\": spider}\n        )\n\n        dfd = self.slot.close()\n\n        def log_failure(msg: str) -> Callable[[Failure], None]:\n            def errback(failure: Failure) -> None:\n                logger.error(\n                    msg, exc_info=failure_to_exc_info(failure), extra={\"spider\": spider}\n                )\n\n            return errback\n\n        dfd.addBoth(lambda _: self.downloader.close())\n        dfd.addErrback(log_failure(\"Downloader close failure\"))\n\n        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n        dfd.addErrback(log_failure(\"Scraper close failure\"))\n\n        if hasattr(self.slot.scheduler, \"close\"):\n            dfd.addBoth(lambda _: cast(Slot, self.slot).scheduler.close(reason))\n            dfd.addErrback(log_failure(\"Scheduler close failure\"))\n\n        dfd.addBoth(\n            lambda _: self.signals.send_catch_log_deferred(\n                signal=signals.spider_closed,\n                spider=spider,\n                reason=reason,\n            )\n        )\n        dfd.addErrback(log_failure(\"Error while sending spider_close signal\"))\n\n        def close_stats(_: Any) -> None:\n            assert self.crawler.stats\n            self.crawler.stats.close_spider(spider, reason=reason)\n\n        dfd.addBoth(close_stats)\n        dfd.addErrback(log_failure(\"Stats close failure\"))\n\n        dfd.addBoth(\n            lambda _: logger.info(\n                \"Spider closed (%(reason)s)\",\n                {\"reason\": reason},\n                extra={\"spider\": spider},\n            )\n        )\n\n        dfd.addBoth(lambda _: setattr(self, \"slot\", None))\n        dfd.addErrback(log_failure(\"Error while unassigning slot\"))\n\n        dfd.addBoth(lambda _: setattr(self, \"spider\", None))\n        dfd.addErrback(log_failure(\"Error while unassigning spider\"))\n\n        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n        return dfd\n", "scrapy/core/scraper.py": "\"\"\"This module implements the Scraper component which parses responses and\nextracts information from them\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections import deque\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    Deque,\n    Generator,\n    Iterable,\n    Iterator,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom itemadapter import is_item\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider, signals\nfrom scrapy.core.spidermw import SpiderMiddlewareManager\nfrom scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.pipelines import ItemPipelineManager\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.utils.defer import (\n    DeferredListResultListT,\n    aiter_errback,\n    defer_fail,\n    defer_succeed,\n    iter_errback,\n    parallel,\n    parallel_async,\n)\nfrom scrapy.utils.log import failure_to_exc_info, logformatter_adapter\nfrom scrapy.utils.misc import load_object, warn_on_generator_with_return_value\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\n_T = TypeVar(\"_T\")\n_ParallelResult = DeferredListResultListT[Iterator[Any]]\n\nif TYPE_CHECKING:\n    # parameterized Deferreds require Twisted 21.7.0\n    _HandleOutputDeferred = Deferred[Union[_ParallelResult, None]]\n    QueueTuple = Tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n\n\nclass Slot:\n    \"\"\"Scraper slot (one per running spider)\"\"\"\n\n    MIN_RESPONSE_SIZE = 1024\n\n    def __init__(self, max_active_size: int = 5000000):\n        self.max_active_size = max_active_size\n        self.queue: Deque[QueueTuple] = deque()\n        self.active: Set[Request] = set()\n        self.active_size: int = 0\n        self.itemproc_size: int = 0\n        self.closing: Optional[Deferred[Spider]] = None\n\n    def add_response_request(\n        self, result: Union[Response, Failure], request: Request\n    ) -> _HandleOutputDeferred:\n        deferred: _HandleOutputDeferred = Deferred()\n        self.queue.append((result, request, deferred))\n        if isinstance(result, Response):\n            self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred\n\n    def next_response_request_deferred(self) -> QueueTuple:\n        response, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return response, request, deferred\n\n    def finish_response(\n        self, result: Union[Response, Failure], request: Request\n    ) -> None:\n        self.active.remove(request)\n        if isinstance(result, Response):\n            self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE\n\n    def is_idle(self) -> bool:\n        return not (self.queue or self.active)\n\n    def needs_backout(self) -> bool:\n        return self.active_size > self.max_active_size\n\n\nclass Scraper:\n    def __init__(self, crawler: Crawler) -> None:\n        self.slot: Optional[Slot] = None\n        self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(\n            crawler\n        )\n        itemproc_cls: Type[ItemPipelineManager] = load_object(\n            crawler.settings[\"ITEM_PROCESSOR\"]\n        )\n        self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n        self.concurrent_items: int = crawler.settings.getint(\"CONCURRENT_ITEMS\")\n        self.crawler: Crawler = crawler\n        self.signals: SignalManager = crawler.signals\n        assert crawler.logformatter\n        self.logformatter: LogFormatter = crawler.logformatter\n\n    @inlineCallbacks\n    def open_spider(self, spider: Spider) -> Generator[Deferred[Any], Any, None]:\n        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n        self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n        yield self.itemproc.open_spider(spider)\n\n    def close_spider(self, spider: Spider) -> Deferred[Spider]:\n        \"\"\"Close a spider being scraped and release its resources\"\"\"\n        if self.slot is None:\n            raise RuntimeError(\"Scraper slot not assigned\")\n        self.slot.closing = Deferred()\n        self.slot.closing.addCallback(self.itemproc.close_spider)\n        self._check_if_closing(spider)\n        return self.slot.closing\n\n    def is_idle(self) -> bool:\n        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n        return not self.slot\n\n    def _check_if_closing(self, spider: Spider) -> None:\n        assert self.slot is not None  # typing\n        if self.slot.closing and self.slot.is_idle():\n            self.slot.closing.callback(spider)\n\n    def enqueue_scrape(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> _HandleOutputDeferred:\n        if self.slot is None:\n            raise RuntimeError(\"Scraper slot not assigned\")\n        dfd = self.slot.add_response_request(result, request)\n\n        def finish_scraping(_: _T) -> _T:\n            assert self.slot is not None\n            self.slot.finish_response(result, request)\n            self._check_if_closing(spider)\n            self._scrape_next(spider)\n            return _\n\n        dfd.addBoth(finish_scraping)\n        dfd.addErrback(\n            lambda f: logger.error(\n                \"Scraper bug processing %(request)s\",\n                {\"request\": request},\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": spider},\n            )\n        )\n        self._scrape_next(spider)\n        return dfd\n\n    def _scrape_next(self, spider: Spider) -> None:\n        assert self.slot is not None  # typing\n        while self.slot.queue:\n            response, request, deferred = self.slot.next_response_request_deferred()\n            self._scrape(response, request, spider).chainDeferred(deferred)\n\n    def _scrape(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> _HandleOutputDeferred:\n        \"\"\"\n        Handle the downloaded response or failure through the spider callback/errback\n        \"\"\"\n        if not isinstance(result, (Response, Failure)):\n            raise TypeError(\n                f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n            )\n        dfd: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = self._scrape2(\n            result, request, spider\n        )  # returns spider's processed output\n        dfd.addErrback(self.handle_spider_error, request, result, spider)\n        dfd2: _HandleOutputDeferred = dfd.addCallback(\n            self.handle_spider_output, request, cast(Response, result), spider\n        )\n        return dfd2\n\n    def _scrape2(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n        \"\"\"\n        Handle the different cases of request's result been a Response or a Failure\n        \"\"\"\n        if isinstance(result, Response):\n            # Deferreds are invariant so Mutable*Chain isn't matched to *Iterable\n            return self.spidermw.scrape_response(  # type: ignore[return-value]\n                self.call_spider, result, request, spider\n            )\n        # else result is a Failure\n        dfd = self.call_spider(result, request, spider)\n        dfd.addErrback(self._log_download_errors, result, request, spider)\n        return dfd\n\n    def call_spider(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n        dfd: Deferred[Any]\n        if isinstance(result, Response):\n            if getattr(result, \"request\", None) is None:\n                result.request = request\n            assert result.request\n            callback = result.request.callback or spider._parse\n            warn_on_generator_with_return_value(spider, callback)\n            dfd = defer_succeed(result)\n            dfd.addCallbacks(\n                callback=callback, callbackKeywords=result.request.cb_kwargs\n            )\n        else:  # result is a Failure\n            # TODO: properly type adding this attribute to a Failure\n            result.request = request  # type: ignore[attr-defined]\n            dfd = defer_fail(result)\n            if request.errback:\n                warn_on_generator_with_return_value(spider, request.errback)\n                dfd.addErrback(request.errback)\n        dfd2: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = dfd.addCallback(\n            iterate_spider_output\n        )\n        return dfd2\n\n    def handle_spider_error(\n        self,\n        _failure: Failure,\n        request: Request,\n        response: Union[Response, Failure],\n        spider: Spider,\n    ) -> None:\n        exc = _failure.value\n        if isinstance(exc, CloseSpider):\n            assert self.crawler.engine is not None  # typing\n            self.crawler.engine.close_spider(spider, exc.reason or \"cancelled\")\n            return\n        logkws = self.logformatter.spider_error(_failure, request, response, spider)\n        logger.log(\n            *logformatter_adapter(logkws),\n            exc_info=failure_to_exc_info(_failure),\n            extra={\"spider\": spider},\n        )\n        self.signals.send_catch_log(\n            signal=signals.spider_error,\n            failure=_failure,\n            response=response,\n            spider=spider,\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\n            f\"spider_exceptions/{_failure.value.__class__.__name__}\", spider=spider\n        )\n\n    def handle_spider_output(\n        self,\n        result: Union[Iterable[_T], AsyncIterable[_T]],\n        request: Request,\n        response: Response,\n        spider: Spider,\n    ) -> _HandleOutputDeferred:\n        if not result:\n            return defer_succeed(None)\n        it: Union[Iterable[_T], AsyncIterable[_T]]\n        dfd: Deferred[_ParallelResult]\n        if isinstance(result, AsyncIterable):\n            it = aiter_errback(\n                result, self.handle_spider_error, request, response, spider\n            )\n            dfd = parallel_async(\n                it,\n                self.concurrent_items,\n                self._process_spidermw_output,\n                request,\n                response,\n                spider,\n            )\n        else:\n            it = iter_errback(\n                result, self.handle_spider_error, request, response, spider\n            )\n            dfd = parallel(\n                it,\n                self.concurrent_items,\n                self._process_spidermw_output,\n                request,\n                response,\n                spider,\n            )\n        # returning Deferred[_ParallelResult] instead of Deferred[Union[_ParallelResult, None]]\n        return dfd  # type: ignore[return-value]\n\n    def _process_spidermw_output(\n        self, output: Any, request: Request, response: Response, spider: Spider\n    ) -> Optional[Deferred[Any]]:\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider\n        \"\"\"\n        assert self.slot is not None  # typing\n        if isinstance(output, Request):\n            assert self.crawler.engine is not None  # typing\n            self.crawler.engine.crawl(request=output)\n        elif is_item(output):\n            self.slot.itemproc_size += 1\n            dfd = self.itemproc.process_item(output, spider)\n            dfd.addBoth(self._itemproc_finished, output, response, spider)\n            return dfd\n        elif output is None:\n            pass\n        else:\n            typename = type(output).__name__\n            logger.error(\n                \"Spider must return request, item, or None, got %(typename)r in %(request)s\",\n                {\"request\": request, \"typename\": typename},\n                extra={\"spider\": spider},\n            )\n        return None\n\n    def _log_download_errors(\n        self,\n        spider_failure: Failure,\n        download_failure: Failure,\n        request: Request,\n        spider: Spider,\n    ) -> Union[Failure, None]:\n        \"\"\"Log and silence errors that come from the engine (typically download\n        errors that got propagated thru here).\n\n        spider_failure: the value passed into the errback of self.call_spider()\n        download_failure: the value passed into _scrape2() from\n        ExecutionEngine._handle_downloader_output() as \"result\"\n        \"\"\"\n        if not download_failure.check(IgnoreRequest):\n            if download_failure.frames:\n                logkws = self.logformatter.download_error(\n                    download_failure, request, spider\n                )\n                logger.log(\n                    *logformatter_adapter(logkws),\n                    extra={\"spider\": spider},\n                    exc_info=failure_to_exc_info(download_failure),\n                )\n            else:\n                errmsg = download_failure.getErrorMessage()\n                if errmsg:\n                    logkws = self.logformatter.download_error(\n                        download_failure, request, spider, errmsg\n                    )\n                    logger.log(\n                        *logformatter_adapter(logkws),\n                        extra={\"spider\": spider},\n                    )\n\n        if spider_failure is not download_failure:\n            return spider_failure\n        return None\n\n    def _itemproc_finished(\n        self, output: Any, item: Any, response: Response, spider: Spider\n    ) -> Deferred:\n        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n        assert self.slot is not None  # typing\n        self.slot.itemproc_size -= 1\n        if isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                logkws = self.logformatter.dropped(item, ex, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_dropped,\n                    item=item,\n                    response=response,\n                    spider=spider,\n                    exception=output.value,\n                )\n            assert ex\n            logkws = self.logformatter.item_error(item, ex, response, spider)\n            logger.log(\n                *logformatter_adapter(logkws),\n                extra={\"spider\": spider},\n                exc_info=failure_to_exc_info(output),\n            )\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_error,\n                item=item,\n                response=response,\n                spider=spider,\n                failure=output,\n            )\n        logkws = self.logformatter.scraped(output, response, spider)\n        if logkws is not None:\n            logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n        return self.signals.send_catch_log_deferred(\n            signal=signals.item_scraped, item=output, response=response, spider=spider\n        )\n", "scrapy/core/spidermw.py": "\"\"\"\nSpider Middleware manager\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom inspect import isasyncgenfunction, iscoroutine\nfrom itertools import islice\nfrom typing import (\n    Any,\n    AsyncIterable,\n    Callable,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Response\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n    mustbe_deferred,\n)\nfrom scrapy.utils.python import MutableAsyncChain, MutableChain\n\nlogger = logging.getLogger(__name__)\n\n\n_T = TypeVar(\"_T\")\nScrapeFunc = Callable[\n    [Union[Response, Failure], Request, Spider], Union[Iterable[_T], AsyncIterable[_T]]\n]\n\n\ndef _isiterable(o: Any) -> bool:\n    return isinstance(o, (Iterable, AsyncIterable))\n\n\nclass SpiderMiddlewareManager(MiddlewareManager):\n    component_name = \"spider middleware\"\n\n    def __init__(self, *middlewares: Any):\n        super().__init__(*middlewares)\n        self.downgrade_warning_done = False\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n\n    def _add_middleware(self, mw: Any) -> None:\n        super()._add_middleware(mw)\n        if hasattr(mw, \"process_spider_input\"):\n            self.methods[\"process_spider_input\"].append(mw.process_spider_input)\n        if hasattr(mw, \"process_start_requests\"):\n            self.methods[\"process_start_requests\"].appendleft(mw.process_start_requests)\n        process_spider_output = self._get_async_method_pair(mw, \"process_spider_output\")\n        self.methods[\"process_spider_output\"].appendleft(process_spider_output)\n        process_spider_exception = getattr(mw, \"process_spider_exception\", None)\n        self.methods[\"process_spider_exception\"].appendleft(process_spider_exception)\n\n    def _process_spider_input(\n        self,\n        scrape_func: ScrapeFunc,\n        response: Response,\n        request: Request,\n        spider: Spider,\n    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n        for method in self.methods[\"process_spider_input\"]:\n            method = cast(Callable, method)\n            try:\n                result = method(response=response, spider=spider)\n                if result is not None:\n                    msg = (\n                        f\"{method.__qualname__} must return None \"\n                        f\"or raise an exception, got {type(result)}\"\n                    )\n                    raise _InvalidOutput(msg)\n            except _InvalidOutput:\n                raise\n            except Exception:\n                return scrape_func(Failure(), request, spider)\n        return scrape_func(response, request, spider)\n\n    def _evaluate_iterable(\n        self,\n        response: Response,\n        spider: Spider,\n        iterable: Union[Iterable[_T], AsyncIterable[_T]],\n        exception_processor_index: int,\n        recover_to: Union[MutableChain[_T], MutableAsyncChain[_T]],\n    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n        def process_sync(iterable: Iterable[_T]) -> Iterable[_T]:\n            try:\n                yield from iterable\n            except Exception as ex:\n                exception_result = cast(\n                    Union[Failure, MutableChain[_T]],\n                    self._process_spider_exception(\n                        response, spider, Failure(ex), exception_processor_index\n                    ),\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                assert isinstance(recover_to, MutableChain)\n                recover_to.extend(exception_result)\n\n        async def process_async(iterable: AsyncIterable[_T]) -> AsyncIterable[_T]:\n            try:\n                async for r in iterable:\n                    yield r\n            except Exception as ex:\n                exception_result = cast(\n                    Union[Failure, MutableAsyncChain[_T]],\n                    self._process_spider_exception(\n                        response, spider, Failure(ex), exception_processor_index\n                    ),\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                assert isinstance(recover_to, MutableAsyncChain)\n                recover_to.extend(exception_result)\n\n        if isinstance(iterable, AsyncIterable):\n            return process_async(iterable)\n        return process_sync(iterable)\n\n    def _process_spider_exception(\n        self,\n        response: Response,\n        spider: Spider,\n        _failure: Failure,\n        start_index: int = 0,\n    ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n        exception = _failure.value\n        # don't handle _InvalidOutput exception\n        if isinstance(exception, _InvalidOutput):\n            return _failure\n        method_list = islice(\n            self.methods[\"process_spider_exception\"], start_index, None\n        )\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            method = cast(Callable, method)\n            result = method(response=response, exception=exception, spider=spider)\n            if _isiterable(result):\n                # stop exception handling by handing control over to the\n                # process_spider_output chain if an iterable has been returned\n                dfd: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n                    self._process_spider_output(\n                        response, spider, result, method_index + 1\n                    )\n                )\n                # _process_spider_output() returns a Deferred only because of downgrading so this can be\n                # simplified when downgrading is removed.\n                if dfd.called:\n                    # the result is available immediately if _process_spider_output didn't do downgrading\n                    return cast(\n                        Union[MutableChain[_T], MutableAsyncChain[_T]], dfd.result\n                    )\n                # we forbid waiting here because otherwise we would need to return a deferred from\n                # _process_spider_exception too, which complicates the architecture\n                msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n                raise _InvalidOutput(msg)\n            elif result is None:\n                continue\n            else:\n                msg = (\n                    f\"{method.__qualname__} must return None \"\n                    f\"or an iterable, got {type(result)}\"\n                )\n                raise _InvalidOutput(msg)\n        return _failure\n\n    # This method cannot be made async def, as _process_spider_exception relies on the Deferred result\n    # being available immediately which doesn't work when it's a wrapped coroutine.\n    # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.\n    @inlineCallbacks\n    def _process_spider_output(\n        self,\n        response: Response,\n        spider: Spider,\n        result: Union[Iterable[_T], AsyncIterable[_T]],\n        start_index: int = 0,\n    ) -> Generator[Deferred[Any], Any, Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n        # items in this iterable do not need to go through the process_spider_output\n        # chain, they went through it already from the process_spider_exception method\n        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n        last_result_is_async = isinstance(result, AsyncIterable)\n        if last_result_is_async:\n            recovered = MutableAsyncChain()\n        else:\n            recovered = MutableChain()\n\n        # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.\n        # 1. def foo. Sync iterables are passed as is, async ones are downgraded.\n        # 2. async def foo. Sync iterables are upgraded, async ones are passed as is.\n        # 3. def foo + async def foo_async. Iterables are passed to the respective method.\n        # Storing methods and method tuples in the same list is weird but we should be able to roll this back\n        # when we drop this compatibility feature.\n\n        method_list = islice(self.methods[\"process_spider_output\"], start_index, None)\n        for method_index, method_pair in enumerate(method_list, start=start_index):\n            if method_pair is None:\n                continue\n            need_upgrade = need_downgrade = False\n            if isinstance(method_pair, tuple):\n                # This tuple handling is only needed until _async compatibility methods are removed.\n                method_sync, method_async = method_pair\n                method = method_async if last_result_is_async else method_sync\n            else:\n                method = method_pair\n                if not last_result_is_async and isasyncgenfunction(method):\n                    need_upgrade = True\n                elif last_result_is_async and not isasyncgenfunction(method):\n                    need_downgrade = True\n            try:\n                if need_upgrade:\n                    # Iterable -> AsyncIterable\n                    result = as_async_generator(result)\n                elif need_downgrade:\n                    if not self.downgrade_warning_done:\n                        logger.warning(\n                            f\"Async iterable passed to {method.__qualname__} \"\n                            f\"was downgraded to a non-async one\"\n                        )\n                        self.downgrade_warning_done = True\n                    assert isinstance(result, AsyncIterable)\n                    # AsyncIterable -> Iterable\n                    result = yield deferred_from_coro(collect_asyncgen(result))\n                    if isinstance(recovered, AsyncIterable):\n                        recovered_collected = yield deferred_from_coro(\n                            collect_asyncgen(recovered)\n                        )\n                        recovered = MutableChain(recovered_collected)\n                # might fail directly if the output value is not a generator\n                result = method(response=response, result=result, spider=spider)\n            except Exception as ex:\n                exception_result: Union[\n                    Failure, MutableChain[_T], MutableAsyncChain[_T]\n                ] = self._process_spider_exception(\n                    response, spider, Failure(ex), method_index + 1\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                return exception_result\n            if _isiterable(result):\n                result = self._evaluate_iterable(\n                    response, spider, result, method_index + 1, recovered\n                )\n            else:\n                if iscoroutine(result):\n                    result.close()  # Silence warning about not awaiting\n                    msg = (\n                        f\"{method.__qualname__} must be an asynchronous \"\n                        f\"generator (i.e. use yield)\"\n                    )\n                else:\n                    msg = (\n                        f\"{method.__qualname__} must return an iterable, got \"\n                        f\"{type(result)}\"\n                    )\n                raise _InvalidOutput(msg)\n            last_result_is_async = isinstance(result, AsyncIterable)\n\n        if last_result_is_async:\n            return MutableAsyncChain(result, recovered)\n        return MutableChain(result, recovered)  # type: ignore[arg-type]\n\n    async def _process_callback_output(\n        self,\n        response: Response,\n        spider: Spider,\n        result: Union[Iterable[_T], AsyncIterable[_T]],\n    ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n        if isinstance(result, AsyncIterable):\n            recovered = MutableAsyncChain()\n        else:\n            recovered = MutableChain()\n        result = self._evaluate_iterable(response, spider, result, 0, recovered)\n        result = await maybe_deferred_to_future(\n            self._process_spider_output(response, spider, result)\n        )\n        if isinstance(result, AsyncIterable):\n            return MutableAsyncChain(result, recovered)\n        if isinstance(recovered, AsyncIterable):\n            recovered_collected = await collect_asyncgen(recovered)\n            recovered = MutableChain(recovered_collected)\n        return MutableChain(result, recovered)\n\n    def scrape_response(\n        self,\n        scrape_func: ScrapeFunc,\n        response: Response,\n        request: Request,\n        spider: Spider,\n    ) -> Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n        async def process_callback_output(\n            result: Union[Iterable[_T], AsyncIterable[_T]]\n        ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n            return await self._process_callback_output(response, spider, result)\n\n        def process_spider_exception(\n            _failure: Failure,\n        ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n            return self._process_spider_exception(response, spider, _failure)\n\n        dfd: Deferred[Union[Iterable[_T], AsyncIterable[_T]]] = mustbe_deferred(\n            self._process_spider_input, scrape_func, response, request, spider\n        )\n        dfd2: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n            dfd.addCallback(deferred_f_from_coro_f(process_callback_output))\n        )\n        dfd2.addErrback(process_spider_exception)\n        return dfd2\n\n    def process_start_requests(\n        self, start_requests: Iterable[Request], spider: Spider\n    ) -> Deferred:\n        return self._process_chain(\"process_start_requests\", start_requests, spider)\n\n    # This method is only needed until _async compatibility methods are removed.\n    @staticmethod\n    def _get_async_method_pair(\n        mw: Any, methodname: str\n    ) -> Union[None, Callable, Tuple[Callable, Callable]]:\n        normal_method: Optional[Callable] = getattr(mw, methodname, None)\n        methodname_async = methodname + \"_async\"\n        async_method: Optional[Callable] = getattr(mw, methodname_async, None)\n        if not async_method:\n            return normal_method\n        if not normal_method:\n            logger.error(\n                f\"Middleware {mw.__qualname__} has {methodname_async} \"\n                f\"without {methodname}, skipping this method.\"\n            )\n            return None\n        if not isasyncgenfunction(async_method):\n            logger.error(\n                f\"{async_method.__qualname__} is not \"\n                f\"an async generator function, skipping this method.\"\n            )\n            return normal_method\n        if isasyncgenfunction(normal_method):\n            logger.error(\n                f\"{normal_method.__qualname__} is an async \"\n                f\"generator function while {methodname_async} exists, \"\n                f\"skipping both methods.\"\n            )\n            return None\n        return normal_method, async_method\n", "scrapy/core/scheduler.py": "from __future__ import annotations\n\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, List, Optional, Type, cast\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.dupefilters import BaseDupeFilter\nfrom scrapy.http.request import Request\nfrom scrapy.pqueues import ScrapyPriorityQueue\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    # requires queuelib >= 1.6.2\n    from queuelib.queue import BaseQueue\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseSchedulerMeta(type):\n    \"\"\"\n    Metaclass to check scheduler classes against the necessary interface\n    \"\"\"\n\n    def __instancecheck__(cls, instance: Any) -> bool:\n        return cls.__subclasscheck__(type(instance))\n\n    def __subclasscheck__(cls, subclass: type) -> bool:\n        return (\n            hasattr(subclass, \"has_pending_requests\")\n            and callable(subclass.has_pending_requests)\n            and hasattr(subclass, \"enqueue_request\")\n            and callable(subclass.enqueue_request)\n            and hasattr(subclass, \"next_request\")\n            and callable(subclass.next_request)\n        )\n\n\nclass BaseScheduler(metaclass=BaseSchedulerMeta):\n    \"\"\"\n    The scheduler component is responsible for storing requests received from\n    the engine, and feeding them back upon request (also to the engine).\n\n    The original sources of said requests are:\n\n    * Spider: ``start_requests`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n    * Spider middleware: ``process_spider_output`` and ``process_spider_exception`` methods\n    * Downloader middleware: ``process_request``, ``process_response`` and ``process_exception`` methods\n\n    The order in which the scheduler returns its stored requests (via the ``next_request`` method)\n    plays a great part in determining the order in which those requests are downloaded.\n\n    The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.\n        \"\"\"\n        return cls()\n\n    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n        \"\"\"\n        Called when the spider is opened by the engine. It receives the spider\n        instance as argument and it's useful to execute initialization code.\n\n        :param spider: the spider object for the current crawl\n        :type spider: :class:`~scrapy.spiders.Spider`\n        \"\"\"\n        pass\n\n    def close(self, reason: str) -> Optional[Deferred[None]]:\n        \"\"\"\n        Called when the spider is closed by the engine. It receives the reason why the crawl\n        finished as argument and it's useful to execute cleaning code.\n\n        :param reason: a string which describes the reason why the spider was closed\n        :type reason: :class:`str`\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def has_pending_requests(self) -> bool:\n        \"\"\"\n        ``True`` if the scheduler has enqueued requests, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Process a request received by the engine.\n\n        Return ``True`` if the request is stored correctly, ``False`` otherwise.\n\n        If ``False``, the engine will fire a ``request_dropped`` signal, and\n        will not make further attempts to schedule the request at a later time.\n        For reference, the default Scrapy scheduler returns ``False`` when the\n        request is rejected by the dupefilter.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def next_request(self) -> Optional[Request]:\n        \"\"\"\n        Return the next :class:`~scrapy.http.Request` to be processed, or ``None``\n        to indicate that there are no requests to be considered ready at the moment.\n\n        Returning ``None`` implies that no request from the scheduler will be sent\n        to the downloader in the current reactor cycle. The engine will continue\n        calling ``next_request`` until ``has_pending_requests`` is ``False``.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass Scheduler(BaseScheduler):\n    \"\"\"\n    Default Scrapy scheduler. This implementation also handles duplication\n    filtering via the :setting:`dupefilter <DUPEFILTER_CLASS>`.\n\n    This scheduler stores requests into several priority queues (defined by the\n    :setting:`SCHEDULER_PRIORITY_QUEUE` setting). In turn, said priority queues\n    are backed by either memory or disk based queues (respectively defined by the\n    :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` settings).\n\n    Request prioritization is almost entirely delegated to the priority queue. The only\n    prioritization performed by this scheduler is using the disk-based queue if present\n    (i.e. if the :setting:`JOBDIR` setting is defined) and falling back to the memory-based\n    queue if a serialization error occurs. If the disk queue is not present, the memory one\n    is used directly.\n\n    :param dupefilter: An object responsible for checking and filtering duplicate requests.\n                       The value for the :setting:`DUPEFILTER_CLASS` setting is used by default.\n    :type dupefilter: :class:`scrapy.dupefilters.BaseDupeFilter` instance or similar:\n                      any class that implements the `BaseDupeFilter` interface\n\n    :param jobdir: The path of a directory to be used for persisting the crawl's state.\n                   The value for the :setting:`JOBDIR` setting is used by default.\n                   See :ref:`topics-jobs`.\n    :type jobdir: :class:`str` or ``None``\n\n    :param dqclass: A class to be used as persistent request queue.\n                    The value for the :setting:`SCHEDULER_DISK_QUEUE` setting is used by default.\n    :type dqclass: class\n\n    :param mqclass: A class to be used as non-persistent request queue.\n                    The value for the :setting:`SCHEDULER_MEMORY_QUEUE` setting is used by default.\n    :type mqclass: class\n\n    :param logunser: A boolean that indicates whether or not unserializable requests should be logged.\n                     The value for the :setting:`SCHEDULER_DEBUG` setting is used by default.\n    :type logunser: bool\n\n    :param stats: A stats collector object to record stats about the request scheduling process.\n                  The value for the :setting:`STATS_CLASS` setting is used by default.\n    :type stats: :class:`scrapy.statscollectors.StatsCollector` instance or similar:\n                 any class that implements the `StatsCollector` interface\n\n    :param pqclass: A class to be used as priority queue for requests.\n                    The value for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting is used by default.\n    :type pqclass: class\n\n    :param crawler: The crawler object corresponding to the current crawl.\n    :type crawler: :class:`scrapy.crawler.Crawler`\n    \"\"\"\n\n    def __init__(\n        self,\n        dupefilter: BaseDupeFilter,\n        jobdir: Optional[str] = None,\n        dqclass: Optional[Type[BaseQueue]] = None,\n        mqclass: Optional[Type[BaseQueue]] = None,\n        logunser: bool = False,\n        stats: Optional[StatsCollector] = None,\n        pqclass: Optional[Type[ScrapyPriorityQueue]] = None,\n        crawler: Optional[Crawler] = None,\n    ):\n        self.df: BaseDupeFilter = dupefilter\n        self.dqdir: Optional[str] = self._dqdir(jobdir)\n        self.pqclass: Optional[Type[ScrapyPriorityQueue]] = pqclass\n        self.dqclass: Optional[Type[BaseQueue]] = dqclass\n        self.mqclass: Optional[Type[BaseQueue]] = mqclass\n        self.logunser: bool = logunser\n        self.stats: Optional[StatsCollector] = stats\n        self.crawler: Optional[Crawler] = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method, initializes the scheduler with arguments taken from the crawl settings\n        \"\"\"\n        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n        return cls(\n            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n            jobdir=job_dir(crawler.settings),\n            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n            stats=crawler.stats,\n            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n            crawler=crawler,\n        )\n\n    def has_pending_requests(self) -> bool:\n        return len(self) > 0\n\n    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n        \"\"\"\n        (1) initialize the memory queue\n        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n        (3) return the result of the dupefilter's ``open`` method\n        \"\"\"\n        self.spider: Spider = spider\n        self.mqs: ScrapyPriorityQueue = self._mq()\n        self.dqs: Optional[ScrapyPriorityQueue] = self._dq() if self.dqdir else None\n        return self.df.open()\n\n    def close(self, reason: str) -> Optional[Deferred[None]]:\n        \"\"\"\n        (1) dump pending requests to disk if there is a disk queue\n        (2) return the result of the dupefilter's ``close`` method\n        \"\"\"\n        if self.dqs is not None:\n            state = self.dqs.close()\n            assert isinstance(self.dqdir, str)\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)\n\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Unless the received request is filtered out by the Dupefilter, attempt to push\n        it into the disk queue, falling back to pushing it into the memory queue.\n\n        Increment the appropriate stats, such as: ``scheduler/enqueued``,\n        ``scheduler/enqueued/disk``, ``scheduler/enqueued/memory``.\n\n        Return ``True`` if the request was stored successfully, ``False`` otherwise.\n        \"\"\"\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        assert self.stats is not None\n        if dqok:\n            self.stats.inc_value(\"scheduler/enqueued/disk\", spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value(\"scheduler/enqueued/memory\", spider=self.spider)\n        self.stats.inc_value(\"scheduler/enqueued\", spider=self.spider)\n        return True\n\n    def next_request(self) -> Optional[Request]:\n        \"\"\"\n        Return a :class:`~scrapy.http.Request` object from the memory queue,\n        falling back to the disk queue if the memory queue is empty.\n        Return ``None`` if there are no more enqueued requests.\n\n        Increment the appropriate stats, such as: ``scheduler/dequeued``,\n        ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n        \"\"\"\n        request: Optional[Request] = self.mqs.pop()\n        assert self.stats is not None\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued/memory\", spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request is not None:\n                self.stats.inc_value(\"scheduler/dequeued/disk\", spider=self.spider)\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued\", spider=self.spider)\n        return request\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the total amount of enqueued requests\n        \"\"\"\n        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)\n\n    def _dqpush(self, request: Request) -> bool:\n        if self.dqs is None:\n            return False\n        try:\n            self.dqs.push(request)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\n                    \"Unable to serialize request: %(request)s - reason:\"\n                    \" %(reason)s - no more unserializable requests will be\"\n                    \" logged (stats being collected)\"\n                )\n                logger.warning(\n                    msg,\n                    {\"request\": request, \"reason\": e},\n                    exc_info=True,\n                    extra={\"spider\": self.spider},\n                )\n                self.logunser = False\n            assert self.stats is not None\n            self.stats.inc_value(\"scheduler/unserializable\", spider=self.spider)\n            return False\n        else:\n            return True\n\n    def _mqpush(self, request: Request) -> None:\n        self.mqs.push(request)\n\n    def _dqpop(self) -> Optional[Request]:\n        if self.dqs is not None:\n            return self.dqs.pop()\n        return None\n\n    def _mq(self) -> ScrapyPriorityQueue:\n        \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n        assert self.crawler\n        assert self.pqclass\n        return build_from_crawler(\n            self.pqclass,\n            self.crawler,\n            downstream_queue_cls=self.mqclass,\n            key=\"\",\n        )\n\n    def _dq(self) -> ScrapyPriorityQueue:\n        \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n        assert self.crawler\n        assert self.dqdir\n        assert self.pqclass\n        state = self._read_dqs_state(self.dqdir)\n        q = build_from_crawler(\n            self.pqclass,\n            self.crawler,\n            downstream_queue_cls=self.dqclass,\n            key=self.dqdir,\n            startprios=state,\n        )\n        if q:\n            logger.info(\n                \"Resuming crawl (%(queuesize)d requests scheduled)\",\n                {\"queuesize\": len(q)},\n                extra={\"spider\": self.spider},\n            )\n        return q\n\n    def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n        \"\"\"Return a folder name to keep disk queue state at\"\"\"\n        if jobdir:\n            dqdir = Path(jobdir, \"requests.queue\")\n            if not dqdir.exists():\n                dqdir.mkdir(parents=True)\n            return str(dqdir)\n        return None\n\n    def _read_dqs_state(self, dqdir: str) -> List[int]:\n        path = Path(dqdir, \"active.json\")\n        if not path.exists():\n            return []\n        with path.open(encoding=\"utf-8\") as f:\n            return cast(List[int], json.load(f))\n\n    def _write_dqs_state(self, dqdir: str, state: List[int]) -> None:\n        with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(state, f)\n", "scrapy/core/__init__.py": "\"\"\"\nScrapy core library classes and functions.\n\"\"\"\n", "scrapy/core/downloader/contextfactory.py": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, List, Optional\n\nfrom OpenSSL import SSL\nfrom twisted.internet._sslverify import _setAcceptableProtocols\nfrom twisted.internet.ssl import (\n    AcceptableCiphers,\n    CertificateOptions,\n    optionsForClientTLS,\n    platformTrust,\n)\nfrom twisted.web.client import BrowserLikePolicyForHTTPS\nfrom twisted.web.iweb import IPolicyForHTTPS\nfrom zope.interface.declarations import implementer\nfrom zope.interface.verify import verifyObject\n\nfrom scrapy.core.downloader.tls import (\n    DEFAULT_CIPHERS,\n    ScrapyClientTLSOptions,\n    openssl_methods,\n)\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    from twisted.internet._sslverify import ClientTLSOptions\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n@implementer(IPolicyForHTTPS)\nclass ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n    \"\"\"\n    Non-peer-certificate verifying HTTPS context factory\n\n    Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)\n    which allows TLS protocol negotiation\n\n    'A TLS/SSL connection established with [this method] may\n     understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n    \"\"\"\n\n    def __init__(\n        self,\n        method: int = SSL.SSLv23_METHOD,\n        tls_verbose_logging: bool = False,\n        tls_ciphers: Optional[str] = None,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n        self._ssl_method: int = method\n        self.tls_verbose_logging: bool = tls_verbose_logging\n        self.tls_ciphers: AcceptableCiphers\n        if tls_ciphers:\n            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n        else:\n            self.tls_ciphers = DEFAULT_CIPHERS\n\n    @classmethod\n    def from_settings(\n        cls,\n        settings: BaseSettings,\n        method: int = SSL.SSLv23_METHOD,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Self:\n        tls_verbose_logging: bool = settings.getbool(\n            \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n        )\n        tls_ciphers: Optional[str] = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n        return cls(  # type: ignore[misc]\n            method=method,\n            tls_verbose_logging=tls_verbose_logging,\n            tls_ciphers=tls_ciphers,\n            *args,\n            **kwargs,\n        )\n\n    def getCertificateOptions(self) -> CertificateOptions:\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n\n        # backward-compatible SSL/TLS method:\n        #\n        # * this will respect `method` attribute in often recommended\n        #   `ScrapyClientContextFactory` subclass\n        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n        #\n        # * getattr() for `_ssl_method` attribute for context factories\n        #   not calling super().__init__\n        return CertificateOptions(\n            verify=False,\n            method=getattr(self, \"method\", getattr(self, \"_ssl_method\", None)),\n            fixBrokenPeers=True,\n            acceptableCiphers=self.tls_ciphers,\n        )\n\n    # kept for old-style HTTP/1.0 downloader context twisted calls,\n    # e.g. connectSSL()\n    def getContext(self, hostname: Any = None, port: Any = None) -> SSL.Context:\n        ctx: SSL.Context = self.getCertificateOptions().getContext()\n        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n        return ctx\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        return ScrapyClientTLSOptions(\n            hostname.decode(\"ascii\"),\n            self.getContext(),\n            verbose_logging=self.tls_verbose_logging,\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass BrowserLikeContextFactory(ScrapyClientContextFactory):\n    \"\"\"\n    Twisted-recommended context factory for web clients.\n\n    Quoting the documentation of the :class:`~twisted.web.client.Agent` class:\n\n        The default is to use a\n        :class:`~twisted.web.client.BrowserLikePolicyForHTTPS`, so unless you\n        have special requirements you can leave this as-is.\n\n    :meth:`creatorForNetloc` is the same as\n    :class:`~twisted.web.client.BrowserLikePolicyForHTTPS` except this context\n    factory allows setting the TLS/SSL method to use.\n\n    The default OpenSSL method is ``TLS_METHOD`` (also called\n    ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n    \"\"\"\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        # trustRoot set to platformTrust() will use the platform's root CAs.\n        #\n        # This means that a website like https://www.cacert.org will be rejected\n        # by default, since CAcert.org CA certificate is seldom shipped.\n        return optionsForClientTLS(\n            hostname=hostname.decode(\"ascii\"),\n            trustRoot=platformTrust(),\n            extraCertificateOptions={\"method\": self._ssl_method},\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass AcceptableProtocolsContextFactory:\n    \"\"\"Context factory to used to override the acceptable protocols\n    to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN\n    negotiation.\n    \"\"\"\n\n    def __init__(self, context_factory: Any, acceptable_protocols: List[bytes]):\n        verifyObject(IPolicyForHTTPS, context_factory)\n        self._wrapped_context_factory: Any = context_factory\n        self._acceptable_protocols: List[bytes] = acceptable_protocols\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        options: ClientTLSOptions = self._wrapped_context_factory.creatorForNetloc(\n            hostname, port\n        )\n        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n        return options\n\n\ndef load_context_factory_from_settings(\n    settings: BaseSettings, crawler: Crawler\n) -> IPolicyForHTTPS:\n    ssl_method = openssl_methods[settings.get(\"DOWNLOADER_CLIENT_TLS_METHOD\")]\n    context_factory_cls = load_object(settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"])\n    # try method-aware context factory\n    try:\n        context_factory = build_from_crawler(\n            context_factory_cls,\n            crawler,\n            method=ssl_method,\n        )\n    except TypeError:\n        # use context factory defaults\n        context_factory = build_from_crawler(\n            context_factory_cls,\n            crawler,\n        )\n        msg = (\n            f\"{settings['DOWNLOADER_CLIENTCONTEXTFACTORY']} does not accept \"\n            \"a `method` argument (type OpenSSL.SSL method, e.g. \"\n            \"OpenSSL.SSL.SSLv23_METHOD) and/or a `tls_verbose_logging` \"\n            \"argument and/or a `tls_ciphers` argument. Please, upgrade your \"\n            \"context factory class to handle them or ignore them.\"\n        )\n        warnings.warn(msg)\n\n    return context_factory\n", "scrapy/core/downloader/middleware.py": "\"\"\"\nDownloader Middleware manager\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Callable, Generator, List, Union, cast\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n\n\nclass DownloaderMiddlewareManager(MiddlewareManager):\n    component_name = \"downloader middleware\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"DOWNLOADER_MIDDLEWARES\"))\n\n    def _add_middleware(self, mw: Any) -> None:\n        if hasattr(mw, \"process_request\"):\n            self.methods[\"process_request\"].append(mw.process_request)\n        if hasattr(mw, \"process_response\"):\n            self.methods[\"process_response\"].appendleft(mw.process_response)\n        if hasattr(mw, \"process_exception\"):\n            self.methods[\"process_exception\"].appendleft(mw.process_exception)\n\n    def download(\n        self,\n        download_func: Callable[[Request, Spider], Deferred[Response]],\n        request: Request,\n        spider: Spider,\n    ) -> Deferred[Union[Response, Request]]:\n        @inlineCallbacks\n        def process_request(\n            request: Request,\n        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n            for method in self.methods[\"process_request\"]:\n                method = cast(Callable, method)\n                response = yield deferred_from_coro(\n                    method(request=request, spider=spider)\n                )\n                if response is not None and not isinstance(\n                    response, (Response, Request)\n                ):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            return (yield download_func(request, spider))\n\n        @inlineCallbacks\n        def process_response(\n            response: Union[Response, Request]\n        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            elif isinstance(response, Request):\n                return response\n\n            for method in self.methods[\"process_response\"]:\n                method = cast(Callable, method)\n                response = yield deferred_from_coro(\n                    method(request=request, response=response, spider=spider)\n                )\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response\n\n        @inlineCallbacks\n        def process_exception(\n            failure: Failure,\n        ) -> Generator[Deferred[Any], Any, Union[Failure, Response, Request]]:\n            exception = failure.value\n            for method in self.methods[\"process_exception\"]:\n                method = cast(Callable, method)\n                response = yield deferred_from_coro(\n                    method(request=request, exception=exception, spider=spider)\n                )\n                if response is not None and not isinstance(\n                    response, (Response, Request)\n                ):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            return failure\n\n        deferred: Deferred[Union[Response, Request]] = mustbe_deferred(\n            process_request, request\n        )\n        deferred.addErrback(process_exception)\n        deferred.addCallback(process_response)\n        return deferred\n", "scrapy/core/downloader/tls.py": "import logging\nfrom typing import Any, Dict\n\nfrom OpenSSL import SSL\nfrom service_identity.exceptions import CertificateError\nfrom twisted.internet._sslverify import (\n    ClientTLSOptions,\n    VerificationError,\n    verifyHostname,\n)\nfrom twisted.internet.ssl import AcceptableCiphers\n\nfrom scrapy.utils.ssl import get_temp_key_info, x509name_to_string\n\nlogger = logging.getLogger(__name__)\n\n\nMETHOD_TLS = \"TLS\"\nMETHOD_TLSv10 = \"TLSv1.0\"\nMETHOD_TLSv11 = \"TLSv1.1\"\nMETHOD_TLSv12 = \"TLSv1.2\"\n\n\nopenssl_methods: Dict[str, int] = {\n    METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n    METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only\n}\n\n\nclass ScrapyClientTLSOptions(ClientTLSOptions):\n    \"\"\"\n    SSL Client connection creator ignoring certificate verification errors\n    (for genuinely invalid certificates or bugs in verification code).\n\n    Same as Twisted's private _sslverify.ClientTLSOptions,\n    except that VerificationError, CertificateError and ValueError\n    exceptions are caught, so that the connection is not closed, only\n    logging warnings. Also, HTTPS connection parameters logging is added.\n    \"\"\"\n\n    def __init__(self, hostname: str, ctx: SSL.Context, verbose_logging: bool = False):\n        super().__init__(hostname, ctx)\n        self.verbose_logging: bool = verbose_logging\n\n    def _identityVerifyingInfoCallback(\n        self, connection: SSL.Connection, where: int, ret: Any\n    ) -> None:\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            connection.set_tlsext_host_name(self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                logger.debug(\n                    \"SSL connection to %s using protocol %s, cipher %s\",\n                    self._hostnameASCII,\n                    connection.get_protocol_version_name(),\n                    connection.get_cipher_name(),\n                )\n                server_cert = connection.get_peer_certificate()\n                if server_cert:\n                    logger.debug(\n                        'SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                        x509name_to_string(server_cert.get_issuer()),\n                        x509name_to_string(server_cert.get_subject()),\n                    )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug(\"SSL temp key: %s\", key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"%s\"; %s',\n                    self._hostnameASCII,\n                    e,\n                )\n\n            except ValueError as e:\n                logger.warning(\n                    \"Ignoring error while verifying certificate \"\n                    'from host \"%s\" (exception: %r)',\n                    self._hostnameASCII,\n                    e,\n                )\n\n\nDEFAULT_CIPHERS: AcceptableCiphers = AcceptableCiphers.fromOpenSSLCipherString(\n    \"DEFAULT\"\n)\n", "scrapy/core/downloader/__init__.py": "from __future__ import annotations\n\nimport random\nimport warnings\nfrom collections import deque\nfrom datetime import datetime\nfrom time import time\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Deque,\n    Dict,\n    Optional,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet import task\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader.handlers import DownloadHandlers\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response\nfrom scrapy.resolver import dnscache\nfrom scrapy.settings import BaseSettings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.utils.defer import mustbe_deferred\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n_T = TypeVar(\"_T\")\n\n\nclass Slot:\n    \"\"\"Downloader slot\"\"\"\n\n    def __init__(\n        self,\n        concurrency: int,\n        delay: float,\n        randomize_delay: bool,\n        *,\n        throttle: Optional[bool] = None,\n    ):\n        self.concurrency: int = concurrency\n        self.delay: float = delay\n        self.randomize_delay: bool = randomize_delay\n        self.throttle = throttle\n\n        self.active: Set[Request] = set()\n        self.queue: Deque[Tuple[Request, Deferred[Response]]] = deque()\n        self.transferring: Set[Request] = set()\n        self.lastseen: float = 0\n        self.latercall = None\n\n    def free_transfer_slots(self) -> int:\n        return self.concurrency - len(self.transferring)\n\n    def download_delay(self) -> float:\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)  # nosec\n        return self.delay\n\n    def close(self) -> None:\n        if self.latercall and self.latercall.active():\n            self.latercall.cancel()\n\n    def __repr__(self) -> str:\n        cls_name = self.__class__.__name__\n        return (\n            f\"{cls_name}(concurrency={self.concurrency!r}, \"\n            f\"delay={self.delay:.2f}, \"\n            f\"randomize_delay={self.randomize_delay!r}, \"\n            f\"throttle={self.throttle!r})\"\n        )\n\n    def __str__(self) -> str:\n        return (\n            f\"<downloader.Slot concurrency={self.concurrency!r} \"\n            f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n            f\"throttle={self.throttle!r} \"\n            f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n            f\"len(transferring)={len(self.transferring)} \"\n            f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n        )\n\n\ndef _get_concurrency_delay(\n    concurrency: int, spider: Spider, settings: BaseSettings\n) -> Tuple[int, float]:\n    delay: float = settings.getfloat(\"DOWNLOAD_DELAY\")\n    if hasattr(spider, \"download_delay\"):\n        delay = spider.download_delay\n\n    if hasattr(spider, \"max_concurrent_requests\"):\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay\n\n\nclass Downloader:\n    DOWNLOAD_SLOT = \"download_slot\"\n\n    def __init__(self, crawler: Crawler):\n        self.settings: BaseSettings = crawler.settings\n        self.signals: SignalManager = crawler.signals\n        self.slots: Dict[str, Slot] = {}\n        self.active: Set[Request] = set()\n        self.handlers: DownloadHandlers = DownloadHandlers(crawler)\n        self.total_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS\")\n        self.domain_concurrency: int = self.settings.getint(\n            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n        )\n        self.ip_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\")\n        self.randomize_delay: bool = self.settings.getbool(\"RANDOMIZE_DOWNLOAD_DELAY\")\n        self.middleware: DownloaderMiddlewareManager = (\n            DownloaderMiddlewareManager.from_crawler(crawler)\n        )\n        self._slot_gc_loop: task.LoopingCall = task.LoopingCall(self._slot_gc)\n        self._slot_gc_loop.start(60)\n        self.per_slot_settings: Dict[str, Dict[str, Any]] = self.settings.getdict(\n            \"DOWNLOAD_SLOTS\", {}\n        )\n\n    def fetch(\n        self, request: Request, spider: Spider\n    ) -> Deferred[Union[Response, Request]]:\n        def _deactivate(response: _T) -> _T:\n            self.active.remove(request)\n            return response\n\n        self.active.add(request)\n        dfd: Deferred[Union[Response, Request]] = self.middleware.download(\n            self._enqueue_request, request, spider\n        )\n        return dfd.addBoth(_deactivate)\n\n    def needs_backout(self) -> bool:\n        return len(self.active) >= self.total_concurrency\n\n    def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n        key = self.get_slot_key(request)\n        if key not in self.slots:\n            slot_settings = self.per_slot_settings.get(key, {})\n            conc = (\n                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            )\n            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n            conc, delay = (\n                slot_settings.get(\"concurrency\", conc),\n                slot_settings.get(\"delay\", delay),\n            )\n            randomize_delay = slot_settings.get(\"randomize_delay\", self.randomize_delay)\n            throttle = slot_settings.get(\"throttle\", None)\n            new_slot = Slot(conc, delay, randomize_delay, throttle=throttle)\n            self.slots[key] = new_slot\n\n        return key, self.slots[key]\n\n    def get_slot_key(self, request: Request) -> str:\n        if self.DOWNLOAD_SLOT in request.meta:\n            return cast(str, request.meta[self.DOWNLOAD_SLOT])\n\n        key = urlparse_cached(request).hostname or \"\"\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key\n\n    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n        warnings.warn(\n            \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.get_slot_key(request)\n\n    def _enqueue_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        key, slot = self._get_slot(request, spider)\n        request.meta[self.DOWNLOAD_SLOT] = key\n\n        def _deactivate(response: Response) -> Response:\n            slot.active.remove(request)\n            return response\n\n        slot.active.add(request)\n        self.signals.send_catch_log(\n            signal=signals.request_reached_downloader, request=request, spider=spider\n        )\n        deferred: Deferred[Response] = Deferred().addBoth(_deactivate)\n        slot.queue.append((request, deferred))\n        self._process_queue(spider, slot)\n        return deferred\n\n    def _process_queue(self, spider: Spider, slot: Slot) -> None:\n        from twisted.internet import reactor\n\n        if slot.latercall and slot.latercall.active():\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = reactor.callLater(\n                    penalty, self._process_queue, spider, slot\n                )\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, deferred = slot.queue.popleft()\n            dfd = self._download(slot, request, spider)\n            dfd.chainDeferred(deferred)\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(spider, slot)\n                break\n\n    def _download(\n        self, slot: Slot, request: Request, spider: Spider\n    ) -> Deferred[Response]:\n        # The order is very important for the following deferreds. Do not change!\n\n        # 1. Create the download deferred\n        dfd: Deferred[Response] = mustbe_deferred(\n            self.handlers.download_request, request, spider\n        )\n\n        # 2. Notify response_downloaded listeners about the recent download\n        # before querying queue for next request\n        def _downloaded(response: Response) -> Response:\n            self.signals.send_catch_log(\n                signal=signals.response_downloaded,\n                response=response,\n                request=request,\n                spider=spider,\n            )\n            return response\n\n        dfd.addCallback(_downloaded)\n\n        # 3. After response arrives, remove the request from transferring\n        # state to free up the transferring slot so it can be used by the\n        # following requests (perhaps those which came from the downloader\n        # middleware itself)\n        slot.transferring.add(request)\n\n        def finish_transferring(_: _T) -> _T:\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            self.signals.send_catch_log(\n                signal=signals.request_left_downloader, request=request, spider=spider\n            )\n            return _\n\n        return dfd.addBoth(finish_transferring)\n\n    def close(self) -> None:\n        self._slot_gc_loop.stop()\n        for slot in self.slots.values():\n            slot.close()\n\n    def _slot_gc(self, age: float = 60) -> None:\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()\n", "scrapy/core/downloader/webclient.py": "import re\nfrom time import time\nfrom typing import Optional, Tuple\nfrom urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n\nfrom twisted.internet import defer\nfrom twisted.internet.protocol import ClientFactory\nfrom twisted.web.http import HTTPClient\n\nfrom scrapy import Request\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes, to_unicode\n\n\ndef _parsed_url_args(parsed: ParseResult) -> Tuple[bytes, bytes, bytes, int, bytes]:\n    # Assume parsed is urlparse-d from Request.url,\n    # which was passed via safe_url_string and is ascii-only.\n    path_str = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n    path = to_bytes(path_str, encoding=\"ascii\")\n    assert parsed.hostname is not None\n    host = to_bytes(parsed.hostname, encoding=\"ascii\")\n    port = parsed.port\n    scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n    netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n    if port is None:\n        port = 443 if scheme == b\"https\" else 80\n    return scheme, netloc, host, port, path\n\n\ndef _parse(url: str) -> Tuple[bytes, bytes, bytes, int, bytes]:\n    \"\"\"Return tuple of (scheme, netloc, host, port, path),\n    all in bytes except for port which is int.\n    Assume url is from Request.url, which was passed via safe_url_string\n    and is ascii-only.\n    \"\"\"\n    url = url.strip()\n    if not re.match(r\"^\\w+://\", url):\n        url = \"//\" + url\n    parsed = urlparse(url)\n    return _parsed_url_args(parsed)\n\n\nclass ScrapyHTTPPageGetter(HTTPClient):\n    delimiter = b\"\\n\"\n\n    def connectionMade(self):\n        self.headers = Headers()  # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)\n\n    def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())\n\n    def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)\n\n    def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)\n\n    def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)\n\n    def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)\n\n    def handleResponse(self, response):\n        if self.factory.method.upper() == b\"HEAD\":\n            self.factory.page(b\"\")\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()\n\n    def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b\"https\"):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\n            defer.TimeoutError(\n                f\"Getting {self.factory.url} took longer \"\n                f\"than {self.factory.timeout} seconds.\"\n            )\n        )\n\n\n# This class used to inherit from Twisted\u2019s\n# twisted.web.client.HTTPClientFactory. When that class was deprecated in\n# Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n# non-overridden code into this class.\nclass ScrapyHTTPClientFactory(ClientFactory):\n    protocol = ScrapyHTTPPageGetter\n\n    waiting = 1\n    noisy = False\n    followRedirect = False\n    afterFoundGet = False\n\n    def _build_response(self, body, request):\n        request.meta[\"download_latency\"] = self.headers_time - self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)\n        return respcls(\n            url=self._url,\n            status=status,\n            headers=headers,\n            body=body,\n            protocol=to_unicode(self.version),\n        )\n\n    def _set_connection_attributes(self, request):\n        parsed = urlparse_cached(request)\n        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(\n            parsed\n        )\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n            self.path = self.url\n\n    def __init__(self, request: Request, timeout: float = 180):\n        self._url: str = urldefrag(request.url)[0]\n        # converting to bytes to comply to Twisted interface\n        self.url: bytes = to_bytes(self._url, encoding=\"ascii\")\n        self.method: bytes = to_bytes(request.method, encoding=\"ascii\")\n        self.body: Optional[bytes] = request.body or None\n        self.headers: Headers = Headers(request.headers)\n        self.response_headers: Optional[Headers] = None\n        self.timeout: float = request.meta.get(\"download_timeout\") or timeout\n        self.start_time: float = time()\n        self.deferred: defer.Deferred[Response] = defer.Deferred().addCallback(\n            self._build_response, request\n        )\n\n        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n        # to have _disconnectedDeferred. See Twisted r32329.\n        # As Scrapy implements it's own logic to handle redirects is not\n        # needed to add the callback _waitForDisconnect.\n        # Specifically this avoids the AttributeError exception when\n        # clientConnectionFailed method is called.\n        self._disconnectedDeferred: defer.Deferred[None] = defer.Deferred()\n\n        self._set_connection_attributes(request)\n\n        # set Host header based on url\n        self.headers.setdefault(\"Host\", self.netloc)\n\n        # set Content-Length based len of body\n        if self.body is not None:\n            self.headers[\"Content-Length\"] = len(self.body)\n            # just in case a broken http/1.1 decides to keep connection alive\n            self.headers.setdefault(\"Connection\", \"close\")\n        # Content-Length must be specified in POST method even with no body\n        elif self.method == b\"POST\":\n            self.headers[\"Content-Length\"] = 0\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self._url}>\"\n\n    def _cancelTimeout(self, result, timeoutCall):\n        if timeoutCall.active():\n            timeoutCall.cancel()\n        return result\n\n    def buildProtocol(self, addr):\n        p = ClientFactory.buildProtocol(self, addr)\n        p.followRedirect = self.followRedirect\n        p.afterFoundGet = self.afterFoundGet\n        if self.timeout:\n            from twisted.internet import reactor\n\n            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n        return p\n\n    def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers\n\n    def gotStatus(self, version, status, message):\n        \"\"\"\n        Set the status of the request on us.\n        @param version: The HTTP version.\n        @type version: L{bytes}\n        @param status: The HTTP status code, an integer represented as a\n        bytestring.\n        @type status: L{bytes}\n        @param message: The HTTP status message.\n        @type message: L{bytes}\n        \"\"\"\n        self.version, self.status, self.message = version, status, message\n\n    def page(self, page):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.callback(page)\n\n    def noPage(self, reason):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.errback(reason)\n\n    def clientConnectionFailed(self, _, reason):\n        \"\"\"\n        When a connection attempt fails, the request cannot be issued.  If no\n        result has yet been provided to the result Deferred, provide the\n        connection failure reason as an error result.\n        \"\"\"\n        if self.waiting:\n            self.waiting = 0\n            # If the connection attempt failed, there is nothing more to\n            # disconnect, so just fire that Deferred now.\n            self._disconnectedDeferred.callback(None)\n            self.deferred.errback(reason)\n", "scrapy/core/downloader/handlers/http.py": "from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\nfrom scrapy.core.downloader.handlers.http11 import (\n    HTTP11DownloadHandler as HTTPDownloadHandler,\n)\n", "scrapy/core/downloader/handlers/http2.py": "from __future__ import annotations\n\nfrom time import time\nfrom typing import TYPE_CHECKING, Optional\nfrom urllib.parse import urldefrag\n\nfrom twisted.internet.base import DelayedCall\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.error import TimeoutError\nfrom twisted.web.client import URI\nfrom twisted.web.iweb import IPolicyForHTTPS\n\nfrom scrapy.core.downloader.contextfactory import load_context_factory_from_settings\nfrom scrapy.core.downloader.webclient import _parse\nfrom scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass H2DownloadHandler:\n    def __init__(self, settings: Settings, crawler: Crawler):\n        self._crawler = crawler\n\n        from twisted.internet import reactor\n\n        self._pool = H2ConnectionPool(reactor, settings)\n        self._context_factory = load_context_factory_from_settings(settings, crawler)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        agent = ScrapyH2Agent(\n            context_factory=self._context_factory,\n            pool=self._pool,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request, spider)\n\n    def close(self) -> None:\n        self._pool.close_connections()\n\n\nclass ScrapyH2Agent:\n    _Agent = H2Agent\n    _ProxyAgent = ScrapyProxyH2Agent\n\n    def __init__(\n        self,\n        context_factory: IPolicyForHTTPS,\n        pool: H2ConnectionPool,\n        connect_timeout: int = 10,\n        bind_address: Optional[bytes] = None,\n        crawler: Optional[Crawler] = None,\n    ) -> None:\n        self._context_factory = context_factory\n        self._connect_timeout = connect_timeout\n        self._bind_address = bind_address\n        self._pool = pool\n        self._crawler = crawler\n\n    def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:\n        from twisted.internet import reactor\n\n        bind_address = request.meta.get(\"bindaddress\") or self._bind_address\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n            scheme = _parse(request.url)[0]\n\n            if scheme == b\"https\":\n                # ToDo\n                raise NotImplementedError(\n                    \"Tunneling via CONNECT method using HTTP/2.0 is not yet supported\"\n                )\n            return self._ProxyAgent(\n                reactor=reactor,\n                context_factory=self._context_factory,\n                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding=\"ascii\")),\n                connect_timeout=timeout,\n                bind_address=bind_address,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            context_factory=self._context_factory,\n            connect_timeout=timeout,\n            bind_address=bind_address,\n            pool=self._pool,\n        )\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        timeout = request.meta.get(\"download_timeout\") or self._connect_timeout\n        agent = self._get_agent(request, timeout)\n\n        start_time = time()\n        d = agent.request(request, spider)\n        d.addCallback(self._cb_latency, request, start_time)\n\n        timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, timeout, timeout_cl)\n        return d\n\n    @staticmethod\n    def _cb_latency(\n        response: Response, request: Request, start_time: float\n    ) -> Response:\n        request.meta[\"download_latency\"] = time() - start_time\n        return response\n\n    @staticmethod\n    def _cb_timeout(\n        response: Response, request: Request, timeout: float, timeout_cl: DelayedCall\n    ) -> Response:\n        if timeout_cl.active():\n            timeout_cl.cancel()\n            return response\n\n        url = urldefrag(request.url)[0]\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n", "scrapy/core/downloader/handlers/ftp.py": "\"\"\"\nAn asynchronous FTP file download handler for scrapy which somehow emulates an http response.\n\nFTP connection parameters are passed using the request meta field:\n- ftp_user (required)\n- ftp_password (required)\n- ftp_passive (by default, enabled) sets FTP connection passive mode\n- ftp_local_filename\n        - If not given, file data will come in the response.body, as a normal scrapy Response,\n        which will imply that the entire file will be on memory.\n        - if given, file data will be saved in a local file with the given name\n        This helps when downloading very big files to avoid memory issues. In addition, for\n        convenience the local file name will also be given in the response body.\n\nThe status of the built html response will be, by default\n- 200 in case of success\n- 404 in case specified file was not found in the server (ftp code 550)\n\nor raise corresponding ftp exception otherwise\n\nThe matching from server ftp command return codes to html response codes is defined in the\nCODE_MAPPING attribute of the handler class. The key 'default' is used for any code\nthat is not explicitly present among the map keys. You may need to overwrite this\nmapping if want a different behaviour than default.\n\nIn case of status 200 request, response.headers will come with two keys:\n    'Local Filename' - with the value of the local filename if given\n    'Size' - with size of the downloaded data\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, Any, BinaryIO, Dict, Optional\nfrom urllib.parse import unquote\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.protocol import ClientCreator, Protocol\nfrom twisted.protocols.ftp import CommandFailed, FTPClient\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass ReceivedDataProtocol(Protocol):\n    def __init__(self, filename: Optional[str] = None):\n        self.__filename: Optional[str] = filename\n        self.body: BinaryIO = open(filename, \"wb\") if filename else BytesIO()\n        self.size: int = 0\n\n    def dataReceived(self, data: bytes) -> None:\n        self.body.write(data)\n        self.size += len(data)\n\n    @property\n    def filename(self) -> Optional[str]:\n        return self.__filename\n\n    def close(self) -> None:\n        self.body.close() if self.filename else self.body.seek(0)\n\n\n_CODE_RE = re.compile(r\"\\d+\")\n\n\nclass FTPDownloadHandler:\n    lazy = False\n\n    CODE_MAPPING: Dict[str, int] = {\n        \"550\": 404,\n        \"default\": 503,\n    }\n\n    def __init__(self, settings: BaseSettings):\n        self.default_user = settings[\"FTP_USER\"]\n        self.default_password = settings[\"FTP_PASSWORD\"]\n        self.passive_mode = settings[\"FTP_PASSIVE_MODE\"]\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        parsed_url = urlparse_cached(request)\n        user = request.meta.get(\"ftp_user\", self.default_user)\n        password = request.meta.get(\"ftp_password\", self.default_password)\n        passive_mode = (\n            1 if bool(request.meta.get(\"ftp_passive\", self.passive_mode)) else 0\n        )\n        creator = ClientCreator(\n            reactor, FTPClient, user, password, passive=passive_mode\n        )\n        dfd: Deferred[FTPClient] = creator.connectTCP(\n            parsed_url.hostname, parsed_url.port or 21\n        )\n        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n\n    def gotClient(\n        self, client: FTPClient, request: Request, filepath: str\n    ) -> Deferred[Response]:\n        self.client = client\n        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n        d = client.retrieveFile(filepath, protocol)\n        d.addCallback(self._build_response, request, protocol)\n        d.addErrback(self._failed, request)\n        return d\n\n    def _build_response(\n        self, result: Any, request: Request, protocol: ReceivedDataProtocol\n    ) -> Response:\n        self.result = result\n        protocol.close()\n        headers = {\"local filename\": protocol.filename or \"\", \"size\": protocol.size}\n        body = to_bytes(protocol.filename or protocol.body.read())\n        respcls = responsetypes.from_args(url=request.url, body=body)\n        # hints for Headers-related types may need to be fixed to not use AnyStr\n        return respcls(url=request.url, status=200, body=body, headers=headers)  # type: ignore[arg-type]\n\n    def _failed(self, result: Failure, request: Request) -> Response:\n        message = result.getErrorMessage()\n        if result.type == CommandFailed:\n            m = _CODE_RE.search(message)\n            if m:\n                ftpcode = m.group()\n                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                return Response(\n                    url=request.url, status=httpcode, body=to_bytes(message)\n                )\n        assert result.type\n        raise result.type(result.value)\n", "scrapy/core/downloader/handlers/s3.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Optional, Type\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider\nfrom scrapy.core.downloader.handlers.http import HTTPDownloadHandler\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import build_from_crawler\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass S3DownloadHandler:\n    def __init__(\n        self,\n        settings: BaseSettings,\n        *,\n        crawler: Crawler,\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n        aws_session_token: Optional[str] = None,\n        httpdownloadhandler: Type[HTTPDownloadHandler] = HTTPDownloadHandler,\n        **kw: Any,\n    ):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n\n        if not aws_access_key_id:\n            aws_access_key_id = settings[\"AWS_ACCESS_KEY_ID\"]\n        if not aws_secret_access_key:\n            aws_secret_access_key = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        if not aws_session_token:\n            aws_session_token = settings[\"AWS_SESSION_TOKEN\"]\n\n        # If no credentials could be found anywhere,\n        # consider this an anonymous connection request by default;\n        # unless 'anon' was set explicitly (True/False).\n        anon = kw.get(\"anon\")\n        if anon is None and not aws_access_key_id and not aws_secret_access_key:\n            kw[\"anon\"] = True\n        self.anon = kw.get(\"anon\")\n\n        self._signer = None\n        import botocore.auth\n        import botocore.credentials\n\n        kw.pop(\"anon\", None)\n        if kw:\n            raise TypeError(f\"Unexpected keyword arguments: {kw}\")\n        if not self.anon:\n            assert aws_access_key_id is not None\n            assert aws_secret_access_key is not None\n            SignerCls = botocore.auth.AUTH_TYPE_MAPS[\"s3\"]\n            # botocore.auth.BaseSigner doesn't have an __init__() with args, only subclasses do\n            self._signer = SignerCls(  # type: ignore[call-arg]\n                botocore.credentials.Credentials(\n                    aws_access_key_id, aws_secret_access_key, aws_session_token\n                )\n            )\n\n        _http_handler = build_from_crawler(\n            httpdownloadhandler,\n            crawler,\n        )\n        self._download_http = _http_handler.download_request\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, **kwargs: Any) -> Self:\n        return cls(crawler.settings, crawler=crawler, **kwargs)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        p = urlparse_cached(request)\n        scheme = \"https\" if request.meta.get(\"is_secure\") else \"http\"\n        bucket = p.hostname\n        path = p.path + \"?\" + p.query if p.query else p.path\n        url = f\"{scheme}://{bucket}.s3.amazonaws.com{path}\"\n        if self.anon:\n            request = request.replace(url=url)\n        else:\n            import botocore.awsrequest\n\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url=f\"{scheme}://s3.amazonaws.com/{bucket}{path}\",\n                headers=request.headers.to_unicode_dict(),\n                data=request.body,\n            )\n            assert self._signer\n            self._signer.add_auth(awsrequest)\n            request = request.replace(url=url, headers=awsrequest.headers.items())\n        return self._download_http(request, spider)\n", "scrapy/core/downloader/handlers/http10.py": "\"\"\"Download handlers for http and https schemes\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Type\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n    from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n\n\nclass HTTP10DownloadHandler:\n    lazy = False\n\n    def __init__(self, settings: BaseSettings, crawler: Crawler):\n        self.HTTPClientFactory: Type[ScrapyHTTPClientFactory] = load_object(\n            settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"]\n        )\n        self.ClientContextFactory: Type[ScrapyClientContextFactory] = load_object(\n            settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n        )\n        self._settings: BaseSettings = settings\n        self._crawler: Crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred\n\n    def _connect(self, factory: ScrapyHTTPClientFactory) -> Deferred:\n        from twisted.internet import reactor\n\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b\"https\":\n            client_context_factory = build_from_crawler(\n                self.ClientContextFactory,\n                self._crawler,\n            )\n            return reactor.connectSSL(host, port, factory, client_context_factory)\n        return reactor.connectTCP(host, port, factory)\n", "scrapy/core/downloader/handlers/file.py": "from pathlib import Path\n\nfrom w3lib.url import file_uri_to_path\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.decorators import defers\n\n\nclass FileDownloadHandler:\n    lazy = False\n\n    @defers\n    def download_request(self, request: Request, spider: Spider) -> Response:\n        filepath = file_uri_to_path(request.url)\n        body = Path(filepath).read_bytes()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)\n", "scrapy/core/downloader/handlers/__init__.py": "\"\"\"Download handlers for different schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generator,\n    Optional,\n    Protocol,\n    Type,\n    Union,\n    cast,\n)\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import NotConfigured, NotSupported\nfrom scrapy.http import Response\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadHandlerProtocol(Protocol):\n    def download_request(\n        self, request: Request, spider: Spider\n    ) -> Deferred[Response]: ...\n\n\nclass DownloadHandlers:\n    def __init__(self, crawler: Crawler):\n        self._crawler: Crawler = crawler\n        self._schemes: Dict[str, Union[str, Callable[..., Any]]] = (\n            {}\n        )  # stores acceptable schemes on instancing\n        self._handlers: Dict[str, DownloadHandlerProtocol] = (\n            {}\n        )  # stores instanced handlers for schemes\n        self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n        handlers: Dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n            cast(\n                Dict[str, Union[str, Callable[..., Any]]],\n                crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n            )\n        )\n        for scheme, clspath in handlers.items():\n            self._schemes[scheme] = clspath\n            self._load_handler(scheme, skip_lazy=True)\n\n        crawler.signals.connect(self._close, signals.engine_stopped)\n\n    def _get_handler(self, scheme: str) -> Optional[DownloadHandlerProtocol]:\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = \"no handler available for that scheme\"\n            return None\n\n        return self._load_handler(scheme)\n\n    def _load_handler(\n        self, scheme: str, skip_lazy: bool = False\n    ) -> Optional[DownloadHandlerProtocol]:\n        path = self._schemes[scheme]\n        try:\n            dhcls: Type[DownloadHandlerProtocol] = load_object(path)\n            if skip_lazy and getattr(dhcls, \"lazy\", True):\n                return None\n            dh = build_from_crawler(\n                dhcls,\n                self._crawler,\n            )\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error(\n                'Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                {\"clspath\": path, \"scheme\": scheme},\n                exc_info=True,\n                extra={\"crawler\": self._crawler},\n            )\n            self._notconfigured[scheme] = str(ex)\n            return None\n        else:\n            self._handlers[scheme] = dh\n            return dh\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(\n                f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\"\n            )\n        return handler.download_request(request, spider)\n\n    @defer.inlineCallbacks\n    def _close(self, *_a: Any, **_kw: Any) -> Generator[Deferred[Any], Any, None]:\n        for dh in self._handlers.values():\n            if hasattr(dh, \"close\"):\n                yield dh.close()\n", "scrapy/core/downloader/handlers/http11.py": "\"\"\"Download handlers for http and https schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport ipaddress\nimport logging\nimport re\nfrom contextlib import suppress\nfrom io import BytesIO\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, List, Optional, Tuple, TypedDict, TypeVar, Union\nfrom urllib.parse import urldefrag, urlunparse\n\nfrom twisted.internet import ssl\nfrom twisted.internet.base import ReactorBase\nfrom twisted.internet.defer import CancelledError, Deferred, succeed\nfrom twisted.internet.endpoints import TCP4ClientEndpoint\nfrom twisted.internet.error import TimeoutError\nfrom twisted.internet.interfaces import IConsumer\nfrom twisted.internet.protocol import Factory, Protocol, connectionDone\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import URI, Agent, HTTPConnectionPool\nfrom twisted.web.client import Response as TxResponse\nfrom twisted.web.client import ResponseDone, ResponseFailed\nfrom twisted.web.http import PotentialDataLoss, _DataLoss\nfrom twisted.web.http_headers import Headers as TxHeaders\nfrom twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS\nfrom zope.interface import implementer\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader.contextfactory import load_context_factory_from_settings\nfrom scrapy.core.downloader.webclient import _parse\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    # typing.NotRequired and typing.Self require Python 3.11\n    from typing_extensions import NotRequired, Self\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass _ResultT(TypedDict):\n    txresponse: TxResponse\n    body: bytes\n    flags: Optional[List[str]]\n    certificate: Optional[ssl.Certificate]\n    ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None]\n    failure: NotRequired[Optional[Failure]]\n\n\nclass HTTP11DownloadHandler:\n    lazy = False\n\n    def __init__(self, settings: BaseSettings, crawler: Crawler):\n        self._crawler = crawler\n\n        from twisted.internet import reactor\n\n        self._pool: HTTPConnectionPool = HTTPConnectionPool(reactor, persistent=True)\n        self._pool.maxPersistentPerHost = settings.getint(\n            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n        )\n        self._pool._factory.noisy = False\n\n        self._contextFactory: IPolicyForHTTPS = load_context_factory_from_settings(\n            settings, crawler\n        )\n        self._default_maxsize: int = settings.getint(\"DOWNLOAD_MAXSIZE\")\n        self._default_warnsize: int = settings.getint(\"DOWNLOAD_WARNSIZE\")\n        self._fail_on_dataloss: bool = settings.getbool(\"DOWNLOAD_FAIL_ON_DATALOSS\")\n        self._disconnect_timeout: int = 1\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(\n            contextFactory=self._contextFactory,\n            pool=self._pool,\n            maxsize=getattr(spider, \"download_maxsize\", self._default_maxsize),\n            warnsize=getattr(spider, \"download_warnsize\", self._default_warnsize),\n            fail_on_dataloss=self._fail_on_dataloss,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request)\n\n    def close(self) -> Deferred[None]:\n        from twisted.internet import reactor\n\n        d: Deferred[None] = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result: _T) -> _T:\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d\n\n\nclass TunnelError(Exception):\n    \"\"\"An HTTP CONNECT tunnel could not be established by the proxy.\"\"\"\n\n\nclass TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n    \"\"\"An endpoint that tunnels through proxies to allow HTTPS downloads. To\n    accomplish that, this endpoint sends an HTTP CONNECT to the proxy.\n    The HTTP CONNECT is always sent when using this endpoint, I think this could\n    be improved as the CONNECT will be redundant if the connection associated\n    with this endpoint comes from the pool and a CONNECT has already been issued\n    for it.\n    \"\"\"\n\n    _truncatedLength = 1000\n    _responseAnswer = (\n        r\"HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,\" + str(_truncatedLength) + r\"})\"\n    )\n    _responseMatcher = re.compile(_responseAnswer.encode())\n\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        host: str,\n        port: int,\n        proxyConf: Tuple[str, int, Optional[bytes]],\n        contextFactory: IPolicyForHTTPS,\n        timeout: float = 30,\n        bindAddress: Optional[Tuple[str, int]] = None,\n    ):\n        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n        super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n        self._tunnelReadyDeferred: Deferred[Protocol] = Deferred()\n        self._tunneledHost: str = host\n        self._tunneledPort: int = port\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n        self._connectBuffer: bytearray = bytearray()\n\n    def requestTunnel(self, protocol: Protocol) -> Protocol:\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        assert protocol.transport\n        tunnelReq = tunnel_request_data(\n            self._tunneledHost, self._tunneledPort, self._proxyAuthHeader\n        )\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse  # type: ignore[method-assign]\n        self._protocol = protocol\n        return protocol\n\n    def processProxyResponse(self, data: bytes) -> None:\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        assert self._protocol.transport\n        self._connectBuffer += data\n        # make sure that enough (all) bytes are consumed\n        # and that we've got all HTTP headers (ending with a blank line)\n        # from the proxy so that we don't send those bytes to the TLS layer\n        #\n        # see https://github.com/scrapy/scrapy/issues/2491\n        if b\"\\r\\n\\r\\n\" not in self._connectBuffer:\n            return\n        self._protocol.dataReceived = self._protocolDataReceived  # type: ignore[method-assign]\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n        if respm and int(respm.group(\"status\")) == 200:\n            # set proper Server Name Indication extension\n            sslOptions = self._contextFactory.creatorForNetloc(  # type: ignore[call-arg,misc]\n                self._tunneledHost, self._tunneledPort\n            )\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            extra: Any\n            if respm:\n                extra = {\n                    \"status\": int(respm.group(\"status\")),\n                    \"reason\": respm.group(\"reason\").strip(),\n                }\n            else:\n                extra = data[: self._truncatedLength]\n            self._tunnelReadyDeferred.errback(\n                TunnelError(\n                    \"Could not open CONNECT tunnel with proxy \"\n                    f\"{self._host}:{self._port} [{extra!r}]\"\n                )\n            )\n\n    def connectFailed(self, reason: Failure) -> None:\n        \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n        self._tunnelReadyDeferred.errback(reason)\n\n    def connect(self, protocolFactory: Factory) -> Deferred[Protocol]:\n        self._protocolFactory = protocolFactory\n        connectDeferred = super().connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred\n\n\ndef tunnel_request_data(\n    host: str, port: int, proxy_auth_header: Optional[bytes] = None\n) -> bytes:\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_unicode as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding=\"ascii\") + b\":\" + to_bytes(str(port))\n    tunnel_req = b\"CONNECT \" + host_value + b\" HTTP/1.1\\r\\n\"\n    tunnel_req += b\"Host: \" + host_value + b\"\\r\\n\"\n    if proxy_auth_header:\n        tunnel_req += b\"Proxy-Authorization: \" + proxy_auth_header + b\"\\r\\n\"\n    tunnel_req += b\"\\r\\n\"\n    return tunnel_req\n\n\nclass TunnelingAgent(Agent):\n    \"\"\"An agent that uses a L{TunnelingTCP4ClientEndpoint} to make HTTPS\n    downloads. It may look strange that we have chosen to subclass Agent and not\n    ProxyAgent but consider that after the tunnel is opened the proxy is\n    transparent to the client; thus the agent should behave like there is no\n    proxy involved.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        reactor: ReactorBase,\n        proxyConf: Tuple[str, int, Optional[bytes]],\n        contextFactory: IPolicyForHTTPS,\n        connectTimeout: Optional[float] = None,\n        bindAddress: Optional[bytes] = None,\n        pool: Optional[HTTPConnectionPool] = None,\n    ):\n        super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n        self._proxyConf: Tuple[str, int, Optional[bytes]] = proxyConf\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n\n    def _getEndpoint(self, uri: URI) -> TunnelingTCP4ClientEndpoint:\n        return TunnelingTCP4ClientEndpoint(\n            reactor=self._reactor,\n            host=uri.host,\n            port=uri.port,\n            proxyConf=self._proxyConf,\n            contextFactory=self._contextFactory,\n            timeout=self._endpointFactory._connectTimeout,\n            bindAddress=self._endpointFactory._bindAddress,\n        )\n\n    def _requestWithEndpoint(\n        self,\n        key: Any,\n        endpoint: TCP4ClientEndpoint,\n        method: bytes,\n        parsedURI: bytes,\n        headers: Optional[TxHeaders],\n        bodyProducer: Optional[IBodyProducer],\n        requestPath: bytes,\n    ) -> Deferred[TxResponse]:\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key += self._proxyConf\n        return super()._requestWithEndpoint(\n            key=key,\n            endpoint=endpoint,\n            method=method,\n            parsedURI=parsedURI,\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=requestPath,\n        )\n\n\nclass ScrapyProxyAgent(Agent):\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        proxyURI: bytes,\n        connectTimeout: Optional[float] = None,\n        bindAddress: Optional[bytes] = None,\n        pool: Optional[HTTPConnectionPool] = None,\n    ):\n        super().__init__(\n            reactor=reactor,\n            connectTimeout=connectTimeout,\n            bindAddress=bindAddress,\n            pool=pool,\n        )\n        self._proxyURI: URI = URI.fromBytes(proxyURI)\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[TxHeaders] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> Deferred[TxResponse]:\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        return self._requestWithEndpoint(\n            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n            endpoint=self._getEndpoint(self._proxyURI),\n            method=method,\n            parsedURI=URI.fromBytes(uri),\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=uri,\n        )\n\n\nclass ScrapyAgent:\n    _Agent = Agent\n    _ProxyAgent = ScrapyProxyAgent\n    _TunnelingAgent = TunnelingAgent\n\n    def __init__(\n        self,\n        *,\n        contextFactory: IPolicyForHTTPS,\n        connectTimeout: float = 10,\n        bindAddress: Optional[bytes] = None,\n        pool: Optional[HTTPConnectionPool] = None,\n        maxsize: int = 0,\n        warnsize: int = 0,\n        fail_on_dataloss: bool = True,\n        crawler: Crawler,\n    ):\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n        self._connectTimeout: float = connectTimeout\n        self._bindAddress: Optional[bytes] = bindAddress\n        self._pool: Optional[HTTPConnectionPool] = pool\n        self._maxsize: int = maxsize\n        self._warnsize: int = warnsize\n        self._fail_on_dataloss: bool = fail_on_dataloss\n        self._txresponse: Optional[TxResponse] = None\n        self._crawler: Crawler = crawler\n\n    def _get_agent(self, request: Request, timeout: float) -> Agent:\n        from twisted.internet import reactor\n\n        bindaddress = request.meta.get(\"bindaddress\") or self._bindAddress\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost_str = to_unicode(proxyHost)\n            if scheme == b\"https\":\n                proxyAuth = request.headers.get(b\"Proxy-Authorization\", None)\n                proxyConf = (proxyHost_str, proxyPort, proxyAuth)\n                return self._TunnelingAgent(\n                    reactor=reactor,\n                    proxyConf=proxyConf,\n                    contextFactory=self._contextFactory,\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n            proxyScheme = proxyScheme or b\"http\"\n            proxyURI = urlunparse(\n                (proxyScheme, proxyNetloc, proxyParams, b\"\", b\"\", b\"\")\n            )\n            return self._ProxyAgent(\n                reactor=reactor,\n                proxyURI=to_bytes(proxyURI, encoding=\"ascii\"),\n                connectTimeout=timeout,\n                bindAddress=bindaddress,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            contextFactory=self._contextFactory,\n            connectTimeout=timeout,\n            bindAddress=bindaddress,\n            pool=self._pool,\n        )\n\n    def download_request(self, request: Request) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        timeout = request.meta.get(\"download_timeout\") or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b\"Proxy-Authorization\")\n        if request.body:\n            bodyproducer = _RequestBodyProducer(request.body)\n        else:\n            bodyproducer = None\n        start_time = time()\n        d: Deferred[TxResponse] = agent.request(\n            method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n        )\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d2: Deferred[_ResultT] = d.addCallback(self._cb_bodyready, request)\n        d3: Deferred[Response] = d2.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d3.cancel)\n        d3.addBoth(self._cb_timeout, request, url, timeout)\n        return d3\n\n    def _cb_timeout(self, result: _T, request: Request, url: str, timeout: float) -> _T:\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n\n    def _cb_latency(self, result: _T, request: Request, start_time: float) -> _T:\n        request.meta[\"download_latency\"] = time() - start_time\n        return result\n\n    @staticmethod\n    def _headers_from_twisted_response(response: TxResponse) -> Headers:\n        headers = Headers()\n        if response.length != UNKNOWN_LENGTH:\n            headers[b\"Content-Length\"] = str(response.length).encode()\n        headers.update(response.headers.getAllRawHeaders())\n        return headers\n\n    def _cb_bodyready(\n        self, txresponse: TxResponse, request: Request\n    ) -> Union[_ResultT, Deferred[_ResultT]]:\n        headers_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.headers_received,\n            headers=self._headers_from_twisted_response(txresponse),\n            body_length=txresponse.length,\n            request=request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in headers_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\n                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n                    {\"request\": request, \"handler\": handler.__qualname__},\n                )\n                txresponse._transport.stopProducing()\n                txresponse._transport.loseConnection()\n                return {\n                    \"txresponse\": txresponse,\n                    \"body\": b\"\",\n                    \"flags\": [\"download_stopped\"],\n                    \"certificate\": None,\n                    \"ip_address\": None,\n                    \"failure\": result if result.value.fail else None,\n                }\n\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return {\n                \"txresponse\": txresponse,\n                \"body\": b\"\",\n                \"flags\": None,\n                \"certificate\": None,\n                \"ip_address\": None,\n            }\n\n        maxsize = request.meta.get(\"download_maxsize\", self._maxsize)\n        warnsize = request.meta.get(\"download_warnsize\", self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n        fail_on_dataloss = request.meta.get(\n            \"download_fail_on_dataloss\", self._fail_on_dataloss\n        )\n\n        if maxsize and expected_size > maxsize:\n            warning_msg = (\n                \"Cancelling download of %(url)s: expected response \"\n                \"size (%(size)s) larger than download max size (%(maxsize)s).\"\n            )\n            warning_args = {\n                \"url\": request.url,\n                \"size\": expected_size,\n                \"maxsize\": maxsize,\n            }\n\n            logger.warning(warning_msg, warning_args)\n\n            txresponse._transport.loseConnection()\n            raise CancelledError(warning_msg % warning_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\n                \"Expected response size (%(size)s) larger than \"\n                \"download warn size (%(warnsize)s) in request %(request)s.\",\n                {\"size\": expected_size, \"warnsize\": warnsize, \"request\": request},\n            )\n\n        def _cancel(_: Any) -> None:\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()\n\n        d: Deferred[_ResultT] = Deferred(_cancel)\n        txresponse.deliverBody(\n            _ResponseReader(\n                finished=d,\n                txresponse=txresponse,\n                request=request,\n                maxsize=maxsize,\n                warnsize=warnsize,\n                fail_on_dataloss=fail_on_dataloss,\n                crawler=self._crawler,\n            )\n        )\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d\n\n    def _cb_bodydone(\n        self, result: _ResultT, request: Request, url: str\n    ) -> Union[Response, Failure]:\n        headers = self._headers_from_twisted_response(result[\"txresponse\"])\n        respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n        try:\n            version = result[\"txresponse\"].version\n            protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n        except (AttributeError, TypeError, IndexError):\n            protocol = None\n        response = respcls(\n            url=url,\n            status=int(result[\"txresponse\"].code),\n            headers=headers,\n            body=result[\"body\"],\n            flags=result[\"flags\"],\n            certificate=result[\"certificate\"],\n            ip_address=result[\"ip_address\"],\n            protocol=protocol,\n        )\n        if result.get(\"failure\"):\n            assert result[\"failure\"]\n            result[\"failure\"].value.response = response\n            return result[\"failure\"]\n        return response\n\n\n@implementer(IBodyProducer)\nclass _RequestBodyProducer:\n    def __init__(self, body: bytes):\n        self.body = body\n        self.length = len(body)\n\n    def startProducing(self, consumer: IConsumer) -> Deferred[None]:\n        consumer.write(self.body)\n        return succeed(None)\n\n    def pauseProducing(self) -> None:\n        pass\n\n    def stopProducing(self) -> None:\n        pass\n\n\nclass _ResponseReader(Protocol):\n    def __init__(\n        self,\n        finished: Deferred[_ResultT],\n        txresponse: TxResponse,\n        request: Request,\n        maxsize: int,\n        warnsize: int,\n        fail_on_dataloss: bool,\n        crawler: Crawler,\n    ):\n        self._finished: Deferred[_ResultT] = finished\n        self._txresponse: TxResponse = txresponse\n        self._request: Request = request\n        self._bodybuf: BytesIO = BytesIO()\n        self._maxsize: int = maxsize\n        self._warnsize: int = warnsize\n        self._fail_on_dataloss: bool = fail_on_dataloss\n        self._fail_on_dataloss_warned: bool = False\n        self._reached_warnsize: bool = False\n        self._bytes_received: int = 0\n        self._certificate: Optional[ssl.Certificate] = None\n        self._ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None] = (\n            None\n        )\n        self._crawler: Crawler = crawler\n\n    def _finish_response(\n        self, flags: Optional[List[str]] = None, failure: Optional[Failure] = None\n    ) -> None:\n        self._finished.callback(\n            {\n                \"txresponse\": self._txresponse,\n                \"body\": self._bodybuf.getvalue(),\n                \"flags\": flags,\n                \"certificate\": self._certificate,\n                \"ip_address\": self._ip_address,\n                \"failure\": failure,\n            }\n        )\n\n    def connectionMade(self) -> None:\n        assert self.transport\n        if self._certificate is None:\n            with suppress(AttributeError):\n                self._certificate = ssl.Certificate(\n                    self.transport._producer.getPeerCertificate()\n                )\n\n        if self._ip_address is None:\n            self._ip_address = ipaddress.ip_address(\n                self.transport._producer.getPeer().host\n            )\n\n    def dataReceived(self, bodyBytes: bytes) -> None:\n        # This maybe called several times after cancel was called with buffered data.\n        if self._finished.called:\n            return\n\n        assert self.transport\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        bytes_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.bytes_received,\n            data=bodyBytes,\n            request=self._request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in bytes_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\n                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n                    {\"request\": self._request, \"handler\": handler.__qualname__},\n                )\n                self.transport.stopProducing()\n                self.transport.loseConnection()\n                failure = result if result.value.fail else None\n                self._finish_response(flags=[\"download_stopped\"], failure=failure)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.warning(\n                \"Received (%(bytes)s) bytes larger than download \"\n                \"max size (%(maxsize)s) in request %(request)s.\",\n                {\n                    \"bytes\": self._bytes_received,\n                    \"maxsize\": self._maxsize,\n                    \"request\": self._request,\n                },\n            )\n            # Clear buffer earlier to avoid keeping data in memory for a long time.\n            self._bodybuf.truncate(0)\n            self._finished.cancel()\n\n        if (\n            self._warnsize\n            and self._bytes_received > self._warnsize\n            and not self._reached_warnsize\n        ):\n            self._reached_warnsize = True\n            logger.warning(\n                \"Received more bytes than download \"\n                \"warn size (%(warnsize)s) in request %(request)s.\",\n                {\"warnsize\": self._warnsize, \"request\": self._request},\n            )\n\n    def connectionLost(self, reason: Failure = connectionDone) -> None:\n        if self._finished.called:\n            return\n\n        if reason.check(ResponseDone):\n            self._finish_response()\n            return\n\n        if reason.check(PotentialDataLoss):\n            self._finish_response(flags=[\"partial\"])\n            return\n\n        if reason.check(ResponseFailed) and any(\n            r.check(_DataLoss) for r in reason.value.reasons\n        ):\n            if not self._fail_on_dataloss:\n                self._finish_response(flags=[\"dataloss\"])\n                return\n\n            if not self._fail_on_dataloss_warned:\n                logger.warning(\n                    \"Got data loss in %s. If you want to process broken \"\n                    \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                    \" -- This message won't be shown in further requests\",\n                    self._txresponse.request.absoluteURI.decode(),\n                )\n                self._fail_on_dataloss_warned = True\n\n        self._finished.errback(reason)\n", "scrapy/core/http2/protocol.py": "import ipaddress\nimport itertools\nimport logging\nfrom collections import deque\nfrom ipaddress import IPv4Address, IPv6Address\nfrom typing import Any, Deque, Dict, List, Optional, Union\n\nfrom h2.config import H2Configuration\nfrom h2.connection import H2Connection\nfrom h2.errors import ErrorCodes\nfrom h2.events import (\n    ConnectionTerminated,\n    DataReceived,\n    Event,\n    ResponseReceived,\n    SettingsAcknowledged,\n    StreamEnded,\n    StreamReset,\n    UnknownFrameReceived,\n    WindowUpdated,\n)\nfrom h2.exceptions import FrameTooLargeError, H2Error\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.error import TimeoutError\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHandshakeListener,\n    IProtocolNegotiationFactory,\n)\nfrom twisted.internet.protocol import Factory, Protocol, connectionDone\nfrom twisted.internet.ssl import Certificate\nfrom twisted.protocols.policies import TimeoutMixin\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import URI\nfrom zope.interface import implementer\n\nfrom scrapy.core.http2.stream import Stream, StreamCloseReason\nfrom scrapy.http import Request\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\n\nlogger = logging.getLogger(__name__)\n\n\nPROTOCOL_NAME = b\"h2\"\n\n\nclass InvalidNegotiatedProtocol(H2Error):\n    def __init__(self, negotiated_protocol: bytes) -> None:\n        self.negotiated_protocol = negotiated_protocol\n\n    def __str__(self) -> str:\n        return f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\"\n\n\nclass RemoteTerminatedConnection(H2Error):\n    def __init__(\n        self,\n        remote_ip_address: Optional[Union[IPv4Address, IPv6Address]],\n        event: ConnectionTerminated,\n    ) -> None:\n        self.remote_ip_address = remote_ip_address\n        self.terminate_event = event\n\n    def __str__(self) -> str:\n        return f\"Received GOAWAY frame from {self.remote_ip_address!r}\"\n\n\nclass MethodNotAllowed405(H2Error):\n    def __init__(\n        self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]\n    ) -> None:\n        self.remote_ip_address = remote_ip_address\n\n    def __str__(self) -> str:\n        return f\"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"\n\n\n@implementer(IHandshakeListener)\nclass H2ClientProtocol(Protocol, TimeoutMixin):\n    IDLE_TIMEOUT = 240\n\n    def __init__(\n        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n    ) -> None:\n        \"\"\"\n        Arguments:\n            uri -- URI of the base url to which HTTP/2 Connection will be made.\n                uri is used to verify that incoming client requests have correct\n                base URL.\n            settings -- Scrapy project settings\n            conn_lost_deferred -- Deferred fires with the reason: Failure to notify\n                that connection was lost\n        \"\"\"\n        self._conn_lost_deferred = conn_lost_deferred\n\n        config = H2Configuration(client_side=True, header_encoding=\"utf-8\")\n        self.conn = H2Connection(config=config)\n\n        # ID of the next request stream\n        # Following the convention - 'Streams initiated by a client MUST\n        # use odd-numbered stream identifiers' (RFC 7540 - Section 5.1.1)\n        self._stream_id_generator = itertools.count(start=1, step=2)\n\n        # Streams are stored in a dictionary keyed off their stream IDs\n        self.streams: Dict[int, Stream] = {}\n\n        # If requests are received before connection is made we keep\n        # all requests in a pool and send them as the connection is made\n        self._pending_request_stream_pool: Deque[Stream] = deque()\n\n        # Save an instance of errors raised which lead to losing the connection\n        # We pass these instances to the streams ResponseFailed() failure\n        self._conn_lost_errors: List[BaseException] = []\n\n        # Some meta data of this connection\n        # initialized when connection is successfully made\n        self.metadata: Dict[str, Any] = {\n            # Peer certificate instance\n            \"certificate\": None,\n            # Address of the server we are connected to which\n            # is updated when HTTP/2 connection is  made successfully\n            \"ip_address\": None,\n            # URI of the peer HTTP/2 connection is made\n            \"uri\": uri,\n            # Both ip_address and uri are used by the Stream before\n            # initiating the request to verify that the base address\n            # Variables taken from Project Settings\n            \"default_download_maxsize\": settings.getint(\"DOWNLOAD_MAXSIZE\"),\n            \"default_download_warnsize\": settings.getint(\"DOWNLOAD_WARNSIZE\"),\n            # Counter to keep track of opened streams. This counter\n            # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n            # streams are opened which leads to ProtocolError\n            # We use simple FIFO policy to handle pending requests\n            \"active_streams\": 0,\n            # Flag to keep track if settings were acknowledged by the remote\n            # This ensures that we have established a HTTP/2 connection\n            \"settings_acknowledged\": False,\n        }\n\n    @property\n    def h2_connected(self) -> bool:\n        \"\"\"Boolean to keep track of the connection status.\n        This is used while initiating pending streams to make sure\n        that we initiate stream only during active HTTP/2 Connection\n        \"\"\"\n        assert self.transport is not None  # typing\n        return bool(self.transport.connected) and self.metadata[\"settings_acknowledged\"]\n\n    @property\n    def allowed_max_concurrent_streams(self) -> int:\n        \"\"\"We keep total two streams for client (sending data) and\n        server side (receiving data) for a single request. To be safe\n        we choose the minimum. Since this value can change in event\n        RemoteSettingsChanged we make variable a property.\n        \"\"\"\n        return min(\n            self.conn.local_settings.max_concurrent_streams,\n            self.conn.remote_settings.max_concurrent_streams,\n        )\n\n    def _send_pending_requests(self) -> None:\n        \"\"\"Initiate all pending requests from the deque following FIFO\n        We make sure that at any time {allowed_max_concurrent_streams}\n        streams are active.\n        \"\"\"\n        while (\n            self._pending_request_stream_pool\n            and self.metadata[\"active_streams\"] < self.allowed_max_concurrent_streams\n            and self.h2_connected\n        ):\n            self.metadata[\"active_streams\"] += 1\n            stream = self._pending_request_stream_pool.popleft()\n            stream.initiate_request()\n            self._write_to_transport()\n\n    def pop_stream(self, stream_id: int) -> Stream:\n        \"\"\"Perform cleanup when a stream is closed\"\"\"\n        stream = self.streams.pop(stream_id)\n        self.metadata[\"active_streams\"] -= 1\n        self._send_pending_requests()\n        return stream\n\n    def _new_stream(self, request: Request, spider: Spider) -> Stream:\n        \"\"\"Instantiates a new Stream object\"\"\"\n        stream = Stream(\n            stream_id=next(self._stream_id_generator),\n            request=request,\n            protocol=self,\n            download_maxsize=getattr(\n                spider, \"download_maxsize\", self.metadata[\"default_download_maxsize\"]\n            ),\n            download_warnsize=getattr(\n                spider, \"download_warnsize\", self.metadata[\"default_download_warnsize\"]\n            ),\n        )\n        self.streams[stream.stream_id] = stream\n        return stream\n\n    def _write_to_transport(self) -> None:\n        \"\"\"Write data to the underlying transport connection\n        from the HTTP2 connection instance if any\n        \"\"\"\n        assert self.transport is not None  # typing\n        # Reset the idle timeout as connection is still actively sending data\n        self.resetTimeout()\n\n        data = self.conn.data_to_send()\n        self.transport.write(data)\n\n    def request(self, request: Request, spider: Spider) -> Deferred:\n        if not isinstance(request, Request):\n            raise TypeError(\n                f\"Expected scrapy.http.Request, received {request.__class__.__qualname__}\"\n            )\n\n        stream = self._new_stream(request, spider)\n        d = stream.get_response()\n\n        # Add the stream to the request pool\n        self._pending_request_stream_pool.append(stream)\n\n        # If we receive a request when connection is idle\n        # We need to initiate pending requests\n        self._send_pending_requests()\n        return d\n\n    def connectionMade(self) -> None:\n        \"\"\"Called by Twisted when the connection is established. We can start\n        sending some data now: we should open with the connection preamble.\n        \"\"\"\n        # Initialize the timeout\n        self.setTimeout(self.IDLE_TIMEOUT)\n\n        assert self.transport is not None  # typing\n        destination = self.transport.getPeer()\n        self.metadata[\"ip_address\"] = ipaddress.ip_address(destination.host)\n\n        # Initiate H2 Connection\n        self.conn.initiate_connection()\n        self._write_to_transport()\n\n    def _lose_connection_with_error(self, errors: List[BaseException]) -> None:\n        \"\"\"Helper function to lose the connection with the error sent as a\n        reason\"\"\"\n        self._conn_lost_errors += errors\n        assert self.transport is not None  # typing\n        self.transport.loseConnection()\n\n    def handshakeCompleted(self) -> None:\n        \"\"\"\n        Close the connection if it's not made via the expected protocol\n        \"\"\"\n        assert self.transport is not None  # typing\n        if (\n            self.transport.negotiatedProtocol is not None\n            and self.transport.negotiatedProtocol != PROTOCOL_NAME\n        ):\n            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n            self._lose_connection_with_error(\n                [InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)]\n            )\n\n    def _check_received_data(self, data: bytes) -> None:\n        \"\"\"Checks for edge cases where the connection to remote fails\n        without raising an appropriate H2Error\n\n        Arguments:\n            data -- Data received from the remote\n        \"\"\"\n        if data.startswith(b\"HTTP/2.0 405 Method Not Allowed\"):\n            raise MethodNotAllowed405(self.metadata[\"ip_address\"])\n\n    def dataReceived(self, data: bytes) -> None:\n        # Reset the idle timeout as connection is still actively receiving data\n        self.resetTimeout()\n\n        try:\n            self._check_received_data(data)\n            events = self.conn.receive_data(data)\n            self._handle_events(events)\n        except H2Error as e:\n            if isinstance(e, FrameTooLargeError):\n                # hyper-h2 does not drop the connection in this scenario, we\n                # need to abort the connection manually.\n                self._conn_lost_errors += [e]\n                assert self.transport is not None  # typing\n                self.transport.abortConnection()\n                return\n\n            # Save this error as ultimately the connection will be dropped\n            # internally by hyper-h2. Saved error will be passed to all the streams\n            # closed with the connection.\n            self._lose_connection_with_error([e])\n        finally:\n            self._write_to_transport()\n\n    def timeoutConnection(self) -> None:\n        \"\"\"Called when the connection times out.\n        We lose the connection with TimeoutError\"\"\"\n\n        # Check whether there are open streams. If there are, we're going to\n        # want to use the error code PROTOCOL_ERROR. If there aren't, use\n        # NO_ERROR.\n        if (\n            self.conn.open_outbound_streams > 0\n            or self.conn.open_inbound_streams > 0\n            or self.metadata[\"active_streams\"] > 0\n        ):\n            error_code = ErrorCodes.PROTOCOL_ERROR\n        else:\n            error_code = ErrorCodes.NO_ERROR\n        self.conn.close_connection(error_code=error_code)\n        self._write_to_transport()\n\n        self._lose_connection_with_error(\n            [TimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")]\n        )\n\n    def connectionLost(self, reason: Failure = connectionDone) -> None:\n        \"\"\"Called by Twisted when the transport connection is lost.\n        No need to write anything to transport here.\n        \"\"\"\n        # Cancel the timeout if not done yet\n        self.setTimeout(None)\n\n        # Notify the connection pool instance such that no new requests are\n        # sent over current connection\n        if not reason.check(connectionDone):\n            self._conn_lost_errors.append(reason)\n\n        self._conn_lost_deferred.callback(self._conn_lost_errors)\n\n        for stream in self.streams.values():\n            if stream.metadata[\"request_sent\"]:\n                close_reason = StreamCloseReason.CONNECTION_LOST\n            else:\n                close_reason = StreamCloseReason.INACTIVE\n            stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n\n        self.metadata[\"active_streams\"] -= len(self.streams)\n        self.streams.clear()\n        self._pending_request_stream_pool.clear()\n        self.conn.close_connection()\n\n    def _handle_events(self, events: List[Event]) -> None:\n        \"\"\"Private method which acts as a bridge between the events\n        received from the HTTP/2 data and IH2EventsHandler\n\n        Arguments:\n            events -- A list of events that the remote peer triggered by sending data\n        \"\"\"\n        for event in events:\n            if isinstance(event, ConnectionTerminated):\n                self.connection_terminated(event)\n            elif isinstance(event, DataReceived):\n                self.data_received(event)\n            elif isinstance(event, ResponseReceived):\n                self.response_received(event)\n            elif isinstance(event, StreamEnded):\n                self.stream_ended(event)\n            elif isinstance(event, StreamReset):\n                self.stream_reset(event)\n            elif isinstance(event, WindowUpdated):\n                self.window_updated(event)\n            elif isinstance(event, SettingsAcknowledged):\n                self.settings_acknowledged(event)\n            elif isinstance(event, UnknownFrameReceived):\n                logger.warning(\"Unknown frame received: %s\", event.frame)\n\n    # Event handler functions starts here\n    def connection_terminated(self, event: ConnectionTerminated) -> None:\n        self._lose_connection_with_error(\n            [RemoteTerminatedConnection(self.metadata[\"ip_address\"], event)]\n        )\n\n    def data_received(self, event: DataReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_data(event.data, event.flow_controlled_length)\n\n    def response_received(self, event: ResponseReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_headers(event.headers)\n\n    def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n        self.metadata[\"settings_acknowledged\"] = True\n\n        # Send off all the pending requests as now we have\n        # established a proper HTTP/2 connection\n        self._send_pending_requests()\n\n        # Update certificate when our HTTP/2 connection is established\n        assert self.transport is not None  # typing\n        self.metadata[\"certificate\"] = Certificate(self.transport.getPeerCertificate())\n\n    def stream_ended(self, event: StreamEnded) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.ENDED, from_protocol=True)\n\n    def stream_reset(self, event: StreamReset) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.RESET, from_protocol=True)\n\n    def window_updated(self, event: WindowUpdated) -> None:\n        if event.stream_id != 0:\n            self.streams[event.stream_id].receive_window_update()\n        else:\n            # Send leftover data for all the streams\n            for stream in self.streams.values():\n                stream.receive_window_update()\n\n\n@implementer(IProtocolNegotiationFactory)\nclass H2ClientFactory(Factory):\n    def __init__(\n        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n    ) -> None:\n        self.uri = uri\n        self.settings = settings\n        self.conn_lost_deferred = conn_lost_deferred\n\n    def buildProtocol(self, addr: IAddress) -> H2ClientProtocol:\n        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n\n    def acceptableProtocols(self) -> List[bytes]:\n        return [PROTOCOL_NAME]\n", "scrapy/core/http2/stream.py": "import logging\nfrom enum import Enum\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n\nfrom h2.errors import ErrorCodes\nfrom h2.exceptions import H2Error, ProtocolError, StreamClosedError\nfrom hpack import HeaderTuple\nfrom twisted.internet.defer import CancelledError, Deferred\nfrom twisted.internet.error import ConnectionClosed\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy.http import Request\nfrom scrapy.http.headers import Headers\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    from scrapy.core.http2.protocol import H2ClientProtocol\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass InactiveStreamClosed(ConnectionClosed):\n    \"\"\"Connection was closed without sending request headers\n    of the stream. This happens when a stream is waiting for other\n    streams to close and connection is lost.\"\"\"\n\n    def __init__(self, request: Request) -> None:\n        self.request = request\n\n    def __str__(self) -> str:\n        return f\"InactiveStreamClosed: Connection was closed without sending the request {self.request!r}\"\n\n\nclass InvalidHostname(H2Error):\n    def __init__(\n        self, request: Request, expected_hostname: str, expected_netloc: str\n    ) -> None:\n        self.request = request\n        self.expected_hostname = expected_hostname\n        self.expected_netloc = expected_netloc\n\n    def __str__(self) -> str:\n        return f\"InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}\"\n\n\nclass StreamCloseReason(Enum):\n    # Received a StreamEnded event from the remote\n    ENDED = 1\n\n    # Received a StreamReset event -- ended abruptly\n    RESET = 2\n\n    # Transport connection was lost\n    CONNECTION_LOST = 3\n\n    # Expected response body size is more than allowed limit\n    MAXSIZE_EXCEEDED = 4\n\n    # Response deferred is cancelled by the client\n    # (happens when client called response_deferred.cancel())\n    CANCELLED = 5\n\n    # Connection lost and the stream was not initiated\n    INACTIVE = 6\n\n    # The hostname of the request is not same as of connected peer hostname\n    # As a result sending this request will the end the connection\n    INVALID_HOSTNAME = 7\n\n\nclass Stream:\n    \"\"\"Represents a single HTTP/2 Stream.\n\n    Stream is a bidirectional flow of bytes within an established connection,\n    which may carry one or more messages. Handles the transfer of HTTP Headers\n    and Data frames.\n\n    Role of this class is to\n    1. Combine all the data frames\n    \"\"\"\n\n    def __init__(\n        self,\n        stream_id: int,\n        request: Request,\n        protocol: \"H2ClientProtocol\",\n        download_maxsize: int = 0,\n        download_warnsize: int = 0,\n    ) -> None:\n        \"\"\"\n        Arguments:\n            stream_id -- Unique identifier for the stream within a single HTTP/2 connection\n            request -- The HTTP request associated to the stream\n            protocol -- Parent H2ClientProtocol instance\n        \"\"\"\n        self.stream_id: int = stream_id\n        self._request: Request = request\n        self._protocol: \"H2ClientProtocol\" = protocol\n\n        self._download_maxsize = self._request.meta.get(\n            \"download_maxsize\", download_maxsize\n        )\n        self._download_warnsize = self._request.meta.get(\n            \"download_warnsize\", download_warnsize\n        )\n\n        # Metadata of an HTTP/2 connection stream\n        # initialized when stream is instantiated\n        self.metadata: Dict[str, Any] = {\n            \"request_content_length\": (\n                0 if self._request.body is None else len(self._request.body)\n            ),\n            # Flag to keep track whether the stream has initiated the request\n            \"request_sent\": False,\n            # Flag to track whether we have logged about exceeding download warnsize\n            \"reached_warnsize\": False,\n            # Each time we send a data frame, we will decrease value by the amount send.\n            \"remaining_content_length\": (\n                0 if self._request.body is None else len(self._request.body)\n            ),\n            # Flag to keep track whether client (self) have closed this stream\n            \"stream_closed_local\": False,\n            # Flag to keep track whether the server has closed the stream\n            \"stream_closed_server\": False,\n        }\n\n        # Private variable used to build the response\n        # this response is then converted to appropriate Response class\n        # passed to the response deferred callback\n        self._response: Dict[str, Any] = {\n            # Data received frame by frame from the server is appended\n            # and passed to the response Deferred when completely received.\n            \"body\": BytesIO(),\n            # The amount of data received that counts against the\n            # flow control window\n            \"flow_controlled_size\": 0,\n            # Headers received after sending the request\n            \"headers\": Headers({}),\n        }\n\n        def _cancel(_: Any) -> None:\n            # Close this stream as gracefully as possible\n            # If the associated request is initiated we reset this stream\n            # else we directly call close() method\n            if self.metadata[\"request_sent\"]:\n                self.reset_stream(StreamCloseReason.CANCELLED)\n            else:\n                self.close(StreamCloseReason.CANCELLED)\n\n        self._deferred_response: Deferred = Deferred(_cancel)\n\n    def __repr__(self) -> str:\n        return f\"Stream(id={self.stream_id!r})\"\n\n    @property\n    def _log_warnsize(self) -> bool:\n        \"\"\"Checks if we have received data which exceeds the download warnsize\n        and whether we have not already logged about it.\n\n        Returns:\n            True if both the above conditions hold true\n            False if any of the conditions is false\n        \"\"\"\n        content_length_header = int(\n            self._response[\"headers\"].get(b\"Content-Length\", -1)\n        )\n        return (\n            self._download_warnsize\n            and (\n                self._response[\"flow_controlled_size\"] > self._download_warnsize\n                or content_length_header > self._download_warnsize\n            )\n            and not self.metadata[\"reached_warnsize\"]\n        )\n\n    def get_response(self) -> Deferred:\n        \"\"\"Simply return a Deferred which fires when response\n        from the asynchronous request is available\n        \"\"\"\n        return self._deferred_response\n\n    def check_request_url(self) -> bool:\n        # Make sure that we are sending the request to the correct URL\n        url = urlparse_cached(self._request)\n        return (\n            url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n            or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n            or url.netloc\n            == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n        )\n\n    def _get_request_headers(self) -> List[Tuple[str, str]]:\n        url = urlparse_cached(self._request)\n\n        path = url.path\n        if url.query:\n            path += \"?\" + url.query\n\n        # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n        # URIs; \"http\" or \"https\" URIs that do not contain a path component\n        # MUST include a value of '/'. The exception to this rule is an\n        # OPTIONS request for an \"http\" or \"https\" URI that does not include\n        # a path component; these MUST include a \":path\" pseudo-header field\n        # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n        if not path:\n            path = \"*\" if self._request.method == \"OPTIONS\" else \"/\"\n\n        # Make sure pseudo-headers comes before all the other headers\n        headers = [\n            (\":method\", self._request.method),\n            (\":authority\", url.netloc),\n        ]\n\n        # The \":scheme\" and \":path\" pseudo-header fields MUST\n        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n        if self._request.method != \"CONNECT\":\n            headers += [\n                (\":scheme\", self._protocol.metadata[\"uri\"].scheme),\n                (\":path\", path),\n            ]\n\n        content_length = str(len(self._request.body))\n        headers.append((\"Content-Length\", content_length))\n\n        content_length_name = self._request.headers.normkey(b\"Content-Length\")\n        for name, values in self._request.headers.items():\n            for value_bytes in values:\n                value = str(value_bytes, \"utf-8\")\n                if name == content_length_name:\n                    if value != content_length:\n                        logger.warning(\n                            \"Ignoring bad Content-Length header %r of request %r, \"\n                            \"sending %r instead\",\n                            value,\n                            self._request,\n                            content_length,\n                        )\n                    continue\n                headers.append((str(name, \"utf-8\"), value))\n\n        return headers\n\n    def initiate_request(self) -> None:\n        if self.check_request_url():\n            headers = self._get_request_headers()\n            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n            self.metadata[\"request_sent\"] = True\n            self.send_data()\n        else:\n            # Close this stream calling the response errback\n            # Note that we have not sent any headers\n            self.close(StreamCloseReason.INVALID_HOSTNAME)\n\n    def send_data(self) -> None:\n        \"\"\"Called immediately after the headers are sent. Here we send all the\n        data as part of the request.\n\n        If the content length is 0 initially then we end the stream immediately and\n        wait for response data.\n\n        Warning: Only call this method when stream not closed from client side\n           and has initiated request already by sending HEADER frame. If not then\n           stream will raise ProtocolError (raise by h2 state machine).\n        \"\"\"\n        if self.metadata[\"stream_closed_local\"]:\n            raise StreamClosedError(self.stream_id)\n\n        # Firstly, check what the flow control window is for current stream.\n        window_size = self._protocol.conn.local_flow_control_window(\n            stream_id=self.stream_id\n        )\n\n        # Next, check what the maximum frame size is.\n        max_frame_size = self._protocol.conn.max_outbound_frame_size\n\n        # We will send no more than the window size or the remaining file size\n        # of data in this call, whichever is smaller.\n        bytes_to_send_size = min(window_size, self.metadata[\"remaining_content_length\"])\n\n        # We now need to send a number of data frames.\n        while bytes_to_send_size > 0:\n            chunk_size = min(bytes_to_send_size, max_frame_size)\n\n            data_chunk_start_id = (\n                self.metadata[\"request_content_length\"]\n                - self.metadata[\"remaining_content_length\"]\n            )\n            data_chunk = self._request.body[\n                data_chunk_start_id : data_chunk_start_id + chunk_size\n            ]\n\n            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n\n            bytes_to_send_size -= chunk_size\n            self.metadata[\"remaining_content_length\"] -= chunk_size\n\n        self.metadata[\"remaining_content_length\"] = max(\n            0, self.metadata[\"remaining_content_length\"]\n        )\n\n        # End the stream if no more data needs to be send\n        if self.metadata[\"remaining_content_length\"] == 0:\n            self._protocol.conn.end_stream(self.stream_id)\n\n        # Q. What about the rest of the data?\n        # Ans: Remaining Data frames will be sent when we get a WindowUpdate frame\n\n    def receive_window_update(self) -> None:\n        \"\"\"Flow control window size was changed.\n        Send data that earlier could not be sent as we were\n        blocked behind the flow control.\n        \"\"\"\n        if (\n            self.metadata[\"remaining_content_length\"]\n            and not self.metadata[\"stream_closed_server\"]\n            and self.metadata[\"request_sent\"]\n        ):\n            self.send_data()\n\n    def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n        self._response[\"body\"].write(data)\n        self._response[\"flow_controlled_size\"] += flow_controlled_length\n\n        # We check maxsize here in case the Content-Length header was not received\n        if (\n            self._download_maxsize\n            and self._response[\"flow_controlled_size\"] > self._download_maxsize\n        ):\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata[\"reached_warnsize\"] = True\n            warning_msg = (\n                f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n                f\"warn size ({self._download_warnsize}) in request {self._request}\"\n            )\n            logger.warning(warning_msg)\n\n        # Acknowledge the data received\n        self._protocol.conn.acknowledge_received_data(\n            self._response[\"flow_controlled_size\"], self.stream_id\n        )\n\n    def receive_headers(self, headers: List[HeaderTuple]) -> None:\n        for name, value in headers:\n            self._response[\"headers\"].appendlist(name, value)\n\n        # Check if we exceed the allowed max data size which can be received\n        expected_size = int(self._response[\"headers\"].get(b\"Content-Length\", -1))\n        if self._download_maxsize and expected_size > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata[\"reached_warnsize\"] = True\n            warning_msg = (\n                f\"Expected response size ({expected_size}) larger than \"\n                f\"download warn size ({self._download_warnsize}) in request {self._request}\"\n            )\n            logger.warning(warning_msg)\n\n    def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n        \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n        if self.metadata[\"stream_closed_local\"]:\n            raise StreamClosedError(self.stream_id)\n\n        # Clear buffer earlier to avoid keeping data in memory for a long time\n        self._response[\"body\"].truncate(0)\n\n        self.metadata[\"stream_closed_local\"] = True\n        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n        self.close(reason)\n\n    def close(\n        self,\n        reason: StreamCloseReason,\n        errors: Optional[List[BaseException]] = None,\n        from_protocol: bool = False,\n    ) -> None:\n        \"\"\"Based on the reason sent we will handle each case.\"\"\"\n        if self.metadata[\"stream_closed_server\"]:\n            raise StreamClosedError(self.stream_id)\n\n        if not isinstance(reason, StreamCloseReason):\n            raise TypeError(\n                f\"Expected StreamCloseReason, received {reason.__class__.__qualname__}\"\n            )\n\n        # Have default value of errors as an empty list as\n        # some cases can add a list of exceptions\n        errors = errors or []\n\n        if not from_protocol:\n            self._protocol.pop_stream(self.stream_id)\n\n        self.metadata[\"stream_closed_server\"] = True\n\n        # We do not check for Content-Length or Transfer-Encoding in response headers\n        # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n        # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)\n\n        # NOTE: Order of handling the events is important here\n        # As we immediately cancel the request when maxsize is exceeded while\n        # receiving DATA_FRAME's when we have received the headers (not\n        # having Content-Length)\n        if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n            expected_size = int(\n                self._response[\"headers\"].get(\n                    b\"Content-Length\", self._response[\"flow_controlled_size\"]\n                )\n            )\n            error_msg = (\n                f\"Cancelling download of {self._request.url}: received response \"\n                f\"size ({expected_size}) larger than download max size ({self._download_maxsize})\"\n            )\n            logger.error(error_msg)\n            self._deferred_response.errback(CancelledError(error_msg))\n\n        elif reason is StreamCloseReason.ENDED:\n            self._fire_response_deferred()\n\n        # Stream was abruptly ended here\n        elif reason is StreamCloseReason.CANCELLED:\n            # Client has cancelled the request. Remove all the data\n            # received and fire the response deferred with no flags set\n\n            # NOTE: The data is already flushed in Stream.reset_stream() called\n            # immediately when the stream needs to be cancelled\n\n            # There maybe no :status in headers, we make\n            # HTTP Status Code: 499 - Client Closed Request\n            self._response[\"headers\"][\":status\"] = \"499\"\n            self._fire_response_deferred()\n\n        elif reason is StreamCloseReason.RESET:\n            self._deferred_response.errback(\n                ResponseFailed(\n                    [\n                        Failure(\n                            f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n                            ProtocolError,\n                        )\n                    ]\n                )\n            )\n\n        elif reason is StreamCloseReason.CONNECTION_LOST:\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        elif reason is StreamCloseReason.INACTIVE:\n            errors.insert(0, InactiveStreamClosed(self._request))\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        else:\n            assert reason is StreamCloseReason.INVALID_HOSTNAME\n            self._deferred_response.errback(\n                InvalidHostname(\n                    self._request,\n                    str(self._protocol.metadata[\"uri\"].host, \"utf-8\"),\n                    f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}',\n                )\n            )\n\n    def _fire_response_deferred(self) -> None:\n        \"\"\"Builds response from the self._response dict\n        and fires the response deferred callback with the\n        generated response instance\"\"\"\n\n        body = self._response[\"body\"].getvalue()\n        response_cls = responsetypes.from_args(\n            headers=self._response[\"headers\"],\n            url=self._request.url,\n            body=body,\n        )\n\n        response = response_cls(\n            url=self._request.url,\n            status=int(self._response[\"headers\"][\":status\"]),\n            headers=self._response[\"headers\"],\n            body=body,\n            request=self._request,\n            certificate=self._protocol.metadata[\"certificate\"],\n            ip_address=self._protocol.metadata[\"ip_address\"],\n            protocol=\"h2\",\n        )\n\n        self._deferred_response.callback(response)\n", "scrapy/core/http2/agent.py": "from collections import deque\nfrom typing import Deque, Dict, List, Optional, Tuple\n\nfrom twisted.internet import defer\nfrom twisted.internet.base import ReactorBase\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.endpoints import HostnameEndpoint\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import (\n    URI,\n    BrowserLikePolicyForHTTPS,\n    ResponseFailed,\n    _StandardEndpointFactory,\n)\nfrom twisted.web.error import SchemeNotSupported\n\nfrom scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\nfrom scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\nfrom scrapy.http.request import Request\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\n\nConnectionKeyT = Tuple[bytes, bytes, int]\n\n\nclass H2ConnectionPool:\n    def __init__(self, reactor: ReactorBase, settings: Settings) -> None:\n        self._reactor = reactor\n        self.settings = settings\n\n        # Store a dictionary which is used to get the respective\n        # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)\n        self._connections: Dict[ConnectionKeyT, H2ClientProtocol] = {}\n\n        # Save all requests that arrive before the connection is established\n        self._pending_requests: Dict[ConnectionKeyT, Deque[Deferred]] = {}\n\n    def get_connection(\n        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n    ) -> Deferred:\n        if key in self._pending_requests:\n            # Received a request while connecting to remote\n            # Create a deferred which will fire with the H2ClientProtocol\n            # instance\n            d: Deferred = Deferred()\n            self._pending_requests[key].append(d)\n            return d\n\n        # Check if we already have a connection to the remote\n        conn = self._connections.get(key, None)\n        if conn:\n            # Return this connection instance wrapped inside a deferred\n            return defer.succeed(conn)\n\n        # No connection is established for the given URI\n        return self._new_connection(key, uri, endpoint)\n\n    def _new_connection(\n        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n    ) -> Deferred:\n        self._pending_requests[key] = deque()\n\n        conn_lost_deferred: Deferred = Deferred()\n        conn_lost_deferred.addCallback(self._remove_connection, key)\n\n        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n        conn_d = endpoint.connect(factory)\n        conn_d.addCallback(self.put_connection, key)\n\n        d: Deferred = Deferred()\n        self._pending_requests[key].append(d)\n        return d\n\n    def put_connection(\n        self, conn: H2ClientProtocol, key: ConnectionKeyT\n    ) -> H2ClientProtocol:\n        self._connections[key] = conn\n\n        # Now as we have established a proper HTTP/2 connection\n        # we fire all the deferred's with the connection instance\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.callback(conn)\n\n        return conn\n\n    def _remove_connection(\n        self, errors: List[BaseException], key: ConnectionKeyT\n    ) -> None:\n        self._connections.pop(key)\n\n        # Call the errback of all the pending requests for this connection\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.errback(ResponseFailed(errors))\n\n    def close_connections(self) -> None:\n        \"\"\"Close all the HTTP/2 connections and remove them from pool\n\n        Returns:\n            Deferred that fires when all connections have been closed\n        \"\"\"\n        for conn in self._connections.values():\n            assert conn.transport is not None  # typing\n            conn.transport.abortConnection()\n\n\nclass H2Agent:\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        pool: H2ConnectionPool,\n        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n        connect_timeout: Optional[float] = None,\n        bind_address: Optional[bytes] = None,\n    ) -> None:\n        self._reactor = reactor\n        self._pool = pool\n        self._context_factory = AcceptableProtocolsContextFactory(\n            context_factory, acceptable_protocols=[b\"h2\"]\n        )\n        self.endpoint_factory = _StandardEndpointFactory(\n            self._reactor, self._context_factory, connect_timeout, bind_address\n        )\n\n    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n        return self.endpoint_factory.endpointForURI(uri)\n\n    def get_key(self, uri: URI) -> ConnectionKeyT:\n        \"\"\"\n        Arguments:\n            uri - URI obtained directly from request URL\n        \"\"\"\n        return uri.scheme, uri.host, uri.port\n\n    def request(self, request: Request, spider: Spider) -> Deferred:\n        uri = URI.fromBytes(bytes(request.url, encoding=\"utf-8\"))\n        try:\n            endpoint = self.get_endpoint(uri)\n        except SchemeNotSupported:\n            return defer.fail(Failure())\n\n        key = self.get_key(uri)\n        d = self._pool.get_connection(key, uri, endpoint)\n        d.addCallback(lambda conn: conn.request(request, spider))\n        return d\n\n\nclass ScrapyProxyH2Agent(H2Agent):\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        proxy_uri: URI,\n        pool: H2ConnectionPool,\n        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n        connect_timeout: Optional[float] = None,\n        bind_address: Optional[bytes] = None,\n    ) -> None:\n        super().__init__(\n            reactor=reactor,\n            pool=pool,\n            context_factory=context_factory,\n            connect_timeout=connect_timeout,\n            bind_address=bind_address,\n        )\n        self._proxy_uri = proxy_uri\n\n    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n        return self.endpoint_factory.endpointForURI(self._proxy_uri)\n\n    def get_key(self, uri: URI) -> ConnectionKeyT:\n        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n        return b\"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n", "scrapy/core/http2/__init__.py": "", "scrapy/extensions/corestats.py": "\"\"\"\nExtension for collecting core stats like items scraped and start/finish times\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass CoreStats:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n        self.start_time: Optional[datetime] = None\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.start_time = datetime.now(tz=timezone.utc)\n        self.stats.set_value(\"start_time\", self.start_time, spider=spider)\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        assert self.start_time is not None\n        finish_time = datetime.now(tz=timezone.utc)\n        elapsed_time = finish_time - self.start_time\n        elapsed_time_seconds = elapsed_time.total_seconds()\n        self.stats.set_value(\n            \"elapsed_time_seconds\", elapsed_time_seconds, spider=spider\n        )\n        self.stats.set_value(\"finish_time\", finish_time, spider=spider)\n        self.stats.set_value(\"finish_reason\", reason, spider=spider)\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        self.stats.inc_value(\"item_scraped_count\", spider=spider)\n\n    def response_received(self, spider: Spider) -> None:\n        self.stats.inc_value(\"response_received_count\", spider=spider)\n\n    def item_dropped(self, item: Any, spider: Spider, exception: BaseException) -> None:\n        reason = exception.__class__.__name__\n        self.stats.inc_value(\"item_dropped_count\", spider=spider)\n        self.stats.inc_value(f\"item_dropped_reasons_count/{reason}\", spider=spider)\n", "scrapy/extensions/debug.py": "\"\"\"\nExtensions for debugging Scrapy\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport signal\nimport sys\nimport threading\nimport traceback\nfrom pdb import Pdb\nfrom types import FrameType\nfrom typing import TYPE_CHECKING, Optional\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.engine import format_engine_status\nfrom scrapy.utils.trackref import format_live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass StackTraceDump:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        try:\n            signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n            signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n        except AttributeError:\n            # win32 platforms don't support SIGUSR signals\n            pass\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def dump_stacktrace(self, signum: int, frame: Optional[FrameType]) -> None:\n        assert self.crawler.engine\n        log_args = {\n            \"stackdumps\": self._thread_stacks(),\n            \"enginestatus\": format_engine_status(self.crawler.engine),\n            \"liverefs\": format_live_refs(),\n        }\n        logger.info(\n            \"Dumping stack trace and engine status\\n\"\n            \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n            log_args,\n            extra={\"crawler\": self.crawler},\n        )\n\n    def _thread_stacks(self) -> str:\n        id2name = {th.ident: th.name for th in threading.enumerate()}\n        dumps = \"\"\n        for id_, frame in sys._current_frames().items():\n            name = id2name.get(id_, \"\")\n            dump = \"\".join(traceback.format_stack(frame))\n            dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n        return dumps\n\n\nclass Debugger:\n    def __init__(self) -> None:\n        try:\n            signal.signal(signal.SIGUSR2, self._enter_debugger)\n        except AttributeError:\n            # win32 platforms don't support SIGUSR signals\n            pass\n\n    def _enter_debugger(self, signum: int, frame: Optional[FrameType]) -> None:\n        assert frame\n        Pdb().set_trace(frame.f_back)  # noqa: T100\n", "scrapy/extensions/memusage.py": "\"\"\"\nMemoryUsage extension\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport socket\nimport sys\nfrom importlib import import_module\nfrom pprint import pformat\nfrom typing import TYPE_CHECKING, List\n\nfrom twisted.internet import task\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.mail import MailSender\nfrom scrapy.utils.engine import get_engine_status\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass MemoryUsage:\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"MEMUSAGE_ENABLED\"):\n            raise NotConfigured\n        try:\n            # stdlib's resource module is only available on unix platforms.\n            self.resource = import_module(\"resource\")\n        except ImportError:\n            raise NotConfigured\n\n        self.crawler: Crawler = crawler\n        self.warned: bool = False\n        self.notify_mails: List[str] = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n        self.limit: int = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n        self.warning: int = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n        self.check_interval: float = crawler.settings.getfloat(\n            \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n        )\n        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def get_virtual_size(self) -> int:\n        size: int = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != \"darwin\":\n            # on macOS ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size\n\n    def engine_started(self) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n        self.tasks: List[task.LoopingCall] = []\n        tsk = task.LoopingCall(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = task.LoopingCall(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = task.LoopingCall(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n\n    def engine_stopped(self) -> None:\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()\n\n    def update(self) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.max_value(\"memusage/max\", self.get_virtual_size())\n\n    def _check_limit(self) -> None:\n        assert self.crawler.engine\n        assert self.crawler.stats\n        peak_mem_usage = self.get_virtual_size()\n        if peak_mem_usage > self.limit:\n            self.crawler.stats.set_value(\"memusage/limit_reached\", 1)\n            mem = self.limit / 1024 / 1024\n            logger.error(\n                \"Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...\",\n                {\"memusage\": mem},\n                extra={\"crawler\": self.crawler},\n            )\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n                    f\"memory usage exceeded {mem}MiB at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value(\"memusage/limit_notified\", 1)\n\n            if self.crawler.engine.spider is not None:\n                self.crawler.engine.close_spider(\n                    self.crawler.engine.spider, \"memusage_exceeded\"\n                )\n            else:\n                self.crawler.stop()\n        else:\n            logger.info(\n                \"Peak memory usage is %(virtualsize)dMiB\",\n                {\"virtualsize\": peak_mem_usage / 1024 / 1024},\n            )\n\n    def _check_warning(self) -> None:\n        if self.warned:  # warn only once\n            return\n        assert self.crawler.stats\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value(\"memusage/warning_reached\", 1)\n            mem = self.warning / 1024 / 1024\n            logger.warning(\n                \"Memory usage reached %(memusage)dMiB\",\n                {\"memusage\": mem},\n                extra={\"crawler\": self.crawler},\n            )\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} warning: \"\n                    f\"memory usage reached {mem}MiB at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value(\"memusage/warning_notified\", 1)\n            self.warned = True\n\n    def _send_report(self, rcpts: List[str], subject: str) -> None:\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        assert self.crawler.engine\n        assert self.crawler.stats\n        stats = self.crawler.stats\n        s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup') / 1024 / 1024}M\\r\\n\"\n        s += f\"Maximum memory usage          : {stats.get_value('memusage/max') / 1024 / 1024}M\\r\\n\"\n        s += f\"Current memory usage          : {self.get_virtual_size() / 1024 / 1024}M\\r\\n\"\n\n        s += (\n            \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        )\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)\n", "scrapy/extensions/logstats.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n\nfrom twisted.internet import task\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass LogStats:\n    \"\"\"Log basic scraping stats periodically like:\n    * RPM - Requests per Minute\n    * IPM - Items per Minute\n    \"\"\"\n\n    def __init__(self, stats: StatsCollector, interval: float = 60.0):\n        self.stats: StatsCollector = stats\n        self.interval: float = interval\n        self.multiplier: float = 60.0 / self.interval\n        self.task: Optional[task.LoopingCall] = None\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n        if not interval:\n            raise NotConfigured\n        assert crawler.stats\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.pagesprev: int = 0\n        self.itemsprev: int = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)\n\n    def log(self, spider: Spider) -> None:\n        self.calculate_stats()\n\n        msg = (\n            \"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n            \"scraped %(items)d items (at %(itemrate)d items/min)\"\n        )\n        log_args = {\n            \"pages\": self.pages,\n            \"pagerate\": self.prate,\n            \"items\": self.items,\n            \"itemrate\": self.irate,\n        }\n        logger.info(msg, log_args, extra={\"spider\": spider})\n\n    def calculate_stats(self) -> None:\n        self.items: int = self.stats.get_value(\"item_scraped_count\", 0)\n        self.pages: int = self.stats.get_value(\"response_received_count\", 0)\n        self.irate: float = (self.items - self.itemsprev) * self.multiplier\n        self.prate: float = (self.pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = self.pages, self.items\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        if self.task and self.task.running:\n            self.task.stop()\n\n        rpm_final, ipm_final = self.calculate_final_stats(spider)\n        self.stats.set_value(\"responses_per_minute\", rpm_final)\n        self.stats.set_value(\"items_per_minute\", ipm_final)\n\n    def calculate_final_stats(\n        self, spider: Spider\n    ) -> Union[Tuple[None, None], Tuple[float, float]]:\n        start_time = self.stats.get_value(\"start_time\")\n        finished_time = self.stats.get_value(\"finished_time\")\n\n        if not start_time or not finished_time:\n            return None, None\n\n        mins_elapsed = (finished_time - start_time).seconds / 60\n\n        items = self.stats.get_value(\"item_scraped_count\", 0)\n        pages = self.stats.get_value(\"response_received_count\", 0)\n\n        return (pages / mins_elapsed), (items / mins_elapsed)\n", "scrapy/extensions/httpcache.py": "import gzip\nimport logging\nimport os\nimport pickle  # nosec\nfrom email.utils import mktime_tz, parsedate_tz\nfrom importlib import import_module\nfrom pathlib import Path\nfrom time import time\nfrom types import ModuleType\nfrom typing import IO, TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union, cast\nfrom weakref import WeakKeyDictionary\n\nfrom w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n\nfrom scrapy.http import Headers, Response\nfrom scrapy.http.request import Request\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.settings import BaseSettings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.project import data_path\nfrom scrapy.utils.python import to_bytes, to_unicode\nfrom scrapy.utils.request import RequestFingerprinter\n\nif TYPE_CHECKING:\n    # typing.Concatenate requires Python 3.10\n    from typing_extensions import Concatenate\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DummyPolicy:\n    def __init__(self, settings: BaseSettings):\n        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n        self.ignore_http_codes: List[int] = [\n            int(x) for x in settings.getlist(\"HTTPCACHE_IGNORE_HTTP_CODES\")\n        ]\n\n    def should_cache_request(self, request: Request) -> bool:\n        return urlparse_cached(request).scheme not in self.ignore_schemes\n\n    def should_cache_response(self, response: Response, request: Request) -> bool:\n        return response.status not in self.ignore_http_codes\n\n    def is_cached_response_fresh(\n        self, cachedresponse: Response, request: Request\n    ) -> bool:\n        return True\n\n    def is_cached_response_valid(\n        self, cachedresponse: Response, response: Response, request: Request\n    ) -> bool:\n        return True\n\n\nclass RFC2616Policy:\n    MAXAGE = 3600 * 24 * 365  # one year\n\n    def __init__(self, settings: BaseSettings):\n        self.always_store: bool = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n        self._cc_parsed: WeakKeyDictionary[\n            Union[Request, Response], Dict[bytes, Optional[bytes]]\n        ] = WeakKeyDictionary()\n        self.ignore_response_cache_controls: List[bytes] = [\n            to_bytes(cc)\n            for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n        ]\n\n    def _parse_cachecontrol(\n        self, r: Union[Request, Response]\n    ) -> Dict[bytes, Optional[bytes]]:\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b\"Cache-Control\", b\"\")\n            assert cch is not None\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]\n\n    def should_cache_request(self, request: Request) -> bool:\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b\"no-store\" in cc:\n            return False\n        # Any other is eligible for caching\n        return True\n\n    def should_cache_response(self, response: Response, request: Request) -> bool:\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b\"no-store\" in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        if response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        if self.always_store:\n            return True\n        # Any hint on response expiration is good\n        if b\"max-age\" in cc or b\"Expires\" in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        if response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        if response.status in (200, 203, 401):\n            return b\"Last-Modified\" in response.headers or b\"ETag\" in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        return False\n\n    def is_cached_response_fresh(\n        self, cachedresponse: Response, request: Request\n    ) -> bool:\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b\"no-cache\" in cc or b\"no-cache\" in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(\n            cachedresponse, request, now\n        )\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b\"max-stale\" in ccreq and b\"must-revalidate\" not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b\"max-stale\"]\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False\n\n    def is_cached_response_valid(\n        self, cachedresponse: Response, response: Response, request: Request\n    ) -> bool:\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b\"must-revalidate\" not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304\n\n    def _set_conditional_validators(\n        self, request: Request, cachedresponse: Response\n    ) -> None:\n        if b\"Last-Modified\" in cachedresponse.headers:\n            request.headers[b\"If-Modified-Since\"] = cachedresponse.headers[\n                b\"Last-Modified\"\n            ]\n\n        if b\"ETag\" in cachedresponse.headers:\n            request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n\n    def _get_max_age(self, cc: Dict[bytes, Optional[bytes]]) -> Optional[int]:\n        try:\n            return max(0, int(cc[b\"max-age\"]))  # type: ignore[arg-type]\n        except (KeyError, ValueError):\n            return None\n\n    def _compute_freshness_lifetime(\n        self, response: Response, request: Request, now: float\n    ) -> float:\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n\n        # Try HTTP/1.0 Expires header\n        if b\"Expires\" in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b\"Expires\"])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b\"Last-Modified\"))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute freshness lifetime\n        return 0\n\n    def _compute_current_age(\n        self, response: Response, request: Request, now: float\n    ) -> float:\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage: float = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n        if now > date:\n            currentage = now - date\n\n        if b\"Age\" in response.headers:\n            try:\n                age = int(response.headers[b\"Age\"])  # type: ignore[arg-type]\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage\n\n\nclass DbmCacheStorage:\n    def __init__(self, settings: BaseSettings):\n        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"], createdir=True)\n        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n        self.dbmodule: ModuleType = import_module(settings[\"HTTPCACHE_DBM_MODULE\"])\n        self.db: Any = None  # the real type is private\n\n    def open_spider(self, spider: Spider) -> None:\n        dbpath = Path(self.cachedir, f\"{spider.name}.db\")\n        self.db = self.dbmodule.open(str(dbpath), \"c\")\n\n        logger.debug(\n            \"Using DBM cache storage in %(cachepath)s\",\n            {\"cachepath\": dbpath},\n            extra={\"spider\": spider},\n        )\n\n        assert spider.crawler.request_fingerprinter\n        self._fingerprinter: RequestFingerprinter = spider.crawler.request_fingerprinter\n\n    def close_spider(self, spider: Spider) -> None:\n        self.db.close()\n\n    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n        data = self._read_data(spider, request)\n        if data is None:\n            return None  # not cached\n        url = data[\"url\"]\n        status = data[\"status\"]\n        headers = Headers(data[\"headers\"])\n        body = data[\"body\"]\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(\n        self, spider: Spider, request: Request, response: Response\n    ) -> None:\n        key = self._fingerprinter.fingerprint(request).hex()\n        data = {\n            \"status\": response.status,\n            \"url\": response.url,\n            \"headers\": dict(response.headers),\n            \"body\": response.body,\n        }\n        self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n        self.db[f\"{key}_time\"] = str(time())\n\n    def _read_data(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n        key = self._fingerprinter.fingerprint(request).hex()\n        db = self.db\n        tkey = f\"{key}_time\"\n        if tkey not in db:\n            return None  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return None  # expired\n\n        return cast(Dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # nosec\n\n\nclass FilesystemCacheStorage:\n    def __init__(self, settings: BaseSettings):\n        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"])\n        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n        self.use_gzip: bool = settings.getbool(\"HTTPCACHE_GZIP\")\n        # https://github.com/python/mypy/issues/10740\n        self._open: Callable[\n            Concatenate[Union[str, os.PathLike], str, ...], IO[bytes]\n        ] = (\n            gzip.open if self.use_gzip else open  # type: ignore[assignment]\n        )\n\n    def open_spider(self, spider: Spider) -> None:\n        logger.debug(\n            \"Using filesystem cache storage in %(cachedir)s\",\n            {\"cachedir\": self.cachedir},\n            extra={\"spider\": spider},\n        )\n\n        assert spider.crawler.request_fingerprinter\n        self._fingerprinter = spider.crawler.request_fingerprinter\n\n    def close_spider(self, spider: Spider) -> None:\n        pass\n\n    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return None  # not cached\n        rpath = Path(self._get_request_path(spider, request))\n        with self._open(rpath / \"response_body\", \"rb\") as f:\n            body = f.read()\n        with self._open(rpath / \"response_headers\", \"rb\") as f:\n            rawheaders = f.read()\n        url = metadata[\"response_url\"]\n        status = metadata[\"status\"]\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(\n        self, spider: Spider, request: Request, response: Response\n    ) -> None:\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = Path(self._get_request_path(spider, request))\n        if not rpath.exists():\n            rpath.mkdir(parents=True)\n        metadata = {\n            \"url\": request.url,\n            \"method\": request.method,\n            \"status\": response.status,\n            \"response_url\": response.url,\n            \"timestamp\": time(),\n        }\n        with self._open(rpath / \"meta\", \"wb\") as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(rpath / \"pickled_meta\", \"wb\") as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(rpath / \"response_headers\", \"wb\") as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(rpath / \"response_body\", \"wb\") as f:\n            f.write(response.body)\n        with self._open(rpath / \"request_headers\", \"wb\") as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(rpath / \"request_body\", \"wb\") as f:\n            f.write(request.body)\n\n    def _get_request_path(self, spider: Spider, request: Request) -> str:\n        key = self._fingerprinter.fingerprint(request).hex()\n        return str(Path(self.cachedir, spider.name, key[0:2], key))\n\n    def _read_meta(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n        rpath = Path(self._get_request_path(spider, request))\n        metapath = rpath / \"pickled_meta\"\n        if not metapath.exists():\n            return None  # not found\n        mtime = metapath.stat().st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return None  # expired\n        with self._open(metapath, \"rb\") as f:\n            return cast(Dict[str, Any], pickle.load(f))  # nosec\n\n\ndef parse_cachecontrol(header: bytes) -> Dict[bytes, Optional[bytes]]:\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b\",\"):\n        key, sep, val = directive.strip().partition(b\"=\")\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives\n\n\ndef rfc1123_to_epoch(date_str: Union[str, bytes, None]) -> Optional[int]:\n    try:\n        date_str = to_unicode(date_str, encoding=\"ascii\")  # type: ignore[arg-type]\n        return mktime_tz(parsedate_tz(date_str))  # type: ignore[arg-type]\n    except Exception:\n        return None\n", "scrapy/extensions/closespider.py": "\"\"\"CloseSpider is an extension that forces spiders to be closed after certain\nconditions are met.\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Dict\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass CloseSpider:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n\n        self.close_on: Dict[str, Any] = {\n            \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n            \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n            \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n            \"errorcount\": crawler.settings.getint(\"CLOSESPIDER_ERRORCOUNT\"),\n            \"timeout_no_item\": crawler.settings.getint(\"CLOSESPIDER_TIMEOUT_NO_ITEM\"),\n        }\n\n        if not any(self.close_on.values()):\n            raise NotConfigured\n\n        self.counter: DefaultDict[str, int] = defaultdict(int)\n\n        if self.close_on.get(\"errorcount\"):\n            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n        if self.close_on.get(\"pagecount\"):\n            crawler.signals.connect(self.page_count, signal=signals.response_received)\n        if self.close_on.get(\"timeout\"):\n            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        if self.close_on.get(\"itemcount\"):\n            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n        if self.close_on.get(\"timeout_no_item\"):\n            self.timeout_no_item: int = self.close_on[\"timeout_no_item\"]\n            self.items_in_period: int = 0\n            crawler.signals.connect(\n                self.spider_opened_no_item, signal=signals.spider_opened\n            )\n            crawler.signals.connect(\n                self.item_scraped_no_item, signal=signals.item_scraped\n            )\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def error_count(self, failure: Failure, response: Response, spider: Spider) -> None:\n        self.counter[\"errorcount\"] += 1\n        if self.counter[\"errorcount\"] == self.close_on[\"errorcount\"]:\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_errorcount\")\n\n    def page_count(self, response: Response, request: Request, spider: Spider) -> None:\n        self.counter[\"pagecount\"] += 1\n        if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_pagecount\")\n\n    def spider_opened(self, spider: Spider) -> None:\n        from twisted.internet import reactor\n\n        assert self.crawler.engine\n        self.task = reactor.callLater(\n            self.close_on[\"timeout\"],\n            self.crawler.engine.close_spider,\n            spider,\n            reason=\"closespider_timeout\",\n        )\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        self.counter[\"itemcount\"] += 1\n        if self.counter[\"itemcount\"] == self.close_on[\"itemcount\"]:\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_itemcount\")\n\n    def spider_closed(self, spider: Spider) -> None:\n        task = getattr(self, \"task\", None)\n        if task and task.active():\n            task.cancel()\n\n        task_no_item = getattr(self, \"task_no_item\", None)\n        if task_no_item and task_no_item.running:\n            task_no_item.stop()\n\n    def spider_opened_no_item(self, spider: Spider) -> None:\n        from twisted.internet import task\n\n        self.task_no_item = task.LoopingCall(self._count_items_produced, spider)\n        self.task_no_item.start(self.timeout_no_item, now=False)\n\n        logger.info(\n            f\"Spider will stop when no items are produced after \"\n            f\"{self.timeout_no_item} seconds.\"\n        )\n\n    def item_scraped_no_item(self, item: Any, spider: Spider) -> None:\n        self.items_in_period += 1\n\n    def _count_items_produced(self, spider: Spider) -> None:\n        if self.items_in_period >= 1:\n            self.items_in_period = 0\n        else:\n            logger.info(\n                f\"Closing spider since no items were produced in the last \"\n                f\"{self.timeout_no_item} seconds.\"\n            )\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_timeout_no_item\")\n", "scrapy/extensions/statsmailer.py": "\"\"\"\nStatsMailer extension sends an email when a spider finishes scraping.\n\nUse STATSMAILER_RCPTS setting to enable and give the recipient mail address\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List, Optional\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.mail import MailSender\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass StatsMailer:\n    def __init__(self, stats: StatsCollector, recipients: List[str], mail: MailSender):\n        self.stats: StatsCollector = stats\n        self.recipients: List[str] = recipients\n        self.mail: MailSender = mail\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        recipients: List[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n        if not recipients:\n            raise NotConfigured\n        mail: MailSender = MailSender.from_settings(crawler.settings)\n        assert crawler.stats\n        o = cls(crawler.stats, recipients, mail)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_closed(self, spider: Spider) -> Optional[Deferred]:\n        spider_stats = self.stats.get_stats(spider)\n        body = \"Global stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n        body += f\"\\n\\n{spider.name} stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in spider_stats.items())\n        return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)\n", "scrapy/extensions/feedexport.py": "\"\"\"\nFeed Exports extension\n\nSee documentation in docs/topics/feed-exports.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport re\nimport sys\nimport warnings\nfrom datetime import datetime, timezone\nfrom pathlib import Path, PureWindowsPath\nfrom tempfile import NamedTemporaryFile\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom urllib.parse import unquote, urlparse\n\nfrom twisted.internet import threads\nfrom twisted.internet.defer import Deferred, DeferredList, maybeDeferred\nfrom twisted.python.failure import Failure\nfrom w3lib.url import file_uri_to_path\nfrom zope.interface import Interface, implementer\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.exporters import BaseItemExporter\nfrom scrapy.extensions.postprocessing import PostProcessingManager\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.conf import feed_complete_default_values_from_settings\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom scrapy.utils.deprecate import create_deprecated_class\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from _typeshed import OpenBinaryMode\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import boto3  # noqa: F401\n\n    IS_BOTO3_AVAILABLE = True\nexcept ImportError:\n    IS_BOTO3_AVAILABLE = False\n\nUriParamsCallableT = Callable[[Dict[str, Any], Spider], Optional[Dict[str, Any]]]\n\n_StorageT = TypeVar(\"_StorageT\", bound=\"FeedStorageProtocol\")\n\n\ndef build_storage(\n    builder: Callable[..., _StorageT],\n    uri: str,\n    *args: Any,\n    feed_options: Optional[Dict[str, Any]] = None,\n    preargs: Iterable[Any] = (),\n    **kwargs: Any,\n) -> _StorageT:\n    kwargs[\"feed_options\"] = feed_options\n    return builder(*preargs, uri, *args, **kwargs)\n\n\nclass ItemFilter:\n    \"\"\"\n    This will be used by FeedExporter to decide if an item should be allowed\n    to be exported to a particular feed.\n\n    :param feed_options: feed specific options passed from FeedExporter\n    :type feed_options: dict\n    \"\"\"\n\n    feed_options: Optional[Dict[str, Any]]\n    item_classes: Tuple[type, ...]\n\n    def __init__(self, feed_options: Optional[Dict[str, Any]]) -> None:\n        self.feed_options = feed_options\n        if feed_options is not None:\n            self.item_classes = tuple(\n                load_object(item_class)\n                for item_class in feed_options.get(\"item_classes\") or ()\n            )\n        else:\n            self.item_classes = ()\n\n    def accepts(self, item: Any) -> bool:\n        \"\"\"\n        Return ``True`` if `item` should be exported or ``False`` otherwise.\n\n        :param item: scraped item which user wants to check if is acceptable\n        :type item: :ref:`Scrapy items <topics-items>`\n        :return: `True` if accepted, `False` otherwise\n        :rtype: bool\n        \"\"\"\n        if self.item_classes:\n            return isinstance(item, self.item_classes)\n        return True  # accept all items by default\n\n\nclass IFeedStorage(Interface):\n    \"\"\"Interface that all Feed Storages must implement\"\"\"\n\n    def __init__(uri, *, feed_options=None):\n        \"\"\"Initialize the storage with the parameters given in the URI and the\n        feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n    def open(spider):\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(file):\n        \"\"\"Store the given file stream\"\"\"\n\n\nclass FeedStorageProtocol(Protocol):\n    \"\"\"Reimplementation of ``IFeedStorage`` that can be used in type hints.\"\"\"\n\n    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n        \"\"\"Initialize the storage with the parameters given in the URI and the\n        feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        \"\"\"Store the given file stream\"\"\"\n\n\n@implementer(IFeedStorage)\nclass BlockingFeedStorage:\n    def open(self, spider: Spider) -> IO[bytes]:\n        path = spider.crawler.settings[\"FEED_TEMPDIR\"]\n        if path and not Path(path).is_dir():\n            raise OSError(\"Not a Directory: \" + str(path))\n\n        return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        return threads.deferToThread(self._store_in_thread, file)\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        raise NotImplementedError\n\n\n@implementer(IFeedStorage)\nclass StdoutFeedStorage:\n    def __init__(\n        self,\n        uri: str,\n        _stdout: Optional[IO[bytes]] = None,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ):\n        if not _stdout:\n            _stdout = sys.stdout.buffer\n        self._stdout: IO[bytes] = _stdout\n        if feed_options and feed_options.get(\"overwrite\", False) is True:\n            logger.warning(\n                \"Standard output (stdout) storage does not support \"\n                \"overwriting. To suppress this warning, remove the \"\n                \"overwrite option from your FEEDS setting, or set \"\n                \"it to False.\"\n            )\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        return self._stdout\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        pass\n\n\n@implementer(IFeedStorage)\nclass FileFeedStorage:\n    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n        self.path: str = file_uri_to_path(uri)\n        feed_options = feed_options or {}\n        self.write_mode: OpenBinaryMode = (\n            \"wb\" if feed_options.get(\"overwrite\", False) else \"ab\"\n        )\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        dirname = Path(self.path).parent\n        if dirname and not dirname.exists():\n            dirname.mkdir(parents=True)\n        return Path(self.path).open(self.write_mode)\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        file.close()\n        return None\n\n\nclass S3FeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        access_key: Optional[str] = None,\n        secret_key: Optional[str] = None,\n        acl: Optional[str] = None,\n        endpoint_url: Optional[str] = None,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n        session_token: Optional[str] = None,\n        region_name: Optional[str] = None,\n    ):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n        u = urlparse(uri)\n        assert u.hostname\n        self.bucketname: str = u.hostname\n        self.access_key: Optional[str] = u.username or access_key\n        self.secret_key: Optional[str] = u.password or secret_key\n        self.session_token: Optional[str] = session_token\n        self.keyname: str = u.path[1:]  # remove first \"/\"\n        self.acl: Optional[str] = acl\n        self.endpoint_url: Optional[str] = endpoint_url\n        self.region_name: Optional[str] = region_name\n        # It can be either botocore.client.BaseClient or mypy_boto3_s3.S3Client,\n        # there seems to be no good way to infer it statically.\n        self.s3_client: Any\n\n        if IS_BOTO3_AVAILABLE:\n            import boto3.session\n\n            boto3_session = boto3.session.Session()\n\n            self.s3_client = boto3_session.client(\n                \"s3\",\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                aws_session_token=self.session_token,\n                endpoint_url=self.endpoint_url,\n                region_name=self.region_name,\n            )\n        else:\n            warnings.warn(\n                \"`botocore` usage has been deprecated for S3 feed \"\n                \"export, please use `boto3` to avoid problems\",\n                category=ScrapyDeprecationWarning,\n            )\n\n            import botocore.session\n\n            botocore_session = botocore.session.get_session()\n\n            self.s3_client = botocore_session.create_client(\n                \"s3\",\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                aws_session_token=self.session_token,\n                endpoint_url=self.endpoint_url,\n                region_name=self.region_name,\n            )\n\n        if feed_options and feed_options.get(\"overwrite\", True) is False:\n            logger.warning(\n                \"S3 does not support appending to files. To \"\n                \"suppress this warning, remove the overwrite \"\n                \"option from your FEEDS setting or set it to True.\"\n            )\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ) -> Self:\n        return build_storage(\n            cls,\n            uri,\n            access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n            secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n            session_token=crawler.settings[\"AWS_SESSION_TOKEN\"],\n            acl=crawler.settings[\"FEED_STORAGE_S3_ACL\"] or None,\n            endpoint_url=crawler.settings[\"AWS_ENDPOINT_URL\"] or None,\n            region_name=crawler.settings[\"AWS_REGION_NAME\"] or None,\n            feed_options=feed_options,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        file.seek(0)\n        kwargs: Dict[str, Any]\n        if IS_BOTO3_AVAILABLE:\n            kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n            self.s3_client.upload_fileobj(\n                Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n            )\n        else:\n            kwargs = {\"ACL\": self.acl} if self.acl else {}\n            self.s3_client.put_object(\n                Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n            )\n        file.close()\n\n\nclass GCSFeedStorage(BlockingFeedStorage):\n    def __init__(self, uri: str, project_id: Optional[str], acl: Optional[str]):\n        self.project_id: Optional[str] = project_id\n        self.acl: Optional[str] = acl\n        u = urlparse(uri)\n        assert u.hostname\n        self.bucket_name: str = u.hostname\n        self.blob_name: str = u.path[1:]  # remove first \"/\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, uri: str) -> Self:\n        return cls(\n            uri,\n            crawler.settings[\"GCS_PROJECT_ID\"],\n            crawler.settings[\"FEED_STORAGE_GCS_ACL\"] or None,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        file.seek(0)\n        from google.cloud.storage import Client\n\n        client = Client(project=self.project_id)\n        bucket = client.get_bucket(self.bucket_name)\n        blob = bucket.blob(self.blob_name)\n        blob.upload_from_file(file, predefined_acl=self.acl)\n\n\nclass FTPFeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        use_active_mode: bool = False,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ):\n        u = urlparse(uri)\n        if not u.hostname:\n            raise ValueError(f\"Got a storage URI without a hostname: {uri}\")\n        self.host: str = u.hostname\n        self.port: int = int(u.port or \"21\")\n        self.username: str = u.username or \"\"\n        self.password: str = unquote(u.password or \"\")\n        self.path: str = u.path\n        self.use_active_mode: bool = use_active_mode\n        self.overwrite: bool = not feed_options or feed_options.get(\"overwrite\", True)\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ) -> Self:\n        return build_storage(\n            cls,\n            uri,\n            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n            feed_options=feed_options,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        ftp_store_file(\n            path=self.path,\n            file=file,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.use_active_mode,\n            overwrite=self.overwrite,\n        )\n\n\nclass FeedSlot:\n    def __init__(\n        self,\n        storage: FeedStorageProtocol,\n        uri: str,\n        format: str,\n        store_empty: bool,\n        batch_id: int,\n        uri_template: str,\n        filter: ItemFilter,\n        feed_options: Dict[str, Any],\n        spider: Spider,\n        exporters: Dict[str, Type[BaseItemExporter]],\n        settings: BaseSettings,\n        crawler: Crawler,\n    ):\n        self.file: Optional[IO[bytes]] = None\n        self.exporter: Optional[BaseItemExporter] = None\n        self.storage: FeedStorageProtocol = storage\n        # feed params\n        self.batch_id: int = batch_id\n        self.format: str = format\n        self.store_empty: bool = store_empty\n        self.uri_template: str = uri_template\n        self.uri: str = uri\n        self.filter: ItemFilter = filter\n        # exporter params\n        self.feed_options: Dict[str, Any] = feed_options\n        self.spider: Spider = spider\n        self.exporters: Dict[str, Type[BaseItemExporter]] = exporters\n        self.settings: BaseSettings = settings\n        self.crawler: Crawler = crawler\n        # flags\n        self.itemcount: int = 0\n        self._exporting: bool = False\n        self._fileloaded: bool = False\n\n    def start_exporting(self) -> None:\n        if not self._fileloaded:\n            self.file = self.storage.open(self.spider)\n            if \"postprocessing\" in self.feed_options:\n                self.file = cast(\n                    IO[bytes],\n                    PostProcessingManager(\n                        self.feed_options[\"postprocessing\"],\n                        self.file,\n                        self.feed_options,\n                    ),\n                )\n            self.exporter = self._get_exporter(\n                file=self.file,\n                format=self.feed_options[\"format\"],\n                fields_to_export=self.feed_options[\"fields\"],\n                encoding=self.feed_options[\"encoding\"],\n                indent=self.feed_options[\"indent\"],\n                **self.feed_options[\"item_export_kwargs\"],\n            )\n            self._fileloaded = True\n\n        if not self._exporting:\n            assert self.exporter\n            self.exporter.start_exporting()\n            self._exporting = True\n\n    def _get_instance(\n        self, objcls: Type[BaseItemExporter], *args: Any, **kwargs: Any\n    ) -> BaseItemExporter:\n        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n\n    def _get_exporter(\n        self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n    ) -> BaseItemExporter:\n        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n\n    def finish_exporting(self) -> None:\n        if self._exporting:\n            assert self.exporter\n            self.exporter.finish_exporting()\n            self._exporting = False\n\n\n_FeedSlot = create_deprecated_class(\n    name=\"_FeedSlot\",\n    new_class=FeedSlot,\n)\n\n\nclass FeedExporter:\n    _pending_deferreds: List[Deferred] = []\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        exporter = cls(crawler)\n        crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n        crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n        return exporter\n\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        self.settings: Settings = crawler.settings\n        self.feeds = {}\n        self.slots: List[FeedSlot] = []\n        self.filters: Dict[str, ItemFilter] = {}\n\n        if not self.settings[\"FEEDS\"] and not self.settings[\"FEED_URI\"]:\n            raise NotConfigured\n\n        # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings\n        if self.settings[\"FEED_URI\"]:\n            warnings.warn(\n                \"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of \"\n                \"the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            uri = self.settings[\"FEED_URI\"]\n            # handle pathlib.Path objects\n            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()\n            feed_options = {\"format\": self.settings.get(\"FEED_FORMAT\", \"jsonlines\")}\n            self.feeds[uri] = feed_complete_default_values_from_settings(\n                feed_options, self.settings\n            )\n            self.filters[uri] = self._load_filter(feed_options)\n        # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n\n        # 'FEEDS' setting takes precedence over 'FEED_URI'\n        for uri, feed_options in self.settings.getdict(\"FEEDS\").items():\n            # handle pathlib.Path objects\n            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()\n            self.feeds[uri] = feed_complete_default_values_from_settings(\n                feed_options, self.settings\n            )\n            self.filters[uri] = self._load_filter(feed_options)\n\n        self.storages: Dict[str, Type[FeedStorageProtocol]] = self._load_components(\n            \"FEED_STORAGES\"\n        )\n        self.exporters: Dict[str, Type[BaseItemExporter]] = self._load_components(\n            \"FEED_EXPORTERS\"\n        )\n        for uri, feed_options in self.feeds.items():\n            if not self._storage_supported(uri, feed_options):\n                raise NotConfigured\n            if not self._settings_are_valid():\n                raise NotConfigured\n            if not self._exporter_supported(feed_options[\"format\"]):\n                raise NotConfigured\n\n    def open_spider(self, spider: Spider) -> None:\n        for uri, feed_options in self.feeds.items():\n            uri_params = self._get_uri_params(spider, feed_options[\"uri_params\"])\n            self.slots.append(\n                self._start_new_batch(\n                    batch_id=1,\n                    uri=uri % uri_params,\n                    feed_options=feed_options,\n                    spider=spider,\n                    uri_template=uri,\n                )\n            )\n\n    async def close_spider(self, spider: Spider) -> None:\n        for slot in self.slots:\n            self._close_slot(slot, spider)\n\n        # Await all deferreds\n        if self._pending_deferreds:\n            await maybe_deferred_to_future(DeferredList(self._pending_deferreds))\n\n        # Send FEED_EXPORTER_CLOSED signal\n        await maybe_deferred_to_future(\n            self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n        )\n\n    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Optional[Deferred]:\n        def get_file(slot_: FeedSlot) -> IO[bytes]:\n            assert slot_.file\n            if isinstance(slot_.file, PostProcessingManager):\n                slot_.file.close()\n                return slot_.file.file\n            return slot_.file\n\n        if slot.itemcount:\n            # Normal case\n            slot.finish_exporting()\n        elif slot.store_empty and slot.batch_id == 1:\n            # Need to store the empty file\n            slot.start_exporting()\n            slot.finish_exporting()\n        else:\n            # In this case, the file is not stored, so no processing is required.\n            return None\n\n        logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n        d: Deferred = maybeDeferred(slot.storage.store, get_file(slot))\n\n        d.addCallback(\n            self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n        )\n        d.addErrback(\n            self._handle_store_error, logmsg, spider, type(slot.storage).__name__\n        )\n        self._pending_deferreds.append(d)\n        d.addCallback(\n            lambda _: self.crawler.signals.send_catch_log_deferred(\n                signals.feed_slot_closed, slot=slot\n            )\n        )\n        d.addBoth(lambda _: self._pending_deferreds.remove(d))\n\n        return d\n\n    def _handle_store_error(\n        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n    ) -> None:\n        logger.error(\n            \"Error storing %s\",\n            logmsg,\n            exc_info=failure_to_exc_info(f),\n            extra={\"spider\": spider},\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n\n    def _handle_store_success(\n        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n    ) -> None:\n        logger.info(\"Stored %s\", logmsg, extra={\"spider\": spider})\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n\n    def _start_new_batch(\n        self,\n        batch_id: int,\n        uri: str,\n        feed_options: Dict[str, Any],\n        spider: Spider,\n        uri_template: str,\n    ) -> FeedSlot:\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n        :param batch_id: sequence number of current batch\n        :param uri: uri of the new batch to start\n        :param feed_options: dict with parameters of feed\n        :param spider: user spider\n        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n        \"\"\"\n        storage = self._get_storage(uri, feed_options)\n        slot = FeedSlot(\n            storage=storage,\n            uri=uri,\n            format=feed_options[\"format\"],\n            store_empty=feed_options[\"store_empty\"],\n            batch_id=batch_id,\n            uri_template=uri_template,\n            filter=self.filters[uri_template],\n            feed_options=feed_options,\n            spider=spider,\n            exporters=self.exporters,\n            settings=self.settings,\n            crawler=self.crawler,\n        )\n        return slot\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        slots = []\n        for slot in self.slots:\n            if not slot.filter.accepts(item):\n                slots.append(\n                    slot\n                )  # if slot doesn't accept item, continue with next slot\n                continue\n\n            slot.start_exporting()\n            assert slot.exporter\n            slot.exporter.export_item(item)\n            slot.itemcount += 1\n            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n            if (\n                self.feeds[slot.uri_template][\"batch_item_count\"]\n                and slot.itemcount >= self.feeds[slot.uri_template][\"batch_item_count\"]\n            ):\n                uri_params = self._get_uri_params(\n                    spider, self.feeds[slot.uri_template][\"uri_params\"], slot\n                )\n                self._close_slot(slot, spider)\n                slots.append(\n                    self._start_new_batch(\n                        batch_id=slot.batch_id + 1,\n                        uri=slot.uri_template % uri_params,\n                        feed_options=self.feeds[slot.uri_template],\n                        spider=spider,\n                        uri_template=slot.uri_template,\n                    )\n                )\n            else:\n                slots.append(slot)\n        self.slots = slots\n\n    def _load_components(self, setting_prefix: str) -> Dict[str, Any]:\n        conf = without_none_values(\n            cast(Dict[str, str], self.settings.getwithbase(setting_prefix))\n        )\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d\n\n    def _exporter_supported(self, format: str) -> bool:\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {\"format\": format})\n        return False\n\n    def _settings_are_valid(self) -> bool:\n        \"\"\"\n        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n        \"\"\"\n        for uri_template, values in self.feeds.items():\n            if values[\"batch_item_count\"] and not re.search(\n                r\"%\\(batch_time\\)s|%\\(batch_id\\)\", uri_template\n            ):\n                logger.error(\n                    \"%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT \"\n                    \"setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: \"\n                    \"https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count\",\n                    uri_template,\n                )\n                return False\n        return True\n\n    def _storage_supported(self, uri: str, feed_options: Dict[str, Any]) -> bool:\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages or PureWindowsPath(uri).drive:\n            try:\n                self._get_storage(uri, feed_options)\n                return True\n            except NotConfigured as e:\n                logger.error(\n                    \"Disabled feed storage scheme: %(scheme)s. \" \"Reason: %(reason)s\",\n                    {\"scheme\": scheme, \"reason\": str(e)},\n                )\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n        return False\n\n    def _get_storage(\n        self, uri: str, feed_options: Dict[str, Any]\n    ) -> FeedStorageProtocol:\n        \"\"\"Fork of create_instance specific to feed storage classes\n\n        It supports not passing the *feed_options* parameters to classes that\n        do not support it, and issuing a deprecation warning instead.\n        \"\"\"\n        feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n        crawler = getattr(self, \"crawler\", None)\n\n        def build_instance(\n            builder: Type[FeedStorageProtocol], *preargs: Any\n        ) -> FeedStorageProtocol:\n            return build_storage(\n                builder, uri, feed_options=feed_options, preargs=preargs\n            )\n\n        instance: FeedStorageProtocol\n        if crawler and hasattr(feedcls, \"from_crawler\"):\n            instance = build_instance(feedcls.from_crawler, crawler)\n            method_name = \"from_crawler\"\n        elif hasattr(feedcls, \"from_settings\"):\n            instance = build_instance(feedcls.from_settings, self.settings)\n            method_name = \"from_settings\"\n        else:\n            instance = build_instance(feedcls)\n            method_name = \"__new__\"\n        if instance is None:\n            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n        return instance\n\n    def _get_uri_params(\n        self,\n        spider: Spider,\n        uri_params_function: Union[str, UriParamsCallableT, None],\n        slot: Optional[FeedSlot] = None,\n    ) -> Dict[str, Any]:\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        utc_now = datetime.now(tz=timezone.utc)\n        params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n        params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n        params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n        uripar_function: UriParamsCallableT = (\n            load_object(uri_params_function)\n            if uri_params_function\n            else lambda params, _: params\n        )\n        new_params = uripar_function(params, spider)\n        return new_params if new_params is not None else params\n\n    def _load_filter(self, feed_options: Dict[str, Any]) -> ItemFilter:\n        # load the item filter if declared else load the default filter class\n        item_filter_class: Type[ItemFilter] = load_object(\n            feed_options.get(\"item_filter\", ItemFilter)\n        )\n        return item_filter_class(feed_options)\n", "scrapy/extensions/spiderstate.py": "from __future__ import annotations\n\nimport pickle  # nosec\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Optional\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.job import job_dir\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass SpiderState:\n    \"\"\"Store and load spider state during a scraping job\"\"\"\n\n    def __init__(self, jobdir: Optional[str] = None):\n        self.jobdir: Optional[str] = jobdir\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj\n\n    def spider_closed(self, spider: Spider) -> None:\n        if self.jobdir:\n            with Path(self.statefn).open(\"wb\") as f:\n                assert hasattr(spider, \"state\")  # set in spider_opened\n                pickle.dump(spider.state, f, protocol=4)\n\n    def spider_opened(self, spider: Spider) -> None:\n        if self.jobdir and Path(self.statefn).exists():\n            with Path(self.statefn).open(\"rb\") as f:\n                spider.state = pickle.load(f)  # type: ignore[attr-defined]  # nosec\n        else:\n            spider.state = {}  # type: ignore[attr-defined]\n\n    @property\n    def statefn(self) -> str:\n        assert self.jobdir\n        return str(Path(self.jobdir, \"spider.state\"))\n", "scrapy/extensions/throttle.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader import Slot\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoThrottle:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        if not crawler.settings.getbool(\"AUTOTHROTTLE_ENABLED\"):\n            raise NotConfigured\n\n        self.debug: bool = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n        self.target_concurrency: float = crawler.settings.getfloat(\n            \"AUTOTHROTTLE_TARGET_CONCURRENCY\"\n        )\n        if self.target_concurrency <= 0.0:\n            raise NotConfigured(\n                f\"AUTOTHROTTLE_TARGET_CONCURRENCY \"\n                f\"({self.target_concurrency!r}) must be higher than 0.\"\n            )\n        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(\n            self._response_downloaded, signal=signals.response_downloaded\n        )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def _spider_opened(self, spider: Spider) -> None:\n        self.mindelay = self._min_delay(spider)\n        self.maxdelay = self._max_delay(spider)\n        spider.download_delay = self._start_delay(spider)  # type: ignore[attr-defined]\n\n    def _min_delay(self, spider: Spider) -> float:\n        s = self.crawler.settings\n        return getattr(spider, \"download_delay\", s.getfloat(\"DOWNLOAD_DELAY\"))\n\n    def _max_delay(self, spider: Spider) -> float:\n        return self.crawler.settings.getfloat(\"AUTOTHROTTLE_MAX_DELAY\")\n\n    def _start_delay(self, spider: Spider) -> float:\n        return max(\n            self.mindelay, self.crawler.settings.getfloat(\"AUTOTHROTTLE_START_DELAY\")\n        )\n\n    def _response_downloaded(\n        self, response: Response, request: Request, spider: Spider\n    ) -> None:\n        key, slot = self._get_slot(request, spider)\n        latency = request.meta.get(\"download_latency\")\n        if latency is None or slot is None or slot.throttle is False:\n            return\n\n        olddelay = slot.delay\n        self._adjust_delay(slot, latency, response)\n        if self.debug:\n            diff = slot.delay - olddelay\n            size = len(response.body)\n            conc = len(slot.transferring)\n            logger.info(\n                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                {\n                    \"slot\": key,\n                    \"concurrency\": conc,\n                    \"delay\": slot.delay * 1000,\n                    \"delaydiff\": diff * 1000,\n                    \"latency\": latency * 1000,\n                    \"size\": size,\n                },\n                extra={\"spider\": spider},\n            )\n\n    def _get_slot(\n        self, request: Request, spider: Spider\n    ) -> Tuple[Optional[str], Optional[Slot]]:\n        key: Optional[str] = request.meta.get(\"download_slot\")\n        if key is None:\n            return None, None\n        assert self.crawler.engine\n        return key, self.crawler.engine.downloader.slots.get(key)\n\n    def _adjust_delay(self, slot: Slot, latency: float, response: Response) -> None:\n        \"\"\"Define delay adjustment policy\"\"\"\n\n        # If a server needs `latency` seconds to respond then\n        # we should send a request each `latency/N` seconds\n        # to have N requests processed in parallel\n        target_delay = latency / self.target_concurrency\n\n        # Adjust the delay to make it closer to target_delay\n        new_delay = (slot.delay + target_delay) / 2.0\n\n        # If target delay is bigger than old delay, then use it instead of mean.\n        # It works better with problematic sites.\n        new_delay = max(target_delay, new_delay)\n\n        # Make sure self.mindelay <= new_delay <= self.max_delay\n        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n        # Dont adjust delay if response status != 200 and new delay is smaller\n        # than old one, as error pages (and redirections) are usually small and\n        # so tend to reduce latency, thus provoking a positive feedback by\n        # reducing delay instead of increase.\n        if response.status != 200 and new_delay <= slot.delay:\n            return\n\n        slot.delay = new_delay\n", "scrapy/extensions/postprocessing.py": "\"\"\"\nExtension for processing data before they are exported to feeds.\n\"\"\"\n\nfrom bz2 import BZ2File\nfrom gzip import GzipFile\nfrom io import IOBase\nfrom lzma import LZMAFile\nfrom typing import IO, Any, BinaryIO, Dict, List, cast\n\nfrom scrapy.utils.misc import load_object\n\n\nclass GzipPlugin:\n    \"\"\"\n    Compresses received data using `gzip <https://en.wikipedia.org/wiki/Gzip>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `gzip_compresslevel`\n    - `gzip_mtime`\n    - `gzip_filename`\n\n    See :py:class:`gzip.GzipFile` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n        compress_level = self.feed_options.get(\"gzip_compresslevel\", 9)\n        mtime = self.feed_options.get(\"gzip_mtime\")\n        filename = self.feed_options.get(\"gzip_filename\")\n        self.gzipfile = GzipFile(\n            fileobj=self.file,\n            mode=\"wb\",\n            compresslevel=compress_level,\n            mtime=mtime,\n            filename=filename,\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.gzipfile.write(data)\n\n    def close(self) -> None:\n        self.gzipfile.close()\n\n\nclass Bz2Plugin:\n    \"\"\"\n    Compresses received data using `bz2 <https://en.wikipedia.org/wiki/Bzip2>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `bz2_compresslevel`\n\n    See :py:class:`bz2.BZ2File` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n        compress_level = self.feed_options.get(\"bz2_compresslevel\", 9)\n        self.bz2file = BZ2File(\n            filename=self.file, mode=\"wb\", compresslevel=compress_level\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.bz2file.write(data)\n\n    def close(self) -> None:\n        self.bz2file.close()\n\n\nclass LZMAPlugin:\n    \"\"\"\n    Compresses received data using `lzma <https://en.wikipedia.org/wiki/Lempel\u2013Ziv\u2013Markov_chain_algorithm>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `lzma_format`\n    - `lzma_check`\n    - `lzma_preset`\n    - `lzma_filters`\n\n    .. note::\n        ``lzma_filters`` cannot be used in pypy version 7.3.1 and older.\n\n    See :py:class:`lzma.LZMAFile` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n\n        format = self.feed_options.get(\"lzma_format\")\n        check = self.feed_options.get(\"lzma_check\", -1)\n        preset = self.feed_options.get(\"lzma_preset\")\n        filters = self.feed_options.get(\"lzma_filters\")\n        self.lzmafile = LZMAFile(\n            filename=self.file,\n            mode=\"wb\",\n            format=format,\n            check=check,\n            preset=preset,\n            filters=filters,\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.lzmafile.write(data)\n\n    def close(self) -> None:\n        self.lzmafile.close()\n\n\n# io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n# instance as a file like writable object. This could be needed by some exporters\n# such as CsvItemExporter which wraps the feed storage with io.TextIOWrapper.\nclass PostProcessingManager(IOBase):\n    \"\"\"\n    This will manage and use declared plugins to process data in a\n    pipeline-ish way.\n    :param plugins: all the declared plugins for the feed\n    :type plugins: list\n    :param file: final target file where the processed data will be written\n    :type file: file like object\n    \"\"\"\n\n    def __init__(\n        self, plugins: List[Any], file: IO[bytes], feed_options: Dict[str, Any]\n    ) -> None:\n        self.plugins = self._load_plugins(plugins)\n        self.file = file\n        self.feed_options = feed_options\n        self.head_plugin = self._get_head_plugin()\n\n    def write(self, data: bytes) -> int:\n        \"\"\"\n        Uses all the declared plugins to process data first, then writes\n        the processed data to target file.\n        :param data: data passed to be written to target file\n        :type data: bytes\n        :return: returns number of bytes written\n        :rtype: int\n        \"\"\"\n        return cast(int, self.head_plugin.write(data))\n\n    def tell(self) -> int:\n        return self.file.tell()\n\n    def close(self) -> None:\n        \"\"\"\n        Close the target file along with all the plugins.\n        \"\"\"\n        self.head_plugin.close()\n\n    def writable(self) -> bool:\n        return True\n\n    def _load_plugins(self, plugins: List[Any]) -> List[Any]:\n        plugins = [load_object(plugin) for plugin in plugins]\n        return plugins\n\n    def _get_head_plugin(self) -> Any:\n        prev = self.file\n        for plugin in self.plugins[::-1]:\n            prev = plugin(prev, self.feed_options)\n        return prev\n", "scrapy/extensions/periodic_log.py": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime, timezone\nfrom json import JSONEncoder\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n\nfrom twisted.internet import task\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass PeriodicLog:\n    \"\"\"Log basic scraping stats periodically\"\"\"\n\n    def __init__(\n        self,\n        stats: StatsCollector,\n        interval: float = 60.0,\n        ext_stats: Dict[str, Any] = {},\n        ext_delta: Dict[str, Any] = {},\n        ext_timing_enabled: bool = False,\n    ):\n        self.stats: StatsCollector = stats\n        self.interval: float = interval\n        self.multiplier: float = 60.0 / self.interval\n        self.task: Optional[task.LoopingCall] = None\n        self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n        self.ext_stats_enabled: bool = bool(ext_stats)\n        self.ext_stats_include: List[str] = ext_stats.get(\"include\", [])\n        self.ext_stats_exclude: List[str] = ext_stats.get(\"exclude\", [])\n        self.ext_delta_enabled: bool = bool(ext_delta)\n        self.ext_delta_include: List[str] = ext_delta.get(\"include\", [])\n        self.ext_delta_exclude: List[str] = ext_delta.get(\"exclude\", [])\n        self.ext_timing_enabled: bool = ext_timing_enabled\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n        if not interval:\n            raise NotConfigured\n        try:\n            ext_stats: Optional[Dict[str, Any]] = crawler.settings.getdict(\n                \"PERIODIC_LOG_STATS\"\n            )\n        except (TypeError, ValueError):\n            ext_stats = (\n                {\"enabled\": True}\n                if crawler.settings.getbool(\"PERIODIC_LOG_STATS\")\n                else None\n            )\n        try:\n            ext_delta: Optional[Dict[str, Any]] = crawler.settings.getdict(\n                \"PERIODIC_LOG_DELTA\"\n            )\n        except (TypeError, ValueError):\n            ext_delta = (\n                {\"enabled\": True}\n                if crawler.settings.getbool(\"PERIODIC_LOG_DELTA\")\n                else None\n            )\n\n        ext_timing_enabled: bool = crawler.settings.getbool(\n            \"PERIODIC_LOG_TIMING_ENABLED\", False\n        )\n        if not (ext_stats or ext_delta or ext_timing_enabled):\n            raise NotConfigured\n        assert crawler.stats\n        assert ext_stats is not None\n        assert ext_delta is not None\n        o = cls(\n            crawler.stats,\n            interval,\n            ext_stats,\n            ext_delta,\n            ext_timing_enabled,\n        )\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.time_prev: datetime = datetime.now(tz=timezone.utc)\n        self.delta_prev: Dict[str, Union[int, float]] = {}\n        self.stats_prev: Dict[str, Union[int, float]] = {}\n\n        self.task = task.LoopingCall(self.log)\n        self.task.start(self.interval)\n\n    def log(self) -> None:\n        data: Dict[str, Any] = {}\n        if self.ext_timing_enabled:\n            data.update(self.log_timing())\n        if self.ext_delta_enabled:\n            data.update(self.log_delta())\n        if self.ext_stats_enabled:\n            data.update(self.log_crawler_stats())\n        logger.info(self.encoder.encode(data))\n\n    def log_delta(self) -> Dict[str, Any]:\n        num_stats: Dict[str, Union[int, float]] = {\n            k: v\n            for k, v in self.stats._stats.items()\n            if isinstance(v, (int, float))\n            and self.param_allowed(k, self.ext_delta_include, self.ext_delta_exclude)\n        }\n        delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}\n        self.delta_prev = num_stats\n        return {\"delta\": delta}\n\n    def log_timing(self) -> Dict[str, Any]:\n        now = datetime.now(tz=timezone.utc)\n        time = {\n            \"log_interval\": self.interval,\n            \"start_time\": self.stats._stats[\"start_time\"],\n            \"utcnow\": now,\n            \"log_interval_real\": (now - self.time_prev).total_seconds(),\n            \"elapsed\": (now - self.stats._stats[\"start_time\"]).total_seconds(),\n        }\n        self.time_prev = now\n        return {\"time\": time}\n\n    def log_crawler_stats(self) -> Dict[str, Any]:\n        stats = {\n            k: v\n            for k, v in self.stats._stats.items()\n            if self.param_allowed(k, self.ext_stats_include, self.ext_stats_exclude)\n        }\n        return {\"stats\": stats}\n\n    def param_allowed(\n        self, stat_name: str, include: List[str], exclude: List[str]\n    ) -> bool:\n        if not include and not exclude:\n            return True\n        for p in exclude:\n            if p in stat_name:\n                return False\n        if exclude and not include:\n            return True\n        for p in include:\n            if p in stat_name:\n                return True\n        return False\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        self.log()\n        if self.task and self.task.running:\n            self.task.stop()\n", "scrapy/extensions/memdebug.py": "\"\"\"\nMemoryDebugger extension\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport gc\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.trackref import live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass MemoryDebugger:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"MEMDEBUG_ENABLED\"):\n            raise NotConfigured\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        gc.collect()\n        self.stats.set_value(\n            \"memdebug/gc_garbage_count\", len(gc.garbage), spider=spider\n        )\n        for cls, wdict in live_refs.items():\n            if not wdict:\n                continue\n            self.stats.set_value(\n                f\"memdebug/live_refs/{cls.__name__}\", len(wdict), spider=spider\n            )\n", "scrapy/extensions/__init__.py": "", "scrapy/extensions/telnet.py": "\"\"\"\nScrapy Telnet Console extension\n\nSee documentation in docs/topics/telnetconsole.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport binascii\nimport logging\nimport os\nimport pprint\nimport traceback\nfrom typing import TYPE_CHECKING, Any, Dict, List\n\nfrom twisted.internet import protocol\nfrom twisted.internet.tcp import Port\n\ntry:\n    from twisted.conch import manhole, telnet\n    from twisted.conch.insults import insults\n\n    TWISTED_CONCH_AVAILABLE = True\nexcept (ImportError, SyntaxError):\n    _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n    TWISTED_CONCH_AVAILABLE = False\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.decorators import defers\nfrom scrapy.utils.engine import print_engine_status\nfrom scrapy.utils.reactor import listen_tcp\nfrom scrapy.utils.trackref import print_live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\nlogger = logging.getLogger(__name__)\n\n# signal to update telnet variables\n# args: telnet_vars\nupdate_telnet_vars = object()\n\n\nclass TelnetConsole(protocol.ServerFactory):\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n            raise NotConfigured\n        if not TWISTED_CONCH_AVAILABLE:\n            raise NotConfigured(\n                \"TELNETCONSOLE_ENABLED setting is True but required twisted \"\n                \"modules failed to import:\\n\" + _TWISTED_CONCH_TRACEBACK\n            )\n        self.crawler: Crawler = crawler\n        self.noisy: bool = False\n        self.portrange: List[int] = [\n            int(x) for x in crawler.settings.getlist(\"TELNETCONSOLE_PORT\")\n        ]\n        self.host: str = crawler.settings[\"TELNETCONSOLE_HOST\"]\n        self.username: str = crawler.settings[\"TELNETCONSOLE_USERNAME\"]\n        self.password: str = crawler.settings[\"TELNETCONSOLE_PASSWORD\"]\n\n        if not self.password:\n            self.password = binascii.hexlify(os.urandom(8)).decode(\"utf8\")\n            logger.info(\"Telnet Password: %s\", self.password)\n\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def start_listening(self) -> None:\n        self.port: Port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\n            \"Telnet console listening on %(host)s:%(port)d\",\n            {\"host\": h.host, \"port\": h.port},\n            extra={\"crawler\": self.crawler},\n        )\n\n    def stop_listening(self) -> None:\n        self.port.stopListening()\n\n    def protocol(self) -> telnet.TelnetTransport:  # type: ignore[override]\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n\n            @defers\n            def login(self_, credentials, mind, *interfaces):\n                if not (\n                    credentials.username == self.username.encode(\"utf8\")\n                    and credentials.checkPassword(self.password.encode(\"utf8\"))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol, manhole.Manhole, self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())\n\n    def _get_telnet_vars(self) -> Dict[str, Any]:\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        assert self.crawler.engine\n        telnet_vars: Dict[str, Any] = {\n            \"engine\": self.crawler.engine,\n            \"spider\": self.crawler.engine.spider,\n            \"slot\": self.crawler.engine.slot,\n            \"crawler\": self.crawler,\n            \"extensions\": self.crawler.extensions,\n            \"stats\": self.crawler.stats,\n            \"settings\": self.crawler.settings,\n            \"est\": lambda: print_engine_status(self.crawler.engine),\n            \"p\": pprint.pprint,\n            \"prefs\": print_live_refs,\n            \"help\": \"This is Scrapy telnet console. For more info see: \"\n            \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars\n", "scrapy/templates/project/module/__init__.py": "", "scrapy/templates/project/module/spiders/__init__.py": "# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n", "scrapy/pipelines/files.py": "\"\"\"\nFiles Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport functools\nimport hashlib\nimport logging\nimport mimetypes\nimport time\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom ftplib import FTP\nfrom io import BytesIO\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    List,\n    NoReturn,\n    Optional,\n    Protocol,\n    Set,\n    Type,\n    TypedDict,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nfrom itemadapter import ItemAdapter\nfrom twisted.internet import defer, threads\nfrom twisted.internet.defer import Deferred\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.datatypes import CaseInsensitiveDict\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.request import referer_str\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _to_string(path: Union[str, PathLike[str]]) -> str:\n    return str(path)  # convert a Path object to string\n\n\ndef _md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> _md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()  # nosec\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\nclass FileException(Exception):\n    \"\"\"General media error exception\"\"\"\n\n\nclass StatInfo(TypedDict, total=False):\n    checksum: str\n    last_modified: float\n\n\nclass FilesStoreProtocol(Protocol):\n    def __init__(self, basedir: str): ...\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Optional[Deferred[Any]]: ...\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Union[StatInfo, Deferred[StatInfo]]: ...\n\n\nclass FSFilesStore:\n    def __init__(self, basedir: Union[str, PathLike[str]]):\n        basedir = _to_string(basedir)\n        if \"://\" in basedir:\n            basedir = basedir.split(\"://\", 1)[1]\n        self.basedir: str = basedir\n        self._mkdir(Path(self.basedir))\n        self.created_directories: DefaultDict[MediaPipeline.SpiderInfo, Set[str]] = (\n            defaultdict(set)\n        )\n\n    def persist_file(\n        self,\n        path: Union[str, PathLike[str]],\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> None:\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(absolute_path.parent, info)\n        absolute_path.write_bytes(buf.getvalue())\n\n    def stat_file(\n        self, path: Union[str, PathLike[str]], info: MediaPipeline.SpiderInfo\n    ) -> StatInfo:\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = absolute_path.stat().st_mtime\n        except OSError:\n            return {}\n\n        with absolute_path.open(\"rb\") as f:\n            checksum = _md5sum(f)\n\n        return {\"last_modified\": last_modified, \"checksum\": checksum}\n\n    def _get_filesystem_path(self, path: Union[str, PathLike[str]]) -> Path:\n        path_comps = _to_string(path).split(\"/\")\n        return Path(self.basedir, *path_comps)\n\n    def _mkdir(\n        self, dirname: Path, domain: Optional[MediaPipeline.SpiderInfo] = None\n    ) -> None:\n        seen: Set[str] = self.created_directories[domain] if domain else set()\n        if str(dirname) not in seen:\n            if not dirname.exists():\n                dirname.mkdir(parents=True)\n            seen.add(str(dirname))\n\n\nclass S3FilesStore:\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n    AWS_SESSION_TOKEN = None\n    AWS_ENDPOINT_URL = None\n    AWS_REGION_NAME = None\n    AWS_USE_SSL = None\n    AWS_VERIFY = None\n\n    POLICY = \"private\"  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n    HEADERS = {\n        \"Cache-Control\": \"max-age=172800\",\n    }\n\n    def __init__(self, uri: str):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n        import botocore.session\n\n        session = botocore.session.get_session()\n        self.s3_client = session.create_client(\n            \"s3\",\n            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n            aws_session_token=self.AWS_SESSION_TOKEN,\n            endpoint_url=self.AWS_ENDPOINT_URL,\n            region_name=self.AWS_REGION_NAME,\n            use_ssl=self.AWS_USE_SSL,\n            verify=self.AWS_VERIFY,\n        )\n        if not uri.startswith(\"s3://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n        self.bucket, self.prefix = uri[5:].split(\"/\", 1)\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _onsuccess(boto_key: Dict[str, Any]) -> StatInfo:\n            checksum = boto_key[\"ETag\"].strip('\"')\n            last_modified = boto_key[\"LastModified\"]\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {\"checksum\": checksum, \"last_modified\": modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)\n\n    def _get_boto_key(self, path: str) -> Deferred[Dict[str, Any]]:\n        key_name = f\"{self.prefix}{path}\"\n        return cast(\n            \"Deferred[Dict[str, Any]]\",\n            threads.deferToThread(\n                self.s3_client.head_object, Bucket=self.bucket, Key=key_name  # type: ignore[attr-defined]\n            ),\n        )\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Deferred[Any]:\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = f\"{self.prefix}{path}\"\n        buf.seek(0)\n        extra = self._headers_to_botocore_kwargs(self.HEADERS)\n        if headers:\n            extra.update(self._headers_to_botocore_kwargs(headers))\n        return threads.deferToThread(\n            self.s3_client.put_object,  # type: ignore[attr-defined]\n            Bucket=self.bucket,\n            Key=key_name,\n            Body=buf,\n            Metadata={k: str(v) for k, v in (meta or {}).items()},\n            ACL=self.POLICY,\n            **extra,\n        )\n\n    def _headers_to_botocore_kwargs(self, headers: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaseInsensitiveDict(\n            {\n                \"Content-Type\": \"ContentType\",\n                \"Cache-Control\": \"CacheControl\",\n                \"Content-Disposition\": \"ContentDisposition\",\n                \"Content-Encoding\": \"ContentEncoding\",\n                \"Content-Language\": \"ContentLanguage\",\n                \"Content-Length\": \"ContentLength\",\n                \"Content-MD5\": \"ContentMD5\",\n                \"Expires\": \"Expires\",\n                \"X-Amz-Grant-Full-Control\": \"GrantFullControl\",\n                \"X-Amz-Grant-Read\": \"GrantRead\",\n                \"X-Amz-Grant-Read-ACP\": \"GrantReadACP\",\n                \"X-Amz-Grant-Write-ACP\": \"GrantWriteACP\",\n                \"X-Amz-Object-Lock-Legal-Hold\": \"ObjectLockLegalHoldStatus\",\n                \"X-Amz-Object-Lock-Mode\": \"ObjectLockMode\",\n                \"X-Amz-Object-Lock-Retain-Until-Date\": \"ObjectLockRetainUntilDate\",\n                \"X-Amz-Request-Payer\": \"RequestPayer\",\n                \"X-Amz-Server-Side-Encryption\": \"ServerSideEncryption\",\n                \"X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id\": \"SSEKMSKeyId\",\n                \"X-Amz-Server-Side-Encryption-Context\": \"SSEKMSEncryptionContext\",\n                \"X-Amz-Server-Side-Encryption-Customer-Algorithm\": \"SSECustomerAlgorithm\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key\": \"SSECustomerKey\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key-Md5\": \"SSECustomerKeyMD5\",\n                \"X-Amz-Storage-Class\": \"StorageClass\",\n                \"X-Amz-Tagging\": \"Tagging\",\n                \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n            }\n        )\n        extra: Dict[str, Any] = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n            else:\n                extra[kwarg] = value\n        return extra\n\n\nclass GCSFilesStore:\n    GCS_PROJECT_ID = None\n\n    CACHE_CONTROL = \"max-age=172800\"\n\n    # The bucket's default object ACL will be applied to the object.\n    # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n    POLICY = None\n\n    def __init__(self, uri: str):\n        from google.cloud import storage\n\n        client = storage.Client(project=self.GCS_PROJECT_ID)\n        bucket, prefix = uri[5:].split(\"/\", 1)\n        self.bucket = client.bucket(bucket)\n        self.prefix: str = prefix\n        permissions = self.bucket.test_iam_permissions(\n            [\"storage.objects.get\", \"storage.objects.create\"]\n        )\n        if \"storage.objects.get\" not in permissions:\n            logger.warning(\n                \"No 'storage.objects.get' permission for GSC bucket %(bucket)s. \"\n                \"Checking if files are up to date will be impossible. Files will be downloaded every time.\",\n                {\"bucket\": bucket},\n            )\n        if \"storage.objects.create\" not in permissions:\n            logger.error(\n                \"No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!\",\n                {\"bucket\": bucket},\n            )\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _onsuccess(blob) -> StatInfo:\n            if blob:\n                checksum = base64.b64decode(blob.md5_hash).hex()\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {\"checksum\": checksum, \"last_modified\": last_modified}\n            return {}\n\n        blob_path = self._get_blob_path(path)\n        return cast(\n            Deferred[StatInfo],\n            threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n                _onsuccess\n            ),\n        )\n\n    def _get_content_type(self, headers: Optional[Dict[str, str]]) -> str:\n        if headers and \"Content-Type\" in headers:\n            return headers[\"Content-Type\"]\n        return \"application/octet-stream\"\n\n    def _get_blob_path(self, path: str) -> str:\n        return self.prefix + path\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Deferred[Any]:\n        blob_path = self._get_blob_path(path)\n        blob = self.bucket.blob(blob_path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return threads.deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY,\n        )\n\n\nclass FTPFilesStore:\n    FTP_USERNAME: Optional[str] = None\n    FTP_PASSWORD: Optional[str] = None\n    USE_ACTIVE_MODE: Optional[bool] = None\n\n    def __init__(self, uri: str):\n        if not uri.startswith(\"ftp://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 'ftp'\")\n        u = urlparse(uri)\n        assert u.port\n        assert u.hostname\n        self.port: int = u.port\n        self.host: str = u.hostname\n        self.port = int(u.port or 21)\n        assert self.FTP_USERNAME\n        assert self.FTP_PASSWORD\n        self.username: str = u.username or self.FTP_USERNAME\n        self.password: str = u.password or self.FTP_PASSWORD\n        self.basedir: str = u.path.rstrip(\"/\")\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Deferred[Any]:\n        path = f\"{self.basedir}/{path}\"\n        return threads.deferToThread(\n            ftp_store_file,\n            path=path,\n            file=buf,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.USE_ACTIVE_MODE,\n        )\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _stat_file(path: str) -> StatInfo:\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()  # nosec\n                ftp.retrbinary(f\"RETR {file_path}\", m.update)\n                return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n\n        return cast(\"Deferred[StatInfo]\", threads.deferToThread(_stat_file, path))\n\n\nclass FilesPipeline(MediaPipeline):\n    \"\"\"Abstract pipeline that implement the file downloading\n\n    This pipeline tries to minimize network transfers and file processing,\n    doing stat of the files and determining if file is new, up-to-date or\n    expired.\n\n    ``new`` files are those that pipeline never processed and needs to be\n        downloaded from supplier site the first time.\n\n    ``uptodate`` files are the ones that the pipeline processed and are still\n        valid files.\n\n    ``expired`` files are those that pipeline already processed but the last\n        modification was made long time ago, so a reprocessing is recommended to\n        refresh it in case of change.\n\n    \"\"\"\n\n    MEDIA_NAME: str = \"file\"\n    EXPIRES: int = 90\n    STORE_SCHEMES: Dict[str, Type[FilesStoreProtocol]] = {\n        \"\": FSFilesStore,\n        \"file\": FSFilesStore,\n        \"s3\": S3FilesStore,\n        \"gs\": GCSFilesStore,\n        \"ftp\": FTPFilesStore,\n    }\n    DEFAULT_FILES_URLS_FIELD: str = \"file_urls\"\n    DEFAULT_FILES_RESULT_FIELD: str = \"files\"\n\n    def __init__(\n        self,\n        store_uri: Union[str, PathLike[str]],\n        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n        settings: Union[Settings, Dict[str, Any], None] = None,\n    ):\n        store_uri = _to_string(store_uri)\n        if not store_uri:\n            raise NotConfigured\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        cls_name = \"FilesPipeline\"\n        self.store: FilesStoreProtocol = self._get_store(store_uri)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=cls_name, settings=settings\n        )\n        self.expires: int = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field: str = settings.get(\n            resolve(\"FILES_URLS_FIELD\"), self.FILES_URLS_FIELD\n        )\n        self.files_result_field: str = settings.get(\n            resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n        )\n\n        super().__init__(download_func=download_func, settings=settings)\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -> Self:\n        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n        s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n\n        gcs_store: Type[GCSFilesStore] = cast(\n            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n        )\n        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n        gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n\n        ftp_store: Type[FTPFilesStore] = cast(\n            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n        )\n        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n        store_uri = settings[\"FILES_STORE\"]\n        return cls(store_uri, settings=settings)\n\n    def _get_store(self, uri: str) -> FilesStoreProtocol:\n        if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n            scheme = \"file\"\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)\n\n    def media_to_download(\n        self, request: Request, info: MediaPipeline.SpiderInfo, *, item: Any = None\n    ) -> Deferred[Optional[FileInfo]]:\n        def _onsuccess(result: StatInfo) -> Optional[FileInfo]:\n            if not result:\n                return None  # returning None force download\n\n            last_modified = result.get(\"last_modified\", None)\n            if not last_modified:\n                return None  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return None  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                \"File (uptodate): Downloaded %(medianame)s from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"medianame\": self.MEDIA_NAME, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            self.inc_stats(info.spider, \"uptodate\")\n\n            checksum = result.get(\"checksum\", None)\n            return {\n                \"url\": request.url,\n                \"path\": path,\n                \"checksum\": checksum,\n                \"status\": \"uptodate\",\n            }\n\n        path = self.file_path(request, info=info, item=item)\n        # defer.maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n        dfd: Deferred[StatInfo] = defer.maybeDeferred(self.store.stat_file, path, info)  # type: ignore[arg-type]\n        dfd2: Deferred[Optional[FileInfo]] = dfd.addCallback(_onsuccess)\n        dfd2.addErrback(lambda _: None)\n        dfd2.addErrback(\n            lambda f: logger.error(\n                self.__class__.__name__ + \".store.stat_file\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": info.spider},\n            )\n        )\n        return dfd2\n\n    def media_failed(\n        self, failure: Failure, request: Request, info: MediaPipeline.SpiderInfo\n    ) -> NoReturn:\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                \"File (unknown-error): Error downloading %(medianame)s from \"\n                \"%(request)s referred in <%(referer)s>: %(exception)s\",\n                {\n                    \"medianame\": self.MEDIA_NAME,\n                    \"request\": request,\n                    \"referer\": referer,\n                    \"exception\": failure.value,\n                },\n                extra={\"spider\": info.spider},\n            )\n\n        raise FileException\n\n    def media_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> FileInfo:\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                \"File (code: %(status)s): Error downloading file from \"\n                \"%(request)s referred in <%(referer)s>\",\n                {\"status\": response.status, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"download-error\")\n\n        if not response.body:\n            logger.warning(\n                \"File (empty-content): Empty file from %(request)s referred \"\n                \"in <%(referer)s>: no-content\",\n                {\"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"empty-content\")\n\n        status = \"cached\" if \"cached\" in response.flags else \"downloaded\"\n        logger.debug(\n            \"File (%(status)s): Downloaded file from %(request)s referred in \"\n            \"<%(referer)s>\",\n            {\"status\": status, \"request\": request, \"referer\": referer},\n            extra={\"spider\": info.spider},\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info, item=item)\n            checksum = self.file_downloaded(response, request, info, item=item)\n        except FileException as exc:\n            logger.warning(\n                \"File (error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>: %(errormsg)s\",\n                {\"request\": request, \"referer\": referer, \"errormsg\": str(exc)},\n                extra={\"spider\": info.spider},\n                exc_info=True,\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                \"File (unknown-error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"request\": request, \"referer\": referer},\n                exc_info=True,\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(str(exc))\n\n        return {\n            \"url\": request.url,\n            \"path\": path,\n            \"checksum\": checksum,\n            \"status\": status,\n        }\n\n    def inc_stats(self, spider: Spider, status: str) -> None:\n        assert spider.crawler.stats\n        spider.crawler.stats.inc_value(\"file_count\", spider=spider)\n        spider.crawler.stats.inc_value(f\"file_status_count/{status}\", spider=spider)\n\n    # Overridable Interface\n    def get_media_requests(\n        self, item: Any, info: MediaPipeline.SpiderInfo\n    ) -> List[Request]:\n        urls = ItemAdapter(item).get(self.files_urls_field, [])\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def file_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        path = self.file_path(request, response=response, info=info, item=item)\n        buf = BytesIO(response.body)\n        checksum = _md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum\n\n    def item_completed(\n        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n    ) -> Any:\n        with suppress(KeyError):\n            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(\n        self,\n        request: Request,\n        response: Optional[Response] = None,\n        info: Optional[MediaPipeline.SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n        media_ext = Path(request.url).suffix\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = \"\"\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = cast(str, mimetypes.guess_extension(media_type))\n        return f\"full/{media_guid}{media_ext}\"\n", "scrapy/pipelines/media.py": "from __future__ import annotations\n\nimport functools\nimport logging\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    List,\n    Literal,\n    NoReturn,\n    Optional,\n    Set,\n    Tuple,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred, DeferredList\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.http.request import NO_CALLBACK, Request\nfrom scrapy.settings import Settings\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.defer import defer_result, mustbe_deferred\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.request import RequestFingerprinter\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n_T = TypeVar(\"_T\")\n\n\nclass FileInfo(TypedDict):\n    url: str\n    path: str\n    checksum: Optional[str]\n    status: str\n\n\nFileInfoOrError = Union[Tuple[Literal[True], FileInfo], Tuple[Literal[False], Failure]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MediaPipeline(ABC):\n    crawler: Crawler\n    _fingerprinter: RequestFingerprinter\n\n    LOG_FAILED_RESULTS: bool = True\n\n    class SpiderInfo:\n        def __init__(self, spider: Spider):\n            self.spider: Spider = spider\n            self.downloading: Set[bytes] = set()\n            self.downloaded: Dict[bytes, Union[FileInfo, Failure]] = {}\n            self.waiting: DefaultDict[bytes, List[Deferred[FileInfo]]] = defaultdict(\n                list\n            )\n\n    def __init__(\n        self,\n        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n        settings: Union[Settings, Dict[str, Any], None] = None,\n    ):\n        self.download_func = download_func\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n        )\n        self.allow_redirects: bool = settings.getbool(\n            resolve(\"MEDIA_ALLOW_REDIRECTS\"), False\n        )\n        self._handle_statuses(self.allow_redirects)\n\n    def _handle_statuses(self, allow_redirects: bool) -> None:\n        self.handle_httpstatus_list = None\n        if allow_redirects:\n            self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n\n    def _key_for_pipe(\n        self,\n        key: str,\n        base_class_name: Optional[str] = None,\n        settings: Optional[Settings] = None,\n    ) -> str:\n        class_name = self.__class__.__name__\n        formatted_key = f\"{class_name.upper()}_{key}\"\n        if (\n            not base_class_name\n            or class_name == base_class_name\n            or settings\n            and not settings.get(formatted_key)\n        ):\n            return key\n        return formatted_key\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        pipe: Self\n        try:\n            pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n        except AttributeError:\n            pipe = cls()\n        pipe.crawler = crawler\n        assert crawler.request_fingerprinter\n        pipe._fingerprinter = crawler.request_fingerprinter\n        return pipe\n\n    def open_spider(self, spider: Spider) -> None:\n        self.spiderinfo = self.SpiderInfo(spider)\n\n    def process_item(\n        self, item: Any, spider: Spider\n    ) -> Deferred[List[FileInfoOrError]]:\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info, item) for r in requests]\n        dfd = cast(\n            \"Deferred[List[FileInfoOrError]]\", DeferredList(dlist, consumeErrors=True)\n        )\n        return dfd.addCallback(self.item_completed, item, info)\n\n    def _process_request(\n        self, request: Request, info: SpiderInfo, item: Any\n    ) -> Deferred[FileInfo]:\n        fp = self._fingerprinter.fingerprint(request)\n        eb = request.errback\n        request.callback = NO_CALLBACK\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            d = defer_result(info.downloaded[fp])\n            if eb:\n                d.addErrback(eb)\n            return d\n\n        # Otherwise, wait for result\n        wad: Deferred[FileInfo] = Deferred()\n        if eb:\n            wad.addErrback(eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd: Deferred[Optional[FileInfo]] = mustbe_deferred(\n            self.media_to_download, request, info, item=item\n        )\n        dfd2: Deferred[FileInfo] = dfd.addCallback(\n            self._check_media_to_download, request, info, item=item\n        )\n        dfd2.addErrback(self._log_exception)\n        dfd2.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        return dfd2.addBoth(lambda _: wad)  # it must return wad at last\n\n    def _log_exception(self, result: Failure) -> Failure:\n        logger.exception(result)\n        return result\n\n    def _modify_media_request(self, request: Request) -> None:\n        if self.handle_httpstatus_list:\n            request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n        else:\n            request.meta[\"handle_httpstatus_all\"] = True\n\n    def _check_media_to_download(\n        self, result: Optional[FileInfo], request: Request, info: SpiderInfo, item: Any\n    ) -> Union[FileInfo, Deferred[FileInfo]]:\n        if result is not None:\n            return result\n        dfd: Deferred[Response]\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n        else:\n            self._modify_media_request(request)\n            assert self.crawler.engine\n            dfd = self.crawler.engine.download(request)\n        dfd2: Deferred[FileInfo] = dfd.addCallback(\n            self.media_downloaded, request, info, item=item\n        )\n        dfd2.addErrback(self.media_failed, request, info)\n        return dfd2\n\n    def _cache_result_and_execute_waiters(\n        self, result: Union[FileInfo, Failure], fp: bytes, info: SpiderInfo\n    ) -> None:\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n\n            # This code fixes a memory leak by avoiding to keep references to\n            # the Request and Response objects on the Media Pipeline cache.\n            #\n            # What happens when the media_downloaded callback raises an\n            # exception, for example a FileException('download-error') when\n            # the Response status code is not 200 OK, is that the original\n            # StopIteration exception (which in turn contains the failed\n            # Response and by extension, the original Request) gets encapsulated\n            # within the FileException context.\n            #\n            # Originally, Scrapy was using twisted.internet.defer.returnValue\n            # inside functions decorated with twisted.internet.defer.inlineCallbacks,\n            # encapsulating the returned Response in a _DefGen_Return exception\n            # instead of a StopIteration.\n            #\n            # To avoid keeping references to the Response and therefore Request\n            # objects on the Media Pipeline cache, we should wipe the context of\n            # the encapsulated exception when it is a StopIteration instance\n            #\n            # This problem does not occur in Python 2.7 since we don't have\n            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n            context = getattr(result.value, \"__context__\", None)\n            if isinstance(context, StopIteration):\n                result.value.__context__ = None\n\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)\n\n    # Overridable Interface\n    @abstractmethod\n    def media_to_download(\n        self, request: Request, info: SpiderInfo, *, item: Any = None\n    ) -> Deferred[Optional[FileInfo]]:\n        \"\"\"Check request before starting download\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_media_requests(self, item: Any, info: SpiderInfo) -> List[Request]:\n        \"\"\"Returns the media requests to download\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def media_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> FileInfo:\n        \"\"\"Handler for success downloads\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def media_failed(\n        self, failure: Failure, request: Request, info: SpiderInfo\n    ) -> NoReturn:\n        \"\"\"Handler for failed downloads\"\"\"\n        raise NotImplementedError()\n\n    def item_completed(\n        self, results: List[FileInfoOrError], item: Any, info: SpiderInfo\n    ) -> Any:\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    assert isinstance(value, Failure)\n                    logger.error(\n                        \"%(class)s found errors processing %(item)s\",\n                        {\"class\": self.__class__.__name__, \"item\": item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={\"spider\": info.spider},\n                    )\n        return item\n\n    @abstractmethod\n    def file_path(\n        self,\n        request: Request,\n        response: Optional[Response] = None,\n        info: Optional[SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        \"\"\"Returns the path where downloaded media should be stored\"\"\"\n        raise NotImplementedError()\n", "scrapy/pipelines/__init__.py": "\"\"\"\nItem pipeline\n\nSee documentation in docs/item-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, List\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Spider\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import deferred_f_from_coro_f\n\n\nclass ItemPipelineManager(MiddlewareManager):\n    component_name = \"item pipeline\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n\n    def _add_middleware(self, pipe: Any) -> None:\n        super()._add_middleware(pipe)\n        if hasattr(pipe, \"process_item\"):\n            self.methods[\"process_item\"].append(\n                deferred_f_from_coro_f(pipe.process_item)\n            )\n\n    def process_item(self, item: Any, spider: Spider) -> Deferred[Any]:\n        return self._process_chain(\"process_item\", item, spider)\n", "scrapy/pipelines/images.py": "\"\"\"\nImages Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport hashlib\nimport warnings\nfrom contextlib import suppress\nfrom io import BytesIO\nfrom os import PathLike\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nfrom itemadapter import ItemAdapter\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.files import (\n    FileException,\n    FilesPipeline,\n    FTPFilesStore,\n    GCSFilesStore,\n    S3FilesStore,\n    _md5sum,\n)\nfrom scrapy.pipelines.media import FileInfoOrError, MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import get_func_args, to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from PIL import Image\n    from typing_extensions import Self\n\n\nclass NoimagesDrop(DropItem):\n    \"\"\"Product with no images exception\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        warnings.warn(\n            \"The NoimagesDrop class is deprecated\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        super().__init__(*args, **kwargs)\n\n\nclass ImageException(FileException):\n    \"\"\"General image error exception\"\"\"\n\n\nclass ImagesPipeline(FilesPipeline):\n    \"\"\"Abstract pipeline that implement the image thumbnail generation logic\"\"\"\n\n    MEDIA_NAME: str = \"image\"\n\n    # Uppercase attributes kept for backward compatibility with code that subclasses\n    # ImagesPipeline. They may be overridden by settings.\n    MIN_WIDTH: int = 0\n    MIN_HEIGHT: int = 0\n    EXPIRES: int = 90\n    THUMBS: Dict[str, Tuple[int, int]] = {}\n    DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n    DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n\n    def __init__(\n        self,\n        store_uri: Union[str, PathLike[str]],\n        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n        settings: Union[Settings, Dict[str, Any], None] = None,\n    ):\n        try:\n            from PIL import Image\n\n            self._Image = Image\n        except ImportError:\n            raise NotConfigured(\n                \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n            )\n\n        super().__init__(store_uri, settings=settings, download_func=download_func)\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        resolve = functools.partial(\n            self._key_for_pipe,\n            base_class_name=\"ImagesPipeline\",\n            settings=settings,\n        )\n        self.expires: int = settings.getint(resolve(\"IMAGES_EXPIRES\"), self.EXPIRES)\n\n        if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n            self.IMAGES_RESULT_FIELD: str = self.DEFAULT_IMAGES_RESULT_FIELD\n        if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n            self.IMAGES_URLS_FIELD: str = self.DEFAULT_IMAGES_URLS_FIELD\n\n        self.images_urls_field: str = settings.get(\n            resolve(\"IMAGES_URLS_FIELD\"), self.IMAGES_URLS_FIELD\n        )\n        self.images_result_field: str = settings.get(\n            resolve(\"IMAGES_RESULT_FIELD\"), self.IMAGES_RESULT_FIELD\n        )\n        self.min_width: int = settings.getint(\n            resolve(\"IMAGES_MIN_WIDTH\"), self.MIN_WIDTH\n        )\n        self.min_height: int = settings.getint(\n            resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT\n        )\n        self.thumbs: Dict[str, Tuple[int, int]] = settings.get(\n            resolve(\"IMAGES_THUMBS\"), self.THUMBS\n        )\n\n        self._deprecated_convert_image: Optional[bool] = None\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -> Self:\n        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n\n        gcs_store: Type[GCSFilesStore] = cast(\n            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n        )\n        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n\n        ftp_store: Type[FTPFilesStore] = cast(\n            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n        )\n        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n        store_uri = settings[\"IMAGES_STORE\"]\n        return cls(store_uri, settings=settings)\n\n    def file_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        return self.image_downloaded(response, request, info, item=item)\n\n    def image_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        checksum: Optional[str] = None\n        for path, image, buf in self.get_images(response, request, info, item=item):\n            if checksum is None:\n                buf.seek(0)\n                checksum = _md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path,\n                buf,\n                info,\n                meta={\"width\": width, \"height\": height},\n                headers={\"Content-Type\": \"image/jpeg\"},\n            )\n        assert checksum is not None\n        return checksum\n\n    def get_images(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> Iterable[Tuple[str, Image.Image, BytesIO]]:\n        path = self.file_path(request, response=response, info=info, item=item)\n        orig_image = self._Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\n                \"Image too small \"\n                f\"({width}x{height} < \"\n                f\"{self.min_width}x{self.min_height})\"\n            )\n\n        if self._deprecated_convert_image is None:\n            self._deprecated_convert_image = \"response_body\" not in get_func_args(\n                self.convert_image\n            )\n            if self._deprecated_convert_image:\n                warnings.warn(\n                    f\"{self.__class__.__name__}.convert_image() method overridden in a deprecated way, \"\n                    \"overridden method does not accept response_body argument.\",\n                    category=ScrapyDeprecationWarning,\n                )\n\n        if self._deprecated_convert_image:\n            image, buf = self.convert_image(orig_image)\n        else:\n            image, buf = self.convert_image(\n                orig_image, response_body=BytesIO(response.body)\n            )\n        yield path, image, buf\n\n        for thumb_id, size in self.thumbs.items():\n            thumb_path = self.thumb_path(\n                request, thumb_id, response=response, info=info, item=item\n            )\n            if self._deprecated_convert_image:\n                thumb_image, thumb_buf = self.convert_image(image, size)\n            else:\n                thumb_image, thumb_buf = self.convert_image(image, size, buf)\n            yield thumb_path, thumb_image, thumb_buf\n\n    def convert_image(\n        self,\n        image: Image.Image,\n        size: Optional[Tuple[int, int]] = None,\n        response_body: Optional[BytesIO] = None,\n    ) -> Tuple[Image.Image, BytesIO]:\n        if response_body is None:\n            warnings.warn(\n                f\"{self.__class__.__name__}.convert_image() method called in a deprecated way, \"\n                \"method called without response_body argument.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n\n        if image.format in (\"PNG\", \"WEBP\") and image.mode == \"RGBA\":\n            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert(\"RGB\")\n        elif image.mode == \"P\":\n            image = image.convert(\"RGBA\")\n            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert(\"RGB\")\n        elif image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        if size:\n            image = image.copy()\n            try:\n                # Image.Resampling.LANCZOS was added in Pillow 9.1.0\n                # remove this try except block,\n                # when updating the minimum requirements for Pillow.\n                resampling_filter = self._Image.Resampling.LANCZOS\n            except AttributeError:\n                resampling_filter = self._Image.ANTIALIAS  # type: ignore[attr-defined]\n            image.thumbnail(size, resampling_filter)\n        elif response_body is not None and image.format == \"JPEG\":\n            return image, response_body\n\n        buf = BytesIO()\n        image.save(buf, \"JPEG\")\n        return image, buf\n\n    def get_media_requests(\n        self, item: Any, info: MediaPipeline.SpiderInfo\n    ) -> List[Request]:\n        urls = ItemAdapter(item).get(self.images_urls_field, [])\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def item_completed(\n        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n    ) -> Any:\n        with suppress(KeyError):\n            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(\n        self,\n        request: Request,\n        response: Optional[Response] = None,\n        info: Optional[MediaPipeline.SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n        return f\"full/{image_guid}.jpg\"\n\n    def thumb_path(\n        self,\n        request: Request,\n        thumb_id: str,\n        response: Optional[Response] = None,\n        info: Optional[MediaPipeline.SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n        return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n", "scrapy/spidermiddlewares/httperror.py": "\"\"\"\nHttpError Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Iterable, List, Optional\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Response\nfrom scrapy.settings import BaseSettings\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass HttpError(IgnoreRequest):\n    \"\"\"A non-200 response was filtered\"\"\"\n\n    def __init__(self, response: Response, *args: Any, **kwargs: Any):\n        self.response = response\n        super().__init__(*args, **kwargs)\n\n\nclass HttpErrorMiddleware:\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def __init__(self, settings: BaseSettings):\n        self.handle_httpstatus_all: bool = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n        self.handle_httpstatus_list: List[int] = settings.getlist(\n            \"HTTPERROR_ALLOWED_CODES\"\n        )\n\n    def process_spider_input(self, response: Response, spider: Spider) -> None:\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if meta.get(\"handle_httpstatus_all\", False):\n            return\n        if \"handle_httpstatus_list\" in meta:\n            allowed_statuses = meta[\"handle_httpstatus_list\"]\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(\n                spider, \"handle_httpstatus_list\", self.handle_httpstatus_list\n            )\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, \"Ignoring non-200 response\")\n\n    def process_spider_exception(\n        self, response: Response, exception: Exception, spider: Spider\n    ) -> Optional[Iterable[Any]]:\n        if isinstance(exception, HttpError):\n            assert spider.crawler.stats\n            spider.crawler.stats.inc_value(\"httperror/response_ignored_count\")\n            spider.crawler.stats.inc_value(\n                f\"httperror/response_ignored_status_count/{response.status}\"\n            )\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {\"response\": response},\n                extra={\"spider\": spider},\n            )\n            return []\n        return None\n", "scrapy/spidermiddlewares/offsite.py": "\"\"\"\nOffsite Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport re\nimport warnings\nfrom typing import TYPE_CHECKING, Any, AsyncIterable, Iterable, Set\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.httpobj import urlparse_cached\n\nwarnings.warn(\n    \"The scrapy.spidermiddlewares.offsite module is deprecated, use \"\n    \"scrapy.downloadermiddlewares.offsite instead.\",\n    ScrapyDeprecationWarning,\n)\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        return (r for r in result if self._filter(r, spider))\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        async for r in result:\n            if self._filter(r, spider):\n                yield r\n\n    def _filter(self, request: Any, spider: Spider) -> bool:\n        if not isinstance(request, Request):\n            return True\n        if request.dont_filter or self.should_follow(request, spider):\n            return True\n        domain = urlparse_cached(request).hostname\n        if domain and domain not in self.domains_seen:\n            self.domains_seen.add(domain)\n            logger.debug(\n                \"Filtered offsite request to %(domain)r: %(request)s\",\n                {\"domain\": domain, \"request\": request},\n                extra={\"spider\": spider},\n            )\n            self.stats.inc_value(\"offsite/domains\", spider=spider)\n        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n        return False\n\n    def should_follow(self, request: Request, spider: Spider) -> bool:\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or \"\"\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, \"allowed_domains\", None)\n        if not allowed_domains:\n            return re.compile(\"\")  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            if url_pattern.match(domain):\n                message = (\n                    \"allowed_domains accepts only domains, not URLs. \"\n                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message, URLWarning)\n            elif port_pattern.search(domain):\n                message = (\n                    \"allowed_domains accepts only domains without ports. \"\n                    f\"Ignoring entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message, PortWarning)\n            else:\n                domains.append(re.escape(domain))\n        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n        return re.compile(regex)\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n        self.domains_seen: Set[str] = set()\n\n\nclass URLWarning(Warning):\n    pass\n\n\nclass PortWarning(Warning):\n    pass\n", "scrapy/spidermiddlewares/referer.py": "\"\"\"\nRefererMiddleware: populates Request referer field, based on the Response which\noriginated it.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    Dict,\n    Iterable,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.url import strip_url\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nLOCAL_SCHEMES: Tuple[str, ...] = (\n    \"about\",\n    \"blob\",\n    \"data\",\n    \"filesystem\",\n)\n\nPOLICY_NO_REFERRER = \"no-referrer\"\nPOLICY_NO_REFERRER_WHEN_DOWNGRADE = \"no-referrer-when-downgrade\"\nPOLICY_SAME_ORIGIN = \"same-origin\"\nPOLICY_ORIGIN = \"origin\"\nPOLICY_STRICT_ORIGIN = \"strict-origin\"\nPOLICY_ORIGIN_WHEN_CROSS_ORIGIN = \"origin-when-cross-origin\"\nPOLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN = \"strict-origin-when-cross-origin\"\nPOLICY_UNSAFE_URL = \"unsafe-url\"\nPOLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n\n\nclass ReferrerPolicy:\n    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES\n    name: str\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        raise NotImplementedError()\n\n    def stripped_referrer(self, url: str) -> Optional[str]:\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.strip_url(url)\n        return None\n\n    def origin_referrer(self, url: str) -> Optional[str]:\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.origin(url)\n        return None\n\n    def strip_url(self, url: str, origin_only: bool = False) -> Optional[str]:\n        \"\"\"\n        https://www.w3.org/TR/referrer-policy/#strip-url\n\n        If url is null, return no referrer.\n        If url's scheme is a local scheme, then return no referrer.\n        Set url's username to the empty string.\n        Set url's password to null.\n        Set url's fragment to null.\n        If the origin-only flag is true, then:\n            Set url's path to null.\n            Set url's query to null.\n        Return url.\n        \"\"\"\n        if not url:\n            return None\n        return strip_url(\n            url,\n            strip_credentials=True,\n            strip_fragment=True,\n            strip_default_port=True,\n            origin_only=origin_only,\n        )\n\n    def origin(self, url: str) -> Optional[str]:\n        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n        return self.strip_url(url, origin_only=True)\n\n    def potentially_trustworthy(self, url: str) -> bool:\n        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in (\"data\",):\n            return False\n        return self.tls_protected(url)\n\n    def tls_protected(self, url: str) -> bool:\n        return urlparse(url).scheme in (\"https\", \"ftps\")\n\n\nclass NoReferrerPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer\n\n    The simplest policy is \"no-referrer\", which specifies that no referrer information\n    is to be sent along with requests made from a particular request client to any origin.\n    The header will be omitted entirely.\n    \"\"\"\n\n    name: str = POLICY_NO_REFERRER\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        return None\n\n\nclass NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n\n    The \"no-referrer-when-downgrade\" policy sends a full URL along with requests\n    from a TLS-protected environment settings object to a potentially trustworthy URL,\n    and requests from clients which are not TLS-protected to any origin.\n\n    Requests from TLS-protected clients to non-potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n\n    This is a user agent's default behavior, if no policy is otherwise specified.\n    \"\"\"\n\n    name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n            return self.stripped_referrer(response_url)\n        return None\n\n\nclass SameOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin\n\n    The \"same-origin\" policy specifies that a full URL, stripped for use as a referrer,\n    is sent as referrer information when making same-origin requests from a particular request client.\n\n    Cross-origin requests, on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_SAME_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        if self.origin(response_url) == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        return None\n\n\nclass OriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-origin\n\n    The \"origin\" policy specifies that only the ASCII serialization\n    of the origin of the request client is sent as referrer information\n    when making both same-origin requests and cross-origin requests\n    from a particular request client.\n    \"\"\"\n\n    name: str = POLICY_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        return self.origin_referrer(response_url)\n\n\nclass StrictOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin\n\n    The \"strict-origin\" policy sends the ASCII serialization\n    of the origin of the request client when making requests:\n    - from a TLS-protected environment settings object to a potentially trustworthy URL, and\n    - from non-TLS-protected environment settings objects to any origin.\n\n    Requests from TLS-protected request clients to non- potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_STRICT_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        if (\n            self.tls_protected(response_url)\n            and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)\n        return None\n\n\nclass OriginWhenCrossOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin\n\n    The \"origin-when-cross-origin\" policy specifies that a full URL,\n    stripped for use as a referrer, is sent as referrer information\n    when making same-origin requests from a particular request client,\n    and only the ASCII serialization of the origin of the request client\n    is sent as referrer information when making cross-origin requests\n    from a particular request client.\n    \"\"\"\n\n    name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        return origin\n\n\nclass StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin\n\n    The \"strict-origin-when-cross-origin\" policy specifies that a full URL,\n    stripped for use as a referrer, is sent as referrer information\n    when making same-origin requests from a particular request client,\n    and only the ASCII serialization of the origin of the request client\n    when making cross-origin requests:\n\n    - from a TLS-protected environment settings object to a potentially trustworthy URL, and\n    - from non-TLS-protected environment settings objects to any origin.\n\n    Requests from TLS-protected clients to non- potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        if (\n            self.tls_protected(response_url)\n            and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)\n        return None\n\n\nclass UnsafeUrlPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url\n\n    The \"unsafe-url\" policy specifies that a full URL, stripped for use as a referrer,\n    is sent along with both cross-origin requests\n    and same-origin requests made from a particular request client.\n\n    Note: The policy's name doesn't lie; it is unsafe.\n    This policy will leak origins and paths from TLS-protected resources\n    to insecure origins.\n    Carefully consider the impact of setting such a policy for potentially sensitive documents.\n    \"\"\"\n\n    name: str = POLICY_UNSAFE_URL\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        return self.stripped_referrer(response_url)\n\n\nclass DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n    \"\"\"\n    A variant of \"no-referrer-when-downgrade\",\n    with the addition that \"Referer\" is not sent if the parent request was\n    using ``file://`` or ``s3://`` scheme.\n    \"\"\"\n\n    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + (\"file\", \"s3\")\n    name: str = POLICY_SCRAPY_DEFAULT\n\n\n_policy_classes: Dict[str, Type[ReferrerPolicy]] = {\n    p.name: p\n    for p in (\n        NoReferrerPolicy,\n        NoReferrerWhenDowngradePolicy,\n        SameOriginPolicy,\n        OriginPolicy,\n        StrictOriginPolicy,\n        OriginWhenCrossOriginPolicy,\n        StrictOriginWhenCrossOriginPolicy,\n        UnsafeUrlPolicy,\n        DefaultReferrerPolicy,\n    )\n}\n\n# Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string\n_policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n\n\ndef _load_policy_class(\n    policy: str, warning_only: bool = False\n) -> Optional[Type[ReferrerPolicy]]:\n    \"\"\"\n    Expect a string for the path to the policy class,\n    otherwise try to interpret the string as a standard value\n    from https://www.w3.org/TR/referrer-policy/#referrer-policies\n    \"\"\"\n    try:\n        return cast(Type[ReferrerPolicy], load_object(policy))\n    except ValueError:\n        tokens = [token.strip() for token in policy.lower().split(\",\")]\n        # https://www.w3.org/TR/referrer-policy/#parse-referrer-policy-from-header\n        for token in tokens[::-1]:\n            if token in _policy_classes:\n                return _policy_classes[token]\n\n        msg = f\"Could not load referrer policy {policy!r}\"\n        if not warning_only:\n            raise RuntimeError(msg)\n        else:\n            warnings.warn(msg, RuntimeWarning)\n            return None\n\n\nclass RefererMiddleware:\n    def __init__(self, settings: Optional[BaseSettings] = None):\n        self.default_policy: Type[ReferrerPolicy] = DefaultReferrerPolicy\n        if settings is not None:\n            settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n            assert settings_policy\n            self.default_policy = settings_policy\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"REFERER_ENABLED\"):\n            raise NotConfigured\n        mw = cls(crawler.settings)\n\n        # Note: this hook is a bit of a hack to intercept redirections\n        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n\n        return mw\n\n    def policy(\n        self, resp_or_url: Union[Response, str], request: Request\n    ) -> ReferrerPolicy:\n        \"\"\"\n        Determine Referrer-Policy to use from a parent Response (or URL),\n        and a Request to be sent.\n\n        - if a valid policy is set in Request meta, it is used.\n        - if the policy is set in meta but is wrong (e.g. a typo error),\n          the policy from settings is used\n        - if the policy is not set in Request meta,\n          but there is a Referrer-policy header in the parent response,\n          it is used if valid\n        - otherwise, the policy from settings is used.\n        \"\"\"\n        policy_name = request.meta.get(\"referrer_policy\")\n        if policy_name is None:\n            if isinstance(resp_or_url, Response):\n                policy_header = resp_or_url.headers.get(\"Referrer-Policy\")\n                if policy_header is not None:\n                    policy_name = to_unicode(policy_header.decode(\"latin1\"))\n        if policy_name is None:\n            return self.default_policy()\n\n        cls = _load_policy_class(policy_name, warning_only=True)\n        return cls() if cls else self.default_policy()\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        return (self._set_referer(r, response) for r in result)\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        async for r in result:\n            yield self._set_referer(r, response)\n\n    def _set_referer(self, r: Any, response: Response) -> Any:\n        if isinstance(r, Request):\n            referrer = self.policy(response, r).referrer(response.url, r.url)\n            if referrer is not None:\n                r.headers.setdefault(\"Referer\", referrer)\n        return r\n\n    def request_scheduled(self, request: Request, spider: Spider) -> None:\n        # check redirected request to patch \"Referer\" header if necessary\n        redirected_urls = request.meta.get(\"redirect_urls\", [])\n        if redirected_urls:\n            request_referrer = request.headers.get(\"Referer\")\n            # we don't patch the referrer value if there is none\n            if request_referrer is not None:\n                # the request's referrer header value acts as a surrogate\n                # for the parent response URL\n                #\n                # Note: if the 3xx response contained a Referrer-Policy header,\n                #       the information is not available using this hook\n                parent_url = safe_url_string(request_referrer)\n                policy_referrer = self.policy(parent_url, request).referrer(\n                    parent_url, request.url\n                )\n                if policy_referrer != request_referrer.decode(\"latin1\"):\n                    if policy_referrer is None:\n                        request.headers.pop(\"Referer\")\n                    else:\n                        request.headers[\"Referer\"] = policy_referrer\n", "scrapy/spidermiddlewares/__init__.py": "", "scrapy/spidermiddlewares/urllength.py": "\"\"\"\nUrl Length Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import BaseSettings\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass UrlLengthMiddleware:\n    def __init__(self, maxlength: int):\n        self.maxlength: int = maxlength\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n        if not maxlength:\n            raise NotConfigured\n        return cls(maxlength)\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        return (r for r in result if self._filter(r, spider))\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        async for r in result:\n            if self._filter(r, spider):\n                yield r\n\n    def _filter(self, request: Any, spider: Spider) -> bool:\n        if isinstance(request, Request) and len(request.url) > self.maxlength:\n            logger.info(\n                \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                {\"maxlength\": self.maxlength, \"url\": request.url},\n                extra={\"spider\": spider},\n            )\n            assert spider.crawler.stats\n            spider.crawler.stats.inc_value(\n                \"urllength/request_ignored_count\", spider=spider\n            )\n            return False\n        return True\n", "scrapy/spidermiddlewares/depth.py": "\"\"\"\nDepth Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Request, Response\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass DepthMiddleware:\n    def __init__(\n        self,\n        maxdepth: int,\n        stats: StatsCollector,\n        verbose_stats: bool = False,\n        prio: int = 1,\n    ):\n        self.maxdepth = maxdepth\n        self.stats = stats\n        self.verbose_stats = verbose_stats\n        self.prio = prio\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        settings = crawler.settings\n        maxdepth = settings.getint(\"DEPTH_LIMIT\")\n        verbose = settings.getbool(\"DEPTH_STATS_VERBOSE\")\n        prio = settings.getint(\"DEPTH_PRIORITY\")\n        assert crawler.stats\n        return cls(maxdepth, crawler.stats, verbose, prio)\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        self._init_depth(response, spider)\n        return (r for r in result if self._filter(r, response, spider))\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        self._init_depth(response, spider)\n        async for r in result:\n            if self._filter(r, response, spider):\n                yield r\n\n    def _init_depth(self, response: Response, spider: Spider) -> None:\n        # base case (depth=0)\n        if \"depth\" not in response.meta:\n            response.meta[\"depth\"] = 0\n            if self.verbose_stats:\n                self.stats.inc_value(\"request_depth_count/0\", spider=spider)\n\n    def _filter(self, request: Any, response: Response, spider: Spider) -> bool:\n        if not isinstance(request, Request):\n            return True\n        depth = response.meta[\"depth\"] + 1\n        request.meta[\"depth\"] = depth\n        if self.prio:\n            request.priority -= depth * self.prio\n        if self.maxdepth and depth > self.maxdepth:\n            logger.debug(\n                \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                {\"maxdepth\": self.maxdepth, \"requrl\": request.url},\n                extra={\"spider\": spider},\n            )\n            return False\n        if self.verbose_stats:\n            self.stats.inc_value(f\"request_depth_count/{depth}\", spider=spider)\n        self.stats.max_value(\"request_depth_max\", depth, spider=spider)\n        return True\n", "scrapy/linkextractors/__init__.py": "\"\"\"\nscrapy.linkextractors\n\nThis package contains a collection of Link Extractors.\n\nFor more info see docs/topics/link-extractors.rst\n\"\"\"\n\nimport re\nfrom typing import Iterable, Pattern\n\n# common file extensions that are not followed if they occur in links\nIGNORED_EXTENSIONS = [\n    # archives\n    \"7z\",\n    \"7zip\",\n    \"bz2\",\n    \"rar\",\n    \"tar\",\n    \"tar.gz\",\n    \"xz\",\n    \"zip\",\n    # images\n    \"mng\",\n    \"pct\",\n    \"bmp\",\n    \"gif\",\n    \"jpg\",\n    \"jpeg\",\n    \"png\",\n    \"pst\",\n    \"psp\",\n    \"tif\",\n    \"tiff\",\n    \"ai\",\n    \"drw\",\n    \"dxf\",\n    \"eps\",\n    \"ps\",\n    \"svg\",\n    \"cdr\",\n    \"ico\",\n    \"webp\",\n    # audio\n    \"mp3\",\n    \"wma\",\n    \"ogg\",\n    \"wav\",\n    \"ra\",\n    \"aac\",\n    \"mid\",\n    \"au\",\n    \"aiff\",\n    # video\n    \"3gp\",\n    \"asf\",\n    \"asx\",\n    \"avi\",\n    \"mov\",\n    \"mp4\",\n    \"mpg\",\n    \"qt\",\n    \"rm\",\n    \"swf\",\n    \"wmv\",\n    \"m4a\",\n    \"m4v\",\n    \"flv\",\n    \"webm\",\n    # office suites\n    \"xls\",\n    \"xlsm\",\n    \"xlsx\",\n    \"xltm\",\n    \"xltx\",\n    \"potm\",\n    \"potx\",\n    \"ppt\",\n    \"pptm\",\n    \"pptx\",\n    \"pps\",\n    \"doc\",\n    \"docb\",\n    \"docm\",\n    \"docx\",\n    \"dotm\",\n    \"dotx\",\n    \"odt\",\n    \"ods\",\n    \"odg\",\n    \"odp\",\n    # other\n    \"css\",\n    \"pdf\",\n    \"exe\",\n    \"bin\",\n    \"rss\",\n    \"dmg\",\n    \"iso\",\n    \"apk\",\n    \"jar\",\n    \"sh\",\n    \"rb\",\n    \"js\",\n    \"hta\",\n    \"bat\",\n    \"cpl\",\n    \"msi\",\n    \"msp\",\n    \"py\",\n]\n\n\ndef _matches(url: str, regexs: Iterable[Pattern[str]]) -> bool:\n    return any(r.search(url) for r in regexs)\n\n\ndef _is_valid_url(url: str) -> bool:\n    return url.split(\"://\", 1)[0] in {\"http\", \"https\", \"file\", \"ftp\"}\n\n\n# Top-level imports\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n", "scrapy/linkextractors/lxmlhtml.py": "\"\"\"\nLink extractor based on lxml.html\n\"\"\"\n\nimport logging\nimport operator\nfrom functools import partial\nfrom typing import (\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import urljoin, urlparse\n\nfrom lxml import etree  # nosec\nfrom lxml.html import HtmlElement  # nosec\nfrom parsel.csstranslator import HTMLTranslator\nfrom w3lib.html import strip_html5_whitespace\nfrom w3lib.url import canonicalize_url, safe_url_string\n\nfrom scrapy import Selector\nfrom scrapy.http import TextResponse\nfrom scrapy.link import Link\nfrom scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches, re\nfrom scrapy.utils.misc import arg_to_iter, rel_has_nofollow\nfrom scrapy.utils.python import unique as unique_list\nfrom scrapy.utils.response import get_base_url\nfrom scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n\nlogger = logging.getLogger(__name__)\n\n# from lxml/src/lxml/html/__init__.py\nXHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n\n_collect_string_content = etree.XPath(\"string()\")\n\n\ndef _nons(tag: Any) -> Any:\n    if isinstance(tag, str):\n        if tag[0] == \"{\" and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n            return tag.split(\"}\")[-1]\n    return tag\n\n\ndef _identity(x: Any) -> Any:\n    return x\n\n\ndef _canonicalize_link_url(link: Link) -> str:\n    return canonicalize_url(link.url, keep_fragments=True)\n\n\nclass LxmlParserLinkExtractor:\n    def __init__(\n        self,\n        tag: Union[str, Callable[[str], bool]] = \"a\",\n        attr: Union[str, Callable[[str], bool]] = \"href\",\n        process: Optional[Callable[[Any], Any]] = None,\n        unique: bool = False,\n        strip: bool = True,\n        canonicalized: bool = False,\n    ):\n        # mypy doesn't infer types for operator.* and also for partial()\n        self.scan_tag: Callable[[str], bool] = (\n            tag\n            if callable(tag)\n            else cast(Callable[[str], bool], partial(operator.eq, tag))\n        )\n        self.scan_attr: Callable[[str], bool] = (\n            attr\n            if callable(attr)\n            else cast(Callable[[str], bool], partial(operator.eq, attr))\n        )\n        self.process_attr: Callable[[Any], Any] = (\n            process if callable(process) else _identity\n        )\n        self.unique: bool = unique\n        self.strip: bool = strip\n        self.link_key: Callable[[Link], str] = (\n            cast(Callable[[Link], str], operator.attrgetter(\"url\"))\n            if canonicalized\n            else _canonicalize_link_url\n        )\n\n    def _iter_links(\n        self, document: HtmlElement\n    ) -> Iterable[Tuple[HtmlElement, str, str]]:\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield el, attrib, attribs[attrib]\n\n    def _extract_links(\n        self,\n        selector: Selector,\n        response_url: str,\n        response_encoding: str,\n        base_url: str,\n    ) -> List[Link]:\n        links: List[Link] = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                if self.strip:\n                    attr_val = strip_html5_whitespace(attr_val)\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue  # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            try:\n                url = safe_url_string(url, encoding=response_encoding)\n            except ValueError:\n                logger.debug(f\"Skipping extraction of link with bad URL {url!r}\")\n                continue\n\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(\n                url,\n                _collect_string_content(el) or \"\",\n                nofollow=rel_has_nofollow(el.get(\"rel\")),\n            )\n            links.append(link)\n        return self._deduplicate_if_needed(links)\n\n    def extract_links(self, response: TextResponse) -> List[Link]:\n        base_url = get_base_url(response)\n        return self._extract_links(\n            response.selector, response.url, response.encoding, base_url\n        )\n\n    def _process_links(self, links: List[Link]) -> List[Link]:\n        \"\"\"Normalize and filter extracted links\n\n        The subclass should override it if necessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)\n\n    def _deduplicate_if_needed(self, links: List[Link]) -> List[Link]:\n        if self.unique:\n            return unique_list(links, key=self.link_key)\n        return links\n\n\n_RegexT = Union[str, Pattern[str]]\n_RegexOrSeveralT = Union[_RegexT, Iterable[_RegexT]]\n\n\nclass LxmlLinkExtractor:\n    _csstranslator = HTMLTranslator()\n\n    def __init__(\n        self,\n        allow: _RegexOrSeveralT = (),\n        deny: _RegexOrSeveralT = (),\n        allow_domains: Union[str, Iterable[str]] = (),\n        deny_domains: Union[str, Iterable[str]] = (),\n        restrict_xpaths: Union[str, Iterable[str]] = (),\n        tags: Union[str, Iterable[str]] = (\"a\", \"area\"),\n        attrs: Union[str, Iterable[str]] = (\"href\",),\n        canonicalize: bool = False,\n        unique: bool = True,\n        process_value: Optional[Callable[[Any], Any]] = None,\n        deny_extensions: Union[str, Iterable[str], None] = None,\n        restrict_css: Union[str, Iterable[str]] = (),\n        strip: bool = True,\n        restrict_text: Optional[_RegexOrSeveralT] = None,\n    ):\n        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n        self.link_extractor = LxmlParserLinkExtractor(\n            tag=partial(operator.contains, tags),\n            attr=partial(operator.contains, attrs),\n            unique=unique,\n            process=process_value,\n            strip=strip,\n            canonicalized=not canonicalize,\n        )\n        self.allow_res: List[Pattern[str]] = self._compile_regexes(allow)\n        self.deny_res: List[Pattern[str]] = self._compile_regexes(deny)\n\n        self.allow_domains: Set[str] = set(arg_to_iter(allow_domains))\n        self.deny_domains: Set[str] = set(arg_to_iter(deny_domains))\n\n        self.restrict_xpaths: Tuple[str, ...] = tuple(arg_to_iter(restrict_xpaths))\n        self.restrict_xpaths += tuple(\n            map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n        )\n\n        if deny_extensions is None:\n            deny_extensions = IGNORED_EXTENSIONS\n        self.canonicalize: bool = canonicalize\n        self.deny_extensions: Set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n        self.restrict_text: List[Pattern[str]] = self._compile_regexes(restrict_text)\n\n    @staticmethod\n    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> List[Pattern[str]]:\n        return [\n            x if isinstance(x, re.Pattern) else re.compile(x)\n            for x in arg_to_iter(value)\n        ]\n\n    def _link_allowed(self, link: Link) -> bool:\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(\n            parsed_url, self.allow_domains\n        ):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(\n            parsed_url, self.deny_extensions\n        ):\n            return False\n        if self.restrict_text and not _matches(link.text, self.restrict_text):\n            return False\n        return True\n\n    def matches(self, url: str) -> bool:\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (\n            (regex.search(url) for regex in self.allow_res)\n            if self.allow_res\n            else [True]\n        )\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)\n\n    def _process_links(self, links: List[Link]) -> List[Link]:\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(link.url)\n        links = self.link_extractor._process_links(links)\n        return links\n\n    def _extract_links(self, *args: Any, **kwargs: Any) -> List[Link]:\n        return self.link_extractor._extract_links(*args, **kwargs)\n\n    def extract_links(self, response: TextResponse) -> List[Link]:\n        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n        specified :class:`response <scrapy.http.Response>`.\n\n        Only links that match the settings passed to the ``__init__`` method of\n        the link extractor are returned.\n\n        Duplicate links are omitted if the ``unique`` attribute is set to ``True``,\n        otherwise they are returned.\n        \"\"\"\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [\n                subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)\n            ]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        if self.link_extractor.unique:\n            return unique_list(all_links, key=self.link_extractor.link_key)\n        return all_links\n", "scrapy/downloadermiddlewares/ajaxcrawl.py": "from __future__ import annotations\n\nimport logging\nimport re\nfrom typing import TYPE_CHECKING, Union\n\nfrom w3lib import html\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.settings import BaseSettings\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass AjaxCrawlMiddleware:\n    \"\"\"\n    Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n    For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(\"AJAXCRAWL_ENABLED\"):\n            raise NotConfigured\n\n        # XXX: Google parses at least first 100k bytes; scrapy's redirect\n        # middleware parses first 4k. 4k turns out to be insufficient\n        # for this middleware, and parsing 100k could be slow.\n        # We use something in between (32K) by default.\n        self.lookup_bytes: int = settings.getint(\"AJAXCRAWL_MAXSIZE\", 32768)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != \"GET\":\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if \"ajax_crawlable\" in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        # scrapy already handles #! links properly\n        ajax_crawl_request = request.replace(url=request.url + \"#!\")\n        logger.debug(\n            \"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n            {\"ajax_crawl_request\": ajax_crawl_request, \"request\": request},\n            extra={\"spider\": spider},\n        )\n\n        ajax_crawl_request.meta[\"ajax_crawlable\"] = True\n        return ajax_crawl_request\n\n    def _has_ajax_crawlable_variant(self, response: Response) -> bool:\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\"\n        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n        \"\"\"\n        body = response.text[: self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)\n\n\n# XXX: move it to w3lib?\n_ajax_crawlable_re: re.Pattern[str] = re.compile(\n    r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'\n)\n\n\ndef _has_ajaxcrawlable_meta(text: str) -> bool:\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if \"fragment\" not in text:\n        return False\n    if \"content\" not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, (\"script\", \"noscript\"))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None\n", "scrapy/downloadermiddlewares/defaultheaders.py": "\"\"\"\nDefaultHeaders downloader middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Iterable, Tuple, Union\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass DefaultHeadersMiddleware:\n    def __init__(self, headers: Iterable[Tuple[str, str]]):\n        self._headers: Iterable[Tuple[str, str]] = headers\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        headers = without_none_values(crawler.settings[\"DEFAULT_REQUEST_HEADERS\"])\n        return cls(headers.items())\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)\n        return None\n", "scrapy/downloadermiddlewares/retry.py": "\"\"\"\nAn extension to retry failed requests that are potentially caused by temporary\nproblems such as a connection timeout or HTTP 500 error.\n\nYou can change the behaviour of this middleware by modifying the scraping settings:\nRETRY_TIMES - how many times to retry a failed page\nRETRY_HTTP_CODES - which HTTP response codes to retry\n\nFailed pages are collected on the scraping process and rescheduled at the end,\nonce the spider has finished crawling all regular (non failed) pages.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom logging import Logger, getLogger\nfrom typing import TYPE_CHECKING, Any, Optional, Tuple, Type, Union\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Response\nfrom scrapy.http.request import Request\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.response import response_status_message\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nretry_logger = getLogger(__name__)\n\n\ndef backwards_compatibility_getattr(self: Any, name: str) -> Tuple[Any, ...]:\n    if name == \"EXCEPTIONS_TO_RETRY\":\n        warnings.warn(\n            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n            \"Use the RETRY_EXCEPTIONS setting instead.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return tuple(\n            load_object(x) if isinstance(x, str) else x\n            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n        )\n    raise AttributeError(\n        f\"{self.__class__.__name__!r} object has no attribute {name!r}\"\n    )\n\n\nclass BackwardsCompatibilityMetaclass(type):\n    __getattr__ = backwards_compatibility_getattr\n\n\ndef get_retry_request(\n    request: Request,\n    *,\n    spider: Spider,\n    reason: Union[str, Exception, Type[Exception]] = \"unspecified\",\n    max_retry_times: Optional[int] = None,\n    priority_adjust: Optional[int] = None,\n    logger: Logger = retry_logger,\n    stats_base_key: str = \"retry\",\n) -> Optional[Request]:\n    \"\"\"\n    Returns a new :class:`~scrapy.Request` object to retry the specified\n    request, or ``None`` if retries of the specified request have been\n    exhausted.\n\n    For example, in a :class:`~scrapy.Spider` callback, you could use it as\n    follows::\n\n        def parse(self, response):\n            if not response.text:\n                new_request_or_none = get_retry_request(\n                    response.request,\n                    spider=self,\n                    reason='empty',\n                )\n                return new_request_or_none\n\n    *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n    retry request. It is used to access the :ref:`settings <topics-settings>`\n    and :ref:`stats <topics-stats>`, and to provide extra logging context (see\n    :func:`logging.debug`).\n\n    *reason* is a string or an :class:`Exception` object that indicates the\n    reason why the request needs to be retried. It is used to name retry stats.\n\n    *max_retry_times* is a number that determines the maximum number of times\n    that *request* can be retried. If not specified or ``None``, the number is\n    read from the :reqmeta:`max_retry_times` meta key of the request. If the\n    :reqmeta:`max_retry_times` meta key is not defined or ``None``, the number\n    is read from the :setting:`RETRY_TIMES` setting.\n\n    *priority_adjust* is a number that determines how the priority of the new\n    request changes in relation to *request*. If not specified, the number is\n    read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n\n    *logger* is the logging.Logger object to be used when logging messages\n\n    *stats_base_key* is a string to be used as the base key for the\n    retry-related job stats\n    \"\"\"\n    settings = spider.crawler.settings\n    assert spider.crawler.stats\n    stats = spider.crawler.stats\n    retry_times = request.meta.get(\"retry_times\", 0) + 1\n    if max_retry_times is None:\n        max_retry_times = request.meta.get(\"max_retry_times\")\n        if max_retry_times is None:\n            max_retry_times = settings.getint(\"RETRY_TIMES\")\n    if retry_times <= max_retry_times:\n        logger.debug(\n            \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n            {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n            extra={\"spider\": spider},\n        )\n        new_request: Request = request.copy()\n        new_request.meta[\"retry_times\"] = retry_times\n        new_request.dont_filter = True\n        if priority_adjust is None:\n            priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n        new_request.priority = request.priority + priority_adjust\n\n        if callable(reason):\n            reason = reason()\n        if isinstance(reason, Exception):\n            reason = global_object_name(reason.__class__)\n\n        stats.inc_value(f\"{stats_base_key}/count\")\n        stats.inc_value(f\"{stats_base_key}/reason_count/{reason}\")\n        return new_request\n    stats.inc_value(f\"{stats_base_key}/max_reached\")\n    logger.error(\n        \"Gave up retrying %(request)s (failed %(retry_times)d times): \" \"%(reason)s\",\n        {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n        extra={\"spider\": spider},\n    )\n    return None\n\n\nclass RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(\"RETRY_ENABLED\"):\n            raise NotConfigured\n        self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n        self.retry_http_codes = {int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")}\n        self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n\n        try:\n            self.exceptions_to_retry = self.__getattribute__(\"EXCEPTIONS_TO_RETRY\")\n        except AttributeError:\n            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n            self.exceptions_to_retry = tuple(\n                load_object(x) if isinstance(x, str) else x\n                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n            )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.meta.get(\"dont_retry\", False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason, spider) or response\n        return response\n\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n            \"dont_retry\", False\n        ):\n            return self._retry(request, exception, spider)\n        return None\n\n    def _retry(\n        self,\n        request: Request,\n        reason: Union[str, Exception, Type[Exception]],\n        spider: Spider,\n    ) -> Optional[Request]:\n        max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n        priority_adjust = request.meta.get(\"priority_adjust\", self.priority_adjust)\n        return get_retry_request(\n            request,\n            reason=reason,\n            spider=spider,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )\n\n    __getattr__ = backwards_compatibility_getattr\n", "scrapy/downloadermiddlewares/httpcache.py": "from __future__ import annotations\n\nfrom email.utils import formatdate\nfrom typing import TYPE_CHECKING, Optional, Union\n\nfrom twisted.internet import defer\nfrom twisted.internet.error import (\n    ConnectError,\n    ConnectionDone,\n    ConnectionLost,\n    ConnectionRefusedError,\n    DNSLookupError,\n    TCPTimedOutError,\n    TimeoutError,\n)\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http.request import Request\nfrom scrapy.http.response import Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass HttpCacheMiddleware:\n    DOWNLOAD_EXCEPTIONS = (\n        defer.TimeoutError,\n        TimeoutError,\n        DNSLookupError,\n        ConnectionRefusedError,\n        ConnectionDone,\n        ConnectError,\n        ConnectionLost,\n        TCPTimedOutError,\n        ResponseFailed,\n        OSError,\n    )\n\n    def __init__(self, settings: Settings, stats: StatsCollector) -> None:\n        if not settings.getbool(\"HTTPCACHE_ENABLED\"):\n            raise NotConfigured\n        self.policy = load_object(settings[\"HTTPCACHE_POLICY\"])(settings)\n        self.storage = load_object(settings[\"HTTPCACHE_STORAGE\"])(settings)\n        self.ignore_missing = settings.getbool(\"HTTPCACHE_IGNORE_MISSING\")\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.storage.open_spider(spider)\n\n    def spider_closed(self, spider: Spider) -> None:\n        self.storage.close_spider(spider)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if request.meta.get(\"dont_cache\", False):\n            return None\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta[\"_dont_cache\"] = True  # flag as uncacheable\n            return None\n\n        # Look for cached response and check if expired\n        cachedresponse: Optional[Response] = self.storage.retrieve_response(\n            spider, request\n        )\n        if cachedresponse is None:\n            self.stats.inc_value(\"httpcache/miss\", spider=spider)\n            if self.ignore_missing:\n                self.stats.inc_value(\"httpcache/ignore\", spider=spider)\n                raise IgnoreRequest(f\"Ignored request not in cache: {request}\")\n            return None  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append(\"cached\")\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value(\"httpcache/hit\", spider=spider)\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta[\"cached_response\"] = cachedresponse\n\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.meta.get(\"dont_cache\", False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if \"cached\" in response.flags or \"_dont_cache\" in request.meta:\n            request.meta.pop(\"_dont_cache\", None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if \"Date\" not in response.headers:\n            response.headers[\"Date\"] = formatdate(usegmt=True)\n\n        # Do not validate first-hand responses\n        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n        if cachedresponse is None:\n            self.stats.inc_value(\"httpcache/firsthand\", spider=spider)\n            self._cache_response(spider, response, request, cachedresponse)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value(\"httpcache/revalidate\", spider=spider)\n            return cachedresponse\n\n        self.stats.inc_value(\"httpcache/invalidate\", spider=spider)\n        self._cache_response(spider, response, request, cachedresponse)\n        return response\n\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Union[Request, Response, None]:\n        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n        if cachedresponse is not None and isinstance(\n            exception, self.DOWNLOAD_EXCEPTIONS\n        ):\n            self.stats.inc_value(\"httpcache/errorrecovery\", spider=spider)\n            return cachedresponse\n        return None\n\n    def _cache_response(\n        self,\n        spider: Spider,\n        response: Response,\n        request: Request,\n        cachedresponse: Optional[Response],\n    ) -> None:\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value(\"httpcache/store\", spider=spider)\n            self.storage.store_response(spider, request, response)\n        else:\n            self.stats.inc_value(\"httpcache/uncacheable\", spider=spider)\n", "scrapy/downloadermiddlewares/offsite.py": "from __future__ import annotations\n\nimport logging\nimport re\nimport warnings\nfrom typing import TYPE_CHECKING, Set\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware:\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n        return o\n\n    def __init__(self, stats: StatsCollector):\n        self.stats = stats\n        self.domains_seen: Set[str] = set()\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n\n    def request_scheduled(self, request: Request, spider: Spider) -> None:\n        self.process_request(request, spider)\n\n    def process_request(self, request: Request, spider: Spider) -> None:\n        if request.dont_filter or self.should_follow(request, spider):\n            return None\n        domain = urlparse_cached(request).hostname\n        if domain and domain not in self.domains_seen:\n            self.domains_seen.add(domain)\n            logger.debug(\n                \"Filtered offsite request to %(domain)r: %(request)s\",\n                {\"domain\": domain, \"request\": request},\n                extra={\"spider\": spider},\n            )\n            self.stats.inc_value(\"offsite/domains\", spider=spider)\n        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n        raise IgnoreRequest\n\n    def should_follow(self, request: Request, spider: Spider) -> bool:\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or \"\"\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, \"allowed_domains\", None)\n        if not allowed_domains:\n            return re.compile(\"\")  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            if url_pattern.match(domain):\n                message = (\n                    \"allowed_domains accepts only domains, not URLs. \"\n                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message)\n            elif port_pattern.search(domain):\n                message = (\n                    \"allowed_domains accepts only domains without ports. \"\n                    f\"Ignoring entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message)\n            else:\n                domains.append(re.escape(domain))\n        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n        return re.compile(regex)\n", "scrapy/downloadermiddlewares/httpcompression.py": "from __future__ import annotations\n\nimport warnings\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils._compression import (\n    _DecompressionMaxSizeExceeded,\n    _inflate,\n    _unbrotli,\n    _unzstd,\n)\nfrom scrapy.utils.deprecate import ScrapyDeprecationWarning\nfrom scrapy.utils.gz import gunzip\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = getLogger(__name__)\n\nACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n\ntry:\n    try:\n        import brotli  # noqa: F401\n    except ImportError:\n        import brotlicffi  # noqa: F401\nexcept ImportError:\n    pass\nelse:\n    ACCEPTED_ENCODINGS.append(b\"br\")\n\ntry:\n    import zstandard  # noqa: F401\nexcept ImportError:\n    pass\nelse:\n    ACCEPTED_ENCODINGS.append(b\"zstd\")\n\n\nclass HttpCompressionMiddleware:\n    \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n    sent/received from web sites\"\"\"\n\n    def __init__(\n        self,\n        stats: Optional[StatsCollector] = None,\n        *,\n        crawler: Optional[Crawler] = None,\n    ):\n        if not crawler:\n            self.stats = stats\n            self._max_size = 1073741824\n            self._warn_size = 33554432\n            return\n        self.stats = crawler.stats\n        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n        crawler.signals.connect(self.open_spider, signals.spider_opened)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n            raise NotConfigured\n        try:\n            return cls(crawler=crawler)\n        except TypeError:\n            warnings.warn(\n                \"HttpCompressionMiddleware subclasses must either modify \"\n                \"their '__init__' method to support a 'crawler' parameter or \"\n                \"reimplement their 'from_crawler' method.\",\n                ScrapyDeprecationWarning,\n            )\n            mw = cls()\n            mw.stats = crawler.stats\n            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n            crawler.signals.connect(mw.open_spider, signals.spider_opened)\n            return mw\n\n    def open_spider(self, spider):\n        if hasattr(spider, \"download_maxsize\"):\n            self._max_size = spider.download_maxsize\n        if hasattr(spider, \"download_warnsize\"):\n            self._warn_size = spider.download_warnsize\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.method == \"HEAD\":\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist(\"Content-Encoding\")\n            if content_encoding:\n                max_size = request.meta.get(\"download_maxsize\", self._max_size)\n                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\n                try:\n                    decoded_body, content_encoding = self._handle_encoding(\n                        response.body, content_encoding, max_size\n                    )\n                except _DecompressionMaxSizeExceeded:\n                    raise IgnoreRequest(\n                        f\"Ignored response {response} because its body \"\n                        f\"({len(response.body)} B compressed) exceeded \"\n                        f\"DOWNLOAD_MAXSIZE ({max_size} B) during \"\n                        f\"decompression.\"\n                    )\n                if len(response.body) < warn_size <= len(decoded_body):\n                    logger.warning(\n                        f\"{response} body size after decompression \"\n                        f\"({len(decoded_body)} B) is larger than the \"\n                        f\"download warning size ({warn_size} B).\"\n                    )\n                response.headers[\"Content-Encoding\"] = content_encoding\n                if self.stats:\n                    self.stats.inc_value(\n                        \"httpcompression/response_bytes\",\n                        len(decoded_body),\n                        spider=spider,\n                    )\n                    self.stats.inc_value(\n                        \"httpcompression/response_count\", spider=spider\n                    )\n                respcls = responsetypes.from_args(\n                    headers=response.headers, url=response.url, body=decoded_body\n                )\n                kwargs: Dict[str, Any] = {\"body\": decoded_body}\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs[\"encoding\"] = None\n                response = response.replace(cls=respcls, **kwargs)\n                if not content_encoding:\n                    del response.headers[\"Content-Encoding\"]\n\n        return response\n\n    def _handle_encoding(\n        self, body: bytes, content_encoding: List[bytes], max_size: int\n    ) -> Tuple[bytes, List[bytes]]:\n        to_decode, to_keep = self._split_encodings(content_encoding)\n        for encoding in to_decode:\n            body = self._decode(body, encoding, max_size)\n        return body, to_keep\n\n    def _split_encodings(\n        self, content_encoding: List[bytes]\n    ) -> Tuple[List[bytes], List[bytes]]:\n        to_keep: List[bytes] = [\n            encoding.strip().lower()\n            for encoding in chain.from_iterable(\n                encodings.split(b\",\") for encodings in content_encoding\n            )\n        ]\n        to_decode: List[bytes] = []\n        while to_keep:\n            encoding = to_keep.pop()\n            if encoding not in ACCEPTED_ENCODINGS:\n                to_keep.append(encoding)\n                return to_decode, to_keep\n            to_decode.append(encoding)\n        return to_decode, to_keep\n\n    def _decode(self, body: bytes, encoding: bytes, max_size: int) -> bytes:\n        if encoding in {b\"gzip\", b\"x-gzip\"}:\n            return gunzip(body, max_size=max_size)\n        if encoding == b\"deflate\":\n            return _inflate(body, max_size=max_size)\n        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n            return _unbrotli(body, max_size=max_size)\n        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n            return _unzstd(body, max_size=max_size)\n        return body\n", "scrapy/downloadermiddlewares/downloadtimeout.py": "\"\"\"\nDownload timeout middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Union\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass DownloadTimeoutMiddleware:\n    def __init__(self, timeout: float = 180):\n        self._timeout: float = timeout\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings.getfloat(\"DOWNLOAD_TIMEOUT\"))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self._timeout = getattr(spider, \"download_timeout\", self._timeout)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if self._timeout:\n            request.meta.setdefault(\"download_timeout\", self._timeout)\n        return None\n", "scrapy/downloadermiddlewares/httpproxy.py": "from __future__ import annotations\n\nimport base64\nfrom typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\nfrom urllib.parse import unquote, urlunparse\nfrom urllib.request import (  # type: ignore[attr-defined]\n    _parse_proxy,\n    getproxies,\n    proxy_bypass,\n)\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass HttpProxyMiddleware:\n    def __init__(self, auth_encoding: Optional[str] = \"latin-1\"):\n        self.auth_encoding: Optional[str] = auth_encoding\n        self.proxies: Dict[str, Tuple[Optional[bytes], str]] = {}\n        for type_, url in getproxies().items():\n            try:\n                self.proxies[type_] = self._get_proxy(url, type_)\n            # some values such as '/var/run/docker.sock' can't be parsed\n            # by _parse_proxy and as such should be skipped\n            except ValueError:\n                continue\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n            raise NotConfigured\n        auth_encoding: Optional[str] = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n        return cls(auth_encoding)\n\n    def _basic_auth_header(self, username: str, password: str) -> bytes:\n        user_pass = to_bytes(\n            f\"{unquote(username)}:{unquote(password)}\", encoding=self.auth_encoding\n        )\n        return base64.b64encode(user_pass)\n\n    def _get_proxy(self, url: str, orig_type: str) -> Tuple[Optional[bytes], str]:\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n\n        if user:\n            creds = self._basic_auth_header(user, password)\n        else:\n            creds = None\n\n        return creds, proxy_url\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        creds, proxy_url, scheme = None, None, None\n        if \"proxy\" in request.meta:\n            if request.meta[\"proxy\"] is not None:\n                creds, proxy_url = self._get_proxy(request.meta[\"proxy\"], \"\")\n        elif self.proxies:\n            parsed = urlparse_cached(request)\n            _scheme = parsed.scheme\n            if (\n                # 'no_proxy' is only supported by http schemes\n                _scheme not in (\"http\", \"https\")\n                or (parsed.hostname and not proxy_bypass(parsed.hostname))\n            ) and _scheme in self.proxies:\n                scheme = _scheme\n                creds, proxy_url = self.proxies[scheme]\n\n        self._set_proxy_and_creds(request, proxy_url, creds, scheme)\n        return None\n\n    def _set_proxy_and_creds(\n        self,\n        request: Request,\n        proxy_url: Optional[str],\n        creds: Optional[bytes],\n        scheme: Optional[str],\n    ) -> None:\n        if scheme:\n            request.meta[\"_scheme_proxy\"] = True\n        if proxy_url:\n            request.meta[\"proxy\"] = proxy_url\n        elif request.meta.get(\"proxy\") is not None:\n            request.meta[\"proxy\"] = None\n        if creds:\n            request.headers[b\"Proxy-Authorization\"] = b\"Basic \" + creds\n            request.meta[\"_auth_proxy\"] = proxy_url\n        elif \"_auth_proxy\" in request.meta:\n            if proxy_url != request.meta[\"_auth_proxy\"]:\n                if b\"Proxy-Authorization\" in request.headers:\n                    del request.headers[b\"Proxy-Authorization\"]\n                del request.meta[\"_auth_proxy\"]\n        elif b\"Proxy-Authorization\" in request.headers:\n            if proxy_url:\n                request.meta[\"_auth_proxy\"] = proxy_url\n            else:\n                del request.headers[b\"Proxy-Authorization\"]\n", "scrapy/downloadermiddlewares/stats.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Dict, List, Tuple, Union\n\nfrom twisted.web import http\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.python import global_object_name, to_bytes\nfrom scrapy.utils.request import request_httprepr\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\ndef get_header_size(\n    headers: Dict[str, Union[List[Union[str, bytes]], Tuple[Union[str, bytes], ...]]]\n) -> int:\n    size = 0\n    for key, value in headers.items():\n        if isinstance(value, (list, tuple)):\n            for v in value:\n                size += len(b\": \") + len(key) + len(v)\n    return size + len(b\"\\r\\n\") * (len(headers.keys()) - 1)\n\n\ndef get_status_size(response_status: int) -> int:\n    return len(to_bytes(http.RESPONSES.get(response_status, b\"\"))) + 15\n    # resp.status + b\"\\r\\n\" + b\"HTTP/1.1 <100-599> \"\n\n\nclass DownloaderStats:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"DOWNLOADER_STATS\"):\n            raise NotConfigured\n        assert crawler.stats\n        return cls(crawler.stats)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        self.stats.inc_value(\"downloader/request_count\", spider=spider)\n        self.stats.inc_value(\n            f\"downloader/request_method_count/{request.method}\", spider=spider\n        )\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value(\"downloader/request_bytes\", reqlen, spider=spider)\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        self.stats.inc_value(\"downloader/response_count\", spider=spider)\n        self.stats.inc_value(\n            f\"downloader/response_status_count/{response.status}\", spider=spider\n        )\n        reslen = (\n            len(response.body)\n            + get_header_size(response.headers)\n            + get_status_size(response.status)\n            + 4\n        )\n        # response.body + b\"\\r\\n\"+ response.header + b\"\\r\\n\" + response.status\n        self.stats.inc_value(\"downloader/response_bytes\", reslen, spider=spider)\n        return response\n\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Union[Request, Response, None]:\n        ex_class = global_object_name(exception.__class__)\n        self.stats.inc_value(\"downloader/exception_count\", spider=spider)\n        self.stats.inc_value(\n            f\"downloader/exception_type_count/{ex_class}\", spider=spider\n        )\n        return None\n", "scrapy/downloadermiddlewares/robotstxt.py": "\"\"\"\nThis is a middleware to respect robots.txt policies. To activate it you must\nenable this middleware and enable the ROBOTSTXT_OBEY setting.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Union\n\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.robotstxt import RobotParser\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass RobotsTxtMiddleware:\n    DOWNLOAD_PRIORITY: int = 1000\n\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"ROBOTSTXT_OBEY\"):\n            raise NotConfigured\n        self._default_useragent: str = crawler.settings.get(\"USER_AGENT\", \"Scrapy\")\n        self._robotstxt_useragent: Optional[str] = crawler.settings.get(\n            \"ROBOTSTXT_USER_AGENT\", None\n        )\n        self.crawler: Crawler = crawler\n        self._parsers: Dict[str, Union[RobotParser, Deferred, None]] = {}\n        self._parserimpl: RobotParser = load_object(\n            crawler.settings.get(\"ROBOTSTXT_PARSER\")\n        )\n\n        # check if parser dependencies are met, this should throw an error otherwise.\n        self._parserimpl.from_crawler(self.crawler, b\"\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def process_request(self, request: Request, spider: Spider) -> Optional[Deferred]:\n        if request.meta.get(\"dont_obey_robotstxt\"):\n            return None\n        if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n            return None\n        d: Deferred = maybeDeferred(self.robot_parser, request, spider)\n        d.addCallback(self.process_request_2, request, spider)\n        return d\n\n    def process_request_2(\n        self, rp: Optional[RobotParser], request: Request, spider: Spider\n    ) -> None:\n        if rp is None:\n            return\n\n        useragent: Union[str, bytes, None] = self._robotstxt_useragent\n        if not useragent:\n            useragent = request.headers.get(b\"User-Agent\", self._default_useragent)\n            assert useragent is not None\n        if not rp.allowed(request.url, useragent):\n            logger.debug(\n                \"Forbidden by robots.txt: %(request)s\",\n                {\"request\": request},\n                extra={\"spider\": spider},\n            )\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(\"robotstxt/forbidden\")\n            raise IgnoreRequest(\"Forbidden by robots.txt\")\n\n    def robot_parser(\n        self, request: Request, spider: Spider\n    ) -> Union[RobotParser, Deferred, None]:\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = f\"{url.scheme}://{url.netloc}/robots.txt\"\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={\"dont_obey_robotstxt\": True},\n                callback=NO_CALLBACK,\n            )\n            assert self.crawler.engine\n            assert self.crawler.stats\n            dfd = self.crawler.engine.download(robotsreq)\n            dfd.addCallback(self._parse_robots, netloc, spider)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n            self.crawler.stats.inc_value(\"robotstxt/request_count\")\n\n        parser = self._parsers[netloc]\n        if isinstance(parser, Deferred):\n            d: Deferred = Deferred()\n\n            def cb(result: Any) -> Any:\n                d.callback(result)\n                return result\n\n            parser.addCallback(cb)\n            return d\n        return parser\n\n    def _logerror(self, failure: Failure, request: Request, spider: Spider) -> Failure:\n        if failure.type is not IgnoreRequest:\n            logger.error(\n                \"Error downloading %(request)s: %(f_exception)s\",\n                {\"request\": request, \"f_exception\": failure.value},\n                exc_info=failure_to_exc_info(failure),\n                extra={\"spider\": spider},\n            )\n        return failure\n\n    def _parse_robots(self, response: Response, netloc: str, spider: Spider) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\"robotstxt/response_count\")\n        self.crawler.stats.inc_value(\n            f\"robotstxt/response_status_count/{response.status}\"\n        )\n        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n        rp_dfd = self._parsers[netloc]\n        assert isinstance(rp_dfd, Deferred)\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)\n\n    def _robots_error(self, failure: Failure, netloc: str) -> None:\n        if failure.type is not IgnoreRequest:\n            key = f\"robotstxt/exception_count/{failure.type}\"\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(key)\n        rp_dfd = self._parsers[netloc]\n        assert isinstance(rp_dfd, Deferred)\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)\n", "scrapy/downloadermiddlewares/redirect.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, List, Union, cast\nfrom urllib.parse import urljoin\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.response import get_meta_refresh\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\ndef _build_redirect_request(\n    source_request: Request, *, url: str, **kwargs: Any\n) -> Request:\n    redirect_request = source_request.replace(\n        url=url,\n        **kwargs,\n        cls=None,\n        cookies=None,\n    )\n    if \"_scheme_proxy\" in redirect_request.meta:\n        source_request_scheme = urlparse_cached(source_request).scheme\n        redirect_request_scheme = urlparse_cached(redirect_request).scheme\n        if source_request_scheme != redirect_request_scheme:\n            redirect_request.meta.pop(\"_scheme_proxy\")\n            redirect_request.meta.pop(\"proxy\", None)\n            redirect_request.meta.pop(\"_auth_proxy\", None)\n            redirect_request.headers.pop(b\"Proxy-Authorization\", None)\n    has_cookie_header = \"Cookie\" in redirect_request.headers\n    has_authorization_header = \"Authorization\" in redirect_request.headers\n    if has_cookie_header or has_authorization_header:\n        default_ports = {\"http\": 80, \"https\": 443}\n\n        parsed_source_request = urlparse_cached(source_request)\n        source_scheme, source_host, source_port = (\n            parsed_source_request.scheme,\n            parsed_source_request.hostname,\n            parsed_source_request.port\n            or default_ports.get(parsed_source_request.scheme),\n        )\n\n        parsed_redirect_request = urlparse_cached(redirect_request)\n        redirect_scheme, redirect_host, redirect_port = (\n            parsed_redirect_request.scheme,\n            parsed_redirect_request.hostname,\n            parsed_redirect_request.port\n            or default_ports.get(parsed_redirect_request.scheme),\n        )\n\n        if has_cookie_header and (\n            redirect_scheme not in {source_scheme, \"https\"}\n            or source_host != redirect_host\n        ):\n            del redirect_request.headers[\"Cookie\"]\n\n        # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name\n        if has_authorization_header and (\n            source_scheme != redirect_scheme\n            or source_host != redirect_host\n            or source_port != redirect_port\n        ):\n            del redirect_request.headers[\"Authorization\"]\n\n    return redirect_request\n\n\nclass BaseRedirectMiddleware:\n    enabled_setting: str = \"REDIRECT_ENABLED\"\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(self.enabled_setting):\n            raise NotConfigured\n\n        self.max_redirect_times: int = settings.getint(\"REDIRECT_MAX_TIMES\")\n        self.priority_adjust: int = settings.getint(\"REDIRECT_PRIORITY_ADJUST\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def _redirect(\n        self, redirected: Request, request: Request, spider: Spider, reason: Any\n    ) -> Request:\n        ttl = request.meta.setdefault(\"redirect_ttl\", self.max_redirect_times)\n        redirects = request.meta.get(\"redirect_times\", 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta[\"redirect_times\"] = redirects\n            redirected.meta[\"redirect_ttl\"] = ttl - 1\n            redirected.meta[\"redirect_urls\"] = request.meta.get(\"redirect_urls\", []) + [\n                request.url\n            ]\n            redirected.meta[\"redirect_reasons\"] = request.meta.get(\n                \"redirect_reasons\", []\n            ) + [reason]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\n                \"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                {\"reason\": reason, \"redirected\": redirected, \"request\": request},\n                extra={\"spider\": spider},\n            )\n            return redirected\n        logger.debug(\n            \"Discarding %(request)s: max redirections reached\",\n            {\"request\": request},\n            extra={\"spider\": spider},\n        )\n        raise IgnoreRequest(\"max redirections reached\")\n\n    def _redirect_request_using_get(\n        self, request: Request, redirect_url: str\n    ) -> Request:\n        redirect_request = _build_redirect_request(\n            request,\n            url=redirect_url,\n            method=\"GET\",\n            body=\"\",\n        )\n        redirect_request.headers.pop(\"Content-Type\", None)\n        redirect_request.headers.pop(\"Content-Length\", None)\n        return redirect_request\n\n\nclass RedirectMiddleware(BaseRedirectMiddleware):\n    \"\"\"\n    Handle redirection of requests based on response status\n    and meta-refresh html tag.\n    \"\"\"\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if (\n            request.meta.get(\"dont_redirect\", False)\n            or response.status in getattr(spider, \"handle_httpstatus_list\", [])\n            or response.status in request.meta.get(\"handle_httpstatus_list\", [])\n            or request.meta.get(\"handle_httpstatus_all\", False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if \"Location\" not in response.headers or response.status not in allowed_status:\n            return response\n\n        assert response.headers[\"Location\"] is not None\n        location = safe_url_string(response.headers[\"Location\"])\n        if response.headers[\"Location\"].startswith(b\"//\"):\n            request_scheme = urlparse_cached(request).scheme\n            location = request_scheme + \"://\" + location.lstrip(\"/\")\n\n        redirected_url = urljoin(request.url, location)\n        redirected = _build_redirect_request(request, url=redirected_url)\n        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n            return response\n\n        if response.status in (301, 307, 308) or request.method == \"HEAD\":\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)\n\n\nclass MetaRefreshMiddleware(BaseRedirectMiddleware):\n    enabled_setting = \"METAREFRESH_ENABLED\"\n\n    def __init__(self, settings: BaseSettings):\n        super().__init__(settings)\n        self._ignore_tags: List[str] = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n        self._maxdelay: int = settings.getint(\"METAREFRESH_MAXDELAY\")\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if (\n            request.meta.get(\"dont_redirect\", False)\n            or request.method == \"HEAD\"\n            or not isinstance(response, HtmlResponse)\n            or urlparse_cached(request).scheme not in {\"http\", \"https\"}\n        ):\n            return response\n\n        interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)\n        if not url:\n            return response\n        redirected = self._redirect_request_using_get(request, url)\n        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n            return response\n        if cast(float, interval) < self._maxdelay:\n            return self._redirect(redirected, request, spider, \"meta refresh\")\n        return response\n", "scrapy/downloadermiddlewares/__init__.py": "", "scrapy/downloadermiddlewares/useragent.py": "\"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Union\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass UserAgentMiddleware:\n    \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n\n    def __init__(self, user_agent: str = \"Scrapy\"):\n        self.user_agent = user_agent\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings[\"USER_AGENT\"])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.user_agent = getattr(spider, \"user_agent\", self.user_agent)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if self.user_agent:\n            request.headers.setdefault(b\"User-Agent\", self.user_agent)\n        return None\n", "scrapy/downloadermiddlewares/cookies.py": "from __future__ import annotations\n\nimport logging\nfrom collections import defaultdict\nfrom http.cookiejar import Cookie\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Iterable, Optional, Sequence, Union\n\nfrom tldextract import TLDExtract\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.http.cookies import CookieJar\nfrom scrapy.http.request import VerboseCookie\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\n_split_domain = TLDExtract(include_psl_private_domains=True)\n_UNSET = object()\n\n\ndef _is_public_domain(domain: str) -> bool:\n    parts = _split_domain(domain)\n    return not parts.domain\n\n\nclass CookiesMiddleware:\n    \"\"\"This middleware enables working with sites that need cookies\"\"\"\n\n    def __init__(self, debug: bool = False):\n        self.jars: DefaultDict[Any, CookieJar] = defaultdict(CookieJar)\n        self.debug: bool = debug\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"COOKIES_ENABLED\"):\n            raise NotConfigured\n        return cls(crawler.settings.getbool(\"COOKIES_DEBUG\"))\n\n    def _process_cookies(\n        self, cookies: Iterable[Cookie], *, jar: CookieJar, request: Request\n    ) -> None:\n        for cookie in cookies:\n            cookie_domain = cookie.domain\n            if cookie_domain.startswith(\".\"):\n                cookie_domain = cookie_domain[1:]\n\n            hostname = urlparse_cached(request).hostname\n            assert hostname is not None\n            request_domain = hostname.lower()\n\n            if cookie_domain and _is_public_domain(cookie_domain):\n                if cookie_domain != request_domain:\n                    continue\n                cookie.domain = request_domain\n\n            jar.set_cookie_if_ok(cookie, request)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if request.meta.get(\"dont_merge_cookies\", False):\n            return None\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        self._process_cookies(cookies, jar=jar, request=request)\n\n        # set Cookie header\n        request.headers.pop(\"Cookie\", None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.meta.get(\"dont_merge_cookies\", False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = jar.make_cookies(response, request)\n        self._process_cookies(cookies, jar=jar, request=request)\n\n        self._debug_set_cookie(response, spider)\n\n        return response\n\n    def _debug_cookie(self, request: Request, spider: Spider) -> None:\n        if self.debug:\n            cl = [\n                to_unicode(c, errors=\"replace\")\n                for c in request.headers.getlist(\"Cookie\")\n            ]\n            if cl:\n                cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n                msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                logger.debug(msg, extra={\"spider\": spider})\n\n    def _debug_set_cookie(self, response: Response, spider: Spider) -> None:\n        if self.debug:\n            cl = [\n                to_unicode(c, errors=\"replace\")\n                for c in response.headers.getlist(\"Set-Cookie\")\n            ]\n            if cl:\n                cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n                msg = f\"Received cookies from: {response}\\n{cookies}\"\n                logger.debug(msg, extra={\"spider\": spider})\n\n    def _format_cookie(self, cookie: VerboseCookie, request: Request) -> Optional[str]:\n        \"\"\"\n        Given a dict consisting of cookie components, return its string representation.\n        Decode from bytes if necessary.\n        \"\"\"\n        decoded = {}\n        flags = set()\n        for key in (\"name\", \"value\", \"path\", \"domain\"):\n            if cookie.get(key) is None:\n                if key in (\"name\", \"value\"):\n                    msg = f\"Invalid cookie found in request {request}: {cookie} ('{key}' is missing)\"\n                    logger.warning(msg)\n                    return None\n                continue\n            # https://github.com/python/mypy/issues/7178, https://github.com/python/mypy/issues/9168\n            if isinstance(cookie[key], (bool, float, int, str)):  # type: ignore[literal-required]\n                decoded[key] = str(cookie[key])  # type: ignore[literal-required]\n            else:\n                try:\n                    decoded[key] = cookie[key].decode(\"utf8\")  # type: ignore[literal-required]\n                except UnicodeDecodeError:\n                    logger.warning(\n                        \"Non UTF-8 encoded cookie found in request %s: %s\",\n                        request,\n                        cookie,\n                    )\n                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")  # type: ignore[literal-required]\n        for flag in (\"secure\",):\n            value = cookie.get(flag, _UNSET)\n            if value is _UNSET or not value:\n                continue\n            flags.add(flag)\n        cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n        for key, value in decoded.items():  # path, domain\n            cookie_str += f\"; {key.capitalize()}={value}\"\n        for flag in flags:  # secure\n            cookie_str += f\"; {flag.capitalize()}\"\n        return cookie_str\n\n    def _get_request_cookies(\n        self, jar: CookieJar, request: Request\n    ) -> Sequence[Cookie]:\n        \"\"\"\n        Extract cookies from the Request.cookies attribute\n        \"\"\"\n        if not request.cookies:\n            return []\n        cookies: Iterable[VerboseCookie]\n        if isinstance(request.cookies, dict):\n            cookies = tuple({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n        else:\n            cookies = request.cookies\n        for cookie in cookies:\n            cookie.setdefault(\"secure\", urlparse_cached(request).scheme == \"https\")\n        formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n        response = Response(request.url, headers={\"Set-Cookie\": formatted})\n        return jar.make_cookies(response, request)\n", "scrapy/downloadermiddlewares/httpauth.py": "\"\"\"\nHTTP basic auth downloader middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Union\n\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.utils.url import url_is_from_any_domain\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass HttpAuthMiddleware:\n    \"\"\"Set Basic HTTP Authorization header\n    (http_user and http_pass spider class attributes)\"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        usr = getattr(spider, \"http_user\", \"\")\n        pwd = getattr(spider, \"http_pass\", \"\")\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)\n            self.domain = spider.http_auth_domain  # type: ignore[attr-defined]\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        auth = getattr(self, \"auth\", None)\n        if auth and b\"Authorization\" not in request.headers:\n            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                request.headers[b\"Authorization\"] = auth\n        return None\n", "scrapy/http/headers.py": "from __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom w3lib.http import headers_dict_to_raw\n\nfrom scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_RawValueT = Union[bytes, str, int]\n\n\n# isn't fully compatible typing-wise with either dict or CaselessDict,\n# but it needs refactoring anyway, see also https://github.com/scrapy/scrapy/pull/5146\nclass Headers(CaselessDict):\n    \"\"\"Case insensitive http headers dictionary\"\"\"\n\n    def __init__(\n        self,\n        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        encoding: str = \"utf-8\",\n    ):\n        self.encoding: str = encoding\n        super().__init__(seq)\n\n    def update(  # type: ignore[override]\n        self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]\n    ) -> None:\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq: Dict[bytes, List[bytes]] = {}\n        for k, v in seq:\n            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n        super().update(iseq)\n\n    def normkey(self, key: AnyStr) -> bytes:  # type: ignore[override]\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())\n\n    def normvalue(self, value: Union[_RawValueT, Iterable[_RawValueT]]) -> List[bytes]:\n        \"\"\"Normalize values to bytes\"\"\"\n        _value: Iterable[_RawValueT]\n        if value is None:\n            _value = []\n        elif isinstance(value, (str, bytes)):\n            _value = [value]\n        elif hasattr(value, \"__iter__\"):\n            _value = value\n        else:\n            _value = [value]\n\n        return [self._tobytes(x) for x in _value]\n\n    def _tobytes(self, x: _RawValueT) -> bytes:\n        if isinstance(x, bytes):\n            return x\n        if isinstance(x, str):\n            return x.encode(self.encoding)\n        if isinstance(x, int):\n            return str(x).encode(self.encoding)\n        raise TypeError(f\"Unsupported value type: {type(x)}\")\n\n    def __getitem__(self, key: AnyStr) -> Optional[bytes]:\n        try:\n            return cast(List[bytes], super().__getitem__(key))[-1]\n        except IndexError:\n            return None\n\n    def get(self, key: AnyStr, def_val: Any = None) -> Optional[bytes]:\n        try:\n            return cast(List[bytes], super().get(key, def_val))[-1]\n        except IndexError:\n            return None\n\n    def getlist(self, key: AnyStr, def_val: Any = None) -> List[bytes]:\n        try:\n            return cast(List[bytes], super().__getitem__(key))\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []\n\n    def setlist(self, key: AnyStr, list_: Iterable[_RawValueT]) -> None:\n        self[key] = list_\n\n    def setlistdefault(\n        self, key: AnyStr, default_list: Iterable[_RawValueT] = ()\n    ) -> Any:\n        return self.setdefault(key, default_list)\n\n    def appendlist(self, key: AnyStr, value: Iterable[_RawValueT]) -> None:\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst\n\n    def items(self) -> Iterable[Tuple[bytes, List[bytes]]]:  # type: ignore[override]\n        return ((k, self.getlist(k)) for k in self.keys())\n\n    def values(self) -> List[Optional[bytes]]:  # type: ignore[override]\n        return [\n            self[k] for k in self.keys()  # pylint: disable=consider-using-dict-items\n        ]\n\n    def to_string(self) -> bytes:\n        return headers_dict_to_raw(self)\n\n    def to_unicode_dict(self) -> CaseInsensitiveDict:\n        \"\"\"Return headers as a CaseInsensitiveDict with str keys\n        and str values. Multiple values are joined with ','.\n        \"\"\"\n        return CaseInsensitiveDict(\n            (\n                to_unicode(key, encoding=self.encoding),\n                to_unicode(b\",\".join(value), encoding=self.encoding),\n            )\n            for key, value in self.items()\n        )\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n", "scrapy/http/__init__.py": "\"\"\"\nModule containing all HTTP related classes\n\nUse this module (instead of the more specific ones) when importing Headers,\nRequest and Response outside this module.\n\"\"\"\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.http.request import Request\nfrom scrapy.http.request.form import FormRequest\nfrom scrapy.http.request.json_request import JsonRequest\nfrom scrapy.http.request.rpc import XmlRpcRequest\nfrom scrapy.http.response import Response\nfrom scrapy.http.response.html import HtmlResponse\nfrom scrapy.http.response.json import JsonResponse\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.http.response.xml import XmlResponse\n", "scrapy/http/cookies.py": "from __future__ import annotations\n\nimport re\nimport time\nfrom http.cookiejar import Cookie\nfrom http.cookiejar import CookieJar as _CookieJar\nfrom http.cookiejar import CookiePolicy, DefaultCookiePolicy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    cast,\n)\n\nfrom scrapy import Request\nfrom scrapy.http import Response\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n# Defined in the http.cookiejar module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\nIPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n\n\nclass CookieJar:\n    def __init__(\n        self,\n        policy: Optional[CookiePolicy] = None,\n        check_expired_frequency: int = 10000,\n    ):\n        self.policy: CookiePolicy = policy or DefaultCookiePolicy()\n        self.jar: _CookieJar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()  # type: ignore[attr-defined]\n        self.check_expired_frequency: int = check_expired_frequency\n        self.processed: int = 0\n\n    def extract_cookies(self, response: Response, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        self.jar.extract_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def add_cookie_header(self, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())  # type: ignore[attr-defined]\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if \".\" not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:  # type: ignore[attr-defined]\n                cookies += self.jar._cookies_for_domain(host, wreq)  # type: ignore[attr-defined]\n\n        attrs = self.jar._cookie_attrs(cookies)  # type: ignore[attr-defined]\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()\n\n    @property\n    def _cookies(self) -> Dict[str, Dict[str, Dict[str, Cookie]]]:\n        return self.jar._cookies  # type: ignore[attr-defined,no-any-return]\n\n    def clear_session_cookies(self) -> None:\n        return self.jar.clear_session_cookies()\n\n    def clear(\n        self,\n        domain: Optional[str] = None,\n        path: Optional[str] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        self.jar.clear(domain, path, name)\n\n    def __iter__(self) -> Iterator[Cookie]:\n        return iter(self.jar)\n\n    def __len__(self) -> int:\n        return len(self.jar)\n\n    def set_policy(self, pol: CookiePolicy) -> None:\n        self.jar.set_policy(pol)\n\n    def make_cookies(self, response: Response, request: Request) -> Sequence[Cookie]:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def set_cookie(self, cookie: Cookie) -> None:\n        self.jar.set_cookie(cookie)\n\n    def set_cookie_if_ok(self, cookie: Cookie, request: Request) -> None:\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))  # type: ignore[arg-type]\n\n\ndef potential_domain_matches(domain: str) -> List[str]:\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index(\".\") + 1\n        end = domain.rindex(\".\")\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index(\".\", start) + 1\n    except ValueError:\n        pass\n    return matches + [\".\" + d for d in matches]\n\n\nclass _DummyLock:\n    def acquire(self) -> None:\n        pass\n\n    def release(self) -> None:\n        pass\n\n\nclass WrappedRequest:\n    \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n\n    see http://docs.python.org/library/urllib2.html#urllib2.Request\n    \"\"\"\n\n    def __init__(self, request: Request):\n        self.request = request\n\n    def get_full_url(self) -> str:\n        return self.request.url\n\n    def get_host(self) -> str:\n        return urlparse_cached(self.request).netloc\n\n    def get_type(self) -> str:\n        return urlparse_cached(self.request).scheme\n\n    def is_unverifiable(self) -> bool:\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return cast(bool, self.request.meta.get(\"is_unverifiable\", False))\n\n    @property\n    def full_url(self) -> str:\n        return self.get_full_url()\n\n    @property\n    def host(self) -> str:\n        return self.get_host()\n\n    @property\n    def type(self) -> str:\n        return self.get_type()\n\n    @property\n    def unverifiable(self) -> bool:\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self) -> str:\n        return cast(str, urlparse_cached(self.request).hostname)\n\n    def has_header(self, name: str) -> bool:\n        return name in self.request.headers\n\n    def get_header(self, name: str, default: Optional[str] = None) -> Optional[str]:\n        value = self.request.headers.get(name, default)\n        return to_unicode(value, errors=\"replace\") if value is not None else None\n\n    def header_items(self) -> List[Tuple[str, List[str]]]:\n        return [\n            (\n                to_unicode(k, errors=\"replace\"),\n                [to_unicode(x, errors=\"replace\") for x in v],\n            )\n            for k, v in self.request.headers.items()\n        ]\n\n    def add_unredirected_header(self, name: str, value: str) -> None:\n        self.request.headers.appendlist(name, value)\n\n\nclass WrappedResponse:\n    def __init__(self, response: Response):\n        self.response = response\n\n    def info(self) -> Self:\n        return self\n\n    def get_all(self, name: str, default: Any = None) -> List[str]:\n        return [\n            to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n        ]\n", "scrapy/http/response/html.py": "\"\"\"\nThis module implements the HtmlResponse class which adds encoding\ndiscovering through HTML encoding declarations to the TextResponse class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass HtmlResponse(TextResponse):\n    pass\n", "scrapy/http/response/xml.py": "\"\"\"\nThis module implements the XmlResponse class which adds encoding\ndiscovering through XML encoding declarations to the TextResponse class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass XmlResponse(TextResponse):\n    pass\n", "scrapy/http/response/json.py": "\"\"\"\nThis module implements the JsonResponse class that is used when the response\nhas a JSON MIME type in its Content-Type header.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass JsonResponse(TextResponse):\n    pass\n", "scrapy/http/response/__init__.py": "\"\"\"\nThis module implements the Response class which is used to represent HTTP\nresponses in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom ipaddress import IPv4Address, IPv6Address\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    overload,\n)\nfrom urllib.parse import urljoin\n\nfrom twisted.internet.ssl import Certificate\n\nfrom scrapy.exceptions import NotSupported\nfrom scrapy.http.headers import Headers\nfrom scrapy.http.request import CookiesT, Request\nfrom scrapy.link import Link\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.selector import SelectorList\n\n\nResponseTypeVar = TypeVar(\"ResponseTypeVar\", bound=\"Response\")\n\n\nclass Response(object_ref):\n    \"\"\"An object that represents an HTTP response, which is usually\n    downloaded (by the Downloader) and fed to the Spiders for processing.\n    \"\"\"\n\n    attributes: Tuple[str, ...] = (\n        \"url\",\n        \"status\",\n        \"headers\",\n        \"body\",\n        \"flags\",\n        \"request\",\n        \"certificate\",\n        \"ip_address\",\n        \"protocol\",\n    )\n    \"\"\"A tuple of :class:`str` objects containing the name of all public\n    attributes of the class that are also keyword parameters of the\n    ``__init__`` method.\n\n    Currently used by :meth:`Response.replace`.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        status: int = 200,\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: bytes = b\"\",\n        flags: Optional[List[str]] = None,\n        request: Optional[Request] = None,\n        certificate: Optional[Certificate] = None,\n        ip_address: Union[IPv4Address, IPv6Address, None] = None,\n        protocol: Optional[str] = None,\n    ):\n        self.headers: Headers = Headers(headers or {})\n        self.status: int = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request: Optional[Request] = request\n        self.flags: List[str] = [] if flags is None else list(flags)\n        self.certificate: Optional[Certificate] = certificate\n        self.ip_address: Union[IPv4Address, IPv6Address, None] = ip_address\n        self.protocol: Optional[str] = protocol\n\n    @property\n    def cb_kwargs(self) -> Dict[str, Any]:\n        try:\n            return self.request.cb_kwargs  # type: ignore[union-attr]\n        except AttributeError:\n            raise AttributeError(\n                \"Response.cb_kwargs not available, this response \"\n                \"is not tied to any request\"\n            )\n\n    @property\n    def meta(self) -> Dict[str, Any]:\n        try:\n            return self.request.meta  # type: ignore[union-attr]\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    def _set_url(self, url: str) -> None:\n        if isinstance(url, str):\n            self._url: str = url\n        else:\n            raise TypeError(\n                f\"{type(self).__name__} url must be str, \" f\"got {type(url).__name__}\"\n            )\n\n    @property\n    def body(self) -> bytes:\n        return self._body\n\n    def _set_body(self, body: Optional[bytes]) -> None:\n        if body is None:\n            self._body = b\"\"\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\"\n            )\n        else:\n            self._body = body\n\n    def __repr__(self) -> str:\n        return f\"<{self.status} {self.url}>\"\n\n    def copy(self) -> Self:\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()\n\n    @overload\n    def replace(\n        self, *args: Any, cls: Type[ResponseTypeVar], **kwargs: Any\n    ) -> ResponseTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: Optional[Type[Response]] = None, **kwargs: Any\n    ) -> Response:\n        \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n        for x in self.attributes:\n            kwargs.setdefault(x, getattr(self, x))\n        if cls is None:\n            cls = self.__class__\n        return cls(*args, **kwargs)\n\n    def urljoin(self, url: str) -> str:\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)\n\n    @property\n    def text(self) -> str:\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as str\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")\n\n    def css(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def jmespath(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def xpath(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def follow(\n        self,\n        url: Union[str, Link],\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n    ) -> Request:\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n        not only an absolute URL.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n\n        .. versionadded:: 2.0\n           The *flags* parameter.\n        \"\"\"\n        if encoding is None:\n            raise ValueError(\"encoding can't be None\")\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n\n        return Request(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n    def follow_all(\n        self,\n        urls: Iterable[Union[str, Link]],\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n    ) -> Iterable[Request]:\n        \"\"\"\n        .. versionadded:: 2.0\n\n        Return an iterable of :class:`~.Request` instances to follow all links\n        in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n        but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n        not only absolute URLs.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if not hasattr(urls, \"__iter__\"):\n            raise TypeError(\"'urls' argument must be an iterable\")\n        return (\n            self.follow(\n                url=url,\n                callback=callback,\n                method=method,\n                headers=headers,\n                body=body,\n                cookies=cookies,\n                meta=meta,\n                encoding=encoding,\n                priority=priority,\n                dont_filter=dont_filter,\n                errback=errback,\n                cb_kwargs=cb_kwargs,\n                flags=flags,\n            )\n            for url in urls\n        )\n", "scrapy/http/response/text.py": "\"\"\"\nThis module implements the TextResponse class which adds encoding handling and\ndiscovering (through HTTP headers) to base Response class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom contextlib import suppress\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import urljoin\n\nimport parsel\nfrom w3lib.encoding import (\n    html_body_declared_encoding,\n    html_to_unicode,\n    http_content_type_encoding,\n    read_bom,\n    resolve_encoding,\n)\nfrom w3lib.html import strip_html5_whitespace\n\nfrom scrapy.http.request import CookiesT, Request\nfrom scrapy.http.response import Response\nfrom scrapy.link import Link\nfrom scrapy.utils.python import memoizemethod_noargs, to_unicode\nfrom scrapy.utils.response import get_base_url\n\nif TYPE_CHECKING:\n    from scrapy.selector import Selector, SelectorList\n\n_NONE = object()\n\n\nclass TextResponse(Response):\n    _DEFAULT_ENCODING = \"ascii\"\n    _cached_decoded_json = _NONE\n\n    attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        self._encoding: Optional[str] = kwargs.pop(\"encoding\", None)\n        self._cached_benc: Optional[str] = None\n        self._cached_ubody: Optional[str] = None\n        self._cached_selector: Optional[Selector] = None\n        super().__init__(*args, **kwargs)\n\n    def _set_body(self, body: Union[str, bytes, None]) -> None:\n        self._body: bytes = b\"\"  # used by encoding detection\n        if isinstance(body, str):\n            if self._encoding is None:\n                raise TypeError(\n                    \"Cannot convert unicode body - \"\n                    f\"{type(self).__name__} has no encoding\"\n                )\n            self._body = body.encode(self._encoding)\n        else:\n            super()._set_body(body)\n\n    @property\n    def encoding(self) -> str:\n        return self._declared_encoding() or self._body_inferred_encoding()\n\n    def _declared_encoding(self) -> Optional[str]:\n        return (\n            self._encoding\n            or self._bom_encoding()\n            or self._headers_encoding()\n            or self._body_declared_encoding()\n        )\n\n    def json(self) -> Any:\n        \"\"\"\n        .. versionadded:: 2.2\n\n        Deserialize a JSON document to a Python object.\n        \"\"\"\n        if self._cached_decoded_json is _NONE:\n            self._cached_decoded_json = json.loads(self.body)\n        return self._cached_decoded_json\n\n    @property\n    def text(self) -> str:\n        \"\"\"Body as unicode\"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = f\"charset={benc}\"\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody\n\n    def urljoin(self, url: str) -> str:\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)\n\n    @memoizemethod_noargs\n    def _headers_encoding(self) -> Optional[str]:\n        content_type = cast(bytes, self.headers.get(b\"Content-Type\", b\"\"))\n        return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n\n    def _body_inferred_encoding(self) -> str:\n        if self._cached_benc is None:\n            content_type = to_unicode(\n                cast(bytes, self.headers.get(b\"Content-Type\", b\"\")), encoding=\"latin-1\"\n            )\n            benc, ubody = html_to_unicode(\n                content_type,\n                self.body,\n                auto_detect_fun=self._auto_detect_fun,\n                default_encoding=self._DEFAULT_ENCODING,\n            )\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc\n\n    def _auto_detect_fun(self, text: bytes) -> Optional[str]:\n        for enc in (self._DEFAULT_ENCODING, \"utf-8\", \"cp1252\"):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)\n        return None\n\n    @memoizemethod_noargs\n    def _body_declared_encoding(self) -> Optional[str]:\n        return html_body_declared_encoding(self.body)\n\n    @memoizemethod_noargs\n    def _bom_encoding(self) -> Optional[str]:\n        return read_bom(self.body)[0]\n\n    @property\n    def selector(self) -> Selector:\n        from scrapy.selector import Selector\n\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector\n\n    def jmespath(self, query: str, **kwargs: Any) -> SelectorList:\n        from scrapy.selector import SelectorList\n\n        if not hasattr(self.selector, \"jmespath\"):\n            raise AttributeError(\n                \"Please install parsel >= 1.8.1 to get jmespath support\"\n            )\n\n        return cast(SelectorList, self.selector.jmespath(query, **kwargs))\n\n    def xpath(self, query: str, **kwargs: Any) -> SelectorList:\n        from scrapy.selector import SelectorList\n\n        return cast(SelectorList, self.selector.xpath(query, **kwargs))\n\n    def css(self, query: str) -> SelectorList:\n        from scrapy.selector import SelectorList\n\n        return cast(SelectorList, self.selector.css(query))\n\n    def follow(\n        self,\n        url: Union[str, Link, parsel.Selector],\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = None,\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n    ) -> Request:\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be not only an absolute URL, but also\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        See :ref:`response-follow-example` for usage examples.\n        \"\"\"\n        if isinstance(url, parsel.Selector):\n            url = _url_from_selector(url)\n        elif isinstance(url, parsel.SelectorList):\n            raise ValueError(\"SelectorList is not supported\")\n        encoding = self.encoding if encoding is None else encoding\n        return super().follow(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n    def follow_all(\n        self,\n        urls: Union[Iterable[Union[str, Link]], parsel.SelectorList, None] = None,\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = None,\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n        css: Optional[str] = None,\n        xpath: Optional[str] = None,\n    ) -> Iterable[Request]:\n        \"\"\"\n        A generator that produces :class:`~.Request` instances to follow all\n        links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n        ``__init__`` method, except that each ``urls`` element does not need to be\n        an absolute URL, it can be any of the following:\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n\n        Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n        using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n        selectors from which links cannot be obtained (for instance, anchor tags without an\n        ``href`` attribute)\n        \"\"\"\n        arguments = [x for x in (urls, css, xpath) if x is not None]\n        if len(arguments) != 1:\n            raise ValueError(\n                \"Please supply exactly one of the following arguments: urls, css, xpath\"\n            )\n        if not urls:\n            if css:\n                urls = self.css(css)\n            if xpath:\n                urls = self.xpath(xpath)\n        if isinstance(urls, parsel.SelectorList):\n            selectors = urls\n            urls = []\n            for sel in selectors:\n                with suppress(_InvalidSelector):\n                    urls.append(_url_from_selector(sel))\n        return super().follow_all(\n            urls=cast(Iterable[Union[str, Link]], urls),\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n\nclass _InvalidSelector(ValueError):\n    \"\"\"\n    Raised when a URL cannot be obtained from a Selector\n    \"\"\"\n\n\ndef _url_from_selector(sel: parsel.Selector) -> str:\n    if isinstance(sel.root, str):\n        # e.g. ::attr(href) result\n        return strip_html5_whitespace(sel.root)\n    if not hasattr(sel.root, \"tag\"):\n        raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n    if sel.root.tag not in (\"a\", \"link\"):\n        raise _InvalidSelector(\n            \"Only <a> and <link> elements are supported; \" f\"got <{sel.root.tag}>\"\n        )\n    href = sel.root.get(\"href\")\n    if href is None:\n        raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n    return strip_html5_whitespace(href)\n", "scrapy/http/request/form.py": "\"\"\"\nThis module implements the FormRequest class which is a more convenient class\n(than Request) to generate Requests based on form data.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n\nfrom lxml.html import FormElement  # nosec\nfrom lxml.html import InputElement  # nosec\nfrom lxml.html import MultipleSelectOptions  # nosec\nfrom lxml.html import SelectElement  # nosec\nfrom lxml.html import TextareaElement  # nosec\nfrom w3lib.html import strip_html5_whitespace\n\nfrom scrapy.http.request import Request\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.utils.python import is_listlike, to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nFormdataVType = Union[str, Iterable[str]]\nFormdataKVType = Tuple[str, FormdataVType]\nFormdataType = Optional[Union[Dict[str, FormdataVType], List[FormdataKVType]]]\n\n\nclass FormRequest(Request):\n    valid_form_methods = [\"GET\", \"POST\"]\n\n    def __init__(\n        self, *args: Any, formdata: FormdataType = None, **kwargs: Any\n    ) -> None:\n        if formdata and kwargs.get(\"method\") is None:\n            kwargs[\"method\"] = \"POST\"\n\n        super().__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            form_query_str = _urlencode(items, self.encoding)\n            if self.method == \"POST\":\n                self.headers.setdefault(\n                    b\"Content-Type\", b\"application/x-www-form-urlencoded\"\n                )\n                self._set_body(form_query_str)\n            else:\n                self._set_url(\n                    urlunsplit(urlsplit(self.url)._replace(query=form_query_str))\n                )\n\n    @classmethod\n    def from_response(\n        cls,\n        response: TextResponse,\n        formname: Optional[str] = None,\n        formid: Optional[str] = None,\n        formnumber: int = 0,\n        formdata: FormdataType = None,\n        clickdata: Optional[Dict[str, Union[str, int]]] = None,\n        dont_click: bool = False,\n        formxpath: Optional[str] = None,\n        formcss: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Self:\n        kwargs.setdefault(\"encoding\", response.encoding)\n\n        if formcss is not None:\n            from parsel.csstranslator import HTMLTranslator\n\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata)\n        url = _get_form_url(form, kwargs.pop(\"url\", None))\n\n        method = kwargs.pop(\"method\", form.method)\n        if method is not None:\n            method = method.upper()\n            if method not in cls.valid_form_methods:\n                method = \"GET\"\n\n        return cls(url=url, method=method, formdata=formdata, **kwargs)\n\n\ndef _get_form_url(form: FormElement, url: Optional[str]) -> str:\n    assert form.base_url is not None  # typing\n    if url is None:\n        action = form.get(\"action\")\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n    return urljoin(form.base_url, url)\n\n\ndef _urlencode(seq: Iterable[FormdataKVType], enc: str) -> str:\n    values = [\n        (to_bytes(k, enc), to_bytes(v, enc))\n        for k, vs in seq\n        for v in (cast(Iterable[str], vs) if is_listlike(vs) else [cast(str, vs)])\n    ]\n    return urlencode(values, doseq=True)\n\n\ndef _get_form(\n    response: TextResponse,\n    formname: Optional[str],\n    formid: Optional[str],\n    formnumber: int,\n    formxpath: Optional[str],\n) -> FormElement:\n    \"\"\"Find the wanted form element within the given response.\"\"\"\n    root = response.selector.root\n    forms = root.xpath(\"//form\")\n    if not forms:\n        raise ValueError(f\"No <form> element found in {response}\")\n\n    if formname is not None:\n        f = root.xpath(f'//form[@name=\"{formname}\"]')\n        if f:\n            return cast(FormElement, f[0])\n\n    if formid is not None:\n        f = root.xpath(f'//form[@id=\"{formid}\"]')\n        if f:\n            return cast(FormElement, f[0])\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == \"form\":\n                    return cast(FormElement, el)\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError(f\"No <form> element found with {formxpath}\")\n\n    # If we get here, it means that either formname was None or invalid\n    try:\n        form = forms[formnumber]\n    except IndexError:\n        raise IndexError(f\"Form number {formnumber} not found in {response}\")\n    else:\n        return cast(FormElement, form)\n\n\ndef _get_inputs(\n    form: FormElement,\n    formdata: FormdataType,\n    dont_click: bool,\n    clickdata: Optional[Dict[str, Union[str, int]]],\n) -> List[FormdataKVType]:\n    \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n    try:\n        formdata_keys = dict(formdata or ()).keys()\n    except (ValueError, TypeError):\n        raise ValueError(\"formdata should be a dict or iterable of tuples\")\n\n    if not formdata:\n        formdata = []\n    inputs = form.xpath(\n        \"descendant::textarea\"\n        \"|descendant::select\"\n        \"|descendant::input[not(@type) or @type[\"\n        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n        \" and (../@checked or\"\n        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n        namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n    )\n    values: List[FormdataKVType] = [\n        (k, \"\" if v is None else v)\n        for k, v in (_value(e) for e in inputs)\n        if k and k not in formdata_keys\n    ]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    formdata_items = formdata.items() if isinstance(formdata, dict) else formdata\n    values.extend((k, v) for k, v in formdata_items if v is not None)\n    return values\n\n\ndef _value(\n    ele: Union[InputElement, SelectElement, TextareaElement]\n) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n    n = ele.name\n    v = ele.value\n    if ele.tag == \"select\":\n        return _select_value(cast(SelectElement, ele), n, v)\n    return n, v\n\n\ndef _select_value(\n    ele: SelectElement, n: Optional[str], v: Union[None, str, MultipleSelectOptions]\n) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags without options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    return n, v\n\n\ndef _get_clickable(\n    clickdata: Optional[Dict[str, Union[str, int]]], form: FormElement\n) -> Optional[Tuple[str, str]]:\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = list(\n        form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n            namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n        )\n    )\n    if not clickables:\n        return None\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get(\"name\"), el.get(\"value\") or \"\")\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get(\"nr\", None)\n    if nr is not None:\n        assert isinstance(nr, int)\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get(\"name\"), el.get(\"value\") or \"\")\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = \".//*\" + \"\".join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get(\"name\"), el[0].get(\"value\") or \"\")\n    if len(el) > 1:\n        raise ValueError(\n            f\"Multiple elements found ({el!r}) matching the \"\n            f\"criteria in clickdata: {clickdata!r}\"\n        )\n    else:\n        raise ValueError(f\"No clickable element matching clickdata: {clickdata!r}\")\n", "scrapy/http/request/json_request.py": "\"\"\"\nThis module implements the JsonRequest class which is a more convenient class\n(than Request) to generate JSON Requests.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Type, overload\n\nfrom scrapy.http.request import Request, RequestTypeVar\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass JsonRequest(Request):\n    attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n\n    def __init__(\n        self, *args: Any, dumps_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> None:\n        dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n        dumps_kwargs.setdefault(\"sort_keys\", True)\n        self._dumps_kwargs: Dict[str, Any] = dumps_kwargs\n\n        body_passed = kwargs.get(\"body\", None) is not None\n        data: Any = kwargs.pop(\"data\", None)\n        data_passed: bool = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn(\"Both body and data passed. data will be ignored\")\n        elif not body_passed and data_passed:\n            kwargs[\"body\"] = self._dumps(data)\n            if \"method\" not in kwargs:\n                kwargs[\"method\"] = \"POST\"\n\n        super().__init__(*args, **kwargs)\n        self.headers.setdefault(\"Content-Type\", \"application/json\")\n        self.headers.setdefault(\n            \"Accept\", \"application/json, text/javascript, */*; q=0.01\"\n        )\n\n    @property\n    def dumps_kwargs(self) -> Dict[str, Any]:\n        return self._dumps_kwargs\n\n    @overload\n    def replace(\n        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n    ) -> RequestTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n    ) -> Request:\n        body_passed = kwargs.get(\"body\", None) is not None\n        data: Any = kwargs.pop(\"data\", None)\n        data_passed: bool = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn(\"Both body and data passed. data will be ignored\")\n        elif not body_passed and data_passed:\n            kwargs[\"body\"] = self._dumps(data)\n\n        return super().replace(*args, cls=cls, **kwargs)\n\n    def _dumps(self, data: Any) -> str:\n        \"\"\"Convert to JSON\"\"\"\n        return json.dumps(data, **self._dumps_kwargs)\n", "scrapy/http/request/__init__.py": "\"\"\"\nThis module implements the Request class which is used to represent HTTP\nrequests in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    NoReturn,\n    Optional,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom w3lib.url import safe_url_string\n\nimport scrapy\nfrom scrapy.http.headers import Headers\nfrom scrapy.utils.curl import curl_to_request_kwargs\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import escape_ajax\n\nif TYPE_CHECKING:\n    # typing.NotRequired and typing.Self require Python 3.11\n    from typing_extensions import NotRequired, Self\n\n\nclass VerboseCookie(TypedDict):\n    name: str\n    value: str\n    domain: NotRequired[str]\n    path: NotRequired[str]\n    secure: NotRequired[bool]\n\n\nCookiesT = Union[Dict[str, str], List[VerboseCookie]]\n\n\nRequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n\n\ndef NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n    \"\"\"When assigned to the ``callback`` parameter of\n    :class:`~scrapy.http.Request`, it indicates that the request is not meant\n    to have a spider callback at all.\n\n    For example:\n\n    .. code-block:: python\n\n       Request(\"https://example.com\", callback=NO_CALLBACK)\n\n    This value should be used by :ref:`components <topics-components>` that\n    create and handle their own requests, e.g. through\n    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader\n    middlewares handling such requests can treat them differently from requests\n    intended for the :meth:`~scrapy.Spider.parse` callback.\n    \"\"\"\n    raise RuntimeError(\n        \"The NO_CALLBACK callback has been called. This is a special callback \"\n        \"value intended for requests whose callback is never meant to be \"\n        \"called.\"\n    )\n\n\nclass Request(object_ref):\n    \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n    executed by the Downloader, thus generating a :class:`Response`.\n    \"\"\"\n\n    attributes: Tuple[str, ...] = (\n        \"url\",\n        \"callback\",\n        \"method\",\n        \"headers\",\n        \"body\",\n        \"cookies\",\n        \"meta\",\n        \"encoding\",\n        \"priority\",\n        \"dont_filter\",\n        \"errback\",\n        \"flags\",\n        \"cb_kwargs\",\n    )\n    \"\"\"A tuple of :class:`str` objects containing the name of all public\n    attributes of the class that are also keyword parameters of the\n    ``__init__`` method.\n\n    Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n    :func:`~scrapy.utils.request.request_from_dict`.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: str = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        flags: Optional[List[str]] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        self._encoding: str = encoding  # this one has to be set first\n        self.method: str = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        if not isinstance(priority, int):\n            raise TypeError(f\"Request priority not an integer: {priority!r}\")\n        self.priority: int = priority\n\n        if not (callable(callback) or callback is None):\n            raise TypeError(\n                f\"callback must be a callable, got {type(callback).__name__}\"\n            )\n        if not (callable(errback) or errback is None):\n            raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n        self.callback: Optional[Callable] = callback\n        self.errback: Optional[Callable] = errback\n\n        self.cookies: CookiesT = cookies or {}\n        self.headers: Headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter: bool = dont_filter\n\n        self._meta: Optional[Dict[str, Any]] = dict(meta) if meta else None\n        self._cb_kwargs: Optional[Dict[str, Any]] = (\n            dict(cb_kwargs) if cb_kwargs else None\n        )\n        self.flags: List[str] = [] if flags is None else list(flags)\n\n    @property\n    def cb_kwargs(self) -> Dict[str, Any]:\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs\n\n    @property\n    def meta(self) -> Dict[str, Any]:\n        if self._meta is None:\n            self._meta = {}\n        return self._meta\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    def _set_url(self, url: str) -> None:\n        if not isinstance(url, str):\n            raise TypeError(f\"Request url must be str, got {type(url).__name__}\")\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if (\n            \"://\" not in self._url\n            and not self._url.startswith(\"about:\")\n            and not self._url.startswith(\"data:\")\n        ):\n            raise ValueError(f\"Missing scheme in request url: {self._url}\")\n\n    @property\n    def body(self) -> bytes:\n        return self._body\n\n    def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n        self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n\n    @property\n    def encoding(self) -> str:\n        return self._encoding\n\n    def __repr__(self) -> str:\n        return f\"<{self.method} {self.url}>\"\n\n    def copy(self) -> Self:\n        return self.replace()\n\n    @overload\n    def replace(\n        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n    ) -> RequestTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n    ) -> Request:\n        \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n        for x in self.attributes:\n            kwargs.setdefault(x, getattr(self, x))\n        if cls is None:\n            cls = self.__class__\n        return cls(*args, **kwargs)\n\n    @classmethod\n    def from_curl(\n        cls,\n        curl_command: str,\n        ignore_unknown_options: bool = True,\n        **kwargs: Any,\n    ) -> Self:\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JsonRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n        To translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n        \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n\n    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> Dict[str, Any]:\n        \"\"\"Return a dictionary containing the Request's data.\n\n        Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n        If a spider is given, this method will try to find out the name of the spider methods used as callback\n        and errback and include them in the output dict, raising an exception if they cannot be found.\n        \"\"\"\n        d = {\n            \"url\": self.url,  # urls are safe (safe_string_url)\n            \"callback\": (\n                _find_method(spider, self.callback)\n                if callable(self.callback)\n                else self.callback\n            ),\n            \"errback\": (\n                _find_method(spider, self.errback)\n                if callable(self.errback)\n                else self.errback\n            ),\n            \"headers\": dict(self.headers),\n        }\n        for attr in self.attributes:\n            d.setdefault(attr, getattr(self, attr))\n        if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck\n            d[\"_class\"] = self.__module__ + \".\" + self.__class__.__name__\n        return d\n\n\ndef _find_method(obj: Any, func: Callable[..., Any]) -> str:\n    \"\"\"Helper function for Request.to_dict\"\"\"\n    # Only instance methods contain ``__func__``\n    if obj and hasattr(func, \"__func__\"):\n        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n        for name, obj_func in members:\n            # We need to use __func__ to access the original function object because instance\n            # method objects are generated each time attribute is retrieved from instance.\n            #\n            # Reference: The standard type hierarchy\n            # https://docs.python.org/3/reference/datamodel.html\n            if obj_func.__func__ is func.__func__:\n                return name\n    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")\n", "scrapy/http/request/rpc.py": "\"\"\"\nThis module implements the XmlRpcRequest class which is a more convenient class\n(that Request) to generate xml-rpc requests.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nimport xmlrpc.client as xmlrpclib\nfrom typing import Any, Optional\n\nimport defusedxml.xmlrpc\n\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import get_func_args\n\ndefusedxml.xmlrpc.monkey_patch()\n\nDUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n\n\nclass XmlRpcRequest(Request):\n    def __init__(self, *args: Any, encoding: Optional[str] = None, **kwargs: Any):\n        if \"body\" not in kwargs and \"params\" in kwargs:\n            kw = {k: kwargs.pop(k) for k in DUMPS_ARGS if k in kwargs}\n            kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault(\"method\", \"POST\")\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault(\"dont_filter\", True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs[\"encoding\"] = encoding\n\n        super().__init__(*args, **kwargs)\n        self.headers.setdefault(\"Content-Type\", \"text/xml\")\n"}