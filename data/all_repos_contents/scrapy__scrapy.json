{"setup.py": "from pathlib import Path\n\nfrom setuptools import find_packages, setup\n\nversion = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n\n\ninstall_requires = [\n    \"Twisted>=18.9.0\",\n    \"cryptography>=36.0.0\",\n    \"cssselect>=0.9.1\",\n    \"itemloaders>=1.0.1\",\n    \"parsel>=1.5.0\",\n    \"pyOpenSSL>=21.0.0\",\n    \"queuelib>=1.4.2\",\n    \"service_identity>=18.1.0\",\n    \"w3lib>=1.17.0\",\n    \"zope.interface>=5.1.0\",\n    \"protego>=0.1.15\",\n    \"itemadapter>=0.1.0\",\n    \"setuptools\",\n    \"packaging\",\n    \"tldextract\",\n    \"lxml>=4.4.1\",\n    \"defusedxml>=0.7.1\",\n]\nextras_require = {\n    ':platform_python_implementation == \"CPython\"': [\"PyDispatcher>=2.0.5\"],\n    ':platform_python_implementation == \"PyPy\"': [\"PyPyDispatcher>=2.1.0\"],\n}\n\n\nsetup(\n    name=\"Scrapy\",\n    version=version,\n    url=\"https://scrapy.org\",\n    project_urls={\n        \"Documentation\": \"https://docs.scrapy.org/\",\n        \"Source\": \"https://github.com/scrapy/scrapy\",\n        \"Tracker\": \"https://github.com/scrapy/scrapy/issues\",\n    },\n    description=\"A high-level Web Crawling and Web Scraping framework\",\n    long_description=open(\"README.rst\", encoding=\"utf-8\").read(),\n    author=\"Scrapy developers\",\n    author_email=\"pablo@pablohoffman.com\",\n    maintainer=\"Pablo Hoffman\",\n    maintainer_email=\"pablo@pablohoffman.com\",\n    license=\"BSD\",\n    packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n    include_package_data=True,\n    zip_safe=False,\n    entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n    classifiers=[\n        \"Framework :: Scrapy\",\n        \"Development Status :: 5 - Production/Stable\",\n        \"Environment :: Console\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=install_requires,\n    extras_require=extras_require,\n)\n", "conftest.py": "from pathlib import Path\n\nimport pytest\nfrom twisted.web.http import H2_ENABLED\n\nfrom scrapy.utils.reactor import install_reactor\nfrom tests.keys import generate_keys\n\n\ndef _py_files(folder):\n    return (str(p) for p in Path(folder).rglob(\"*.py\"))\n\n\ncollect_ignore = [\n    # not a test, but looks like a test\n    \"scrapy/utils/testsite.py\",\n    \"tests/ftpserver.py\",\n    \"tests/mockserver.py\",\n    \"tests/pipelines.py\",\n    \"tests/spiders.py\",\n    # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n    *_py_files(\"tests/CrawlerProcess\"),\n    # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess\n    *_py_files(\"tests/CrawlerRunner\"),\n]\n\nwith Path(\"tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n    for line in reader:\n        file_path = line.strip()\n        if file_path and file_path[0] != \"#\":\n            collect_ignore.append(file_path)\n\nif not H2_ENABLED:\n    collect_ignore.extend(\n        (\n            \"scrapy/core/downloader/handlers/http2.py\",\n            *_py_files(\"scrapy/core/http2\"),\n        )\n    )\n\n\n@pytest.fixture()\ndef chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--reactor\",\n        default=\"default\",\n        choices=[\"default\", \"asyncio\"],\n    )\n\n\n@pytest.fixture(scope=\"class\")\ndef reactor_pytest(request):\n    if not request.cls:\n        # doctests\n        return\n    request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n    return request.cls.reactor_pytest\n\n\n@pytest.fixture(autouse=True)\ndef only_asyncio(request, reactor_pytest):\n    if request.node.get_closest_marker(\"only_asyncio\") and reactor_pytest != \"asyncio\":\n        pytest.skip(\"This test is only run with --reactor=asyncio\")\n\n\n@pytest.fixture(autouse=True)\ndef only_not_asyncio(request, reactor_pytest):\n    if (\n        request.node.get_closest_marker(\"only_not_asyncio\")\n        and reactor_pytest == \"asyncio\"\n    ):\n        pytest.skip(\"This test is only run without --reactor=asyncio\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_uvloop(request):\n    if not request.node.get_closest_marker(\"requires_uvloop\"):\n        return\n    try:\n        import uvloop\n\n        del uvloop\n    except ImportError:\n        pytest.skip(\"uvloop is not installed\")\n\n\ndef pytest_configure(config):\n    if config.getoption(\"--reactor\") == \"asyncio\":\n        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n\n# Generate localhost certificate files, needed by some tests\ngenerate_keys()\n", "extras/qps-bench-server.py": "#!/usr/bin/env python\nfrom collections import deque\nfrom time import time\n\nfrom twisted.internet import reactor\nfrom twisted.web.resource import Resource\nfrom twisted.web.server import NOT_DONE_YET, Site\n\n\nclass Root(Resource):\n    def __init__(self):\n        Resource.__init__(self)\n        self.concurrent = 0\n        self.tail = deque(maxlen=100)\n        self._reset_stats()\n\n    def _reset_stats(self):\n        self.tail.clear()\n        self.start = self.lastmark = self.lasttime = time()\n\n    def getChild(self, request, name):\n        return self\n\n    def render(self, request):\n        now = time()\n        delta = now - self.lasttime\n\n        # reset stats on high iter-request times caused by client restarts\n        if delta > 3:  # seconds\n            self._reset_stats()\n            return \"\"\n\n        self.tail.appendleft(delta)\n        self.lasttime = now\n        self.concurrent += 1\n\n        if now - self.lastmark >= 3:\n            self.lastmark = now\n            qps = len(self.tail) / sum(self.tail)\n            print(\n                f\"samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}\"\n            )\n\n        if \"latency\" in request.args:\n            latency = float(request.args[\"latency\"][0])\n            reactor.callLater(latency, self._finish, request)\n            return NOT_DONE_YET\n\n        self.concurrent -= 1\n        return \"\"\n\n    def _finish(self, request):\n        self.concurrent -= 1\n        if not request.finished and not request._disconnected:\n            request.finish()\n\n\nroot = Root()\nfactory = Site(root)\nreactor.listenTCP(8880, factory)\nreactor.run()\n", "extras/qpsclient.py": "\"\"\"\nA spider that generate light requests to measure QPS throughput\n\nusage:\n\n    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0\n     --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n\n\"\"\"\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\n\n\nclass QPSSpider(Spider):\n    name = \"qps\"\n    benchurl = \"http://localhost:8880/\"\n\n    # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n    max_concurrent_requests = 8\n    # Requests per second goal\n    qps = None  # same as: 1 / download_delay\n    download_delay = None\n    # time in seconds to delay server responses\n    latency = None\n    # number of slots to create\n    slots = 1\n\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        if self.qps is not None:\n            self.qps = float(self.qps)\n            self.download_delay = 1 / self.qps\n        elif self.download_delay is not None:\n            self.download_delay = float(self.download_delay)\n\n    def start_requests(self):\n        url = self.benchurl\n        if self.latency is not None:\n            url += f\"?latency={self.latency}\"\n\n        slots = int(self.slots)\n        if slots > 1:\n            urls = [url.replace(\"localhost\", f\"127.0.0.{x + 1}\") for x in range(slots)]\n        else:\n            urls = [url]\n\n        idx = 0\n        while True:\n            url = urls[idx % len(urls)]\n            yield Request(url, dont_filter=True)\n            idx += 1\n\n    def parse(self, response):\n        pass\n", "scrapy/shell.py": "\"\"\"Scrapy Shell\n\nSee documentation in docs/topics/shell.rst\n\n\"\"\"\n\nimport os\nimport signal\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nfrom itemadapter import is_item\nfrom twisted.internet import defer, threads\nfrom twisted.python import threadable\nfrom w3lib.url import any_to_uri\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.conf import get_config\nfrom scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\nfrom scrapy.utils.response import open_in_browser\n\n\nclass Shell:\n    relevant_classes: Tuple[type, ...] = (Crawler, Spider, Request, Response, Settings)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        update_vars: Optional[Callable[[Dict[str, Any]], None]] = None,\n        code: Optional[str] = None,\n    ):\n        self.crawler: Crawler = crawler\n        self.update_vars: Callable[[Dict[str, Any]], None] = update_vars or (\n            lambda x: None\n        )\n        self.item_class: type = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n        self.spider: Optional[Spider] = None\n        self.inthread: bool = not threadable.isInIOThread()\n        self.code: Optional[str] = code\n        self.vars: Dict[str, Any] = {}\n\n    def start(\n        self,\n        url: Optional[str] = None,\n        request: Optional[Request] = None,\n        response: Optional[Response] = None,\n        spider: Optional[Spider] = None,\n        redirect: bool = True,\n    ) -> None:\n        # disable accidental Ctrl-C key press from shutting down the engine\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        if url:\n            self.fetch(url, spider, redirect=redirect)\n        elif request:\n            self.fetch(request, spider)\n        elif response:\n            request = response.request\n            self.populate_vars(response, request, spider)\n        else:\n            self.populate_vars()\n        if self.code:\n            print(eval(self.code, globals(), self.vars))  # nosec\n        else:\n            \"\"\"\n            Detect interactive shell setting in scrapy.cfg\n            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n            [settings]\n            # shell can be one of ipython, bpython or python;\n            # to be used as the interactive python console, if available.\n            # (default is ipython, fallbacks in the order listed above)\n            shell = python\n            \"\"\"\n            cfg = get_config()\n            section, option = \"settings\", \"shell\"\n            env = os.environ.get(\"SCRAPY_PYTHON_SHELL\")\n            shells = []\n            if env:\n                shells += env.strip().lower().split(\",\")\n            elif cfg.has_option(section, option):\n                shells += [cfg.get(section, option).strip().lower()]\n            else:  # try all by default\n                shells += DEFAULT_PYTHON_SHELLS.keys()\n            # always add standard shell as fallback\n            shells += [\"python\"]\n            start_python_console(\n                self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n            )\n\n    def _schedule(self, request: Request, spider: Optional[Spider]) -> defer.Deferred:\n        if is_asyncio_reactor_installed():\n            # set the asyncio event loop for the current thread\n            event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n            set_asyncio_event_loop(event_loop_path)\n        spider = self._open_spider(request, spider)\n        d = _request_deferred(request)\n        d.addCallback(lambda x: (x, spider))\n        assert self.crawler.engine\n        self.crawler.engine.crawl(request)\n        return d\n\n    def _open_spider(self, request: Request, spider: Optional[Spider]) -> Spider:\n        if self.spider:\n            return self.spider\n\n        if spider is None:\n            spider = self.crawler.spider or self.crawler._create_spider()\n\n        self.crawler.spider = spider\n        assert self.crawler.engine\n        self.crawler.engine.open_spider(spider, close_if_idle=False)\n        self.spider = spider\n        return spider\n\n    def fetch(\n        self,\n        request_or_url: Union[Request, str],\n        spider: Optional[Spider] = None,\n        redirect: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        from twisted.internet import reactor\n\n        if isinstance(request_or_url, Request):\n            request = request_or_url\n        else:\n            url = any_to_uri(request_or_url)\n            request = Request(url, dont_filter=True, **kwargs)\n            if redirect:\n                request.meta[\"handle_httpstatus_list\"] = SequenceExclude(\n                    range(300, 400)\n                )\n            else:\n                request.meta[\"handle_httpstatus_all\"] = True\n        response = None\n        try:\n            response, spider = threads.blockingCallFromThread(\n                reactor, self._schedule, request, spider\n            )\n        except IgnoreRequest:\n            pass\n        self.populate_vars(response, request, spider)\n\n    def populate_vars(\n        self,\n        response: Optional[Response] = None,\n        request: Optional[Request] = None,\n        spider: Optional[Spider] = None,\n    ) -> None:\n        import scrapy\n\n        self.vars[\"scrapy\"] = scrapy\n        self.vars[\"crawler\"] = self.crawler\n        self.vars[\"item\"] = self.item_class()\n        self.vars[\"settings\"] = self.crawler.settings\n        self.vars[\"spider\"] = spider\n        self.vars[\"request\"] = request\n        self.vars[\"response\"] = response\n        if self.inthread:\n            self.vars[\"fetch\"] = self.fetch\n        self.vars[\"view\"] = open_in_browser\n        self.vars[\"shelp\"] = self.print_help\n        self.update_vars(self.vars)\n        if not self.code:\n            self.vars[\"banner\"] = self.get_help()\n\n    def print_help(self) -> None:\n        print(self.get_help())\n\n    def get_help(self) -> str:\n        b = []\n        b.append(\"Available Scrapy objects:\")\n        b.append(\n            \"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\"\n        )\n        for k, v in sorted(self.vars.items()):\n            if self._is_relevant(v):\n                b.append(f\"  {k:<10} {v}\")\n        b.append(\"Useful shortcuts:\")\n        if self.inthread:\n            b.append(\n                \"  fetch(url[, redirect=True]) \"\n                \"Fetch URL and update local objects (by default, redirects are followed)\"\n            )\n            b.append(\n                \"  fetch(req)                  \"\n                \"Fetch a scrapy.Request and update local objects \"\n            )\n        b.append(\"  shelp()           Shell help (print this help)\")\n        b.append(\"  view(response)    View response in a browser\")\n\n        return \"\\n\".join(f\"[s] {line}\" for line in b)\n\n    def _is_relevant(self, value: Any) -> bool:\n        return isinstance(value, self.relevant_classes) or is_item(value)\n\n\ndef inspect_response(response: Response, spider: Spider) -> None:\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    # Shell.start removes the SIGINT handler, so save it and re-add it after\n    # the shell has closed\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    Shell(spider.crawler).start(response=response, spider=spider)\n    signal.signal(signal.SIGINT, sigint_handler)\n\n\ndef _request_deferred(request: Request) -> defer.Deferred:\n    \"\"\"Wrap a request inside a Deferred.\n\n    This function is harmful, do not use it until you know what you are doing.\n\n    This returns a Deferred whose first pair of callbacks are the request\n    callback and errback. The Deferred also triggers when the request\n    callback/errback is executed (i.e. when the request is downloaded)\n\n    WARNING: Do not call request.replace() until after the deferred is called.\n    \"\"\"\n    request_callback = request.callback\n    request_errback = request.errback\n\n    def _restore_callbacks(result: Any) -> Any:\n        request.callback = request_callback\n        request.errback = request_errback\n        return result\n\n    d: defer.Deferred = defer.Deferred()\n    d.addBoth(_restore_callbacks)\n    if request.callback:\n        d.addCallback(request.callback)\n    if request.errback:\n        d.addErrback(request.errback)\n\n    request.callback, request.errback = d.callback, d.errback\n    return d\n", "scrapy/dupefilters.py": "from __future__ import annotations\n\nimport logging\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Optional, Set\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.http.request import Request\nfrom scrapy.settings import BaseSettings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.request import (\n    RequestFingerprinter,\n    RequestFingerprinterProtocol,\n    referer_str,\n)\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nclass BaseDupeFilter:\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls()\n\n    def request_seen(self, request: Request) -> bool:\n        return False\n\n    def open(self) -> Optional[Deferred]:\n        pass\n\n    def close(self, reason: str) -> Optional[Deferred]:\n        pass\n\n    def log(self, request: Request, spider: Spider) -> None:\n        \"\"\"Log that a request has been filtered\"\"\"\n        pass\n\n\nclass RFPDupeFilter(BaseDupeFilter):\n    \"\"\"Request Fingerprint duplicates filter\"\"\"\n\n    def __init__(\n        self,\n        path: Optional[str] = None,\n        debug: bool = False,\n        *,\n        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n    ) -> None:\n        self.file = None\n        self.fingerprinter: RequestFingerprinterProtocol = (\n            fingerprinter or RequestFingerprinter()\n        )\n        self.fingerprints: Set[str] = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = Path(path, \"requests.seen\").open(\"a+\", encoding=\"utf-8\")\n            self.file.seek(0)\n            self.fingerprints.update(x.rstrip() for x in self.file)\n\n    @classmethod\n    def from_settings(\n        cls,\n        settings: BaseSettings,\n        *,\n        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n    ) -> Self:\n        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.request_fingerprinter\n        return cls.from_settings(\n            crawler.settings,\n            fingerprinter=crawler.request_fingerprinter,\n        )\n\n    def request_seen(self, request: Request) -> bool:\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + \"\\n\")\n        return False\n\n    def request_fingerprint(self, request: Request) -> str:\n        return self.fingerprinter.fingerprint(request).hex()\n\n    def close(self, reason: str) -> None:\n        if self.file:\n            self.file.close()\n\n    def log(self, request: Request, spider: Spider) -> None:\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n            args = {\"request\": request, \"referer\": referer_str(request)}\n            self.logger.debug(msg, args, extra={\"spider\": spider})\n        elif self.logdupes:\n            msg = (\n                \"Filtered duplicate request: %(request)s\"\n                \" - no more duplicates will be shown\"\n                \" (see DUPEFILTER_DEBUG to show all duplicates)\"\n            )\n            self.logger.debug(msg, {\"request\": request}, extra={\"spider\": spider})\n            self.logdupes = False\n\n        assert spider.crawler.stats\n        spider.crawler.stats.inc_value(\"dupefilter/filtered\", spider=spider)\n", "scrapy/spiderloader.py": "from __future__ import annotations\n\nimport traceback\nimport warnings\nfrom collections import defaultdict\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type\n\nfrom zope.interface import implementer\n\nfrom scrapy import Request, Spider\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import walk_modules\nfrom scrapy.utils.spider import iter_spider_classes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n@implementer(ISpiderLoader)\nclass SpiderLoader:\n    \"\"\"\n    SpiderLoader is a class which locates and loads spiders\n    in a Scrapy project.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        self.spider_modules: List[str] = settings.getlist(\"SPIDER_MODULES\")\n        self.warn_only: bool = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n        self._spiders: Dict[str, Type[Spider]] = {}\n        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n        self._load_all_spiders()\n\n    def _check_name_duplicates(self) -> None:\n        dupes = []\n        for name, locations in self._found.items():\n            dupes.extend(\n                [\n                    f\"  {cls} named {name!r} (in {mod})\"\n                    for mod, cls in locations\n                    if len(locations) > 1\n                ]\n            )\n\n        if dupes:\n            dupes_string = \"\\n\\n\".join(dupes)\n            warnings.warn(\n                \"There are several spiders with the same name:\\n\\n\"\n                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                category=UserWarning,\n            )\n\n    def _load_spiders(self, module: ModuleType) -> None:\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls\n\n    def _load_all_spiders(self) -> None:\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError:\n                if self.warn_only:\n                    warnings.warn(\n                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n                        f\"from module '{name}'. \"\n                        \"See above traceback for details.\",\n                        category=RuntimeWarning,\n                    )\n                else:\n                    raise\n        self._check_name_duplicates()\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(settings)\n\n    def load(self, spider_name: str) -> Type[Spider]:\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(f\"Spider not found: {spider_name}\")\n\n    def find_by_request(self, request: Request) -> List[str]:\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [\n            name for name, cls in self._spiders.items() if cls.handles_request(request)\n        ]\n\n    def list(self) -> List[str]:\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())\n", "scrapy/exceptions.py": "\"\"\"\nScrapy core exceptions\n\nThese exceptions are documented in docs/topics/exceptions.rst. Please don't add\nnew exceptions here without documenting them there.\n\"\"\"\n\nfrom typing import Any\n\n# Internal\n\n\nclass NotConfigured(Exception):\n    \"\"\"Indicates a missing configuration situation\"\"\"\n\n    pass\n\n\nclass _InvalidOutput(TypeError):\n    \"\"\"\n    Indicates an invalid value has been returned by a middleware's processing method.\n    Internal and undocumented, it should not be raised or caught by user code.\n    \"\"\"\n\n    pass\n\n\n# HTTP and crawling\n\n\nclass IgnoreRequest(Exception):\n    \"\"\"Indicates a decision was made not to process a request\"\"\"\n\n\nclass DontCloseSpider(Exception):\n    \"\"\"Request the spider not to be closed yet\"\"\"\n\n    pass\n\n\nclass CloseSpider(Exception):\n    \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n\n    def __init__(self, reason: str = \"cancelled\"):\n        super().__init__()\n        self.reason = reason\n\n\nclass StopDownload(Exception):\n    \"\"\"\n    Stop the download of the body for a given response.\n    The 'fail' boolean parameter indicates whether or not the resulting partial response\n    should be handled by the request errback. Note that 'fail' is a keyword-only argument.\n    \"\"\"\n\n    def __init__(self, *, fail: bool = True):\n        super().__init__()\n        self.fail = fail\n\n\n# Items\n\n\nclass DropItem(Exception):\n    \"\"\"Drop item from the item pipeline\"\"\"\n\n    pass\n\n\nclass NotSupported(Exception):\n    \"\"\"Indicates a feature or method is not supported\"\"\"\n\n    pass\n\n\n# Commands\n\n\nclass UsageError(Exception):\n    \"\"\"To indicate a command-line usage error\"\"\"\n\n    def __init__(self, *a: Any, **kw: Any):\n        self.print_help = kw.pop(\"print_help\", True)\n        super().__init__(*a, **kw)\n\n\nclass ScrapyDeprecationWarning(Warning):\n    \"\"\"Warning category for deprecated features, since the default\n    DeprecationWarning is silenced on Python 2.7+\n    \"\"\"\n\n    pass\n\n\nclass ContractFail(AssertionError):\n    \"\"\"Error raised in case of a failing contract\"\"\"\n\n    pass\n", "scrapy/interfaces.py": "from zope.interface import Interface\n\n\nclass ISpiderLoader(Interface):\n    def from_settings(settings):\n        \"\"\"Return an instance of the class for the given settings\"\"\"\n\n    def load(spider_name):\n        \"\"\"Return the Spider class for the given spider name. If the spider\n        name is not found, it must raise a KeyError.\"\"\"\n\n    def list():\n        \"\"\"Return a list with the names of all spiders available in the\n        project\"\"\"\n\n    def find_by_request(request):\n        \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n", "scrapy/statscollectors.py": "\"\"\"\nScrapy extension for collecting scraping stats\n\"\"\"\n\nimport logging\nimport pprint\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nfrom scrapy import Spider\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nStatsT = Dict[str, Any]\n\n\nclass StatsCollector:\n    def __init__(self, crawler: \"Crawler\"):\n        self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n        self._stats: StatsT = {}\n\n    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return self._stats.get(key, default)\n\n    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:\n        return self._stats\n\n    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = value\n\n    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        self._stats = stats\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count\n\n    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = max(self._stats.setdefault(key, value), value)\n\n    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = min(self._stats.setdefault(key, value), value)\n\n    def clear_stats(self, spider: Optional[Spider] = None) -> None:\n        self._stats.clear()\n\n    def open_spider(self, spider: Spider) -> None:\n        pass\n\n    def close_spider(self, spider: Spider, reason: str) -> None:\n        if self._dump:\n            logger.info(\n                \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                extra={\"spider\": spider},\n            )\n        self._persist_stats(self._stats, spider)\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        pass\n\n\nclass MemoryStatsCollector(StatsCollector):\n    def __init__(self, crawler: \"Crawler\"):\n        super().__init__(crawler)\n        self.spider_stats: Dict[str, StatsT] = {}\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        self.spider_stats[spider.name] = stats\n\n\nclass DummyStatsCollector(StatsCollector):\n    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return default\n\n    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        pass\n\n    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n", "scrapy/resolver.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, List, Optional, Sequence, Type\n\nfrom twisted.internet import defer\nfrom twisted.internet.base import ReactorBase, ThreadedResolver\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostnameResolver,\n    IHostResolution,\n    IResolutionReceiver,\n    IResolverSimple,\n)\nfrom zope.interface.declarations import implementer, provider\n\nfrom scrapy.utils.datatypes import LocalCache\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n# TODO: cache misses\ndnscache: LocalCache[str, Any] = LocalCache(10000)\n\n\n@implementer(IResolverSimple)\nclass CachingThreadedResolver(ThreadedResolver):\n    \"\"\"\n    Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.\n    \"\"\"\n\n    def __init__(self, reactor: ReactorBase, cache_size: int, timeout: float):\n        super().__init__(reactor)\n        dnscache.limit = cache_size\n        self.timeout = timeout\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size, crawler.settings.getfloat(\"DNS_TIMEOUT\"))\n\n    def install_on_reactor(self) -> None:\n        self.reactor.installResolver(self)\n\n    def getHostByName(self, name: str, timeout: Sequence[int] = ()) -> Deferred[str]:\n        if name in dnscache:\n            return defer.succeed(dnscache[name])\n        # in Twisted<=16.6, getHostByName() is always called with\n        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n        # so the input argument above is simply overridden\n        # to enforce Scrapy's DNS_TIMEOUT setting's value\n        # The timeout arg is typed as Sequence[int] but supports floats.\n        timeout = (self.timeout,)  # type: ignore[assignment]\n        d = super().getHostByName(name, timeout)\n        if dnscache.limit:\n            d.addCallback(self._cache_result, name)\n        return d\n\n    def _cache_result(self, result: Any, name: str) -> Any:\n        dnscache[name] = result\n        return result\n\n\n@implementer(IHostResolution)\nclass HostResolution:\n    def __init__(self, name: str):\n        self.name: str = name\n\n    def cancel(self) -> None:\n        raise NotImplementedError()\n\n\n@provider(IResolutionReceiver)\nclass _CachingResolutionReceiver:\n    def __init__(self, resolutionReceiver: IResolutionReceiver, hostName: str):\n        self.resolutionReceiver: IResolutionReceiver = resolutionReceiver\n        self.hostName: str = hostName\n        self.addresses: List[IAddress] = []\n\n    def resolutionBegan(self, resolution: IHostResolution) -> None:\n        self.resolutionReceiver.resolutionBegan(resolution)\n        self.resolution = resolution\n\n    def addressResolved(self, address: IAddress) -> None:\n        self.resolutionReceiver.addressResolved(address)\n        self.addresses.append(address)\n\n    def resolutionComplete(self) -> None:\n        self.resolutionReceiver.resolutionComplete()\n        if self.addresses:\n            dnscache[self.hostName] = self.addresses\n\n\n@implementer(IHostnameResolver)\nclass CachingHostnameResolver:\n    \"\"\"\n    Experimental caching resolver. Resolves IPv4 and IPv6 addresses,\n    does not support setting a timeout value for DNS requests.\n    \"\"\"\n\n    def __init__(self, reactor: ReactorBase, cache_size: int):\n        self.reactor: ReactorBase = reactor\n        self.original_resolver: IHostnameResolver = reactor.nameResolver\n        dnscache.limit = cache_size\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n        else:\n            cache_size = 0\n        return cls(reactor, cache_size)\n\n    def install_on_reactor(self) -> None:\n        self.reactor.installNameResolver(self)\n\n    def resolveHostName(\n        self,\n        resolutionReceiver: IResolutionReceiver,\n        hostName: str,\n        portNumber: int = 0,\n        addressTypes: Optional[Sequence[Type[IAddress]]] = None,\n        transportSemantics: str = \"TCP\",\n    ) -> IHostResolution:\n        try:\n            addresses = dnscache[hostName]\n        except KeyError:\n            return self.original_resolver.resolveHostName(\n                _CachingResolutionReceiver(resolutionReceiver, hostName),\n                hostName,\n                portNumber,\n                addressTypes,\n                transportSemantics,\n            )\n        else:\n            resolutionReceiver.resolutionBegan(HostResolution(hostName))\n            for addr in addresses:\n                resolutionReceiver.addressResolved(addr)\n            resolutionReceiver.resolutionComplete()\n            return resolutionReceiver\n", "scrapy/extension.py": "\"\"\"\nThe Extension Manager\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom typing import Any, List\n\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import build_component_list\n\n\nclass ExtensionManager(MiddlewareManager):\n    component_name = \"extension\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"EXTENSIONS\"))\n", "scrapy/logformatter.py": "from __future__ import annotations\n\nimport logging\nimport os\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, TypedDict, Union\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import Response\nfrom scrapy.utils.request import referer_str\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nSCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\nDROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\nCRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\nITEMERRORMSG = \"Error processing %(item)s\"\nSPIDERERRORMSG = \"Spider error processing %(request)s (referer: %(referer)s)\"\nDOWNLOADERRORMSG_SHORT = \"Error downloading %(request)s\"\nDOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n\n\nclass LogFormatterResult(TypedDict):\n    level: int\n    msg: str\n    args: Union[Dict[str, Any], Tuple[Any, ...]]\n\n\nclass LogFormatter:\n    \"\"\"Class for generating log messages for different actions.\n\n    All methods must return a dictionary listing the parameters ``level``, ``msg``\n    and ``args`` which are going to be used for constructing the log message when\n    calling ``logging.log``.\n\n    Dictionary keys for the method outputs:\n\n    *   ``level`` is the log level for that action, you can use those from the\n        `python logging library <https://docs.python.org/3/library/logging.html>`_ :\n        ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR``\n        and ``logging.CRITICAL``.\n    *   ``msg`` should be a string that can contain different formatting placeholders.\n        This string, formatted with the provided ``args``, is going to be the long message\n        for that action.\n    *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.\n        The final log message is computed as ``msg % args``.\n\n    Users can define their own ``LogFormatter`` class if they want to customize how\n    each action is logged or if they want to omit it entirely. In order to omit\n    logging an action the method must return ``None``.\n\n    Here is an example on how to create a custom log formatter to lower the severity level of\n    the log message when an item is dropped from the pipeline::\n\n            class PoliteLogFormatter(logformatter.LogFormatter):\n                def dropped(self, item, exception, response, spider):\n                    return {\n                        'level': logging.INFO, # lowering the level from logging.WARNING\n                        'msg': \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\",\n                        'args': {\n                            'exception': exception,\n                            'item': item,\n                        }\n                    }\n    \"\"\"\n\n    def crawled(\n        self, request: Request, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n        request_flags = f\" {str(request.flags)}\" if request.flags else \"\"\n        response_flags = f\" {str(response.flags)}\" if response.flags else \"\"\n        return {\n            \"level\": logging.DEBUG,\n            \"msg\": CRAWLEDMSG,\n            \"args\": {\n                \"status\": response.status,\n                \"request\": request,\n                \"request_flags\": request_flags,\n                \"referer\": referer_str(request),\n                \"response_flags\": response_flags,\n                # backward compatibility with Scrapy logformatter below 1.4 version\n                \"flags\": response_flags,\n            },\n        }\n\n    def scraped(\n        self, item: Any, response: Union[Response, Failure], spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n        src: Any\n        if isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            \"level\": logging.DEBUG,\n            \"msg\": SCRAPEDMSG,\n            \"args\": {\n                \"src\": src,\n                \"item\": item,\n            },\n        }\n\n    def dropped(\n        self, item: Any, exception: BaseException, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n        return {\n            \"level\": logging.WARNING,\n            \"msg\": DROPPEDMSG,\n            \"args\": {\n                \"exception\": exception,\n                \"item\": item,\n            },\n        }\n\n    def item_error(\n        self, item: Any, exception: BaseException, response: Response, spider: Spider\n    ) -> LogFormatterResult:\n        \"\"\"Logs a message when an item causes an error while it is passing\n        through the item pipeline.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": ITEMERRORMSG,\n            \"args\": {\n                \"item\": item,\n            },\n        }\n\n    def spider_error(\n        self,\n        failure: Failure,\n        request: Request,\n        response: Union[Response, Failure],\n        spider: Spider,\n    ) -> LogFormatterResult:\n        \"\"\"Logs an error message from a spider.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": SPIDERERRORMSG,\n            \"args\": {\n                \"request\": request,\n                \"referer\": referer_str(request),\n            },\n        }\n\n    def download_error(\n        self,\n        failure: Failure,\n        request: Request,\n        spider: Spider,\n        errmsg: Optional[str] = None,\n    ) -> LogFormatterResult:\n        \"\"\"Logs a download error message from a spider (typically coming from\n        the engine).\n\n        .. versionadded:: 2.0\n        \"\"\"\n        args: Dict[str, Any] = {\"request\": request}\n        if errmsg:\n            msg = DOWNLOADERRORMSG_LONG\n            args[\"errmsg\"] = errmsg\n        else:\n            msg = DOWNLOADERRORMSG_SHORT\n        return {\n            \"level\": logging.ERROR,\n            \"msg\": msg,\n            \"args\": args,\n        }\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls()\n", "scrapy/signals.py": "\"\"\"\nScrapy signals\n\nThese signals are documented in docs/topics/signals.rst. Please don't add new\nsignals here without documenting them there.\n\"\"\"\n\nengine_started = object()\nengine_stopped = object()\nspider_opened = object()\nspider_idle = object()\nspider_closed = object()\nspider_error = object()\nrequest_scheduled = object()\nrequest_dropped = object()\nrequest_reached_downloader = object()\nrequest_left_downloader = object()\nresponse_received = object()\nresponse_downloaded = object()\nheaders_received = object()\nbytes_received = object()\nitem_scraped = object()\nitem_dropped = object()\nitem_error = object()\nfeed_slot_closed = object()\nfeed_exporter_closed = object()\n\n# for backward compatibility\nstats_spider_opened = spider_opened\nstats_spider_closing = spider_closed\nstats_spider_closed = spider_closed\n\nitem_passed = item_scraped\n\nrequest_received = request_scheduled\n", "scrapy/cmdline.py": "from __future__ import annotations\n\nimport argparse\nimport cProfile\nimport inspect\nimport os\nimport sys\nfrom importlib.metadata import entry_points\nfrom typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Type\n\nimport scrapy\nfrom scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.exceptions import UsageError\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.misc import walk_modules\nfrom scrapy.utils.project import get_project_settings, inside_project\nfrom scrapy.utils.python import garbage_collect\n\nif TYPE_CHECKING:\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n\nclass ScrapyArgumentParser(argparse.ArgumentParser):\n    def _parse_optional(\n        self, arg_string: str\n    ) -> Optional[Tuple[Optional[argparse.Action], str, Optional[str]]]:\n        # if starts with -: it means that is a parameter not a argument\n        if arg_string[:2] == \"-:\":\n            return None\n\n        return super()._parse_optional(arg_string)\n\n\ndef _iter_command_classes(module_name: str) -> Iterable[Type[ScrapyCommand]]:\n    # TODO: add `name` attribute to commands and merge this function with\n    # scrapy.utils.spider.iter_spider_classes\n    for module in walk_modules(module_name):\n        for obj in vars(module).values():\n            if (\n                inspect.isclass(obj)\n                and issubclass(obj, ScrapyCommand)\n                and obj.__module__ == module.__name__\n                and obj not in (ScrapyCommand, BaseRunSpiderCommand)\n            ):\n                yield obj\n\n\ndef _get_commands_from_module(module: str, inproject: bool) -> Dict[str, ScrapyCommand]:\n    d: Dict[str, ScrapyCommand] = {}\n    for cmd in _iter_command_classes(module):\n        if inproject or not cmd.requires_project:\n            cmdname = cmd.__module__.split(\".\")[-1]\n            d[cmdname] = cmd()\n    return d\n\n\ndef _get_commands_from_entry_points(\n    inproject: bool, group: str = \"scrapy.commands\"\n) -> Dict[str, ScrapyCommand]:\n    cmds: Dict[str, ScrapyCommand] = {}\n    if sys.version_info >= (3, 10):\n        eps = entry_points(group=group)\n    else:\n        eps = entry_points().get(group, ())\n    for entry_point in eps:\n        obj = entry_point.load()\n        if inspect.isclass(obj):\n            cmds[entry_point.name] = obj()\n        else:\n            raise Exception(f\"Invalid entry point {entry_point.name}\")\n    return cmds\n\n\ndef _get_commands_dict(\n    settings: BaseSettings, inproject: bool\n) -> Dict[str, ScrapyCommand]:\n    cmds = _get_commands_from_module(\"scrapy.commands\", inproject)\n    cmds.update(_get_commands_from_entry_points(inproject))\n    cmds_module = settings[\"COMMANDS_MODULE\"]\n    if cmds_module:\n        cmds.update(_get_commands_from_module(cmds_module, inproject))\n    return cmds\n\n\ndef _pop_command_name(argv: List[str]) -> Optional[str]:\n    i = 0\n    for arg in argv[1:]:\n        if not arg.startswith(\"-\"):\n            del argv[i]\n            return arg\n        i += 1\n    return None\n\n\ndef _print_header(settings: BaseSettings, inproject: bool) -> None:\n    version = scrapy.__version__\n    if inproject:\n        print(f\"Scrapy {version} - active project: {settings['BOT_NAME']}\\n\")\n\n    else:\n        print(f\"Scrapy {version} - no active project\\n\")\n\n\ndef _print_commands(settings: BaseSettings, inproject: bool) -> None:\n    _print_header(settings, inproject)\n    print(\"Usage:\")\n    print(\"  scrapy <command> [options] [args]\\n\")\n    print(\"Available commands:\")\n    cmds = _get_commands_dict(settings, inproject)\n    for cmdname, cmdclass in sorted(cmds.items()):\n        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n    if not inproject:\n        print()\n        print(\"  [ more ]      More commands available when run from project directory\")\n    print()\n    print('Use \"scrapy <command> -h\" to see more info about a command')\n\n\ndef _print_unknown_command(\n    settings: BaseSettings, cmdname: str, inproject: bool\n) -> None:\n    _print_header(settings, inproject)\n    print(f\"Unknown command: {cmdname}\\n\")\n    print('Use \"scrapy\" to see available commands')\n\n\ndef _run_print_help(\n    parser: argparse.ArgumentParser,\n    func: Callable[_P, None],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> None:\n    try:\n        func(*a, **kw)\n    except UsageError as e:\n        if str(e):\n            parser.error(str(e))\n        if e.print_help:\n            parser.print_help()\n        sys.exit(2)\n\n\ndef execute(\n    argv: Optional[List[str]] = None, settings: Optional[Settings] = None\n) -> None:\n    if argv is None:\n        argv = sys.argv\n\n    if settings is None:\n        settings = get_project_settings()\n        # set EDITOR from environment if available\n        try:\n            editor = os.environ[\"EDITOR\"]\n        except KeyError:\n            pass\n        else:\n            settings[\"EDITOR\"] = editor\n\n    inproject = inside_project()\n    cmds = _get_commands_dict(settings, inproject)\n    cmdname = _pop_command_name(argv)\n    if not cmdname:\n        _print_commands(settings, inproject)\n        sys.exit(0)\n    elif cmdname not in cmds:\n        _print_unknown_command(settings, cmdname, inproject)\n        sys.exit(2)\n\n    cmd = cmds[cmdname]\n    parser = ScrapyArgumentParser(\n        formatter_class=ScrapyHelpFormatter,\n        usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n        conflict_handler=\"resolve\",\n        description=cmd.long_desc(),\n    )\n    settings.setdict(cmd.default_settings, priority=\"command\")\n    cmd.settings = settings\n    cmd.add_options(parser)\n    opts, args = parser.parse_known_args(args=argv[1:])\n    _run_print_help(parser, cmd.process_options, args, opts)\n\n    cmd.crawler_process = CrawlerProcess(settings)\n    _run_print_help(parser, _run_command, cmd, args, opts)\n    sys.exit(cmd.exitcode)\n\n\ndef _run_command(cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace) -> None:\n    if opts.profile:\n        _run_command_profiled(cmd, args, opts)\n    else:\n        cmd.run(args, opts)\n\n\ndef _run_command_profiled(\n    cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace\n) -> None:\n    if opts.profile:\n        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n    loc = locals()\n    p = cProfile.Profile()\n    p.runctx(\"cmd.run(args, opts)\", globals(), loc)\n    if opts.profile:\n        p.dump_stats(opts.profile)\n\n\nif __name__ == \"__main__\":\n    try:\n        execute()\n    finally:\n        # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect() on exit:\n        # http://doc.pypy.org/en/latest/cpython_differences.html\n        # ?highlight=gc.collect#differences-related-to-garbage-collection-strategies\n        garbage_collect()\n", "scrapy/mail.py": "\"\"\"\nMail sending helpers\n\nSee documentation in docs/topics/email.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom email import encoders as Encoders\nfrom email.mime.base import MIMEBase\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.nonmultipart import MIMENonMultipart\nfrom email.mime.text import MIMEText\nfrom email.utils import formatdate\nfrom io import BytesIO\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nfrom twisted import version as twisted_version\nfrom twisted.internet import ssl\nfrom twisted.internet.defer import Deferred\nfrom twisted.python.failure import Failure\nfrom twisted.python.versions import Version\n\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # imports twisted.internet.reactor\n    from twisted.mail.smtp import ESMTPSenderFactory\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\n# Defined in the email.utils module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/email/utils.py#L42\nCOMMASPACE = \", \"\n\n\ndef _to_bytes_or_none(text: Union[str, bytes, None]) -> Optional[bytes]:\n    if text is None:\n        return None\n    return to_bytes(text)\n\n\nclass MailSender:\n    def __init__(\n        self,\n        smtphost: str = \"localhost\",\n        mailfrom: str = \"scrapy@localhost\",\n        smtpuser: Optional[str] = None,\n        smtppass: Optional[str] = None,\n        smtpport: int = 25,\n        smtptls: bool = False,\n        smtpssl: bool = False,\n        debug: bool = False,\n    ):\n        self.smtphost: str = smtphost\n        self.smtpport: int = smtpport\n        self.smtpuser: Optional[bytes] = _to_bytes_or_none(smtpuser)\n        self.smtppass: Optional[bytes] = _to_bytes_or_none(smtppass)\n        self.smtptls: bool = smtptls\n        self.smtpssl: bool = smtpssl\n        self.mailfrom: str = mailfrom\n        self.debug: bool = debug\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(\n            smtphost=settings[\"MAIL_HOST\"],\n            mailfrom=settings[\"MAIL_FROM\"],\n            smtpuser=settings[\"MAIL_USER\"],\n            smtppass=settings[\"MAIL_PASS\"],\n            smtpport=settings.getint(\"MAIL_PORT\"),\n            smtptls=settings.getbool(\"MAIL_TLS\"),\n            smtpssl=settings.getbool(\"MAIL_SSL\"),\n        )\n\n    def send(\n        self,\n        to: Union[str, List[str]],\n        subject: str,\n        body: str,\n        cc: Union[str, List[str], None] = None,\n        attachs: Sequence[Tuple[str, str, IO[Any]]] = (),\n        mimetype: str = \"text/plain\",\n        charset: Optional[str] = None,\n        _callback: Optional[Callable[..., None]] = None,\n    ) -> Optional[Deferred]:\n        from twisted.internet import reactor\n\n        msg: MIMEBase\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split(\"/\", 1))\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg[\"From\"] = self.mailfrom\n        msg[\"To\"] = COMMASPACE.join(to)\n        msg[\"Date\"] = formatdate(localtime=True)\n        msg[\"Subject\"] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg[\"Cc\"] = COMMASPACE.join(cc)\n\n        if attachs:\n            if charset:\n                msg.set_charset(charset)\n            msg.attach(MIMEText(body, \"plain\", charset or \"us-ascii\"))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split(\"/\"))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header(\n                    \"Content-Disposition\", \"attachment\", filename=attach_name\n                )\n                msg.attach(part)\n        else:\n            msg.set_payload(body, charset)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug(\n                \"Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n                'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                {\n                    \"mailto\": to,\n                    \"mailcc\": cc,\n                    \"mailsubject\": subject,\n                    \"mailattachs\": len(attachs),\n                },\n            )\n            return None\n\n        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or \"utf-8\"))\n        dfd.addCallback(self._sent_ok, to, cc, subject, len(attachs))\n        dfd.addErrback(self._sent_failed, to, cc, subject, len(attachs))\n        reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n        return dfd\n\n    def _sent_ok(\n        self, result: Any, to: List[str], cc: List[str], subject: str, nattachs: int\n    ) -> None:\n        logger.info(\n            \"Mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n            'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n            {\n                \"mailto\": to,\n                \"mailcc\": cc,\n                \"mailsubject\": subject,\n                \"mailattachs\": nattachs,\n            },\n        )\n\n    def _sent_failed(\n        self,\n        failure: Failure,\n        to: List[str],\n        cc: List[str],\n        subject: str,\n        nattachs: int,\n    ) -> Failure:\n        errstr = str(failure.value)\n        logger.error(\n            \"Unable to send mail: To=%(mailto)s Cc=%(mailcc)s \"\n            'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n            \"- %(mailerr)s\",\n            {\n                \"mailto\": to,\n                \"mailcc\": cc,\n                \"mailsubject\": subject,\n                \"mailattachs\": nattachs,\n                \"mailerr\": errstr,\n            },\n        )\n        return failure\n\n    def _sendmail(self, to_addrs: List[str], msg: bytes) -> Deferred:\n        from twisted.internet import reactor\n\n        msg_io = BytesIO(msg)\n        d: Deferred = Deferred()\n\n        factory = self._create_sender_factory(to_addrs, msg_io, d)\n\n        if self.smtpssl:\n            reactor.connectSSL(\n                self.smtphost, self.smtpport, factory, ssl.ClientContextFactory()\n            )\n        else:\n            reactor.connectTCP(self.smtphost, self.smtpport, factory)\n\n        return d\n\n    def _create_sender_factory(\n        self, to_addrs: List[str], msg: IO[bytes], d: Deferred\n    ) -> ESMTPSenderFactory:\n        from twisted.mail.smtp import ESMTPSenderFactory\n\n        factory_keywords: Dict[str, Any] = {\n            \"heloFallback\": True,\n            \"requireAuthentication\": False,\n            \"requireTransportSecurity\": self.smtptls,\n        }\n\n        # Newer versions of twisted require the hostname to use STARTTLS\n        if twisted_version >= Version(\"twisted\", 21, 2, 0):\n            factory_keywords[\"hostname\"] = self.smtphost\n\n        factory = ESMTPSenderFactory(\n            self.smtpuser,\n            self.smtppass,\n            self.mailfrom,\n            to_addrs,\n            msg,\n            d,\n            **factory_keywords,\n        )\n        factory.noisy = False\n        return factory\n", "scrapy/signalmanager.py": "from typing import Any, List, Tuple\n\nfrom pydispatch import dispatcher\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils import signal as _signal\n\n\nclass SignalManager:\n    def __init__(self, sender: Any = dispatcher.Anonymous):\n        self.sender: Any = sender\n\n    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.connect(receiver, signal, **kwargs)\n\n    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.disconnect(receiver, signal, **kwargs)\n\n    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log(signal, **kwargs)\n\n    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning\n        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)\n\n    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        _signal.disconnect_all(signal, **kwargs)\n", "scrapy/addons.py": "import logging\nfrom typing import TYPE_CHECKING, Any, List\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nclass AddonManager:\n    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n\n    def __init__(self, crawler: \"Crawler\") -> None:\n        self.crawler: \"Crawler\" = crawler\n        self.addons: List[Any] = []\n\n    def load_settings(self, settings: Settings) -> None:\n        \"\"\"Load add-ons and configurations from a settings object and apply them.\n\n        This will load the add-on for every add-on path in the\n        ``ADDONS`` setting and execute their ``update_settings`` methods.\n\n        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n            which to read the add-on configuration\n        :type settings: :class:`~scrapy.settings.Settings`\n        \"\"\"\n        for clspath in build_component_list(settings[\"ADDONS\"]):\n            try:\n                addoncls = load_object(clspath)\n                addon = build_from_crawler(addoncls, self.crawler)\n                addon.update_settings(settings)\n                self.addons.append(addon)\n            except NotConfigured as e:\n                if e.args:\n                    logger.warning(\n                        \"Disabled %(clspath)s: %(eargs)s\",\n                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                        extra={\"crawler\": self.crawler},\n                    )\n        logger.info(\n            \"Enabled addons:\\n%(addons)s\",\n            {\n                \"addons\": self.addons,\n            },\n            extra={\"crawler\": self.crawler},\n        )\n", "scrapy/exporters.py": "\"\"\"\nItem Exporters are used to export/serialize items into different formats.\n\"\"\"\n\nimport csv\nimport marshal\nimport pickle  # nosec\nimport pprint\nfrom io import BytesIO, TextIOWrapper\nfrom json import JSONEncoder\nfrom typing import Any, Callable, Dict, Iterable, Mapping, Optional, Tuple, Union\nfrom xml.sax.saxutils import XMLGenerator  # nosec\nfrom xml.sax.xmlreader import AttributesImpl  # nosec\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.python import is_listlike, to_bytes, to_unicode\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\n__all__ = [\n    \"BaseItemExporter\",\n    \"PprintItemExporter\",\n    \"PickleItemExporter\",\n    \"CsvItemExporter\",\n    \"XmlItemExporter\",\n    \"JsonLinesItemExporter\",\n    \"JsonItemExporter\",\n    \"MarshalItemExporter\",\n]\n\n\nclass BaseItemExporter:\n    def __init__(self, *, dont_fail: bool = False, **kwargs: Any):\n        self._kwargs: Dict[str, Any] = kwargs\n        self._configure(kwargs, dont_fail=dont_fail)\n\n    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n        \"\"\"Configure the exporter by popping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding: Optional[str] = options.pop(\"encoding\", None)\n        self.fields_to_export: Union[Mapping[str, str], Iterable[str], None] = (\n            options.pop(\"fields_to_export\", None)\n        )\n        self.export_empty_fields: bool = options.pop(\"export_empty_fields\", False)\n        self.indent: Optional[int] = options.pop(\"indent\", None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n\n    def export_item(self, item: Any) -> None:\n        raise NotImplementedError\n\n    def serialize_field(\n        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\"serializer\", lambda x: x)\n        return serializer(value)\n\n    def start_exporting(self) -> None:\n        pass\n\n    def finish_exporting(self) -> None:\n        pass\n\n    def _get_serialized_fields(\n        self, item: Any, default_value: Any = None, include_empty: Optional[bool] = None\n    ) -> Iterable[Tuple[str, Any]]:\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            if include_empty:\n                field_iter = item.field_names()\n            else:\n                field_iter = item.keys()\n        elif isinstance(self.fields_to_export, Mapping):\n            if include_empty:\n                field_iter = self.fields_to_export.items()\n            else:\n                field_iter = (\n                    (x, y) for x, y in self.fields_to_export.items() if x in item\n                )\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if isinstance(field_name, str):\n                item_field, output_field = field_name, field_name\n            else:\n                item_field, output_field = field_name\n            if item_field in item:\n                field_meta = item.get_field_meta(item_field)\n                value = self.serialize_field(field_meta, output_field, item[item_field])\n            else:\n                value = default_value\n\n            yield output_field, value\n\n\nclass JsonLinesItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file: BytesIO = file\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder: JSONEncoder = ScrapyJSONEncoder(**self._kwargs)\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + \"\\n\"\n        self.file.write(to_bytes(data, self.encoding))\n\n\nclass JsonItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file: BytesIO = file\n        # there is a small difference between the behaviour or JsonItemExporter.indent\n        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n        # the addition of newlines everywhere\n        json_indent = (\n            self.indent if self.indent is not None and self.indent > 0 else None\n        )\n        self._kwargs.setdefault(\"indent\", json_indent)\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n        self.first_item = True\n\n    def _beautify_newline(self) -> None:\n        if self.indent is not None:\n            self.file.write(b\"\\n\")\n\n    def _add_comma_after_first(self) -> None:\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b\",\")\n            self._beautify_newline()\n\n    def start_exporting(self) -> None:\n        self.file.write(b\"[\")\n        self._beautify_newline()\n\n    def finish_exporting(self) -> None:\n        self._beautify_newline()\n        self.file.write(b\"]\")\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n        self._add_comma_after_first()\n        self.file.write(data)\n\n\nclass XmlItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        self.item_element = kwargs.pop(\"item_element\", \"item\")\n        self.root_element = kwargs.pop(\"root_element\", \"items\")\n        super().__init__(**kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.xg = XMLGenerator(file, encoding=self.encoding)\n\n    def _beautify_newline(self, new_item: bool = False) -> None:\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters(\"\\n\")\n\n    def _beautify_indent(self, depth: int = 1) -> None:\n        if self.indent:\n            self.xg.characters(\" \" * self.indent * depth)\n\n    def start_exporting(self) -> None:\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, AttributesImpl({}))\n        self._beautify_newline(new_item=True)\n\n    def export_item(self, item: Any) -> None:\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, AttributesImpl({}))\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=\"\"):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)\n\n    def finish_exporting(self) -> None:\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()\n\n    def _export_xml_field(self, name: str, serialized_value: Any, depth: int) -> None:\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, AttributesImpl({}))\n        if hasattr(serialized_value, \"items\"):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field(\"value\", value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()\n\n\nclass CsvItemExporter(BaseItemExporter):\n    def __init__(\n        self,\n        file: BytesIO,\n        include_headers_line: bool = True,\n        join_multivalued: str = \",\",\n        errors: Optional[str] = None,\n        **kwargs: Any,\n    ):\n        super().__init__(dont_fail=True, **kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.include_headers_line = include_headers_line\n        self.stream = TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding,\n            newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n            errors=errors,\n        )\n        self.csv_writer = csv.writer(self.stream, **self._kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued\n\n    def serialize_field(\n        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\"serializer\", self._join_if_needed)\n        return serializer(value)\n\n    def _join_if_needed(self, value: Any) -> Any:\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value\n\n    def export_item(self, item: Any) -> None:\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)\n\n    def finish_exporting(self) -> None:\n        self.stream.detach()  # Avoid closing the wrapped file.\n\n    def _build_row(self, values: Iterable[Any]) -> Iterable[Any]:\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s\n\n    def _write_headers_and_set_fields_to_export(self, item: Any) -> None:\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            fields: Iterable[str]\n            if isinstance(self.fields_to_export, Mapping):\n                fields = self.fields_to_export.values()\n            else:\n                assert self.fields_to_export\n                fields = self.fields_to_export\n            row = list(self._build_row(fields))\n            self.csv_writer.writerow(row)\n\n\nclass PickleItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, protocol: int = 4, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n        self.protocol: int = protocol\n\n    def export_item(self, item: Any) -> None:\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)\n\n\nclass MarshalItemExporter(BaseItemExporter):\n    \"\"\"Exports items in a Python-specific binary format (see\n    :mod:`marshal`).\n\n    :param file: The file-like object to use for exporting the data. Its\n                 ``write`` method should accept :class:`bytes` (a disk file\n                 opened in binary mode, a :class:`~io.BytesIO` object, etc)\n    \"\"\"\n\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n\n    def export_item(self, item: Any) -> None:\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n\n\nclass PprintItemExporter(BaseItemExporter):\n    def __init__(self, file: BytesIO, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.file: BytesIO = file\n\n    def export_item(self, item: Any) -> None:\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n\n\nclass PythonItemExporter(BaseItemExporter):\n    \"\"\"This is a base class for item exporters that extends\n    :class:`BaseItemExporter` with support for nested items.\n\n    It serializes items to built-in Python types, so that any serialization\n    library (e.g. :mod:`json` or msgpack_) can be used on top of it.\n\n    .. _msgpack: https://pypi.org/project/msgpack/\n    \"\"\"\n\n    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n        super()._configure(options, dont_fail)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n\n    def serialize_field(\n        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n    ) -> Any:\n        serializer: Callable[[Any], Any] = field.get(\n            \"serializer\", self._serialize_value\n        )\n        return serializer(value)\n\n    def _serialize_value(self, value: Any) -> Any:\n        if isinstance(value, Item):\n            return self.export_item(value)\n        if is_item(value):\n            return dict(self._serialize_item(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        if isinstance(value, (str, bytes)):\n            return to_unicode(value, encoding=self.encoding)\n        return value\n\n    def _serialize_item(self, item: Any) -> Iterable[Tuple[Union[str, bytes], Any]]:\n        for key, value in ItemAdapter(item).items():\n            yield key, self._serialize_value(value)\n\n    def export_item(self, item: Any) -> Dict[Union[str, bytes], Any]:  # type: ignore[override]\n        result: Dict[Union[str, bytes], Any] = dict(self._get_serialized_fields(item))\n        return result\n", "scrapy/squeues.py": "\"\"\"\nScheduler queues\n\"\"\"\n\nfrom __future__ import annotations\n\nimport marshal\nimport pickle  # nosec\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, Optional, Type, Union\n\nfrom queuelib import queue\n\nfrom scrapy import Request\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.request import request_from_dict\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\ndef _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n    class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n        def __init__(self, path: Union[str, PathLike], *args: Any, **kwargs: Any):\n            dirname = Path(path).parent\n            if not dirname.exists():\n                dirname.mkdir(parents=True, exist_ok=True)\n            super().__init__(path, *args, **kwargs)\n\n    return DirectoriesCreated\n\n\ndef _serializable_queue(\n    queue_class: Type[queue.BaseQueue],\n    serialize: Callable[[Any], bytes],\n    deserialize: Callable[[bytes], Any],\n) -> Type[queue.BaseQueue]:\n    class SerializableQueue(queue_class):  # type: ignore[valid-type,misc]\n        def push(self, obj: Any) -> None:\n            s = serialize(obj)\n            super().push(s)\n\n        def pop(self) -> Optional[Any]:\n            s = super().pop()\n            if s:\n                return deserialize(s)\n            return None\n\n        def peek(self) -> Optional[Any]:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            try:\n                s = super().peek()\n            except AttributeError as ex:\n                raise NotImplementedError(\n                    \"The underlying queue class does not implement 'peek'\"\n                ) from ex\n            if s:\n                return deserialize(s)\n            return None\n\n    return SerializableQueue\n\n\ndef _scrapy_serialization_queue(\n    queue_class: Type[queue.BaseQueue],\n) -> Type[queue.BaseQueue]:\n    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n        def __init__(self, crawler: Crawler, key: str):\n            self.spider = crawler.spider\n            super().__init__(key)\n\n        @classmethod\n        def from_crawler(\n            cls, crawler: Crawler, key: str, *args: Any, **kwargs: Any\n        ) -> Self:\n            return cls(crawler, key)\n\n        def push(self, request: Request) -> None:\n            request_dict = request.to_dict(spider=self.spider)\n            super().push(request_dict)\n\n        def pop(self) -> Optional[Request]:\n            request = super().pop()\n            if not request:\n                return None\n            return request_from_dict(request, spider=self.spider)\n\n        def peek(self) -> Optional[Request]:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            request = super().peek()\n            if not request:\n                return None\n            return request_from_dict(request, spider=self.spider)\n\n    return ScrapyRequestQueue\n\n\ndef _scrapy_non_serialization_queue(\n    queue_class: Type[queue.BaseQueue],\n) -> Type[queue.BaseQueue]:\n    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n        @classmethod\n        def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n            return cls()\n\n        def peek(self) -> Optional[Any]:\n            \"\"\"Returns the next object to be returned by :meth:`pop`,\n            but without removing it from the queue.\n\n            Raises :exc:`NotImplementedError` if the underlying queue class does\n            not implement a ``peek`` method, which is optional for queues.\n            \"\"\"\n            try:\n                s = super().peek()\n            except AttributeError as ex:\n                raise NotImplementedError(\n                    \"The underlying queue class does not implement 'peek'\"\n                ) from ex\n            return s\n\n    return ScrapyRequestQueue\n\n\ndef _pickle_serialize(obj: Any) -> bytes:\n    try:\n        return pickle.dumps(obj, protocol=4)\n    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n    # TypeError is raised from parsel.Selector\n    except (pickle.PicklingError, AttributeError, TypeError) as e:\n        raise ValueError(str(e)) from e\n\n\n# queue.*Queue aren't subclasses of queue.BaseQueue\n_PickleFifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n)\n_PickleLifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n)\n_MarshalFifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n)\n_MarshalLifoSerializationDiskQueue = _serializable_queue(\n    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n)\n\n# public queue classes\nPickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQueue)\nPickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)\nMarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)\nMarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)\nFifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)  # type: ignore[arg-type]\nLifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)  # type: ignore[arg-type]\n", "scrapy/middleware.py": "from __future__ import annotations\n\nimport logging\nimport pprint\nfrom collections import defaultdict, deque\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Deque,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.defer import process_chain, process_parallel\nfrom scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MiddlewareManager:\n    \"\"\"Base class for implementing middleware managers\"\"\"\n\n    component_name = \"foo middleware\"\n\n    def __init__(self, *middlewares: Any) -> None:\n        self.middlewares = middlewares\n        # Only process_spider_output and process_spider_exception can be None.\n        # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n        self.methods: Dict[\n            str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]\n        ] = defaultdict(deque)\n        for mw in middlewares:\n            self._add_middleware(mw)\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n        raise NotImplementedError\n\n    @classmethod\n    def from_settings(\n        cls, settings: Settings, crawler: Optional[Crawler] = None\n    ) -> Self:\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                if crawler is not None:\n                    mw = build_from_crawler(mwcls, crawler)\n                else:\n                    mw = build_from_settings(mwcls, settings)\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    logger.warning(\n                        \"Disabled %(clspath)s: %(eargs)s\",\n                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                        extra={\"crawler\": crawler},\n                    )\n\n        logger.info(\n            \"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n            {\n                \"componentname\": cls.component_name,\n                \"enabledlist\": pprint.pformat(enabled),\n            },\n            extra={\"crawler\": crawler},\n        )\n        return cls(*middlewares)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls.from_settings(crawler.settings, crawler)\n\n    def _add_middleware(self, mw: Any) -> None:\n        if hasattr(mw, \"open_spider\"):\n            self.methods[\"open_spider\"].append(mw.open_spider)\n        if hasattr(mw, \"close_spider\"):\n            self.methods[\"close_spider\"].appendleft(mw.close_spider)\n\n    def _process_parallel(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n        methods = cast(Iterable[Callable], self.methods[methodname])\n        return process_parallel(methods, obj, *args)\n\n    def _process_chain(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n        methods = cast(Iterable[Callable], self.methods[methodname])\n        return process_chain(methods, obj, *args)\n\n    def open_spider(self, spider: Spider) -> Deferred:\n        return self._process_parallel(\"open_spider\", spider)\n\n    def close_spider(self, spider: Spider) -> Deferred:\n        return self._process_parallel(\"close_spider\", spider)\n", "scrapy/robotstxt.py": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom typing import TYPE_CHECKING, Optional, Union\nfrom warnings import warn\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef decode_robotstxt(\n    robotstxt_body: bytes, spider: Optional[Spider], to_native_str_type: bool = False\n) -> str:\n    try:\n        if to_native_str_type:\n            body_decoded = to_unicode(robotstxt_body)\n        else:\n            body_decoded = robotstxt_body.decode(\"utf-8\", errors=\"ignore\")\n    except UnicodeDecodeError:\n        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n        # Switch to 'allow all' state.\n        logger.warning(\n            \"Failure while parsing robots.txt. File either contains garbage or \"\n            \"is in an encoding other than UTF-8, treating it as an empty file.\",\n            exc_info=sys.exc_info(),\n            extra={\"spider\": spider},\n        )\n        body_decoded = \"\"\n    return body_decoded\n\n\nclass RobotParser(metaclass=ABCMeta):\n    @classmethod\n    @abstractmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n        It must return a new instance of the parser backend.\n\n        :param crawler: crawler which made the request\n        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n\n        :param robotstxt_body: content of a robots.txt_ file.\n        :type robotstxt_body: bytes\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n\n        :param url: Absolute URL\n        :type url: str or bytes\n\n        :param user_agent: User agent\n        :type user_agent: str or bytes\n        \"\"\"\n        pass\n\n\nclass PythonRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        from urllib.robotparser import RobotFileParser\n\n        self.spider: Optional[Spider] = spider\n        body_decoded = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n        self.rp: RobotFileParser = RobotFileParser()\n        self.rp.parse(body_decoded.splitlines())\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(user_agent, url)\n\n\nclass ReppyRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        warn(\"ReppyRobotParser is deprecated.\", ScrapyDeprecationWarning, stacklevel=2)\n        from reppy.robots import Robots\n\n        self.spider: Optional[Spider] = spider\n        self.rp = Robots.parse(\"\", robotstxt_body)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        return self.rp.allowed(url, user_agent)\n\n\nclass RerpRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        from robotexclusionrulesparser import RobotExclusionRulesParser\n\n        self.spider: Optional[Spider] = spider\n        self.rp: RobotExclusionRulesParser = RobotExclusionRulesParser()\n        body_decoded = decode_robotstxt(robotstxt_body, spider)\n        self.rp.parse(body_decoded)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.is_allowed(user_agent, url)\n\n\nclass ProtegoRobotParser(RobotParser):\n    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n        from protego import Protego\n\n        self.spider: Optional[Spider] = spider\n        body_decoded = decode_robotstxt(robotstxt_body, spider)\n        self.rp = Protego.parse(body_decoded)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o\n\n    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(url, user_agent)\n", "scrapy/__main__.py": "from scrapy.cmdline import execute\n\nif __name__ == \"__main__\":\n    execute()\n", "scrapy/pqueues.py": "from __future__ import annotations\n\nimport hashlib\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    cast,\n)\n\nfrom scrapy import Request\nfrom scrapy.core.downloader import Downloader\nfrom scrapy.utils.misc import build_from_crawler\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef _path_safe(text: str) -> str:\n    \"\"\"\n    Return a filesystem-safe version of a string ``text``\n\n    >>> _path_safe('simple.org').startswith('simple.org')\n    True\n    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n    True\n    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n    True\n    \"\"\"\n    pathable_slot = \"\".join([c if c.isalnum() or c in \"-._\" else \"_\" for c in text])\n    # as we replace some letters we can get collision for different slots\n    # add we add unique part\n    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # nosec\n    return \"-\".join([pathable_slot, unique_slot])\n\n\nclass QueueProtocol(Protocol):\n    \"\"\"Protocol for downstream queues of ``ScrapyPriorityQueue``.\"\"\"\n\n    def push(self, request: Request) -> None: ...\n\n    def pop(self) -> Optional[Request]: ...\n\n    def close(self) -> None: ...\n\n    def __len__(self) -> int: ...\n\n\nclass ScrapyPriorityQueue:\n    \"\"\"A priority queue implemented using multiple internal queues (typically,\n    FIFO queues). It uses one internal queue for each priority value. The internal\n    queue must implement the following methods:\n\n        * push(obj)\n        * pop()\n        * close()\n        * __len__()\n\n    Optionally, the queue could provide a ``peek`` method, that should return the\n    next object to be returned by ``pop``, but without removing it from the queue.\n\n    ``__init__`` method of ScrapyPriorityQueue receives a downstream_queue_cls\n    argument, which is a class used to instantiate a new (internal) queue when\n    a new priority is allocated.\n\n    Only integer priorities should be used. Lower numbers are higher\n    priorities.\n\n    startprios is a sequence of priorities to start with. If the queue was\n    previously closed leaving some priority buckets non-empty, those priorities\n    should be passed in startprios.\n\n    \"\"\"\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        startprios: Iterable[int] = (),\n    ) -> Self:\n        return cls(crawler, downstream_queue_cls, key, startprios)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        startprios: Iterable[int] = (),\n    ):\n        self.crawler: Crawler = crawler\n        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n        self.key: str = key\n        self.queues: Dict[int, QueueProtocol] = {}\n        self.curprio: Optional[int] = None\n        self.init_prios(startprios)\n\n    def init_prios(self, startprios: Iterable[int]) -> None:\n        if not startprios:\n            return\n\n        for priority in startprios:\n            self.queues[priority] = self.qfactory(priority)\n\n        self.curprio = min(startprios)\n\n    def qfactory(self, key: int) -> QueueProtocol:\n        return build_from_crawler(\n            self.downstream_queue_cls,\n            self.crawler,\n            self.key + \"/\" + str(key),\n        )\n\n    def priority(self, request: Request) -> int:\n        return -request.priority\n\n    def push(self, request: Request) -> None:\n        priority = self.priority(request)\n        if priority not in self.queues:\n            self.queues[priority] = self.qfactory(priority)\n        q = self.queues[priority]\n        q.push(request)  # this may fail (eg. serialization error)\n        if self.curprio is None or priority < self.curprio:\n            self.curprio = priority\n\n    def pop(self) -> Optional[Request]:\n        if self.curprio is None:\n            return None\n        q = self.queues[self.curprio]\n        m = q.pop()\n        if not q:\n            del self.queues[self.curprio]\n            q.close()\n            prios = [p for p, q in self.queues.items() if q]\n            self.curprio = min(prios) if prios else None\n        return m\n\n    def peek(self) -> Optional[Request]:\n        \"\"\"Returns the next object to be returned by :meth:`pop`,\n        but without removing it from the queue.\n\n        Raises :exc:`NotImplementedError` if the underlying queue class does\n        not implement a ``peek`` method, which is optional for queues.\n        \"\"\"\n        if self.curprio is None:\n            return None\n        queue = self.queues[self.curprio]\n        # Protocols can't declare optional members\n        return cast(Request, queue.peek())  # type: ignore[attr-defined]\n\n    def close(self) -> List[int]:\n        active: List[int] = []\n        for p, q in self.queues.items():\n            active.append(p)\n            q.close()\n        return active\n\n    def __len__(self) -> int:\n        return sum(len(x) for x in self.queues.values()) if self.queues else 0\n\n\nclass DownloaderInterface:\n    def __init__(self, crawler: Crawler):\n        assert crawler.engine\n        self.downloader: Downloader = crawler.engine.downloader\n\n    def stats(self, possible_slots: Iterable[str]) -> List[Tuple[int, str]]:\n        return [(self._active_downloads(slot), slot) for slot in possible_slots]\n\n    def get_slot_key(self, request: Request) -> str:\n        return self.downloader.get_slot_key(request)\n\n    def _active_downloads(self, slot: str) -> int:\n        \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n        if slot not in self.downloader.slots:\n            return 0\n        return len(self.downloader.slots[slot].active)\n\n\nclass DownloaderAwarePriorityQueue:\n    \"\"\"PriorityQueue which takes Downloader activity into account:\n    domains (slots) with the least amount of active downloads are dequeued\n    first.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        startprios: Optional[Dict[str, Iterable[int]]] = None,\n    ) -> Self:\n        return cls(crawler, downstream_queue_cls, key, startprios)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        downstream_queue_cls: Type[QueueProtocol],\n        key: str,\n        slot_startprios: Optional[Dict[str, Iterable[int]]] = None,\n    ):\n        if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n            raise ValueError(\n                f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP'\n            )\n\n        if slot_startprios and not isinstance(slot_startprios, dict):\n            raise ValueError(\n                \"DownloaderAwarePriorityQueue accepts \"\n                \"``slot_startprios`` as a dict; \"\n                f\"{slot_startprios.__class__!r} instance \"\n                \"is passed. Most likely, it means the state is\"\n                \"created by an incompatible priority queue. \"\n                \"Only a crawl started with the same priority \"\n                \"queue class can be resumed.\"\n            )\n\n        self._downloader_interface: DownloaderInterface = DownloaderInterface(crawler)\n        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n        self.key: str = key\n        self.crawler: Crawler = crawler\n\n        self.pqueues: Dict[str, ScrapyPriorityQueue] = {}  # slot -> priority queue\n        for slot, startprios in (slot_startprios or {}).items():\n            self.pqueues[slot] = self.pqfactory(slot, startprios)\n\n    def pqfactory(\n        self, slot: str, startprios: Iterable[int] = ()\n    ) -> ScrapyPriorityQueue:\n        return ScrapyPriorityQueue(\n            self.crawler,\n            self.downstream_queue_cls,\n            self.key + \"/\" + _path_safe(slot),\n            startprios,\n        )\n\n    def pop(self) -> Optional[Request]:\n        stats = self._downloader_interface.stats(self.pqueues)\n\n        if not stats:\n            return None\n\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        request = queue.pop()\n        if len(queue) == 0:\n            del self.pqueues[slot]\n        return request\n\n    def push(self, request: Request) -> None:\n        slot = self._downloader_interface.get_slot_key(request)\n        if slot not in self.pqueues:\n            self.pqueues[slot] = self.pqfactory(slot)\n        queue = self.pqueues[slot]\n        queue.push(request)\n\n    def peek(self) -> Optional[Request]:\n        \"\"\"Returns the next object to be returned by :meth:`pop`,\n        but without removing it from the queue.\n\n        Raises :exc:`NotImplementedError` if the underlying queue class does\n        not implement a ``peek`` method, which is optional for queues.\n        \"\"\"\n        stats = self._downloader_interface.stats(self.pqueues)\n        if not stats:\n            return None\n        slot = min(stats)[1]\n        queue = self.pqueues[slot]\n        return queue.peek()\n\n    def close(self) -> Dict[str, List[int]]:\n        active = {slot: queue.close() for slot, queue in self.pqueues.items()}\n        self.pqueues.clear()\n        return active\n\n    def __len__(self) -> int:\n        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n\n    def __contains__(self, slot: str) -> bool:\n        return slot in self.pqueues\n", "scrapy/link.py": "\"\"\"\nThis module defines the Link object used in Link extractors.\n\nFor actual link extractors implementation see scrapy.linkextractors, or\nits documentation in: docs/topics/link-extractors.rst\n\"\"\"\n\nfrom typing import Any\n\n\nclass Link:\n    \"\"\"Link objects represent an extracted link by the LinkExtractor.\n\n    Using the anchor tag sample below to illustrate the parameters::\n\n            <a href=\"https://example.com/nofollow.html#foo\" rel=\"nofollow\">Dont follow this one</a>\n\n    :param url: the absolute url being linked to in the anchor tag.\n                From the sample, this is ``https://example.com/nofollow.html``.\n\n    :param text: the text in the anchor tag. From the sample, this is ``Dont follow this one``.\n\n    :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.\n\n    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute\n                    of the anchor tag.\n    \"\"\"\n\n    __slots__ = [\"url\", \"text\", \"fragment\", \"nofollow\"]\n\n    def __init__(\n        self, url: str, text: str = \"\", fragment: str = \"\", nofollow: bool = False\n    ):\n        if not isinstance(url, str):\n            got = url.__class__.__name__\n            raise TypeError(f\"Link urls must be str objects, got {got}\")\n        self.url: str = url\n        self.text: str = text\n        self.fragment: str = fragment\n        self.nofollow: bool = nofollow\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Link):\n            raise NotImplementedError\n        return (\n            self.url == other.url\n            and self.text == other.text\n            and self.fragment == other.fragment\n            and self.nofollow == other.nofollow\n        )\n\n    def __hash__(self) -> int:\n        return (\n            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n        )\n\n    def __repr__(self) -> str:\n        return (\n            f\"Link(url={self.url!r}, text={self.text!r}, \"\n            f\"fragment={self.fragment!r}, nofollow={self.nofollow!r})\"\n        )\n", "scrapy/__init__.py": "\"\"\"\nScrapy - a web crawling and web scraping framework written for Python\n\"\"\"\n\nimport pkgutil\nimport sys\nimport warnings\n\nfrom twisted import version as _txv\n\n# Declare top-level shortcuts\nfrom scrapy.http import FormRequest, Request\nfrom scrapy.item import Field, Item\nfrom scrapy.selector import Selector\nfrom scrapy.spiders import Spider\n\n__all__ = [\n    \"__version__\",\n    \"version_info\",\n    \"twisted_version\",\n    \"Spider\",\n    \"Request\",\n    \"FormRequest\",\n    \"Selector\",\n    \"Item\",\n    \"Field\",\n]\n\n\n# Scrapy and Twisted versions\n__version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\nversion_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\ntwisted_version = (_txv.major, _txv.minor, _txv.micro)\n\n\n# Ignore noisy twisted deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\n\n\ndel pkgutil\ndel sys\ndel warnings\n", "scrapy/crawler.py": "from __future__ import annotations\n\nimport logging\nimport pprint\nimport signal\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union, cast\n\nfrom twisted.internet.defer import (\n    Deferred,\n    DeferredList,\n    inlineCallbacks,\n    maybeDeferred,\n)\n\ntry:\n    # zope >= 5.0 only supports MultipleInvalid\n    from zope.interface.exceptions import MultipleInvalid\nexcept ImportError:\n    MultipleInvalid = None\n\nfrom zope.interface.verify import verifyClass\n\nfrom scrapy import Spider, signals\nfrom scrapy.addons import AddonManager\nfrom scrapy.core.engine import ExecutionEngine\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.extension import ExtensionManager\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.settings import BaseSettings, Settings, overridden_settings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.log import (\n    LogCounterHandler,\n    configure_logging,\n    get_scrapy_root_handler,\n    install_scrapy_root_handler,\n    log_reactor_info,\n    log_scrapy_info,\n)\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.ossignal import install_shutdown_handlers, signal_names\nfrom scrapy.utils.reactor import (\n    install_reactor,\n    is_asyncio_reactor_installed,\n    verify_installed_asyncio_event_loop,\n    verify_installed_reactor,\n)\n\nif TYPE_CHECKING:\n    from scrapy.utils.request import RequestFingerprinter\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Crawler:\n    def __init__(\n        self,\n        spidercls: Type[Spider],\n        settings: Union[None, Dict[str, Any], Settings] = None,\n        init_reactor: bool = False,\n    ):\n        if isinstance(spidercls, Spider):\n            raise ValueError(\"The spidercls argument must be a class, not an object\")\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        self.spidercls: Type[Spider] = spidercls\n        self.settings: Settings = settings.copy()\n        self.spidercls.update_settings(self.settings)\n        self._update_root_log_handler()\n\n        self.addons: AddonManager = AddonManager(self)\n        self.signals: SignalManager = SignalManager(self)\n\n        self._init_reactor: bool = init_reactor\n        self.crawling: bool = False\n        self._started: bool = False\n\n        self.extensions: Optional[ExtensionManager] = None\n        self.stats: Optional[StatsCollector] = None\n        self.logformatter: Optional[LogFormatter] = None\n        self.request_fingerprinter: Optional[RequestFingerprinter] = None\n        self.spider: Optional[Spider] = None\n        self.engine: Optional[ExecutionEngine] = None\n\n    def _update_root_log_handler(self) -> None:\n        if get_scrapy_root_handler() is not None:\n            # scrapy root handler already installed: update it with new settings\n            install_scrapy_root_handler(self.settings)\n\n    def _apply_settings(self) -> None:\n        if self.settings.frozen:\n            return\n\n        self.addons.load_settings(self.settings)\n        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n\n        handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n        logging.root.addHandler(handler)\n        # lambda is assigned to Crawler attribute because this way it is not\n        # garbage collected after leaving the scope\n        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n\n        lf_cls: Type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n        self.logformatter = lf_cls.from_crawler(self)\n\n        self.request_fingerprinter = build_from_crawler(\n            load_object(self.settings[\"REQUEST_FINGERPRINTER_CLASS\"]),\n            self,\n        )\n\n        reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n        event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n        if self._init_reactor:\n            # this needs to be done after the spider settings are merged,\n            # but before something imports twisted.internet.reactor\n            if reactor_class:\n                install_reactor(reactor_class, event_loop)\n            else:\n                from twisted.internet import reactor  # noqa: F401\n            log_reactor_info()\n        if reactor_class:\n            verify_installed_reactor(reactor_class)\n            if is_asyncio_reactor_installed() and event_loop:\n                verify_installed_asyncio_event_loop(event_loop)\n\n            log_reactor_info()\n\n        self.extensions = ExtensionManager.from_crawler(self)\n        self.settings.freeze()\n\n        d = dict(overridden_settings(self.settings))\n        logger.info(\n            \"Overridden settings:\\n%(settings)s\", {\"settings\": pprint.pformat(d)}\n        )\n\n    @inlineCallbacks\n    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n        if self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        if self._started:\n            warnings.warn(\n                \"Running Crawler.crawl() more than once is deprecated.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n        self.crawling = self._started = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self._apply_settings()\n            self._update_root_log_handler()\n            self.engine = self._create_engine()\n            start_requests = iter(self.spider.start_requests())\n            yield self.engine.open_spider(self.spider, start_requests)\n            yield maybeDeferred(self.engine.start)\n        except Exception:\n            self.crawling = False\n            if self.engine is not None:\n                yield self.engine.close()\n            raise\n\n    def _create_spider(self, *args: Any, **kwargs: Any) -> Spider:\n        return self.spidercls.from_crawler(self, *args, **kwargs)\n\n    def _create_engine(self) -> ExecutionEngine:\n        return ExecutionEngine(self, lambda _: self.stop())\n\n    @inlineCallbacks\n    def stop(self) -> Generator[Deferred, Any, None]:\n        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n        fired when the crawler is stopped.\"\"\"\n        if self.crawling:\n            self.crawling = False\n            assert self.engine\n            yield maybeDeferred(self.engine.stop)\n\n    @staticmethod\n    def _get_component(component_class, components):\n        for component in components:\n            if isinstance(component, component_class):\n                return component\n        return None\n\n    def get_addon(self, cls):\n        return self._get_component(cls, self.addons.addons)\n\n    def get_downloader_middleware(self, cls):\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_downloader_middleware() can only be called after \"\n                \"the crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n\n    def get_extension(self, cls):\n        if not self.extensions:\n            raise RuntimeError(\n                \"Crawler.get_extension() can only be called after the \"\n                \"extension manager has been created.\"\n            )\n        return self._get_component(cls, self.extensions.middlewares)\n\n    def get_item_pipeline(self, cls):\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_item_pipeline() can only be called after the \"\n                \"crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n\n    def get_spider_middleware(self, cls):\n        if not self.engine:\n            raise RuntimeError(\n                \"Crawler.get_spider_middleware() can only be called after the \"\n                \"crawl engine has been created.\"\n            )\n        return self._get_component(cls, self.engine.scraper.spidermw.middlewares)\n\n\nclass CrawlerRunner:\n    \"\"\"\n    This is a convenient helper class that keeps track of, manages and runs\n    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n\n    The CrawlerRunner object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    \"\"\"\n\n    crawlers = property(\n        lambda self: self._crawlers,\n        doc=\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by \"\n        \":meth:`crawl` and managed by this class.\",\n    )\n\n    @staticmethod\n    def _get_spider_loader(settings: BaseSettings):\n        \"\"\"Get SpiderLoader instance from settings\"\"\"\n        cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n        loader_cls = load_object(cls_path)\n        verifyClass(ISpiderLoader, loader_cls)\n        return loader_cls.from_settings(settings.frozencopy())\n\n    def __init__(self, settings: Union[Dict[str, Any], Settings, None] = None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        self.settings = settings\n        self.spider_loader = self._get_spider_loader(settings)\n        self._crawlers: Set[Crawler] = set()\n        self._active: Set[Deferred] = set()\n        self.bootstrap_failed = False\n\n    def crawl(\n        self,\n        crawler_or_spidercls: Union[Type[Spider], str, Crawler],\n        *args: Any,\n        **kwargs: Any,\n    ) -> Deferred:\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param args: arguments to initialize the spider\n\n        :param kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)\n\n    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> Deferred:\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result: Any) -> Any:\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n            return result\n\n        return d.addBoth(_done)\n\n    def create_crawler(\n        self, crawler_or_spidercls: Union[Type[Spider], str, Crawler]\n    ) -> Crawler:\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If ``crawler_or_spidercls`` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                \"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)\n\n    def _create_crawler(self, spidercls: Union[str, Type[Spider]]) -> Crawler:\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        # temporary cast until self.spider_loader is typed\n        return Crawler(cast(Type[Spider], spidercls), self.settings)\n\n    def stop(self) -> Deferred:\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return DeferredList([c.stop() for c in list(self.crawlers)])\n\n    @inlineCallbacks\n    def join(self) -> Generator[Deferred, Any, None]:\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield DeferredList(self._active)\n\n\nclass CrawlerProcess(CrawlerRunner):\n    \"\"\"\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n    signals, like the keyboard interrupt command Ctrl-C. It also configures\n    top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n    :mod:`~twisted.internet.reactor` within your application.\n\n    The CrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: Union[Dict[str, Any], Settings, None] = None,\n        install_root_handler: bool = True,\n    ):\n        super().__init__(settings)\n        configure_logging(self.settings, install_root_handler)\n        log_scrapy_info(self.settings)\n        self._initialized_reactor = False\n\n    def _signal_shutdown(self, signum: int, _: Any) -> None:\n        from twisted.internet import reactor\n\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\n            \"Received %(signame)s, shutting down gracefully. Send again to force \",\n            {\"signame\": signame},\n        )\n        reactor.callFromThread(self._graceful_stop_reactor)\n\n    def _signal_kill(self, signum: int, _: Any) -> None:\n        from twisted.internet import reactor\n\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info(\n            \"Received %(signame)s twice, forcing unclean shutdown\", {\"signame\": signame}\n        )\n        reactor.callFromThread(self._stop_reactor)\n\n    def _create_crawler(self, spidercls: Union[Type[Spider], str]) -> Crawler:\n        if isinstance(spidercls, str):\n            spidercls = self.spider_loader.load(spidercls)\n        init_reactor = not self._initialized_reactor\n        self._initialized_reactor = True\n        # temporary cast until self.spider_loader is typed\n        return Crawler(\n            cast(Type[Spider], spidercls), self.settings, init_reactor=init_reactor\n        )\n\n    def start(\n        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n    ) -> None:\n        \"\"\"\n        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param bool stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n\n        :param bool install_signal_handlers: whether to install the OS signal\n            handlers from Twisted and Scrapy (default: True)\n        \"\"\"\n        from twisted.internet import reactor\n\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(self._stop_reactor)\n\n        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n        # We pass self, which is CrawlerProcess, instead of Crawler here,\n        # which works because the default resolvers only use crawler.settings.\n        resolver = build_from_crawler(resolver_class, self, reactor=reactor)  # type: ignore[arg-type]\n        resolver.install_on_reactor()\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n        reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n        if install_signal_handlers:\n            reactor.addSystemEventTrigger(\n                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n            )\n        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n\n    def _graceful_stop_reactor(self) -> Deferred:\n        d = self.stop()\n        d.addBoth(self._stop_reactor)\n        return d\n\n    def _stop_reactor(self, _: Any = None) -> None:\n        from twisted.internet import reactor\n\n        try:\n            reactor.stop()\n        except RuntimeError:  # raised if already stopped or in shutdown stage\n            pass\n", "scrapy/responsetypes.py": "\"\"\"\nThis module implements a class which returns the appropriate Response class\nbased on different criteria.\n\"\"\"\n\nfrom io import StringIO\nfrom mimetypes import MimeTypes\nfrom pkgutil import get_data\nfrom typing import Dict, Mapping, Optional, Type, Union\n\nfrom scrapy.http import Response\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n\n\nclass ResponseTypes:\n    CLASSES = {\n        \"text/html\": \"scrapy.http.HtmlResponse\",\n        \"application/atom+xml\": \"scrapy.http.XmlResponse\",\n        \"application/rdf+xml\": \"scrapy.http.XmlResponse\",\n        \"application/rss+xml\": \"scrapy.http.XmlResponse\",\n        \"application/xhtml+xml\": \"scrapy.http.HtmlResponse\",\n        \"application/vnd.wap.xhtml+xml\": \"scrapy.http.HtmlResponse\",\n        \"application/xml\": \"scrapy.http.XmlResponse\",\n        \"application/json\": \"scrapy.http.JsonResponse\",\n        \"application/x-json\": \"scrapy.http.JsonResponse\",\n        \"application/json-amazonui-streaming\": \"scrapy.http.JsonResponse\",\n        \"application/javascript\": \"scrapy.http.TextResponse\",\n        \"application/x-javascript\": \"scrapy.http.TextResponse\",\n        \"text/xml\": \"scrapy.http.XmlResponse\",\n        \"text/*\": \"scrapy.http.TextResponse\",\n    }\n\n    def __init__(self) -> None:\n        self.classes: Dict[str, Type[Response]] = {}\n        self.mimetypes: MimeTypes = MimeTypes()\n        mimedata = get_data(\"scrapy\", \"mime.types\")\n        if not mimedata:\n            raise ValueError(\n                \"The mime.types file is not found in the Scrapy installation\"\n            )\n        self.mimetypes.readfp(StringIO(mimedata.decode(\"utf8\")))\n        for mimetype, cls in self.CLASSES.items():\n            self.classes[mimetype] = load_object(cls)\n\n    def from_mimetype(self, mimetype: str) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        if mimetype in self.classes:\n            return self.classes[mimetype]\n        basetype = f\"{mimetype.split('/')[0]}/*\"\n        return self.classes.get(basetype, Response)\n\n    def from_content_type(\n        self, content_type: Union[str, bytes], content_encoding: Optional[bytes] = None\n    ) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header\"\"\"\n        if content_encoding:\n            return Response\n        mimetype = (\n            to_unicode(content_type, encoding=\"latin-1\").split(\";\")[0].strip().lower()\n        )\n        return self.from_mimetype(mimetype)\n\n    def from_content_disposition(\n        self, content_disposition: Union[str, bytes]\n    ) -> Type[Response]:\n        try:\n            filename = (\n                to_unicode(content_disposition, encoding=\"latin-1\", errors=\"replace\")\n                .split(\";\")[1]\n                .split(\"=\")[1]\n                .strip(\"\\\"'\")\n            )\n            return self.from_filename(filename)\n        except IndexError:\n            return Response\n\n    def from_headers(self, headers: Mapping[bytes, bytes]) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b\"Content-Type\" in headers:\n            cls = self.from_content_type(\n                content_type=headers[b\"Content-Type\"],\n                content_encoding=headers.get(b\"Content-Encoding\"),\n            )\n        if cls is Response and b\"Content-Disposition\" in headers:\n            cls = self.from_content_disposition(headers[b\"Content-Disposition\"])\n        return cls\n\n    def from_filename(self, filename: str) -> Type[Response]:\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        return Response\n\n    def from_body(self, body: bytes) -> Type[Response]:\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype(\"application/octet-stream\")\n        lowercase_chunk = chunk.lower()\n        if b\"<html>\" in lowercase_chunk:\n            return self.from_mimetype(\"text/html\")\n        if b\"<?xml\" in lowercase_chunk:\n            return self.from_mimetype(\"text/xml\")\n        if b\"<!doctype html>\" in lowercase_chunk:\n            return self.from_mimetype(\"text/html\")\n        return self.from_mimetype(\"text\")\n\n    def from_args(\n        self,\n        headers: Optional[Mapping[bytes, bytes]] = None,\n        url: Optional[str] = None,\n        filename: Optional[str] = None,\n        body: Optional[bytes] = None,\n    ) -> Type[Response]:\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls\n\n\nresponsetypes = ResponseTypes()\n", "scrapy/item.py": "\"\"\"\nScrapy Item\n\nSee documentation in docs/topics/item.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABCMeta\nfrom copy import deepcopy\nfrom pprint import pformat\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    KeysView,\n    MutableMapping,\n    NoReturn,\n    Tuple,\n)\n\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass Field(Dict[str, Any]):\n    \"\"\"Container of field metadata\"\"\"\n\n\nclass ItemMeta(ABCMeta):\n    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n\n    .. _metaclass: https://realpython.com/python-metaclasses\n    \"\"\"\n\n    def __new__(\n        mcs, class_name: str, bases: Tuple[type, ...], attrs: Dict[str, Any]\n    ) -> ItemMeta:\n        classcell = attrs.pop(\"__classcell__\", None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n        _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n\n        fields = getattr(_class, \"fields\", {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs[\"fields\"] = fields\n        new_attrs[\"_class\"] = _class\n        if classcell is not None:\n            new_attrs[\"__classcell__\"] = classcell\n        return super().__new__(mcs, class_name, bases, new_attrs)\n\n\nclass Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n    \"\"\"\n    Base class for scraped items.\n\n    In Scrapy, an object is considered an ``item`` if it is an instance of either\n    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n    spider callback is evaluated, only instances of :class:`Item` or\n    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n\n    If you need instances of a custom class to be considered items by Scrapy,\n    you must inherit from either :class:`Item` or :class:`dict`.\n\n    Items must declare :class:`Field` attributes, which are processed and stored\n    in the ``fields`` attribute. This restricts the set of allowed field names\n    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n    Additionally, fields can be used to define metadata and control the way\n    data is processed internally. Please refer to the :ref:`documentation\n    about fields <topics-items-fields>` for additional information.\n\n    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n    \"\"\"\n\n    fields: Dict[str, Field]\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        self._values: Dict[str, Any] = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in dict(*args, **kwargs).items():\n                self[k] = v\n\n    def __getitem__(self, key: str) -> Any:\n        return self._values[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n\n    def __delitem__(self, key: str) -> None:\n        del self._values[key]\n\n    def __getattr__(self, name: str) -> NoReturn:\n        if name in self.fields:\n            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n        raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        if not name.startswith(\"_\"):\n            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n        super().__setattr__(name, value)\n\n    def __len__(self) -> int:\n        return len(self._values)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._values)\n\n    __hash__ = object_ref.__hash__\n\n    def keys(self) -> KeysView[str]:\n        return self._values.keys()\n\n    def __repr__(self) -> str:\n        return pformat(dict(self))\n\n    def copy(self) -> Self:\n        return self.__class__(self)\n\n    def deepcopy(self) -> Self:\n        \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n        return deepcopy(self)\n", "scrapy/selector/unified.py": "\"\"\"\nXPath selectors based on lxml\n\"\"\"\n\nfrom typing import Any, Optional, Type, Union\n\nfrom parsel import Selector as _ParselSelector\n\nfrom scrapy.http import HtmlResponse, TextResponse, XmlResponse\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.response import get_base_url\nfrom scrapy.utils.trackref import object_ref\n\n__all__ = [\"Selector\", \"SelectorList\"]\n\n_NOT_SET = object()\n\n\ndef _st(response: Optional[TextResponse], st: Optional[str]) -> str:\n    if st is None:\n        return \"xml\" if isinstance(response, XmlResponse) else \"html\"\n    return st\n\n\ndef _response_from_text(text: Union[str, bytes], st: Optional[str]) -> TextResponse:\n    rt: Type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n    return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n\n\nclass SelectorList(_ParselSelector.selectorlist_cls, object_ref):\n    \"\"\"\n    The :class:`SelectorList` class is a subclass of the builtin ``list``\n    class, which provides a few additional methods.\n    \"\"\"\n\n\nclass Selector(_ParselSelector, object_ref):\n    \"\"\"\n    An instance of :class:`Selector` is a wrapper over response to select\n    certain parts of its content.\n\n    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an\n    :class:`~scrapy.http.XmlResponse` object that will be used for selecting\n    and extracting data.\n\n    ``text`` is a unicode string or utf-8 encoded text for cases when a\n    ``response`` isn't available. Using ``text`` and ``response`` together is\n    undefined behavior.\n\n    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``, ``\"json\"``\n    or ``None`` (default).\n\n    If ``type`` is ``None``, the selector automatically chooses the best type\n    based on ``response`` type (see below), or defaults to ``\"html\"`` in case it\n    is used together with ``text``.\n\n    If ``type`` is ``None`` and a ``response`` is passed, the selector type is\n    inferred from the response type as follows:\n\n    * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n    * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n    * ``\"json\"`` for :class:`~scrapy.http.TextResponse` type\n    * ``\"html\"`` for anything else\n\n    Otherwise, if ``type`` is set, the selector type will be forced and no\n    detection will occur.\n    \"\"\"\n\n    __slots__ = [\"response\"]\n    selectorlist_cls = SelectorList\n\n    def __init__(\n        self,\n        response: Optional[TextResponse] = None,\n        text: Optional[str] = None,\n        type: Optional[str] = None,\n        root: Optional[Any] = _NOT_SET,\n        **kwargs: Any,\n    ):\n        if response is not None and text is not None:\n            raise ValueError(\n                f\"{self.__class__.__name__}.__init__() received \"\n                \"both response and text\"\n            )\n\n        st = _st(response, type)\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            text = response.text\n            kwargs.setdefault(\"base_url\", get_base_url(response))\n\n        self.response = response\n\n        if root is not _NOT_SET:\n            kwargs[\"root\"] = root\n\n        super().__init__(text=text, type=st, **kwargs)\n", "scrapy/selector/__init__.py": "\"\"\"\nSelectors\n\"\"\"\n\n# top-level imports\nfrom scrapy.selector.unified import Selector, SelectorList\n", "scrapy/utils/response.py": "\"\"\"\nThis module provides some useful functions for working with\nscrapy.http.Response objects\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport tempfile\nimport webbrowser\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable, Tuple, Union\nfrom weakref import WeakKeyDictionary\n\nfrom twisted.web import http\nfrom w3lib import html\n\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    from scrapy.http import Response, TextResponse\n\n_baseurl_cache: WeakKeyDictionary[Response, str] = WeakKeyDictionary()\n\n\ndef get_base_url(response: TextResponse) -> str:\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(\n            text, response.url, response.encoding\n        )\n    return _baseurl_cache[response]\n\n\n_metaref_cache: WeakKeyDictionary[\n    Response, Union[Tuple[None, None], Tuple[float, str]]\n] = WeakKeyDictionary()\n\n\ndef get_meta_refresh(\n    response: TextResponse,\n    ignore_tags: Iterable[str] = (\"script\", \"noscript\"),\n) -> Union[Tuple[None, None], Tuple[float, str]]:\n    \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(\n            text, response.url, response.encoding, ignore_tags=ignore_tags\n        )\n    return _metaref_cache[response]\n\n\ndef response_status_message(status: Union[bytes, float, int, str]) -> str:\n    \"\"\"Return status code plus status text descriptive message\"\"\"\n    status_int = int(status)\n    message = http.RESPONSES.get(status_int, \"Unknown Status\")\n    return f\"{status_int} {to_unicode(message)}\"\n\n\ndef _remove_html_comments(body: bytes) -> bytes:\n    start = body.find(b\"<!--\")\n    while start != -1:\n        end = body.find(b\"-->\", start + 1)\n        if end == -1:\n            return body[:start]\n        else:\n            body = body[:start] + body[end + 3 :]\n            start = body.find(b\"<!--\")\n    return body\n\n\ndef open_in_browser(\n    response: TextResponse,\n    _openfunc: Callable[[str], Any] = webbrowser.open,\n) -> Any:\n    \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for\n    external links to work, e.g. so that images and styles are displayed.\n\n    .. _base tag: https://www.w3schools.com/tags/tag_base.asp\n\n    For example:\n\n    .. code-block:: python\n\n        from scrapy.utils.response import open_in_browser\n\n\n        def parse_details(self, response):\n            if \"item name\" not in response.body:\n                open_in_browser(response)\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b\"<base\" not in body:\n            _remove_html_comments(body)\n            repl = rf'\\0<base href=\"{response.url}\">'\n            body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)\n        ext = \".html\"\n    elif isinstance(response, TextResponse):\n        ext = \".txt\"\n    else:\n        raise TypeError(\"Unsupported response type: \" f\"{response.__class__.__name__}\")\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(f\"file://{fname}\")\n", "scrapy/utils/engine.py": "\"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n\nfrom __future__ import annotations\n\n# used in global tests code\nfrom time import time  # noqa: F401\nfrom typing import Any, List, Tuple\n\nfrom scrapy.core.engine import ExecutionEngine\n\n\ndef get_engine_status(engine: ExecutionEngine) -> List[Tuple[str, Any]]:\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle()\",\n        \"engine.slot.closing\",\n        \"len(engine.slot.inprogress)\",\n        \"len(engine.slot.scheduler.dqs or [])\",\n        \"len(engine.slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks: List[Tuple[str, Any]] = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]  # nosec\n        except Exception as e:\n            checks += [(test, f\"{type(e).__name__} (exception)\")]\n\n    return checks\n\n\ndef format_engine_status(engine: ExecutionEngine) -> str:\n    checks = get_engine_status(engine)\n    s = \"Execution engine status\\n\\n\"\n    for test, result in checks:\n        s += f\"{test:<47} : {result}\\n\"\n    s += \"\\n\"\n\n    return s\n\n\ndef print_engine_status(engine: ExecutionEngine) -> None:\n    print(format_engine_status(engine))\n", "scrapy/utils/display.py": "\"\"\"\npprint and pformat wrappers with colorization support\n\"\"\"\n\nimport ctypes\nimport platform\nimport sys\nfrom pprint import pformat as pformat_\nfrom typing import Any\n\nfrom packaging.version import Version as parse_version\n\n\ndef _enable_windows_terminal_processing() -> bool:\n    # https://stackoverflow.com/a/36760881\n    kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]\n    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))\n\n\ndef _tty_supports_color() -> bool:\n    if sys.platform != \"win32\":\n        return True\n\n    if parse_version(platform.version()) < parse_version(\"10.0.14393\"):\n        return True\n\n    # Windows >= 10.0.14393 interprets ANSI escape sequences providing terminal\n    # processing is enabled.\n    return _enable_windows_terminal_processing()\n\n\ndef _colorize(text: str, colorize: bool = True) -> str:\n    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n        return text\n    try:\n        from pygments import highlight\n    except ImportError:\n        return text\n    else:\n        from pygments.formatters import TerminalFormatter\n        from pygments.lexers import PythonLexer\n\n        return highlight(text, PythonLexer(), TerminalFormatter())\n\n\ndef pformat(obj: Any, *args: Any, **kwargs: Any) -> str:\n    return _colorize(pformat_(obj), kwargs.pop(\"colorize\", True))\n\n\ndef pprint(obj: Any, *args: Any, **kwargs: Any) -> None:\n    print(pformat(obj, *args, **kwargs))\n", "scrapy/utils/trackref.py": "\"\"\"This module provides some functions and classes to record and report\nreferences to live object instances.\n\nIf you want live objects for a particular class to be tracked, you only have to\nsubclass from object_ref (instead of object).\n\nAbout performance: This library has a minimal performance impact when enabled,\nand no performance penalty at all when disabled (as object_ref becomes just an\nalias to object in that case).\n\"\"\"\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Iterable\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nNoneType = type(None)\nlive_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n\n\nclass object_ref:\n    \"\"\"Inherit from this class to a keep a record of live instances\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> \"Self\":\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj\n\n\n# using Any as it's hard to type type(None)\ndef format_live_refs(ignore: Any = NoneType) -> str:\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(wdict.values())\n        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n    return s\n\n\ndef print_live_refs(*a: Any, **kw: Any) -> None:\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))\n\n\ndef get_oldest(class_name: str) -> Any:\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(wdict.items(), key=itemgetter(1))[0]\n\n\ndef iter_all(class_name: str) -> Iterable[Any]:\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            return wdict.keys()\n    return []\n", "scrapy/utils/log.py": "from __future__ import annotations\n\nimport logging\nimport sys\nfrom logging.config import dictConfig\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    List,\n    MutableMapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nfrom twisted.python import log as twisted_log\nfrom twisted.python.failure import Failure\n\nimport scrapy\nfrom scrapy.logformatter import LogFormatterResult\nfrom scrapy.settings import Settings, _SettingsKeyT\nfrom scrapy.utils.versions import scrapy_components_versions\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\ndef failure_to_exc_info(\n    failure: Failure,\n) -> Optional[Tuple[Type[BaseException], BaseException, Optional[TracebackType]]]:\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        assert failure.type\n        assert failure.value\n        return (\n            failure.type,\n            failure.value,\n            cast(Optional[TracebackType], failure.getTracebackObject()),\n        )\n    return None\n\n\nclass TopLevelFormatter(logging.Filter):\n    \"\"\"Keep only top level loggers's name (direct children from root) from\n    records.\n\n    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics\n    the old Scrapy log behaviour and helps shortening long names.\n\n    Since it can't be set for just one logger (it won't propagate for its\n    children), it's going to be set in the root handler, with a parametrized\n    ``loggers`` list where it should act.\n    \"\"\"\n\n    def __init__(self, loggers: Optional[List[str]] = None):\n        self.loggers: List[str] = loggers or []\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        if any(record.name.startswith(logger + \".\") for logger in self.loggers):\n            record.name = record.name.split(\".\", 1)[0]\n        return True\n\n\nDEFAULT_LOGGING = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"loggers\": {\n        \"filelock\": {\n            \"level\": \"ERROR\",\n        },\n        \"hpack\": {\n            \"level\": \"ERROR\",\n        },\n        \"scrapy\": {\n            \"level\": \"DEBUG\",\n        },\n        \"twisted\": {\n            \"level\": \"ERROR\",\n        },\n    },\n}\n\n\ndef configure_logging(\n    settings: Union[Settings, Dict[_SettingsKeyT, Any], None] = None,\n    install_root_handler: bool = True,\n) -> None:\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver(\"twisted\")\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool(\"LOG_STDOUT\"):\n        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))  # type: ignore[assignment]\n\n    if install_root_handler:\n        install_scrapy_root_handler(settings)\n\n\n_scrapy_root_handler: Optional[logging.Handler] = None\n\n\ndef install_scrapy_root_handler(settings: Settings) -> None:\n    global _scrapy_root_handler\n\n    if (\n        _scrapy_root_handler is not None\n        and _scrapy_root_handler in logging.root.handlers\n    ):\n        logging.root.removeHandler(_scrapy_root_handler)\n    logging.root.setLevel(logging.NOTSET)\n    _scrapy_root_handler = _get_handler(settings)\n    logging.root.addHandler(_scrapy_root_handler)\n\n\ndef get_scrapy_root_handler() -> Optional[logging.Handler]:\n    return _scrapy_root_handler\n\n\ndef _get_handler(settings: Settings) -> logging.Handler:\n    \"\"\"Return a log handler object according to settings\"\"\"\n    filename = settings.get(\"LOG_FILE\")\n    handler: logging.Handler\n    if filename:\n        mode = \"a\" if settings.getbool(\"LOG_FILE_APPEND\") else \"w\"\n        encoding = settings.get(\"LOG_ENCODING\")\n        handler = logging.FileHandler(filename, mode=mode, encoding=encoding)\n    elif settings.getbool(\"LOG_ENABLED\"):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get(\"LOG_FORMAT\"), datefmt=settings.get(\"LOG_DATEFORMAT\")\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get(\"LOG_LEVEL\"))\n    if settings.getbool(\"LOG_SHORT_NAMES\"):\n        handler.addFilter(TopLevelFormatter([\"scrapy\"]))\n    return handler\n\n\ndef log_scrapy_info(settings: Settings) -> None:\n    logger.info(\n        \"Scrapy %(version)s started (bot: %(bot)s)\",\n        {\"version\": scrapy.__version__, \"bot\": settings[\"BOT_NAME\"]},\n    )\n    versions = [\n        f\"{name} {version}\"\n        for name, version in scrapy_components_versions()\n        if name != \"Scrapy\"\n    ]\n    logger.info(\"Versions: %(versions)s\", {\"versions\": \", \".join(versions)})\n\n\ndef log_reactor_info() -> None:\n    from twisted.internet import reactor\n\n    logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n    from twisted.internet import asyncioreactor\n\n    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n        logger.debug(\n            \"Using asyncio event loop: %s.%s\",\n            reactor._asyncioEventloop.__module__,\n            reactor._asyncioEventloop.__class__.__name__,\n        )\n\n\nclass StreamLogger:\n    \"\"\"Fake file-like stream object that redirects writes to a logger instance\n\n    Taken from:\n        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n    \"\"\"\n\n    def __init__(self, logger: logging.Logger, log_level: int = logging.INFO):\n        self.logger: logging.Logger = logger\n        self.log_level: int = log_level\n        self.linebuf: str = \"\"\n\n    def write(self, buf: str) -> None:\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())\n\n    def flush(self) -> None:\n        for h in self.logger.handlers:\n            h.flush()\n\n\nclass LogCounterHandler(logging.Handler):\n    \"\"\"Record log levels count into a crawler stats\"\"\"\n\n    def __init__(self, crawler: Crawler, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.crawler: Crawler = crawler\n\n    def emit(self, record: logging.LogRecord) -> None:\n        sname = f\"log_count/{record.levelname}\"\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(sname)\n\n\ndef logformatter_adapter(\n    logkws: LogFormatterResult,\n) -> Tuple[int, str, Union[Dict[str, Any], Tuple[Any, ...]]]:\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n\n    level = logkws.get(\"level\", logging.INFO)\n    message = logkws.get(\"msg\") or \"\"\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = cast(Dict[str, Any], logkws) if not logkws.get(\"args\") else logkws[\"args\"]\n\n    return (level, message, args)\n\n\nclass SpiderLoggerAdapter(logging.LoggerAdapter):\n    def process(\n        self, msg: str, kwargs: MutableMapping[str, Any]\n    ) -> Tuple[str, MutableMapping[str, Any]]:\n        \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n        if isinstance(kwargs.get(\"extra\"), MutableMapping):\n            kwargs[\"extra\"].update(self.extra)\n        else:\n            kwargs[\"extra\"] = self.extra\n\n        return msg, kwargs\n", "scrapy/utils/boto.py": "\"\"\"Boto/botocore helpers\"\"\"\n\n\ndef is_botocore_available() -> bool:\n    try:\n        import botocore  # noqa: F401\n\n        return True\n    except ImportError:\n        return False\n", "scrapy/utils/spider.py": "from __future__ import annotations\n\nimport inspect\nimport logging\nfrom types import CoroutineType, ModuleType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Iterable,\n    Literal,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.defer import deferred_from_coro\nfrom scrapy.utils.misc import arg_to_iter\n\nif TYPE_CHECKING:\n    from scrapy.spiderloader import SpiderLoader\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\n# https://stackoverflow.com/questions/60222982\n@overload\ndef iterate_spider_output(result: AsyncGenerator[_T, None]) -> AsyncGenerator[_T, None]: ...  # type: ignore[overload-overlap]\n\n\n@overload\ndef iterate_spider_output(result: CoroutineType[Any, Any, _T]) -> Deferred[_T]: ...\n\n\n@overload\ndef iterate_spider_output(result: _T) -> Iterable[Any]: ...\n\n\ndef iterate_spider_output(\n    result: Any,\n) -> Union[Iterable[Any], AsyncGenerator[_T, None], Deferred[_T]]:\n    if inspect.isasyncgen(result):\n        return result\n    if inspect.iscoroutine(result):\n        d = deferred_from_coro(result)\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(deferred_from_coro(result))\n\n\ndef iter_spider_classes(module: ModuleType) -> Iterable[Type[Spider]]:\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (i.e. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in vars(module).values():\n        if (\n            inspect.isclass(obj)\n            and issubclass(obj, Spider)\n            and obj.__module__ == module.__name__\n            and getattr(obj, \"name\", None)\n        ):\n            yield obj\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    default_spidercls: Type[Spider],\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> Type[Spider]: ...\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    default_spidercls: Literal[None],\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> Optional[Type[Spider]]: ...\n\n\n@overload\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    *,\n    log_none: bool = ...,\n    log_multiple: bool = ...,\n) -> Optional[Type[Spider]]: ...\n\n\ndef spidercls_for_request(\n    spider_loader: SpiderLoader,\n    request: Request,\n    default_spidercls: Optional[Type[Spider]] = None,\n    log_none: bool = False,\n    log_multiple: bool = False,\n) -> Optional[Type[Spider]]:\n    \"\"\"Return a spider class that handles the given Request.\n\n    This will look for the spiders that can handle the given request (using\n    the spider loader) and return a Spider class if (and only if) there is\n    only one Spider able to handle the Request.\n\n    If multiple spiders (or no spider) are found, it will return the\n    default_spidercls passed. It can optionally log if multiple or no spiders\n    are found.\n    \"\"\"\n    snames = spider_loader.find_by_request(request)\n    if len(snames) == 1:\n        return spider_loader.load(snames[0])\n\n    if len(snames) > 1 and log_multiple:\n        logger.error(\n            \"More than one spider can handle: %(request)s - %(snames)s\",\n            {\"request\": request, \"snames\": \", \".join(snames)},\n        )\n\n    if len(snames) == 0 and log_none:\n        logger.error(\n            \"Unable to find spider that handles: %(request)s\", {\"request\": request}\n        )\n\n    return default_spidercls\n\n\nclass DefaultSpider(Spider):\n    name = \"default\"\n", "scrapy/utils/serialize.py": "import datetime\nimport decimal\nimport json\nfrom typing import Any\n\nfrom itemadapter import ItemAdapter, is_item\nfrom twisted.internet import defer\n\nfrom scrapy.http import Request, Response\n\n\nclass ScrapyJSONEncoder(json.JSONEncoder):\n    DATE_FORMAT = \"%Y-%m-%d\"\n    TIME_FORMAT = \"%H:%M:%S\"\n\n    def default(self, o: Any) -> Any:\n        if isinstance(o, set):\n            return list(o)\n        if isinstance(o, datetime.datetime):\n            return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n        if isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        if isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        if isinstance(o, decimal.Decimal):\n            return str(o)\n        if isinstance(o, defer.Deferred):\n            return str(o)\n        if is_item(o):\n            return ItemAdapter(o).asdict()\n        if isinstance(o, Request):\n            return f\"<{type(o).__name__} {o.method} {o.url}>\"\n        if isinstance(o, Response):\n            return f\"<{type(o).__name__} {o.status} {o.url}>\"\n        return super().default(o)\n\n\nclass ScrapyJSONDecoder(json.JSONDecoder):\n    pass\n", "scrapy/utils/reactor.py": "from __future__ import annotations\n\nimport asyncio\nimport sys\nfrom asyncio import AbstractEventLoop, AbstractEventLoopPolicy\nfrom contextlib import suppress\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n)\nfrom warnings import catch_warnings, filterwarnings, warn\n\nfrom twisted.internet import asyncioreactor, error\nfrom twisted.internet.base import DelayedCall\nfrom twisted.internet.protocol import ServerFactory\nfrom twisted.internet.tcp import Port\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n\n\ndef listen_tcp(portrange: List[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    from twisted.internet import reactor\n\n    if len(portrange) > 2:\n        raise ValueError(f\"invalid portrange: {portrange}\")\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1] + 1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise\n\n\nclass CallLaterOnce(Generic[_T]):\n    \"\"\"Schedule a function to be called in the next reactor loop, but only if\n    it hasn't been already scheduled since the last time it ran.\n    \"\"\"\n\n    def __init__(self, func: Callable[_P, _T], *a: _P.args, **kw: _P.kwargs):\n        self._func: Callable[_P, _T] = func\n        self._a: Tuple[Any, ...] = a\n        self._kw: Dict[str, Any] = kw\n        self._call: Optional[DelayedCall] = None\n\n    def schedule(self, delay: float = 0) -> None:\n        from twisted.internet import reactor\n\n        if self._call is None:\n            self._call = reactor.callLater(delay, self)\n\n    def cancel(self) -> None:\n        if self._call:\n            self._call.cancel()\n\n    def __call__(self) -> _T:\n        self._call = None\n        return self._func(*self._a, **self._kw)\n\n\ndef set_asyncio_event_loop_policy() -> None:\n    \"\"\"The policy functions from asyncio often behave unexpectedly,\n    so we restrict their use to the absolutely essential case.\n    This should only be used to install the reactor.\n    \"\"\"\n    _get_asyncio_event_loop_policy()\n\n\ndef get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n    warn(\n        \"Call to deprecated function \"\n        \"scrapy.utils.reactor.get_asyncio_event_loop_policy().\\n\"\n        \"\\n\"\n        \"Please use get_event_loop, new_event_loop and set_event_loop\"\n        \" from asyncio instead, as the corresponding policy methods may lead\"\n        \" to unexpected behaviour.\\n\"\n        \"This function is replaced by set_asyncio_event_loop_policy and\"\n        \" is meant to be used only when the reactor is being installed.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    return _get_asyncio_event_loop_policy()\n\n\ndef _get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n    policy = asyncio.get_event_loop_policy()\n    if (\n        sys.version_info >= (3, 8)\n        and sys.platform == \"win32\"\n        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\n    ):\n        policy = asyncio.WindowsSelectorEventLoopPolicy()\n        asyncio.set_event_loop_policy(policy)\n    return policy\n\n\ndef install_reactor(reactor_path: str, event_loop_path: Optional[str] = None) -> None:\n    \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n    import path. Also installs the asyncio event loop with the specified import\n    path if the asyncio reactor is enabled\"\"\"\n    reactor_class = load_object(reactor_path)\n    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n        set_asyncio_event_loop_policy()\n        with suppress(error.ReactorAlreadyInstalledError):\n            event_loop = set_asyncio_event_loop(event_loop_path)\n            asyncioreactor.install(eventloop=event_loop)\n    else:\n        *module, _ = reactor_path.split(\".\")\n        installer_path = module + [\"install\"]\n        installer = load_object(\".\".join(installer_path))\n        with suppress(error.ReactorAlreadyInstalledError):\n            installer()\n\n\ndef _get_asyncio_event_loop() -> AbstractEventLoop:\n    return set_asyncio_event_loop(None)\n\n\ndef set_asyncio_event_loop(event_loop_path: Optional[str]) -> AbstractEventLoop:\n    \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n    if event_loop_path is not None:\n        event_loop_class: Type[AbstractEventLoop] = load_object(event_loop_path)\n        event_loop = event_loop_class()\n        asyncio.set_event_loop(event_loop)\n    else:\n        try:\n            with catch_warnings():\n                # In Python 3.10.9, 3.11.1, 3.12 and 3.13, a DeprecationWarning\n                # is emitted about the lack of a current event loop, because in\n                # Python 3.14 and later `get_event_loop` will raise a\n                # RuntimeError in that event. Because our code is already\n                # prepared for that future behavior, we ignore the deprecation\n                # warning.\n                filterwarnings(\n                    \"ignore\",\n                    message=\"There is no current event loop\",\n                    category=DeprecationWarning,\n                )\n                event_loop = asyncio.get_event_loop()\n        except RuntimeError:\n            # `get_event_loop` raises RuntimeError when called with no asyncio\n            # event loop yet installed in the following scenarios:\n            # - Previsibly on Python 3.14 and later.\n            #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902\n            event_loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(event_loop)\n    return event_loop\n\n\ndef verify_installed_reactor(reactor_path: str) -> None:\n    \"\"\"Raises :exc:`Exception` if the installed\n    :mod:`~twisted.internet.reactor` does not match the specified import\n    path.\"\"\"\n    from twisted.internet import reactor\n\n    reactor_class = load_object(reactor_path)\n    if not reactor.__class__ == reactor_class:\n        msg = (\n            \"The installed reactor \"\n            f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n            f\"match the requested one ({reactor_path})\"\n        )\n        raise Exception(msg)\n\n\ndef verify_installed_asyncio_event_loop(loop_path: str) -> None:\n    from twisted.internet import reactor\n\n    loop_class = load_object(loop_path)\n    if isinstance(reactor._asyncioEventloop, loop_class):\n        return\n    installed = (\n        f\"{reactor._asyncioEventloop.__class__.__module__}\"\n        f\".{reactor._asyncioEventloop.__class__.__qualname__}\"\n    )\n    specified = f\"{loop_class.__module__}.{loop_class.__qualname__}\"\n    raise Exception(\n        \"Scrapy found an asyncio Twisted reactor already \"\n        f\"installed, and its event loop class ({installed}) does \"\n        \"not match the one specified in the ASYNCIO_EVENT_LOOP \"\n        f\"setting ({specified})\"\n    )\n\n\ndef is_asyncio_reactor_installed() -> bool:\n    from twisted.internet import reactor\n\n    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n", "scrapy/utils/signal.py": "\"\"\"Helper functions for working with signals\"\"\"\n\nimport collections.abc\nimport logging\nfrom typing import Any as TypingAny\nfrom typing import List, Tuple\n\nfrom pydispatch.dispatcher import (\n    Anonymous,\n    Any,\n    disconnect,\n    getAllReceivers,\n    liveReceivers,\n)\nfrom pydispatch.robustapply import robustApply\nfrom twisted.internet.defer import Deferred, DeferredList\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.utils.defer import maybeDeferred_coro\nfrom scrapy.utils.log import failure_to_exc_info\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_catch_log(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny\n) -> List[Tuple[TypingAny, TypingAny]]:\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop(\"dont_log\", ())\n    dont_log = (\n        tuple(dont_log)\n        if isinstance(dont_log, collections.abc.Sequence)\n        else (dont_log,)\n    )\n    dont_log += (StopDownload,)\n    spider = named.get(\"spider\", None)\n    responses: List[Tuple[TypingAny, TypingAny]] = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        result: TypingAny\n        try:\n            response = robustApply(\n                receiver, signal=signal, sender=sender, *arguments, **named\n            )\n            if isinstance(response, Deferred):\n                logger.error(\n                    \"Cannot return deferreds from signal handler: %(receiver)s\",\n                    {\"receiver\": receiver},\n                    extra={\"spider\": spider},\n                )\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\n                \"Error caught on signal handler: %(receiver)s\",\n                {\"receiver\": receiver},\n                exc_info=True,\n                extra={\"spider\": spider},\n            )\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses\n\n\ndef send_catch_log_deferred(\n    signal: TypingAny = Any,\n    sender: TypingAny = Anonymous,\n    *arguments: TypingAny,\n    **named: TypingAny\n) -> Deferred:\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n\n    def logerror(failure: Failure, recv: Any) -> Failure:\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\n                \"Error caught on signal handler: %(receiver)s\",\n                {\"receiver\": recv},\n                exc_info=failure_to_exc_info(failure),\n                extra={\"spider\": spider},\n            )\n        return failure\n\n    dont_log = named.pop(\"dont_log\", None)\n    spider = named.get(\"spider\", None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred_coro(\n            robustApply, receiver, signal=signal, sender=sender, *arguments, **named\n        )\n        d.addErrback(logerror, receiver)\n        # TODO https://pylint.readthedocs.io/en/latest/user_guide/messages/warning/cell-var-from-loop.html\n        d.addBoth(\n            lambda result: (\n                receiver,  # pylint: disable=cell-var-from-loop  # noqa: B023\n                result,\n            )\n        )\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d\n\n\ndef disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)\n", "scrapy/utils/sitemap.py": "\"\"\"\nModule for processing Sitemaps.\n\nNote: The main purpose of this module is to provide support for the\nSitemapSpider, its API is subject to change without notice.\n\"\"\"\n\nfrom typing import Any, Dict, Iterable, Iterator, Optional, Union\nfrom urllib.parse import urljoin\n\nimport lxml.etree  # nosec\n\n\nclass Sitemap:\n    \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n    (type=sitemapindex) files\"\"\"\n\n    def __init__(self, xmltext: Union[str, bytes]):\n        xmlp = lxml.etree.XMLParser(\n            recover=True, remove_comments=True, resolve_entities=False\n        )\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # nosec\n        rt = self._root.tag\n        self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n\n    def __iter__(self) -> Iterator[Dict[str, Any]]:\n        for elem in self._root.getchildren():\n            d: Dict[str, Any] = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n\n                if name == \"link\":\n                    if \"href\" in el.attrib:\n                        d.setdefault(\"alternate\", []).append(el.get(\"href\"))\n                else:\n                    d[name] = el.text.strip() if el.text else \"\"\n\n            if \"loc\" in d:\n                yield d\n\n\ndef sitemap_urls_from_robots(\n    robots_text: str, base_url: Optional[str] = None\n) -> Iterable[str]:\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith(\"sitemap:\"):\n            url = line.split(\":\", 1)[1].strip()\n            yield urljoin(base_url or \"\", url)\n", "scrapy/utils/gz.py": "import struct\nfrom gzip import GzipFile\nfrom io import BytesIO\n\nfrom scrapy.http import Response\n\nfrom ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\n\n\ndef gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output_stream = BytesIO()\n    chunk = b\".\"\n    decompressed_size = 0\n    while chunk:\n        try:\n            chunk = f.read1(_CHUNK_SIZE)\n        except (OSError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output_stream is empty\n            if output_stream.getbuffer().nbytes > 0:\n                break\n            raise\n        decompressed_size += len(chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef gzip_magic_number(response: Response) -> bool:\n    return response.body[:3] == b\"\\x1f\\x8b\\x08\"\n", "scrapy/utils/ossignal.py": "import signal\nfrom types import FrameType\nfrom typing import Any, Callable, Dict, Optional, Union\n\n# copy of _HANDLER from typeshed/stdlib/signal.pyi\nSignalHandlerT = Union[\n    Callable[[int, Optional[FrameType]], Any], int, signal.Handlers, None\n]\n\nsignal_names: Dict[int, str] = {}\nfor signame in dir(signal):\n    if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n        signum = getattr(signal, signame)\n        if isinstance(signum, int):\n            signal_names[signum] = signame\n\n\ndef install_shutdown_handlers(\n    function: SignalHandlerT, override_sigint: bool = True\n) -> None:\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n    SIGINT handler won't be installed if there is already a handler in place\n    (e.g. Pdb)\n    \"\"\"\n    signal.signal(signal.SIGTERM, function)\n    if (\n        signal.getsignal(signal.SIGINT)  # pylint: disable=comparison-with-callable\n        == signal.default_int_handler\n        or override_sigint\n    ):\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, \"SIGBREAK\"):\n        signal.signal(signal.SIGBREAK, function)\n", "scrapy/utils/request.py": "\"\"\"\nThis module provides some useful functions for working with\nscrapy.http.Request objects\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    Union,\n)\nfrom urllib.parse import urlunparse\nfrom weakref import WeakKeyDictionary\n\nfrom w3lib.http import basic_auth_header\nfrom w3lib.url import canonicalize_url\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\ndef _serialize_headers(headers: Iterable[bytes], request: Request) -> Iterable[bytes]:\n    for header in headers:\n        if header in request.headers:\n            yield header\n            yield from request.headers.getlist(header)\n\n\n_fingerprint_cache: WeakKeyDictionary[\n    Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]\n]\n_fingerprint_cache = WeakKeyDictionary()\n\n\ndef fingerprint(\n    request: Request,\n    *,\n    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n    keep_fragments: bool = False,\n) -> bytes:\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (i.e. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accessible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lots of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingerprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    Also, servers usually ignore fragments in urls when handling requests,\n    so they are also ignored by default when calculating the fingerprint.\n    If you want to include them, set the keep_fragments argument to True\n    (for instance when handling requests with a headless browser).\n    \"\"\"\n    processed_include_headers: Optional[Tuple[bytes, ...]] = None\n    if include_headers:\n        processed_include_headers = tuple(\n            to_bytes(h.lower()) for h in sorted(include_headers)\n        )\n    cache = _fingerprint_cache.setdefault(request, {})\n    cache_key = (processed_include_headers, keep_fragments)\n    if cache_key not in cache:\n        # To decode bytes reliably (JSON does not support bytes), regardless of\n        # character encoding, we use bytes.hex()\n        headers: Dict[str, List[str]] = {}\n        if processed_include_headers:\n            for header in processed_include_headers:\n                if header in request.headers:\n                    headers[header.hex()] = [\n                        header_value.hex()\n                        for header_value in request.headers.getlist(header)\n                    ]\n        fingerprint_data = {\n            \"method\": to_unicode(request.method),\n            \"url\": canonicalize_url(request.url, keep_fragments=keep_fragments),\n            \"body\": (request.body or b\"\").hex(),\n            \"headers\": headers,\n        }\n        fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()  # nosec\n    return cache[cache_key]\n\n\nclass RequestFingerprinterProtocol(Protocol):\n    def fingerprint(self, request: Request) -> bytes: ...\n\n\nclass RequestFingerprinter:\n    \"\"\"Default fingerprinter.\n\n    It takes into account a canonical version\n    (:func:`w3lib.url.canonicalize_url`) of :attr:`request.url\n    <scrapy.http.Request.url>` and the values of :attr:`request.method\n    <scrapy.http.Request.method>` and :attr:`request.body\n    <scrapy.http.Request.body>`. It then generates an `SHA1\n    <https://en.wikipedia.org/wiki/SHA-1>`_ hash.\n\n    .. seealso:: :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION`.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def __init__(self, crawler: Optional[Crawler] = None):\n        if crawler:\n            implementation = crawler.settings.get(\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n            )\n        else:\n            implementation = \"SENTINEL\"\n\n        if implementation != \"SENTINEL\":\n            message = (\n                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n                \"And it will be removed in future version of Scrapy.\"\n            )\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n        self._fingerprint = fingerprint\n\n    def fingerprint(self, request: Request) -> bytes:\n        return self._fingerprint(request)\n\n\ndef request_authenticate(\n    request: Request,\n    username: str,\n    password: str,\n) -> None:\n    \"\"\"Authenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers[\"Authorization\"] = basic_auth_header(username, password)\n\n\ndef request_httprepr(request: Request) -> bytes:\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b\"\") + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s\n\n\ndef referer_str(request: Request) -> Optional[str]:\n    \"\"\"Return Referer HTTP header suitable for logging.\"\"\"\n    referrer = request.headers.get(\"Referer\")\n    if referrer is None:\n        return referrer\n    return to_unicode(referrer, errors=\"replace\")\n\n\ndef request_from_dict(d: Dict[str, Any], *, spider: Optional[Spider] = None) -> Request:\n    \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    request_cls: Type[Request] = load_object(d[\"_class\"]) if \"_class\" in d else Request\n    kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}\n    if d.get(\"callback\") and spider:\n        kwargs[\"callback\"] = _get_method(spider, d[\"callback\"])\n    if d.get(\"errback\") and spider:\n        kwargs[\"errback\"] = _get_method(spider, d[\"errback\"])\n    return request_cls(**kwargs)\n\n\ndef _get_method(obj: Any, name: Any) -> Any:\n    \"\"\"Helper function for request_from_dict\"\"\"\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(f\"Method {name!r} not found in: {obj}\")\n\n\ndef request_to_curl(request: Request) -> str:\n    \"\"\"\n    Converts a :class:`~scrapy.Request` object to a curl command.\n\n    :param :class:`~scrapy.Request`: Request object to be converted\n    :return: string containing the curl command\n    \"\"\"\n    method = request.method\n\n    data = f\"--data-raw '{request.body.decode('utf-8')}'\" if request.body else \"\"\n\n    headers = \" \".join(\n        f\"-H '{k.decode()}: {v[0].decode()}'\" for k, v in request.headers.items()\n    )\n\n    url = request.url\n    cookies = \"\"\n    if request.cookies:\n        if isinstance(request.cookies, dict):\n            cookie = \"; \".join(f\"{k}={v}\" for k, v in request.cookies.items())\n            cookies = f\"--cookie '{cookie}'\"\n        elif isinstance(request.cookies, list):\n            cookie = \"; \".join(\n                f\"{list(c.keys())[0]}={list(c.values())[0]}\" for c in request.cookies\n            )\n            cookies = f\"--cookie '{cookie}'\"\n\n    curl_cmd = f\"curl -X {method} {url} {data} {headers} {cookies}\".strip()\n    return \" \".join(curl_cmd.split())\n", "scrapy/utils/testsite.py": "from urllib.parse import urljoin\n\nfrom twisted.web import resource, server, static, util\n\n\nclass SiteTest:\n    def setUp(self):\n        from twisted.internet import reactor\n\n        super().setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"\n\n    def tearDown(self):\n        super().tearDown()\n        self.site.stopListening()\n\n    def url(self, path: str) -> str:\n        return urljoin(self.baseurl, path)\n\n\nclass NoMetaRefreshRedirect(util.Redirect):\n    def render(self, request: server.Request) -> bytes:\n        content = util.Redirect.render(self, request)\n        return content.replace(\n            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n        )\n\n\ndef test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(\n        b\"html\",\n        static.Data(\n            b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n            \"text/html\",\n        ),\n    )\n    r.putChild(\n        b\"enc-gb18030\",\n        static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n    )\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor\n\n    port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n    print(f\"http://localhost:{port.getHost().port}/\")\n    reactor.run()\n", "scrapy/utils/iterators.py": "import csv\nimport logging\nimport re\nfrom io import StringIO\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Union,\n    cast,\n    overload,\n)\nfrom warnings import warn\n\nfrom lxml import etree  # nosec\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import re_rsearch\n\nlogger = logging.getLogger(__name__)\n\n\ndef xmliter(obj: Union[Response, str, bytes], nodename: str) -> Iterator[Selector]:\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    warn(\n        (\n            \"xmliter is deprecated and its use strongly discouraged because \"\n            \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"\n            \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    nodename_patt = re.escape(nodename)\n\n    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)\n    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)\n    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n    text = _body_or_str(obj)\n\n    document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n    document_header = (\n        document_header_match.group().strip() if document_header_match else \"\"\n    )\n    header_end_idx = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"\n    namespaces: Dict[str, str] = {}\n    if header_end:\n        for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n            assert header_end_idx\n            tag = re.search(\n                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n            )\n            if tag:\n                for x in re.findall(NAMESPACE_RE, tag.group()):\n                    namespaces[x[1]] = x[0]\n\n    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = (\n            document_header\n            + match.group().replace(\n                nodename, f'{nodename} {\" \".join(namespaces.values())}', 1\n            )\n            + header_end\n        )\n        yield Selector(text=nodetext, type=\"xml\")\n\n\ndef xmliter_lxml(\n    obj: Union[Response, str, bytes],\n    nodename: str,\n    namespace: Optional[str] = None,\n    prefix: str = \"x\",\n) -> Iterator[Selector]:\n    reader = _StreamReader(obj)\n    tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n    iterable = etree.iterparse(\n        reader,\n        encoding=reader.encoding,\n        events=(\"end\", \"start-ns\"),\n        resolve_entities=False,\n        huge_tree=True,\n    )\n    selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n    needs_namespace_resolution = not namespace and \":\" in nodename\n    if needs_namespace_resolution:\n        prefix, nodename = nodename.split(\":\", maxsplit=1)\n    for event, data in iterable:\n        if event == \"start-ns\":\n            assert isinstance(data, tuple)\n            if needs_namespace_resolution:\n                _prefix, _namespace = data\n                if _prefix != prefix:\n                    continue\n                namespace = _namespace\n                needs_namespace_resolution = False\n                selxpath = f\"//{prefix}:{nodename}\"\n                tag = f\"{{{namespace}}}{nodename}\"\n            continue\n        assert isinstance(data, etree._Element)\n        node = data\n        if node.tag != tag:\n            continue\n        nodetext = etree.tostring(node, encoding=\"unicode\")\n        node.clear()\n        xs = Selector(text=nodetext, type=\"xml\")\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]\n\n\nclass _StreamReader:\n    def __init__(self, obj: Union[Response, str, bytes]):\n        self._ptr: int = 0\n        self._text: Union[str, bytes]\n        if isinstance(obj, TextResponse):\n            self._text, self.encoding = obj.body, obj.encoding\n        elif isinstance(obj, Response):\n            self._text, self.encoding = obj.body, \"utf-8\"\n        else:\n            self._text, self.encoding = obj, \"utf-8\"\n        self._is_unicode: bool = isinstance(self._text, str)\n        self._is_first_read: bool = True\n\n    def read(self, n: int = 65535) -> bytes:\n        method: Callable[[int], bytes] = (\n            self._read_unicode if self._is_unicode else self._read_string\n        )\n        result = method(n)\n        if self._is_first_read:\n            self._is_first_read = False\n            result = result.lstrip()\n        return result\n\n    def _read_string(self, n: int = 65535) -> bytes:\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return cast(bytes, self._text)[s:e]\n\n    def _read_unicode(self, n: int = 65535) -> bytes:\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return cast(str, self._text)[s:e].encode(\"utf-8\")\n\n\ndef csviter(\n    obj: Union[Response, str, bytes],\n    delimiter: Optional[str] = None,\n    headers: Optional[List[str]] = None,\n    encoding: Optional[str] = None,\n    quotechar: Optional[str] = None,\n) -> Iterator[Dict[str, str]]:\n    \"\"\"Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    if encoding is not None:\n        warn(\n            \"The encoding argument of csviter() is ignored and will be removed\"\n            \" in a future Scrapy version.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n\n    lines = StringIO(_body_or_str(obj, unicode=True))\n\n    kwargs: Dict[str, Any] = {}\n    if delimiter:\n        kwargs[\"delimiter\"] = delimiter\n    if quotechar:\n        kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            headers = next(csv_r)\n        except StopIteration:\n            return\n\n    for row in csv_r:\n        if len(row) != len(headers):\n            logger.warning(\n                \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                \"should be: %(csvheader)d)\",\n                {\n                    \"csvlnum\": csv_r.line_num,\n                    \"csvrow\": len(row),\n                    \"csvheader\": len(headers),\n                },\n            )\n            continue\n        yield dict(zip(headers, row))\n\n\n@overload\ndef _body_or_str(obj: Union[Response, str, bytes]) -> str: ...\n\n\n@overload\ndef _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str: ...\n\n\n@overload\ndef _body_or_str(\n    obj: Union[Response, str, bytes], unicode: Literal[False]\n) -> bytes: ...\n\n\ndef _body_or_str(\n    obj: Union[Response, str, bytes], unicode: bool = True\n) -> Union[str, bytes]:\n    expected_types = (Response, str, bytes)\n    if not isinstance(obj, expected_types):\n        expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n        raise TypeError(\n            f\"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}\"\n        )\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        if isinstance(obj, TextResponse):\n            return obj.text\n        return obj.body.decode(\"utf-8\")\n    if isinstance(obj, str):\n        return obj if unicode else obj.encode(\"utf-8\")\n    return obj.decode(\"utf-8\") if unicode else obj\n", "scrapy/utils/versions.py": "import platform\nimport sys\nfrom typing import List, Tuple\n\nimport cryptography\nimport cssselect\nimport lxml.etree  # nosec\nimport parsel\nimport twisted\nimport w3lib\n\nimport scrapy\nfrom scrapy.utils.ssl import get_openssl_version\n\n\ndef scrapy_components_versions() -> List[Tuple[str, str]]:\n    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n\n    return [\n        (\"Scrapy\", scrapy.__version__),\n        (\"lxml\", lxml_version),\n        (\"libxml2\", libxml2_version),\n        (\"cssselect\", cssselect.__version__),\n        (\"parsel\", parsel.__version__),\n        (\"w3lib\", w3lib.__version__),\n        (\"Twisted\", twisted.version.short()),\n        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n        (\"pyOpenSSL\", get_openssl_version()),\n        (\"cryptography\", cryptography.__version__),\n        (\"Platform\", platform.platform()),\n    ]\n", "scrapy/utils/httpobj.py": "\"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n\nfrom typing import Union\nfrom urllib.parse import ParseResult, urlparse\nfrom weakref import WeakKeyDictionary\n\nfrom scrapy.http import Request, Response\n\n_urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = (\n    WeakKeyDictionary()\n)\n\n\ndef urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]\n", "scrapy/utils/ftp.py": "import posixpath\nfrom ftplib import FTP, error_perm\nfrom posixpath import dirname\nfrom typing import IO\n\n\ndef ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool = True) -> None:\n    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n    argument (as a ftplib.FTP object), creating all parent directories if they\n    don't exist. The ftplib.FTP object must be already connected and logged in.\n    \"\"\"\n    try:\n        ftp.cwd(path)\n    except error_perm:\n        ftp_makedirs_cwd(ftp, dirname(path), False)\n        ftp.mkd(path)\n        if first_call:\n            ftp.cwd(path)\n\n\ndef ftp_store_file(\n    *,\n    path: str,\n    file: IO[bytes],\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n    overwrite: bool = True,\n) -> None:\n    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n    to the directory extracted from given path, then uploads the file to server\n    \"\"\"\n    with FTP() as ftp:\n        ftp.connect(host, port)\n        ftp.login(username, password)\n        if use_active_mode:\n            ftp.set_pasv(False)\n        file.seek(0)\n        dirname, filename = posixpath.split(path)\n        ftp_makedirs_cwd(ftp, dirname)\n        command = \"STOR\" if overwrite else \"APPE\"\n        ftp.storbinary(f\"{command} {filename}\", file)\n        file.close()\n", "scrapy/utils/job.py": "from pathlib import Path\nfrom typing import Optional\n\nfrom scrapy.settings import BaseSettings\n\n\ndef job_dir(settings: BaseSettings) -> Optional[str]:\n    path: Optional[str] = settings[\"JOBDIR\"]\n    if not path:\n        return None\n    if not Path(path).exists():\n        Path(path).mkdir(parents=True)\n    return path\n", "scrapy/utils/conf.py": "import numbers\nimport os\nimport sys\nimport warnings\nfrom configparser import ConfigParser\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Collection,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning, UsageError\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.deprecate import update_classpath\nfrom scrapy.utils.python import without_none_values\n\n\ndef build_component_list(\n    compdict: MutableMapping[Any, Any],\n    custom: Any = None,\n    convert: Callable[[Any], Any] = update_classpath,\n) -> List[Any]:\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist: Collection[Any]) -> None:\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(\n                f\"Some paths in {complist!r} convert to the same object, \"\n                \"please update your settings\"\n            )\n\n    def _map_keys(compdict: Mapping[Any, Any]) -> Union[BaseSettings, Dict[Any, Any]]:\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                assert prio is not None\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(\n                        f\"Some paths in {list(compdict.keys())!r} \"\n                        \"convert to the same \"\n                        \"object, please update your settings\"\n                    )\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        _check_components(compdict)\n        return {convert(k): v for k, v in compdict.items()}\n\n    def _validate_values(compdict: Mapping[Any, Any]) -> None:\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(\n                    f\"Invalid value {value} for component {name}, \"\n                    \"please provide a real number or None instead\"\n                )\n\n    if custom is not None:\n        warnings.warn(\n            \"The 'custom' attribute of build_component_list() is deprecated. \"\n            \"Please merge its value into 'compdict' manually or change your \"\n            \"code to use Settings.getwithbase().\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        if isinstance(custom, (list, tuple)):\n            _check_components(custom)\n            return type(custom)(convert(c) for c in custom)  # type: ignore[return-value]\n        compdict.update(custom)\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n\n\ndef arglist_to_dict(arglist: List[str]) -> Dict[str, str]:\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split(\"=\", 1) for x in arglist)\n\n\ndef closest_scrapy_cfg(\n    path: Union[str, os.PathLike] = \".\",\n    prevpath: Optional[Union[str, os.PathLike]] = None,\n) -> str:\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if prevpath is not None and str(path) == str(prevpath):\n        return \"\"\n    path = Path(path).resolve()\n    cfgfile = path / \"scrapy.cfg\"\n    if cfgfile.exists():\n        return str(cfgfile)\n    return closest_scrapy_cfg(path.parent, path)\n\n\ndef init_env(project: str = \"default\", set_syspath: bool = True) -> None:\n    \"\"\"Initialize environment to use command-line tool from inside a project\n    dir. This sets the Scrapy settings module and modifies the Python path to\n    be able to locate the project module.\n    \"\"\"\n    cfg = get_config()\n    if cfg.has_option(\"settings\", project):\n        os.environ[\"SCRAPY_SETTINGS_MODULE\"] = cfg.get(\"settings\", project)\n    closest = closest_scrapy_cfg()\n    if closest:\n        projdir = str(Path(closest).parent)\n        if set_syspath and projdir not in sys.path:\n            sys.path.append(projdir)\n\n\ndef get_config(use_closest: bool = True) -> ConfigParser:\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg\n\n\ndef get_sources(use_closest: bool = True) -> List[str]:\n    xdg_config_home = (\n        os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n    )\n    sources = [\n        \"/etc/scrapy.cfg\",\n        r\"c:\\scrapy\\scrapy.cfg\",\n        str(Path(xdg_config_home) / \"scrapy.cfg\"),\n        str(Path(\"~/.scrapy.cfg\").expanduser()),\n    ]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources\n\n\ndef feed_complete_default_values_from_settings(\n    feed: Dict[str, Any], settings: BaseSettings\n) -> Dict[str, Any]:\n    out = feed.copy()\n    out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n    out.setdefault(\"item_export_kwargs\", {})\n    if settings[\"FEED_EXPORT_INDENT\"] is None:\n        out.setdefault(\"indent\", None)\n    else:\n        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n    return out\n\n\ndef feed_process_params_from_cli(\n    settings: BaseSettings,\n    output: List[str],\n    output_format: Optional[str] = None,\n    overwrite_output: Optional[List[str]] = None,\n) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Receives feed export params (from the 'crawl' or 'runspider' commands),\n    checks for inconsistencies in their quantities and returns a dictionary\n    suitable to be used as the FEEDS setting.\n    \"\"\"\n    valid_output_formats: Iterable[str] = without_none_values(\n        cast(Dict[str, str], settings.getwithbase(\"FEED_EXPORTERS\"))\n    ).keys()\n\n    def check_valid_format(output_format: str) -> None:\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )\n\n    overwrite = False\n    if overwrite_output:\n        if output:\n            raise UsageError(\n                \"Please use only one of -o/--output and -O/--overwrite-output\"\n            )\n        if output_format:\n            raise UsageError(\n                \"-t/--output-format is a deprecated command line option\"\n                \" and does not work in combination with -O/--overwrite-output.\"\n                \" To specify a format please specify it after a colon at the end of the\"\n                \" output URI (i.e. -O <URI>:<FORMAT>).\"\n                \" Example working in the tutorial: \"\n                \"scrapy crawl quotes -O quotes.json:json\"\n            )\n        output = overwrite_output\n        overwrite = True\n\n    if output_format:\n        if len(output) == 1:\n            check_valid_format(output_format)\n            message = (\n                \"The -t/--output-format command line option is deprecated in favor of \"\n                \"specifying the output format within the output URI using the -o/--output or the\"\n                \" -O/--overwrite-output option (i.e. -o/-O <URI>:<FORMAT>). See the documentation\"\n                \" of the -o or -O option or the following examples for more information. \"\n                \"Examples working in the tutorial: \"\n                \"scrapy crawl quotes -o quotes.csv:csv   or   \"\n                \"scrapy crawl quotes -O quotes.json:json\"\n            )\n            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n            return {output[0]: {\"format\": output_format}}\n        raise UsageError(\n            \"The -t command-line option cannot be used if multiple output \"\n            \"URIs are specified\"\n        )\n\n    result: Dict[str, Dict[str, Any]] = {}\n    for element in output:\n        try:\n            feed_uri, feed_format = element.rsplit(\":\", 1)\n            check_valid_format(feed_format)\n        except (ValueError, UsageError):\n            feed_uri = element\n            feed_format = Path(element).suffix.replace(\".\", \"\")\n        else:\n            if feed_uri == \"-\":\n                feed_uri = \"stdout:\"\n        check_valid_format(feed_format)\n        result[feed_uri] = {\"format\": feed_format}\n        if overwrite:\n            result[feed_uri][\"overwrite\"] = True\n\n    # FEEDS setting should take precedence over the matching CLI options\n    result.update(settings.getdict(\"FEEDS\"))\n\n    return result\n", "scrapy/utils/_compression.py": "import zlib\nfrom io import BytesIO\nfrom warnings import warn\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\ntry:\n    try:\n        import brotli\n    except ImportError:\n        import brotlicffi as brotli\nexcept ImportError:\n    pass\nelse:\n    try:\n        brotli.Decompressor.process\n    except AttributeError:\n        warn(\n            (\n                \"You have brotlipy installed, and Scrapy will use it, but \"\n                \"Scrapy support for brotlipy is deprecated and will stop \"\n                \"working in a future version of Scrapy. brotlipy itself is \"\n                \"deprecated, it has been superseded by brotlicffi. Please, \"\n                \"uninstall brotlipy and install brotli or brotlicffi instead. \"\n                \"brotlipy has the same import name as brotli, so keeping both \"\n                \"installed is strongly discouraged.\"\n            ),\n            ScrapyDeprecationWarning,\n        )\n\n        def _brotli_decompress(decompressor, data):\n            return decompressor.decompress(data)\n\n    else:\n\n        def _brotli_decompress(decompressor, data):\n            return decompressor.process(data)\n\n\ntry:\n    import zstandard\nexcept ImportError:\n    pass\n\n\n_CHUNK_SIZE = 65536  # 64 KiB\n\n\nclass _DecompressionMaxSizeExceeded(ValueError):\n    pass\n\n\ndef _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = zlib.decompressobj()\n    raw_decompressor = zlib.decompressobj(wbits=-15)\n    input_stream = BytesIO(data)\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        input_chunk = input_stream.read(_CHUNK_SIZE)\n        try:\n            output_chunk = decompressor.decompress(input_chunk)\n        except zlib.error:\n            if decompressor != raw_decompressor:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                decompressor = raw_decompressor\n                output_chunk = decompressor.decompress(input_chunk)\n            else:\n                raise\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = brotli.Decompressor()\n    input_stream = BytesIO(data)\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        input_chunk = input_stream.read(_CHUNK_SIZE)\n        output_chunk = _brotli_decompress(decompressor, input_chunk)\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n\n\ndef _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n    decompressor = zstandard.ZstdDecompressor()\n    stream_reader = decompressor.stream_reader(BytesIO(data))\n    output_stream = BytesIO()\n    output_chunk = b\".\"\n    decompressed_size = 0\n    while output_chunk:\n        output_chunk = stream_reader.read(_CHUNK_SIZE)\n        decompressed_size += len(output_chunk)\n        if max_size and decompressed_size > max_size:\n            raise _DecompressionMaxSizeExceeded(\n                f\"The number of bytes decompressed so far \"\n                f\"({decompressed_size} B) exceed the specified maximum \"\n                f\"({max_size} B).\"\n            )\n        output_stream.write(output_chunk)\n    output_stream.seek(0)\n    return output_stream.read()\n", "scrapy/utils/python.py": "\"\"\"\nThis module contains essential stuff that should've come with Python itself ;)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport collections.abc\nimport gc\nimport inspect\nimport re\nimport sys\nimport weakref\nfrom functools import partial, wraps\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Pattern,\n    Tuple,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom scrapy.utils.asyncgen import as_async_generator\n\nif TYPE_CHECKING:\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\ndef flatten(x: Iterable[Any]) -> List[Any]:\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))\n\n\ndef iflatten(x: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            yield from iflatten(el)\n        else:\n            yield el\n\n\ndef is_listlike(x: Any) -> bool:\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n\n\ndef unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> List[_T]:\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result: List[_T] = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result\n\n\ndef to_unicode(\n    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n) -> str:\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError(\n            \"to_unicode must receive a bytes or str \"\n            f\"object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.decode(encoding, errors)\n\n\ndef to_bytes(\n    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n) -> bytes:\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError(\n            \"to_bytes must receive a str or bytes \" f\"object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.encode(encoding, errors)\n\n\ndef re_rsearch(\n    pattern: Union[str, Pattern[str]], text: str, chunk_size: int = 1024\n) -> Optional[Tuple[int, int]]:\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter() -> Iterable[Tuple[str, int]]:\n        offset = len(text)\n        while True:\n            offset -= chunk_size * 1024\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = list(pattern.finditer(chunk))\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None\n\n\n_SelfT = TypeVar(\"_SelfT\")\n\n\ndef memoizemethod_noargs(\n    method: Callable[Concatenate[_SelfT, _P], _T]\n) -> Callable[Concatenate[_SelfT, _P], _T]:\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache: weakref.WeakKeyDictionary[_SelfT, _T] = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self: _SelfT, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method\n\n\n_BINARYCHARS = {\n    i for i in range(32) if to_bytes(chr(i)) not in {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n}\n\n\ndef binary_is_text(data: bytes) -> bool:\n    \"\"\"Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)\n\n\ndef get_func_args(func: Callable[..., Any], stripself: bool = False) -> List[str]:\n    \"\"\"Return the argument name list of a callable object\"\"\"\n    if not callable(func):\n        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n\n    args: List[str] = []\n    try:\n        sig = inspect.signature(func)\n    except ValueError:\n        return args\n\n    if isinstance(func, partial):\n        partial_args = func.args\n        partial_kw = func.keywords\n\n        for name, param in sig.parameters.items():\n            if param.name in partial_args:\n                continue\n            if partial_kw and param.name in partial_kw:\n                continue\n            args.append(name)\n    else:\n        for name in sig.parameters.keys():\n            args.append(name)\n\n    if stripself and args and args[0] == \"self\":\n        args = args[1:]\n    return args\n\n\ndef get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getfullargspec(func)\n    elif hasattr(func, \"__call__\"):  # noqa: B004\n        spec = inspect.getfullargspec(func.__call__)\n    else:\n        raise TypeError(f\"{type(func)} is not callable\")\n\n    defaults: Tuple[Any, ...] = spec.defaults or ()\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs\n\n\ndef equal_attributes(\n    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable[[Any], Any]]]]\n) -> bool:\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True\n\n\n@overload\ndef without_none_values(iterable: Mapping[_KT, _VT]) -> Dict[_KT, _VT]: ...\n\n\n@overload\ndef without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n\n\ndef without_none_values(\n    iterable: Union[Mapping[_KT, _VT], Iterable[_KT]]\n) -> Union[Dict[_KT, _VT], Iterable[_KT]]:\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    if isinstance(iterable, collections.abc.Mapping):\n        return {k: v for k, v in iterable.items() if v is not None}\n    else:\n        # the iterable __init__ must take another iterable\n        return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n\n\ndef global_object_name(obj: Any) -> str:\n    \"\"\"\n    Return full name of a global object.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__qualname__}\"\n\n\nif hasattr(sys, \"pypy_version_info\"):\n\n    def garbage_collect() -> None:\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()\n\nelse:\n\n    def garbage_collect() -> None:\n        gc.collect()\n\n\nclass MutableChain(Iterable[_T]):\n    \"\"\"\n    Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T]):\n        self.data: Iterator[_T] = chain.from_iterable(args)\n\n    def extend(self, *iterables: Iterable[_T]) -> None:\n        self.data = chain(self.data, chain.from_iterable(iterables))\n\n    def __iter__(self) -> Iterator[_T]:\n        return self\n\n    def __next__(self) -> _T:\n        return next(self.data)\n\n\nasync def _async_chain(\n    *iterables: Union[Iterable[_T], AsyncIterable[_T]]\n) -> AsyncIterator[_T]:\n    for it in iterables:\n        async for o in as_async_generator(it):\n            yield o\n\n\nclass MutableAsyncChain(AsyncIterable[_T]):\n    \"\"\"\n    Similar to MutableChain but for async iterables\n    \"\"\"\n\n    def __init__(self, *args: Union[Iterable[_T], AsyncIterable[_T]]):\n        self.data: AsyncIterator[_T] = _async_chain(*args)\n\n    def extend(self, *iterables: Union[Iterable[_T], AsyncIterable[_T]]) -> None:\n        self.data = _async_chain(self.data, _async_chain(*iterables))\n\n    def __aiter__(self) -> AsyncIterator[_T]:\n        return self\n\n    async def __anext__(self) -> _T:\n        return await self.data.__anext__()\n", "scrapy/utils/template.py": "\"\"\"Helper functions for working with templates\"\"\"\n\nimport re\nimport string\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Any, Union\n\n\ndef render_templatefile(path: Union[str, PathLike], **kwargs: Any) -> None:\n    path_obj = Path(path)\n    raw = path_obj.read_text(\"utf8\")\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path_obj.with_suffix(\"\") if path_obj.suffix == \".tmpl\" else path_obj\n\n    if path_obj.suffix == \".tmpl\":\n        path_obj.rename(render_path)\n\n    render_path.write_text(content, \"utf8\")\n\n\nCAMELCASE_INVALID_CHARS = re.compile(r\"[^a-zA-Z\\d]\")\n\n\ndef string_camelcase(string: str) -> str:\n    \"\"\"Convert a word  to its CamelCase version and remove invalid chars\n\n    >>> string_camelcase('lost-pound')\n    'LostPound'\n\n    >>> string_camelcase('missing_images')\n    'MissingImages'\n\n    \"\"\"\n    return CAMELCASE_INVALID_CHARS.sub(\"\", string.title())\n", "scrapy/utils/testproc.py": "from __future__ import annotations\n\nimport os\nimport sys\nfrom typing import Iterable, List, Optional, Tuple, cast\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.error import ProcessTerminated\nfrom twisted.internet.protocol import ProcessProtocol\nfrom twisted.python.failure import Failure\n\n\nclass ProcessTest:\n    command: Optional[str] = None\n    prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n    cwd = os.getcwd()  # trial chdirs to temp dir\n\n    def execute(\n        self,\n        args: Iterable[str],\n        check_code: bool = True,\n        settings: Optional[str] = None,\n    ) -> Deferred:\n        from twisted.internet import reactor\n\n        env = os.environ.copy()\n        if settings is not None:\n            env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n        assert self.command\n        cmd = self.prefix + [self.command] + list(args)\n        pp = TestProcessProtocol()\n        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred\n\n    def _process_finished(\n        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool\n    ) -> Tuple[int, bytes, bytes]:\n        if pp.exitcode and check_code:\n            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n            msg += f\"\\n>>> stdout <<<\\n{pp.out.decode()}\"\n            msg += \"\\n\"\n            msg += f\"\\n>>> stderr <<<\\n{pp.err.decode()}\"\n            raise RuntimeError(msg)\n        return cast(int, pp.exitcode), pp.out, pp.err\n\n\nclass TestProcessProtocol(ProcessProtocol):\n    def __init__(self) -> None:\n        self.deferred: Deferred = Deferred()\n        self.out: bytes = b\"\"\n        self.err: bytes = b\"\"\n        self.exitcode: Optional[int] = None\n\n    def outReceived(self, data: bytes) -> None:\n        self.out += data\n\n    def errReceived(self, data: bytes) -> None:\n        self.err += data\n\n    def processEnded(self, status: Failure) -> None:\n        self.exitcode = cast(ProcessTerminated, status.value).exitCode\n        self.deferred.callback(self)\n", "scrapy/utils/misc.py": "\"\"\"Helper functions which don't fit anywhere else\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport hashlib\nimport inspect\nimport os\nimport re\nimport warnings\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom importlib import import_module\nfrom pkgutil import iter_modules\nfrom types import ModuleType\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Deque,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.item import Item\nfrom scrapy.utils.datatypes import LocalWeakReferencedCache\n\nif TYPE_CHECKING:\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings\n\n_ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\nT = TypeVar(\"T\")\n\n\ndef arg_to_iter(arg: Any) -> Iterable[Any]:\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, \"__iter__\"):\n        return cast(Iterable[Any], arg)\n    return [arg]\n\n\ndef load_object(path: Union[str, Callable[..., Any]]) -> Any:\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    The object can be the import path of a class, function, variable or an\n    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n\n    If ``path`` is not a string, but is a callable object, such as a class or\n    a function, then return it as is.\n    \"\"\"\n\n    if not isinstance(path, str):\n        if callable(path):\n            return path\n        raise TypeError(\n            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n        )\n\n    try:\n        dot = path.rindex(\".\")\n    except ValueError:\n        raise ValueError(f\"Error loading object '{path}': not a full path\")\n\n    module, name = path[:dot], path[dot + 1 :]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n\n    return obj\n\n\ndef walk_modules(path: str) -> List[ModuleType]:\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods: List[ModuleType] = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, \"__path__\"):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + \".\" + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods\n\n\ndef md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    warnings.warn(\n        (\n            \"The scrapy.utils.misc.md5sum function is deprecated, and will be \"\n            \"removed in a future version of Scrapy.\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    m = hashlib.md5()  # nosec\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\ndef rel_has_nofollow(rel: Optional[str]) -> bool:\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n\n\ndef create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n\n    .. versionchanged:: 2.2\n       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n       extension has not been implemented correctly).\n    \"\"\"\n    warnings.warn(\n        \"The create_instance() function is deprecated. \"\n        \"Please use build_from_crawler() or build_from_settings() instead.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specify at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return instance\n\n\ndef build_from_crawler(\n    objcls: Type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n) -> T:\n    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n\n    Raises ``TypeError`` if the resulting instance is ``None``.\n    \"\"\"\n    if hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return cast(T, instance)\n\n\ndef build_from_settings(\n    objcls: Type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n) -> T:\n    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n\n    Raises ``TypeError`` if the resulting instance is ``None``.\n    \"\"\"\n    if hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return cast(T, instance)\n\n\n@contextmanager\ndef set_environ(**kwargs: str) -> Iterator[None]:\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v\n\n\ndef walk_callable(node: ast.AST) -> Iterable[ast.AST]:\n    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n    functions defined within the node.\n    \"\"\"\n    todo: Deque[ast.AST] = deque([node])\n    walked_func_def = False\n    while todo:\n        node = todo.popleft()\n        if isinstance(node, ast.FunctionDef):\n            if walked_func_def:\n                continue\n            walked_func_def = True\n        todo.extend(ast.iter_child_nodes(node))\n        yield node\n\n\n_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n\n\ndef is_generator_with_return_value(callable: Callable[..., Any]) -> bool:\n    \"\"\"\n    Returns True if a callable is a generator function which includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n    if callable in _generator_callbacks_cache:\n        return bool(_generator_callbacks_cache[callable])\n\n    def returns_none(return_node: ast.Return) -> bool:\n        value = return_node.value\n        return (\n            value is None or isinstance(value, ast.NameConstant) and value.value is None\n        )\n\n    if inspect.isgeneratorfunction(callable):\n        func = callable\n        while isinstance(func, partial):\n            func = func.func\n\n        src = inspect.getsource(func)\n        pattern = re.compile(r\"(^[\\t ]+)\")\n        code = pattern.sub(\"\", src)\n\n        match = pattern.match(src)  # finds indentation\n        if match:\n            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n\n        tree = ast.parse(code)\n        for node in walk_callable(tree):\n            if isinstance(node, ast.Return) and not returns_none(node):\n                _generator_callbacks_cache[callable] = True\n                return bool(_generator_callbacks_cache[callable])\n\n    _generator_callbacks_cache[callable] = False\n    return bool(_generator_callbacks_cache[callable])\n\n\ndef warn_on_generator_with_return_value(\n    spider: Spider, callable: Callable[..., Any]\n) -> None:\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    try:\n        if is_generator_with_return_value(callable):\n            warnings.warn(\n                f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                'a generator and includes a \"return\" statement with a value '\n                \"different than None. This could lead to unexpected behaviour. Please see \"\n                \"https://docs.python.org/3/reference/simple_stmts.html#the-return-statement \"\n                'for details about the semantics of the \"return\" statement within generators',\n                stacklevel=2,\n            )\n    except IndentationError:\n        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n        warnings.warn(\n            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n            \"This will not prevent your code from working, but it prevents Scrapy from detecting \"\n            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n            \"Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), \"\n            f'including the code of \"{callable_name}\"',\n            stacklevel=2,\n        )\n", "scrapy/utils/datatypes.py": "\"\"\"\nThis module contains data types used by Scrapy which are not included in the\nPython Standard Library.\n\nThis module must not depend on any module outside the Standard Library.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport warnings\nimport weakref\nfrom collections.abc import Mapping\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Iterable,\n    Optional,\n    OrderedDict,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n)\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\nclass CaselessDict(dict):\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n        from scrapy.http.headers import Headers\n\n        if issubclass(cls, CaselessDict) and not issubclass(cls, Headers):\n            warnings.warn(\n                \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__new__(cls, *args, **kwargs)\n\n    def __init__(\n        self,\n        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n    ):\n        super().__init__()\n        if seq:\n            self.update(seq)\n\n    def __getitem__(self, key: AnyStr) -> Any:\n        return dict.__getitem__(self, self.normkey(key))\n\n    def __setitem__(self, key: AnyStr, value: Any) -> None:\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))\n\n    def __delitem__(self, key: AnyStr) -> None:\n        dict.__delitem__(self, self.normkey(key))\n\n    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n        return dict.__contains__(self, self.normkey(key))\n\n    has_key = __contains__\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n\n    def normkey(self, key: AnyStr) -> AnyStr:\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()\n\n    def normvalue(self, value: Any) -> Any:\n        \"\"\"Method to normalize values prior to be set\"\"\"\n        return value\n\n    def get(self, key: AnyStr, def_val: Any = None) -> Any:\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))\n\n    def setdefault(self, key: AnyStr, def_val: Any = None) -> Any:\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))  # type: ignore[arg-type]\n\n    # doesn't fully implement MutableMapping.update()\n    def update(self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]) -> None:  # type: ignore[override]\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super().update(iseq)\n\n    @classmethod\n    def fromkeys(cls, keys: Iterable[AnyStr], value: Any = None) -> Self:  # type: ignore[override]\n        return cls((k, value) for k in keys)  # type: ignore[misc]\n\n    def pop(self, key: AnyStr, *args: Any) -> Any:\n        return dict.pop(self, self.normkey(key), *args)\n\n\nclass CaseInsensitiveDict(collections.UserDict):\n    \"\"\"A dict-like structure that accepts strings or bytes\n    as keys and allows case-insensitive lookups.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._keys: dict = {}\n        super().__init__(*args, **kwargs)\n\n    def __getitem__(self, key: AnyStr) -> Any:\n        normalized_key = self._normkey(key)\n        return super().__getitem__(self._keys[normalized_key.lower()])\n\n    def __setitem__(self, key: AnyStr, value: Any) -> None:\n        normalized_key = self._normkey(key)\n        try:\n            lower_key = self._keys[normalized_key.lower()]\n            del self[lower_key]\n        except KeyError:\n            pass\n        super().__setitem__(normalized_key, self._normvalue(value))\n        self._keys[normalized_key.lower()] = normalized_key\n\n    def __delitem__(self, key: AnyStr) -> None:\n        normalized_key = self._normkey(key)\n        stored_key = self._keys.pop(normalized_key.lower())\n        super().__delitem__(stored_key)\n\n    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n        normalized_key = self._normkey(key)\n        return normalized_key.lower() in self._keys\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {super().__repr__()}>\"\n\n    def _normkey(self, key: AnyStr) -> AnyStr:\n        return key\n\n    def _normvalue(self, value: Any) -> Any:\n        return value\n\n\nclass LocalCache(OrderedDict[_KT, _VT]):\n    \"\"\"Dictionary with a finite number of keys.\n\n    Older items expires first.\n    \"\"\"\n\n    def __init__(self, limit: Optional[int] = None):\n        super().__init__()\n        self.limit: Optional[int] = limit\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        if self.limit:\n            while len(self) >= self.limit:\n                self.popitem(last=False)\n        super().__setitem__(key, value)\n\n\nclass LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n    \"\"\"\n    A weakref.WeakKeyDictionary implementation that uses LocalCache as its\n    underlying data structure, making it ordered and capable of being size-limited.\n\n    Useful for memoization, while avoiding keeping received\n    arguments in memory only because of the cached references.\n\n    Note: like LocalCache and unlike weakref.WeakKeyDictionary,\n    it cannot be instantiated with an initial dictionary.\n    \"\"\"\n\n    def __init__(self, limit: Optional[int] = None):\n        super().__init__()\n        self.data: LocalCache = LocalCache(limit=limit)\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        try:\n            super().__setitem__(key, value)\n        except TypeError:\n            pass  # key is not weak-referenceable, skip caching\n\n    def __getitem__(self, key: _KT) -> Optional[_VT]:  # type: ignore[override]\n        try:\n            return super().__getitem__(key)\n        except (TypeError, KeyError):\n            return None  # key is either not weak-referenceable or not cached\n\n\nclass SequenceExclude:\n    \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n\n    def __init__(self, seq: Sequence[Any]):\n        self.seq: Sequence[Any] = seq\n\n    def __contains__(self, item: Any) -> bool:\n        return item not in self.seq\n", "scrapy/utils/ssl.py": "from typing import Any, Optional\n\nimport OpenSSL._util as pyOpenSSLutil\nimport OpenSSL.SSL\nimport OpenSSL.version\nfrom OpenSSL.crypto import X509Name\n\nfrom scrapy.utils.python import to_unicode\n\n\ndef ffi_buf_to_string(buf: Any) -> str:\n    return to_unicode(pyOpenSSLutil.ffi.string(buf))\n\n\ndef x509name_to_string(x509name: X509Name) -> str:\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer: Any = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(\n        x509name._name, result_buffer, len(result_buffer)  # type: ignore[attr-defined]\n    )\n\n    return ffi_buf_to_string(result_buffer)\n\n\ndef get_temp_key_info(ssl_object: Any) -> Optional[str]:\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    if not hasattr(pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"):\n        # removed in cryptography 40.0.0\n        return None\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append(\"RSA\")\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append(\"DH\")\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append(\"ECDH\")\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(\n            pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key)\n        )\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append(f\"{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits\")\n    return \", \".join(key_info)\n\n\ndef get_openssl_version() -> str:\n    system_openssl_bytes = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n    system_openssl = system_openssl_bytes.decode(\"ascii\", errors=\"replace\")\n    return f\"{OpenSSL.version.__version__} ({system_openssl})\"\n", "scrapy/utils/decorators.py": "from __future__ import annotations\n\nimport warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar\n\nfrom twisted.internet import defer, threads\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    # typing.ParamSpec requires Python 3.10\n    from typing_extensions import ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n\n_T = TypeVar(\"_T\")\n\n\ndef deprecated(\n    use_instead: Any = None,\n) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    def deco(func: Callable[_P, _T]) -> Callable[_P, _T]:\n        @wraps(func)\n        def wrapped(*args: _P.args, **kwargs: _P.kwargs) -> Any:\n            message = f\"Call to deprecated function {func.__name__}.\"\n            if use_instead:\n                message += f\" Use {use_instead} instead.\"\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return wrapped\n\n    if callable(use_instead):\n        deco = deco(use_instead)\n        use_instead = None\n    return deco\n\n\ndef defers(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n\n    @wraps(func)\n    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n        return defer.maybeDeferred(func, *a, **kw)\n\n    return wrapped\n\n\ndef inthread(func: Callable[_P, _T]) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Decorator to call a function in a thread and return a deferred with the\n    result\n    \"\"\"\n\n    @wraps(func)\n    def wrapped(*a: _P.args, **kw: _P.kwargs) -> Deferred[_T]:\n        return threads.deferToThread(func, *a, **kw)\n\n    return wrapped\n", "scrapy/utils/benchserver.py": "import random\nfrom typing import Any\nfrom urllib.parse import urlencode\n\nfrom twisted.web.resource import Resource\nfrom twisted.web.server import Request, Site\n\n\nclass Root(Resource):\n    isLeaf = True\n\n    def getChild(self, name: str, request: Request) -> Resource:\n        return self\n\n    def render(self, request: Request) -> bytes:\n        total = _getarg(request, b\"total\", 100, int)\n        show = _getarg(request, b\"show\", 10, int)\n        nlist = [random.randint(1, total) for _ in range(show)]  # nosec\n        request.write(b\"<html><head></head><body>\")\n        assert request.args is not None\n        args = request.args.copy()\n        for nl in nlist:\n            args[\"n\"] = nl\n            argstr = urlencode(args, doseq=True)\n            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\".encode())\n        request.write(b\"</body></html>\")\n        return b\"\"\n\n\ndef _getarg(request, name: bytes, default: Any = None, type=str):\n    return type(request.args[name][0]) if name in request.args else default\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor\n\n    root = Root()\n    factory = Site(root)\n    httpPort = reactor.listenTCP(8998, Site(root))\n\n    def _print_listening() -> None:\n        httpHost = httpPort.getHost()\n        print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n\n    reactor.callWhenRunning(_print_listening)\n    reactor.run()\n", "scrapy/utils/url.py": "\"\"\"\nThis module contains general purpose URL functions not found in the standard\nlibrary.\n\nSome of the functions that used to be imported from this module have been moved\nto the w3lib.url module. Always import those from there instead.\n\"\"\"\n\nimport re\nfrom typing import TYPE_CHECKING, Iterable, Optional, Type, Union, cast\nfrom urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n\n# scrapy.utils.url was moved to w3lib.url and import * ensures this\n# move doesn't break old code\nfrom w3lib.url import *\nfrom w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from scrapy import Spider\n\n\nUrlT = Union[str, bytes, ParseResult]\n\n\ndef url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith(f\".{d}\")) for d in domains)\n\n\ndef url_is_from_spider(url: UrlT, spider: Type[\"Spider\"]) -> bool:\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(\n        url, [spider.name] + list(getattr(spider, \"allowed_domains\", []))\n    )\n\n\ndef url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n    \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n    lowercase_path = parse_url(url).path.lower()\n    return any(lowercase_path.endswith(ext) for ext in extensions)\n\n\ndef parse_url(url: UrlT, encoding: Optional[str] = None) -> ParseResult:\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return cast(ParseResult, urlparse(to_unicode(url, encoding)))\n\n\ndef escape_ajax(url: str) -> str:\n    \"\"\"\n    Return the crawlable url according to:\n    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith(\"!\"):\n        return url\n    return add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n\n\ndef add_http_if_no_scheme(url: str) -> str:\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.I)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url\n\n\ndef _is_posix_path(string: str) -> bool:\n    return bool(\n        re.match(\n            r\"\"\"\n            ^                   # start with...\n            (\n                \\.              # ...a single dot,\n                (\n                    \\. | [^/\\.]+  # optionally followed by\n                )?                # either a second dot or some characters\n                |\n                ~   # $HOME\n            )?      # optional match of \".\", \"..\" or \".blabla\"\n            /       # at least one \"/\" for a file path,\n            .       # and something after the \"/\"\n            \"\"\",\n            string,\n            flags=re.VERBOSE,\n        )\n    )\n\n\ndef _is_windows_path(string: str) -> bool:\n    return bool(\n        re.match(\n            r\"\"\"\n            ^\n            (\n                [a-z]:\\\\\n                | \\\\\\\\\n            )\n            \"\"\",\n            string,\n            flags=re.IGNORECASE | re.VERBOSE,\n        )\n    )\n\n\ndef _is_filesystem_path(string: str) -> bool:\n    return _is_posix_path(string) or _is_windows_path(string)\n\n\ndef guess_scheme(url: str) -> str:\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n    http:// otherwise.\"\"\"\n    if _is_filesystem_path(url):\n        return any_to_uri(url)\n    return add_http_if_no_scheme(url)\n\n\ndef strip_url(\n    url: str,\n    strip_credentials: bool = True,\n    strip_default_port: bool = True,\n    origin_only: bool = False,\n    strip_fragment: bool = True,\n) -> str:\n    \"\"\"Strip URL string from some of its components:\n\n    - ``strip_credentials`` removes \"user:password@\"\n    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n      from http:// (resp. https://, ftp://) URLs\n    - ``origin_only`` replaces path component with \"/\", also dropping\n      query and fragment components ; it also strips credentials\n    - ``strip_fragment`` drops any #fragment component\n    \"\"\"\n\n    parsed_url = urlparse(url)\n    netloc = parsed_url.netloc\n    if (strip_credentials or origin_only) and (\n        parsed_url.username or parsed_url.password\n    ):\n        netloc = netloc.split(\"@\")[-1]\n    if strip_default_port and parsed_url.port:\n        if (parsed_url.scheme, parsed_url.port) in (\n            (\"http\", 80),\n            (\"https\", 443),\n            (\"ftp\", 21),\n        ):\n            netloc = netloc.replace(f\":{parsed_url.port}\", \"\")\n    return urlunparse(\n        (\n            parsed_url.scheme,\n            netloc,\n            \"/\" if origin_only else parsed_url.path,\n            \"\" if origin_only else parsed_url.params,\n            \"\" if origin_only else parsed_url.query,\n            \"\" if strip_fragment else parsed_url.fragment,\n        )\n    )\n", "scrapy/utils/curl.py": "import argparse\nimport warnings\nfrom http.cookies import SimpleCookie\nfrom shlex import split\nfrom typing import Any, Dict, List, NoReturn, Optional, Sequence, Tuple, Union\nfrom urllib.parse import urlparse\n\nfrom w3lib.http import basic_auth_header\n\n\nclass DataAction(argparse.Action):\n    def __call__(\n        self,\n        parser: argparse.ArgumentParser,\n        namespace: argparse.Namespace,\n        values: Union[str, Sequence[Any], None],\n        option_string: Optional[str] = None,\n    ) -> None:\n        value = str(values)\n        if value.startswith(\"$\"):\n            value = value[1:]\n        setattr(namespace, self.dest, value)\n\n\nclass CurlParser(argparse.ArgumentParser):\n    def error(self, message: str) -> NoReturn:\n        error_msg = f\"There was an error parsing the curl command: {message}\"\n        raise ValueError(error_msg)\n\n\ncurl_parser = CurlParser()\ncurl_parser.add_argument(\"url\")\ncurl_parser.add_argument(\"-H\", \"--header\", dest=\"headers\", action=\"append\")\ncurl_parser.add_argument(\"-X\", \"--request\", dest=\"method\")\ncurl_parser.add_argument(\"-d\", \"--data\", \"--data-raw\", dest=\"data\", action=DataAction)\ncurl_parser.add_argument(\"-u\", \"--user\", dest=\"auth\")\n\n\nsafe_to_ignore_arguments = [\n    [\"--compressed\"],\n    # `--compressed` argument is not safe to ignore, but it's included here\n    # because the `HttpCompressionMiddleware` is enabled by default\n    [\"-s\", \"--silent\"],\n    [\"-v\", \"--verbose\"],\n    [\"-#\", \"--progress-bar\"],\n]\n\nfor argument in safe_to_ignore_arguments:\n    curl_parser.add_argument(*argument, action=\"store_true\")\n\n\ndef _parse_headers_and_cookies(\n    parsed_args: argparse.Namespace,\n) -> Tuple[List[Tuple[str, bytes]], Dict[str, str]]:\n    headers: List[Tuple[str, bytes]] = []\n    cookies: Dict[str, str] = {}\n    for header in parsed_args.headers or ():\n        name, val = header.split(\":\", 1)\n        name = name.strip()\n        val = val.strip()\n        if name.title() == \"Cookie\":\n            for name, morsel in SimpleCookie(val).items():\n                cookies[name] = morsel.value\n        else:\n            headers.append((name, val))\n\n    if parsed_args.auth:\n        user, password = parsed_args.auth.split(\":\", 1)\n        headers.append((\"Authorization\", basic_auth_header(user, password)))\n\n    return headers, cookies\n\n\ndef curl_to_request_kwargs(\n    curl_command: str, ignore_unknown_options: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Convert a cURL command syntax to Request kwargs.\n\n    :param str curl_command: string containing the curl command\n    :param bool ignore_unknown_options: If true, only a warning is emitted when\n                                        cURL options are unknown. Otherwise\n                                        raises an error. (default: True)\n    :return: dictionary of Request kwargs\n    \"\"\"\n\n    curl_args = split(curl_command)\n\n    if curl_args[0] != \"curl\":\n        raise ValueError('A curl command must start with \"curl\"')\n\n    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n\n    if argv:\n        msg = f'Unrecognized options: {\", \".join(argv)}'\n        if ignore_unknown_options:\n            warnings.warn(msg)\n        else:\n            raise ValueError(msg)\n\n    url = parsed_args.url\n\n    # curl automatically prepends 'http' if the scheme is missing, but Request\n    # needs the scheme to work\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme:\n        url = \"http://\" + url\n\n    method = parsed_args.method or \"GET\"\n\n    result: Dict[str, Any] = {\"method\": method.upper(), \"url\": url}\n\n    headers, cookies = _parse_headers_and_cookies(parsed_args)\n\n    if headers:\n        result[\"headers\"] = headers\n    if cookies:\n        result[\"cookies\"] = cookies\n    if parsed_args.data:\n        result[\"body\"] = parsed_args.data\n        if not parsed_args.method:\n            # if the \"data\" is specified but the \"method\" is not specified,\n            # the default method is 'POST'\n            result[\"method\"] = \"POST\"\n\n    return result\n", "scrapy/utils/console.py": "from functools import wraps\nfrom typing import Any, Callable, Dict, Iterable, Optional\n\nEmbedFuncT = Callable[..., None]\nKnownShellsT = Dict[str, Callable[..., EmbedFuncT]]\n\n\ndef _embed_ipython_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start an IPython Shell\"\"\"\n    try:\n        from IPython.terminal.embed import InteractiveShellEmbed  # noqa: T100\n        from IPython.terminal.ipapp import load_default_config\n    except ImportError:\n        from IPython.frontend.terminal.embed import (  # type: ignore[no-redef]  # noqa: T100\n            InteractiveShellEmbed,\n        )\n        from IPython.frontend.terminal.ipapp import (  # type: ignore[no-redef]\n            load_default_config,\n        )\n\n    @wraps(_embed_ipython_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        config = load_default_config()\n        # Always use .instance() to ensure _instance propagation to all parents\n        # this is needed for <TAB> completion works well for new imports\n        # and clear the instance to always have the fresh env\n        # on repeated breaks like with inspect_response()\n        InteractiveShellEmbed.clear_instance()\n        shell = InteractiveShellEmbed.instance(\n            banner1=banner, user_ns=namespace, config=config\n        )\n        shell()\n\n    return wrapper\n\n\ndef _embed_bpython_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a bpython shell\"\"\"\n    import bpython\n\n    @wraps(_embed_bpython_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        bpython.embed(locals_=namespace, banner=banner)\n\n    return wrapper\n\n\ndef _embed_ptpython_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a ptpython shell\"\"\"\n    import ptpython.repl\n\n    @wraps(_embed_ptpython_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        print(banner)\n        ptpython.repl.embed(locals=namespace)\n\n    return wrapper\n\n\ndef _embed_standard_shell(\n    namespace: Dict[str, Any] = {}, banner: str = \"\"\n) -> EmbedFuncT:\n    \"\"\"Start a standard python shell\"\"\"\n    import code\n\n    try:  # readline module is only available on unix systems\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter  # noqa: F401\n\n        readline.parse_and_bind(\"tab:complete\")\n\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n        code.interact(banner=banner, local=namespace)\n\n    return wrapper\n\n\nDEFAULT_PYTHON_SHELLS: KnownShellsT = {\n    \"ptpython\": _embed_ptpython_shell,\n    \"ipython\": _embed_ipython_shell,\n    \"bpython\": _embed_bpython_shell,\n    \"python\": _embed_standard_shell,\n}\n\n\ndef get_shell_embed_func(\n    shells: Optional[Iterable[str]] = None, known_shells: Optional[KnownShellsT] = None\n) -> Any:\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None:  # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None:  # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue\n\n\ndef start_python_console(\n    namespace: Optional[Dict[str, Any]] = None,\n    banner: str = \"\",\n    shells: Optional[Iterable[str]] = None,\n) -> None:\n    \"\"\"Start Python console bound to the given namespace.\n    Readline support and tab completion will be used on Unix, if available.\n    \"\"\"\n    if namespace is None:\n        namespace = {}\n\n    try:\n        shell = get_shell_embed_func(shells)\n        if shell is not None:\n            shell(namespace=namespace, banner=banner)\n    except SystemExit:  # raised when using exit() in python code.interact\n        pass\n", "scrapy/utils/__init__.py": "", "scrapy/utils/asyncgen.py": "from typing import AsyncGenerator, AsyncIterable, Iterable, List, TypeVar, Union\n\n_T = TypeVar(\"_T\")\n\n\nasync def collect_asyncgen(result: AsyncIterable[_T]) -> List[_T]:\n    results = []\n    async for x in result:\n        results.append(x)\n    return results\n\n\nasync def as_async_generator(\n    it: Union[Iterable[_T], AsyncIterable[_T]]\n) -> AsyncGenerator[_T, None]:\n    \"\"\"Wraps an iterable (sync or async) into an async generator.\"\"\"\n    if isinstance(it, AsyncIterable):\n        async for r in it:\n            yield r\n    else:\n        for r in it:\n            yield r\n", "scrapy/utils/project.py": "import os\nimport warnings\nfrom importlib import import_module\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Union\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n\nENVVAR = \"SCRAPY_SETTINGS_MODULE\"\nDATADIR_CFG_SECTION = \"datadir\"\n\n\ndef inside_project() -> bool:\n    scrapy_module = os.environ.get(ENVVAR)\n    if scrapy_module:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(\n                f\"Cannot import scrapy settings module {scrapy_module}: {exc}\"\n            )\n        else:\n            return True\n    return bool(closest_scrapy_cfg())\n\n\ndef project_data_dir(project: str = \"default\") -> str:\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = Path(cfg.get(DATADIR_CFG_SECTION, project))\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\n                \"Unable to find scrapy.cfg file to infer project data dir\"\n            )\n        d = (Path(scrapy_cfg).parent / \".scrapy\").resolve()\n    if not d.exists():\n        d.mkdir(parents=True)\n    return str(d)\n\n\ndef data_path(path: Union[str, PathLike], createdir: bool = False) -> str:\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    path_obj = Path(path)\n    if not path_obj.is_absolute():\n        if inside_project():\n            path_obj = Path(project_data_dir(), path)\n        else:\n            path_obj = Path(\".scrapy\", path)\n    if createdir and not path_obj.exists():\n        path_obj.mkdir(parents=True)\n    return str(path_obj)\n\n\ndef get_project_settings() -> Settings:\n    if ENVVAR not in os.environ:\n        project = os.environ.get(\"SCRAPY_PROJECT\", \"default\")\n        init_env(project)\n\n    settings = Settings()\n    settings_module_path = os.environ.get(ENVVAR)\n    if settings_module_path:\n        settings.setmodule(settings_module_path, priority=\"project\")\n\n    valid_envvars = {\n        \"CHECK\",\n        \"PROJECT\",\n        \"PYTHON_SHELL\",\n        \"SETTINGS_MODULE\",\n    }\n\n    scrapy_envvars = {\n        k[7:]: v\n        for k, v in os.environ.items()\n        if k.startswith(\"SCRAPY_\") and k.replace(\"SCRAPY_\", \"\") in valid_envvars\n    }\n\n    settings.setdict(scrapy_envvars, priority=\"project\")\n\n    return settings\n", "scrapy/utils/defer.py": "\"\"\"\nHelper functions for dealing with Twisted deferreds\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport warnings\nfrom asyncio import Future\nfrom functools import wraps\nfrom types import CoroutineType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Coroutine,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred, DeferredList, ensureDeferred\nfrom twisted.internet.task import Cooperator\nfrom twisted.python import failure\nfrom twisted.python.failure import Failure\n\nfrom scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\nfrom scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n\nif TYPE_CHECKING:\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_T2 = TypeVar(\"_T2\")\n\n# copied from twisted.internet.defer\n_SelfResultT = TypeVar(\"_SelfResultT\")\n_DeferredListResultItemT = Tuple[bool, _SelfResultT]\nDeferredListResultListT = List[_DeferredListResultItemT[_SelfResultT]]\n\n\ndef defer_fail(_failure: Failure) -> Deferred:\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n\n    d: Deferred = Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d\n\n\ndef defer_succeed(result: _T) -> Deferred[_T]:\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    from twisted.internet import reactor\n\n    d: Deferred = Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d\n\n\ndef defer_result(result: Any) -> Deferred:\n    if isinstance(result, Deferred):\n        return result\n    if isinstance(result, failure.Failure):\n        return defer_fail(result)\n    return defer_succeed(result)\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, Deferred[_T]], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[_T]: ...\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, Coroutine[Deferred[Any], Any, _T]],\n    *args: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[_T]: ...\n\n\n@overload\ndef mustbe_deferred(\n    f: Callable[_P, _T], *args: _P.args, **kw: _P.kwargs\n) -> Deferred[_T]: ...\n\n\ndef mustbe_deferred(\n    f: Callable[_P, Union[Deferred[_T], Coroutine[Deferred[Any], Any, _T], _T]],\n    *args: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred[_T]:\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except Exception:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)\n\n\ndef parallel(\n    iterable: Iterable[_T],\n    count: int,\n    callable: Callable[Concatenate[_T, _P], _T2],\n    *args: _P.args,\n    **named: _P.kwargs,\n) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: https://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = Cooperator()\n    work = (callable(elem, *args, **named) for elem in iterable)\n    return DeferredList([coop.coiterate(work) for _ in range(count)])\n\n\nclass _AsyncCooperatorAdapter(Iterator[Deferred]):\n    \"\"\"A class that wraps an async iterable into a normal iterator suitable\n    for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n    it calls the callable directly in the callback, instead of providing a more\n    generic interface.\n\n    On the outside, this class behaves as an iterator that yields Deferreds.\n    Each Deferred is fired with the result of the callable which was called on\n    the next result from aiterator. It raises StopIteration when aiterator is\n    exhausted, as expected.\n\n    Cooperator calls __next__() multiple times and waits on the Deferreds\n    returned from it. As async generators (since Python 3.8) don't support\n    awaiting on __anext__() several times in parallel, we need to serialize\n    this. It's done by storing the Deferreds returned from __next__() and\n    firing the oldest one when a result from __anext__() is available.\n\n    The workflow:\n    1. When __next__() is called for the first time, it creates a Deferred, stores it\n    in self.waiting_deferreds and returns it. It also makes a Deferred that will wait\n    for self.aiterator.__anext__() and puts it into self.anext_deferred.\n    2. If __next__() is called again before self.anext_deferred fires, more Deferreds\n    are added to self.waiting_deferreds.\n    3. When self.anext_deferred fires, it either calls _callback() or _errback(). Both\n    clear self.anext_deferred.\n    3.1. _callback() calls the callable passing the result value that it takes, pops a\n    Deferred from self.waiting_deferreds, and if the callable result was a Deferred, it\n    chains those Deferreds so that the waiting Deferred will fire when the result\n    Deferred does, otherwise it fires it directly. This causes one awaiting task to\n    receive a result. If self.waiting_deferreds is still not empty, new __anext__() is\n    called and self.anext_deferred is populated.\n    3.2. _errback() checks the exception class. If it's StopAsyncIteration it means\n    self.aiterator is exhausted and so it sets self.finished and fires all\n    self.waiting_deferreds. Other exceptions are propagated.\n    4. If __next__() is called after __anext__() was handled, then if self.finished is\n    True, it raises StopIteration, otherwise it acts like in step 2, but if\n    self.anext_deferred is now empty is also populates it with a new __anext__().\n\n    Note that CooperativeTask ignores the value returned from the Deferred that it waits\n    for, so we fire them with None when needed.\n\n    It may be possible to write an async iterator-aware replacement for\n    Cooperator/CooperativeTask and use it instead of this adapter to achieve the same\n    goal.\n    \"\"\"\n\n    def __init__(\n        self,\n        aiterable: AsyncIterable[_T],\n        callable: Callable[Concatenate[_T, _P], _T2],\n        *callable_args: _P.args,\n        **callable_kwargs: _P.kwargs,\n    ):\n        self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n        self.callable: Callable[Concatenate[_T, _P], _T2] = callable\n        self.callable_args: Tuple[Any, ...] = callable_args\n        self.callable_kwargs: Dict[str, Any] = callable_kwargs\n        self.finished: bool = False\n        self.waiting_deferreds: List[Deferred] = []\n        self.anext_deferred: Optional[Deferred[_T]] = None\n\n    def _callback(self, result: _T) -> None:\n        # This gets called when the result from aiterator.__anext__() is available.\n        # It calls the callable on it and sends the result to the oldest waiting Deferred\n        # (by chaining if the result is a Deferred too or by firing if not).\n        self.anext_deferred = None\n        callable_result = self.callable(\n            result, *self.callable_args, **self.callable_kwargs\n        )\n        d = self.waiting_deferreds.pop(0)\n        if isinstance(callable_result, Deferred):\n            callable_result.chainDeferred(d)\n        else:\n            d.callback(None)\n        if self.waiting_deferreds:\n            self._call_anext()\n\n    def _errback(self, failure: Failure) -> None:\n        # This gets called on any exceptions in aiterator.__anext__().\n        # It handles StopAsyncIteration by stopping the iteration and reraises all others.\n        self.anext_deferred = None\n        failure.trap(StopAsyncIteration)\n        self.finished = True\n        for d in self.waiting_deferreds:\n            d.callback(None)\n\n    def _call_anext(self) -> None:\n        # This starts waiting for the next result from aiterator.\n        # If aiterator is exhausted, _errback will be called.\n        self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n        self.anext_deferred.addCallbacks(self._callback, self._errback)\n\n    def __next__(self) -> Deferred:\n        # This puts a new Deferred into self.waiting_deferreds and returns it.\n        # It also calls __anext__() if needed.\n        if self.finished:\n            raise StopIteration\n        d: Deferred = Deferred()\n        self.waiting_deferreds.append(d)\n        if not self.anext_deferred:\n            self._call_anext()\n        return d\n\n\ndef parallel_async(\n    async_iterable: AsyncIterable[_T],\n    count: int,\n    callable: Callable[Concatenate[_T, _P], _T2],\n    *args: _P.args,\n    **named: _P.kwargs,\n) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n    \"\"\"Like parallel but for async iterators\"\"\"\n    coop = Cooperator()\n    work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n    dl: Deferred = DeferredList([coop.coiterate(work) for _ in range(count)])\n    return dl\n\n\ndef process_chain(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    input: Any,\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d: Deferred = Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d\n\n\ndef process_chain_both(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    errbacks: Iterable[Callable[Concatenate[Failure, _P], Any]],\n    input: Any,\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    warnings.warn(\n        \"process_chain_both() is deprecated and will be removed in a future\"\n        \" Scrapy version.\",\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    d: Deferred = Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallback(cb, *a, **kw)\n        d.addErrback(eb, *a, **kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d\n\n\ndef process_parallel(\n    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n    input: Any,\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Deferred:\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d: Deferred = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n    d.addCallback(lambda r: [x[1] for x in r])\n    d.addErrback(lambda f: f.value.subFailure)\n    return d\n\n\ndef iter_errback(\n    iterable: Iterable[_T],\n    errback: Callable[Concatenate[Failure, _P], Any],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> Iterable[_T]:\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\nasync def aiter_errback(\n    aiterable: AsyncIterable[_T],\n    errback: Callable[Concatenate[Failure, _P], Any],\n    *a: _P.args,\n    **kw: _P.kwargs,\n) -> AsyncIterable[_T]:\n    \"\"\"Wraps an async iterable calling an errback if an error is caught while\n    iterating it. Similar to scrapy.utils.defer.iter_errback()\n    \"\"\"\n    it = aiterable.__aiter__()\n    while True:\n        try:\n            yield await it.__anext__()\n        except StopAsyncIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\n_CT = TypeVar(\"_CT\", bound=Union[Awaitable, CoroutineType, Future])\n\n\n@overload\ndef deferred_from_coro(o: _CT) -> Deferred: ...\n\n\n@overload\ndef deferred_from_coro(o: _T) -> _T: ...\n\n\ndef deferred_from_coro(o: _T) -> Union[Deferred, _T]:\n    \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n    if isinstance(o, Deferred):\n        return o\n    if asyncio.isfuture(o) or inspect.isawaitable(o):\n        if not is_asyncio_reactor_installed():\n            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n            return ensureDeferred(cast(Coroutine[Deferred, Any, Any], o))\n        # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n        event_loop = _get_asyncio_event_loop()\n        return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n    return o\n\n\ndef deferred_f_from_coro_f(\n    coro_f: Callable[_P, Coroutine[Any, Any, _T]]\n) -> Callable[_P, Deferred[_T]]:\n    \"\"\"Converts a coroutine function into a function that returns a Deferred.\n\n    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n    This is useful for callback chains, as callback functions are called with the previous callback result.\n    \"\"\"\n\n    @wraps(coro_f)\n    def f(*coro_args: _P.args, **coro_kwargs: _P.kwargs) -> Any:\n        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n\n    return f\n\n\ndef maybeDeferred_coro(\n    f: Callable[_P, Any], *args: _P.args, **kw: _P.kwargs\n) -> Deferred:\n    \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n    try:\n        result = f(*args, **kw)\n    except:  # noqa: E722,B001\n        return defer.fail(failure.Failure(captureVars=Deferred.debug))\n\n    if isinstance(result, Deferred):\n        return result\n    if asyncio.isfuture(result) or inspect.isawaitable(result):\n        return deferred_from_coro(result)\n    if isinstance(result, failure.Failure):\n        return defer.fail(result)\n    return defer.succeed(result)\n\n\ndef deferred_to_future(d: Deferred) -> Future:\n    \"\"\"\n    .. versionadded:: 2.6.0\n\n    Return an :class:`asyncio.Future` object that wraps *d*.\n\n    When :ref:`using the asyncio reactor <install-asyncio>`, you cannot await\n    on :class:`~twisted.internet.defer.Deferred` objects from :ref:`Scrapy\n    callables defined as coroutines <coroutine-support>`, you can only await on\n    ``Future`` objects. Wrapping ``Deferred`` objects into ``Future`` objects\n    allows you to wait on them::\n\n        class MySpider(Spider):\n            ...\n            async def parse(self, response):\n                additional_request = scrapy.Request('https://example.org/price')\n                deferred = self.crawler.engine.download(additional_request)\n                additional_response = await deferred_to_future(deferred)\n    \"\"\"\n    return d.asFuture(_get_asyncio_event_loop())\n\n\ndef maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n    \"\"\"\n    .. versionadded:: 2.6.0\n\n    Return *d* as an object that can be awaited from a :ref:`Scrapy callable\n    defined as a coroutine <coroutine-support>`.\n\n    What you can await in Scrapy callables defined as coroutines depends on the\n    value of :setting:`TWISTED_REACTOR`:\n\n    -   When not using the asyncio reactor, you can only await on\n        :class:`~twisted.internet.defer.Deferred` objects.\n\n    -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only\n        await on :class:`asyncio.Future` objects.\n\n    If you want to write code that uses ``Deferred`` objects but works with any\n    reactor, use this function on all ``Deferred`` objects::\n\n        class MySpider(Spider):\n            ...\n            async def parse(self, response):\n                additional_request = scrapy.Request('https://example.org/price')\n                deferred = self.crawler.engine.download(additional_request)\n                additional_response = await maybe_deferred_to_future(deferred)\n    \"\"\"\n    if not is_asyncio_reactor_installed():\n        return d\n    return deferred_to_future(d)\n", "scrapy/utils/deprecate.py": "\"\"\"Some helpers for deprecation messages\"\"\"\n\nimport inspect\nimport warnings\nfrom typing import Any, Dict, List, Optional, Tuple, Type, overload\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\n\ndef attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> None:\n    cname = obj.__class__.__name__\n    warnings.warn(\n        f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n        f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n        ScrapyDeprecationWarning,\n        stacklevel=3,\n    )\n\n\ndef create_deprecated_class(\n    name: str,\n    new_class: type,\n    clsdict: Optional[Dict[str, Any]] = None,\n    warn_category: Type[Warning] = ScrapyDeprecationWarning,\n    warn_once: bool = True,\n    old_class_path: Optional[str] = None,\n    new_class_path: Optional[str] = None,\n    subclass_warn_message: str = \"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n    instance_warn_message: str = \"{cls} is deprecated, instantiate {new} instead.\",\n) -> type:\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    # https://github.com/python/mypy/issues/4177\n    class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]\n        deprecated_class: Optional[type] = None\n        warned_on_subclass: bool = False\n\n        def __new__(\n            metacls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]\n        ) -> type:\n            cls = super().__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(\n                    cls=_clspath(cls),\n                    old=_clspath(old, old_class_path),\n                    new=_clspath(new_class, new_class_path),\n                )\n                if warn_once:\n                    msg += \" (warning only on first subclass, there may be others)\"\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super().__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst: Any) -> bool:\n            return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))\n\n        def __subclasscheck__(cls, sub: type) -> bool:\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super().__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, \"__mro__\", ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args: Any, **kwargs: Any) -> Any:\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(\n                    cls=_clspath(cls, old_class_path),\n                    new=_clspath(new_class, new_class_path),\n                )\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super().__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(f\"Error detecting parent module: {e!r}\")\n\n    return deprecated_cls\n\n\ndef _clspath(cls: type, forced: Optional[str] = None) -> str:\n    if forced is not None:\n        return forced\n    return f\"{cls.__module__}.{cls.__name__}\"\n\n\nDEPRECATION_RULES: List[Tuple[str, str]] = []\n\n\n@overload\ndef update_classpath(path: str) -> str: ...\n\n\n@overload\ndef update_classpath(path: Any) -> Any: ...\n\n\ndef update_classpath(path: Any) -> Any:\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if isinstance(path, str) and path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\n                f\"`{path}` class is deprecated, use `{new_path}` instead\",\n                ScrapyDeprecationWarning,\n            )\n            return new_path\n    return path\n\n\ndef method_is_overridden(subclass: type, base_class: type, method_name: str) -> bool:\n    \"\"\"\n    Return True if a method named ``method_name`` of a ``base_class``\n    is overridden in a ``subclass``.\n\n    >>> class Base:\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub1(Base):\n    ...     pass\n    >>> class Sub2(Base):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub3(Sub1):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub4(Sub2):\n    ...     pass\n    >>> method_is_overridden(Sub1, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub2, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub3, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub4, Base, 'foo')\n    True\n    \"\"\"\n    base_method = getattr(base_class, method_name)\n    sub_method = getattr(subclass, method_name)\n    return base_method.__code__ is not sub_method.__code__\n", "scrapy/utils/test.py": "\"\"\"\nThis module contains some assorted functions used in tests\n\"\"\"\n\nimport asyncio\nimport os\nfrom importlib import import_module\nfrom pathlib import Path\nfrom posixpath import split\nfrom typing import Any, Awaitable, Dict, List, Optional, Tuple, Type, TypeVar\nfrom unittest import TestCase, mock\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.trial.unittest import SkipTest\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.boto import is_botocore_available\n\n_T = TypeVar(\"_T\")\n\n\ndef assert_gcs_environ() -> None:\n    if \"GCS_PROJECT_ID\" not in os.environ:\n        raise SkipTest(\"GCS_PROJECT_ID not found\")\n\n\ndef skip_if_no_boto() -> None:\n    if not is_botocore_available():\n        raise SkipTest(\"missing botocore library\")\n\n\ndef get_gcs_content_and_delete(\n    bucket: Any, path: str\n) -> Tuple[bytes, List[Dict[str, str]], Any]:\n    from google.cloud import storage\n\n    client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob\n\n\ndef get_ftp_content_and_delete(\n    path: str,\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n) -> bytes:\n    from ftplib import FTP\n\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data: List[bytes] = []\n\n    def buffer_data(data: bytes) -> None:\n        ftp_data.append(data)\n\n    ftp.retrbinary(f\"RETR {path}\", buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return b\"\".join(ftp_data)\n\n\nclass TestSpider(Spider):\n    name = \"test\"\n\n\ndef get_crawler(\n    spidercls: Optional[Type[Spider]] = None,\n    settings_dict: Optional[Dict[str, Any]] = None,\n    prevent_warnings: bool = True,\n) -> Crawler:\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n\n    # Set by default settings that prevent deprecation warnings.\n    settings: Dict[str, Any] = {}\n    settings.update(settings_dict or {})\n    runner = CrawlerRunner(settings)\n    crawler = runner.create_crawler(spidercls or TestSpider)\n    crawler._apply_settings()\n    return crawler\n\n\ndef get_pythonpath() -> str:\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module(\"scrapy\").__path__[0]\n    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n\n\ndef get_testenv() -> Dict[str, str]:\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = get_pythonpath()\n    return env\n\n\ndef assert_samelines(\n    testcase: TestCase, text1: str, text2: str, msg: Optional[str] = None\n) -> None:\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n\n\ndef get_from_asyncio_queue(value: _T) -> Awaitable[_T]:\n    q: asyncio.Queue[_T] = asyncio.Queue()\n    getter = q.get()\n    q.put_nowait(value)\n    return getter\n\n\ndef mock_google_cloud_storage() -> Tuple[Any, Any, Any]:\n    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n    classes and set their proper return values.\n    \"\"\"\n    from google.cloud.storage import Blob, Bucket, Client\n\n    client_mock = mock.create_autospec(Client)\n\n    bucket_mock = mock.create_autospec(Bucket)\n    client_mock.get_bucket.return_value = bucket_mock\n\n    blob_mock = mock.create_autospec(Blob)\n    bucket_mock.blob.return_value = blob_mock\n\n    return (client_mock, bucket_mock, blob_mock)\n\n\ndef get_web_client_agent_req(url: str) -> Deferred:\n    from twisted.internet import reactor\n    from twisted.web.client import Agent  # imports twisted.internet.reactor\n\n    agent = Agent(reactor)\n    return agent.request(b\"GET\", url.encode(\"utf-8\"))\n", "scrapy/commands/shell.py": "\"\"\"\nScrapy Shell\n\nSee documentation in docs/topics/shell.rst\n\"\"\"\n\nfrom argparse import ArgumentParser, Namespace\nfrom threading import Thread\nfrom typing import Any, Dict, List, Type\n\nfrom scrapy import Spider\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.http import Request\nfrom scrapy.shell import Shell\nfrom scrapy.utils.spider import DefaultSpider, spidercls_for_request\nfrom scrapy.utils.url import guess_scheme\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\n        \"KEEP_ALIVE\": True,\n        \"LOGSTATS_INTERVAL\": 0,\n        \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n    }\n\n    def syntax(self) -> str:\n        return \"[url|file]\"\n\n    def short_desc(self) -> str:\n        return \"Interactive scraping console\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Interactive console for scraping the given url or file. \"\n            \"Use ./file.html syntax or full path for local file.\"\n        )\n\n    def add_options(self, parser: ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-c\",\n            dest=\"code\",\n            help=\"evaluate the code in the shell, print the result and exit\",\n        )\n        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_argument(\n            \"--no-redirect\",\n            dest=\"no_redirect\",\n            action=\"store_true\",\n            default=False,\n            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n        )\n\n    def update_vars(self, vars: Dict[str, Any]) -> None:\n        \"\"\"You can use this function to update the Scrapy objects that will be\n        available in the shell\n        \"\"\"\n        pass\n\n    def run(self, args: List[str], opts: Namespace) -> None:\n        url = args[0] if args else None\n        if url:\n            # first argument may be a local file\n            url = guess_scheme(url)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        spidercls: Type[Spider] = DefaultSpider\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        elif url:\n            spidercls = spidercls_for_request(\n                spider_loader, Request(url), spidercls, log_multiple=True\n            )\n\n        # The crawler is created this way since the Shell manually handles the\n        # crawling engine, so the set up in the crawl method won't work\n        crawler = self.crawler_process._create_crawler(spidercls)\n        crawler._apply_settings()\n        # The Shell class needs a persistent engine in the crawler\n        crawler.engine = crawler._create_engine()\n        crawler.engine.start()\n\n        self._start_crawler_thread()\n\n        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n        shell.start(url=url, redirect=not opts.no_redirect)\n\n    def _start_crawler_thread(self) -> None:\n        assert self.crawler_process\n        t = Thread(\n            target=self.crawler_process.start,\n            kwargs={\"stop_after_crawl\": False, \"install_signal_handlers\": False},\n        )\n        t.daemon = True\n        t.start()\n", "scrapy/commands/fetch.py": "import sys\nfrom argparse import ArgumentParser, Namespace\nfrom typing import Dict, List, Type\n\nfrom w3lib.url import is_url\n\nfrom scrapy import Spider\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.spider import DefaultSpider, spidercls_for_request\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n\n    def syntax(self) -> str:\n        return \"[options] <url>\"\n\n    def short_desc(self) -> str:\n        return \"Fetch a URL using the Scrapy downloader\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Fetch a URL using the Scrapy downloader and print its content\"\n            \" to stdout. You may want to use --nolog to disable logging\"\n        )\n\n    def add_options(self, parser: ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n        parser.add_argument(\n            \"--headers\",\n            dest=\"headers\",\n            action=\"store_true\",\n            help=\"print response HTTP headers instead of body\",\n        )\n        parser.add_argument(\n            \"--no-redirect\",\n            dest=\"no_redirect\",\n            action=\"store_true\",\n            default=False,\n            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n        )\n\n    def _print_headers(self, headers: Dict[bytes, List[bytes]], prefix: bytes) -> None:\n        for key, values in headers.items():\n            for value in values:\n                self._print_bytes(prefix + b\" \" + key + b\": \" + value)\n\n    def _print_response(self, response: Response, opts: Namespace) -> None:\n        if opts.headers:\n            assert response.request\n            self._print_headers(response.request.headers, b\">\")\n            print(\">\")\n            self._print_headers(response.headers, b\"<\")\n        else:\n            self._print_bytes(response.body)\n\n    def _print_bytes(self, bytes_: bytes) -> None:\n        sys.stdout.buffer.write(bytes_ + b\"\\n\")\n\n    def run(self, args: List[str], opts: Namespace) -> None:\n        if len(args) != 1 or not is_url(args[0]):\n            raise UsageError()\n        request = Request(\n            args[0],\n            callback=self._print_response,\n            cb_kwargs={\"opts\": opts},\n            dont_filter=True,\n        )\n        # by default, let the framework handle redirects,\n        # i.e. command handles all codes expect 3xx\n        if not opts.no_redirect:\n            request.meta[\"handle_httpstatus_list\"] = SequenceExclude(range(300, 400))\n        else:\n            request.meta[\"handle_httpstatus_all\"] = True\n\n        spidercls: Type[Spider] = DefaultSpider\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            spidercls = spider_loader.load(opts.spider)\n        else:\n            spidercls = spidercls_for_request(spider_loader, request, spidercls)\n        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n        self.crawler_process.start()\n", "scrapy/commands/view.py": "import argparse\nimport logging\n\nfrom scrapy.commands import fetch\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.utils.response import open_in_browser\n\nlogger = logging.getLogger(__name__)\n\n\nclass Command(fetch.Command):\n    def short_desc(self) -> str:\n        return \"Open URL in browser, as seen by Scrapy\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n        )\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\"--headers\", help=argparse.SUPPRESS)\n\n    def _print_response(self, response: Response, opts: argparse.Namespace) -> None:\n        if not isinstance(response, TextResponse):\n            logger.error(\"Cannot view a non-text response.\")\n            return\n        open_in_browser(response)\n", "scrapy/commands/runspider.py": "import argparse\nimport sys\nfrom importlib import import_module\nfrom os import PathLike\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import List, Union\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.spider import iter_spider_classes\n\n\ndef _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n    abspath = Path(filepath).resolve()\n    if abspath.suffix not in (\".py\", \".pyw\"):\n        raise ValueError(f\"Not a Python source file: {abspath}\")\n    dirname = str(abspath.parent)\n    sys.path = [dirname] + sys.path\n    try:\n        module = import_module(abspath.stem)\n    finally:\n        sys.path.pop(0)\n    return module\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = False\n    default_settings = {\"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"[options] <spider_file>\"\n\n    def short_desc(self) -> str:\n        return \"Run a self-contained spider (without creating a project)\"\n\n    def long_desc(self) -> str:\n        return \"Run the spider defined in the given file\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) != 1:\n            raise UsageError()\n        filename = Path(args[0])\n        if not filename.exists():\n            raise UsageError(f\"File not found: {filename}\\n\")\n        try:\n            module = _import_file(filename)\n        except (ImportError, ValueError) as e:\n            raise UsageError(f\"Unable to load {str(filename)!r}: {e}\\n\")\n        spclasses = list(iter_spider_classes(module))\n        if not spclasses:\n            raise UsageError(f\"No spider found in file: {filename}\\n\")\n        spidercls = spclasses.pop()\n\n        assert self.crawler_process\n        self.crawler_process.crawl(spidercls, **opts.spargs)\n        self.crawler_process.start()\n\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1\n", "scrapy/commands/parse.py": "from __future__ import annotations\n\nimport argparse\nimport functools\nimport inspect\nimport json\nimport logging\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Coroutine,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom itemadapter import ItemAdapter, is_item\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom twisted.python.failure import Failure\nfrom w3lib.url import is_url\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils import display\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.defer import aiter_errback, deferred_from_coro\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.spider import spidercls_for_request\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = True\n\n    spider = None\n    items: Dict[int, List[Any]] = {}\n    requests: Dict[int, List[Request]] = {}\n\n    first_response = None\n\n    def syntax(self) -> str:\n        return \"[options] <url>\"\n\n    def short_desc(self) -> str:\n        return \"Parse URL (using its spider) and print the results\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--spider\",\n            dest=\"spider\",\n            default=None,\n            help=\"use this spider without looking for one\",\n        )\n        parser.add_argument(\n            \"--pipelines\", action=\"store_true\", help=\"process items through pipelines\"\n        )\n        parser.add_argument(\n            \"--nolinks\",\n            dest=\"nolinks\",\n            action=\"store_true\",\n            help=\"don't show links to follow (extracted requests)\",\n        )\n        parser.add_argument(\n            \"--noitems\",\n            dest=\"noitems\",\n            action=\"store_true\",\n            help=\"don't show scraped items\",\n        )\n        parser.add_argument(\n            \"--nocolour\",\n            dest=\"nocolour\",\n            action=\"store_true\",\n            help=\"avoid using pygments to colorize the output\",\n        )\n        parser.add_argument(\n            \"-r\",\n            \"--rules\",\n            dest=\"rules\",\n            action=\"store_true\",\n            help=\"use CrawlSpider rules to discover the callback\",\n        )\n        parser.add_argument(\n            \"-c\",\n            \"--callback\",\n            dest=\"callback\",\n            help=\"use this callback for parsing, instead looking for a callback\",\n        )\n        parser.add_argument(\n            \"-m\",\n            \"--meta\",\n            dest=\"meta\",\n            help=\"inject extra meta into the Request, it must be a valid raw json string\",\n        )\n        parser.add_argument(\n            \"--cbkwargs\",\n            dest=\"cbkwargs\",\n            help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\",\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--depth\",\n            dest=\"depth\",\n            type=int,\n            default=1,\n            help=\"maximum depth for parsing requests [default: %(default)s]\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            action=\"store_true\",\n            help=\"print each depth level one by one\",\n        )\n\n    @property\n    def max_level(self) -> int:\n        max_items, max_requests = 0, 0\n        if self.items:\n            max_items = max(self.items)\n        if self.requests:\n            max_requests = max(self.requests)\n        return max(max_items, max_requests)\n\n    def handle_exception(self, _failure: Failure) -> None:\n        logger.error(\n            \"An error is caught while iterating the async iterable\",\n            exc_info=failure_to_exc_info(_failure),\n        )\n\n    @overload\n    def iterate_spider_output(\n        self, result: Union[AsyncGenerator[_T, None], Coroutine[Any, Any, _T]]\n    ) -> Deferred[_T]: ...\n\n    @overload\n    def iterate_spider_output(self, result: _T) -> Iterable[Any]: ...\n\n    def iterate_spider_output(self, result: Any) -> Union[Iterable[Any], Deferred]:\n        if inspect.isasyncgen(result):\n            d = deferred_from_coro(\n                collect_asyncgen(aiter_errback(result, self.handle_exception))\n            )\n            d.addCallback(self.iterate_spider_output)\n            return d\n        if inspect.iscoroutine(result):\n            d = deferred_from_coro(result)\n            d.addCallback(self.iterate_spider_output)\n            return d\n        return arg_to_iter(deferred_from_coro(result))\n\n    def add_items(self, lvl: int, new_items: List[Any]) -> None:\n        old_items = self.items.get(lvl, [])\n        self.items[lvl] = old_items + new_items\n\n    def add_requests(self, lvl: int, new_reqs: List[Request]) -> None:\n        old_reqs = self.requests.get(lvl, [])\n        self.requests[lvl] = old_reqs + new_reqs\n\n    def print_items(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n        if lvl is None:\n            items = [item for lst in self.items.values() for item in lst]\n        else:\n            items = self.items.get(lvl, [])\n\n        print(\"# Scraped Items \", \"-\" * 60)\n        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n\n    def print_requests(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n        if lvl is None:\n            if self.requests:\n                requests = self.requests[max(self.requests)]\n            else:\n                requests = []\n        else:\n            requests = self.requests.get(lvl, [])\n\n        print(\"# Requests \", \"-\" * 65)\n        display.pprint(requests, colorize=colour)\n\n    def print_results(self, opts: argparse.Namespace) -> None:\n        colour = not opts.nocolour\n\n        if opts.verbose:\n            for level in range(1, self.max_level + 1):\n                print(f\"\\n>>> DEPTH LEVEL: {level} <<<\")\n                if not opts.noitems:\n                    self.print_items(level, colour)\n                if not opts.nolinks:\n                    self.print_requests(level, colour)\n        else:\n            print(f\"\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<\")\n            if not opts.noitems:\n                self.print_items(colour=colour)\n            if not opts.nolinks:\n                self.print_requests(colour=colour)\n\n    def _get_items_and_requests(\n        self,\n        spider_output: Iterable[Any],\n        opts: argparse.Namespace,\n        depth: int,\n        spider: Spider,\n        callback: Callable,\n    ) -> Tuple[List[Any], List[Request], argparse.Namespace, int, Spider, Callable]:\n        items, requests = [], []\n        for x in spider_output:\n            if is_item(x):\n                items.append(x)\n            elif isinstance(x, Request):\n                requests.append(x)\n        return items, requests, opts, depth, spider, callback\n\n    def run_callback(\n        self,\n        response: Response,\n        callback: Callable,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Deferred:\n        cb_kwargs = cb_kwargs or {}\n        d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n        return d\n\n    def get_callback_from_rules(\n        self, spider: Spider, response: Response\n    ) -> Union[Callable, str, None]:\n        if getattr(spider, \"rules\", None):\n            for rule in spider.rules:  # type: ignore[attr-defined]\n                if rule.link_extractor.matches(response.url):\n                    return rule.callback or \"parse\"\n        else:\n            logger.error(\n                \"No CrawlSpider rules found in spider %(spider)r, \"\n                \"please specify a callback to use for parsing\",\n                {\"spider\": spider.name},\n            )\n        return None\n\n    def set_spidercls(self, url: str, opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n        if opts.spider:\n            try:\n                self.spidercls = spider_loader.load(opts.spider)\n            except KeyError:\n                logger.error(\n                    \"Unable to find spider: %(spider)s\", {\"spider\": opts.spider}\n                )\n        else:\n            self.spidercls = spidercls_for_request(spider_loader, Request(url))\n            if not self.spidercls:\n                logger.error(\"Unable to find spider for: %(url)s\", {\"url\": url})\n\n        def _start_requests(spider: Spider) -> Iterable[Request]:\n            yield self.prepare_request(spider, Request(url), opts)\n\n        if self.spidercls:\n            self.spidercls.start_requests = _start_requests\n\n    def start_parsing(self, url: str, opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        self.crawler_process.crawl(self.spidercls, **opts.spargs)\n        self.pcrawler = list(self.crawler_process.crawlers)[0]\n        self.crawler_process.start()\n\n        if not self.first_response:\n            logger.error(\"No response downloaded for: %(url)s\", {\"url\": url})\n\n    def scraped_data(\n        self,\n        args: Tuple[\n            List[Any], List[Request], argparse.Namespace, int, Spider, Callable\n        ],\n    ) -> List[Any]:\n        items, requests, opts, depth, spider, callback = args\n        if opts.pipelines:\n            itemproc = self.pcrawler.engine.scraper.itemproc\n            for item in items:\n                itemproc.process_item(item, spider)\n        self.add_items(depth, items)\n        self.add_requests(depth, requests)\n\n        scraped_data = items if opts.output else []\n        if depth < opts.depth:\n            for req in requests:\n                req.meta[\"_depth\"] = depth + 1\n                req.meta[\"_callback\"] = req.callback\n                req.callback = callback\n            scraped_data += requests\n\n        return scraped_data\n\n    def _get_callback(\n        self,\n        *,\n        spider: Spider,\n        opts: argparse.Namespace,\n        response: Optional[Response] = None,\n    ) -> Callable:\n        cb: Union[str, Callable, None] = None\n        if response:\n            cb = response.meta[\"_callback\"]\n        if not cb:\n            if opts.callback:\n                cb = opts.callback\n            elif response and opts.rules and self.first_response == response:\n                cb = self.get_callback_from_rules(spider, response)\n                if not cb:\n                    raise ValueError(\n                        f\"Cannot find a rule that matches {response.url!r} in spider: \"\n                        f\"{spider.name}\"\n                    )\n            else:\n                cb = \"parse\"\n\n        if not callable(cb):\n            assert cb is not None\n            cb_method = getattr(spider, cb, None)\n            if callable(cb_method):\n                cb = cb_method\n            else:\n                raise ValueError(\n                    f\"Cannot find callback {cb!r} in spider: {spider.name}\"\n                )\n        assert callable(cb)\n        return cb\n\n    def prepare_request(\n        self, spider: Spider, request: Request, opts: argparse.Namespace\n    ) -> Request:\n        def callback(response: Response, **cb_kwargs: Any) -> Deferred:\n            # memorize first request\n            if not self.first_response:\n                self.first_response = response\n\n            cb = self._get_callback(spider=spider, opts=opts, response=response)\n\n            # parse items and requests\n            depth: int = response.meta[\"_depth\"]\n\n            d = self.run_callback(response, cb, cb_kwargs)\n            d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)\n            d.addCallback(self.scraped_data)\n            return d\n\n        # update request meta if any extra meta was passed through the --meta/-m opts.\n        if opts.meta:\n            request.meta.update(opts.meta)\n\n        # update cb_kwargs if any extra values were was passed through the --cbkwargs option.\n        if opts.cbkwargs:\n            request.cb_kwargs.update(opts.cbkwargs)\n\n        request.meta[\"_depth\"] = 1\n        request.meta[\"_callback\"] = request.callback\n        if not request.callback and not opts.rules:\n            cb = self._get_callback(spider=spider, opts=opts)\n            functools.update_wrapper(callback, cb)\n        request.callback = callback\n        return request\n\n    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n\n        self.process_request_meta(opts)\n        self.process_request_cb_kwargs(opts)\n\n    def process_request_meta(self, opts: argparse.Namespace) -> None:\n        if opts.meta:\n            try:\n                opts.meta = json.loads(opts.meta)\n            except ValueError:\n                raise UsageError(\n                    \"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n                    'Example: --meta=\\'{\"foo\" : \"bar\"}\\'',\n                    print_help=False,\n                )\n\n    def process_request_cb_kwargs(self, opts: argparse.Namespace) -> None:\n        if opts.cbkwargs:\n            try:\n                opts.cbkwargs = json.loads(opts.cbkwargs)\n            except ValueError:\n                raise UsageError(\n                    \"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n                    'Example: --cbkwargs=\\'{\"foo\" : \"bar\"}\\'',\n                    print_help=False,\n                )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        # parse arguments\n        if not len(args) == 1 or not is_url(args[0]):\n            raise UsageError()\n        else:\n            url = args[0]\n\n        # prepare spidercls\n        self.set_spidercls(url, opts)\n\n        if self.spidercls and opts.depth > 0:\n            self.start_parsing(url, opts)\n            self.print_results(opts)\n", "scrapy/commands/settings.py": "import argparse\nimport json\nfrom typing import List\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.settings import BaseSettings\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"[options]\"\n\n    def short_desc(self) -> str:\n        return \"Get settings values\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--get\", dest=\"get\", metavar=\"SETTING\", help=\"print raw setting value\"\n        )\n        parser.add_argument(\n            \"--getbool\",\n            dest=\"getbool\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a boolean\",\n        )\n        parser.add_argument(\n            \"--getint\",\n            dest=\"getint\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as an integer\",\n        )\n        parser.add_argument(\n            \"--getfloat\",\n            dest=\"getfloat\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a float\",\n        )\n        parser.add_argument(\n            \"--getlist\",\n            dest=\"getlist\",\n            metavar=\"SETTING\",\n            help=\"print setting value, interpreted as a list\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        settings = self.crawler_process.settings\n        if opts.get:\n            s = settings.get(opts.get)\n            if isinstance(s, BaseSettings):\n                print(json.dumps(s.copy_to_dict()))\n            else:\n                print(s)\n        elif opts.getbool:\n            print(settings.getbool(opts.getbool))\n        elif opts.getint:\n            print(settings.getint(opts.getint))\n        elif opts.getfloat:\n            print(settings.getfloat(opts.getfloat))\n        elif opts.getlist:\n            print(settings.getlist(opts.getlist))\n", "scrapy/commands/genspider.py": "import argparse\nimport os\nimport shutil\nimport string\nfrom importlib import import_module\nfrom pathlib import Path\nfrom typing import List, Optional, Union, cast\nfrom urllib.parse import urlparse\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.template import render_templatefile, string_camelcase\n\n\ndef sanitize_module_name(module_name: str) -> str:\n    \"\"\"Sanitize the given module name, by replacing dashes and points\n    with underscores and prefixing it with a letter if it doesn't start\n    with one\n    \"\"\"\n    module_name = module_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n    if module_name[0] not in string.ascii_letters:\n        module_name = \"a\" + module_name\n    return module_name\n\n\ndef extract_domain(url: str) -> str:\n    \"\"\"Extract domain name from URL string\"\"\"\n    o = urlparse(url)\n    if o.scheme == \"\" and o.netloc == \"\":\n        o = urlparse(\"//\" + url.lstrip(\"/\"))\n    return o.netloc\n\n\ndef verify_url_scheme(url: str) -> str:\n    \"\"\"Check url for scheme and insert https if none found.\"\"\"\n    parsed = urlparse(url)\n    if parsed.scheme == \"\" and parsed.netloc == \"\":\n        parsed = urlparse(\"//\" + url)._replace(scheme=\"https\")\n    return parsed.geturl()\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <name> <domain>\"\n\n    def short_desc(self) -> str:\n        return \"Generate new spider using pre-defined templates\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"List available templates\",\n        )\n        parser.add_argument(\n            \"-e\",\n            \"--edit\",\n            dest=\"edit\",\n            action=\"store_true\",\n            help=\"Edit spider after creating it\",\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--dump\",\n            dest=\"dump\",\n            metavar=\"TEMPLATE\",\n            help=\"Dump template to standard output\",\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--template\",\n            dest=\"template\",\n            default=\"basic\",\n            help=\"Uses a custom template.\",\n        )\n        parser.add_argument(\n            \"--force\",\n            dest=\"force\",\n            action=\"store_true\",\n            help=\"If the spider already exists, overwrite it with the template\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if opts.list:\n            self._list_templates()\n            return\n        if opts.dump:\n            template_file = self._find_template(opts.dump)\n            if template_file:\n                print(template_file.read_text(encoding=\"utf-8\"))\n            return\n        if len(args) != 2:\n            raise UsageError()\n\n        name, url = args[0:2]\n        url = verify_url_scheme(url)\n        module = sanitize_module_name(name)\n\n        if self.settings.get(\"BOT_NAME\") == module:\n            print(\"Cannot create a spider with the same name as your project\")\n            return\n\n        if not opts.force and self._spider_exists(name):\n            return\n\n        template_file = self._find_template(opts.template)\n        if template_file:\n            self._genspider(module, name, url, opts.template, template_file)\n            if opts.edit:\n                self.exitcode = os.system(f'scrapy edit \"{name}\"')  # nosec\n\n    def _genspider(\n        self,\n        module: str,\n        name: str,\n        url: str,\n        template_name: str,\n        template_file: Union[str, os.PathLike],\n    ) -> None:\n        \"\"\"Generate the spider module, based on the given template\"\"\"\n        capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n        domain = extract_domain(url)\n        tvars = {\n            \"project_name\": self.settings.get(\"BOT_NAME\"),\n            \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n            \"module\": module,\n            \"name\": name,\n            \"url\": url,\n            \"domain\": domain,\n            \"classname\": f\"{capitalized_module}Spider\",\n        }\n        if self.settings.get(\"NEWSPIDER_MODULE\"):\n            spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n            assert spiders_module.__file__\n            spiders_dir = Path(spiders_module.__file__).parent.resolve()\n        else:\n            spiders_module = None\n            spiders_dir = Path(\".\")\n        spider_file = f\"{spiders_dir / module}.py\"\n        shutil.copyfile(template_file, spider_file)\n        render_templatefile(spider_file, **tvars)\n        print(\n            f\"Created spider {name!r} using template {template_name!r} \",\n            end=(\"\" if spiders_module else \"\\n\"),\n        )\n        if spiders_module:\n            print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n\n    def _find_template(self, template: str) -> Optional[Path]:\n        template_file = Path(self.templates_dir, f\"{template}.tmpl\")\n        if template_file.exists():\n            return template_file\n        print(f\"Unable to find template: {template}\\n\")\n        print('Use \"scrapy genspider --list\" to see all available templates.')\n        return None\n\n    def _list_templates(self) -> None:\n        print(\"Available templates:\")\n        for file in sorted(Path(self.templates_dir).iterdir()):\n            if file.suffix == \".tmpl\":\n                print(f\"  {file.stem}\")\n\n    def _spider_exists(self, name: str) -> bool:\n        if not self.settings.get(\"NEWSPIDER_MODULE\"):\n            # if run as a standalone command and file with same filename already exists\n            path = Path(name + \".py\")\n            if path.exists():\n                print(f\"{path.resolve()} already exists\")\n                return True\n            return False\n\n        assert (\n            self.crawler_process is not None\n        ), \"crawler_process must be set before calling run\"\n\n        try:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n        else:\n            # if spider with same name exists\n            print(f\"Spider {name!r} already exists in module:\")\n            print(f\"  {spidercls.__module__}\")\n            return True\n\n        # a file with the same name exists in the target directory\n        spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n        spiders_dir = Path(cast(str, spiders_module.__file__)).parent\n        spiders_dir_abs = spiders_dir.resolve()\n        path = spiders_dir_abs / (name + \".py\")\n        if path.exists():\n            print(f\"{path} already exists\")\n            return True\n\n        return False\n\n    @property\n    def templates_dir(self) -> str:\n        return str(\n            Path(\n                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n                \"spiders\",\n            )\n        )\n", "scrapy/commands/version.py": "import argparse\nfrom typing import List\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.utils.versions import scrapy_components_versions\n\n\nclass Command(ScrapyCommand):\n    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"[-v]\"\n\n    def short_desc(self) -> str:\n        return \"Print Scrapy version\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"--verbose\",\n            \"-v\",\n            dest=\"verbose\",\n            action=\"store_true\",\n            help=\"also display twisted/python/platform info (useful for bug reports)\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if opts.verbose:\n            versions = scrapy_components_versions()\n            width = max(len(n) for (n, _) in versions)\n            for name, version in versions:\n                print(f\"{name:<{width}} : {version}\")\n        else:\n            print(f\"Scrapy {scrapy.__version__}\")\n", "scrapy/commands/crawl.py": "import argparse\nfrom typing import List, cast\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy.commands import BaseRunSpiderCommand\nfrom scrapy.exceptions import UsageError\n\n\nclass Command(BaseRunSpiderCommand):\n    requires_project = True\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Run a spider\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) < 1:\n            raise UsageError()\n        elif len(args) > 1:\n            raise UsageError(\n                \"running 'scrapy crawl' with more than one spider is not supported\"\n            )\n        spname = args[0]\n\n        assert self.crawler_process\n        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n        if getattr(crawl_defer, \"result\", None) is not None and issubclass(\n            cast(Failure, crawl_defer.result).type, Exception\n        ):\n            self.exitcode = 1\n        else:\n            self.crawler_process.start()\n\n            if (\n                self.crawler_process.bootstrap_failed\n                or hasattr(self.crawler_process, \"has_exception\")\n                and self.crawler_process.has_exception\n            ):\n                self.exitcode = 1\n", "scrapy/commands/bench.py": "import argparse\nimport subprocess  # nosec\nimport sys\nimport time\nfrom typing import Any, Iterable, List\nfrom urllib.parse import urlencode\n\nimport scrapy\nfrom scrapy import Request\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.linkextractors import LinkExtractor\n\n\nclass Command(ScrapyCommand):\n    default_settings = {\n        \"LOG_LEVEL\": \"INFO\",\n        \"LOGSTATS_INTERVAL\": 1,\n        \"CLOSESPIDER_TIMEOUT\": 10,\n    }\n\n    def short_desc(self) -> str:\n        return \"Run quick benchmark test\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        with _BenchServer():\n            assert self.crawler_process\n            self.crawler_process.crawl(_BenchSpider, total=100000)\n            self.crawler_process.start()\n\n\nclass _BenchServer:\n    def __enter__(self) -> None:\n        from scrapy.utils.test import get_testenv\n\n        pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n        self.proc = subprocess.Popen(\n            pargs, stdout=subprocess.PIPE, env=get_testenv()\n        )  # nosec\n        assert self.proc.stdout\n        self.proc.stdout.readline()\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:\n        self.proc.kill()\n        self.proc.wait()\n        time.sleep(0.2)\n\n\nclass _BenchSpider(scrapy.Spider):\n    \"\"\"A spider that follows all links\"\"\"\n\n    name = \"follow\"\n    total = 10000\n    show = 20\n    baseurl = \"http://localhost:8998\"\n    link_extractor = LinkExtractor()\n\n    def start_requests(self) -> Iterable[Request]:\n        qargs = {\"total\": self.total, \"show\": self.show}\n        url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n        return [scrapy.Request(url, dont_filter=True)]\n\n    def parse(self, response: Response) -> Any:\n        assert isinstance(Response, TextResponse)\n        for link in self.link_extractor.extract_links(response):\n            yield scrapy.Request(link.url, callback=self.parse)\n", "scrapy/commands/startproject.py": "import argparse\nimport os\nimport re\nimport string\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom shutil import copy2, copystat, ignore_patterns, move\nfrom stat import S_IWUSR as OWNER_WRITE_PERMISSION\nfrom typing import List, Tuple, Union\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.template import render_templatefile, string_camelcase\n\nTEMPLATES_TO_RENDER: Tuple[Tuple[str, ...], ...] = (\n    (\"scrapy.cfg\",),\n    (\"${project_name}\", \"settings.py.tmpl\"),\n    (\"${project_name}\", \"items.py.tmpl\"),\n    (\"${project_name}\", \"pipelines.py.tmpl\"),\n    (\"${project_name}\", \"middlewares.py.tmpl\"),\n)\n\nIGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n\n\ndef _make_writable(path: Union[str, os.PathLike]) -> None:\n    current_permissions = os.stat(path).st_mode\n    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)\n\n\nclass Command(ScrapyCommand):\n    requires_project = False\n    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n\n    def syntax(self) -> str:\n        return \"<project_name> [project_dir]\"\n\n    def short_desc(self) -> str:\n        return \"Create new project\"\n\n    def _is_valid_name(self, project_name: str) -> bool:\n        def _module_exists(module_name: str) -> bool:\n            spec = find_spec(module_name)\n            return spec is not None and spec.loader is not None\n\n        if not re.search(r\"^[_a-zA-Z]\\w*$\", project_name):\n            print(\n                \"Error: Project names must begin with a letter and contain\"\n                \" only\\nletters, numbers and underscores\"\n            )\n        elif _module_exists(project_name):\n            print(f\"Error: Module {project_name!r} already exists\")\n        else:\n            return True\n        return False\n\n    def _copytree(self, src: Path, dst: Path) -> None:\n        \"\"\"\n        Since the original function always creates the directory, to resolve\n        the issue a new function had to be created. It's a simple copy and\n        was reduced for this case.\n\n        More info at:\n        https://github.com/scrapy/scrapy/pull/2005\n        \"\"\"\n        ignore = IGNORE\n        names = [x.name for x in src.iterdir()]\n        ignored_names = ignore(src, names)\n\n        if not dst.exists():\n            dst.mkdir(parents=True)\n\n        for name in names:\n            if name in ignored_names:\n                continue\n\n            srcname = src / name\n            dstname = dst / name\n            if srcname.is_dir():\n                self._copytree(srcname, dstname)\n            else:\n                copy2(srcname, dstname)\n                _make_writable(dstname)\n\n        copystat(src, dst)\n        _make_writable(dst)\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) not in (1, 2):\n            raise UsageError()\n\n        project_name = args[0]\n\n        if len(args) == 2:\n            project_dir = Path(args[1])\n        else:\n            project_dir = Path(args[0])\n\n        if (project_dir / \"scrapy.cfg\").exists():\n            self.exitcode = 1\n            print(f\"Error: scrapy.cfg already exists in {project_dir.resolve()}\")\n            return\n\n        if not self._is_valid_name(project_name):\n            self.exitcode = 1\n            return\n\n        self._copytree(Path(self.templates_dir), project_dir.resolve())\n        # On 3.8 shutil.move doesn't fully support Path args, but it supports our use case\n        # See https://bugs.python.org/issue32689\n        move(project_dir / \"module\", project_dir / project_name)  # type: ignore[arg-type]\n        for paths in TEMPLATES_TO_RENDER:\n            tplfile = Path(\n                project_dir,\n                *(\n                    string.Template(s).substitute(project_name=project_name)\n                    for s in paths\n                ),\n            )\n            render_templatefile(\n                tplfile,\n                project_name=project_name,\n                ProjectName=string_camelcase(project_name),\n            )\n        print(\n            f\"New Scrapy project '{project_name}', using template directory \"\n            f\"'{self.templates_dir}', created in:\"\n        )\n        print(f\"    {project_dir.resolve()}\\n\")\n        print(\"You can start your first spider with:\")\n        print(f\"    cd {project_dir}\")\n        print(\"    scrapy genspider example example.com\")\n\n    @property\n    def templates_dir(self) -> str:\n        return str(\n            Path(\n                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n                \"project\",\n            )\n        )\n", "scrapy/commands/__init__.py": "\"\"\"\nBase class for Scrapy commands\n\"\"\"\n\nimport argparse\nimport builtins\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, List, Optional\n\nfrom twisted.python import failure\n\nfrom scrapy.crawler import Crawler, CrawlerProcess\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n\n\nclass ScrapyCommand:\n    requires_project: bool = False\n    crawler_process: Optional[CrawlerProcess] = None\n\n    # default settings to be used for this command instead of global defaults\n    default_settings: Dict[str, Any] = {}\n\n    exitcode: int = 0\n\n    def __init__(self) -> None:\n        self.settings: Any = None  # set in scrapy.cmdline\n\n    def set_crawler(self, crawler: Crawler) -> None:\n        if hasattr(self, \"_crawler\"):\n            raise RuntimeError(\"crawler already set\")\n        self._crawler: Crawler = crawler\n\n    def syntax(self) -> str:\n        \"\"\"\n        Command syntax (preferably one-line). Do not include command name.\n        \"\"\"\n        return \"\"\n\n    def short_desc(self) -> str:\n        \"\"\"\n        A short description of the command\n        \"\"\"\n        return \"\"\n\n    def long_desc(self) -> str:\n        \"\"\"A long description of the command. Return short description when not\n        available. It cannot contain newlines since contents will be formatted\n        by optparser which removes newlines and wraps text.\n        \"\"\"\n        return self.short_desc()\n\n    def help(self) -> str:\n        \"\"\"An extensive help for the command. It will be shown when using the\n        \"help\" command. It can contain newlines since no post-formatting will\n        be applied to its contents.\n        \"\"\"\n        return self.long_desc()\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        \"\"\"\n        Populate option parse with options available for this command\n        \"\"\"\n        group = parser.add_argument_group(title=\"Global Options\")\n        group.add_argument(\n            \"--logfile\", metavar=\"FILE\", help=\"log file. if omitted stderr will be used\"\n        )\n        group.add_argument(\n            \"-L\",\n            \"--loglevel\",\n            metavar=\"LEVEL\",\n            default=None,\n            help=f\"log level (default: {self.settings['LOG_LEVEL']})\",\n        )\n        group.add_argument(\n            \"--nolog\", action=\"store_true\", help=\"disable logging completely\"\n        )\n        group.add_argument(\n            \"--profile\",\n            metavar=\"FILE\",\n            default=None,\n            help=\"write python cProfile stats to FILE\",\n        )\n        group.add_argument(\"--pidfile\", metavar=\"FILE\", help=\"write process ID to FILE\")\n        group.add_argument(\n            \"-s\",\n            \"--set\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set/override setting (may be repeated)\",\n        )\n        group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n\n    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n        try:\n            self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n        except ValueError:\n            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n\n        if opts.logfile:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_FILE\", opts.logfile, priority=\"cmdline\")\n\n        if opts.loglevel:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_LEVEL\", opts.loglevel, priority=\"cmdline\")\n\n        if opts.nolog:\n            self.settings.set(\"LOG_ENABLED\", False, priority=\"cmdline\")\n\n        if opts.pidfile:\n            Path(opts.pidfile).write_text(\n                str(os.getpid()) + os.linesep, encoding=\"utf-8\"\n            )\n\n        if opts.pdb:\n            failure.startDebugMode()\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        \"\"\"\n        Entry point for running commands\n        \"\"\"\n        raise NotImplementedError\n\n\nclass BaseRunSpiderCommand(ScrapyCommand):\n    \"\"\"\n    Common class used to share functionality between the crawl, parse and runspider commands\n    \"\"\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-a\",\n            dest=\"spargs\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set spider argument (may be repeated)\",\n        )\n        parser.add_argument(\n            \"-o\",\n            \"--output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"append scraped items to the end of FILE (use - for stdout),\"\n            \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\",\n        )\n        parser.add_argument(\n            \"-O\",\n            \"--overwrite-output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"dump scraped items into FILE, overwriting any existing file,\"\n            \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\",\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--output-format\",\n            metavar=\"FORMAT\",\n            help=\"format to use for dumping items\",\n        )\n\n    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n        try:\n            opts.spargs = arglist_to_dict(opts.spargs)\n        except ValueError:\n            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n        if opts.output or opts.overwrite_output:\n            feeds = feed_process_params_from_cli(\n                self.settings,\n                opts.output,\n                opts.output_format,\n                opts.overwrite_output,\n            )\n            self.settings.set(\"FEEDS\", feeds, priority=\"cmdline\")\n\n\nclass ScrapyHelpFormatter(argparse.HelpFormatter):\n    \"\"\"\n    Help Formatter for scrapy command line help messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        prog: str,\n        indent_increment: int = 2,\n        max_help_position: int = 24,\n        width: Optional[int] = None,\n    ):\n        super().__init__(\n            prog,\n            indent_increment=indent_increment,\n            max_help_position=max_help_position,\n            width=width,\n        )\n\n    def _join_parts(self, part_strings: Iterable[str]) -> str:\n        # scrapy.commands.list shadows builtins.list\n        parts = self.format_part_strings(builtins.list(part_strings))\n        return super()._join_parts(parts)\n\n    def format_part_strings(self, part_strings: List[str]) -> List[str]:\n        \"\"\"\n        Underline and title case command line help message headers.\n        \"\"\"\n        if part_strings and part_strings[0].startswith(\"usage: \"):\n            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len(\"usage: \") :]\n        headings = [\n            i for i in range(len(part_strings)) if part_strings[i].endswith(\":\\n\")\n        ]\n        for index in headings[::-1]:\n            char = \"-\" if \"Global Options\" in part_strings[index] else \"=\"\n            part_strings[index] = part_strings[index][:-2].title()\n            underline = \"\".join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n            part_strings.insert(index + 1, underline)\n        return part_strings\n", "scrapy/commands/edit.py": "import argparse\nimport os\nimport sys\nfrom typing import List\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.exceptions import UsageError\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"<spider>\"\n\n    def short_desc(self) -> str:\n        return \"Edit spider\"\n\n    def long_desc(self) -> str:\n        return (\n            \"Edit a spider using the editor defined in the EDITOR environment\"\n            \" variable or else the EDITOR setting\"\n        )\n\n    def _err(self, msg: str) -> None:\n        sys.stderr.write(msg + os.linesep)\n        self.exitcode = 1\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        if len(args) != 1:\n            raise UsageError()\n\n        editor = self.settings[\"EDITOR\"]\n        assert self.crawler_process\n        try:\n            spidercls = self.crawler_process.spider_loader.load(args[0])\n        except KeyError:\n            return self._err(f\"Spider not found: {args[0]}\")\n\n        sfile = sys.modules[spidercls.__module__].__file__\n        assert sfile\n        sfile = sfile.replace(\".pyc\", \".py\")\n        self.exitcode = os.system(f'{editor} \"{sfile}\"')  # nosec\n", "scrapy/commands/check.py": "import argparse\nimport time\nfrom collections import defaultdict\nfrom typing import List\nfrom unittest import TextTestResult as _TextTestResult\nfrom unittest import TextTestRunner\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.contracts import ContractsManager\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import load_object, set_environ\n\n\nclass TextTestResult(_TextTestResult):\n    def printSummary(self, start: float, stop: float) -> None:\n        write = self.stream.write\n        # _WritelnDecorator isn't implemented in typeshed yet\n        writeln = self.stream.writeln  # type: ignore[attr-defined]\n\n        run = self.testsRun\n        plural = \"s\" if run != 1 else \"\"\n\n        writeln(self.separator2)\n        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n        writeln()\n\n        infos = []\n        if not self.wasSuccessful():\n            write(\"FAILED\")\n            failed, errored = map(len, (self.failures, self.errors))\n            if failed:\n                infos.append(f\"failures={failed}\")\n            if errored:\n                infos.append(f\"errors={errored}\")\n        else:\n            write(\"OK\")\n\n        if infos:\n            writeln(f\" ({', '.join(infos)})\")\n        else:\n            write(\"\\n\")\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Check spider contracts\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"only list contracts, without checking them\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            default=False,\n            action=\"store_true\",\n            help=\"print contract tests for all spiders\",\n        )\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        # load contracts\n        contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n        conman = ContractsManager(load_object(c) for c in contracts)\n        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n        # contract requests\n        contract_reqs = defaultdict(list)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        with set_environ(SCRAPY_CHECK=\"true\"):\n            for spidername in args or spider_loader.list():\n                spidercls = spider_loader.load(spidername)\n                spidercls.start_requests = lambda s: conman.from_spider(s, result)\n\n                tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                if opts.list:\n                    for method in tested_methods:\n                        contract_reqs[spidercls.name].append(method)\n                elif tested_methods:\n                    self.crawler_process.crawl(spidercls)\n\n            # start checks\n            if opts.list:\n                for spider, methods in sorted(contract_reqs.items()):\n                    if not methods and not opts.verbose:\n                        continue\n                    print(spider)\n                    for method in sorted(methods):\n                        print(f\"  * {method}\")\n            else:\n                start = time.time()\n                self.crawler_process.start()\n                stop = time.time()\n\n                result.printErrors()\n                result.printSummary(start, stop)\n                self.exitcode = int(not result.wasSuccessful())\n", "scrapy/commands/list.py": "import argparse\nfrom typing import List\n\nfrom scrapy.commands import ScrapyCommand\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def short_desc(self) -> str:\n        return \"List available spiders\"\n\n    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n        assert self.crawler_process\n        for s in sorted(self.crawler_process.spider_loader.list()):\n            print(s)\n", "scrapy/contracts/__init__.py": "import re\nimport sys\nfrom functools import wraps\nfrom inspect import getmembers\nfrom types import CoroutineType\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\nfrom unittest import TestCase, TestResult\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.python import get_spec\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass Contract:\n    \"\"\"Abstract class for contracts\"\"\"\n\n    request_cls: Optional[Type[Request]] = None\n    name: str\n\n    def __init__(self, method: Callable, *args: Any):\n        self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n        self.testcase_post = _create_testcase(method, f\"@{self.name} post-hook\")\n        self.args: Tuple[Any, ...] = args\n\n    def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n        if hasattr(self, \"pre_process\"):\n            cb = request.callback\n            assert cb is not None\n\n            @wraps(cb)\n            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    cb_result = cb(response, **cb_kwargs)\n                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                        raise TypeError(\"Contracts don't support async callbacks\")\n                    return list(  # pylint: disable=return-in-finally\n                        iterate_spider_output(cb_result)\n                    )\n\n            request.callback = wrapper\n\n        return request\n\n    def add_post_hook(self, request: Request, results: TestResult) -> Request:\n        if hasattr(self, \"post_process\"):\n            cb = request.callback\n            assert cb is not None\n\n            @wraps(cb)\n            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n                cb_result = cb(response, **cb_kwargs)\n                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                    raise TypeError(\"Contracts don't support async callbacks\")\n                output = list(iterate_spider_output(cb_result))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output  # pylint: disable=return-in-finally\n\n            request.callback = wrapper\n\n        return request\n\n    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n        return args\n\n\nclass ContractsManager:\n    contracts: Dict[str, Type[Contract]] = {}\n\n    def __init__(self, contracts: Iterable[Type[Contract]]):\n        for contract in contracts:\n            self.contracts[contract.name] = contract\n\n    def tested_methods_from_spidercls(self, spidercls: Type[Spider]) -> List[str]:\n        is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n        methods = []\n        for key, value in getmembers(spidercls):\n            if callable(value) and value.__doc__ and is_method(value.__doc__):\n                methods.append(key)\n\n        return methods\n\n    def extract_contracts(self, method: Callable) -> List[Contract]:\n        contracts: List[Contract] = []\n        assert method.__doc__ is not None\n        for line in method.__doc__.split(\"\\n\"):\n            line = line.strip()\n\n            if line.startswith(\"@\"):\n                m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n                if m is None:\n                    continue\n                name, args = m.groups()\n                args = re.split(r\"\\s+\", args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts\n\n    def from_spider(\n        self, spider: Spider, results: TestResult\n    ) -> List[Optional[Request]]:\n        requests: List[Optional[Request]] = []\n        for method in self.tested_methods_from_spidercls(type(spider)):\n            bound_method = spider.__getattribute__(method)\n            try:\n                requests.append(self.from_method(bound_method, results))\n            except Exception:\n                case = _create_testcase(bound_method, \"contract\")\n                results.addError(case, sys.exc_info())\n\n        return requests\n\n    def from_method(self, method: Callable, results: TestResult) -> Optional[Request]:\n        contracts = self.extract_contracts(method)\n        if contracts:\n            request_cls = Request\n            for contract in contracts:\n                if contract.request_cls is not None:\n                    request_cls = contract.request_cls\n\n            # calculate request args\n            args, kwargs = get_spec(request_cls.__init__)\n\n            # Don't filter requests to allow\n            # testing different callbacks on the same URL.\n            kwargs[\"dont_filter\"] = True\n            kwargs[\"callback\"] = method\n\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            args.remove(\"self\")\n\n            # check if all positional arguments are defined in kwargs\n            if set(args).issubset(set(kwargs)):\n                request = request_cls(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request\n        return None\n\n    def _clean_req(\n        self, request: Request, method: Callable, results: TestResult\n    ) -> None:\n        \"\"\"stop the request from returning objects and records any errors\"\"\"\n\n        cb = request.callback\n        assert cb is not None\n\n        @wraps(cb)\n        def cb_wrapper(response: Response, **cb_kwargs: Any) -> None:\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, \"callback\")\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure: Failure) -> None:\n            case = _create_testcase(method, \"errback\")\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper\n\n\ndef _create_testcase(method: Callable, desc: str) -> TestCase:\n    spider = method.__self__.name  # type: ignore[attr-defined]\n\n    class ContractTestCase(TestCase):\n        def __str__(_self) -> str:\n            return f\"[{spider}] {method.__name__} ({desc})\"\n\n    name = f\"{spider}_{method.__name__}\"\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)\n", "scrapy/contracts/default.py": "import json\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.contracts import Contract\nfrom scrapy.exceptions import ContractFail\nfrom scrapy.http import Request\n\n\n# contracts\nclass UrlContract(Contract):\n    \"\"\"Contract to set the url of the request (mandatory)\n    @url http://scrapy.org\n    \"\"\"\n\n    name = \"url\"\n\n    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n        args[\"url\"] = self.args[0]\n        return args\n\n\nclass CallbackKeywordArgumentsContract(Contract):\n    \"\"\"Contract to set the keyword arguments for the request.\n    The value should be a JSON-encoded dictionary, e.g.:\n\n    @cb_kwargs {\"arg1\": \"some value\"}\n    \"\"\"\n\n    name = \"cb_kwargs\"\n\n    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n        args[\"cb_kwargs\"] = json.loads(\" \".join(self.args))\n        return args\n\n\nclass ReturnsContract(Contract):\n    \"\"\"Contract to check the output of a callback\n\n    general form:\n    @returns request(s)/item(s) [min=1 [max]]\n\n    e.g.:\n    @returns request\n    @returns request 2\n    @returns request 2 10\n    @returns request 0 10\n    \"\"\"\n\n    name = \"returns\"\n    object_type_verifiers: Dict[Optional[str], Callable[[Any], bool]] = {\n        \"request\": lambda x: isinstance(x, Request),\n        \"requests\": lambda x: isinstance(x, Request),\n        \"item\": is_item,\n        \"items\": is_item,\n    }\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n\n        if len(self.args) not in [1, 2, 3]:\n            raise ValueError(\n                f\"Incorrect argument quantity: expected 1, 2 or 3, got {len(self.args)}\"\n            )\n        self.obj_name = self.args[0] or None\n        self.obj_type_verifier = self.object_type_verifiers[self.obj_name]\n\n        try:\n            self.min_bound: float = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound: float = int(self.args[2])\n        except IndexError:\n            self.max_bound = float(\"inf\")\n\n    def post_process(self, output: List[Any]) -> None:\n        occurrences = 0\n        for x in output:\n            if self.obj_type_verifier(x):\n                occurrences += 1\n\n        assertion = self.min_bound <= occurrences <= self.max_bound\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = str(self.min_bound)\n            else:\n                expected = f\"{self.min_bound}..{self.max_bound}\"\n\n            raise ContractFail(\n                f\"Returned {occurrences} {self.obj_name}, expected {expected}\"\n            )\n\n\nclass ScrapesContract(Contract):\n    \"\"\"Contract to check presence of fields in scraped items\n    @scrapes page_name page_body\n    \"\"\"\n\n    name = \"scrapes\"\n\n    def post_process(self, output: List[Any]) -> None:\n        for x in output:\n            if is_item(x):\n                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                if missing:\n                    missing_fields = \", \".join(missing)\n                    raise ContractFail(f\"Missing fields: {missing_fields}\")\n", "scrapy/spiders/init.py": "from typing import Any, Iterable, Optional, cast\n\nfrom scrapy import Request\nfrom scrapy.http import Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass InitSpider(Spider):\n    \"\"\"Base Spider with initialization facilities\"\"\"\n\n    def start_requests(self) -> Iterable[Request]:\n        self._postinit_reqs: Iterable[Request] = super().start_requests()\n        return cast(Iterable[Request], iterate_spider_output(self.init_request()))\n\n    def initialized(self, response: Optional[Response] = None) -> Any:\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop(\"_postinit_reqs\")\n\n    def init_request(self) -> Any:\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()\n", "scrapy/spiders/feed.py": "\"\"\"\nThis module implements the XMLFeedSpider which is the recommended spider to use\nfor scraping from an XML feed.\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n\nfrom scrapy.exceptions import NotConfigured, NotSupported\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.selector import Selector\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.iterators import csviter, xmliter_lxml\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass XMLFeedSpider(Spider):\n    \"\"\"\n    This class intends to be the base class for spiders that scrape\n    from XML feeds.\n\n    You can choose whether to parse the file using the 'iternodes' iterator, an\n    'xml' selector, or an 'html' selector.  In most cases, it's convenient to\n    use iternodes, since it's a faster and cleaner.\n    \"\"\"\n\n    iterator: str = \"iternodes\"\n    itertag: str = \"item\"\n    namespaces: Sequence[Tuple[str, str]] = ()\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (items or requests).\n        \"\"\"\n        return results\n\n    def adapt_response(self, response: Response) -> Response:\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response\n\n    def parse_node(self, response: Response, selector: Selector) -> Any:\n        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n        if hasattr(self, \"parse_item\"):  # backward compatibility\n            return self.parse_item(response, selector)\n        raise NotImplementedError\n\n    def parse_nodes(self, response: Response, nodes: Iterable[Selector]) -> Any:\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either an item, a request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            yield from self.process_results(response, ret)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        if not hasattr(self, \"parse_node\"):\n            raise NotConfigured(\n                \"You must define parse_node method in order to scrape this XML feed\"\n            )\n\n        response = self.adapt_response(response)\n        nodes: Iterable[Selector]\n        if self.iterator == \"iternodes\":\n            nodes = self._iternodes(response)\n        elif self.iterator == \"xml\":\n            if not isinstance(response, TextResponse):\n                raise ValueError(\"Response content isn't text\")\n            selector = Selector(response, type=\"xml\")\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f\"//{self.itertag}\")\n        elif self.iterator == \"html\":\n            if not isinstance(response, TextResponse):\n                raise ValueError(\"Response content isn't text\")\n            selector = Selector(response, type=\"html\")\n            self._register_namespaces(selector)\n            nodes = selector.xpath(f\"//{self.itertag}\")\n        else:\n            raise NotSupported(\"Unsupported node iterator\")\n\n        return self.parse_nodes(response, nodes)\n\n    def _iternodes(self, response: Response) -> Iterable[Selector]:\n        for node in xmliter_lxml(response, self.itertag):\n            self._register_namespaces(node)\n            yield node\n\n    def _register_namespaces(self, selector: Selector) -> None:\n        for prefix, uri in self.namespaces:\n            selector.register_namespace(prefix, uri)\n\n\nclass CSVFeedSpider(Spider):\n    \"\"\"Spider for parsing CSV feeds.\n    It receives a CSV file in a response; iterates through each of its rows,\n    and calls parse_row with a dict containing each field's data.\n\n    You can set some options regarding the CSV file, such as the delimiter, quotechar\n    and the file's headers.\n    \"\"\"\n\n    delimiter: Optional[str] = (\n        None  # When this is None, python's csv module's default delimiter is used\n    )\n    quotechar: Optional[str] = (\n        None  # When this is None, python's csv module's default quotechar is used\n    )\n    headers: Optional[List[str]] = None\n\n    def process_results(\n        self, response: Response, results: Iterable[Any]\n    ) -> Iterable[Any]:\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return results\n\n    def adapt_response(self, response: Response) -> Response:\n        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n        return response\n\n    def parse_row(self, response: Response, row: Dict[str, str]) -> Any:\n        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n        raise NotImplementedError\n\n    def parse_rows(self, response: Response) -> Any:\n        \"\"\"Receives a response and a dict (representing each row) with a key for\n        each provided (or detected) header of the CSV file.  This spider also\n        gives the opportunity to override adapt_response and\n        process_results methods for pre and post-processing purposes.\n        \"\"\"\n\n        for row in csviter(\n            response, self.delimiter, self.headers, quotechar=self.quotechar\n        ):\n            ret = iterate_spider_output(self.parse_row(response, row))\n            yield from self.process_results(response, ret)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        if not hasattr(self, \"parse_row\"):\n            raise NotConfigured(\n                \"You must define parse_row method in order to scrape this CSV feed\"\n            )\n        response = self.adapt_response(response)\n        return self.parse_rows(response)\n", "scrapy/spiders/sitemap.py": "from __future__ import annotations\n\nimport logging\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom scrapy.http import Request, Response, XmlResponse\nfrom scrapy.spiders import Spider\nfrom scrapy.utils._compression import _DecompressionMaxSizeExceeded\nfrom scrapy.utils.gz import gunzip, gzip_magic_number\nfrom scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nclass SitemapSpider(Spider):\n    sitemap_urls: Sequence[str] = ()\n    sitemap_rules: Sequence[\n        Tuple[Union[re.Pattern[str], str], Union[str, Callable]]\n    ] = [(\"\", \"parse\")]\n    sitemap_follow: Sequence[Union[re.Pattern[str], str]] = [\"\"]\n    sitemap_alternate_links: bool = False\n    _max_size: int\n    _warn_size: int\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._max_size = getattr(\n            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\n        )\n        spider._warn_size = getattr(\n            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\n        )\n        return spider\n\n    def __init__(self, *a: Any, **kw: Any):\n        super().__init__(*a, **kw)\n        self._cbs: List[Tuple[re.Pattern[str], Callable]] = []\n        for r, c in self.sitemap_rules:\n            if isinstance(c, str):\n                c = cast(Callable, getattr(self, c))\n            self._cbs.append((regex(r), c))\n        self._follow: List[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n\n    def start_requests(self) -> Iterable[Request]:\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)\n\n    def sitemap_filter(\n        self, entries: Iterable[Dict[str, Any]]\n    ) -> Iterable[Dict[str, Any]]:\n        \"\"\"This method can be used to filter sitemap entries by their\n        attributes, for example, you can filter locs with lastmod greater\n        than a given date (see docs).\n        \"\"\"\n        yield from entries\n\n    def _parse_sitemap(self, response: Response) -> Iterable[Request]:\n        if response.url.endswith(\"/robots.txt\"):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\n                    \"Ignoring invalid sitemap: %(response)s\",\n                    {\"response\": response},\n                    extra={\"spider\": self},\n                )\n                return\n\n            s = Sitemap(body)\n            it = self.sitemap_filter(s)\n\n            if s.type == \"sitemapindex\":\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == \"urlset\":\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break\n\n    def _get_sitemap_body(self, response: Response) -> Optional[bytes]:\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        if gzip_magic_number(response):\n            uncompressed_size = len(response.body)\n            max_size = response.meta.get(\"download_maxsize\", self._max_size)\n            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\n            try:\n                body = gunzip(response.body, max_size=max_size)\n            except _DecompressionMaxSizeExceeded:\n                return None\n            if uncompressed_size < warn_size <= len(body):\n                logger.warning(\n                    f\"{response} body size after decompression ({len(body)} B) \"\n                    f\"is larger than the download warning size ({warn_size} B).\"\n                )\n            return body\n        # actual gzipped sitemap files are decompressed above ;\n        # if we are here (response body is not gzipped)\n        # and have a response for .xml.gz,\n        # it usually means that it was already gunzipped\n        # by HttpCompression middleware,\n        # the HTTP response being sent with \"Content-Encoding: gzip\"\n        # without actually being a .xml.gz file in the first place,\n        # merely XML gzip-compressed on the fly,\n        # in other word, here, we have plain XML\n        if response.url.endswith(\".xml\") or response.url.endswith(\".xml.gz\"):\n            return response.body\n        return None\n\n\ndef regex(x: Union[re.Pattern[str], str]) -> re.Pattern[str]:\n    if isinstance(x, str):\n        return re.compile(x)\n    return x\n\n\ndef iterloc(it: Iterable[Dict[str, Any]], alt: bool = False) -> Iterable[str]:\n    for d in it:\n        yield d[\"loc\"]\n\n        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n        if alt and \"alternate\" in d:\n            yield from d[\"alternate\"]\n", "scrapy/spiders/crawl.py": "\"\"\"\nThis modules implements the CrawlSpider which is the recommended spider to use\nfor scraping typical web sites that requires crawling pages.\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.link import Link\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.crawler import Crawler\n\n\n_T = TypeVar(\"_T\")\nProcessLinksT = Callable[[List[Link]], List[Link]]\nProcessRequestT = Callable[[Request, Response], Optional[Request]]\n\n\ndef _identity(x: _T) -> _T:\n    return x\n\n\ndef _identity_process_request(\n    request: Request, response: Response\n) -> Optional[Request]:\n    return request\n\n\ndef _get_method(\n    method: Union[Callable, str, None], spider: Spider\n) -> Optional[Callable]:\n    if callable(method):\n        return method\n    if isinstance(method, str):\n        return getattr(spider, method, None)\n    return None\n\n\n_default_link_extractor = LinkExtractor()\n\n\nclass Rule:\n    def __init__(\n        self,\n        link_extractor: Optional[LinkExtractor] = None,\n        callback: Union[Callable, str, None] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        follow: Optional[bool] = None,\n        process_links: Union[ProcessLinksT, str, None] = None,\n        process_request: Union[ProcessRequestT, str, None] = None,\n        errback: Union[Callable[[Failure], Any], str, None] = None,\n    ):\n        self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n        self.callback: Union[Callable, str, None] = callback\n        self.errback: Union[Callable[[Failure], Any], str, None] = errback\n        self.cb_kwargs: Dict[str, Any] = cb_kwargs or {}\n        self.process_links: Union[ProcessLinksT, str] = process_links or _identity\n        self.process_request: Union[ProcessRequestT, str] = (\n            process_request or _identity_process_request\n        )\n        self.follow: bool = follow if follow is not None else not callback\n\n    def _compile(self, spider: Spider) -> None:\n        # this replaces method names with methods and we can't express this in type hints\n        self.callback = _get_method(self.callback, spider)\n        self.errback = cast(Callable[[Failure], Any], _get_method(self.errback, spider))\n        self.process_links = cast(\n            ProcessLinksT, _get_method(self.process_links, spider)\n        )\n        self.process_request = cast(\n            ProcessRequestT, _get_method(self.process_request, spider)\n        )\n\n\nclass CrawlSpider(Spider):\n    rules: Sequence[Rule] = ()\n    _rules: List[Rule]\n    _follow_links: bool\n\n    def __init__(self, *a: Any, **kw: Any):\n        super().__init__(*a, **kw)\n        self._compile_rules()\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        return self._parse_response(\n            response=response,\n            callback=self.parse_start_url,\n            cb_kwargs=kwargs,\n            follow=True,\n        )\n\n    def parse_start_url(self, response: Response, **kwargs: Any) -> Any:\n        return []\n\n    def process_results(self, response: Response, results: Any) -> Any:\n        return results\n\n    def _build_request(self, rule_index: int, link: Link) -> Request:\n        return Request(\n            url=link.url,\n            callback=self._callback,\n            errback=self._errback,\n            meta={\"rule\": rule_index, \"link_text\": link.text},\n        )\n\n    def _requests_to_follow(self, response: Response) -> Iterable[Optional[Request]]:\n        if not isinstance(response, HtmlResponse):\n            return\n        seen: Set[Link] = set()\n        for rule_index, rule in enumerate(self._rules):\n            links: List[Link] = [\n                lnk\n                for lnk in rule.link_extractor.extract_links(response)\n                if lnk not in seen\n            ]\n            for link in cast(ProcessLinksT, rule.process_links)(links):\n                seen.add(link)\n                request = self._build_request(rule_index, link)\n                yield cast(ProcessRequestT, rule.process_request)(request, response)\n\n    def _callback(self, response: Response, **cb_kwargs: Any) -> Any:\n        rule = self._rules[cast(int, response.meta[\"rule\"])]\n        return self._parse_response(\n            response,\n            cast(Callable, rule.callback),\n            {**rule.cb_kwargs, **cb_kwargs},\n            rule.follow,\n        )\n\n    def _errback(self, failure: Failure) -> Iterable[Any]:\n        rule = self._rules[cast(int, failure.request.meta[\"rule\"])]  # type: ignore[attr-defined]\n        return self._handle_failure(\n            failure, cast(Callable[[Failure], Any], rule.errback)\n        )\n\n    async def _parse_response(\n        self,\n        response: Response,\n        callback: Optional[Callable],\n        cb_kwargs: Dict[str, Any],\n        follow: bool = True,\n    ) -> AsyncIterable[Any]:\n        if callback:\n            cb_res = callback(response, **cb_kwargs) or ()\n            if isinstance(cb_res, AsyncIterable):\n                cb_res = await collect_asyncgen(cb_res)\n            elif isinstance(cb_res, Awaitable):\n                cb_res = await cb_res\n            cb_res = self.process_results(response, cb_res)\n            for request_or_item in iterate_spider_output(cb_res):\n                yield request_or_item\n\n        if follow and self._follow_links:\n            for request_or_item in self._requests_to_follow(response):\n                yield request_or_item\n\n    def _handle_failure(\n        self, failure: Failure, errback: Optional[Callable[[Failure], Any]]\n    ) -> Iterable[Any]:\n        if errback:\n            results = errback(failure) or ()\n            yield from iterate_spider_output(results)\n\n    def _compile_rules(self) -> None:\n        self._rules = []\n        for rule in self.rules:\n            self._rules.append(copy.copy(rule))\n            self._rules[-1]._compile(self)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool(\n            \"CRAWLSPIDER_FOLLOW_LINKS\", True\n        )\n        return spider\n", "scrapy/spiders/__init__.py": "\"\"\"\nBase class for Scrapy spiders\n\nSee documentation in docs/topics/spiders.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Union, cast\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import signals\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import url_is_from_spider\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n    # typing.Concatenate requires Python 3.10\n    # typing.Self requires Python 3.11\n    from typing_extensions import Concatenate, Self\n\n    from scrapy.crawler import Crawler\n    from scrapy.settings import BaseSettings, _SettingsKeyT\n    from scrapy.utils.log import SpiderLoggerAdapter\n\n    CallbackT = Callable[Concatenate[Response, ...], Any]\n\n\nclass Spider(object_ref):\n    \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n    class.\n    \"\"\"\n\n    name: str\n    custom_settings: Optional[Dict[_SettingsKeyT, Any]] = None\n\n    def __init__(self, name: Optional[str] = None, **kwargs: Any):\n        if name is not None:\n            self.name: str = name\n        elif not getattr(self, \"name\", None):\n            raise ValueError(f\"{type(self).__name__} must have a name\")\n        self.__dict__.update(kwargs)\n        if not hasattr(self, \"start_urls\"):\n            self.start_urls: List[str] = []\n\n    @property\n    def logger(self) -> SpiderLoggerAdapter:\n        from scrapy.utils.log import SpiderLoggerAdapter\n\n        logger = logging.getLogger(self.name)\n        return SpiderLoggerAdapter(logger, {\"spider\": self})\n\n    def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider\n\n    def _set_crawler(self, crawler: Crawler) -> None:\n        self.crawler: Crawler = crawler\n        self.settings: BaseSettings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)\n\n    def start_requests(self) -> Iterable[Request]:\n        if not self.start_urls and hasattr(self, \"start_url\"):\n            raise AttributeError(\n                \"Crawling could not start: 'start_urls' not found \"\n                \"or empty (but found 'start_url' attribute instead, \"\n                \"did you miss an 's'?)\"\n            )\n        for url in self.start_urls:\n            yield Request(url, dont_filter=True)\n\n    def _parse(self, response: Response, **kwargs: Any) -> Any:\n        return self.parse(response, **kwargs)\n\n    if TYPE_CHECKING:\n        parse: CallbackT\n    else:\n\n        def parse(self, response: Response, **kwargs: Any) -> Any:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__}.parse callback is not defined\"\n            )\n\n    @classmethod\n    def update_settings(cls, settings: BaseSettings) -> None:\n        settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n\n    @classmethod\n    def handles_request(cls, request: Request) -> bool:\n        return url_is_from_spider(request.url, cls)\n\n    @staticmethod\n    def close(spider: Spider, reason: str) -> Union[Deferred, None]:\n        closed = getattr(spider, \"closed\", None)\n        if callable(closed):\n            return cast(Union[Deferred, None], closed(reason))\n        return None\n\n    def __repr__(self) -> str:\n        return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"\n\n\n# Top-level imports\nfrom scrapy.spiders.crawl import CrawlSpider, Rule\nfrom scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\nfrom scrapy.spiders.sitemap import SitemapSpider\n", "scrapy/loader/__init__.py": "\"\"\"\nItem Loader\n\nSee documentation in docs/topics/loaders.rst\n\"\"\"\n\nfrom typing import Any, Optional\n\nimport itemloaders\n\nfrom scrapy.http import TextResponse\nfrom scrapy.item import Item\nfrom scrapy.selector import Selector\n\n\nclass ItemLoader(itemloaders.ItemLoader):\n    \"\"\"\n    A user-friendly abstraction to populate an :ref:`item <topics-items>` with data\n    by applying :ref:`field processors <topics-loaders-processors>` to scraped data.\n    When instantiated with a ``selector`` or a ``response`` it supports\n    data extraction from web pages using :ref:`selectors <topics-selectors>`.\n\n    :param item: The item instance to populate using subsequent calls to\n        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n        or :meth:`~ItemLoader.add_value`.\n    :type item: scrapy.item.Item\n\n    :param selector: The selector to extract data from, when using the\n        :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or\n        :meth:`replace_css` method.\n    :type selector: :class:`~scrapy.selector.Selector` object\n\n    :param response: The response used to construct the selector using the\n        :attr:`default_selector_class`, unless the selector argument is given,\n        in which case this argument is ignored.\n    :type response: :class:`~scrapy.http.Response` object\n\n    If no item is given, one is instantiated automatically using the class in\n    :attr:`default_item_class`.\n\n    The item, selector, response and remaining keyword arguments are\n    assigned to the Loader context (accessible through the :attr:`context` attribute).\n\n    .. attribute:: item\n\n        The item object being parsed by this Item Loader.\n        This is mostly used as a property so, when attempting to override this\n        value, you may want to check out :attr:`default_item_class` first.\n\n    .. attribute:: context\n\n        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n\n    .. attribute:: default_item_class\n\n        An :ref:`item <topics-items>` class (or factory), used to instantiate\n        items when not given in the ``__init__`` method.\n\n    .. attribute:: default_input_processor\n\n        The default input processor to use for those fields which don't specify\n        one.\n\n    .. attribute:: default_output_processor\n\n        The default output processor to use for those fields which don't specify\n        one.\n\n    .. attribute:: default_selector_class\n\n        The class used to construct the :attr:`selector` of this\n        :class:`ItemLoader`, if only a response is given in the ``__init__`` method.\n        If a selector is given in the ``__init__`` method this attribute is ignored.\n        This attribute is sometimes overridden in subclasses.\n\n    .. attribute:: selector\n\n        The :class:`~scrapy.selector.Selector` object to extract data from.\n        It's either the selector given in the ``__init__`` method or one created from\n        the response given in the ``__init__`` method using the\n        :attr:`default_selector_class`. This attribute is meant to be\n        read-only.\n    \"\"\"\n\n    default_item_class: type = Item\n    default_selector_class = Selector\n\n    def __init__(\n        self,\n        item: Any = None,\n        selector: Optional[Selector] = None,\n        response: Optional[TextResponse] = None,\n        parent: Optional[itemloaders.ItemLoader] = None,\n        **context: Any\n    ):\n        if selector is None and response is not None:\n            try:\n                selector = self.default_selector_class(response)\n            except AttributeError:\n                selector = None\n        context.update(response=response)\n        super().__init__(item=item, selector=selector, parent=parent, **context)\n", "scrapy/settings/__init__.py": "from __future__ import annotations\n\nimport copy\nimport json\nfrom importlib import import_module\nfrom pprint import pformat\nfrom types import ModuleType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom scrapy.settings import default_settings\n\n# The key types are restricted in BaseSettings._get_key() to ones supported by JSON,\n# see https://github.com/scrapy/scrapy/issues/5383.\n_SettingsKeyT = Union[bool, float, int, str, None]\n\nif TYPE_CHECKING:\n    # https://github.com/python/typing/issues/445#issuecomment-1131458824\n    from _typeshed import SupportsItems\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    _SettingsInputT = Union[SupportsItems[_SettingsKeyT, Any], str, None]\n\n\nSETTINGS_PRIORITIES: Dict[str, int] = {\n    \"default\": 0,\n    \"command\": 10,\n    \"addon\": 15,\n    \"project\": 20,\n    \"spider\": 30,\n    \"cmdline\": 40,\n}\n\n\ndef get_settings_priority(priority: Union[int, str]) -> int:\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, str):\n        return SETTINGS_PRIORITIES[priority]\n    return priority\n\n\nclass SettingsAttribute:\n    \"\"\"Class for storing data related to settings attributes.\n\n    This class is intended for internal usage, you should try Settings class\n    for settings configuration, not this one.\n    \"\"\"\n\n    def __init__(self, value: Any, priority: int):\n        self.value: Any = value\n        self.priority: int\n        if isinstance(self.value, BaseSettings):\n            self.priority = max(self.value.maxpriority(), priority)\n        else:\n            self.priority = priority\n\n    def set(self, value: Any, priority: int) -> None:\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority\n\n    def __repr__(self) -> str:\n        return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"\n\n\nclass BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n    \"\"\"\n    Instances of this class behave like dictionaries, but store priorities\n    along with their ``(key, value)`` pairs, and can be frozen (i.e. marked\n    immutable).\n\n    Key-value entries can be passed on initialization with the ``values``\n    argument, and they would take the ``priority`` level (unless ``values`` is\n    already an instance of :class:`~scrapy.settings.BaseSettings`, in which\n    case the existing priority levels will be kept).  If the ``priority``\n    argument is a string, the priority name will be looked up in\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer\n    should be provided.\n\n    Once the object is created, new settings can be loaded or updated with the\n    :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with\n    the square bracket notation of dictionaries, or with the\n    :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its\n    value conversion variants. When requesting a stored key, the value with the\n    highest priority will be retrieved.\n    \"\"\"\n\n    __default = object()\n\n    def __init__(\n        self, values: _SettingsInputT = None, priority: Union[int, str] = \"project\"\n    ):\n        self.frozen: bool = False\n        self.attributes: dict[_SettingsKeyT, SettingsAttribute] = {}\n        if values:\n            self.update(values, priority)\n\n    def __getitem__(self, opt_name: _SettingsKeyT) -> Any:\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value\n\n    def __contains__(self, name: Any) -> bool:\n        return name in self.attributes\n\n    def get(self, name: _SettingsKeyT, default: Any = None) -> Any:\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return self[name] if self[name] is not None else default\n\n    def getbool(self, name: _SettingsKeyT, default: bool = False) -> bool:\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\n                \"Supported values for boolean settings \"\n                \"are 0/1, True/False, '0'/'1', \"\n                \"'True'/'False' and 'true'/'false'\"\n            )\n\n    def getint(self, name: _SettingsKeyT, default: int = 0) -> int:\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return int(self.get(name, default))\n\n    def getfloat(self, name: _SettingsKeyT, default: float = 0.0) -> float:\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        return float(self.get(name, default))\n\n    def getlist(\n        self, name: _SettingsKeyT, default: Optional[List[Any]] = None\n    ) -> List[Any]:\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list, a\n        copy of it will be returned. If it's a string it will be split by \",\".\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or [])\n        if isinstance(value, str):\n            value = value.split(\",\")\n        return list(value)\n\n    def getdict(\n        self, name: _SettingsKeyT, default: Optional[Dict[Any, Any]] = None\n    ) -> Dict[Any, Any]:\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: str\n\n        :param default: the value to return if no setting is found\n        :type default: object\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, str):\n            value = json.loads(value)\n        return dict(value)\n\n    def getdictorlist(\n        self,\n        name: _SettingsKeyT,\n        default: Union[Dict[Any, Any], List[Any], Tuple[Any], None] = None,\n    ) -> Union[Dict[Any, Any], List[Any]]:\n        \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n\n        If the setting is already a dict or a list, a copy of it will be\n        returned.\n\n        If it is a string it will be evaluated as JSON, or as a comma-separated\n        list of strings as a fallback.\n\n        For example, settings populated from the command line will return:\n\n        -   ``{'key1': 'value1', 'key2': 'value2'}`` if set to\n            ``'{\"key1\": \"value1\", \"key2\": \"value2\"}'``\n\n        -   ``['one', 'two']`` if set to ``'[\"one\", \"two\"]'`` or ``'one,two'``\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default)\n        if value is None:\n            return {}\n        if isinstance(value, str):\n            try:\n                value_loaded = json.loads(value)\n                assert isinstance(value_loaded, (dict, list))\n                return value_loaded\n            except ValueError:\n                return value.split(\",\")\n        if isinstance(value, tuple):\n            return list(value)\n        assert isinstance(value, (dict, list))\n        return copy.deepcopy(value)\n\n    def getwithbase(self, name: _SettingsKeyT) -> BaseSettings:\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: str\n        \"\"\"\n        if not isinstance(name, str):\n            raise ValueError(f\"Base setting key must be a string, got {name}\")\n        compbs = BaseSettings()\n        compbs.update(self[name + \"_BASE\"])\n        compbs.update(self[name])\n        return compbs\n\n    def getpriority(self, name: _SettingsKeyT) -> Optional[int]:\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: str\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority\n\n    def maxpriority(self) -> int:\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(cast(int, self.getpriority(name)) for name in self)\n        return get_settings_priority(\"default\")\n\n    def __setitem__(self, name: _SettingsKeyT, value: Any) -> None:\n        self.set(name, value)\n\n    def set(\n        self, name: _SettingsKeyT, value: Any, priority: Union[int, str] = \"project\"\n    ) -> None:\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: str\n\n        :param value: the value to associate with the setting\n        :type value: object\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)\n\n    def setdefault(\n        self,\n        name: _SettingsKeyT,\n        default: Any = None,\n        priority: Union[int, str] = \"project\",\n    ) -> Any:\n        if name not in self:\n            self.set(name, default, priority)\n            return default\n\n        return self.attributes[name].value\n\n    def setdict(\n        self, values: _SettingsInputT, priority: Union[int, str] = \"project\"\n    ) -> None:\n        self.update(values, priority)\n\n    def setmodule(\n        self, module: Union[ModuleType, str], priority: Union[int, str] = \"project\"\n    ) -> None:\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: types.ModuleType or str\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, str):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)\n\n    # BaseSettings.update() doesn't support all inputs that MutableMapping.update() supports\n    def update(self, values: _SettingsInputT, priority: Union[int, str] = \"project\") -> None:  # type: ignore[override]\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: str or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, str):\n            values = cast(Dict[_SettingsKeyT, Any], json.loads(values))\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in values.items():\n                    self.set(name, value, cast(int, values.getpriority(name)))\n            else:\n                for name, value in values.items():\n                    self.set(name, value, priority)\n\n    def delete(\n        self, name: _SettingsKeyT, priority: Union[int, str] = \"project\"\n    ) -> None:\n        if name not in self:\n            raise KeyError(name)\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= cast(int, self.getpriority(name)):\n            del self.attributes[name]\n\n    def __delitem__(self, name: _SettingsKeyT) -> None:\n        self._assert_mutability()\n        del self.attributes[name]\n\n    def _assert_mutability(self) -> None:\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")\n\n    def copy(self) -> Self:\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)\n\n    def freeze(self) -> None:\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True\n\n    def frozencopy(self) -> Self:\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy\n\n    def __iter__(self) -> Iterator[_SettingsKeyT]:\n        return iter(self.attributes)\n\n    def __len__(self) -> int:\n        return len(self.attributes)\n\n    def _to_dict(self) -> Dict[_SettingsKeyT, Any]:\n        return {\n            self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n            for k, v in self.items()\n        }\n\n    def _get_key(self, key_value: Any) -> _SettingsKeyT:\n        return (\n            key_value\n            if isinstance(key_value, (bool, float, int, str, type(None)))\n            else str(key_value)\n        )\n\n    def copy_to_dict(self) -> Dict[_SettingsKeyT, Any]:\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()\n\n    # https://ipython.readthedocs.io/en/stable/config/integrating.html#pretty-printing\n    def _repr_pretty_(self, p: Any, cycle: bool) -> None:\n        if cycle:\n            p.text(repr(self))\n        else:\n            p.text(pformat(self.copy_to_dict()))\n\n    def pop(self, name: _SettingsKeyT, default: Any = __default) -> Any:\n        try:\n            value = self.attributes[name].value\n        except KeyError:\n            if default is self.__default:\n                raise\n\n            return default\n        else:\n            self.__delitem__(name)\n            return value\n\n\nclass Settings(BaseSettings):\n    \"\"\"\n    This object stores Scrapy settings for the configuration of internal\n    components, and can be used for any further customization.\n\n    It is a direct subclass and supports all methods of\n    :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation\n    of this class, the new object will have the global default settings\n    described on :ref:`topics-settings-ref` already populated.\n    \"\"\"\n\n    def __init__(\n        self, values: _SettingsInputT = None, priority: Union[int, str] = \"project\"\n    ):\n        # Do not pass kwarg values here. We don't want to promote user-defined\n        # dicts, and we want to update, not replace, default dicts with the\n        # values given by the user\n        super().__init__()\n        self.setmodule(default_settings, \"default\")\n        # Promote default dictionaries to BaseSettings instances for per-key\n        # priorities\n        for name, val in self.items():\n            if isinstance(val, dict):\n                self.set(name, BaseSettings(val, \"default\"), \"default\")\n        self.update(values, priority)\n\n\ndef iter_default_settings() -> Iterable[Tuple[str, Any]]:\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)\n\n\ndef overridden_settings(\n    settings: Mapping[_SettingsKeyT, Any]\n) -> Iterable[Tuple[str, Any]]:\n    \"\"\"Return an iterable of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value\n", "scrapy/settings/default_settings.py": "\"\"\"\nThis module contains the default values for all settings used by Scrapy.\n\nFor more information about these settings you can read the settings\ndocumentation in docs/topics/settings.rst\n\nScrapy developers, if you add a setting here remember to:\n\n* add it in alphabetical order\n* group similar settings without leaving blank lines\n* add its documentation to the available settings documentation\n  (docs/topics/settings.rst)\n\n\"\"\"\n\nimport sys\nfrom importlib import import_module\nfrom pathlib import Path\n\nADDONS = {}\n\nAJAXCRAWL_ENABLED = False\n\nASYNCIO_EVENT_LOOP = None\n\nAUTOTHROTTLE_ENABLED = False\nAUTOTHROTTLE_DEBUG = False\nAUTOTHROTTLE_MAX_DELAY = 60.0\nAUTOTHROTTLE_START_DELAY = 5.0\nAUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n\nBOT_NAME = \"scrapybot\"\n\nCLOSESPIDER_TIMEOUT = 0\nCLOSESPIDER_PAGECOUNT = 0\nCLOSESPIDER_ITEMCOUNT = 0\nCLOSESPIDER_ERRORCOUNT = 0\n\nCOMMANDS_MODULE = \"\"\n\nCOMPRESSION_ENABLED = True\n\nCONCURRENT_ITEMS = 100\n\nCONCURRENT_REQUESTS = 16\nCONCURRENT_REQUESTS_PER_DOMAIN = 8\nCONCURRENT_REQUESTS_PER_IP = 0\n\nCOOKIES_ENABLED = True\nCOOKIES_DEBUG = False\n\nDEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n\nDEFAULT_REQUEST_HEADERS = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"en\",\n}\n\nDEPTH_LIMIT = 0\nDEPTH_STATS_VERBOSE = False\nDEPTH_PRIORITY = 0\n\nDNSCACHE_ENABLED = True\nDNSCACHE_SIZE = 10000\nDNS_RESOLVER = \"scrapy.resolver.CachingThreadedResolver\"\nDNS_TIMEOUT = 60\n\nDOWNLOAD_DELAY = 0\n\nDOWNLOAD_HANDLERS = {}\nDOWNLOAD_HANDLERS_BASE = {\n    \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n    \"file\": \"scrapy.core.downloader.handlers.file.FileDownloadHandler\",\n    \"http\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"https\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"s3\": \"scrapy.core.downloader.handlers.s3.S3DownloadHandler\",\n    \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\",\n}\n\nDOWNLOAD_TIMEOUT = 180  # 3mins\n\nDOWNLOAD_MAXSIZE = 1024 * 1024 * 1024  # 1024m\nDOWNLOAD_WARNSIZE = 32 * 1024 * 1024  # 32m\n\nDOWNLOAD_FAIL_ON_DATALOSS = True\n\nDOWNLOADER = \"scrapy.core.downloader.Downloader\"\n\nDOWNLOADER_HTTPCLIENTFACTORY = (\n    \"scrapy.core.downloader.webclient.ScrapyHTTPClientFactory\"\n)\nDOWNLOADER_CLIENTCONTEXTFACTORY = (\n    \"scrapy.core.downloader.contextfactory.ScrapyClientContextFactory\"\n)\nDOWNLOADER_CLIENT_TLS_CIPHERS = \"DEFAULT\"\n# Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:\nDOWNLOADER_CLIENT_TLS_METHOD = \"TLS\"\nDOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n\nDOWNLOADER_MIDDLEWARES = {}\n\nDOWNLOADER_MIDDLEWARES_BASE = {\n    # Engine side\n    \"scrapy.downloadermiddlewares.offsite.OffsiteMiddleware\": 50,\n    \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n    \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n    \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n    \"scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\": 400,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": 500,\n    \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 550,\n    \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware\": 560,\n    \"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\": 580,\n    \"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\": 590,\n    \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 600,\n    \"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\": 700,\n    \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\": 750,\n    \"scrapy.downloadermiddlewares.stats.DownloaderStats\": 850,\n    \"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\": 900,\n    # Downloader side\n}\n\nDOWNLOADER_STATS = True\n\nDUPEFILTER_CLASS = \"scrapy.dupefilters.RFPDupeFilter\"\n\nEDITOR = \"vi\"\nif sys.platform == \"win32\":\n    EDITOR = \"%s -m idlelib.idle\"\n\nEXTENSIONS = {}\n\nEXTENSIONS_BASE = {\n    \"scrapy.extensions.corestats.CoreStats\": 0,\n    \"scrapy.extensions.telnet.TelnetConsole\": 0,\n    \"scrapy.extensions.memusage.MemoryUsage\": 0,\n    \"scrapy.extensions.memdebug.MemoryDebugger\": 0,\n    \"scrapy.extensions.closespider.CloseSpider\": 0,\n    \"scrapy.extensions.feedexport.FeedExporter\": 0,\n    \"scrapy.extensions.logstats.LogStats\": 0,\n    \"scrapy.extensions.spiderstate.SpiderState\": 0,\n    \"scrapy.extensions.throttle.AutoThrottle\": 0,\n}\n\nFEED_TEMPDIR = None\nFEEDS = {}\nFEED_URI_PARAMS = None  # a function to extend uri arguments\nFEED_STORE_EMPTY = True\nFEED_EXPORT_ENCODING = None\nFEED_EXPORT_FIELDS = None\nFEED_STORAGES = {}\nFEED_STORAGES_BASE = {\n    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n    \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n}\nFEED_EXPORT_BATCH_ITEM_COUNT = 0\nFEED_EXPORTERS = {}\nFEED_EXPORTERS_BASE = {\n    \"json\": \"scrapy.exporters.JsonItemExporter\",\n    \"jsonlines\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"jsonl\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"jl\": \"scrapy.exporters.JsonLinesItemExporter\",\n    \"csv\": \"scrapy.exporters.CsvItemExporter\",\n    \"xml\": \"scrapy.exporters.XmlItemExporter\",\n    \"marshal\": \"scrapy.exporters.MarshalItemExporter\",\n    \"pickle\": \"scrapy.exporters.PickleItemExporter\",\n}\nFEED_EXPORT_INDENT = 0\n\nFEED_STORAGE_FTP_ACTIVE = False\nFEED_STORAGE_GCS_ACL = \"\"\nFEED_STORAGE_S3_ACL = \"\"\n\nFILES_STORE_S3_ACL = \"private\"\nFILES_STORE_GCS_ACL = \"\"\n\nFTP_USER = \"anonymous\"\nFTP_PASSWORD = \"guest\"  # nosec\nFTP_PASSIVE_MODE = True\n\nGCS_PROJECT_ID = None\n\nHTTPCACHE_ENABLED = False\nHTTPCACHE_DIR = \"httpcache\"\nHTTPCACHE_IGNORE_MISSING = False\nHTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\nHTTPCACHE_EXPIRATION_SECS = 0\nHTTPCACHE_ALWAYS_STORE = False\nHTTPCACHE_IGNORE_HTTP_CODES = []\nHTTPCACHE_IGNORE_SCHEMES = [\"file\"]\nHTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\nHTTPCACHE_DBM_MODULE = \"dbm\"\nHTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\nHTTPCACHE_GZIP = False\n\nHTTPPROXY_ENABLED = True\nHTTPPROXY_AUTH_ENCODING = \"latin-1\"\n\nIMAGES_STORE_S3_ACL = \"private\"\nIMAGES_STORE_GCS_ACL = \"\"\n\nITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n\nITEM_PIPELINES = {}\nITEM_PIPELINES_BASE = {}\n\nJOBDIR = None\n\nLOG_ENABLED = True\nLOG_ENCODING = \"utf-8\"\nLOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\nLOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\"\nLOG_DATEFORMAT = \"%Y-%m-%d %H:%M:%S\"\nLOG_STDOUT = False\nLOG_LEVEL = \"DEBUG\"\nLOG_FILE = None\nLOG_FILE_APPEND = True\nLOG_SHORT_NAMES = False\n\nSCHEDULER_DEBUG = False\n\nLOGSTATS_INTERVAL = 60.0\n\nMAIL_HOST = \"localhost\"\nMAIL_PORT = 25\nMAIL_FROM = \"scrapy@localhost\"\nMAIL_PASS = None\nMAIL_USER = None\n\nMEMDEBUG_ENABLED = False  # enable memory debugging\nMEMDEBUG_NOTIFY = []  # send memory debugging report by mail at engine shutdown\n\nMEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\nMEMUSAGE_ENABLED = True\nMEMUSAGE_LIMIT_MB = 0\nMEMUSAGE_NOTIFY_MAIL = []\nMEMUSAGE_WARNING_MB = 0\n\nMETAREFRESH_ENABLED = True\nMETAREFRESH_IGNORE_TAGS = [\"noscript\"]\nMETAREFRESH_MAXDELAY = 100\n\nNEWSPIDER_MODULE = \"\"\n\nPERIODIC_LOG_DELTA = None\nPERIODIC_LOG_STATS = None\nPERIODIC_LOG_TIMING_ENABLED = False\n\nRANDOMIZE_DOWNLOAD_DELAY = True\n\nREACTOR_THREADPOOL_MAXSIZE = 10\n\nREDIRECT_ENABLED = True\nREDIRECT_MAX_TIMES = 20  # uses Firefox default setting\nREDIRECT_PRIORITY_ADJUST = +2\n\nREFERER_ENABLED = True\nREFERRER_POLICY = \"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"\n\nREQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\nREQUEST_FINGERPRINTER_IMPLEMENTATION = \"SENTINEL\"\n\nRETRY_ENABLED = True\nRETRY_TIMES = 2  # initial response + 2 retries = 3 requests\nRETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\nRETRY_PRIORITY_ADJUST = -1\nRETRY_EXCEPTIONS = [\n    \"twisted.internet.defer.TimeoutError\",\n    \"twisted.internet.error.TimeoutError\",\n    \"twisted.internet.error.DNSLookupError\",\n    \"twisted.internet.error.ConnectionRefusedError\",\n    \"twisted.internet.error.ConnectionDone\",\n    \"twisted.internet.error.ConnectError\",\n    \"twisted.internet.error.ConnectionLost\",\n    \"twisted.internet.error.TCPTimedOutError\",\n    \"twisted.web.client.ResponseFailed\",\n    # OSError is raised by the HttpCompression middleware when trying to\n    # decompress an empty response\n    OSError,\n    \"scrapy.core.downloader.handlers.http11.TunnelError\",\n]\n\nROBOTSTXT_OBEY = False\nROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\nROBOTSTXT_USER_AGENT = None\n\nSCHEDULER = \"scrapy.core.scheduler.Scheduler\"\nSCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleLifoDiskQueue\"\nSCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.LifoMemoryQueue\"\nSCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.ScrapyPriorityQueue\"\n\nSCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n\nSPIDER_LOADER_CLASS = \"scrapy.spiderloader.SpiderLoader\"\nSPIDER_LOADER_WARN_ONLY = False\n\nSPIDER_MIDDLEWARES = {}\n\nSPIDER_MIDDLEWARES_BASE = {\n    # Engine side\n    \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n    \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n    \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n    \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900,\n    # Spider side\n}\n\nSPIDER_MODULES = []\n\nSTATS_CLASS = \"scrapy.statscollectors.MemoryStatsCollector\"\nSTATS_DUMP = True\n\nSTATSMAILER_RCPTS = []\n\nTEMPLATES_DIR = str((Path(__file__).parent / \"..\" / \"templates\").resolve())\n\nURLLENGTH_LIMIT = 2083\n\nUSER_AGENT = f'Scrapy/{import_module(\"scrapy\").__version__} (+https://scrapy.org)'\n\nTELNETCONSOLE_ENABLED = 1\nTELNETCONSOLE_PORT = [6023, 6073]\nTELNETCONSOLE_HOST = \"127.0.0.1\"\nTELNETCONSOLE_USERNAME = \"scrapy\"\nTELNETCONSOLE_PASSWORD = None\n\nTWISTED_REACTOR = None\n\nSPIDER_CONTRACTS = {}\nSPIDER_CONTRACTS_BASE = {\n    \"scrapy.contracts.default.UrlContract\": 1,\n    \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n    \"scrapy.contracts.default.ReturnsContract\": 2,\n    \"scrapy.contracts.default.ScrapesContract\": 3,\n}\n", "scrapy/core/engine.py": "\"\"\"\nThis is the Scrapy engine which controls the Scheduler, Downloader and Spider.\n\nFor more information see docs/topics/architecture.rst\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom time import time\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generator,\n    Iterable,\n    Iterator,\n    Optional,\n    Set,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks, succeed\nfrom twisted.internet.task import LoopingCall\nfrom twisted.python.failure import Failure\n\nfrom scrapy import signals\nfrom scrapy.core.downloader import Downloader\nfrom scrapy.core.scraper import Scraper\nfrom scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.log import failure_to_exc_info, logformatter_adapter\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.reactor import CallLaterOnce\n\nif TYPE_CHECKING:\n    from scrapy.core.scheduler import BaseScheduler\n    from scrapy.core.scraper import _HandleOutputDeferred\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass Slot:\n    def __init__(\n        self,\n        start_requests: Iterable[Request],\n        close_if_idle: bool,\n        nextcall: CallLaterOnce[None],\n        scheduler: BaseScheduler,\n    ) -> None:\n        self.closing: Optional[Deferred[None]] = None\n        self.inprogress: Set[Request] = set()\n        self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n        self.close_if_idle: bool = close_if_idle\n        self.nextcall: CallLaterOnce[None] = nextcall\n        self.scheduler: BaseScheduler = scheduler\n        self.heartbeat: LoopingCall = LoopingCall(nextcall.schedule)\n\n    def add_request(self, request: Request) -> None:\n        self.inprogress.add(request)\n\n    def remove_request(self, request: Request) -> None:\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()\n\n    def close(self) -> Deferred[None]:\n        self.closing = Deferred()\n        self._maybe_fire_closing()\n        return self.closing\n\n    def _maybe_fire_closing(self) -> None:\n        if self.closing is not None and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)\n\n\nclass ExecutionEngine:\n    def __init__(\n        self,\n        crawler: Crawler,\n        spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]],\n    ) -> None:\n        self.crawler: Crawler = crawler\n        self.settings: Settings = crawler.settings\n        self.signals: SignalManager = crawler.signals\n        assert crawler.logformatter\n        self.logformatter: LogFormatter = crawler.logformatter\n        self.slot: Optional[Slot] = None\n        self.spider: Optional[Spider] = None\n        self.running: bool = False\n        self.paused: bool = False\n        self.scheduler_cls: Type[BaseScheduler] = self._get_scheduler_class(\n            crawler.settings\n        )\n        downloader_cls: Type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n        self.downloader: Downloader = downloader_cls(crawler)\n        self.scraper = Scraper(crawler)\n        self._spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]] = (\n            spider_closed_callback\n        )\n        self.start_time: Optional[float] = None\n\n    def _get_scheduler_class(self, settings: BaseSettings) -> Type[BaseScheduler]:\n        from scrapy.core.scheduler import BaseScheduler\n\n        scheduler_cls: Type[BaseScheduler] = load_object(settings[\"SCHEDULER\"])\n        if not issubclass(scheduler_cls, BaseScheduler):\n            raise TypeError(\n                f\"The provided scheduler class ({settings['SCHEDULER']})\"\n                \" does not fully implement the scheduler interface\"\n            )\n        return scheduler_cls\n\n    @inlineCallbacks\n    def start(self) -> Generator[Deferred[Any], Any, None]:\n        if self.running:\n            raise RuntimeError(\"Engine already running\")\n        self.start_time = time()\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n        self.running = True\n        self._closewait: Deferred[None] = Deferred()\n        yield self._closewait\n\n    def stop(self) -> Deferred[None]:\n        \"\"\"Gracefully stop the execution engine\"\"\"\n\n        @inlineCallbacks\n        def _finish_stopping_engine(_: Any) -> Generator[Deferred[Any], Any, None]:\n            yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n            self._closewait.callback(None)\n\n        if not self.running:\n            raise RuntimeError(\"Engine not running\")\n\n        self.running = False\n        dfd = (\n            self.close_spider(self.spider, reason=\"shutdown\")\n            if self.spider is not None\n            else succeed(None)\n        )\n        return dfd.addBoth(_finish_stopping_engine)\n\n    def close(self) -> Deferred[None]:\n        \"\"\"\n        Gracefully close the execution engine.\n        If it has already been started, stop it. In all cases, close the spider and the downloader.\n        \"\"\"\n        if self.running:\n            return self.stop()  # will also close spider and downloader\n        if self.spider is not None:\n            return self.close_spider(\n                self.spider, reason=\"shutdown\"\n            )  # will also close downloader\n        self.downloader.close()\n        return succeed(None)\n\n    def pause(self) -> None:\n        self.paused = True\n\n    def unpause(self) -> None:\n        self.paused = False\n\n    def _next_request(self) -> None:\n        if self.slot is None:\n            return\n\n        assert self.spider is not None  # typing\n\n        if self.paused:\n            return None\n\n        while (\n            not self._needs_backout()\n            and self._next_request_from_scheduler() is not None\n        ):\n            pass\n\n        if self.slot.start_requests is not None and not self._needs_backout():\n            try:\n                request = next(self.slot.start_requests)\n            except StopIteration:\n                self.slot.start_requests = None\n            except Exception:\n                self.slot.start_requests = None\n                logger.error(\n                    \"Error while obtaining start requests\",\n                    exc_info=True,\n                    extra={\"spider\": self.spider},\n                )\n            else:\n                self.crawl(request)\n\n        if self.spider_is_idle() and self.slot.close_if_idle:\n            self._spider_idle()\n\n    def _needs_backout(self) -> bool:\n        assert self.slot is not None  # typing\n        assert self.scraper.slot is not None  # typing\n        return (\n            not self.running\n            or bool(self.slot.closing)\n            or self.downloader.needs_backout()\n            or self.scraper.slot.needs_backout()\n        )\n\n    def _next_request_from_scheduler(self) -> Optional[Deferred[None]]:\n        assert self.slot is not None  # typing\n        assert self.spider is not None  # typing\n\n        request = self.slot.scheduler.next_request()\n        if request is None:\n            return None\n\n        d: Deferred[Union[Response, Request]] = self._download(request)\n        d.addBoth(self._handle_downloader_output, request)\n        d.addErrback(\n            lambda f: logger.info(\n                \"Error while handling downloader output\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n\n        def _remove_request(_: Any) -> None:\n            assert self.slot\n            self.slot.remove_request(request)\n\n        d2: Deferred[None] = d.addBoth(_remove_request)\n        d2.addErrback(\n            lambda f: logger.info(\n                \"Error while removing request from slot\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        slot = self.slot\n        d2.addBoth(lambda _: slot.nextcall.schedule())\n        d2.addErrback(\n            lambda f: logger.info(\n                \"Error while scheduling new request\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        return d2\n\n    def _handle_downloader_output(\n        self, result: Union[Request, Response, Failure], request: Request\n    ) -> Optional[_HandleOutputDeferred]:\n        assert self.spider is not None  # typing\n\n        if not isinstance(result, (Request, Response, Failure)):\n            raise TypeError(\n                f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\"\n            )\n\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(result, Request):\n            self.crawl(result)\n            return None\n\n        d = self.scraper.enqueue_scrape(result, request, self.spider)\n        d.addErrback(\n            lambda f: logger.error(\n                \"Error while enqueuing downloader output\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": self.spider},\n            )\n        )\n        return d\n\n    def spider_is_idle(self) -> bool:\n        if self.slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n        if not self.scraper.slot.is_idle():  # type: ignore[union-attr]\n            return False\n        if self.downloader.active:  # downloader has pending requests\n            return False\n        if self.slot.start_requests is not None:  # not all start requests are handled\n            return False\n        if self.slot.scheduler.has_pending_requests():\n            return False\n        return True\n\n    def crawl(self, request: Request) -> None:\n        \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        self._schedule_request(request, self.spider)\n        self.slot.nextcall.schedule()  # type: ignore[union-attr]\n\n    def _schedule_request(self, request: Request, spider: Spider) -> None:\n        request_scheduled_result = self.signals.send_catch_log(\n            signals.request_scheduled,\n            request=request,\n            spider=spider,\n            dont_log=IgnoreRequest,\n        )\n        for handler, result in request_scheduled_result:\n            if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n                logger.debug(\n                    f\"Signal handler {global_object_name(handler)} dropped \"\n                    f\"request {request} before it reached the scheduler.\"\n                )\n                return\n        if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n            self.signals.send_catch_log(\n                signals.request_dropped, request=request, spider=spider\n            )\n\n    def download(self, request: Request) -> Deferred[Response]:\n        \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        d: Deferred[Union[Response, Request]] = self._download(request)\n        # Deferred.addBoth() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n        d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[arg-type]\n        return d2\n\n    def _downloaded(\n        self, result: Union[Response, Request, Failure], request: Request\n    ) -> Union[Deferred[Response], Response, Failure]:\n        assert self.slot is not None  # typing\n        self.slot.remove_request(request)\n        return self.download(result) if isinstance(result, Request) else result\n\n    def _download(self, request: Request) -> Deferred[Union[Response, Request]]:\n        assert self.slot is not None  # typing\n\n        self.slot.add_request(request)\n\n        def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n            if not isinstance(result, (Response, Request)):\n                raise TypeError(\n                    f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\"\n                )\n            if isinstance(result, Response):\n                if result.request is None:\n                    result.request = request\n                assert self.spider is not None\n                logkws = self.logformatter.crawled(result.request, result, self.spider)\n                if logkws is not None:\n                    logger.log(\n                        *logformatter_adapter(logkws), extra={\"spider\": self.spider}\n                    )\n                self.signals.send_catch_log(\n                    signal=signals.response_received,\n                    response=result,\n                    request=result.request,\n                    spider=self.spider,\n                )\n            return result\n\n        def _on_complete(_: _T) -> _T:\n            assert self.slot is not None\n            self.slot.nextcall.schedule()\n            return _\n\n        assert self.spider is not None\n        dwld: Deferred[Union[Response, Request]] = self.downloader.fetch(\n            request, self.spider\n        )\n        dwld.addCallback(_on_success)\n        dwld.addBoth(_on_complete)\n        return dwld\n\n    @inlineCallbacks\n    def open_spider(\n        self,\n        spider: Spider,\n        start_requests: Iterable[Request] = (),\n        close_if_idle: bool = True,\n    ) -> Generator[Deferred[Any], Any, None]:\n        if self.slot is not None:\n            raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n        logger.info(\"Spider opened\", extra={\"spider\": spider})\n        nextcall = CallLaterOnce(self._next_request)\n        scheduler = build_from_crawler(self.scheduler_cls, self.crawler)\n        start_requests = yield self.scraper.spidermw.process_start_requests(\n            start_requests, spider\n        )\n        self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n        self.spider = spider\n        if hasattr(scheduler, \"open\"):\n            if d := scheduler.open(spider):\n                yield d\n        yield self.scraper.open_spider(spider)\n        assert self.crawler.stats\n        self.crawler.stats.open_spider(spider)\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n        self.slot.nextcall.schedule()\n        self.slot.heartbeat.start(5)\n\n    def _spider_idle(self) -> None:\n        \"\"\"\n        Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n        It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n        exception, the spider is not closed until the next loop and this function is guaranteed to be called\n        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.\n        \"\"\"\n        assert self.spider is not None  # typing\n        expected_ex = (DontCloseSpider, CloseSpider)\n        res = self.signals.send_catch_log(\n            signals.spider_idle, spider=self.spider, dont_log=expected_ex\n        )\n        detected_ex = {\n            ex: x.value\n            for _, x in res\n            for ex in expected_ex\n            if isinstance(x, Failure) and isinstance(x.value, ex)\n        }\n        if DontCloseSpider in detected_ex:\n            return None\n        if self.spider_is_idle():\n            ex = detected_ex.get(CloseSpider, CloseSpider(reason=\"finished\"))\n            assert isinstance(ex, CloseSpider)  # typing\n            self.close_spider(self.spider, reason=ex.reason)\n\n    def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred[None]:\n        \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n        if self.slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n\n        if self.slot.closing is not None:\n            return self.slot.closing\n\n        logger.info(\n            \"Closing spider (%(reason)s)\", {\"reason\": reason}, extra={\"spider\": spider}\n        )\n\n        dfd = self.slot.close()\n\n        def log_failure(msg: str) -> Callable[[Failure], None]:\n            def errback(failure: Failure) -> None:\n                logger.error(\n                    msg, exc_info=failure_to_exc_info(failure), extra={\"spider\": spider}\n                )\n\n            return errback\n\n        dfd.addBoth(lambda _: self.downloader.close())\n        dfd.addErrback(log_failure(\"Downloader close failure\"))\n\n        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n        dfd.addErrback(log_failure(\"Scraper close failure\"))\n\n        if hasattr(self.slot.scheduler, \"close\"):\n            dfd.addBoth(lambda _: cast(Slot, self.slot).scheduler.close(reason))\n            dfd.addErrback(log_failure(\"Scheduler close failure\"))\n\n        dfd.addBoth(\n            lambda _: self.signals.send_catch_log_deferred(\n                signal=signals.spider_closed,\n                spider=spider,\n                reason=reason,\n            )\n        )\n        dfd.addErrback(log_failure(\"Error while sending spider_close signal\"))\n\n        def close_stats(_: Any) -> None:\n            assert self.crawler.stats\n            self.crawler.stats.close_spider(spider, reason=reason)\n\n        dfd.addBoth(close_stats)\n        dfd.addErrback(log_failure(\"Stats close failure\"))\n\n        dfd.addBoth(\n            lambda _: logger.info(\n                \"Spider closed (%(reason)s)\",\n                {\"reason\": reason},\n                extra={\"spider\": spider},\n            )\n        )\n\n        dfd.addBoth(lambda _: setattr(self, \"slot\", None))\n        dfd.addErrback(log_failure(\"Error while unassigning slot\"))\n\n        dfd.addBoth(lambda _: setattr(self, \"spider\", None))\n        dfd.addErrback(log_failure(\"Error while unassigning spider\"))\n\n        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n        return dfd\n", "scrapy/core/scraper.py": "\"\"\"This module implements the Scraper component which parses responses and\nextracts information from them\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections import deque\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    Deque,\n    Generator,\n    Iterable,\n    Iterator,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom itemadapter import is_item\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider, signals\nfrom scrapy.core.spidermw import SpiderMiddlewareManager\nfrom scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.pipelines import ItemPipelineManager\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.utils.defer import (\n    DeferredListResultListT,\n    aiter_errback,\n    defer_fail,\n    defer_succeed,\n    iter_errback,\n    parallel,\n    parallel_async,\n)\nfrom scrapy.utils.log import failure_to_exc_info, logformatter_adapter\nfrom scrapy.utils.misc import load_object, warn_on_generator_with_return_value\nfrom scrapy.utils.spider import iterate_spider_output\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nlogger = logging.getLogger(__name__)\n\n\n_T = TypeVar(\"_T\")\n_ParallelResult = DeferredListResultListT[Iterator[Any]]\n\nif TYPE_CHECKING:\n    # parameterized Deferreds require Twisted 21.7.0\n    _HandleOutputDeferred = Deferred[Union[_ParallelResult, None]]\n    QueueTuple = Tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n\n\nclass Slot:\n    \"\"\"Scraper slot (one per running spider)\"\"\"\n\n    MIN_RESPONSE_SIZE = 1024\n\n    def __init__(self, max_active_size: int = 5000000):\n        self.max_active_size = max_active_size\n        self.queue: Deque[QueueTuple] = deque()\n        self.active: Set[Request] = set()\n        self.active_size: int = 0\n        self.itemproc_size: int = 0\n        self.closing: Optional[Deferred[Spider]] = None\n\n    def add_response_request(\n        self, result: Union[Response, Failure], request: Request\n    ) -> _HandleOutputDeferred:\n        deferred: _HandleOutputDeferred = Deferred()\n        self.queue.append((result, request, deferred))\n        if isinstance(result, Response):\n            self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred\n\n    def next_response_request_deferred(self) -> QueueTuple:\n        response, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return response, request, deferred\n\n    def finish_response(\n        self, result: Union[Response, Failure], request: Request\n    ) -> None:\n        self.active.remove(request)\n        if isinstance(result, Response):\n            self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE\n\n    def is_idle(self) -> bool:\n        return not (self.queue or self.active)\n\n    def needs_backout(self) -> bool:\n        return self.active_size > self.max_active_size\n\n\nclass Scraper:\n    def __init__(self, crawler: Crawler) -> None:\n        self.slot: Optional[Slot] = None\n        self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(\n            crawler\n        )\n        itemproc_cls: Type[ItemPipelineManager] = load_object(\n            crawler.settings[\"ITEM_PROCESSOR\"]\n        )\n        self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n        self.concurrent_items: int = crawler.settings.getint(\"CONCURRENT_ITEMS\")\n        self.crawler: Crawler = crawler\n        self.signals: SignalManager = crawler.signals\n        assert crawler.logformatter\n        self.logformatter: LogFormatter = crawler.logformatter\n\n    @inlineCallbacks\n    def open_spider(self, spider: Spider) -> Generator[Deferred[Any], Any, None]:\n        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n        self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n        yield self.itemproc.open_spider(spider)\n\n    def close_spider(self, spider: Spider) -> Deferred[Spider]:\n        \"\"\"Close a spider being scraped and release its resources\"\"\"\n        if self.slot is None:\n            raise RuntimeError(\"Scraper slot not assigned\")\n        self.slot.closing = Deferred()\n        self.slot.closing.addCallback(self.itemproc.close_spider)\n        self._check_if_closing(spider)\n        return self.slot.closing\n\n    def is_idle(self) -> bool:\n        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n        return not self.slot\n\n    def _check_if_closing(self, spider: Spider) -> None:\n        assert self.slot is not None  # typing\n        if self.slot.closing and self.slot.is_idle():\n            self.slot.closing.callback(spider)\n\n    def enqueue_scrape(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> _HandleOutputDeferred:\n        if self.slot is None:\n            raise RuntimeError(\"Scraper slot not assigned\")\n        dfd = self.slot.add_response_request(result, request)\n\n        def finish_scraping(_: _T) -> _T:\n            assert self.slot is not None\n            self.slot.finish_response(result, request)\n            self._check_if_closing(spider)\n            self._scrape_next(spider)\n            return _\n\n        dfd.addBoth(finish_scraping)\n        dfd.addErrback(\n            lambda f: logger.error(\n                \"Scraper bug processing %(request)s\",\n                {\"request\": request},\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": spider},\n            )\n        )\n        self._scrape_next(spider)\n        return dfd\n\n    def _scrape_next(self, spider: Spider) -> None:\n        assert self.slot is not None  # typing\n        while self.slot.queue:\n            response, request, deferred = self.slot.next_response_request_deferred()\n            self._scrape(response, request, spider).chainDeferred(deferred)\n\n    def _scrape(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> _HandleOutputDeferred:\n        \"\"\"\n        Handle the downloaded response or failure through the spider callback/errback\n        \"\"\"\n        if not isinstance(result, (Response, Failure)):\n            raise TypeError(\n                f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n            )\n        dfd: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = self._scrape2(\n            result, request, spider\n        )  # returns spider's processed output\n        dfd.addErrback(self.handle_spider_error, request, result, spider)\n        dfd2: _HandleOutputDeferred = dfd.addCallback(\n            self.handle_spider_output, request, cast(Response, result), spider\n        )\n        return dfd2\n\n    def _scrape2(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n        \"\"\"\n        Handle the different cases of request's result been a Response or a Failure\n        \"\"\"\n        if isinstance(result, Response):\n            # Deferreds are invariant so Mutable*Chain isn't matched to *Iterable\n            return self.spidermw.scrape_response(  # type: ignore[return-value]\n                self.call_spider, result, request, spider\n            )\n        # else result is a Failure\n        dfd = self.call_spider(result, request, spider)\n        dfd.addErrback(self._log_download_errors, result, request, spider)\n        return dfd\n\n    def call_spider(\n        self, result: Union[Response, Failure], request: Request, spider: Spider\n    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n        dfd: Deferred[Any]\n        if isinstance(result, Response):\n            if getattr(result, \"request\", None) is None:\n                result.request = request\n            assert result.request\n            callback = result.request.callback or spider._parse\n            warn_on_generator_with_return_value(spider, callback)\n            dfd = defer_succeed(result)\n            dfd.addCallbacks(\n                callback=callback, callbackKeywords=result.request.cb_kwargs\n            )\n        else:  # result is a Failure\n            # TODO: properly type adding this attribute to a Failure\n            result.request = request  # type: ignore[attr-defined]\n            dfd = defer_fail(result)\n            if request.errback:\n                warn_on_generator_with_return_value(spider, request.errback)\n                dfd.addErrback(request.errback)\n        dfd2: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = dfd.addCallback(\n            iterate_spider_output\n        )\n        return dfd2\n\n    def handle_spider_error(\n        self,\n        _failure: Failure,\n        request: Request,\n        response: Union[Response, Failure],\n        spider: Spider,\n    ) -> None:\n        exc = _failure.value\n        if isinstance(exc, CloseSpider):\n            assert self.crawler.engine is not None  # typing\n            self.crawler.engine.close_spider(spider, exc.reason or \"cancelled\")\n            return\n        logkws = self.logformatter.spider_error(_failure, request, response, spider)\n        logger.log(\n            *logformatter_adapter(logkws),\n            exc_info=failure_to_exc_info(_failure),\n            extra={\"spider\": spider},\n        )\n        self.signals.send_catch_log(\n            signal=signals.spider_error,\n            failure=_failure,\n            response=response,\n            spider=spider,\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\n            f\"spider_exceptions/{_failure.value.__class__.__name__}\", spider=spider\n        )\n\n    def handle_spider_output(\n        self,\n        result: Union[Iterable[_T], AsyncIterable[_T]],\n        request: Request,\n        response: Response,\n        spider: Spider,\n    ) -> _HandleOutputDeferred:\n        if not result:\n            return defer_succeed(None)\n        it: Union[Iterable[_T], AsyncIterable[_T]]\n        dfd: Deferred[_ParallelResult]\n        if isinstance(result, AsyncIterable):\n            it = aiter_errback(\n                result, self.handle_spider_error, request, response, spider\n            )\n            dfd = parallel_async(\n                it,\n                self.concurrent_items,\n                self._process_spidermw_output,\n                request,\n                response,\n                spider,\n            )\n        else:\n            it = iter_errback(\n                result, self.handle_spider_error, request, response, spider\n            )\n            dfd = parallel(\n                it,\n                self.concurrent_items,\n                self._process_spidermw_output,\n                request,\n                response,\n                spider,\n            )\n        # returning Deferred[_ParallelResult] instead of Deferred[Union[_ParallelResult, None]]\n        return dfd  # type: ignore[return-value]\n\n    def _process_spidermw_output(\n        self, output: Any, request: Request, response: Response, spider: Spider\n    ) -> Optional[Deferred[Any]]:\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider\n        \"\"\"\n        assert self.slot is not None  # typing\n        if isinstance(output, Request):\n            assert self.crawler.engine is not None  # typing\n            self.crawler.engine.crawl(request=output)\n        elif is_item(output):\n            self.slot.itemproc_size += 1\n            dfd = self.itemproc.process_item(output, spider)\n            dfd.addBoth(self._itemproc_finished, output, response, spider)\n            return dfd\n        elif output is None:\n            pass\n        else:\n            typename = type(output).__name__\n            logger.error(\n                \"Spider must return request, item, or None, got %(typename)r in %(request)s\",\n                {\"request\": request, \"typename\": typename},\n                extra={\"spider\": spider},\n            )\n        return None\n\n    def _log_download_errors(\n        self,\n        spider_failure: Failure,\n        download_failure: Failure,\n        request: Request,\n        spider: Spider,\n    ) -> Union[Failure, None]:\n        \"\"\"Log and silence errors that come from the engine (typically download\n        errors that got propagated thru here).\n\n        spider_failure: the value passed into the errback of self.call_spider()\n        download_failure: the value passed into _scrape2() from\n        ExecutionEngine._handle_downloader_output() as \"result\"\n        \"\"\"\n        if not download_failure.check(IgnoreRequest):\n            if download_failure.frames:\n                logkws = self.logformatter.download_error(\n                    download_failure, request, spider\n                )\n                logger.log(\n                    *logformatter_adapter(logkws),\n                    extra={\"spider\": spider},\n                    exc_info=failure_to_exc_info(download_failure),\n                )\n            else:\n                errmsg = download_failure.getErrorMessage()\n                if errmsg:\n                    logkws = self.logformatter.download_error(\n                        download_failure, request, spider, errmsg\n                    )\n                    logger.log(\n                        *logformatter_adapter(logkws),\n                        extra={\"spider\": spider},\n                    )\n\n        if spider_failure is not download_failure:\n            return spider_failure\n        return None\n\n    def _itemproc_finished(\n        self, output: Any, item: Any, response: Response, spider: Spider\n    ) -> Deferred:\n        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n        assert self.slot is not None  # typing\n        self.slot.itemproc_size -= 1\n        if isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                logkws = self.logformatter.dropped(item, ex, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_dropped,\n                    item=item,\n                    response=response,\n                    spider=spider,\n                    exception=output.value,\n                )\n            assert ex\n            logkws = self.logformatter.item_error(item, ex, response, spider)\n            logger.log(\n                *logformatter_adapter(logkws),\n                extra={\"spider\": spider},\n                exc_info=failure_to_exc_info(output),\n            )\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_error,\n                item=item,\n                response=response,\n                spider=spider,\n                failure=output,\n            )\n        logkws = self.logformatter.scraped(output, response, spider)\n        if logkws is not None:\n            logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n        return self.signals.send_catch_log_deferred(\n            signal=signals.item_scraped, item=output, response=response, spider=spider\n        )\n", "scrapy/core/spidermw.py": "\"\"\"\nSpider Middleware manager\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom inspect import isasyncgenfunction, iscoroutine\nfrom itertools import islice\nfrom typing import (\n    Any,\n    AsyncIterable,\n    Callable,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Response\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import (\n    deferred_f_from_coro_f,\n    deferred_from_coro,\n    maybe_deferred_to_future,\n    mustbe_deferred,\n)\nfrom scrapy.utils.python import MutableAsyncChain, MutableChain\n\nlogger = logging.getLogger(__name__)\n\n\n_T = TypeVar(\"_T\")\nScrapeFunc = Callable[\n    [Union[Response, Failure], Request, Spider], Union[Iterable[_T], AsyncIterable[_T]]\n]\n\n\ndef _isiterable(o: Any) -> bool:\n    return isinstance(o, (Iterable, AsyncIterable))\n\n\nclass SpiderMiddlewareManager(MiddlewareManager):\n    component_name = \"spider middleware\"\n\n    def __init__(self, *middlewares: Any):\n        super().__init__(*middlewares)\n        self.downgrade_warning_done = False\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n\n    def _add_middleware(self, mw: Any) -> None:\n        super()._add_middleware(mw)\n        if hasattr(mw, \"process_spider_input\"):\n            self.methods[\"process_spider_input\"].append(mw.process_spider_input)\n        if hasattr(mw, \"process_start_requests\"):\n            self.methods[\"process_start_requests\"].appendleft(mw.process_start_requests)\n        process_spider_output = self._get_async_method_pair(mw, \"process_spider_output\")\n        self.methods[\"process_spider_output\"].appendleft(process_spider_output)\n        process_spider_exception = getattr(mw, \"process_spider_exception\", None)\n        self.methods[\"process_spider_exception\"].appendleft(process_spider_exception)\n\n    def _process_spider_input(\n        self,\n        scrape_func: ScrapeFunc,\n        response: Response,\n        request: Request,\n        spider: Spider,\n    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n        for method in self.methods[\"process_spider_input\"]:\n            method = cast(Callable, method)\n            try:\n                result = method(response=response, spider=spider)\n                if result is not None:\n                    msg = (\n                        f\"{method.__qualname__} must return None \"\n                        f\"or raise an exception, got {type(result)}\"\n                    )\n                    raise _InvalidOutput(msg)\n            except _InvalidOutput:\n                raise\n            except Exception:\n                return scrape_func(Failure(), request, spider)\n        return scrape_func(response, request, spider)\n\n    def _evaluate_iterable(\n        self,\n        response: Response,\n        spider: Spider,\n        iterable: Union[Iterable[_T], AsyncIterable[_T]],\n        exception_processor_index: int,\n        recover_to: Union[MutableChain[_T], MutableAsyncChain[_T]],\n    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n        def process_sync(iterable: Iterable[_T]) -> Iterable[_T]:\n            try:\n                yield from iterable\n            except Exception as ex:\n                exception_result = cast(\n                    Union[Failure, MutableChain[_T]],\n                    self._process_spider_exception(\n                        response, spider, Failure(ex), exception_processor_index\n                    ),\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                assert isinstance(recover_to, MutableChain)\n                recover_to.extend(exception_result)\n\n        async def process_async(iterable: AsyncIterable[_T]) -> AsyncIterable[_T]:\n            try:\n                async for r in iterable:\n                    yield r\n            except Exception as ex:\n                exception_result = cast(\n                    Union[Failure, MutableAsyncChain[_T]],\n                    self._process_spider_exception(\n                        response, spider, Failure(ex), exception_processor_index\n                    ),\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                assert isinstance(recover_to, MutableAsyncChain)\n                recover_to.extend(exception_result)\n\n        if isinstance(iterable, AsyncIterable):\n            return process_async(iterable)\n        return process_sync(iterable)\n\n    def _process_spider_exception(\n        self,\n        response: Response,\n        spider: Spider,\n        _failure: Failure,\n        start_index: int = 0,\n    ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n        exception = _failure.value\n        # don't handle _InvalidOutput exception\n        if isinstance(exception, _InvalidOutput):\n            return _failure\n        method_list = islice(\n            self.methods[\"process_spider_exception\"], start_index, None\n        )\n        for method_index, method in enumerate(method_list, start=start_index):\n            if method is None:\n                continue\n            method = cast(Callable, method)\n            result = method(response=response, exception=exception, spider=spider)\n            if _isiterable(result):\n                # stop exception handling by handing control over to the\n                # process_spider_output chain if an iterable has been returned\n                dfd: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n                    self._process_spider_output(\n                        response, spider, result, method_index + 1\n                    )\n                )\n                # _process_spider_output() returns a Deferred only because of downgrading so this can be\n                # simplified when downgrading is removed.\n                if dfd.called:\n                    # the result is available immediately if _process_spider_output didn't do downgrading\n                    return cast(\n                        Union[MutableChain[_T], MutableAsyncChain[_T]], dfd.result\n                    )\n                # we forbid waiting here because otherwise we would need to return a deferred from\n                # _process_spider_exception too, which complicates the architecture\n                msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n                raise _InvalidOutput(msg)\n            elif result is None:\n                continue\n            else:\n                msg = (\n                    f\"{method.__qualname__} must return None \"\n                    f\"or an iterable, got {type(result)}\"\n                )\n                raise _InvalidOutput(msg)\n        return _failure\n\n    # This method cannot be made async def, as _process_spider_exception relies on the Deferred result\n    # being available immediately which doesn't work when it's a wrapped coroutine.\n    # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.\n    @inlineCallbacks\n    def _process_spider_output(\n        self,\n        response: Response,\n        spider: Spider,\n        result: Union[Iterable[_T], AsyncIterable[_T]],\n        start_index: int = 0,\n    ) -> Generator[Deferred[Any], Any, Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n        # items in this iterable do not need to go through the process_spider_output\n        # chain, they went through it already from the process_spider_exception method\n        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n        last_result_is_async = isinstance(result, AsyncIterable)\n        if last_result_is_async:\n            recovered = MutableAsyncChain()\n        else:\n            recovered = MutableChain()\n\n        # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.\n        # 1. def foo. Sync iterables are passed as is, async ones are downgraded.\n        # 2. async def foo. Sync iterables are upgraded, async ones are passed as is.\n        # 3. def foo + async def foo_async. Iterables are passed to the respective method.\n        # Storing methods and method tuples in the same list is weird but we should be able to roll this back\n        # when we drop this compatibility feature.\n\n        method_list = islice(self.methods[\"process_spider_output\"], start_index, None)\n        for method_index, method_pair in enumerate(method_list, start=start_index):\n            if method_pair is None:\n                continue\n            need_upgrade = need_downgrade = False\n            if isinstance(method_pair, tuple):\n                # This tuple handling is only needed until _async compatibility methods are removed.\n                method_sync, method_async = method_pair\n                method = method_async if last_result_is_async else method_sync\n            else:\n                method = method_pair\n                if not last_result_is_async and isasyncgenfunction(method):\n                    need_upgrade = True\n                elif last_result_is_async and not isasyncgenfunction(method):\n                    need_downgrade = True\n            try:\n                if need_upgrade:\n                    # Iterable -> AsyncIterable\n                    result = as_async_generator(result)\n                elif need_downgrade:\n                    if not self.downgrade_warning_done:\n                        logger.warning(\n                            f\"Async iterable passed to {method.__qualname__} \"\n                            f\"was downgraded to a non-async one\"\n                        )\n                        self.downgrade_warning_done = True\n                    assert isinstance(result, AsyncIterable)\n                    # AsyncIterable -> Iterable\n                    result = yield deferred_from_coro(collect_asyncgen(result))\n                    if isinstance(recovered, AsyncIterable):\n                        recovered_collected = yield deferred_from_coro(\n                            collect_asyncgen(recovered)\n                        )\n                        recovered = MutableChain(recovered_collected)\n                # might fail directly if the output value is not a generator\n                result = method(response=response, result=result, spider=spider)\n            except Exception as ex:\n                exception_result: Union[\n                    Failure, MutableChain[_T], MutableAsyncChain[_T]\n                ] = self._process_spider_exception(\n                    response, spider, Failure(ex), method_index + 1\n                )\n                if isinstance(exception_result, Failure):\n                    raise\n                return exception_result\n            if _isiterable(result):\n                result = self._evaluate_iterable(\n                    response, spider, result, method_index + 1, recovered\n                )\n            else:\n                if iscoroutine(result):\n                    result.close()  # Silence warning about not awaiting\n                    msg = (\n                        f\"{method.__qualname__} must be an asynchronous \"\n                        f\"generator (i.e. use yield)\"\n                    )\n                else:\n                    msg = (\n                        f\"{method.__qualname__} must return an iterable, got \"\n                        f\"{type(result)}\"\n                    )\n                raise _InvalidOutput(msg)\n            last_result_is_async = isinstance(result, AsyncIterable)\n\n        if last_result_is_async:\n            return MutableAsyncChain(result, recovered)\n        return MutableChain(result, recovered)  # type: ignore[arg-type]\n\n    async def _process_callback_output(\n        self,\n        response: Response,\n        spider: Spider,\n        result: Union[Iterable[_T], AsyncIterable[_T]],\n    ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n        if isinstance(result, AsyncIterable):\n            recovered = MutableAsyncChain()\n        else:\n            recovered = MutableChain()\n        result = self._evaluate_iterable(response, spider, result, 0, recovered)\n        result = await maybe_deferred_to_future(\n            self._process_spider_output(response, spider, result)\n        )\n        if isinstance(result, AsyncIterable):\n            return MutableAsyncChain(result, recovered)\n        if isinstance(recovered, AsyncIterable):\n            recovered_collected = await collect_asyncgen(recovered)\n            recovered = MutableChain(recovered_collected)\n        return MutableChain(result, recovered)\n\n    def scrape_response(\n        self,\n        scrape_func: ScrapeFunc,\n        response: Response,\n        request: Request,\n        spider: Spider,\n    ) -> Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n        async def process_callback_output(\n            result: Union[Iterable[_T], AsyncIterable[_T]]\n        ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n            return await self._process_callback_output(response, spider, result)\n\n        def process_spider_exception(\n            _failure: Failure,\n        ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n            return self._process_spider_exception(response, spider, _failure)\n\n        dfd: Deferred[Union[Iterable[_T], AsyncIterable[_T]]] = mustbe_deferred(\n            self._process_spider_input, scrape_func, response, request, spider\n        )\n        dfd2: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n            dfd.addCallback(deferred_f_from_coro_f(process_callback_output))\n        )\n        dfd2.addErrback(process_spider_exception)\n        return dfd2\n\n    def process_start_requests(\n        self, start_requests: Iterable[Request], spider: Spider\n    ) -> Deferred:\n        return self._process_chain(\"process_start_requests\", start_requests, spider)\n\n    # This method is only needed until _async compatibility methods are removed.\n    @staticmethod\n    def _get_async_method_pair(\n        mw: Any, methodname: str\n    ) -> Union[None, Callable, Tuple[Callable, Callable]]:\n        normal_method: Optional[Callable] = getattr(mw, methodname, None)\n        methodname_async = methodname + \"_async\"\n        async_method: Optional[Callable] = getattr(mw, methodname_async, None)\n        if not async_method:\n            return normal_method\n        if not normal_method:\n            logger.error(\n                f\"Middleware {mw.__qualname__} has {methodname_async} \"\n                f\"without {methodname}, skipping this method.\"\n            )\n            return None\n        if not isasyncgenfunction(async_method):\n            logger.error(\n                f\"{async_method.__qualname__} is not \"\n                f\"an async generator function, skipping this method.\"\n            )\n            return normal_method\n        if isasyncgenfunction(normal_method):\n            logger.error(\n                f\"{normal_method.__qualname__} is an async \"\n                f\"generator function while {methodname_async} exists, \"\n                f\"skipping both methods.\"\n            )\n            return None\n        return normal_method, async_method\n", "scrapy/core/scheduler.py": "from __future__ import annotations\n\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, List, Optional, Type, cast\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.dupefilters import BaseDupeFilter\nfrom scrapy.http.request import Request\nfrom scrapy.pqueues import ScrapyPriorityQueue\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    # requires queuelib >= 1.6.2\n    from queuelib.queue import BaseQueue\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseSchedulerMeta(type):\n    \"\"\"\n    Metaclass to check scheduler classes against the necessary interface\n    \"\"\"\n\n    def __instancecheck__(cls, instance: Any) -> bool:\n        return cls.__subclasscheck__(type(instance))\n\n    def __subclasscheck__(cls, subclass: type) -> bool:\n        return (\n            hasattr(subclass, \"has_pending_requests\")\n            and callable(subclass.has_pending_requests)\n            and hasattr(subclass, \"enqueue_request\")\n            and callable(subclass.enqueue_request)\n            and hasattr(subclass, \"next_request\")\n            and callable(subclass.next_request)\n        )\n\n\nclass BaseScheduler(metaclass=BaseSchedulerMeta):\n    \"\"\"\n    The scheduler component is responsible for storing requests received from\n    the engine, and feeding them back upon request (also to the engine).\n\n    The original sources of said requests are:\n\n    * Spider: ``start_requests`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n    * Spider middleware: ``process_spider_output`` and ``process_spider_exception`` methods\n    * Downloader middleware: ``process_request``, ``process_response`` and ``process_exception`` methods\n\n    The order in which the scheduler returns its stored requests (via the ``next_request`` method)\n    plays a great part in determining the order in which those requests are downloaded.\n\n    The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.\n    \"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.\n        \"\"\"\n        return cls()\n\n    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n        \"\"\"\n        Called when the spider is opened by the engine. It receives the spider\n        instance as argument and it's useful to execute initialization code.\n\n        :param spider: the spider object for the current crawl\n        :type spider: :class:`~scrapy.spiders.Spider`\n        \"\"\"\n        pass\n\n    def close(self, reason: str) -> Optional[Deferred[None]]:\n        \"\"\"\n        Called when the spider is closed by the engine. It receives the reason why the crawl\n        finished as argument and it's useful to execute cleaning code.\n\n        :param reason: a string which describes the reason why the spider was closed\n        :type reason: :class:`str`\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def has_pending_requests(self) -> bool:\n        \"\"\"\n        ``True`` if the scheduler has enqueued requests, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Process a request received by the engine.\n\n        Return ``True`` if the request is stored correctly, ``False`` otherwise.\n\n        If ``False``, the engine will fire a ``request_dropped`` signal, and\n        will not make further attempts to schedule the request at a later time.\n        For reference, the default Scrapy scheduler returns ``False`` when the\n        request is rejected by the dupefilter.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def next_request(self) -> Optional[Request]:\n        \"\"\"\n        Return the next :class:`~scrapy.http.Request` to be processed, or ``None``\n        to indicate that there are no requests to be considered ready at the moment.\n\n        Returning ``None`` implies that no request from the scheduler will be sent\n        to the downloader in the current reactor cycle. The engine will continue\n        calling ``next_request`` until ``has_pending_requests`` is ``False``.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass Scheduler(BaseScheduler):\n    \"\"\"\n    Default Scrapy scheduler. This implementation also handles duplication\n    filtering via the :setting:`dupefilter <DUPEFILTER_CLASS>`.\n\n    This scheduler stores requests into several priority queues (defined by the\n    :setting:`SCHEDULER_PRIORITY_QUEUE` setting). In turn, said priority queues\n    are backed by either memory or disk based queues (respectively defined by the\n    :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` settings).\n\n    Request prioritization is almost entirely delegated to the priority queue. The only\n    prioritization performed by this scheduler is using the disk-based queue if present\n    (i.e. if the :setting:`JOBDIR` setting is defined) and falling back to the memory-based\n    queue if a serialization error occurs. If the disk queue is not present, the memory one\n    is used directly.\n\n    :param dupefilter: An object responsible for checking and filtering duplicate requests.\n                       The value for the :setting:`DUPEFILTER_CLASS` setting is used by default.\n    :type dupefilter: :class:`scrapy.dupefilters.BaseDupeFilter` instance or similar:\n                      any class that implements the `BaseDupeFilter` interface\n\n    :param jobdir: The path of a directory to be used for persisting the crawl's state.\n                   The value for the :setting:`JOBDIR` setting is used by default.\n                   See :ref:`topics-jobs`.\n    :type jobdir: :class:`str` or ``None``\n\n    :param dqclass: A class to be used as persistent request queue.\n                    The value for the :setting:`SCHEDULER_DISK_QUEUE` setting is used by default.\n    :type dqclass: class\n\n    :param mqclass: A class to be used as non-persistent request queue.\n                    The value for the :setting:`SCHEDULER_MEMORY_QUEUE` setting is used by default.\n    :type mqclass: class\n\n    :param logunser: A boolean that indicates whether or not unserializable requests should be logged.\n                     The value for the :setting:`SCHEDULER_DEBUG` setting is used by default.\n    :type logunser: bool\n\n    :param stats: A stats collector object to record stats about the request scheduling process.\n                  The value for the :setting:`STATS_CLASS` setting is used by default.\n    :type stats: :class:`scrapy.statscollectors.StatsCollector` instance or similar:\n                 any class that implements the `StatsCollector` interface\n\n    :param pqclass: A class to be used as priority queue for requests.\n                    The value for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting is used by default.\n    :type pqclass: class\n\n    :param crawler: The crawler object corresponding to the current crawl.\n    :type crawler: :class:`scrapy.crawler.Crawler`\n    \"\"\"\n\n    def __init__(\n        self,\n        dupefilter: BaseDupeFilter,\n        jobdir: Optional[str] = None,\n        dqclass: Optional[Type[BaseQueue]] = None,\n        mqclass: Optional[Type[BaseQueue]] = None,\n        logunser: bool = False,\n        stats: Optional[StatsCollector] = None,\n        pqclass: Optional[Type[ScrapyPriorityQueue]] = None,\n        crawler: Optional[Crawler] = None,\n    ):\n        self.df: BaseDupeFilter = dupefilter\n        self.dqdir: Optional[str] = self._dqdir(jobdir)\n        self.pqclass: Optional[Type[ScrapyPriorityQueue]] = pqclass\n        self.dqclass: Optional[Type[BaseQueue]] = dqclass\n        self.mqclass: Optional[Type[BaseQueue]] = mqclass\n        self.logunser: bool = logunser\n        self.stats: Optional[StatsCollector] = stats\n        self.crawler: Optional[Crawler] = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method, initializes the scheduler with arguments taken from the crawl settings\n        \"\"\"\n        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n        return cls(\n            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n            jobdir=job_dir(crawler.settings),\n            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n            stats=crawler.stats,\n            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n            crawler=crawler,\n        )\n\n    def has_pending_requests(self) -> bool:\n        return len(self) > 0\n\n    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n        \"\"\"\n        (1) initialize the memory queue\n        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n        (3) return the result of the dupefilter's ``open`` method\n        \"\"\"\n        self.spider: Spider = spider\n        self.mqs: ScrapyPriorityQueue = self._mq()\n        self.dqs: Optional[ScrapyPriorityQueue] = self._dq() if self.dqdir else None\n        return self.df.open()\n\n    def close(self, reason: str) -> Optional[Deferred[None]]:\n        \"\"\"\n        (1) dump pending requests to disk if there is a disk queue\n        (2) return the result of the dupefilter's ``close`` method\n        \"\"\"\n        if self.dqs is not None:\n            state = self.dqs.close()\n            assert isinstance(self.dqdir, str)\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)\n\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Unless the received request is filtered out by the Dupefilter, attempt to push\n        it into the disk queue, falling back to pushing it into the memory queue.\n\n        Increment the appropriate stats, such as: ``scheduler/enqueued``,\n        ``scheduler/enqueued/disk``, ``scheduler/enqueued/memory``.\n\n        Return ``True`` if the request was stored successfully, ``False`` otherwise.\n        \"\"\"\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        assert self.stats is not None\n        if dqok:\n            self.stats.inc_value(\"scheduler/enqueued/disk\", spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value(\"scheduler/enqueued/memory\", spider=self.spider)\n        self.stats.inc_value(\"scheduler/enqueued\", spider=self.spider)\n        return True\n\n    def next_request(self) -> Optional[Request]:\n        \"\"\"\n        Return a :class:`~scrapy.http.Request` object from the memory queue,\n        falling back to the disk queue if the memory queue is empty.\n        Return ``None`` if there are no more enqueued requests.\n\n        Increment the appropriate stats, such as: ``scheduler/dequeued``,\n        ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n        \"\"\"\n        request: Optional[Request] = self.mqs.pop()\n        assert self.stats is not None\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued/memory\", spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request is not None:\n                self.stats.inc_value(\"scheduler/dequeued/disk\", spider=self.spider)\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued\", spider=self.spider)\n        return request\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the total amount of enqueued requests\n        \"\"\"\n        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)\n\n    def _dqpush(self, request: Request) -> bool:\n        if self.dqs is None:\n            return False\n        try:\n            self.dqs.push(request)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\n                    \"Unable to serialize request: %(request)s - reason:\"\n                    \" %(reason)s - no more unserializable requests will be\"\n                    \" logged (stats being collected)\"\n                )\n                logger.warning(\n                    msg,\n                    {\"request\": request, \"reason\": e},\n                    exc_info=True,\n                    extra={\"spider\": self.spider},\n                )\n                self.logunser = False\n            assert self.stats is not None\n            self.stats.inc_value(\"scheduler/unserializable\", spider=self.spider)\n            return False\n        else:\n            return True\n\n    def _mqpush(self, request: Request) -> None:\n        self.mqs.push(request)\n\n    def _dqpop(self) -> Optional[Request]:\n        if self.dqs is not None:\n            return self.dqs.pop()\n        return None\n\n    def _mq(self) -> ScrapyPriorityQueue:\n        \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n        assert self.crawler\n        assert self.pqclass\n        return build_from_crawler(\n            self.pqclass,\n            self.crawler,\n            downstream_queue_cls=self.mqclass,\n            key=\"\",\n        )\n\n    def _dq(self) -> ScrapyPriorityQueue:\n        \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n        assert self.crawler\n        assert self.dqdir\n        assert self.pqclass\n        state = self._read_dqs_state(self.dqdir)\n        q = build_from_crawler(\n            self.pqclass,\n            self.crawler,\n            downstream_queue_cls=self.dqclass,\n            key=self.dqdir,\n            startprios=state,\n        )\n        if q:\n            logger.info(\n                \"Resuming crawl (%(queuesize)d requests scheduled)\",\n                {\"queuesize\": len(q)},\n                extra={\"spider\": self.spider},\n            )\n        return q\n\n    def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n        \"\"\"Return a folder name to keep disk queue state at\"\"\"\n        if jobdir:\n            dqdir = Path(jobdir, \"requests.queue\")\n            if not dqdir.exists():\n                dqdir.mkdir(parents=True)\n            return str(dqdir)\n        return None\n\n    def _read_dqs_state(self, dqdir: str) -> List[int]:\n        path = Path(dqdir, \"active.json\")\n        if not path.exists():\n            return []\n        with path.open(encoding=\"utf-8\") as f:\n            return cast(List[int], json.load(f))\n\n    def _write_dqs_state(self, dqdir: str, state: List[int]) -> None:\n        with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(state, f)\n", "scrapy/core/__init__.py": "\"\"\"\nScrapy core library classes and functions.\n\"\"\"\n", "scrapy/core/downloader/contextfactory.py": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, List, Optional\n\nfrom OpenSSL import SSL\nfrom twisted.internet._sslverify import _setAcceptableProtocols\nfrom twisted.internet.ssl import (\n    AcceptableCiphers,\n    CertificateOptions,\n    optionsForClientTLS,\n    platformTrust,\n)\nfrom twisted.web.client import BrowserLikePolicyForHTTPS\nfrom twisted.web.iweb import IPolicyForHTTPS\nfrom zope.interface.declarations import implementer\nfrom zope.interface.verify import verifyObject\n\nfrom scrapy.core.downloader.tls import (\n    DEFAULT_CIPHERS,\n    ScrapyClientTLSOptions,\n    openssl_methods,\n)\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import build_from_crawler, load_object\n\nif TYPE_CHECKING:\n    from twisted.internet._sslverify import ClientTLSOptions\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n@implementer(IPolicyForHTTPS)\nclass ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n    \"\"\"\n    Non-peer-certificate verifying HTTPS context factory\n\n    Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)\n    which allows TLS protocol negotiation\n\n    'A TLS/SSL connection established with [this method] may\n     understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n    \"\"\"\n\n    def __init__(\n        self,\n        method: int = SSL.SSLv23_METHOD,\n        tls_verbose_logging: bool = False,\n        tls_ciphers: Optional[str] = None,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n        self._ssl_method: int = method\n        self.tls_verbose_logging: bool = tls_verbose_logging\n        self.tls_ciphers: AcceptableCiphers\n        if tls_ciphers:\n            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n        else:\n            self.tls_ciphers = DEFAULT_CIPHERS\n\n    @classmethod\n    def from_settings(\n        cls,\n        settings: BaseSettings,\n        method: int = SSL.SSLv23_METHOD,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Self:\n        tls_verbose_logging: bool = settings.getbool(\n            \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n        )\n        tls_ciphers: Optional[str] = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n        return cls(  # type: ignore[misc]\n            method=method,\n            tls_verbose_logging=tls_verbose_logging,\n            tls_ciphers=tls_ciphers,\n            *args,\n            **kwargs,\n        )\n\n    def getCertificateOptions(self) -> CertificateOptions:\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n\n        # backward-compatible SSL/TLS method:\n        #\n        # * this will respect `method` attribute in often recommended\n        #   `ScrapyClientContextFactory` subclass\n        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n        #\n        # * getattr() for `_ssl_method` attribute for context factories\n        #   not calling super().__init__\n        return CertificateOptions(\n            verify=False,\n            method=getattr(self, \"method\", getattr(self, \"_ssl_method\", None)),\n            fixBrokenPeers=True,\n            acceptableCiphers=self.tls_ciphers,\n        )\n\n    # kept for old-style HTTP/1.0 downloader context twisted calls,\n    # e.g. connectSSL()\n    def getContext(self, hostname: Any = None, port: Any = None) -> SSL.Context:\n        ctx: SSL.Context = self.getCertificateOptions().getContext()\n        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n        return ctx\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        return ScrapyClientTLSOptions(\n            hostname.decode(\"ascii\"),\n            self.getContext(),\n            verbose_logging=self.tls_verbose_logging,\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass BrowserLikeContextFactory(ScrapyClientContextFactory):\n    \"\"\"\n    Twisted-recommended context factory for web clients.\n\n    Quoting the documentation of the :class:`~twisted.web.client.Agent` class:\n\n        The default is to use a\n        :class:`~twisted.web.client.BrowserLikePolicyForHTTPS`, so unless you\n        have special requirements you can leave this as-is.\n\n    :meth:`creatorForNetloc` is the same as\n    :class:`~twisted.web.client.BrowserLikePolicyForHTTPS` except this context\n    factory allows setting the TLS/SSL method to use.\n\n    The default OpenSSL method is ``TLS_METHOD`` (also called\n    ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n    \"\"\"\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        # trustRoot set to platformTrust() will use the platform's root CAs.\n        #\n        # This means that a website like https://www.cacert.org will be rejected\n        # by default, since CAcert.org CA certificate is seldom shipped.\n        return optionsForClientTLS(\n            hostname=hostname.decode(\"ascii\"),\n            trustRoot=platformTrust(),\n            extraCertificateOptions={\"method\": self._ssl_method},\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass AcceptableProtocolsContextFactory:\n    \"\"\"Context factory to used to override the acceptable protocols\n    to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN\n    negotiation.\n    \"\"\"\n\n    def __init__(self, context_factory: Any, acceptable_protocols: List[bytes]):\n        verifyObject(IPolicyForHTTPS, context_factory)\n        self._wrapped_context_factory: Any = context_factory\n        self._acceptable_protocols: List[bytes] = acceptable_protocols\n\n    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n        options: ClientTLSOptions = self._wrapped_context_factory.creatorForNetloc(\n            hostname, port\n        )\n        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n        return options\n\n\ndef load_context_factory_from_settings(\n    settings: BaseSettings, crawler: Crawler\n) -> IPolicyForHTTPS:\n    ssl_method = openssl_methods[settings.get(\"DOWNLOADER_CLIENT_TLS_METHOD\")]\n    context_factory_cls = load_object(settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"])\n    # try method-aware context factory\n    try:\n        context_factory = build_from_crawler(\n            context_factory_cls,\n            crawler,\n            method=ssl_method,\n        )\n    except TypeError:\n        # use context factory defaults\n        context_factory = build_from_crawler(\n            context_factory_cls,\n            crawler,\n        )\n        msg = (\n            f\"{settings['DOWNLOADER_CLIENTCONTEXTFACTORY']} does not accept \"\n            \"a `method` argument (type OpenSSL.SSL method, e.g. \"\n            \"OpenSSL.SSL.SSLv23_METHOD) and/or a `tls_verbose_logging` \"\n            \"argument and/or a `tls_ciphers` argument. Please, upgrade your \"\n            \"context factory class to handle them or ignore them.\"\n        )\n        warnings.warn(msg)\n\n    return context_factory\n", "scrapy/core/downloader/middleware.py": "\"\"\"\nDownloader Middleware manager\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Callable, Generator, List, Union, cast\n\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n\n\nclass DownloaderMiddlewareManager(MiddlewareManager):\n    component_name = \"downloader middleware\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"DOWNLOADER_MIDDLEWARES\"))\n\n    def _add_middleware(self, mw: Any) -> None:\n        if hasattr(mw, \"process_request\"):\n            self.methods[\"process_request\"].append(mw.process_request)\n        if hasattr(mw, \"process_response\"):\n            self.methods[\"process_response\"].appendleft(mw.process_response)\n        if hasattr(mw, \"process_exception\"):\n            self.methods[\"process_exception\"].appendleft(mw.process_exception)\n\n    def download(\n        self,\n        download_func: Callable[[Request, Spider], Deferred[Response]],\n        request: Request,\n        spider: Spider,\n    ) -> Deferred[Union[Response, Request]]:\n        @inlineCallbacks\n        def process_request(\n            request: Request,\n        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n            for method in self.methods[\"process_request\"]:\n                method = cast(Callable, method)\n                response = yield deferred_from_coro(\n                    method(request=request, spider=spider)\n                )\n                if response is not None and not isinstance(\n                    response, (Response, Request)\n                ):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {response.__class__.__name__}\"\n                    )\n                if response:\n                    return response\n            return (yield download_func(request, spider))\n\n        @inlineCallbacks\n        def process_response(\n            response: Union[Response, Request]\n        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n            if response is None:\n                raise TypeError(\"Received None in process_response\")\n            elif isinstance(response, Request):\n                return response\n\n            for method in self.methods[\"process_response\"]:\n                method = cast(Callable, method)\n                response = yield deferred_from_coro(\n                    method(request=request, response=response, spider=spider)\n                )\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return Response or Request, \"\n                        f\"got {type(response)}\"\n                    )\n                if isinstance(response, Request):\n                    return response\n            return response\n\n        @inlineCallbacks\n        def process_exception(\n            failure: Failure,\n        ) -> Generator[Deferred[Any], Any, Union[Failure, Response, Request]]:\n            exception = failure.value\n            for method in self.methods[\"process_exception\"]:\n                method = cast(Callable, method)\n                response = yield deferred_from_coro(\n                    method(request=request, exception=exception, spider=spider)\n                )\n                if response is not None and not isinstance(\n                    response, (Response, Request)\n                ):\n                    raise _InvalidOutput(\n                        f\"Middleware {method.__qualname__} must return None, Response or \"\n                        f\"Request, got {type(response)}\"\n                    )\n                if response:\n                    return response\n            return failure\n\n        deferred: Deferred[Union[Response, Request]] = mustbe_deferred(\n            process_request, request\n        )\n        deferred.addErrback(process_exception)\n        deferred.addCallback(process_response)\n        return deferred\n", "scrapy/core/downloader/tls.py": "import logging\nfrom typing import Any, Dict\n\nfrom OpenSSL import SSL\nfrom service_identity.exceptions import CertificateError\nfrom twisted.internet._sslverify import (\n    ClientTLSOptions,\n    VerificationError,\n    verifyHostname,\n)\nfrom twisted.internet.ssl import AcceptableCiphers\n\nfrom scrapy.utils.ssl import get_temp_key_info, x509name_to_string\n\nlogger = logging.getLogger(__name__)\n\n\nMETHOD_TLS = \"TLS\"\nMETHOD_TLSv10 = \"TLSv1.0\"\nMETHOD_TLSv11 = \"TLSv1.1\"\nMETHOD_TLSv12 = \"TLSv1.2\"\n\n\nopenssl_methods: Dict[str, int] = {\n    METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n    METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only\n}\n\n\nclass ScrapyClientTLSOptions(ClientTLSOptions):\n    \"\"\"\n    SSL Client connection creator ignoring certificate verification errors\n    (for genuinely invalid certificates or bugs in verification code).\n\n    Same as Twisted's private _sslverify.ClientTLSOptions,\n    except that VerificationError, CertificateError and ValueError\n    exceptions are caught, so that the connection is not closed, only\n    logging warnings. Also, HTTPS connection parameters logging is added.\n    \"\"\"\n\n    def __init__(self, hostname: str, ctx: SSL.Context, verbose_logging: bool = False):\n        super().__init__(hostname, ctx)\n        self.verbose_logging: bool = verbose_logging\n\n    def _identityVerifyingInfoCallback(\n        self, connection: SSL.Connection, where: int, ret: Any\n    ) -> None:\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            connection.set_tlsext_host_name(self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                logger.debug(\n                    \"SSL connection to %s using protocol %s, cipher %s\",\n                    self._hostnameASCII,\n                    connection.get_protocol_version_name(),\n                    connection.get_cipher_name(),\n                )\n                server_cert = connection.get_peer_certificate()\n                if server_cert:\n                    logger.debug(\n                        'SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                        x509name_to_string(server_cert.get_issuer()),\n                        x509name_to_string(server_cert.get_subject()),\n                    )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug(\"SSL temp key: %s\", key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"%s\"; %s',\n                    self._hostnameASCII,\n                    e,\n                )\n\n            except ValueError as e:\n                logger.warning(\n                    \"Ignoring error while verifying certificate \"\n                    'from host \"%s\" (exception: %r)',\n                    self._hostnameASCII,\n                    e,\n                )\n\n\nDEFAULT_CIPHERS: AcceptableCiphers = AcceptableCiphers.fromOpenSSLCipherString(\n    \"DEFAULT\"\n)\n", "scrapy/core/downloader/__init__.py": "from __future__ import annotations\n\nimport random\nimport warnings\nfrom collections import deque\nfrom datetime import datetime\nfrom time import time\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Deque,\n    Dict,\n    Optional,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet import task\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader.handlers import DownloadHandlers\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response\nfrom scrapy.resolver import dnscache\nfrom scrapy.settings import BaseSettings\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.utils.defer import mustbe_deferred\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n_T = TypeVar(\"_T\")\n\n\nclass Slot:\n    \"\"\"Downloader slot\"\"\"\n\n    def __init__(\n        self,\n        concurrency: int,\n        delay: float,\n        randomize_delay: bool,\n        *,\n        throttle: Optional[bool] = None,\n    ):\n        self.concurrency: int = concurrency\n        self.delay: float = delay\n        self.randomize_delay: bool = randomize_delay\n        self.throttle = throttle\n\n        self.active: Set[Request] = set()\n        self.queue: Deque[Tuple[Request, Deferred[Response]]] = deque()\n        self.transferring: Set[Request] = set()\n        self.lastseen: float = 0\n        self.latercall = None\n\n    def free_transfer_slots(self) -> int:\n        return self.concurrency - len(self.transferring)\n\n    def download_delay(self) -> float:\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)  # nosec\n        return self.delay\n\n    def close(self) -> None:\n        if self.latercall and self.latercall.active():\n            self.latercall.cancel()\n\n    def __repr__(self) -> str:\n        cls_name = self.__class__.__name__\n        return (\n            f\"{cls_name}(concurrency={self.concurrency!r}, \"\n            f\"delay={self.delay:.2f}, \"\n            f\"randomize_delay={self.randomize_delay!r}, \"\n            f\"throttle={self.throttle!r})\"\n        )\n\n    def __str__(self) -> str:\n        return (\n            f\"<downloader.Slot concurrency={self.concurrency!r} \"\n            f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n            f\"throttle={self.throttle!r} \"\n            f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n            f\"len(transferring)={len(self.transferring)} \"\n            f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n        )\n\n\ndef _get_concurrency_delay(\n    concurrency: int, spider: Spider, settings: BaseSettings\n) -> Tuple[int, float]:\n    delay: float = settings.getfloat(\"DOWNLOAD_DELAY\")\n    if hasattr(spider, \"download_delay\"):\n        delay = spider.download_delay\n\n    if hasattr(spider, \"max_concurrent_requests\"):\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay\n\n\nclass Downloader:\n    DOWNLOAD_SLOT = \"download_slot\"\n\n    def __init__(self, crawler: Crawler):\n        self.settings: BaseSettings = crawler.settings\n        self.signals: SignalManager = crawler.signals\n        self.slots: Dict[str, Slot] = {}\n        self.active: Set[Request] = set()\n        self.handlers: DownloadHandlers = DownloadHandlers(crawler)\n        self.total_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS\")\n        self.domain_concurrency: int = self.settings.getint(\n            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n        )\n        self.ip_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\")\n        self.randomize_delay: bool = self.settings.getbool(\"RANDOMIZE_DOWNLOAD_DELAY\")\n        self.middleware: DownloaderMiddlewareManager = (\n            DownloaderMiddlewareManager.from_crawler(crawler)\n        )\n        self._slot_gc_loop: task.LoopingCall = task.LoopingCall(self._slot_gc)\n        self._slot_gc_loop.start(60)\n        self.per_slot_settings: Dict[str, Dict[str, Any]] = self.settings.getdict(\n            \"DOWNLOAD_SLOTS\", {}\n        )\n\n    def fetch(\n        self, request: Request, spider: Spider\n    ) -> Deferred[Union[Response, Request]]:\n        def _deactivate(response: _T) -> _T:\n            self.active.remove(request)\n            return response\n\n        self.active.add(request)\n        dfd: Deferred[Union[Response, Request]] = self.middleware.download(\n            self._enqueue_request, request, spider\n        )\n        return dfd.addBoth(_deactivate)\n\n    def needs_backout(self) -> bool:\n        return len(self.active) >= self.total_concurrency\n\n    def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n        key = self.get_slot_key(request)\n        if key not in self.slots:\n            slot_settings = self.per_slot_settings.get(key, {})\n            conc = (\n                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            )\n            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n            conc, delay = (\n                slot_settings.get(\"concurrency\", conc),\n                slot_settings.get(\"delay\", delay),\n            )\n            randomize_delay = slot_settings.get(\"randomize_delay\", self.randomize_delay)\n            throttle = slot_settings.get(\"throttle\", None)\n            new_slot = Slot(conc, delay, randomize_delay, throttle=throttle)\n            self.slots[key] = new_slot\n\n        return key, self.slots[key]\n\n    def get_slot_key(self, request: Request) -> str:\n        if self.DOWNLOAD_SLOT in request.meta:\n            return cast(str, request.meta[self.DOWNLOAD_SLOT])\n\n        key = urlparse_cached(request).hostname or \"\"\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key\n\n    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n        warnings.warn(\n            \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return self.get_slot_key(request)\n\n    def _enqueue_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        key, slot = self._get_slot(request, spider)\n        request.meta[self.DOWNLOAD_SLOT] = key\n\n        def _deactivate(response: Response) -> Response:\n            slot.active.remove(request)\n            return response\n\n        slot.active.add(request)\n        self.signals.send_catch_log(\n            signal=signals.request_reached_downloader, request=request, spider=spider\n        )\n        deferred: Deferred[Response] = Deferred().addBoth(_deactivate)\n        slot.queue.append((request, deferred))\n        self._process_queue(spider, slot)\n        return deferred\n\n    def _process_queue(self, spider: Spider, slot: Slot) -> None:\n        from twisted.internet import reactor\n\n        if slot.latercall and slot.latercall.active():\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = reactor.callLater(\n                    penalty, self._process_queue, spider, slot\n                )\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, deferred = slot.queue.popleft()\n            dfd = self._download(slot, request, spider)\n            dfd.chainDeferred(deferred)\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(spider, slot)\n                break\n\n    def _download(\n        self, slot: Slot, request: Request, spider: Spider\n    ) -> Deferred[Response]:\n        # The order is very important for the following deferreds. Do not change!\n\n        # 1. Create the download deferred\n        dfd: Deferred[Response] = mustbe_deferred(\n            self.handlers.download_request, request, spider\n        )\n\n        # 2. Notify response_downloaded listeners about the recent download\n        # before querying queue for next request\n        def _downloaded(response: Response) -> Response:\n            self.signals.send_catch_log(\n                signal=signals.response_downloaded,\n                response=response,\n                request=request,\n                spider=spider,\n            )\n            return response\n\n        dfd.addCallback(_downloaded)\n\n        # 3. After response arrives, remove the request from transferring\n        # state to free up the transferring slot so it can be used by the\n        # following requests (perhaps those which came from the downloader\n        # middleware itself)\n        slot.transferring.add(request)\n\n        def finish_transferring(_: _T) -> _T:\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            self.signals.send_catch_log(\n                signal=signals.request_left_downloader, request=request, spider=spider\n            )\n            return _\n\n        return dfd.addBoth(finish_transferring)\n\n    def close(self) -> None:\n        self._slot_gc_loop.stop()\n        for slot in self.slots.values():\n            slot.close()\n\n    def _slot_gc(self, age: float = 60) -> None:\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()\n", "scrapy/core/downloader/webclient.py": "import re\nfrom time import time\nfrom typing import Optional, Tuple\nfrom urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n\nfrom twisted.internet import defer\nfrom twisted.internet.protocol import ClientFactory\nfrom twisted.web.http import HTTPClient\n\nfrom scrapy import Request\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes, to_unicode\n\n\ndef _parsed_url_args(parsed: ParseResult) -> Tuple[bytes, bytes, bytes, int, bytes]:\n    # Assume parsed is urlparse-d from Request.url,\n    # which was passed via safe_url_string and is ascii-only.\n    path_str = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n    path = to_bytes(path_str, encoding=\"ascii\")\n    assert parsed.hostname is not None\n    host = to_bytes(parsed.hostname, encoding=\"ascii\")\n    port = parsed.port\n    scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n    netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n    if port is None:\n        port = 443 if scheme == b\"https\" else 80\n    return scheme, netloc, host, port, path\n\n\ndef _parse(url: str) -> Tuple[bytes, bytes, bytes, int, bytes]:\n    \"\"\"Return tuple of (scheme, netloc, host, port, path),\n    all in bytes except for port which is int.\n    Assume url is from Request.url, which was passed via safe_url_string\n    and is ascii-only.\n    \"\"\"\n    url = url.strip()\n    if not re.match(r\"^\\w+://\", url):\n        url = \"//\" + url\n    parsed = urlparse(url)\n    return _parsed_url_args(parsed)\n\n\nclass ScrapyHTTPPageGetter(HTTPClient):\n    delimiter = b\"\\n\"\n\n    def connectionMade(self):\n        self.headers = Headers()  # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)\n\n    def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())\n\n    def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)\n\n    def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)\n\n    def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)\n\n    def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)\n\n    def handleResponse(self, response):\n        if self.factory.method.upper() == b\"HEAD\":\n            self.factory.page(b\"\")\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()\n\n    def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b\"https\"):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\n            defer.TimeoutError(\n                f\"Getting {self.factory.url} took longer \"\n                f\"than {self.factory.timeout} seconds.\"\n            )\n        )\n\n\n# This class used to inherit from Twisted\u2019s\n# twisted.web.client.HTTPClientFactory. When that class was deprecated in\n# Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n# non-overridden code into this class.\nclass ScrapyHTTPClientFactory(ClientFactory):\n    protocol = ScrapyHTTPPageGetter\n\n    waiting = 1\n    noisy = False\n    followRedirect = False\n    afterFoundGet = False\n\n    def _build_response(self, body, request):\n        request.meta[\"download_latency\"] = self.headers_time - self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)\n        return respcls(\n            url=self._url,\n            status=status,\n            headers=headers,\n            body=body,\n            protocol=to_unicode(self.version),\n        )\n\n    def _set_connection_attributes(self, request):\n        parsed = urlparse_cached(request)\n        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(\n            parsed\n        )\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n            self.path = self.url\n\n    def __init__(self, request: Request, timeout: float = 180):\n        self._url: str = urldefrag(request.url)[0]\n        # converting to bytes to comply to Twisted interface\n        self.url: bytes = to_bytes(self._url, encoding=\"ascii\")\n        self.method: bytes = to_bytes(request.method, encoding=\"ascii\")\n        self.body: Optional[bytes] = request.body or None\n        self.headers: Headers = Headers(request.headers)\n        self.response_headers: Optional[Headers] = None\n        self.timeout: float = request.meta.get(\"download_timeout\") or timeout\n        self.start_time: float = time()\n        self.deferred: defer.Deferred[Response] = defer.Deferred().addCallback(\n            self._build_response, request\n        )\n\n        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n        # to have _disconnectedDeferred. See Twisted r32329.\n        # As Scrapy implements it's own logic to handle redirects is not\n        # needed to add the callback _waitForDisconnect.\n        # Specifically this avoids the AttributeError exception when\n        # clientConnectionFailed method is called.\n        self._disconnectedDeferred: defer.Deferred[None] = defer.Deferred()\n\n        self._set_connection_attributes(request)\n\n        # set Host header based on url\n        self.headers.setdefault(\"Host\", self.netloc)\n\n        # set Content-Length based len of body\n        if self.body is not None:\n            self.headers[\"Content-Length\"] = len(self.body)\n            # just in case a broken http/1.1 decides to keep connection alive\n            self.headers.setdefault(\"Connection\", \"close\")\n        # Content-Length must be specified in POST method even with no body\n        elif self.method == b\"POST\":\n            self.headers[\"Content-Length\"] = 0\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self._url}>\"\n\n    def _cancelTimeout(self, result, timeoutCall):\n        if timeoutCall.active():\n            timeoutCall.cancel()\n        return result\n\n    def buildProtocol(self, addr):\n        p = ClientFactory.buildProtocol(self, addr)\n        p.followRedirect = self.followRedirect\n        p.afterFoundGet = self.afterFoundGet\n        if self.timeout:\n            from twisted.internet import reactor\n\n            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n        return p\n\n    def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers\n\n    def gotStatus(self, version, status, message):\n        \"\"\"\n        Set the status of the request on us.\n        @param version: The HTTP version.\n        @type version: L{bytes}\n        @param status: The HTTP status code, an integer represented as a\n        bytestring.\n        @type status: L{bytes}\n        @param message: The HTTP status message.\n        @type message: L{bytes}\n        \"\"\"\n        self.version, self.status, self.message = version, status, message\n\n    def page(self, page):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.callback(page)\n\n    def noPage(self, reason):\n        if self.waiting:\n            self.waiting = 0\n            self.deferred.errback(reason)\n\n    def clientConnectionFailed(self, _, reason):\n        \"\"\"\n        When a connection attempt fails, the request cannot be issued.  If no\n        result has yet been provided to the result Deferred, provide the\n        connection failure reason as an error result.\n        \"\"\"\n        if self.waiting:\n            self.waiting = 0\n            # If the connection attempt failed, there is nothing more to\n            # disconnect, so just fire that Deferred now.\n            self._disconnectedDeferred.callback(None)\n            self.deferred.errback(reason)\n", "scrapy/core/downloader/handlers/http.py": "from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\nfrom scrapy.core.downloader.handlers.http11 import (\n    HTTP11DownloadHandler as HTTPDownloadHandler,\n)\n", "scrapy/core/downloader/handlers/http2.py": "from __future__ import annotations\n\nfrom time import time\nfrom typing import TYPE_CHECKING, Optional\nfrom urllib.parse import urldefrag\n\nfrom twisted.internet.base import DelayedCall\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.error import TimeoutError\nfrom twisted.web.client import URI\nfrom twisted.web.iweb import IPolicyForHTTPS\n\nfrom scrapy.core.downloader.contextfactory import load_context_factory_from_settings\nfrom scrapy.core.downloader.webclient import _parse\nfrom scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass H2DownloadHandler:\n    def __init__(self, settings: Settings, crawler: Crawler):\n        self._crawler = crawler\n\n        from twisted.internet import reactor\n\n        self._pool = H2ConnectionPool(reactor, settings)\n        self._context_factory = load_context_factory_from_settings(settings, crawler)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        agent = ScrapyH2Agent(\n            context_factory=self._context_factory,\n            pool=self._pool,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request, spider)\n\n    def close(self) -> None:\n        self._pool.close_connections()\n\n\nclass ScrapyH2Agent:\n    _Agent = H2Agent\n    _ProxyAgent = ScrapyProxyH2Agent\n\n    def __init__(\n        self,\n        context_factory: IPolicyForHTTPS,\n        pool: H2ConnectionPool,\n        connect_timeout: int = 10,\n        bind_address: Optional[bytes] = None,\n        crawler: Optional[Crawler] = None,\n    ) -> None:\n        self._context_factory = context_factory\n        self._connect_timeout = connect_timeout\n        self._bind_address = bind_address\n        self._pool = pool\n        self._crawler = crawler\n\n    def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:\n        from twisted.internet import reactor\n\n        bind_address = request.meta.get(\"bindaddress\") or self._bind_address\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n            scheme = _parse(request.url)[0]\n\n            if scheme == b\"https\":\n                # ToDo\n                raise NotImplementedError(\n                    \"Tunneling via CONNECT method using HTTP/2.0 is not yet supported\"\n                )\n            return self._ProxyAgent(\n                reactor=reactor,\n                context_factory=self._context_factory,\n                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding=\"ascii\")),\n                connect_timeout=timeout,\n                bind_address=bind_address,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            context_factory=self._context_factory,\n            connect_timeout=timeout,\n            bind_address=bind_address,\n            pool=self._pool,\n        )\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        timeout = request.meta.get(\"download_timeout\") or self._connect_timeout\n        agent = self._get_agent(request, timeout)\n\n        start_time = time()\n        d = agent.request(request, spider)\n        d.addCallback(self._cb_latency, request, start_time)\n\n        timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, timeout, timeout_cl)\n        return d\n\n    @staticmethod\n    def _cb_latency(\n        response: Response, request: Request, start_time: float\n    ) -> Response:\n        request.meta[\"download_latency\"] = time() - start_time\n        return response\n\n    @staticmethod\n    def _cb_timeout(\n        response: Response, request: Request, timeout: float, timeout_cl: DelayedCall\n    ) -> Response:\n        if timeout_cl.active():\n            timeout_cl.cancel()\n            return response\n\n        url = urldefrag(request.url)[0]\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n", "scrapy/core/downloader/handlers/ftp.py": "\"\"\"\nAn asynchronous FTP file download handler for scrapy which somehow emulates an http response.\n\nFTP connection parameters are passed using the request meta field:\n- ftp_user (required)\n- ftp_password (required)\n- ftp_passive (by default, enabled) sets FTP connection passive mode\n- ftp_local_filename\n        - If not given, file data will come in the response.body, as a normal scrapy Response,\n        which will imply that the entire file will be on memory.\n        - if given, file data will be saved in a local file with the given name\n        This helps when downloading very big files to avoid memory issues. In addition, for\n        convenience the local file name will also be given in the response body.\n\nThe status of the built html response will be, by default\n- 200 in case of success\n- 404 in case specified file was not found in the server (ftp code 550)\n\nor raise corresponding ftp exception otherwise\n\nThe matching from server ftp command return codes to html response codes is defined in the\nCODE_MAPPING attribute of the handler class. The key 'default' is used for any code\nthat is not explicitly present among the map keys. You may need to overwrite this\nmapping if want a different behaviour than default.\n\nIn case of status 200 request, response.headers will come with two keys:\n    'Local Filename' - with the value of the local filename if given\n    'Size' - with size of the downloaded data\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, Any, BinaryIO, Dict, Optional\nfrom urllib.parse import unquote\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.protocol import ClientCreator, Protocol\nfrom twisted.protocols.ftp import CommandFailed, FTPClient\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass ReceivedDataProtocol(Protocol):\n    def __init__(self, filename: Optional[str] = None):\n        self.__filename: Optional[str] = filename\n        self.body: BinaryIO = open(filename, \"wb\") if filename else BytesIO()\n        self.size: int = 0\n\n    def dataReceived(self, data: bytes) -> None:\n        self.body.write(data)\n        self.size += len(data)\n\n    @property\n    def filename(self) -> Optional[str]:\n        return self.__filename\n\n    def close(self) -> None:\n        self.body.close() if self.filename else self.body.seek(0)\n\n\n_CODE_RE = re.compile(r\"\\d+\")\n\n\nclass FTPDownloadHandler:\n    lazy = False\n\n    CODE_MAPPING: Dict[str, int] = {\n        \"550\": 404,\n        \"default\": 503,\n    }\n\n    def __init__(self, settings: BaseSettings):\n        self.default_user = settings[\"FTP_USER\"]\n        self.default_password = settings[\"FTP_PASSWORD\"]\n        self.passive_mode = settings[\"FTP_PASSIVE_MODE\"]\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        parsed_url = urlparse_cached(request)\n        user = request.meta.get(\"ftp_user\", self.default_user)\n        password = request.meta.get(\"ftp_password\", self.default_password)\n        passive_mode = (\n            1 if bool(request.meta.get(\"ftp_passive\", self.passive_mode)) else 0\n        )\n        creator = ClientCreator(\n            reactor, FTPClient, user, password, passive=passive_mode\n        )\n        dfd: Deferred[FTPClient] = creator.connectTCP(\n            parsed_url.hostname, parsed_url.port or 21\n        )\n        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n\n    def gotClient(\n        self, client: FTPClient, request: Request, filepath: str\n    ) -> Deferred[Response]:\n        self.client = client\n        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n        d = client.retrieveFile(filepath, protocol)\n        d.addCallback(self._build_response, request, protocol)\n        d.addErrback(self._failed, request)\n        return d\n\n    def _build_response(\n        self, result: Any, request: Request, protocol: ReceivedDataProtocol\n    ) -> Response:\n        self.result = result\n        protocol.close()\n        headers = {\"local filename\": protocol.filename or \"\", \"size\": protocol.size}\n        body = to_bytes(protocol.filename or protocol.body.read())\n        respcls = responsetypes.from_args(url=request.url, body=body)\n        # hints for Headers-related types may need to be fixed to not use AnyStr\n        return respcls(url=request.url, status=200, body=body, headers=headers)  # type: ignore[arg-type]\n\n    def _failed(self, result: Failure, request: Request) -> Response:\n        message = result.getErrorMessage()\n        if result.type == CommandFailed:\n            m = _CODE_RE.search(message)\n            if m:\n                ftpcode = m.group()\n                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                return Response(\n                    url=request.url, status=httpcode, body=to_bytes(message)\n                )\n        assert result.type\n        raise result.type(result.value)\n", "scrapy/core/downloader/handlers/s3.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Optional, Type\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider\nfrom scrapy.core.downloader.handlers.http import HTTPDownloadHandler\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import build_from_crawler\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass S3DownloadHandler:\n    def __init__(\n        self,\n        settings: BaseSettings,\n        *,\n        crawler: Crawler,\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n        aws_session_token: Optional[str] = None,\n        httpdownloadhandler: Type[HTTPDownloadHandler] = HTTPDownloadHandler,\n        **kw: Any,\n    ):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n\n        if not aws_access_key_id:\n            aws_access_key_id = settings[\"AWS_ACCESS_KEY_ID\"]\n        if not aws_secret_access_key:\n            aws_secret_access_key = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        if not aws_session_token:\n            aws_session_token = settings[\"AWS_SESSION_TOKEN\"]\n\n        # If no credentials could be found anywhere,\n        # consider this an anonymous connection request by default;\n        # unless 'anon' was set explicitly (True/False).\n        anon = kw.get(\"anon\")\n        if anon is None and not aws_access_key_id and not aws_secret_access_key:\n            kw[\"anon\"] = True\n        self.anon = kw.get(\"anon\")\n\n        self._signer = None\n        import botocore.auth\n        import botocore.credentials\n\n        kw.pop(\"anon\", None)\n        if kw:\n            raise TypeError(f\"Unexpected keyword arguments: {kw}\")\n        if not self.anon:\n            assert aws_access_key_id is not None\n            assert aws_secret_access_key is not None\n            SignerCls = botocore.auth.AUTH_TYPE_MAPS[\"s3\"]\n            # botocore.auth.BaseSigner doesn't have an __init__() with args, only subclasses do\n            self._signer = SignerCls(  # type: ignore[call-arg]\n                botocore.credentials.Credentials(\n                    aws_access_key_id, aws_secret_access_key, aws_session_token\n                )\n            )\n\n        _http_handler = build_from_crawler(\n            httpdownloadhandler,\n            crawler,\n        )\n        self._download_http = _http_handler.download_request\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, **kwargs: Any) -> Self:\n        return cls(crawler.settings, crawler=crawler, **kwargs)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        p = urlparse_cached(request)\n        scheme = \"https\" if request.meta.get(\"is_secure\") else \"http\"\n        bucket = p.hostname\n        path = p.path + \"?\" + p.query if p.query else p.path\n        url = f\"{scheme}://{bucket}.s3.amazonaws.com{path}\"\n        if self.anon:\n            request = request.replace(url=url)\n        else:\n            import botocore.awsrequest\n\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url=f\"{scheme}://s3.amazonaws.com/{bucket}{path}\",\n                headers=request.headers.to_unicode_dict(),\n                data=request.body,\n            )\n            assert self._signer\n            self._signer.add_auth(awsrequest)\n            request = request.replace(url=url, headers=awsrequest.headers.items())\n        return self._download_http(request, spider)\n", "scrapy/core/downloader/handlers/http10.py": "\"\"\"Download handlers for http and https schemes\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Type\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n    from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n\n\nclass HTTP10DownloadHandler:\n    lazy = False\n\n    def __init__(self, settings: BaseSettings, crawler: Crawler):\n        self.HTTPClientFactory: Type[ScrapyHTTPClientFactory] = load_object(\n            settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"]\n        )\n        self.ClientContextFactory: Type[ScrapyClientContextFactory] = load_object(\n            settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n        )\n        self._settings: BaseSettings = settings\n        self._crawler: Crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred\n\n    def _connect(self, factory: ScrapyHTTPClientFactory) -> Deferred:\n        from twisted.internet import reactor\n\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b\"https\":\n            client_context_factory = build_from_crawler(\n                self.ClientContextFactory,\n                self._crawler,\n            )\n            return reactor.connectSSL(host, port, factory, client_context_factory)\n        return reactor.connectTCP(host, port, factory)\n", "scrapy/core/downloader/handlers/file.py": "from pathlib import Path\n\nfrom w3lib.url import file_uri_to_path\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.decorators import defers\n\n\nclass FileDownloadHandler:\n    lazy = False\n\n    @defers\n    def download_request(self, request: Request, spider: Spider) -> Response:\n        filepath = file_uri_to_path(request.url)\n        body = Path(filepath).read_bytes()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)\n", "scrapy/core/downloader/handlers/datauri.py": "from typing import Any, Dict\n\nfrom w3lib.url import parse_data_uri\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.decorators import defers\n\n\nclass DataURIDownloadHandler:\n    lazy = False\n\n    @defers\n    def download_request(self, request: Request, spider: Spider) -> Response:\n        uri = parse_data_uri(request.url)\n        respcls = responsetypes.from_mimetype(uri.media_type)\n\n        resp_kwargs: Dict[str, Any] = {}\n        if issubclass(respcls, TextResponse) and uri.media_type.split(\"/\")[0] == \"text\":\n            charset = uri.media_type_parameters.get(\"charset\")\n            resp_kwargs[\"encoding\"] = charset\n\n        return respcls(url=request.url, body=uri.data, **resp_kwargs)\n", "scrapy/core/downloader/handlers/__init__.py": "\"\"\"Download handlers for different schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generator,\n    Optional,\n    Protocol,\n    Type,\n    Union,\n    cast,\n)\n\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.exceptions import NotConfigured, NotSupported\nfrom scrapy.http import Response\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadHandlerProtocol(Protocol):\n    def download_request(\n        self, request: Request, spider: Spider\n    ) -> Deferred[Response]: ...\n\n\nclass DownloadHandlers:\n    def __init__(self, crawler: Crawler):\n        self._crawler: Crawler = crawler\n        self._schemes: Dict[str, Union[str, Callable[..., Any]]] = (\n            {}\n        )  # stores acceptable schemes on instancing\n        self._handlers: Dict[str, DownloadHandlerProtocol] = (\n            {}\n        )  # stores instanced handlers for schemes\n        self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n        handlers: Dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n            cast(\n                Dict[str, Union[str, Callable[..., Any]]],\n                crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n            )\n        )\n        for scheme, clspath in handlers.items():\n            self._schemes[scheme] = clspath\n            self._load_handler(scheme, skip_lazy=True)\n\n        crawler.signals.connect(self._close, signals.engine_stopped)\n\n    def _get_handler(self, scheme: str) -> Optional[DownloadHandlerProtocol]:\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = \"no handler available for that scheme\"\n            return None\n\n        return self._load_handler(scheme)\n\n    def _load_handler(\n        self, scheme: str, skip_lazy: bool = False\n    ) -> Optional[DownloadHandlerProtocol]:\n        path = self._schemes[scheme]\n        try:\n            dhcls: Type[DownloadHandlerProtocol] = load_object(path)\n            if skip_lazy and getattr(dhcls, \"lazy\", True):\n                return None\n            dh = build_from_crawler(\n                dhcls,\n                self._crawler,\n            )\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error(\n                'Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                {\"clspath\": path, \"scheme\": scheme},\n                exc_info=True,\n                extra={\"crawler\": self._crawler},\n            )\n            self._notconfigured[scheme] = str(ex)\n            return None\n        else:\n            self._handlers[scheme] = dh\n            return dh\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(\n                f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\"\n            )\n        return handler.download_request(request, spider)\n\n    @defer.inlineCallbacks\n    def _close(self, *_a: Any, **_kw: Any) -> Generator[Deferred[Any], Any, None]:\n        for dh in self._handlers.values():\n            if hasattr(dh, \"close\"):\n                yield dh.close()\n", "scrapy/core/downloader/handlers/http11.py": "\"\"\"Download handlers for http and https schemes\"\"\"\n\nfrom __future__ import annotations\n\nimport ipaddress\nimport logging\nimport re\nfrom contextlib import suppress\nfrom io import BytesIO\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, List, Optional, Tuple, TypedDict, TypeVar, Union\nfrom urllib.parse import urldefrag, urlunparse\n\nfrom twisted.internet import ssl\nfrom twisted.internet.base import ReactorBase\nfrom twisted.internet.defer import CancelledError, Deferred, succeed\nfrom twisted.internet.endpoints import TCP4ClientEndpoint\nfrom twisted.internet.error import TimeoutError\nfrom twisted.internet.interfaces import IConsumer\nfrom twisted.internet.protocol import Factory, Protocol, connectionDone\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import URI, Agent, HTTPConnectionPool\nfrom twisted.web.client import Response as TxResponse\nfrom twisted.web.client import ResponseDone, ResponseFailed\nfrom twisted.web.http import PotentialDataLoss, _DataLoss\nfrom twisted.web.http_headers import Headers as TxHeaders\nfrom twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS\nfrom zope.interface import implementer\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader.contextfactory import load_context_factory_from_settings\nfrom scrapy.core.downloader.webclient import _parse\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.python import to_bytes, to_unicode\n\nif TYPE_CHECKING:\n    # typing.NotRequired and typing.Self require Python 3.11\n    from typing_extensions import NotRequired, Self\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\")\n\n\nclass _ResultT(TypedDict):\n    txresponse: TxResponse\n    body: bytes\n    flags: Optional[List[str]]\n    certificate: Optional[ssl.Certificate]\n    ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None]\n    failure: NotRequired[Optional[Failure]]\n\n\nclass HTTP11DownloadHandler:\n    lazy = False\n\n    def __init__(self, settings: BaseSettings, crawler: Crawler):\n        self._crawler = crawler\n\n        from twisted.internet import reactor\n\n        self._pool: HTTPConnectionPool = HTTPConnectionPool(reactor, persistent=True)\n        self._pool.maxPersistentPerHost = settings.getint(\n            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n        )\n        self._pool._factory.noisy = False\n\n        self._contextFactory: IPolicyForHTTPS = load_context_factory_from_settings(\n            settings, crawler\n        )\n        self._default_maxsize: int = settings.getint(\"DOWNLOAD_MAXSIZE\")\n        self._default_warnsize: int = settings.getint(\"DOWNLOAD_WARNSIZE\")\n        self._fail_on_dataloss: bool = settings.getbool(\"DOWNLOAD_FAIL_ON_DATALOSS\")\n        self._disconnect_timeout: int = 1\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings, crawler)\n\n    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(\n            contextFactory=self._contextFactory,\n            pool=self._pool,\n            maxsize=getattr(spider, \"download_maxsize\", self._default_maxsize),\n            warnsize=getattr(spider, \"download_warnsize\", self._default_warnsize),\n            fail_on_dataloss=self._fail_on_dataloss,\n            crawler=self._crawler,\n        )\n        return agent.download_request(request)\n\n    def close(self) -> Deferred[None]:\n        from twisted.internet import reactor\n\n        d: Deferred[None] = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result: _T) -> _T:\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d\n\n\nclass TunnelError(Exception):\n    \"\"\"An HTTP CONNECT tunnel could not be established by the proxy.\"\"\"\n\n\nclass TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n    \"\"\"An endpoint that tunnels through proxies to allow HTTPS downloads. To\n    accomplish that, this endpoint sends an HTTP CONNECT to the proxy.\n    The HTTP CONNECT is always sent when using this endpoint, I think this could\n    be improved as the CONNECT will be redundant if the connection associated\n    with this endpoint comes from the pool and a CONNECT has already been issued\n    for it.\n    \"\"\"\n\n    _truncatedLength = 1000\n    _responseAnswer = (\n        r\"HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,\" + str(_truncatedLength) + r\"})\"\n    )\n    _responseMatcher = re.compile(_responseAnswer.encode())\n\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        host: str,\n        port: int,\n        proxyConf: Tuple[str, int, Optional[bytes]],\n        contextFactory: IPolicyForHTTPS,\n        timeout: float = 30,\n        bindAddress: Optional[Tuple[str, int]] = None,\n    ):\n        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n        super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n        self._tunnelReadyDeferred: Deferred[Protocol] = Deferred()\n        self._tunneledHost: str = host\n        self._tunneledPort: int = port\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n        self._connectBuffer: bytearray = bytearray()\n\n    def requestTunnel(self, protocol: Protocol) -> Protocol:\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        assert protocol.transport\n        tunnelReq = tunnel_request_data(\n            self._tunneledHost, self._tunneledPort, self._proxyAuthHeader\n        )\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse  # type: ignore[method-assign]\n        self._protocol = protocol\n        return protocol\n\n    def processProxyResponse(self, data: bytes) -> None:\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        assert self._protocol.transport\n        self._connectBuffer += data\n        # make sure that enough (all) bytes are consumed\n        # and that we've got all HTTP headers (ending with a blank line)\n        # from the proxy so that we don't send those bytes to the TLS layer\n        #\n        # see https://github.com/scrapy/scrapy/issues/2491\n        if b\"\\r\\n\\r\\n\" not in self._connectBuffer:\n            return\n        self._protocol.dataReceived = self._protocolDataReceived  # type: ignore[method-assign]\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n        if respm and int(respm.group(\"status\")) == 200:\n            # set proper Server Name Indication extension\n            sslOptions = self._contextFactory.creatorForNetloc(  # type: ignore[call-arg,misc]\n                self._tunneledHost, self._tunneledPort\n            )\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            extra: Any\n            if respm:\n                extra = {\n                    \"status\": int(respm.group(\"status\")),\n                    \"reason\": respm.group(\"reason\").strip(),\n                }\n            else:\n                extra = data[: self._truncatedLength]\n            self._tunnelReadyDeferred.errback(\n                TunnelError(\n                    \"Could not open CONNECT tunnel with proxy \"\n                    f\"{self._host}:{self._port} [{extra!r}]\"\n                )\n            )\n\n    def connectFailed(self, reason: Failure) -> None:\n        \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n        self._tunnelReadyDeferred.errback(reason)\n\n    def connect(self, protocolFactory: Factory) -> Deferred[Protocol]:\n        self._protocolFactory = protocolFactory\n        connectDeferred = super().connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred\n\n\ndef tunnel_request_data(\n    host: str, port: int, proxy_auth_header: Optional[bytes] = None\n) -> bytes:\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_unicode as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding=\"ascii\") + b\":\" + to_bytes(str(port))\n    tunnel_req = b\"CONNECT \" + host_value + b\" HTTP/1.1\\r\\n\"\n    tunnel_req += b\"Host: \" + host_value + b\"\\r\\n\"\n    if proxy_auth_header:\n        tunnel_req += b\"Proxy-Authorization: \" + proxy_auth_header + b\"\\r\\n\"\n    tunnel_req += b\"\\r\\n\"\n    return tunnel_req\n\n\nclass TunnelingAgent(Agent):\n    \"\"\"An agent that uses a L{TunnelingTCP4ClientEndpoint} to make HTTPS\n    downloads. It may look strange that we have chosen to subclass Agent and not\n    ProxyAgent but consider that after the tunnel is opened the proxy is\n    transparent to the client; thus the agent should behave like there is no\n    proxy involved.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        reactor: ReactorBase,\n        proxyConf: Tuple[str, int, Optional[bytes]],\n        contextFactory: IPolicyForHTTPS,\n        connectTimeout: Optional[float] = None,\n        bindAddress: Optional[bytes] = None,\n        pool: Optional[HTTPConnectionPool] = None,\n    ):\n        super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n        self._proxyConf: Tuple[str, int, Optional[bytes]] = proxyConf\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n\n    def _getEndpoint(self, uri: URI) -> TunnelingTCP4ClientEndpoint:\n        return TunnelingTCP4ClientEndpoint(\n            reactor=self._reactor,\n            host=uri.host,\n            port=uri.port,\n            proxyConf=self._proxyConf,\n            contextFactory=self._contextFactory,\n            timeout=self._endpointFactory._connectTimeout,\n            bindAddress=self._endpointFactory._bindAddress,\n        )\n\n    def _requestWithEndpoint(\n        self,\n        key: Any,\n        endpoint: TCP4ClientEndpoint,\n        method: bytes,\n        parsedURI: bytes,\n        headers: Optional[TxHeaders],\n        bodyProducer: Optional[IBodyProducer],\n        requestPath: bytes,\n    ) -> Deferred[TxResponse]:\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key += self._proxyConf\n        return super()._requestWithEndpoint(\n            key=key,\n            endpoint=endpoint,\n            method=method,\n            parsedURI=parsedURI,\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=requestPath,\n        )\n\n\nclass ScrapyProxyAgent(Agent):\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        proxyURI: bytes,\n        connectTimeout: Optional[float] = None,\n        bindAddress: Optional[bytes] = None,\n        pool: Optional[HTTPConnectionPool] = None,\n    ):\n        super().__init__(\n            reactor=reactor,\n            connectTimeout=connectTimeout,\n            bindAddress=bindAddress,\n            pool=pool,\n        )\n        self._proxyURI: URI = URI.fromBytes(proxyURI)\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[TxHeaders] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> Deferred[TxResponse]:\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        return self._requestWithEndpoint(\n            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n            endpoint=self._getEndpoint(self._proxyURI),\n            method=method,\n            parsedURI=URI.fromBytes(uri),\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=uri,\n        )\n\n\nclass ScrapyAgent:\n    _Agent = Agent\n    _ProxyAgent = ScrapyProxyAgent\n    _TunnelingAgent = TunnelingAgent\n\n    def __init__(\n        self,\n        *,\n        contextFactory: IPolicyForHTTPS,\n        connectTimeout: float = 10,\n        bindAddress: Optional[bytes] = None,\n        pool: Optional[HTTPConnectionPool] = None,\n        maxsize: int = 0,\n        warnsize: int = 0,\n        fail_on_dataloss: bool = True,\n        crawler: Crawler,\n    ):\n        self._contextFactory: IPolicyForHTTPS = contextFactory\n        self._connectTimeout: float = connectTimeout\n        self._bindAddress: Optional[bytes] = bindAddress\n        self._pool: Optional[HTTPConnectionPool] = pool\n        self._maxsize: int = maxsize\n        self._warnsize: int = warnsize\n        self._fail_on_dataloss: bool = fail_on_dataloss\n        self._txresponse: Optional[TxResponse] = None\n        self._crawler: Crawler = crawler\n\n    def _get_agent(self, request: Request, timeout: float) -> Agent:\n        from twisted.internet import reactor\n\n        bindaddress = request.meta.get(\"bindaddress\") or self._bindAddress\n        proxy = request.meta.get(\"proxy\")\n        if proxy:\n            proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost_str = to_unicode(proxyHost)\n            if scheme == b\"https\":\n                proxyAuth = request.headers.get(b\"Proxy-Authorization\", None)\n                proxyConf = (proxyHost_str, proxyPort, proxyAuth)\n                return self._TunnelingAgent(\n                    reactor=reactor,\n                    proxyConf=proxyConf,\n                    contextFactory=self._contextFactory,\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n            proxyScheme = proxyScheme or b\"http\"\n            proxyURI = urlunparse(\n                (proxyScheme, proxyNetloc, proxyParams, b\"\", b\"\", b\"\")\n            )\n            return self._ProxyAgent(\n                reactor=reactor,\n                proxyURI=to_bytes(proxyURI, encoding=\"ascii\"),\n                connectTimeout=timeout,\n                bindAddress=bindaddress,\n                pool=self._pool,\n            )\n\n        return self._Agent(\n            reactor=reactor,\n            contextFactory=self._contextFactory,\n            connectTimeout=timeout,\n            bindAddress=bindaddress,\n            pool=self._pool,\n        )\n\n    def download_request(self, request: Request) -> Deferred[Response]:\n        from twisted.internet import reactor\n\n        timeout = request.meta.get(\"download_timeout\") or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b\"Proxy-Authorization\")\n        if request.body:\n            bodyproducer = _RequestBodyProducer(request.body)\n        else:\n            bodyproducer = None\n        start_time = time()\n        d: Deferred[TxResponse] = agent.request(\n            method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n        )\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d2: Deferred[_ResultT] = d.addCallback(self._cb_bodyready, request)\n        d3: Deferred[Response] = d2.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d3.cancel)\n        d3.addBoth(self._cb_timeout, request, url, timeout)\n        return d3\n\n    def _cb_timeout(self, result: _T, request: Request, url: str, timeout: float) -> _T:\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n\n    def _cb_latency(self, result: _T, request: Request, start_time: float) -> _T:\n        request.meta[\"download_latency\"] = time() - start_time\n        return result\n\n    @staticmethod\n    def _headers_from_twisted_response(response: TxResponse) -> Headers:\n        headers = Headers()\n        if response.length != UNKNOWN_LENGTH:\n            headers[b\"Content-Length\"] = str(response.length).encode()\n        headers.update(response.headers.getAllRawHeaders())\n        return headers\n\n    def _cb_bodyready(\n        self, txresponse: TxResponse, request: Request\n    ) -> Union[_ResultT, Deferred[_ResultT]]:\n        headers_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.headers_received,\n            headers=self._headers_from_twisted_response(txresponse),\n            body_length=txresponse.length,\n            request=request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in headers_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\n                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n                    {\"request\": request, \"handler\": handler.__qualname__},\n                )\n                txresponse._transport.stopProducing()\n                txresponse._transport.loseConnection()\n                return {\n                    \"txresponse\": txresponse,\n                    \"body\": b\"\",\n                    \"flags\": [\"download_stopped\"],\n                    \"certificate\": None,\n                    \"ip_address\": None,\n                    \"failure\": result if result.value.fail else None,\n                }\n\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return {\n                \"txresponse\": txresponse,\n                \"body\": b\"\",\n                \"flags\": None,\n                \"certificate\": None,\n                \"ip_address\": None,\n            }\n\n        maxsize = request.meta.get(\"download_maxsize\", self._maxsize)\n        warnsize = request.meta.get(\"download_warnsize\", self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n        fail_on_dataloss = request.meta.get(\n            \"download_fail_on_dataloss\", self._fail_on_dataloss\n        )\n\n        if maxsize and expected_size > maxsize:\n            warning_msg = (\n                \"Cancelling download of %(url)s: expected response \"\n                \"size (%(size)s) larger than download max size (%(maxsize)s).\"\n            )\n            warning_args = {\n                \"url\": request.url,\n                \"size\": expected_size,\n                \"maxsize\": maxsize,\n            }\n\n            logger.warning(warning_msg, warning_args)\n\n            txresponse._transport.loseConnection()\n            raise CancelledError(warning_msg % warning_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\n                \"Expected response size (%(size)s) larger than \"\n                \"download warn size (%(warnsize)s) in request %(request)s.\",\n                {\"size\": expected_size, \"warnsize\": warnsize, \"request\": request},\n            )\n\n        def _cancel(_: Any) -> None:\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()\n\n        d: Deferred[_ResultT] = Deferred(_cancel)\n        txresponse.deliverBody(\n            _ResponseReader(\n                finished=d,\n                txresponse=txresponse,\n                request=request,\n                maxsize=maxsize,\n                warnsize=warnsize,\n                fail_on_dataloss=fail_on_dataloss,\n                crawler=self._crawler,\n            )\n        )\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d\n\n    def _cb_bodydone(\n        self, result: _ResultT, request: Request, url: str\n    ) -> Union[Response, Failure]:\n        headers = self._headers_from_twisted_response(result[\"txresponse\"])\n        respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n        try:\n            version = result[\"txresponse\"].version\n            protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n        except (AttributeError, TypeError, IndexError):\n            protocol = None\n        response = respcls(\n            url=url,\n            status=int(result[\"txresponse\"].code),\n            headers=headers,\n            body=result[\"body\"],\n            flags=result[\"flags\"],\n            certificate=result[\"certificate\"],\n            ip_address=result[\"ip_address\"],\n            protocol=protocol,\n        )\n        if result.get(\"failure\"):\n            assert result[\"failure\"]\n            result[\"failure\"].value.response = response\n            return result[\"failure\"]\n        return response\n\n\n@implementer(IBodyProducer)\nclass _RequestBodyProducer:\n    def __init__(self, body: bytes):\n        self.body = body\n        self.length = len(body)\n\n    def startProducing(self, consumer: IConsumer) -> Deferred[None]:\n        consumer.write(self.body)\n        return succeed(None)\n\n    def pauseProducing(self) -> None:\n        pass\n\n    def stopProducing(self) -> None:\n        pass\n\n\nclass _ResponseReader(Protocol):\n    def __init__(\n        self,\n        finished: Deferred[_ResultT],\n        txresponse: TxResponse,\n        request: Request,\n        maxsize: int,\n        warnsize: int,\n        fail_on_dataloss: bool,\n        crawler: Crawler,\n    ):\n        self._finished: Deferred[_ResultT] = finished\n        self._txresponse: TxResponse = txresponse\n        self._request: Request = request\n        self._bodybuf: BytesIO = BytesIO()\n        self._maxsize: int = maxsize\n        self._warnsize: int = warnsize\n        self._fail_on_dataloss: bool = fail_on_dataloss\n        self._fail_on_dataloss_warned: bool = False\n        self._reached_warnsize: bool = False\n        self._bytes_received: int = 0\n        self._certificate: Optional[ssl.Certificate] = None\n        self._ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None] = (\n            None\n        )\n        self._crawler: Crawler = crawler\n\n    def _finish_response(\n        self, flags: Optional[List[str]] = None, failure: Optional[Failure] = None\n    ) -> None:\n        self._finished.callback(\n            {\n                \"txresponse\": self._txresponse,\n                \"body\": self._bodybuf.getvalue(),\n                \"flags\": flags,\n                \"certificate\": self._certificate,\n                \"ip_address\": self._ip_address,\n                \"failure\": failure,\n            }\n        )\n\n    def connectionMade(self) -> None:\n        assert self.transport\n        if self._certificate is None:\n            with suppress(AttributeError):\n                self._certificate = ssl.Certificate(\n                    self.transport._producer.getPeerCertificate()\n                )\n\n        if self._ip_address is None:\n            self._ip_address = ipaddress.ip_address(\n                self.transport._producer.getPeer().host\n            )\n\n    def dataReceived(self, bodyBytes: bytes) -> None:\n        # This maybe called several times after cancel was called with buffered data.\n        if self._finished.called:\n            return\n\n        assert self.transport\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        bytes_received_result = self._crawler.signals.send_catch_log(\n            signal=signals.bytes_received,\n            data=bodyBytes,\n            request=self._request,\n            spider=self._crawler.spider,\n        )\n        for handler, result in bytes_received_result:\n            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                logger.debug(\n                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n                    {\"request\": self._request, \"handler\": handler.__qualname__},\n                )\n                self.transport.stopProducing()\n                self.transport.loseConnection()\n                failure = result if result.value.fail else None\n                self._finish_response(flags=[\"download_stopped\"], failure=failure)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.warning(\n                \"Received (%(bytes)s) bytes larger than download \"\n                \"max size (%(maxsize)s) in request %(request)s.\",\n                {\n                    \"bytes\": self._bytes_received,\n                    \"maxsize\": self._maxsize,\n                    \"request\": self._request,\n                },\n            )\n            # Clear buffer earlier to avoid keeping data in memory for a long time.\n            self._bodybuf.truncate(0)\n            self._finished.cancel()\n\n        if (\n            self._warnsize\n            and self._bytes_received > self._warnsize\n            and not self._reached_warnsize\n        ):\n            self._reached_warnsize = True\n            logger.warning(\n                \"Received more bytes than download \"\n                \"warn size (%(warnsize)s) in request %(request)s.\",\n                {\"warnsize\": self._warnsize, \"request\": self._request},\n            )\n\n    def connectionLost(self, reason: Failure = connectionDone) -> None:\n        if self._finished.called:\n            return\n\n        if reason.check(ResponseDone):\n            self._finish_response()\n            return\n\n        if reason.check(PotentialDataLoss):\n            self._finish_response(flags=[\"partial\"])\n            return\n\n        if reason.check(ResponseFailed) and any(\n            r.check(_DataLoss) for r in reason.value.reasons\n        ):\n            if not self._fail_on_dataloss:\n                self._finish_response(flags=[\"dataloss\"])\n                return\n\n            if not self._fail_on_dataloss_warned:\n                logger.warning(\n                    \"Got data loss in %s. If you want to process broken \"\n                    \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                    \" -- This message won't be shown in further requests\",\n                    self._txresponse.request.absoluteURI.decode(),\n                )\n                self._fail_on_dataloss_warned = True\n\n        self._finished.errback(reason)\n", "scrapy/core/http2/protocol.py": "import ipaddress\nimport itertools\nimport logging\nfrom collections import deque\nfrom ipaddress import IPv4Address, IPv6Address\nfrom typing import Any, Deque, Dict, List, Optional, Union\n\nfrom h2.config import H2Configuration\nfrom h2.connection import H2Connection\nfrom h2.errors import ErrorCodes\nfrom h2.events import (\n    ConnectionTerminated,\n    DataReceived,\n    Event,\n    ResponseReceived,\n    SettingsAcknowledged,\n    StreamEnded,\n    StreamReset,\n    UnknownFrameReceived,\n    WindowUpdated,\n)\nfrom h2.exceptions import FrameTooLargeError, H2Error\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.error import TimeoutError\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHandshakeListener,\n    IProtocolNegotiationFactory,\n)\nfrom twisted.internet.protocol import Factory, Protocol, connectionDone\nfrom twisted.internet.ssl import Certificate\nfrom twisted.protocols.policies import TimeoutMixin\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import URI\nfrom zope.interface import implementer\n\nfrom scrapy.core.http2.stream import Stream, StreamCloseReason\nfrom scrapy.http import Request\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\n\nlogger = logging.getLogger(__name__)\n\n\nPROTOCOL_NAME = b\"h2\"\n\n\nclass InvalidNegotiatedProtocol(H2Error):\n    def __init__(self, negotiated_protocol: bytes) -> None:\n        self.negotiated_protocol = negotiated_protocol\n\n    def __str__(self) -> str:\n        return f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\"\n\n\nclass RemoteTerminatedConnection(H2Error):\n    def __init__(\n        self,\n        remote_ip_address: Optional[Union[IPv4Address, IPv6Address]],\n        event: ConnectionTerminated,\n    ) -> None:\n        self.remote_ip_address = remote_ip_address\n        self.terminate_event = event\n\n    def __str__(self) -> str:\n        return f\"Received GOAWAY frame from {self.remote_ip_address!r}\"\n\n\nclass MethodNotAllowed405(H2Error):\n    def __init__(\n        self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]\n    ) -> None:\n        self.remote_ip_address = remote_ip_address\n\n    def __str__(self) -> str:\n        return f\"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"\n\n\n@implementer(IHandshakeListener)\nclass H2ClientProtocol(Protocol, TimeoutMixin):\n    IDLE_TIMEOUT = 240\n\n    def __init__(\n        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n    ) -> None:\n        \"\"\"\n        Arguments:\n            uri -- URI of the base url to which HTTP/2 Connection will be made.\n                uri is used to verify that incoming client requests have correct\n                base URL.\n            settings -- Scrapy project settings\n            conn_lost_deferred -- Deferred fires with the reason: Failure to notify\n                that connection was lost\n        \"\"\"\n        self._conn_lost_deferred = conn_lost_deferred\n\n        config = H2Configuration(client_side=True, header_encoding=\"utf-8\")\n        self.conn = H2Connection(config=config)\n\n        # ID of the next request stream\n        # Following the convention - 'Streams initiated by a client MUST\n        # use odd-numbered stream identifiers' (RFC 7540 - Section 5.1.1)\n        self._stream_id_generator = itertools.count(start=1, step=2)\n\n        # Streams are stored in a dictionary keyed off their stream IDs\n        self.streams: Dict[int, Stream] = {}\n\n        # If requests are received before connection is made we keep\n        # all requests in a pool and send them as the connection is made\n        self._pending_request_stream_pool: Deque[Stream] = deque()\n\n        # Save an instance of errors raised which lead to losing the connection\n        # We pass these instances to the streams ResponseFailed() failure\n        self._conn_lost_errors: List[BaseException] = []\n\n        # Some meta data of this connection\n        # initialized when connection is successfully made\n        self.metadata: Dict[str, Any] = {\n            # Peer certificate instance\n            \"certificate\": None,\n            # Address of the server we are connected to which\n            # is updated when HTTP/2 connection is  made successfully\n            \"ip_address\": None,\n            # URI of the peer HTTP/2 connection is made\n            \"uri\": uri,\n            # Both ip_address and uri are used by the Stream before\n            # initiating the request to verify that the base address\n            # Variables taken from Project Settings\n            \"default_download_maxsize\": settings.getint(\"DOWNLOAD_MAXSIZE\"),\n            \"default_download_warnsize\": settings.getint(\"DOWNLOAD_WARNSIZE\"),\n            # Counter to keep track of opened streams. This counter\n            # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n            # streams are opened which leads to ProtocolError\n            # We use simple FIFO policy to handle pending requests\n            \"active_streams\": 0,\n            # Flag to keep track if settings were acknowledged by the remote\n            # This ensures that we have established a HTTP/2 connection\n            \"settings_acknowledged\": False,\n        }\n\n    @property\n    def h2_connected(self) -> bool:\n        \"\"\"Boolean to keep track of the connection status.\n        This is used while initiating pending streams to make sure\n        that we initiate stream only during active HTTP/2 Connection\n        \"\"\"\n        assert self.transport is not None  # typing\n        return bool(self.transport.connected) and self.metadata[\"settings_acknowledged\"]\n\n    @property\n    def allowed_max_concurrent_streams(self) -> int:\n        \"\"\"We keep total two streams for client (sending data) and\n        server side (receiving data) for a single request. To be safe\n        we choose the minimum. Since this value can change in event\n        RemoteSettingsChanged we make variable a property.\n        \"\"\"\n        return min(\n            self.conn.local_settings.max_concurrent_streams,\n            self.conn.remote_settings.max_concurrent_streams,\n        )\n\n    def _send_pending_requests(self) -> None:\n        \"\"\"Initiate all pending requests from the deque following FIFO\n        We make sure that at any time {allowed_max_concurrent_streams}\n        streams are active.\n        \"\"\"\n        while (\n            self._pending_request_stream_pool\n            and self.metadata[\"active_streams\"] < self.allowed_max_concurrent_streams\n            and self.h2_connected\n        ):\n            self.metadata[\"active_streams\"] += 1\n            stream = self._pending_request_stream_pool.popleft()\n            stream.initiate_request()\n            self._write_to_transport()\n\n    def pop_stream(self, stream_id: int) -> Stream:\n        \"\"\"Perform cleanup when a stream is closed\"\"\"\n        stream = self.streams.pop(stream_id)\n        self.metadata[\"active_streams\"] -= 1\n        self._send_pending_requests()\n        return stream\n\n    def _new_stream(self, request: Request, spider: Spider) -> Stream:\n        \"\"\"Instantiates a new Stream object\"\"\"\n        stream = Stream(\n            stream_id=next(self._stream_id_generator),\n            request=request,\n            protocol=self,\n            download_maxsize=getattr(\n                spider, \"download_maxsize\", self.metadata[\"default_download_maxsize\"]\n            ),\n            download_warnsize=getattr(\n                spider, \"download_warnsize\", self.metadata[\"default_download_warnsize\"]\n            ),\n        )\n        self.streams[stream.stream_id] = stream\n        return stream\n\n    def _write_to_transport(self) -> None:\n        \"\"\"Write data to the underlying transport connection\n        from the HTTP2 connection instance if any\n        \"\"\"\n        assert self.transport is not None  # typing\n        # Reset the idle timeout as connection is still actively sending data\n        self.resetTimeout()\n\n        data = self.conn.data_to_send()\n        self.transport.write(data)\n\n    def request(self, request: Request, spider: Spider) -> Deferred:\n        if not isinstance(request, Request):\n            raise TypeError(\n                f\"Expected scrapy.http.Request, received {request.__class__.__qualname__}\"\n            )\n\n        stream = self._new_stream(request, spider)\n        d = stream.get_response()\n\n        # Add the stream to the request pool\n        self._pending_request_stream_pool.append(stream)\n\n        # If we receive a request when connection is idle\n        # We need to initiate pending requests\n        self._send_pending_requests()\n        return d\n\n    def connectionMade(self) -> None:\n        \"\"\"Called by Twisted when the connection is established. We can start\n        sending some data now: we should open with the connection preamble.\n        \"\"\"\n        # Initialize the timeout\n        self.setTimeout(self.IDLE_TIMEOUT)\n\n        assert self.transport is not None  # typing\n        destination = self.transport.getPeer()\n        self.metadata[\"ip_address\"] = ipaddress.ip_address(destination.host)\n\n        # Initiate H2 Connection\n        self.conn.initiate_connection()\n        self._write_to_transport()\n\n    def _lose_connection_with_error(self, errors: List[BaseException]) -> None:\n        \"\"\"Helper function to lose the connection with the error sent as a\n        reason\"\"\"\n        self._conn_lost_errors += errors\n        assert self.transport is not None  # typing\n        self.transport.loseConnection()\n\n    def handshakeCompleted(self) -> None:\n        \"\"\"\n        Close the connection if it's not made via the expected protocol\n        \"\"\"\n        assert self.transport is not None  # typing\n        if (\n            self.transport.negotiatedProtocol is not None\n            and self.transport.negotiatedProtocol != PROTOCOL_NAME\n        ):\n            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n            self._lose_connection_with_error(\n                [InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)]\n            )\n\n    def _check_received_data(self, data: bytes) -> None:\n        \"\"\"Checks for edge cases where the connection to remote fails\n        without raising an appropriate H2Error\n\n        Arguments:\n            data -- Data received from the remote\n        \"\"\"\n        if data.startswith(b\"HTTP/2.0 405 Method Not Allowed\"):\n            raise MethodNotAllowed405(self.metadata[\"ip_address\"])\n\n    def dataReceived(self, data: bytes) -> None:\n        # Reset the idle timeout as connection is still actively receiving data\n        self.resetTimeout()\n\n        try:\n            self._check_received_data(data)\n            events = self.conn.receive_data(data)\n            self._handle_events(events)\n        except H2Error as e:\n            if isinstance(e, FrameTooLargeError):\n                # hyper-h2 does not drop the connection in this scenario, we\n                # need to abort the connection manually.\n                self._conn_lost_errors += [e]\n                assert self.transport is not None  # typing\n                self.transport.abortConnection()\n                return\n\n            # Save this error as ultimately the connection will be dropped\n            # internally by hyper-h2. Saved error will be passed to all the streams\n            # closed with the connection.\n            self._lose_connection_with_error([e])\n        finally:\n            self._write_to_transport()\n\n    def timeoutConnection(self) -> None:\n        \"\"\"Called when the connection times out.\n        We lose the connection with TimeoutError\"\"\"\n\n        # Check whether there are open streams. If there are, we're going to\n        # want to use the error code PROTOCOL_ERROR. If there aren't, use\n        # NO_ERROR.\n        if (\n            self.conn.open_outbound_streams > 0\n            or self.conn.open_inbound_streams > 0\n            or self.metadata[\"active_streams\"] > 0\n        ):\n            error_code = ErrorCodes.PROTOCOL_ERROR\n        else:\n            error_code = ErrorCodes.NO_ERROR\n        self.conn.close_connection(error_code=error_code)\n        self._write_to_transport()\n\n        self._lose_connection_with_error(\n            [TimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")]\n        )\n\n    def connectionLost(self, reason: Failure = connectionDone) -> None:\n        \"\"\"Called by Twisted when the transport connection is lost.\n        No need to write anything to transport here.\n        \"\"\"\n        # Cancel the timeout if not done yet\n        self.setTimeout(None)\n\n        # Notify the connection pool instance such that no new requests are\n        # sent over current connection\n        if not reason.check(connectionDone):\n            self._conn_lost_errors.append(reason)\n\n        self._conn_lost_deferred.callback(self._conn_lost_errors)\n\n        for stream in self.streams.values():\n            if stream.metadata[\"request_sent\"]:\n                close_reason = StreamCloseReason.CONNECTION_LOST\n            else:\n                close_reason = StreamCloseReason.INACTIVE\n            stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n\n        self.metadata[\"active_streams\"] -= len(self.streams)\n        self.streams.clear()\n        self._pending_request_stream_pool.clear()\n        self.conn.close_connection()\n\n    def _handle_events(self, events: List[Event]) -> None:\n        \"\"\"Private method which acts as a bridge between the events\n        received from the HTTP/2 data and IH2EventsHandler\n\n        Arguments:\n            events -- A list of events that the remote peer triggered by sending data\n        \"\"\"\n        for event in events:\n            if isinstance(event, ConnectionTerminated):\n                self.connection_terminated(event)\n            elif isinstance(event, DataReceived):\n                self.data_received(event)\n            elif isinstance(event, ResponseReceived):\n                self.response_received(event)\n            elif isinstance(event, StreamEnded):\n                self.stream_ended(event)\n            elif isinstance(event, StreamReset):\n                self.stream_reset(event)\n            elif isinstance(event, WindowUpdated):\n                self.window_updated(event)\n            elif isinstance(event, SettingsAcknowledged):\n                self.settings_acknowledged(event)\n            elif isinstance(event, UnknownFrameReceived):\n                logger.warning(\"Unknown frame received: %s\", event.frame)\n\n    # Event handler functions starts here\n    def connection_terminated(self, event: ConnectionTerminated) -> None:\n        self._lose_connection_with_error(\n            [RemoteTerminatedConnection(self.metadata[\"ip_address\"], event)]\n        )\n\n    def data_received(self, event: DataReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_data(event.data, event.flow_controlled_length)\n\n    def response_received(self, event: ResponseReceived) -> None:\n        try:\n            stream = self.streams[event.stream_id]\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.receive_headers(event.headers)\n\n    def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n        self.metadata[\"settings_acknowledged\"] = True\n\n        # Send off all the pending requests as now we have\n        # established a proper HTTP/2 connection\n        self._send_pending_requests()\n\n        # Update certificate when our HTTP/2 connection is established\n        assert self.transport is not None  # typing\n        self.metadata[\"certificate\"] = Certificate(self.transport.getPeerCertificate())\n\n    def stream_ended(self, event: StreamEnded) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.ENDED, from_protocol=True)\n\n    def stream_reset(self, event: StreamReset) -> None:\n        try:\n            stream = self.pop_stream(event.stream_id)\n        except KeyError:\n            pass  # We ignore server-initiated events\n        else:\n            stream.close(StreamCloseReason.RESET, from_protocol=True)\n\n    def window_updated(self, event: WindowUpdated) -> None:\n        if event.stream_id != 0:\n            self.streams[event.stream_id].receive_window_update()\n        else:\n            # Send leftover data for all the streams\n            for stream in self.streams.values():\n                stream.receive_window_update()\n\n\n@implementer(IProtocolNegotiationFactory)\nclass H2ClientFactory(Factory):\n    def __init__(\n        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n    ) -> None:\n        self.uri = uri\n        self.settings = settings\n        self.conn_lost_deferred = conn_lost_deferred\n\n    def buildProtocol(self, addr: IAddress) -> H2ClientProtocol:\n        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n\n    def acceptableProtocols(self) -> List[bytes]:\n        return [PROTOCOL_NAME]\n", "scrapy/core/http2/stream.py": "import logging\nfrom enum import Enum\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n\nfrom h2.errors import ErrorCodes\nfrom h2.exceptions import H2Error, ProtocolError, StreamClosedError\nfrom hpack import HeaderTuple\nfrom twisted.internet.defer import CancelledError, Deferred\nfrom twisted.internet.error import ConnectionClosed\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy.http import Request\nfrom scrapy.http.headers import Headers\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    from scrapy.core.http2.protocol import H2ClientProtocol\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass InactiveStreamClosed(ConnectionClosed):\n    \"\"\"Connection was closed without sending request headers\n    of the stream. This happens when a stream is waiting for other\n    streams to close and connection is lost.\"\"\"\n\n    def __init__(self, request: Request) -> None:\n        self.request = request\n\n    def __str__(self) -> str:\n        return f\"InactiveStreamClosed: Connection was closed without sending the request {self.request!r}\"\n\n\nclass InvalidHostname(H2Error):\n    def __init__(\n        self, request: Request, expected_hostname: str, expected_netloc: str\n    ) -> None:\n        self.request = request\n        self.expected_hostname = expected_hostname\n        self.expected_netloc = expected_netloc\n\n    def __str__(self) -> str:\n        return f\"InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}\"\n\n\nclass StreamCloseReason(Enum):\n    # Received a StreamEnded event from the remote\n    ENDED = 1\n\n    # Received a StreamReset event -- ended abruptly\n    RESET = 2\n\n    # Transport connection was lost\n    CONNECTION_LOST = 3\n\n    # Expected response body size is more than allowed limit\n    MAXSIZE_EXCEEDED = 4\n\n    # Response deferred is cancelled by the client\n    # (happens when client called response_deferred.cancel())\n    CANCELLED = 5\n\n    # Connection lost and the stream was not initiated\n    INACTIVE = 6\n\n    # The hostname of the request is not same as of connected peer hostname\n    # As a result sending this request will the end the connection\n    INVALID_HOSTNAME = 7\n\n\nclass Stream:\n    \"\"\"Represents a single HTTP/2 Stream.\n\n    Stream is a bidirectional flow of bytes within an established connection,\n    which may carry one or more messages. Handles the transfer of HTTP Headers\n    and Data frames.\n\n    Role of this class is to\n    1. Combine all the data frames\n    \"\"\"\n\n    def __init__(\n        self,\n        stream_id: int,\n        request: Request,\n        protocol: \"H2ClientProtocol\",\n        download_maxsize: int = 0,\n        download_warnsize: int = 0,\n    ) -> None:\n        \"\"\"\n        Arguments:\n            stream_id -- Unique identifier for the stream within a single HTTP/2 connection\n            request -- The HTTP request associated to the stream\n            protocol -- Parent H2ClientProtocol instance\n        \"\"\"\n        self.stream_id: int = stream_id\n        self._request: Request = request\n        self._protocol: \"H2ClientProtocol\" = protocol\n\n        self._download_maxsize = self._request.meta.get(\n            \"download_maxsize\", download_maxsize\n        )\n        self._download_warnsize = self._request.meta.get(\n            \"download_warnsize\", download_warnsize\n        )\n\n        # Metadata of an HTTP/2 connection stream\n        # initialized when stream is instantiated\n        self.metadata: Dict[str, Any] = {\n            \"request_content_length\": (\n                0 if self._request.body is None else len(self._request.body)\n            ),\n            # Flag to keep track whether the stream has initiated the request\n            \"request_sent\": False,\n            # Flag to track whether we have logged about exceeding download warnsize\n            \"reached_warnsize\": False,\n            # Each time we send a data frame, we will decrease value by the amount send.\n            \"remaining_content_length\": (\n                0 if self._request.body is None else len(self._request.body)\n            ),\n            # Flag to keep track whether client (self) have closed this stream\n            \"stream_closed_local\": False,\n            # Flag to keep track whether the server has closed the stream\n            \"stream_closed_server\": False,\n        }\n\n        # Private variable used to build the response\n        # this response is then converted to appropriate Response class\n        # passed to the response deferred callback\n        self._response: Dict[str, Any] = {\n            # Data received frame by frame from the server is appended\n            # and passed to the response Deferred when completely received.\n            \"body\": BytesIO(),\n            # The amount of data received that counts against the\n            # flow control window\n            \"flow_controlled_size\": 0,\n            # Headers received after sending the request\n            \"headers\": Headers({}),\n        }\n\n        def _cancel(_: Any) -> None:\n            # Close this stream as gracefully as possible\n            # If the associated request is initiated we reset this stream\n            # else we directly call close() method\n            if self.metadata[\"request_sent\"]:\n                self.reset_stream(StreamCloseReason.CANCELLED)\n            else:\n                self.close(StreamCloseReason.CANCELLED)\n\n        self._deferred_response: Deferred = Deferred(_cancel)\n\n    def __repr__(self) -> str:\n        return f\"Stream(id={self.stream_id!r})\"\n\n    @property\n    def _log_warnsize(self) -> bool:\n        \"\"\"Checks if we have received data which exceeds the download warnsize\n        and whether we have not already logged about it.\n\n        Returns:\n            True if both the above conditions hold true\n            False if any of the conditions is false\n        \"\"\"\n        content_length_header = int(\n            self._response[\"headers\"].get(b\"Content-Length\", -1)\n        )\n        return (\n            self._download_warnsize\n            and (\n                self._response[\"flow_controlled_size\"] > self._download_warnsize\n                or content_length_header > self._download_warnsize\n            )\n            and not self.metadata[\"reached_warnsize\"]\n        )\n\n    def get_response(self) -> Deferred:\n        \"\"\"Simply return a Deferred which fires when response\n        from the asynchronous request is available\n        \"\"\"\n        return self._deferred_response\n\n    def check_request_url(self) -> bool:\n        # Make sure that we are sending the request to the correct URL\n        url = urlparse_cached(self._request)\n        return (\n            url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n            or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n            or url.netloc\n            == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n        )\n\n    def _get_request_headers(self) -> List[Tuple[str, str]]:\n        url = urlparse_cached(self._request)\n\n        path = url.path\n        if url.query:\n            path += \"?\" + url.query\n\n        # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n        # URIs; \"http\" or \"https\" URIs that do not contain a path component\n        # MUST include a value of '/'. The exception to this rule is an\n        # OPTIONS request for an \"http\" or \"https\" URI that does not include\n        # a path component; these MUST include a \":path\" pseudo-header field\n        # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n        if not path:\n            path = \"*\" if self._request.method == \"OPTIONS\" else \"/\"\n\n        # Make sure pseudo-headers comes before all the other headers\n        headers = [\n            (\":method\", self._request.method),\n            (\":authority\", url.netloc),\n        ]\n\n        # The \":scheme\" and \":path\" pseudo-header fields MUST\n        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n        if self._request.method != \"CONNECT\":\n            headers += [\n                (\":scheme\", self._protocol.metadata[\"uri\"].scheme),\n                (\":path\", path),\n            ]\n\n        content_length = str(len(self._request.body))\n        headers.append((\"Content-Length\", content_length))\n\n        content_length_name = self._request.headers.normkey(b\"Content-Length\")\n        for name, values in self._request.headers.items():\n            for value_bytes in values:\n                value = str(value_bytes, \"utf-8\")\n                if name == content_length_name:\n                    if value != content_length:\n                        logger.warning(\n                            \"Ignoring bad Content-Length header %r of request %r, \"\n                            \"sending %r instead\",\n                            value,\n                            self._request,\n                            content_length,\n                        )\n                    continue\n                headers.append((str(name, \"utf-8\"), value))\n\n        return headers\n\n    def initiate_request(self) -> None:\n        if self.check_request_url():\n            headers = self._get_request_headers()\n            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n            self.metadata[\"request_sent\"] = True\n            self.send_data()\n        else:\n            # Close this stream calling the response errback\n            # Note that we have not sent any headers\n            self.close(StreamCloseReason.INVALID_HOSTNAME)\n\n    def send_data(self) -> None:\n        \"\"\"Called immediately after the headers are sent. Here we send all the\n        data as part of the request.\n\n        If the content length is 0 initially then we end the stream immediately and\n        wait for response data.\n\n        Warning: Only call this method when stream not closed from client side\n           and has initiated request already by sending HEADER frame. If not then\n           stream will raise ProtocolError (raise by h2 state machine).\n        \"\"\"\n        if self.metadata[\"stream_closed_local\"]:\n            raise StreamClosedError(self.stream_id)\n\n        # Firstly, check what the flow control window is for current stream.\n        window_size = self._protocol.conn.local_flow_control_window(\n            stream_id=self.stream_id\n        )\n\n        # Next, check what the maximum frame size is.\n        max_frame_size = self._protocol.conn.max_outbound_frame_size\n\n        # We will send no more than the window size or the remaining file size\n        # of data in this call, whichever is smaller.\n        bytes_to_send_size = min(window_size, self.metadata[\"remaining_content_length\"])\n\n        # We now need to send a number of data frames.\n        while bytes_to_send_size > 0:\n            chunk_size = min(bytes_to_send_size, max_frame_size)\n\n            data_chunk_start_id = (\n                self.metadata[\"request_content_length\"]\n                - self.metadata[\"remaining_content_length\"]\n            )\n            data_chunk = self._request.body[\n                data_chunk_start_id : data_chunk_start_id + chunk_size\n            ]\n\n            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n\n            bytes_to_send_size -= chunk_size\n            self.metadata[\"remaining_content_length\"] -= chunk_size\n\n        self.metadata[\"remaining_content_length\"] = max(\n            0, self.metadata[\"remaining_content_length\"]\n        )\n\n        # End the stream if no more data needs to be send\n        if self.metadata[\"remaining_content_length\"] == 0:\n            self._protocol.conn.end_stream(self.stream_id)\n\n        # Q. What about the rest of the data?\n        # Ans: Remaining Data frames will be sent when we get a WindowUpdate frame\n\n    def receive_window_update(self) -> None:\n        \"\"\"Flow control window size was changed.\n        Send data that earlier could not be sent as we were\n        blocked behind the flow control.\n        \"\"\"\n        if (\n            self.metadata[\"remaining_content_length\"]\n            and not self.metadata[\"stream_closed_server\"]\n            and self.metadata[\"request_sent\"]\n        ):\n            self.send_data()\n\n    def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n        self._response[\"body\"].write(data)\n        self._response[\"flow_controlled_size\"] += flow_controlled_length\n\n        # We check maxsize here in case the Content-Length header was not received\n        if (\n            self._download_maxsize\n            and self._response[\"flow_controlled_size\"] > self._download_maxsize\n        ):\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata[\"reached_warnsize\"] = True\n            warning_msg = (\n                f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n                f\"warn size ({self._download_warnsize}) in request {self._request}\"\n            )\n            logger.warning(warning_msg)\n\n        # Acknowledge the data received\n        self._protocol.conn.acknowledge_received_data(\n            self._response[\"flow_controlled_size\"], self.stream_id\n        )\n\n    def receive_headers(self, headers: List[HeaderTuple]) -> None:\n        for name, value in headers:\n            self._response[\"headers\"].appendlist(name, value)\n\n        # Check if we exceed the allowed max data size which can be received\n        expected_size = int(self._response[\"headers\"].get(b\"Content-Length\", -1))\n        if self._download_maxsize and expected_size > self._download_maxsize:\n            self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n            return\n\n        if self._log_warnsize:\n            self.metadata[\"reached_warnsize\"] = True\n            warning_msg = (\n                f\"Expected response size ({expected_size}) larger than \"\n                f\"download warn size ({self._download_warnsize}) in request {self._request}\"\n            )\n            logger.warning(warning_msg)\n\n    def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n        \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n        if self.metadata[\"stream_closed_local\"]:\n            raise StreamClosedError(self.stream_id)\n\n        # Clear buffer earlier to avoid keeping data in memory for a long time\n        self._response[\"body\"].truncate(0)\n\n        self.metadata[\"stream_closed_local\"] = True\n        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n        self.close(reason)\n\n    def close(\n        self,\n        reason: StreamCloseReason,\n        errors: Optional[List[BaseException]] = None,\n        from_protocol: bool = False,\n    ) -> None:\n        \"\"\"Based on the reason sent we will handle each case.\"\"\"\n        if self.metadata[\"stream_closed_server\"]:\n            raise StreamClosedError(self.stream_id)\n\n        if not isinstance(reason, StreamCloseReason):\n            raise TypeError(\n                f\"Expected StreamCloseReason, received {reason.__class__.__qualname__}\"\n            )\n\n        # Have default value of errors as an empty list as\n        # some cases can add a list of exceptions\n        errors = errors or []\n\n        if not from_protocol:\n            self._protocol.pop_stream(self.stream_id)\n\n        self.metadata[\"stream_closed_server\"] = True\n\n        # We do not check for Content-Length or Transfer-Encoding in response headers\n        # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n        # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)\n\n        # NOTE: Order of handling the events is important here\n        # As we immediately cancel the request when maxsize is exceeded while\n        # receiving DATA_FRAME's when we have received the headers (not\n        # having Content-Length)\n        if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n            expected_size = int(\n                self._response[\"headers\"].get(\n                    b\"Content-Length\", self._response[\"flow_controlled_size\"]\n                )\n            )\n            error_msg = (\n                f\"Cancelling download of {self._request.url}: received response \"\n                f\"size ({expected_size}) larger than download max size ({self._download_maxsize})\"\n            )\n            logger.error(error_msg)\n            self._deferred_response.errback(CancelledError(error_msg))\n\n        elif reason is StreamCloseReason.ENDED:\n            self._fire_response_deferred()\n\n        # Stream was abruptly ended here\n        elif reason is StreamCloseReason.CANCELLED:\n            # Client has cancelled the request. Remove all the data\n            # received and fire the response deferred with no flags set\n\n            # NOTE: The data is already flushed in Stream.reset_stream() called\n            # immediately when the stream needs to be cancelled\n\n            # There maybe no :status in headers, we make\n            # HTTP Status Code: 499 - Client Closed Request\n            self._response[\"headers\"][\":status\"] = \"499\"\n            self._fire_response_deferred()\n\n        elif reason is StreamCloseReason.RESET:\n            self._deferred_response.errback(\n                ResponseFailed(\n                    [\n                        Failure(\n                            f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n                            ProtocolError,\n                        )\n                    ]\n                )\n            )\n\n        elif reason is StreamCloseReason.CONNECTION_LOST:\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        elif reason is StreamCloseReason.INACTIVE:\n            errors.insert(0, InactiveStreamClosed(self._request))\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        else:\n            assert reason is StreamCloseReason.INVALID_HOSTNAME\n            self._deferred_response.errback(\n                InvalidHostname(\n                    self._request,\n                    str(self._protocol.metadata[\"uri\"].host, \"utf-8\"),\n                    f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}',\n                )\n            )\n\n    def _fire_response_deferred(self) -> None:\n        \"\"\"Builds response from the self._response dict\n        and fires the response deferred callback with the\n        generated response instance\"\"\"\n\n        body = self._response[\"body\"].getvalue()\n        response_cls = responsetypes.from_args(\n            headers=self._response[\"headers\"],\n            url=self._request.url,\n            body=body,\n        )\n\n        response = response_cls(\n            url=self._request.url,\n            status=int(self._response[\"headers\"][\":status\"]),\n            headers=self._response[\"headers\"],\n            body=body,\n            request=self._request,\n            certificate=self._protocol.metadata[\"certificate\"],\n            ip_address=self._protocol.metadata[\"ip_address\"],\n            protocol=\"h2\",\n        )\n\n        self._deferred_response.callback(response)\n", "scrapy/core/http2/agent.py": "from collections import deque\nfrom typing import Deque, Dict, List, Optional, Tuple\n\nfrom twisted.internet import defer\nfrom twisted.internet.base import ReactorBase\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.endpoints import HostnameEndpoint\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import (\n    URI,\n    BrowserLikePolicyForHTTPS,\n    ResponseFailed,\n    _StandardEndpointFactory,\n)\nfrom twisted.web.error import SchemeNotSupported\n\nfrom scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\nfrom scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\nfrom scrapy.http.request import Request\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\n\nConnectionKeyT = Tuple[bytes, bytes, int]\n\n\nclass H2ConnectionPool:\n    def __init__(self, reactor: ReactorBase, settings: Settings) -> None:\n        self._reactor = reactor\n        self.settings = settings\n\n        # Store a dictionary which is used to get the respective\n        # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)\n        self._connections: Dict[ConnectionKeyT, H2ClientProtocol] = {}\n\n        # Save all requests that arrive before the connection is established\n        self._pending_requests: Dict[ConnectionKeyT, Deque[Deferred]] = {}\n\n    def get_connection(\n        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n    ) -> Deferred:\n        if key in self._pending_requests:\n            # Received a request while connecting to remote\n            # Create a deferred which will fire with the H2ClientProtocol\n            # instance\n            d: Deferred = Deferred()\n            self._pending_requests[key].append(d)\n            return d\n\n        # Check if we already have a connection to the remote\n        conn = self._connections.get(key, None)\n        if conn:\n            # Return this connection instance wrapped inside a deferred\n            return defer.succeed(conn)\n\n        # No connection is established for the given URI\n        return self._new_connection(key, uri, endpoint)\n\n    def _new_connection(\n        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n    ) -> Deferred:\n        self._pending_requests[key] = deque()\n\n        conn_lost_deferred: Deferred = Deferred()\n        conn_lost_deferred.addCallback(self._remove_connection, key)\n\n        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n        conn_d = endpoint.connect(factory)\n        conn_d.addCallback(self.put_connection, key)\n\n        d: Deferred = Deferred()\n        self._pending_requests[key].append(d)\n        return d\n\n    def put_connection(\n        self, conn: H2ClientProtocol, key: ConnectionKeyT\n    ) -> H2ClientProtocol:\n        self._connections[key] = conn\n\n        # Now as we have established a proper HTTP/2 connection\n        # we fire all the deferred's with the connection instance\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.callback(conn)\n\n        return conn\n\n    def _remove_connection(\n        self, errors: List[BaseException], key: ConnectionKeyT\n    ) -> None:\n        self._connections.pop(key)\n\n        # Call the errback of all the pending requests for this connection\n        pending_requests = self._pending_requests.pop(key, None)\n        while pending_requests:\n            d = pending_requests.popleft()\n            d.errback(ResponseFailed(errors))\n\n    def close_connections(self) -> None:\n        \"\"\"Close all the HTTP/2 connections and remove them from pool\n\n        Returns:\n            Deferred that fires when all connections have been closed\n        \"\"\"\n        for conn in self._connections.values():\n            assert conn.transport is not None  # typing\n            conn.transport.abortConnection()\n\n\nclass H2Agent:\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        pool: H2ConnectionPool,\n        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n        connect_timeout: Optional[float] = None,\n        bind_address: Optional[bytes] = None,\n    ) -> None:\n        self._reactor = reactor\n        self._pool = pool\n        self._context_factory = AcceptableProtocolsContextFactory(\n            context_factory, acceptable_protocols=[b\"h2\"]\n        )\n        self.endpoint_factory = _StandardEndpointFactory(\n            self._reactor, self._context_factory, connect_timeout, bind_address\n        )\n\n    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n        return self.endpoint_factory.endpointForURI(uri)\n\n    def get_key(self, uri: URI) -> ConnectionKeyT:\n        \"\"\"\n        Arguments:\n            uri - URI obtained directly from request URL\n        \"\"\"\n        return uri.scheme, uri.host, uri.port\n\n    def request(self, request: Request, spider: Spider) -> Deferred:\n        uri = URI.fromBytes(bytes(request.url, encoding=\"utf-8\"))\n        try:\n            endpoint = self.get_endpoint(uri)\n        except SchemeNotSupported:\n            return defer.fail(Failure())\n\n        key = self.get_key(uri)\n        d = self._pool.get_connection(key, uri, endpoint)\n        d.addCallback(lambda conn: conn.request(request, spider))\n        return d\n\n\nclass ScrapyProxyH2Agent(H2Agent):\n    def __init__(\n        self,\n        reactor: ReactorBase,\n        proxy_uri: URI,\n        pool: H2ConnectionPool,\n        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n        connect_timeout: Optional[float] = None,\n        bind_address: Optional[bytes] = None,\n    ) -> None:\n        super().__init__(\n            reactor=reactor,\n            pool=pool,\n            context_factory=context_factory,\n            connect_timeout=connect_timeout,\n            bind_address=bind_address,\n        )\n        self._proxy_uri = proxy_uri\n\n    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n        return self.endpoint_factory.endpointForURI(self._proxy_uri)\n\n    def get_key(self, uri: URI) -> ConnectionKeyT:\n        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n        return b\"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n", "scrapy/core/http2/__init__.py": "", "scrapy/extensions/corestats.py": "\"\"\"\nExtension for collecting core stats like items scraped and start/finish times\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass CoreStats:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n        self.start_time: Optional[datetime] = None\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.start_time = datetime.now(tz=timezone.utc)\n        self.stats.set_value(\"start_time\", self.start_time, spider=spider)\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        assert self.start_time is not None\n        finish_time = datetime.now(tz=timezone.utc)\n        elapsed_time = finish_time - self.start_time\n        elapsed_time_seconds = elapsed_time.total_seconds()\n        self.stats.set_value(\n            \"elapsed_time_seconds\", elapsed_time_seconds, spider=spider\n        )\n        self.stats.set_value(\"finish_time\", finish_time, spider=spider)\n        self.stats.set_value(\"finish_reason\", reason, spider=spider)\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        self.stats.inc_value(\"item_scraped_count\", spider=spider)\n\n    def response_received(self, spider: Spider) -> None:\n        self.stats.inc_value(\"response_received_count\", spider=spider)\n\n    def item_dropped(self, item: Any, spider: Spider, exception: BaseException) -> None:\n        reason = exception.__class__.__name__\n        self.stats.inc_value(\"item_dropped_count\", spider=spider)\n        self.stats.inc_value(f\"item_dropped_reasons_count/{reason}\", spider=spider)\n", "scrapy/extensions/debug.py": "\"\"\"\nExtensions for debugging Scrapy\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport signal\nimport sys\nimport threading\nimport traceback\nfrom pdb import Pdb\nfrom types import FrameType\nfrom typing import TYPE_CHECKING, Optional\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.engine import format_engine_status\nfrom scrapy.utils.trackref import format_live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass StackTraceDump:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        try:\n            signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n            signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n        except AttributeError:\n            # win32 platforms don't support SIGUSR signals\n            pass\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def dump_stacktrace(self, signum: int, frame: Optional[FrameType]) -> None:\n        assert self.crawler.engine\n        log_args = {\n            \"stackdumps\": self._thread_stacks(),\n            \"enginestatus\": format_engine_status(self.crawler.engine),\n            \"liverefs\": format_live_refs(),\n        }\n        logger.info(\n            \"Dumping stack trace and engine status\\n\"\n            \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n            log_args,\n            extra={\"crawler\": self.crawler},\n        )\n\n    def _thread_stacks(self) -> str:\n        id2name = {th.ident: th.name for th in threading.enumerate()}\n        dumps = \"\"\n        for id_, frame in sys._current_frames().items():\n            name = id2name.get(id_, \"\")\n            dump = \"\".join(traceback.format_stack(frame))\n            dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n        return dumps\n\n\nclass Debugger:\n    def __init__(self) -> None:\n        try:\n            signal.signal(signal.SIGUSR2, self._enter_debugger)\n        except AttributeError:\n            # win32 platforms don't support SIGUSR signals\n            pass\n\n    def _enter_debugger(self, signum: int, frame: Optional[FrameType]) -> None:\n        assert frame\n        Pdb().set_trace(frame.f_back)  # noqa: T100\n", "scrapy/extensions/memusage.py": "\"\"\"\nMemoryUsage extension\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport socket\nimport sys\nfrom importlib import import_module\nfrom pprint import pformat\nfrom typing import TYPE_CHECKING, List\n\nfrom twisted.internet import task\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.mail import MailSender\nfrom scrapy.utils.engine import get_engine_status\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass MemoryUsage:\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"MEMUSAGE_ENABLED\"):\n            raise NotConfigured\n        try:\n            # stdlib's resource module is only available on unix platforms.\n            self.resource = import_module(\"resource\")\n        except ImportError:\n            raise NotConfigured\n\n        self.crawler: Crawler = crawler\n        self.warned: bool = False\n        self.notify_mails: List[str] = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n        self.limit: int = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n        self.warning: int = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n        self.check_interval: float = crawler.settings.getfloat(\n            \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n        )\n        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def get_virtual_size(self) -> int:\n        size: int = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != \"darwin\":\n            # on macOS ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size\n\n    def engine_started(self) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n        self.tasks: List[task.LoopingCall] = []\n        tsk = task.LoopingCall(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = task.LoopingCall(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = task.LoopingCall(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n\n    def engine_stopped(self) -> None:\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()\n\n    def update(self) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.max_value(\"memusage/max\", self.get_virtual_size())\n\n    def _check_limit(self) -> None:\n        assert self.crawler.engine\n        assert self.crawler.stats\n        peak_mem_usage = self.get_virtual_size()\n        if peak_mem_usage > self.limit:\n            self.crawler.stats.set_value(\"memusage/limit_reached\", 1)\n            mem = self.limit / 1024 / 1024\n            logger.error(\n                \"Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...\",\n                {\"memusage\": mem},\n                extra={\"crawler\": self.crawler},\n            )\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n                    f\"memory usage exceeded {mem}MiB at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value(\"memusage/limit_notified\", 1)\n\n            if self.crawler.engine.spider is not None:\n                self.crawler.engine.close_spider(\n                    self.crawler.engine.spider, \"memusage_exceeded\"\n                )\n            else:\n                self.crawler.stop()\n        else:\n            logger.info(\n                \"Peak memory usage is %(virtualsize)dMiB\",\n                {\"virtualsize\": peak_mem_usage / 1024 / 1024},\n            )\n\n    def _check_warning(self) -> None:\n        if self.warned:  # warn only once\n            return\n        assert self.crawler.stats\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value(\"memusage/warning_reached\", 1)\n            mem = self.warning / 1024 / 1024\n            logger.warning(\n                \"Memory usage reached %(memusage)dMiB\",\n                {\"memusage\": mem},\n                extra={\"crawler\": self.crawler},\n            )\n            if self.notify_mails:\n                subj = (\n                    f\"{self.crawler.settings['BOT_NAME']} warning: \"\n                    f\"memory usage reached {mem}MiB at {socket.gethostname()}\"\n                )\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value(\"memusage/warning_notified\", 1)\n            self.warned = True\n\n    def _send_report(self, rcpts: List[str], subject: str) -> None:\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        assert self.crawler.engine\n        assert self.crawler.stats\n        stats = self.crawler.stats\n        s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup') / 1024 / 1024}M\\r\\n\"\n        s += f\"Maximum memory usage          : {stats.get_value('memusage/max') / 1024 / 1024}M\\r\\n\"\n        s += f\"Current memory usage          : {self.get_virtual_size() / 1024 / 1024}M\\r\\n\"\n\n        s += (\n            \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        )\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)\n", "scrapy/extensions/logstats.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n\nfrom twisted.internet import task\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass LogStats:\n    \"\"\"Log basic scraping stats periodically like:\n    * RPM - Requests per Minute\n    * IPM - Items per Minute\n    \"\"\"\n\n    def __init__(self, stats: StatsCollector, interval: float = 60.0):\n        self.stats: StatsCollector = stats\n        self.interval: float = interval\n        self.multiplier: float = 60.0 / self.interval\n        self.task: Optional[task.LoopingCall] = None\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n        if not interval:\n            raise NotConfigured\n        assert crawler.stats\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.pagesprev: int = 0\n        self.itemsprev: int = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)\n\n    def log(self, spider: Spider) -> None:\n        self.calculate_stats()\n\n        msg = (\n            \"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n            \"scraped %(items)d items (at %(itemrate)d items/min)\"\n        )\n        log_args = {\n            \"pages\": self.pages,\n            \"pagerate\": self.prate,\n            \"items\": self.items,\n            \"itemrate\": self.irate,\n        }\n        logger.info(msg, log_args, extra={\"spider\": spider})\n\n    def calculate_stats(self) -> None:\n        self.items: int = self.stats.get_value(\"item_scraped_count\", 0)\n        self.pages: int = self.stats.get_value(\"response_received_count\", 0)\n        self.irate: float = (self.items - self.itemsprev) * self.multiplier\n        self.prate: float = (self.pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = self.pages, self.items\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        if self.task and self.task.running:\n            self.task.stop()\n\n        rpm_final, ipm_final = self.calculate_final_stats(spider)\n        self.stats.set_value(\"responses_per_minute\", rpm_final)\n        self.stats.set_value(\"items_per_minute\", ipm_final)\n\n    def calculate_final_stats(\n        self, spider: Spider\n    ) -> Union[Tuple[None, None], Tuple[float, float]]:\n        start_time = self.stats.get_value(\"start_time\")\n        finished_time = self.stats.get_value(\"finished_time\")\n\n        if not start_time or not finished_time:\n            return None, None\n\n        mins_elapsed = (finished_time - start_time).seconds / 60\n\n        items = self.stats.get_value(\"item_scraped_count\", 0)\n        pages = self.stats.get_value(\"response_received_count\", 0)\n\n        return (pages / mins_elapsed), (items / mins_elapsed)\n", "scrapy/extensions/httpcache.py": "import gzip\nimport logging\nimport os\nimport pickle  # nosec\nfrom email.utils import mktime_tz, parsedate_tz\nfrom importlib import import_module\nfrom pathlib import Path\nfrom time import time\nfrom types import ModuleType\nfrom typing import IO, TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union, cast\nfrom weakref import WeakKeyDictionary\n\nfrom w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n\nfrom scrapy.http import Headers, Response\nfrom scrapy.http.request import Request\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.settings import BaseSettings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.project import data_path\nfrom scrapy.utils.python import to_bytes, to_unicode\nfrom scrapy.utils.request import RequestFingerprinter\n\nif TYPE_CHECKING:\n    # typing.Concatenate requires Python 3.10\n    from typing_extensions import Concatenate\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DummyPolicy:\n    def __init__(self, settings: BaseSettings):\n        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n        self.ignore_http_codes: List[int] = [\n            int(x) for x in settings.getlist(\"HTTPCACHE_IGNORE_HTTP_CODES\")\n        ]\n\n    def should_cache_request(self, request: Request) -> bool:\n        return urlparse_cached(request).scheme not in self.ignore_schemes\n\n    def should_cache_response(self, response: Response, request: Request) -> bool:\n        return response.status not in self.ignore_http_codes\n\n    def is_cached_response_fresh(\n        self, cachedresponse: Response, request: Request\n    ) -> bool:\n        return True\n\n    def is_cached_response_valid(\n        self, cachedresponse: Response, response: Response, request: Request\n    ) -> bool:\n        return True\n\n\nclass RFC2616Policy:\n    MAXAGE = 3600 * 24 * 365  # one year\n\n    def __init__(self, settings: BaseSettings):\n        self.always_store: bool = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n        self._cc_parsed: WeakKeyDictionary[\n            Union[Request, Response], Dict[bytes, Optional[bytes]]\n        ] = WeakKeyDictionary()\n        self.ignore_response_cache_controls: List[bytes] = [\n            to_bytes(cc)\n            for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n        ]\n\n    def _parse_cachecontrol(\n        self, r: Union[Request, Response]\n    ) -> Dict[bytes, Optional[bytes]]:\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b\"Cache-Control\", b\"\")\n            assert cch is not None\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]\n\n    def should_cache_request(self, request: Request) -> bool:\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b\"no-store\" in cc:\n            return False\n        # Any other is eligible for caching\n        return True\n\n    def should_cache_response(self, response: Response, request: Request) -> bool:\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b\"no-store\" in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        if response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        if self.always_store:\n            return True\n        # Any hint on response expiration is good\n        if b\"max-age\" in cc or b\"Expires\" in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        if response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        if response.status in (200, 203, 401):\n            return b\"Last-Modified\" in response.headers or b\"ETag\" in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        return False\n\n    def is_cached_response_fresh(\n        self, cachedresponse: Response, request: Request\n    ) -> bool:\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b\"no-cache\" in cc or b\"no-cache\" in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(\n            cachedresponse, request, now\n        )\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b\"max-stale\" in ccreq and b\"must-revalidate\" not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b\"max-stale\"]\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False\n\n    def is_cached_response_valid(\n        self, cachedresponse: Response, response: Response, request: Request\n    ) -> bool:\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b\"must-revalidate\" not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304\n\n    def _set_conditional_validators(\n        self, request: Request, cachedresponse: Response\n    ) -> None:\n        if b\"Last-Modified\" in cachedresponse.headers:\n            request.headers[b\"If-Modified-Since\"] = cachedresponse.headers[\n                b\"Last-Modified\"\n            ]\n\n        if b\"ETag\" in cachedresponse.headers:\n            request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n\n    def _get_max_age(self, cc: Dict[bytes, Optional[bytes]]) -> Optional[int]:\n        try:\n            return max(0, int(cc[b\"max-age\"]))  # type: ignore[arg-type]\n        except (KeyError, ValueError):\n            return None\n\n    def _compute_freshness_lifetime(\n        self, response: Response, request: Request, now: float\n    ) -> float:\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n\n        # Try HTTP/1.0 Expires header\n        if b\"Expires\" in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b\"Expires\"])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b\"Last-Modified\"))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute freshness lifetime\n        return 0\n\n    def _compute_current_age(\n        self, response: Response, request: Request, now: float\n    ) -> float:\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage: float = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n        if now > date:\n            currentage = now - date\n\n        if b\"Age\" in response.headers:\n            try:\n                age = int(response.headers[b\"Age\"])  # type: ignore[arg-type]\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage\n\n\nclass DbmCacheStorage:\n    def __init__(self, settings: BaseSettings):\n        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"], createdir=True)\n        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n        self.dbmodule: ModuleType = import_module(settings[\"HTTPCACHE_DBM_MODULE\"])\n        self.db: Any = None  # the real type is private\n\n    def open_spider(self, spider: Spider) -> None:\n        dbpath = Path(self.cachedir, f\"{spider.name}.db\")\n        self.db = self.dbmodule.open(str(dbpath), \"c\")\n\n        logger.debug(\n            \"Using DBM cache storage in %(cachepath)s\",\n            {\"cachepath\": dbpath},\n            extra={\"spider\": spider},\n        )\n\n        assert spider.crawler.request_fingerprinter\n        self._fingerprinter: RequestFingerprinter = spider.crawler.request_fingerprinter\n\n    def close_spider(self, spider: Spider) -> None:\n        self.db.close()\n\n    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n        data = self._read_data(spider, request)\n        if data is None:\n            return None  # not cached\n        url = data[\"url\"]\n        status = data[\"status\"]\n        headers = Headers(data[\"headers\"])\n        body = data[\"body\"]\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(\n        self, spider: Spider, request: Request, response: Response\n    ) -> None:\n        key = self._fingerprinter.fingerprint(request).hex()\n        data = {\n            \"status\": response.status,\n            \"url\": response.url,\n            \"headers\": dict(response.headers),\n            \"body\": response.body,\n        }\n        self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n        self.db[f\"{key}_time\"] = str(time())\n\n    def _read_data(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n        key = self._fingerprinter.fingerprint(request).hex()\n        db = self.db\n        tkey = f\"{key}_time\"\n        if tkey not in db:\n            return None  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return None  # expired\n\n        return cast(Dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # nosec\n\n\nclass FilesystemCacheStorage:\n    def __init__(self, settings: BaseSettings):\n        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"])\n        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n        self.use_gzip: bool = settings.getbool(\"HTTPCACHE_GZIP\")\n        # https://github.com/python/mypy/issues/10740\n        self._open: Callable[\n            Concatenate[Union[str, os.PathLike], str, ...], IO[bytes]\n        ] = (\n            gzip.open if self.use_gzip else open  # type: ignore[assignment]\n        )\n\n    def open_spider(self, spider: Spider) -> None:\n        logger.debug(\n            \"Using filesystem cache storage in %(cachedir)s\",\n            {\"cachedir\": self.cachedir},\n            extra={\"spider\": spider},\n        )\n\n        assert spider.crawler.request_fingerprinter\n        self._fingerprinter = spider.crawler.request_fingerprinter\n\n    def close_spider(self, spider: Spider) -> None:\n        pass\n\n    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return None  # not cached\n        rpath = Path(self._get_request_path(spider, request))\n        with self._open(rpath / \"response_body\", \"rb\") as f:\n            body = f.read()\n        with self._open(rpath / \"response_headers\", \"rb\") as f:\n            rawheaders = f.read()\n        url = metadata[\"response_url\"]\n        status = metadata[\"status\"]\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(\n        self, spider: Spider, request: Request, response: Response\n    ) -> None:\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = Path(self._get_request_path(spider, request))\n        if not rpath.exists():\n            rpath.mkdir(parents=True)\n        metadata = {\n            \"url\": request.url,\n            \"method\": request.method,\n            \"status\": response.status,\n            \"response_url\": response.url,\n            \"timestamp\": time(),\n        }\n        with self._open(rpath / \"meta\", \"wb\") as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(rpath / \"pickled_meta\", \"wb\") as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(rpath / \"response_headers\", \"wb\") as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(rpath / \"response_body\", \"wb\") as f:\n            f.write(response.body)\n        with self._open(rpath / \"request_headers\", \"wb\") as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(rpath / \"request_body\", \"wb\") as f:\n            f.write(request.body)\n\n    def _get_request_path(self, spider: Spider, request: Request) -> str:\n        key = self._fingerprinter.fingerprint(request).hex()\n        return str(Path(self.cachedir, spider.name, key[0:2], key))\n\n    def _read_meta(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n        rpath = Path(self._get_request_path(spider, request))\n        metapath = rpath / \"pickled_meta\"\n        if not metapath.exists():\n            return None  # not found\n        mtime = metapath.stat().st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return None  # expired\n        with self._open(metapath, \"rb\") as f:\n            return cast(Dict[str, Any], pickle.load(f))  # nosec\n\n\ndef parse_cachecontrol(header: bytes) -> Dict[bytes, Optional[bytes]]:\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b\",\"):\n        key, sep, val = directive.strip().partition(b\"=\")\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives\n\n\ndef rfc1123_to_epoch(date_str: Union[str, bytes, None]) -> Optional[int]:\n    try:\n        date_str = to_unicode(date_str, encoding=\"ascii\")  # type: ignore[arg-type]\n        return mktime_tz(parsedate_tz(date_str))  # type: ignore[arg-type]\n    except Exception:\n        return None\n", "scrapy/extensions/closespider.py": "\"\"\"CloseSpider is an extension that forces spiders to be closed after certain\nconditions are met.\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Dict\n\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass CloseSpider:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n\n        self.close_on: Dict[str, Any] = {\n            \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n            \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n            \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n            \"errorcount\": crawler.settings.getint(\"CLOSESPIDER_ERRORCOUNT\"),\n            \"timeout_no_item\": crawler.settings.getint(\"CLOSESPIDER_TIMEOUT_NO_ITEM\"),\n        }\n\n        if not any(self.close_on.values()):\n            raise NotConfigured\n\n        self.counter: DefaultDict[str, int] = defaultdict(int)\n\n        if self.close_on.get(\"errorcount\"):\n            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n        if self.close_on.get(\"pagecount\"):\n            crawler.signals.connect(self.page_count, signal=signals.response_received)\n        if self.close_on.get(\"timeout\"):\n            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        if self.close_on.get(\"itemcount\"):\n            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n        if self.close_on.get(\"timeout_no_item\"):\n            self.timeout_no_item: int = self.close_on[\"timeout_no_item\"]\n            self.items_in_period: int = 0\n            crawler.signals.connect(\n                self.spider_opened_no_item, signal=signals.spider_opened\n            )\n            crawler.signals.connect(\n                self.item_scraped_no_item, signal=signals.item_scraped\n            )\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def error_count(self, failure: Failure, response: Response, spider: Spider) -> None:\n        self.counter[\"errorcount\"] += 1\n        if self.counter[\"errorcount\"] == self.close_on[\"errorcount\"]:\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_errorcount\")\n\n    def page_count(self, response: Response, request: Request, spider: Spider) -> None:\n        self.counter[\"pagecount\"] += 1\n        if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_pagecount\")\n\n    def spider_opened(self, spider: Spider) -> None:\n        from twisted.internet import reactor\n\n        assert self.crawler.engine\n        self.task = reactor.callLater(\n            self.close_on[\"timeout\"],\n            self.crawler.engine.close_spider,\n            spider,\n            reason=\"closespider_timeout\",\n        )\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        self.counter[\"itemcount\"] += 1\n        if self.counter[\"itemcount\"] == self.close_on[\"itemcount\"]:\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_itemcount\")\n\n    def spider_closed(self, spider: Spider) -> None:\n        task = getattr(self, \"task\", None)\n        if task and task.active():\n            task.cancel()\n\n        task_no_item = getattr(self, \"task_no_item\", None)\n        if task_no_item and task_no_item.running:\n            task_no_item.stop()\n\n    def spider_opened_no_item(self, spider: Spider) -> None:\n        from twisted.internet import task\n\n        self.task_no_item = task.LoopingCall(self._count_items_produced, spider)\n        self.task_no_item.start(self.timeout_no_item, now=False)\n\n        logger.info(\n            f\"Spider will stop when no items are produced after \"\n            f\"{self.timeout_no_item} seconds.\"\n        )\n\n    def item_scraped_no_item(self, item: Any, spider: Spider) -> None:\n        self.items_in_period += 1\n\n    def _count_items_produced(self, spider: Spider) -> None:\n        if self.items_in_period >= 1:\n            self.items_in_period = 0\n        else:\n            logger.info(\n                f\"Closing spider since no items were produced in the last \"\n                f\"{self.timeout_no_item} seconds.\"\n            )\n            assert self.crawler.engine\n            self.crawler.engine.close_spider(spider, \"closespider_timeout_no_item\")\n", "scrapy/extensions/statsmailer.py": "\"\"\"\nStatsMailer extension sends an email when a spider finishes scraping.\n\nUse STATSMAILER_RCPTS setting to enable and give the recipient mail address\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List, Optional\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.mail import MailSender\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass StatsMailer:\n    def __init__(self, stats: StatsCollector, recipients: List[str], mail: MailSender):\n        self.stats: StatsCollector = stats\n        self.recipients: List[str] = recipients\n        self.mail: MailSender = mail\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        recipients: List[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n        if not recipients:\n            raise NotConfigured\n        mail: MailSender = MailSender.from_settings(crawler.settings)\n        assert crawler.stats\n        o = cls(crawler.stats, recipients, mail)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_closed(self, spider: Spider) -> Optional[Deferred]:\n        spider_stats = self.stats.get_stats(spider)\n        body = \"Global stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n        body += f\"\\n\\n{spider.name} stats\\n\\n\"\n        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in spider_stats.items())\n        return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)\n", "scrapy/extensions/feedexport.py": "\"\"\"\nFeed Exports extension\n\nSee documentation in docs/topics/feed-exports.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport re\nimport sys\nimport warnings\nfrom datetime import datetime, timezone\nfrom pathlib import Path, PureWindowsPath\nfrom tempfile import NamedTemporaryFile\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom urllib.parse import unquote, urlparse\n\nfrom twisted.internet import threads\nfrom twisted.internet.defer import Deferred, DeferredList, maybeDeferred\nfrom twisted.python.failure import Failure\nfrom w3lib.url import file_uri_to_path\nfrom zope.interface import Interface, implementer\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.exporters import BaseItemExporter\nfrom scrapy.extensions.postprocessing import PostProcessingManager\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.conf import feed_complete_default_values_from_settings\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom scrapy.utils.deprecate import create_deprecated_class\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import build_from_crawler, load_object\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from _typeshed import OpenBinaryMode\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import boto3  # noqa: F401\n\n    IS_BOTO3_AVAILABLE = True\nexcept ImportError:\n    IS_BOTO3_AVAILABLE = False\n\nUriParamsCallableT = Callable[[Dict[str, Any], Spider], Optional[Dict[str, Any]]]\n\n_StorageT = TypeVar(\"_StorageT\", bound=\"FeedStorageProtocol\")\n\n\ndef build_storage(\n    builder: Callable[..., _StorageT],\n    uri: str,\n    *args: Any,\n    feed_options: Optional[Dict[str, Any]] = None,\n    preargs: Iterable[Any] = (),\n    **kwargs: Any,\n) -> _StorageT:\n    kwargs[\"feed_options\"] = feed_options\n    return builder(*preargs, uri, *args, **kwargs)\n\n\nclass ItemFilter:\n    \"\"\"\n    This will be used by FeedExporter to decide if an item should be allowed\n    to be exported to a particular feed.\n\n    :param feed_options: feed specific options passed from FeedExporter\n    :type feed_options: dict\n    \"\"\"\n\n    feed_options: Optional[Dict[str, Any]]\n    item_classes: Tuple[type, ...]\n\n    def __init__(self, feed_options: Optional[Dict[str, Any]]) -> None:\n        self.feed_options = feed_options\n        if feed_options is not None:\n            self.item_classes = tuple(\n                load_object(item_class)\n                for item_class in feed_options.get(\"item_classes\") or ()\n            )\n        else:\n            self.item_classes = ()\n\n    def accepts(self, item: Any) -> bool:\n        \"\"\"\n        Return ``True`` if `item` should be exported or ``False`` otherwise.\n\n        :param item: scraped item which user wants to check if is acceptable\n        :type item: :ref:`Scrapy items <topics-items>`\n        :return: `True` if accepted, `False` otherwise\n        :rtype: bool\n        \"\"\"\n        if self.item_classes:\n            return isinstance(item, self.item_classes)\n        return True  # accept all items by default\n\n\nclass IFeedStorage(Interface):\n    \"\"\"Interface that all Feed Storages must implement\"\"\"\n\n    def __init__(uri, *, feed_options=None):\n        \"\"\"Initialize the storage with the parameters given in the URI and the\n        feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n    def open(spider):\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(file):\n        \"\"\"Store the given file stream\"\"\"\n\n\nclass FeedStorageProtocol(Protocol):\n    \"\"\"Reimplementation of ``IFeedStorage`` that can be used in type hints.\"\"\"\n\n    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n        \"\"\"Initialize the storage with the parameters given in the URI and the\n        feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        \"\"\"Store the given file stream\"\"\"\n\n\n@implementer(IFeedStorage)\nclass BlockingFeedStorage:\n    def open(self, spider: Spider) -> IO[bytes]:\n        path = spider.crawler.settings[\"FEED_TEMPDIR\"]\n        if path and not Path(path).is_dir():\n            raise OSError(\"Not a Directory: \" + str(path))\n\n        return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        return threads.deferToThread(self._store_in_thread, file)\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        raise NotImplementedError\n\n\n@implementer(IFeedStorage)\nclass StdoutFeedStorage:\n    def __init__(\n        self,\n        uri: str,\n        _stdout: Optional[IO[bytes]] = None,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ):\n        if not _stdout:\n            _stdout = sys.stdout.buffer\n        self._stdout: IO[bytes] = _stdout\n        if feed_options and feed_options.get(\"overwrite\", False) is True:\n            logger.warning(\n                \"Standard output (stdout) storage does not support \"\n                \"overwriting. To suppress this warning, remove the \"\n                \"overwrite option from your FEEDS setting, or set \"\n                \"it to False.\"\n            )\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        return self._stdout\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        pass\n\n\n@implementer(IFeedStorage)\nclass FileFeedStorage:\n    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n        self.path: str = file_uri_to_path(uri)\n        feed_options = feed_options or {}\n        self.write_mode: OpenBinaryMode = (\n            \"wb\" if feed_options.get(\"overwrite\", False) else \"ab\"\n        )\n\n    def open(self, spider: Spider) -> IO[bytes]:\n        dirname = Path(self.path).parent\n        if dirname and not dirname.exists():\n            dirname.mkdir(parents=True)\n        return Path(self.path).open(self.write_mode)\n\n    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n        file.close()\n        return None\n\n\nclass S3FeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        access_key: Optional[str] = None,\n        secret_key: Optional[str] = None,\n        acl: Optional[str] = None,\n        endpoint_url: Optional[str] = None,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n        session_token: Optional[str] = None,\n        region_name: Optional[str] = None,\n    ):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n        u = urlparse(uri)\n        assert u.hostname\n        self.bucketname: str = u.hostname\n        self.access_key: Optional[str] = u.username or access_key\n        self.secret_key: Optional[str] = u.password or secret_key\n        self.session_token: Optional[str] = session_token\n        self.keyname: str = u.path[1:]  # remove first \"/\"\n        self.acl: Optional[str] = acl\n        self.endpoint_url: Optional[str] = endpoint_url\n        self.region_name: Optional[str] = region_name\n        # It can be either botocore.client.BaseClient or mypy_boto3_s3.S3Client,\n        # there seems to be no good way to infer it statically.\n        self.s3_client: Any\n\n        if IS_BOTO3_AVAILABLE:\n            import boto3.session\n\n            boto3_session = boto3.session.Session()\n\n            self.s3_client = boto3_session.client(\n                \"s3\",\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                aws_session_token=self.session_token,\n                endpoint_url=self.endpoint_url,\n                region_name=self.region_name,\n            )\n        else:\n            warnings.warn(\n                \"`botocore` usage has been deprecated for S3 feed \"\n                \"export, please use `boto3` to avoid problems\",\n                category=ScrapyDeprecationWarning,\n            )\n\n            import botocore.session\n\n            botocore_session = botocore.session.get_session()\n\n            self.s3_client = botocore_session.create_client(\n                \"s3\",\n                aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key,\n                aws_session_token=self.session_token,\n                endpoint_url=self.endpoint_url,\n                region_name=self.region_name,\n            )\n\n        if feed_options and feed_options.get(\"overwrite\", True) is False:\n            logger.warning(\n                \"S3 does not support appending to files. To \"\n                \"suppress this warning, remove the overwrite \"\n                \"option from your FEEDS setting or set it to True.\"\n            )\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ) -> Self:\n        return build_storage(\n            cls,\n            uri,\n            access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n            secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n            session_token=crawler.settings[\"AWS_SESSION_TOKEN\"],\n            acl=crawler.settings[\"FEED_STORAGE_S3_ACL\"] or None,\n            endpoint_url=crawler.settings[\"AWS_ENDPOINT_URL\"] or None,\n            region_name=crawler.settings[\"AWS_REGION_NAME\"] or None,\n            feed_options=feed_options,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        file.seek(0)\n        kwargs: Dict[str, Any]\n        if IS_BOTO3_AVAILABLE:\n            kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n            self.s3_client.upload_fileobj(\n                Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n            )\n        else:\n            kwargs = {\"ACL\": self.acl} if self.acl else {}\n            self.s3_client.put_object(\n                Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n            )\n        file.close()\n\n\nclass GCSFeedStorage(BlockingFeedStorage):\n    def __init__(self, uri: str, project_id: Optional[str], acl: Optional[str]):\n        self.project_id: Optional[str] = project_id\n        self.acl: Optional[str] = acl\n        u = urlparse(uri)\n        assert u.hostname\n        self.bucket_name: str = u.hostname\n        self.blob_name: str = u.path[1:]  # remove first \"/\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, uri: str) -> Self:\n        return cls(\n            uri,\n            crawler.settings[\"GCS_PROJECT_ID\"],\n            crawler.settings[\"FEED_STORAGE_GCS_ACL\"] or None,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        file.seek(0)\n        from google.cloud.storage import Client\n\n        client = Client(project=self.project_id)\n        bucket = client.get_bucket(self.bucket_name)\n        blob = bucket.blob(self.blob_name)\n        blob.upload_from_file(file, predefined_acl=self.acl)\n\n\nclass FTPFeedStorage(BlockingFeedStorage):\n    def __init__(\n        self,\n        uri: str,\n        use_active_mode: bool = False,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ):\n        u = urlparse(uri)\n        if not u.hostname:\n            raise ValueError(f\"Got a storage URI without a hostname: {uri}\")\n        self.host: str = u.hostname\n        self.port: int = int(u.port or \"21\")\n        self.username: str = u.username or \"\"\n        self.password: str = unquote(u.password or \"\")\n        self.path: str = u.path\n        self.use_active_mode: bool = use_active_mode\n        self.overwrite: bool = not feed_options or feed_options.get(\"overwrite\", True)\n\n    @classmethod\n    def from_crawler(\n        cls,\n        crawler: Crawler,\n        uri: str,\n        *,\n        feed_options: Optional[Dict[str, Any]] = None,\n    ) -> Self:\n        return build_storage(\n            cls,\n            uri,\n            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n            feed_options=feed_options,\n        )\n\n    def _store_in_thread(self, file: IO[bytes]) -> None:\n        ftp_store_file(\n            path=self.path,\n            file=file,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.use_active_mode,\n            overwrite=self.overwrite,\n        )\n\n\nclass FeedSlot:\n    def __init__(\n        self,\n        storage: FeedStorageProtocol,\n        uri: str,\n        format: str,\n        store_empty: bool,\n        batch_id: int,\n        uri_template: str,\n        filter: ItemFilter,\n        feed_options: Dict[str, Any],\n        spider: Spider,\n        exporters: Dict[str, Type[BaseItemExporter]],\n        settings: BaseSettings,\n        crawler: Crawler,\n    ):\n        self.file: Optional[IO[bytes]] = None\n        self.exporter: Optional[BaseItemExporter] = None\n        self.storage: FeedStorageProtocol = storage\n        # feed params\n        self.batch_id: int = batch_id\n        self.format: str = format\n        self.store_empty: bool = store_empty\n        self.uri_template: str = uri_template\n        self.uri: str = uri\n        self.filter: ItemFilter = filter\n        # exporter params\n        self.feed_options: Dict[str, Any] = feed_options\n        self.spider: Spider = spider\n        self.exporters: Dict[str, Type[BaseItemExporter]] = exporters\n        self.settings: BaseSettings = settings\n        self.crawler: Crawler = crawler\n        # flags\n        self.itemcount: int = 0\n        self._exporting: bool = False\n        self._fileloaded: bool = False\n\n    def start_exporting(self) -> None:\n        if not self._fileloaded:\n            self.file = self.storage.open(self.spider)\n            if \"postprocessing\" in self.feed_options:\n                self.file = cast(\n                    IO[bytes],\n                    PostProcessingManager(\n                        self.feed_options[\"postprocessing\"],\n                        self.file,\n                        self.feed_options,\n                    ),\n                )\n            self.exporter = self._get_exporter(\n                file=self.file,\n                format=self.feed_options[\"format\"],\n                fields_to_export=self.feed_options[\"fields\"],\n                encoding=self.feed_options[\"encoding\"],\n                indent=self.feed_options[\"indent\"],\n                **self.feed_options[\"item_export_kwargs\"],\n            )\n            self._fileloaded = True\n\n        if not self._exporting:\n            assert self.exporter\n            self.exporter.start_exporting()\n            self._exporting = True\n\n    def _get_instance(\n        self, objcls: Type[BaseItemExporter], *args: Any, **kwargs: Any\n    ) -> BaseItemExporter:\n        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n\n    def _get_exporter(\n        self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n    ) -> BaseItemExporter:\n        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n\n    def finish_exporting(self) -> None:\n        if self._exporting:\n            assert self.exporter\n            self.exporter.finish_exporting()\n            self._exporting = False\n\n\n_FeedSlot = create_deprecated_class(\n    name=\"_FeedSlot\",\n    new_class=FeedSlot,\n)\n\n\nclass FeedExporter:\n    _pending_deferreds: List[Deferred] = []\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        exporter = cls(crawler)\n        crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n        crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n        return exporter\n\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        self.settings: Settings = crawler.settings\n        self.feeds = {}\n        self.slots: List[FeedSlot] = []\n        self.filters: Dict[str, ItemFilter] = {}\n\n        if not self.settings[\"FEEDS\"] and not self.settings[\"FEED_URI\"]:\n            raise NotConfigured\n\n        # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings\n        if self.settings[\"FEED_URI\"]:\n            warnings.warn(\n                \"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of \"\n                \"the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            uri = self.settings[\"FEED_URI\"]\n            # handle pathlib.Path objects\n            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()\n            feed_options = {\"format\": self.settings.get(\"FEED_FORMAT\", \"jsonlines\")}\n            self.feeds[uri] = feed_complete_default_values_from_settings(\n                feed_options, self.settings\n            )\n            self.filters[uri] = self._load_filter(feed_options)\n        # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n\n        # 'FEEDS' setting takes precedence over 'FEED_URI'\n        for uri, feed_options in self.settings.getdict(\"FEEDS\").items():\n            # handle pathlib.Path objects\n            uri = str(uri) if not isinstance(uri, Path) else uri.absolute().as_uri()\n            self.feeds[uri] = feed_complete_default_values_from_settings(\n                feed_options, self.settings\n            )\n            self.filters[uri] = self._load_filter(feed_options)\n\n        self.storages: Dict[str, Type[FeedStorageProtocol]] = self._load_components(\n            \"FEED_STORAGES\"\n        )\n        self.exporters: Dict[str, Type[BaseItemExporter]] = self._load_components(\n            \"FEED_EXPORTERS\"\n        )\n        for uri, feed_options in self.feeds.items():\n            if not self._storage_supported(uri, feed_options):\n                raise NotConfigured\n            if not self._settings_are_valid():\n                raise NotConfigured\n            if not self._exporter_supported(feed_options[\"format\"]):\n                raise NotConfigured\n\n    def open_spider(self, spider: Spider) -> None:\n        for uri, feed_options in self.feeds.items():\n            uri_params = self._get_uri_params(spider, feed_options[\"uri_params\"])\n            self.slots.append(\n                self._start_new_batch(\n                    batch_id=1,\n                    uri=uri % uri_params,\n                    feed_options=feed_options,\n                    spider=spider,\n                    uri_template=uri,\n                )\n            )\n\n    async def close_spider(self, spider: Spider) -> None:\n        for slot in self.slots:\n            self._close_slot(slot, spider)\n\n        # Await all deferreds\n        if self._pending_deferreds:\n            await maybe_deferred_to_future(DeferredList(self._pending_deferreds))\n\n        # Send FEED_EXPORTER_CLOSED signal\n        await maybe_deferred_to_future(\n            self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n        )\n\n    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Optional[Deferred]:\n        def get_file(slot_: FeedSlot) -> IO[bytes]:\n            assert slot_.file\n            if isinstance(slot_.file, PostProcessingManager):\n                slot_.file.close()\n                return slot_.file.file\n            return slot_.file\n\n        if slot.itemcount:\n            # Normal case\n            slot.finish_exporting()\n        elif slot.store_empty and slot.batch_id == 1:\n            # Need to store the empty file\n            slot.start_exporting()\n            slot.finish_exporting()\n        else:\n            # In this case, the file is not stored, so no processing is required.\n            return None\n\n        logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n        d: Deferred = maybeDeferred(slot.storage.store, get_file(slot))\n\n        d.addCallback(\n            self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n        )\n        d.addErrback(\n            self._handle_store_error, logmsg, spider, type(slot.storage).__name__\n        )\n        self._pending_deferreds.append(d)\n        d.addCallback(\n            lambda _: self.crawler.signals.send_catch_log_deferred(\n                signals.feed_slot_closed, slot=slot\n            )\n        )\n        d.addBoth(lambda _: self._pending_deferreds.remove(d))\n\n        return d\n\n    def _handle_store_error(\n        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n    ) -> None:\n        logger.error(\n            \"Error storing %s\",\n            logmsg,\n            exc_info=failure_to_exc_info(f),\n            extra={\"spider\": spider},\n        )\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n\n    def _handle_store_success(\n        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n    ) -> None:\n        logger.info(\"Stored %s\", logmsg, extra={\"spider\": spider})\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n\n    def _start_new_batch(\n        self,\n        batch_id: int,\n        uri: str,\n        feed_options: Dict[str, Any],\n        spider: Spider,\n        uri_template: str,\n    ) -> FeedSlot:\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n        :param batch_id: sequence number of current batch\n        :param uri: uri of the new batch to start\n        :param feed_options: dict with parameters of feed\n        :param spider: user spider\n        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n        \"\"\"\n        storage = self._get_storage(uri, feed_options)\n        slot = FeedSlot(\n            storage=storage,\n            uri=uri,\n            format=feed_options[\"format\"],\n            store_empty=feed_options[\"store_empty\"],\n            batch_id=batch_id,\n            uri_template=uri_template,\n            filter=self.filters[uri_template],\n            feed_options=feed_options,\n            spider=spider,\n            exporters=self.exporters,\n            settings=self.settings,\n            crawler=self.crawler,\n        )\n        return slot\n\n    def item_scraped(self, item: Any, spider: Spider) -> None:\n        slots = []\n        for slot in self.slots:\n            if not slot.filter.accepts(item):\n                slots.append(\n                    slot\n                )  # if slot doesn't accept item, continue with next slot\n                continue\n\n            slot.start_exporting()\n            assert slot.exporter\n            slot.exporter.export_item(item)\n            slot.itemcount += 1\n            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n            if (\n                self.feeds[slot.uri_template][\"batch_item_count\"]\n                and slot.itemcount >= self.feeds[slot.uri_template][\"batch_item_count\"]\n            ):\n                uri_params = self._get_uri_params(\n                    spider, self.feeds[slot.uri_template][\"uri_params\"], slot\n                )\n                self._close_slot(slot, spider)\n                slots.append(\n                    self._start_new_batch(\n                        batch_id=slot.batch_id + 1,\n                        uri=slot.uri_template % uri_params,\n                        feed_options=self.feeds[slot.uri_template],\n                        spider=spider,\n                        uri_template=slot.uri_template,\n                    )\n                )\n            else:\n                slots.append(slot)\n        self.slots = slots\n\n    def _load_components(self, setting_prefix: str) -> Dict[str, Any]:\n        conf = without_none_values(\n            cast(Dict[str, str], self.settings.getwithbase(setting_prefix))\n        )\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d\n\n    def _exporter_supported(self, format: str) -> bool:\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {\"format\": format})\n        return False\n\n    def _settings_are_valid(self) -> bool:\n        \"\"\"\n        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n        \"\"\"\n        for uri_template, values in self.feeds.items():\n            if values[\"batch_item_count\"] and not re.search(\n                r\"%\\(batch_time\\)s|%\\(batch_id\\)\", uri_template\n            ):\n                logger.error(\n                    \"%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT \"\n                    \"setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: \"\n                    \"https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count\",\n                    uri_template,\n                )\n                return False\n        return True\n\n    def _storage_supported(self, uri: str, feed_options: Dict[str, Any]) -> bool:\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages or PureWindowsPath(uri).drive:\n            try:\n                self._get_storage(uri, feed_options)\n                return True\n            except NotConfigured as e:\n                logger.error(\n                    \"Disabled feed storage scheme: %(scheme)s. \" \"Reason: %(reason)s\",\n                    {\"scheme\": scheme, \"reason\": str(e)},\n                )\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n        return False\n\n    def _get_storage(\n        self, uri: str, feed_options: Dict[str, Any]\n    ) -> FeedStorageProtocol:\n        \"\"\"Fork of create_instance specific to feed storage classes\n\n        It supports not passing the *feed_options* parameters to classes that\n        do not support it, and issuing a deprecation warning instead.\n        \"\"\"\n        feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n        crawler = getattr(self, \"crawler\", None)\n\n        def build_instance(\n            builder: Type[FeedStorageProtocol], *preargs: Any\n        ) -> FeedStorageProtocol:\n            return build_storage(\n                builder, uri, feed_options=feed_options, preargs=preargs\n            )\n\n        instance: FeedStorageProtocol\n        if crawler and hasattr(feedcls, \"from_crawler\"):\n            instance = build_instance(feedcls.from_crawler, crawler)\n            method_name = \"from_crawler\"\n        elif hasattr(feedcls, \"from_settings\"):\n            instance = build_instance(feedcls.from_settings, self.settings)\n            method_name = \"from_settings\"\n        else:\n            instance = build_instance(feedcls)\n            method_name = \"__new__\"\n        if instance is None:\n            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n        return instance\n\n    def _get_uri_params(\n        self,\n        spider: Spider,\n        uri_params_function: Union[str, UriParamsCallableT, None],\n        slot: Optional[FeedSlot] = None,\n    ) -> Dict[str, Any]:\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        utc_now = datetime.now(tz=timezone.utc)\n        params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n        params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n        params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n        uripar_function: UriParamsCallableT = (\n            load_object(uri_params_function)\n            if uri_params_function\n            else lambda params, _: params\n        )\n        new_params = uripar_function(params, spider)\n        return new_params if new_params is not None else params\n\n    def _load_filter(self, feed_options: Dict[str, Any]) -> ItemFilter:\n        # load the item filter if declared else load the default filter class\n        item_filter_class: Type[ItemFilter] = load_object(\n            feed_options.get(\"item_filter\", ItemFilter)\n        )\n        return item_filter_class(feed_options)\n", "scrapy/extensions/spiderstate.py": "from __future__ import annotations\n\nimport pickle  # nosec\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Optional\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.job import job_dir\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass SpiderState:\n    \"\"\"Store and load spider state during a scraping job\"\"\"\n\n    def __init__(self, jobdir: Optional[str] = None):\n        self.jobdir: Optional[str] = jobdir\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj\n\n    def spider_closed(self, spider: Spider) -> None:\n        if self.jobdir:\n            with Path(self.statefn).open(\"wb\") as f:\n                assert hasattr(spider, \"state\")  # set in spider_opened\n                pickle.dump(spider.state, f, protocol=4)\n\n    def spider_opened(self, spider: Spider) -> None:\n        if self.jobdir and Path(self.statefn).exists():\n            with Path(self.statefn).open(\"rb\") as f:\n                spider.state = pickle.load(f)  # type: ignore[attr-defined]  # nosec\n        else:\n            spider.state = {}  # type: ignore[attr-defined]\n\n    @property\n    def statefn(self) -> str:\n        assert self.jobdir\n        return str(Path(self.jobdir, \"spider.state\"))\n", "scrapy/extensions/throttle.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.core.downloader import Slot\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoThrottle:\n    def __init__(self, crawler: Crawler):\n        self.crawler: Crawler = crawler\n        if not crawler.settings.getbool(\"AUTOTHROTTLE_ENABLED\"):\n            raise NotConfigured\n\n        self.debug: bool = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n        self.target_concurrency: float = crawler.settings.getfloat(\n            \"AUTOTHROTTLE_TARGET_CONCURRENCY\"\n        )\n        if self.target_concurrency <= 0.0:\n            raise NotConfigured(\n                f\"AUTOTHROTTLE_TARGET_CONCURRENCY \"\n                f\"({self.target_concurrency!r}) must be higher than 0.\"\n            )\n        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(\n            self._response_downloaded, signal=signals.response_downloaded\n        )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def _spider_opened(self, spider: Spider) -> None:\n        self.mindelay = self._min_delay(spider)\n        self.maxdelay = self._max_delay(spider)\n        spider.download_delay = self._start_delay(spider)  # type: ignore[attr-defined]\n\n    def _min_delay(self, spider: Spider) -> float:\n        s = self.crawler.settings\n        return getattr(spider, \"download_delay\", s.getfloat(\"DOWNLOAD_DELAY\"))\n\n    def _max_delay(self, spider: Spider) -> float:\n        return self.crawler.settings.getfloat(\"AUTOTHROTTLE_MAX_DELAY\")\n\n    def _start_delay(self, spider: Spider) -> float:\n        return max(\n            self.mindelay, self.crawler.settings.getfloat(\"AUTOTHROTTLE_START_DELAY\")\n        )\n\n    def _response_downloaded(\n        self, response: Response, request: Request, spider: Spider\n    ) -> None:\n        key, slot = self._get_slot(request, spider)\n        latency = request.meta.get(\"download_latency\")\n        if latency is None or slot is None or slot.throttle is False:\n            return\n\n        olddelay = slot.delay\n        self._adjust_delay(slot, latency, response)\n        if self.debug:\n            diff = slot.delay - olddelay\n            size = len(response.body)\n            conc = len(slot.transferring)\n            logger.info(\n                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                {\n                    \"slot\": key,\n                    \"concurrency\": conc,\n                    \"delay\": slot.delay * 1000,\n                    \"delaydiff\": diff * 1000,\n                    \"latency\": latency * 1000,\n                    \"size\": size,\n                },\n                extra={\"spider\": spider},\n            )\n\n    def _get_slot(\n        self, request: Request, spider: Spider\n    ) -> Tuple[Optional[str], Optional[Slot]]:\n        key: Optional[str] = request.meta.get(\"download_slot\")\n        if key is None:\n            return None, None\n        assert self.crawler.engine\n        return key, self.crawler.engine.downloader.slots.get(key)\n\n    def _adjust_delay(self, slot: Slot, latency: float, response: Response) -> None:\n        \"\"\"Define delay adjustment policy\"\"\"\n\n        # If a server needs `latency` seconds to respond then\n        # we should send a request each `latency/N` seconds\n        # to have N requests processed in parallel\n        target_delay = latency / self.target_concurrency\n\n        # Adjust the delay to make it closer to target_delay\n        new_delay = (slot.delay + target_delay) / 2.0\n\n        # If target delay is bigger than old delay, then use it instead of mean.\n        # It works better with problematic sites.\n        new_delay = max(target_delay, new_delay)\n\n        # Make sure self.mindelay <= new_delay <= self.max_delay\n        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n        # Dont adjust delay if response status != 200 and new delay is smaller\n        # than old one, as error pages (and redirections) are usually small and\n        # so tend to reduce latency, thus provoking a positive feedback by\n        # reducing delay instead of increase.\n        if response.status != 200 and new_delay <= slot.delay:\n            return\n\n        slot.delay = new_delay\n", "scrapy/extensions/postprocessing.py": "\"\"\"\nExtension for processing data before they are exported to feeds.\n\"\"\"\n\nfrom bz2 import BZ2File\nfrom gzip import GzipFile\nfrom io import IOBase\nfrom lzma import LZMAFile\nfrom typing import IO, Any, BinaryIO, Dict, List, cast\n\nfrom scrapy.utils.misc import load_object\n\n\nclass GzipPlugin:\n    \"\"\"\n    Compresses received data using `gzip <https://en.wikipedia.org/wiki/Gzip>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `gzip_compresslevel`\n    - `gzip_mtime`\n    - `gzip_filename`\n\n    See :py:class:`gzip.GzipFile` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n        compress_level = self.feed_options.get(\"gzip_compresslevel\", 9)\n        mtime = self.feed_options.get(\"gzip_mtime\")\n        filename = self.feed_options.get(\"gzip_filename\")\n        self.gzipfile = GzipFile(\n            fileobj=self.file,\n            mode=\"wb\",\n            compresslevel=compress_level,\n            mtime=mtime,\n            filename=filename,\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.gzipfile.write(data)\n\n    def close(self) -> None:\n        self.gzipfile.close()\n\n\nclass Bz2Plugin:\n    \"\"\"\n    Compresses received data using `bz2 <https://en.wikipedia.org/wiki/Bzip2>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `bz2_compresslevel`\n\n    See :py:class:`bz2.BZ2File` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n        compress_level = self.feed_options.get(\"bz2_compresslevel\", 9)\n        self.bz2file = BZ2File(\n            filename=self.file, mode=\"wb\", compresslevel=compress_level\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.bz2file.write(data)\n\n    def close(self) -> None:\n        self.bz2file.close()\n\n\nclass LZMAPlugin:\n    \"\"\"\n    Compresses received data using `lzma <https://en.wikipedia.org/wiki/Lempel\u2013Ziv\u2013Markov_chain_algorithm>`_.\n\n    Accepted ``feed_options`` parameters:\n\n    - `lzma_format`\n    - `lzma_check`\n    - `lzma_preset`\n    - `lzma_filters`\n\n    .. note::\n        ``lzma_filters`` cannot be used in pypy version 7.3.1 and older.\n\n    See :py:class:`lzma.LZMAFile` for more info about parameters.\n    \"\"\"\n\n    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n        self.file = file\n        self.feed_options = feed_options\n\n        format = self.feed_options.get(\"lzma_format\")\n        check = self.feed_options.get(\"lzma_check\", -1)\n        preset = self.feed_options.get(\"lzma_preset\")\n        filters = self.feed_options.get(\"lzma_filters\")\n        self.lzmafile = LZMAFile(\n            filename=self.file,\n            mode=\"wb\",\n            format=format,\n            check=check,\n            preset=preset,\n            filters=filters,\n        )\n\n    def write(self, data: bytes) -> int:\n        return self.lzmafile.write(data)\n\n    def close(self) -> None:\n        self.lzmafile.close()\n\n\n# io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n# instance as a file like writable object. This could be needed by some exporters\n# such as CsvItemExporter which wraps the feed storage with io.TextIOWrapper.\nclass PostProcessingManager(IOBase):\n    \"\"\"\n    This will manage and use declared plugins to process data in a\n    pipeline-ish way.\n    :param plugins: all the declared plugins for the feed\n    :type plugins: list\n    :param file: final target file where the processed data will be written\n    :type file: file like object\n    \"\"\"\n\n    def __init__(\n        self, plugins: List[Any], file: IO[bytes], feed_options: Dict[str, Any]\n    ) -> None:\n        self.plugins = self._load_plugins(plugins)\n        self.file = file\n        self.feed_options = feed_options\n        self.head_plugin = self._get_head_plugin()\n\n    def write(self, data: bytes) -> int:\n        \"\"\"\n        Uses all the declared plugins to process data first, then writes\n        the processed data to target file.\n        :param data: data passed to be written to target file\n        :type data: bytes\n        :return: returns number of bytes written\n        :rtype: int\n        \"\"\"\n        return cast(int, self.head_plugin.write(data))\n\n    def tell(self) -> int:\n        return self.file.tell()\n\n    def close(self) -> None:\n        \"\"\"\n        Close the target file along with all the plugins.\n        \"\"\"\n        self.head_plugin.close()\n\n    def writable(self) -> bool:\n        return True\n\n    def _load_plugins(self, plugins: List[Any]) -> List[Any]:\n        plugins = [load_object(plugin) for plugin in plugins]\n        return plugins\n\n    def _get_head_plugin(self) -> Any:\n        prev = self.file\n        for plugin in self.plugins[::-1]:\n            prev = plugin(prev, self.feed_options)\n        return prev\n", "scrapy/extensions/periodic_log.py": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime, timezone\nfrom json import JSONEncoder\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n\nfrom twisted.internet import task\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass PeriodicLog:\n    \"\"\"Log basic scraping stats periodically\"\"\"\n\n    def __init__(\n        self,\n        stats: StatsCollector,\n        interval: float = 60.0,\n        ext_stats: Dict[str, Any] = {},\n        ext_delta: Dict[str, Any] = {},\n        ext_timing_enabled: bool = False,\n    ):\n        self.stats: StatsCollector = stats\n        self.interval: float = interval\n        self.multiplier: float = 60.0 / self.interval\n        self.task: Optional[task.LoopingCall] = None\n        self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n        self.ext_stats_enabled: bool = bool(ext_stats)\n        self.ext_stats_include: List[str] = ext_stats.get(\"include\", [])\n        self.ext_stats_exclude: List[str] = ext_stats.get(\"exclude\", [])\n        self.ext_delta_enabled: bool = bool(ext_delta)\n        self.ext_delta_include: List[str] = ext_delta.get(\"include\", [])\n        self.ext_delta_exclude: List[str] = ext_delta.get(\"exclude\", [])\n        self.ext_timing_enabled: bool = ext_timing_enabled\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n        if not interval:\n            raise NotConfigured\n        try:\n            ext_stats: Optional[Dict[str, Any]] = crawler.settings.getdict(\n                \"PERIODIC_LOG_STATS\"\n            )\n        except (TypeError, ValueError):\n            ext_stats = (\n                {\"enabled\": True}\n                if crawler.settings.getbool(\"PERIODIC_LOG_STATS\")\n                else None\n            )\n        try:\n            ext_delta: Optional[Dict[str, Any]] = crawler.settings.getdict(\n                \"PERIODIC_LOG_DELTA\"\n            )\n        except (TypeError, ValueError):\n            ext_delta = (\n                {\"enabled\": True}\n                if crawler.settings.getbool(\"PERIODIC_LOG_DELTA\")\n                else None\n            )\n\n        ext_timing_enabled: bool = crawler.settings.getbool(\n            \"PERIODIC_LOG_TIMING_ENABLED\", False\n        )\n        if not (ext_stats or ext_delta or ext_timing_enabled):\n            raise NotConfigured\n        assert crawler.stats\n        assert ext_stats is not None\n        assert ext_delta is not None\n        o = cls(\n            crawler.stats,\n            interval,\n            ext_stats,\n            ext_delta,\n            ext_timing_enabled,\n        )\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.time_prev: datetime = datetime.now(tz=timezone.utc)\n        self.delta_prev: Dict[str, Union[int, float]] = {}\n        self.stats_prev: Dict[str, Union[int, float]] = {}\n\n        self.task = task.LoopingCall(self.log)\n        self.task.start(self.interval)\n\n    def log(self) -> None:\n        data: Dict[str, Any] = {}\n        if self.ext_timing_enabled:\n            data.update(self.log_timing())\n        if self.ext_delta_enabled:\n            data.update(self.log_delta())\n        if self.ext_stats_enabled:\n            data.update(self.log_crawler_stats())\n        logger.info(self.encoder.encode(data))\n\n    def log_delta(self) -> Dict[str, Any]:\n        num_stats: Dict[str, Union[int, float]] = {\n            k: v\n            for k, v in self.stats._stats.items()\n            if isinstance(v, (int, float))\n            and self.param_allowed(k, self.ext_delta_include, self.ext_delta_exclude)\n        }\n        delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}\n        self.delta_prev = num_stats\n        return {\"delta\": delta}\n\n    def log_timing(self) -> Dict[str, Any]:\n        now = datetime.now(tz=timezone.utc)\n        time = {\n            \"log_interval\": self.interval,\n            \"start_time\": self.stats._stats[\"start_time\"],\n            \"utcnow\": now,\n            \"log_interval_real\": (now - self.time_prev).total_seconds(),\n            \"elapsed\": (now - self.stats._stats[\"start_time\"]).total_seconds(),\n        }\n        self.time_prev = now\n        return {\"time\": time}\n\n    def log_crawler_stats(self) -> Dict[str, Any]:\n        stats = {\n            k: v\n            for k, v in self.stats._stats.items()\n            if self.param_allowed(k, self.ext_stats_include, self.ext_stats_exclude)\n        }\n        return {\"stats\": stats}\n\n    def param_allowed(\n        self, stat_name: str, include: List[str], exclude: List[str]\n    ) -> bool:\n        if not include and not exclude:\n            return True\n        for p in exclude:\n            if p in stat_name:\n                return False\n        if exclude and not include:\n            return True\n        for p in include:\n            if p in stat_name:\n                return True\n        return False\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        self.log()\n        if self.task and self.task.running:\n            self.task.stop()\n", "scrapy/extensions/memdebug.py": "\"\"\"\nMemoryDebugger extension\n\nSee documentation in docs/topics/extensions.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport gc\nfrom typing import TYPE_CHECKING\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.trackref import live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass MemoryDebugger:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"MEMDEBUG_ENABLED\"):\n            raise NotConfigured\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_closed(self, spider: Spider, reason: str) -> None:\n        gc.collect()\n        self.stats.set_value(\n            \"memdebug/gc_garbage_count\", len(gc.garbage), spider=spider\n        )\n        for cls, wdict in live_refs.items():\n            if not wdict:\n                continue\n            self.stats.set_value(\n                f\"memdebug/live_refs/{cls.__name__}\", len(wdict), spider=spider\n            )\n", "scrapy/extensions/__init__.py": "", "scrapy/extensions/telnet.py": "\"\"\"\nScrapy Telnet Console extension\n\nSee documentation in docs/topics/telnetconsole.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport binascii\nimport logging\nimport os\nimport pprint\nimport traceback\nfrom typing import TYPE_CHECKING, Any, Dict, List\n\nfrom twisted.internet import protocol\nfrom twisted.internet.tcp import Port\n\ntry:\n    from twisted.conch import manhole, telnet\n    from twisted.conch.insults import insults\n\n    TWISTED_CONCH_AVAILABLE = True\nexcept (ImportError, SyntaxError):\n    _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n    TWISTED_CONCH_AVAILABLE = False\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.decorators import defers\nfrom scrapy.utils.engine import print_engine_status\nfrom scrapy.utils.reactor import listen_tcp\nfrom scrapy.utils.trackref import print_live_refs\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\nlogger = logging.getLogger(__name__)\n\n# signal to update telnet variables\n# args: telnet_vars\nupdate_telnet_vars = object()\n\n\nclass TelnetConsole(protocol.ServerFactory):\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n            raise NotConfigured\n        if not TWISTED_CONCH_AVAILABLE:\n            raise NotConfigured(\n                \"TELNETCONSOLE_ENABLED setting is True but required twisted \"\n                \"modules failed to import:\\n\" + _TWISTED_CONCH_TRACEBACK\n            )\n        self.crawler: Crawler = crawler\n        self.noisy: bool = False\n        self.portrange: List[int] = [\n            int(x) for x in crawler.settings.getlist(\"TELNETCONSOLE_PORT\")\n        ]\n        self.host: str = crawler.settings[\"TELNETCONSOLE_HOST\"]\n        self.username: str = crawler.settings[\"TELNETCONSOLE_USERNAME\"]\n        self.password: str = crawler.settings[\"TELNETCONSOLE_PASSWORD\"]\n\n        if not self.password:\n            self.password = binascii.hexlify(os.urandom(8)).decode(\"utf8\")\n            logger.info(\"Telnet Password: %s\", self.password)\n\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def start_listening(self) -> None:\n        self.port: Port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\n            \"Telnet console listening on %(host)s:%(port)d\",\n            {\"host\": h.host, \"port\": h.port},\n            extra={\"crawler\": self.crawler},\n        )\n\n    def stop_listening(self) -> None:\n        self.port.stopListening()\n\n    def protocol(self) -> telnet.TelnetTransport:  # type: ignore[override]\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n\n            @defers\n            def login(self_, credentials, mind, *interfaces):\n                if not (\n                    credentials.username == self.username.encode(\"utf8\")\n                    and credentials.checkPassword(self.password.encode(\"utf8\"))\n                ):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol, manhole.Manhole, self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())\n\n    def _get_telnet_vars(self) -> Dict[str, Any]:\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        assert self.crawler.engine\n        telnet_vars: Dict[str, Any] = {\n            \"engine\": self.crawler.engine,\n            \"spider\": self.crawler.engine.spider,\n            \"slot\": self.crawler.engine.slot,\n            \"crawler\": self.crawler,\n            \"extensions\": self.crawler.extensions,\n            \"stats\": self.crawler.stats,\n            \"settings\": self.crawler.settings,\n            \"est\": lambda: print_engine_status(self.crawler.engine),\n            \"p\": pprint.pprint,\n            \"prefs\": print_live_refs,\n            \"help\": \"This is Scrapy telnet console. For more info see: \"\n            \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars\n", "scrapy/templates/project/module/__init__.py": "", "scrapy/templates/project/module/spiders/__init__.py": "# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n", "scrapy/pipelines/files.py": "\"\"\"\nFiles Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport functools\nimport hashlib\nimport logging\nimport mimetypes\nimport time\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom ftplib import FTP\nfrom io import BytesIO\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    List,\n    NoReturn,\n    Optional,\n    Protocol,\n    Set,\n    Type,\n    TypedDict,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nfrom itemadapter import ItemAdapter\nfrom twisted.internet import defer, threads\nfrom twisted.internet.defer import Deferred\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.datatypes import CaseInsensitiveDict\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.request import referer_str\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _to_string(path: Union[str, PathLike[str]]) -> str:\n    return str(path)  # convert a Path object to string\n\n\ndef _md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> _md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()  # nosec\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\nclass FileException(Exception):\n    \"\"\"General media error exception\"\"\"\n\n\nclass StatInfo(TypedDict, total=False):\n    checksum: str\n    last_modified: float\n\n\nclass FilesStoreProtocol(Protocol):\n    def __init__(self, basedir: str): ...\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Optional[Deferred[Any]]: ...\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Union[StatInfo, Deferred[StatInfo]]: ...\n\n\nclass FSFilesStore:\n    def __init__(self, basedir: Union[str, PathLike[str]]):\n        basedir = _to_string(basedir)\n        if \"://\" in basedir:\n            basedir = basedir.split(\"://\", 1)[1]\n        self.basedir: str = basedir\n        self._mkdir(Path(self.basedir))\n        self.created_directories: DefaultDict[MediaPipeline.SpiderInfo, Set[str]] = (\n            defaultdict(set)\n        )\n\n    def persist_file(\n        self,\n        path: Union[str, PathLike[str]],\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> None:\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(absolute_path.parent, info)\n        absolute_path.write_bytes(buf.getvalue())\n\n    def stat_file(\n        self, path: Union[str, PathLike[str]], info: MediaPipeline.SpiderInfo\n    ) -> StatInfo:\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = absolute_path.stat().st_mtime\n        except OSError:\n            return {}\n\n        with absolute_path.open(\"rb\") as f:\n            checksum = _md5sum(f)\n\n        return {\"last_modified\": last_modified, \"checksum\": checksum}\n\n    def _get_filesystem_path(self, path: Union[str, PathLike[str]]) -> Path:\n        path_comps = _to_string(path).split(\"/\")\n        return Path(self.basedir, *path_comps)\n\n    def _mkdir(\n        self, dirname: Path, domain: Optional[MediaPipeline.SpiderInfo] = None\n    ) -> None:\n        seen: Set[str] = self.created_directories[domain] if domain else set()\n        if str(dirname) not in seen:\n            if not dirname.exists():\n                dirname.mkdir(parents=True)\n            seen.add(str(dirname))\n\n\nclass S3FilesStore:\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n    AWS_SESSION_TOKEN = None\n    AWS_ENDPOINT_URL = None\n    AWS_REGION_NAME = None\n    AWS_USE_SSL = None\n    AWS_VERIFY = None\n\n    POLICY = \"private\"  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n    HEADERS = {\n        \"Cache-Control\": \"max-age=172800\",\n    }\n\n    def __init__(self, uri: str):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n        import botocore.session\n\n        session = botocore.session.get_session()\n        self.s3_client = session.create_client(\n            \"s3\",\n            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n            aws_session_token=self.AWS_SESSION_TOKEN,\n            endpoint_url=self.AWS_ENDPOINT_URL,\n            region_name=self.AWS_REGION_NAME,\n            use_ssl=self.AWS_USE_SSL,\n            verify=self.AWS_VERIFY,\n        )\n        if not uri.startswith(\"s3://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n        self.bucket, self.prefix = uri[5:].split(\"/\", 1)\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _onsuccess(boto_key: Dict[str, Any]) -> StatInfo:\n            checksum = boto_key[\"ETag\"].strip('\"')\n            last_modified = boto_key[\"LastModified\"]\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {\"checksum\": checksum, \"last_modified\": modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)\n\n    def _get_boto_key(self, path: str) -> Deferred[Dict[str, Any]]:\n        key_name = f\"{self.prefix}{path}\"\n        return cast(\n            \"Deferred[Dict[str, Any]]\",\n            threads.deferToThread(\n                self.s3_client.head_object, Bucket=self.bucket, Key=key_name  # type: ignore[attr-defined]\n            ),\n        )\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Deferred[Any]:\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = f\"{self.prefix}{path}\"\n        buf.seek(0)\n        extra = self._headers_to_botocore_kwargs(self.HEADERS)\n        if headers:\n            extra.update(self._headers_to_botocore_kwargs(headers))\n        return threads.deferToThread(\n            self.s3_client.put_object,  # type: ignore[attr-defined]\n            Bucket=self.bucket,\n            Key=key_name,\n            Body=buf,\n            Metadata={k: str(v) for k, v in (meta or {}).items()},\n            ACL=self.POLICY,\n            **extra,\n        )\n\n    def _headers_to_botocore_kwargs(self, headers: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaseInsensitiveDict(\n            {\n                \"Content-Type\": \"ContentType\",\n                \"Cache-Control\": \"CacheControl\",\n                \"Content-Disposition\": \"ContentDisposition\",\n                \"Content-Encoding\": \"ContentEncoding\",\n                \"Content-Language\": \"ContentLanguage\",\n                \"Content-Length\": \"ContentLength\",\n                \"Content-MD5\": \"ContentMD5\",\n                \"Expires\": \"Expires\",\n                \"X-Amz-Grant-Full-Control\": \"GrantFullControl\",\n                \"X-Amz-Grant-Read\": \"GrantRead\",\n                \"X-Amz-Grant-Read-ACP\": \"GrantReadACP\",\n                \"X-Amz-Grant-Write-ACP\": \"GrantWriteACP\",\n                \"X-Amz-Object-Lock-Legal-Hold\": \"ObjectLockLegalHoldStatus\",\n                \"X-Amz-Object-Lock-Mode\": \"ObjectLockMode\",\n                \"X-Amz-Object-Lock-Retain-Until-Date\": \"ObjectLockRetainUntilDate\",\n                \"X-Amz-Request-Payer\": \"RequestPayer\",\n                \"X-Amz-Server-Side-Encryption\": \"ServerSideEncryption\",\n                \"X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id\": \"SSEKMSKeyId\",\n                \"X-Amz-Server-Side-Encryption-Context\": \"SSEKMSEncryptionContext\",\n                \"X-Amz-Server-Side-Encryption-Customer-Algorithm\": \"SSECustomerAlgorithm\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key\": \"SSECustomerKey\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key-Md5\": \"SSECustomerKeyMD5\",\n                \"X-Amz-Storage-Class\": \"StorageClass\",\n                \"X-Amz-Tagging\": \"Tagging\",\n                \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n            }\n        )\n        extra: Dict[str, Any] = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n            else:\n                extra[kwarg] = value\n        return extra\n\n\nclass GCSFilesStore:\n    GCS_PROJECT_ID = None\n\n    CACHE_CONTROL = \"max-age=172800\"\n\n    # The bucket's default object ACL will be applied to the object.\n    # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n    POLICY = None\n\n    def __init__(self, uri: str):\n        from google.cloud import storage\n\n        client = storage.Client(project=self.GCS_PROJECT_ID)\n        bucket, prefix = uri[5:].split(\"/\", 1)\n        self.bucket = client.bucket(bucket)\n        self.prefix: str = prefix\n        permissions = self.bucket.test_iam_permissions(\n            [\"storage.objects.get\", \"storage.objects.create\"]\n        )\n        if \"storage.objects.get\" not in permissions:\n            logger.warning(\n                \"No 'storage.objects.get' permission for GSC bucket %(bucket)s. \"\n                \"Checking if files are up to date will be impossible. Files will be downloaded every time.\",\n                {\"bucket\": bucket},\n            )\n        if \"storage.objects.create\" not in permissions:\n            logger.error(\n                \"No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!\",\n                {\"bucket\": bucket},\n            )\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _onsuccess(blob) -> StatInfo:\n            if blob:\n                checksum = base64.b64decode(blob.md5_hash).hex()\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {\"checksum\": checksum, \"last_modified\": last_modified}\n            return {}\n\n        blob_path = self._get_blob_path(path)\n        return cast(\n            Deferred[StatInfo],\n            threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n                _onsuccess\n            ),\n        )\n\n    def _get_content_type(self, headers: Optional[Dict[str, str]]) -> str:\n        if headers and \"Content-Type\" in headers:\n            return headers[\"Content-Type\"]\n        return \"application/octet-stream\"\n\n    def _get_blob_path(self, path: str) -> str:\n        return self.prefix + path\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Deferred[Any]:\n        blob_path = self._get_blob_path(path)\n        blob = self.bucket.blob(blob_path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return threads.deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY,\n        )\n\n\nclass FTPFilesStore:\n    FTP_USERNAME: Optional[str] = None\n    FTP_PASSWORD: Optional[str] = None\n    USE_ACTIVE_MODE: Optional[bool] = None\n\n    def __init__(self, uri: str):\n        if not uri.startswith(\"ftp://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 'ftp'\")\n        u = urlparse(uri)\n        assert u.port\n        assert u.hostname\n        self.port: int = u.port\n        self.host: str = u.hostname\n        self.port = int(u.port or 21)\n        assert self.FTP_USERNAME\n        assert self.FTP_PASSWORD\n        self.username: str = u.username or self.FTP_USERNAME\n        self.password: str = u.password or self.FTP_PASSWORD\n        self.basedir: str = u.path.rstrip(\"/\")\n\n    def persist_file(\n        self,\n        path: str,\n        buf: BytesIO,\n        info: MediaPipeline.SpiderInfo,\n        meta: Optional[Dict[str, Any]] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Deferred[Any]:\n        path = f\"{self.basedir}/{path}\"\n        return threads.deferToThread(\n            ftp_store_file,\n            path=path,\n            file=buf,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.USE_ACTIVE_MODE,\n        )\n\n    def stat_file(\n        self, path: str, info: MediaPipeline.SpiderInfo\n    ) -> Deferred[StatInfo]:\n        def _stat_file(path: str) -> StatInfo:\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()  # nosec\n                ftp.retrbinary(f\"RETR {file_path}\", m.update)\n                return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n\n        return cast(\"Deferred[StatInfo]\", threads.deferToThread(_stat_file, path))\n\n\nclass FilesPipeline(MediaPipeline):\n    \"\"\"Abstract pipeline that implement the file downloading\n\n    This pipeline tries to minimize network transfers and file processing,\n    doing stat of the files and determining if file is new, up-to-date or\n    expired.\n\n    ``new`` files are those that pipeline never processed and needs to be\n        downloaded from supplier site the first time.\n\n    ``uptodate`` files are the ones that the pipeline processed and are still\n        valid files.\n\n    ``expired`` files are those that pipeline already processed but the last\n        modification was made long time ago, so a reprocessing is recommended to\n        refresh it in case of change.\n\n    \"\"\"\n\n    MEDIA_NAME: str = \"file\"\n    EXPIRES: int = 90\n    STORE_SCHEMES: Dict[str, Type[FilesStoreProtocol]] = {\n        \"\": FSFilesStore,\n        \"file\": FSFilesStore,\n        \"s3\": S3FilesStore,\n        \"gs\": GCSFilesStore,\n        \"ftp\": FTPFilesStore,\n    }\n    DEFAULT_FILES_URLS_FIELD: str = \"file_urls\"\n    DEFAULT_FILES_RESULT_FIELD: str = \"files\"\n\n    def __init__(\n        self,\n        store_uri: Union[str, PathLike[str]],\n        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n        settings: Union[Settings, Dict[str, Any], None] = None,\n    ):\n        store_uri = _to_string(store_uri)\n        if not store_uri:\n            raise NotConfigured\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        cls_name = \"FilesPipeline\"\n        self.store: FilesStoreProtocol = self._get_store(store_uri)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=cls_name, settings=settings\n        )\n        self.expires: int = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field: str = settings.get(\n            resolve(\"FILES_URLS_FIELD\"), self.FILES_URLS_FIELD\n        )\n        self.files_result_field: str = settings.get(\n            resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n        )\n\n        super().__init__(download_func=download_func, settings=settings)\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -> Self:\n        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n        s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n\n        gcs_store: Type[GCSFilesStore] = cast(\n            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n        )\n        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n        gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n\n        ftp_store: Type[FTPFilesStore] = cast(\n            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n        )\n        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n        store_uri = settings[\"FILES_STORE\"]\n        return cls(store_uri, settings=settings)\n\n    def _get_store(self, uri: str) -> FilesStoreProtocol:\n        if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n            scheme = \"file\"\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)\n\n    def media_to_download(\n        self, request: Request, info: MediaPipeline.SpiderInfo, *, item: Any = None\n    ) -> Deferred[Optional[FileInfo]]:\n        def _onsuccess(result: StatInfo) -> Optional[FileInfo]:\n            if not result:\n                return None  # returning None force download\n\n            last_modified = result.get(\"last_modified\", None)\n            if not last_modified:\n                return None  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return None  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                \"File (uptodate): Downloaded %(medianame)s from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"medianame\": self.MEDIA_NAME, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            self.inc_stats(info.spider, \"uptodate\")\n\n            checksum = result.get(\"checksum\", None)\n            return {\n                \"url\": request.url,\n                \"path\": path,\n                \"checksum\": checksum,\n                \"status\": \"uptodate\",\n            }\n\n        path = self.file_path(request, info=info, item=item)\n        # defer.maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n        dfd: Deferred[StatInfo] = defer.maybeDeferred(self.store.stat_file, path, info)  # type: ignore[arg-type]\n        dfd2: Deferred[Optional[FileInfo]] = dfd.addCallback(_onsuccess)\n        dfd2.addErrback(lambda _: None)\n        dfd2.addErrback(\n            lambda f: logger.error(\n                self.__class__.__name__ + \".store.stat_file\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": info.spider},\n            )\n        )\n        return dfd2\n\n    def media_failed(\n        self, failure: Failure, request: Request, info: MediaPipeline.SpiderInfo\n    ) -> NoReturn:\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                \"File (unknown-error): Error downloading %(medianame)s from \"\n                \"%(request)s referred in <%(referer)s>: %(exception)s\",\n                {\n                    \"medianame\": self.MEDIA_NAME,\n                    \"request\": request,\n                    \"referer\": referer,\n                    \"exception\": failure.value,\n                },\n                extra={\"spider\": info.spider},\n            )\n\n        raise FileException\n\n    def media_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> FileInfo:\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                \"File (code: %(status)s): Error downloading file from \"\n                \"%(request)s referred in <%(referer)s>\",\n                {\"status\": response.status, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"download-error\")\n\n        if not response.body:\n            logger.warning(\n                \"File (empty-content): Empty file from %(request)s referred \"\n                \"in <%(referer)s>: no-content\",\n                {\"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"empty-content\")\n\n        status = \"cached\" if \"cached\" in response.flags else \"downloaded\"\n        logger.debug(\n            \"File (%(status)s): Downloaded file from %(request)s referred in \"\n            \"<%(referer)s>\",\n            {\"status\": status, \"request\": request, \"referer\": referer},\n            extra={\"spider\": info.spider},\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info, item=item)\n            checksum = self.file_downloaded(response, request, info, item=item)\n        except FileException as exc:\n            logger.warning(\n                \"File (error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>: %(errormsg)s\",\n                {\"request\": request, \"referer\": referer, \"errormsg\": str(exc)},\n                extra={\"spider\": info.spider},\n                exc_info=True,\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                \"File (unknown-error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"request\": request, \"referer\": referer},\n                exc_info=True,\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(str(exc))\n\n        return {\n            \"url\": request.url,\n            \"path\": path,\n            \"checksum\": checksum,\n            \"status\": status,\n        }\n\n    def inc_stats(self, spider: Spider, status: str) -> None:\n        assert spider.crawler.stats\n        spider.crawler.stats.inc_value(\"file_count\", spider=spider)\n        spider.crawler.stats.inc_value(f\"file_status_count/{status}\", spider=spider)\n\n    # Overridable Interface\n    def get_media_requests(\n        self, item: Any, info: MediaPipeline.SpiderInfo\n    ) -> List[Request]:\n        urls = ItemAdapter(item).get(self.files_urls_field, [])\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def file_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        path = self.file_path(request, response=response, info=info, item=item)\n        buf = BytesIO(response.body)\n        checksum = _md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum\n\n    def item_completed(\n        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n    ) -> Any:\n        with suppress(KeyError):\n            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(\n        self,\n        request: Request,\n        response: Optional[Response] = None,\n        info: Optional[MediaPipeline.SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n        media_ext = Path(request.url).suffix\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = \"\"\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = cast(str, mimetypes.guess_extension(media_type))\n        return f\"full/{media_guid}{media_ext}\"\n", "scrapy/pipelines/media.py": "from __future__ import annotations\n\nimport functools\nimport logging\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    List,\n    Literal,\n    NoReturn,\n    Optional,\n    Set,\n    Tuple,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom twisted.internet.defer import Deferred, DeferredList\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.http.request import NO_CALLBACK, Request\nfrom scrapy.settings import Settings\nfrom scrapy.utils.datatypes import SequenceExclude\nfrom scrapy.utils.defer import defer_result, mustbe_deferred\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.request import RequestFingerprinter\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n_T = TypeVar(\"_T\")\n\n\nclass FileInfo(TypedDict):\n    url: str\n    path: str\n    checksum: Optional[str]\n    status: str\n\n\nFileInfoOrError = Union[Tuple[Literal[True], FileInfo], Tuple[Literal[False], Failure]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MediaPipeline(ABC):\n    crawler: Crawler\n    _fingerprinter: RequestFingerprinter\n\n    LOG_FAILED_RESULTS: bool = True\n\n    class SpiderInfo:\n        def __init__(self, spider: Spider):\n            self.spider: Spider = spider\n            self.downloading: Set[bytes] = set()\n            self.downloaded: Dict[bytes, Union[FileInfo, Failure]] = {}\n            self.waiting: DefaultDict[bytes, List[Deferred[FileInfo]]] = defaultdict(\n                list\n            )\n\n    def __init__(\n        self,\n        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n        settings: Union[Settings, Dict[str, Any], None] = None,\n    ):\n        self.download_func = download_func\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n        )\n        self.allow_redirects: bool = settings.getbool(\n            resolve(\"MEDIA_ALLOW_REDIRECTS\"), False\n        )\n        self._handle_statuses(self.allow_redirects)\n\n    def _handle_statuses(self, allow_redirects: bool) -> None:\n        self.handle_httpstatus_list = None\n        if allow_redirects:\n            self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n\n    def _key_for_pipe(\n        self,\n        key: str,\n        base_class_name: Optional[str] = None,\n        settings: Optional[Settings] = None,\n    ) -> str:\n        class_name = self.__class__.__name__\n        formatted_key = f\"{class_name.upper()}_{key}\"\n        if (\n            not base_class_name\n            or class_name == base_class_name\n            or settings\n            and not settings.get(formatted_key)\n        ):\n            return key\n        return formatted_key\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        pipe: Self\n        try:\n            pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n        except AttributeError:\n            pipe = cls()\n        pipe.crawler = crawler\n        assert crawler.request_fingerprinter\n        pipe._fingerprinter = crawler.request_fingerprinter\n        return pipe\n\n    def open_spider(self, spider: Spider) -> None:\n        self.spiderinfo = self.SpiderInfo(spider)\n\n    def process_item(\n        self, item: Any, spider: Spider\n    ) -> Deferred[List[FileInfoOrError]]:\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info, item) for r in requests]\n        dfd = cast(\n            \"Deferred[List[FileInfoOrError]]\", DeferredList(dlist, consumeErrors=True)\n        )\n        return dfd.addCallback(self.item_completed, item, info)\n\n    def _process_request(\n        self, request: Request, info: SpiderInfo, item: Any\n    ) -> Deferred[FileInfo]:\n        fp = self._fingerprinter.fingerprint(request)\n        eb = request.errback\n        request.callback = NO_CALLBACK\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            d = defer_result(info.downloaded[fp])\n            if eb:\n                d.addErrback(eb)\n            return d\n\n        # Otherwise, wait for result\n        wad: Deferred[FileInfo] = Deferred()\n        if eb:\n            wad.addErrback(eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd: Deferred[Optional[FileInfo]] = mustbe_deferred(\n            self.media_to_download, request, info, item=item\n        )\n        dfd2: Deferred[FileInfo] = dfd.addCallback(\n            self._check_media_to_download, request, info, item=item\n        )\n        dfd2.addErrback(self._log_exception)\n        dfd2.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        return dfd2.addBoth(lambda _: wad)  # it must return wad at last\n\n    def _log_exception(self, result: Failure) -> Failure:\n        logger.exception(result)\n        return result\n\n    def _modify_media_request(self, request: Request) -> None:\n        if self.handle_httpstatus_list:\n            request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n        else:\n            request.meta[\"handle_httpstatus_all\"] = True\n\n    def _check_media_to_download(\n        self, result: Optional[FileInfo], request: Request, info: SpiderInfo, item: Any\n    ) -> Union[FileInfo, Deferred[FileInfo]]:\n        if result is not None:\n            return result\n        dfd: Deferred[Response]\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n        else:\n            self._modify_media_request(request)\n            assert self.crawler.engine\n            dfd = self.crawler.engine.download(request)\n        dfd2: Deferred[FileInfo] = dfd.addCallback(\n            self.media_downloaded, request, info, item=item\n        )\n        dfd2.addErrback(self.media_failed, request, info)\n        return dfd2\n\n    def _cache_result_and_execute_waiters(\n        self, result: Union[FileInfo, Failure], fp: bytes, info: SpiderInfo\n    ) -> None:\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n\n            # This code fixes a memory leak by avoiding to keep references to\n            # the Request and Response objects on the Media Pipeline cache.\n            #\n            # What happens when the media_downloaded callback raises an\n            # exception, for example a FileException('download-error') when\n            # the Response status code is not 200 OK, is that the original\n            # StopIteration exception (which in turn contains the failed\n            # Response and by extension, the original Request) gets encapsulated\n            # within the FileException context.\n            #\n            # Originally, Scrapy was using twisted.internet.defer.returnValue\n            # inside functions decorated with twisted.internet.defer.inlineCallbacks,\n            # encapsulating the returned Response in a _DefGen_Return exception\n            # instead of a StopIteration.\n            #\n            # To avoid keeping references to the Response and therefore Request\n            # objects on the Media Pipeline cache, we should wipe the context of\n            # the encapsulated exception when it is a StopIteration instance\n            #\n            # This problem does not occur in Python 2.7 since we don't have\n            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n            context = getattr(result.value, \"__context__\", None)\n            if isinstance(context, StopIteration):\n                result.value.__context__ = None\n\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)\n\n    # Overridable Interface\n    @abstractmethod\n    def media_to_download(\n        self, request: Request, info: SpiderInfo, *, item: Any = None\n    ) -> Deferred[Optional[FileInfo]]:\n        \"\"\"Check request before starting download\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_media_requests(self, item: Any, info: SpiderInfo) -> List[Request]:\n        \"\"\"Returns the media requests to download\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def media_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> FileInfo:\n        \"\"\"Handler for success downloads\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def media_failed(\n        self, failure: Failure, request: Request, info: SpiderInfo\n    ) -> NoReturn:\n        \"\"\"Handler for failed downloads\"\"\"\n        raise NotImplementedError()\n\n    def item_completed(\n        self, results: List[FileInfoOrError], item: Any, info: SpiderInfo\n    ) -> Any:\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    assert isinstance(value, Failure)\n                    logger.error(\n                        \"%(class)s found errors processing %(item)s\",\n                        {\"class\": self.__class__.__name__, \"item\": item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={\"spider\": info.spider},\n                    )\n        return item\n\n    @abstractmethod\n    def file_path(\n        self,\n        request: Request,\n        response: Optional[Response] = None,\n        info: Optional[SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        \"\"\"Returns the path where downloaded media should be stored\"\"\"\n        raise NotImplementedError()\n", "scrapy/pipelines/__init__.py": "\"\"\"\nItem pipeline\n\nSee documentation in docs/item-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, List\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy import Spider\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import Settings\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.defer import deferred_f_from_coro_f\n\n\nclass ItemPipelineManager(MiddlewareManager):\n    component_name = \"item pipeline\"\n\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n        return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n\n    def _add_middleware(self, pipe: Any) -> None:\n        super()._add_middleware(pipe)\n        if hasattr(pipe, \"process_item\"):\n            self.methods[\"process_item\"].append(\n                deferred_f_from_coro_f(pipe.process_item)\n            )\n\n    def process_item(self, item: Any, spider: Spider) -> Deferred[Any]:\n        return self._process_chain(\"process_item\", item, spider)\n", "scrapy/pipelines/images.py": "\"\"\"\nImages Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport hashlib\nimport warnings\nfrom contextlib import suppress\nfrom io import BytesIO\nfrom os import PathLike\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nfrom itemadapter import ItemAdapter\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.files import (\n    FileException,\n    FilesPipeline,\n    FTPFilesStore,\n    GCSFilesStore,\n    S3FilesStore,\n    _md5sum,\n)\nfrom scrapy.pipelines.media import FileInfoOrError, MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import get_func_args, to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from PIL import Image\n    from typing_extensions import Self\n\n\nclass NoimagesDrop(DropItem):\n    \"\"\"Product with no images exception\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        warnings.warn(\n            \"The NoimagesDrop class is deprecated\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        super().__init__(*args, **kwargs)\n\n\nclass ImageException(FileException):\n    \"\"\"General image error exception\"\"\"\n\n\nclass ImagesPipeline(FilesPipeline):\n    \"\"\"Abstract pipeline that implement the image thumbnail generation logic\"\"\"\n\n    MEDIA_NAME: str = \"image\"\n\n    # Uppercase attributes kept for backward compatibility with code that subclasses\n    # ImagesPipeline. They may be overridden by settings.\n    MIN_WIDTH: int = 0\n    MIN_HEIGHT: int = 0\n    EXPIRES: int = 90\n    THUMBS: Dict[str, Tuple[int, int]] = {}\n    DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n    DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n\n    def __init__(\n        self,\n        store_uri: Union[str, PathLike[str]],\n        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n        settings: Union[Settings, Dict[str, Any], None] = None,\n    ):\n        try:\n            from PIL import Image\n\n            self._Image = Image\n        except ImportError:\n            raise NotConfigured(\n                \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n            )\n\n        super().__init__(store_uri, settings=settings, download_func=download_func)\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        resolve = functools.partial(\n            self._key_for_pipe,\n            base_class_name=\"ImagesPipeline\",\n            settings=settings,\n        )\n        self.expires: int = settings.getint(resolve(\"IMAGES_EXPIRES\"), self.EXPIRES)\n\n        if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n            self.IMAGES_RESULT_FIELD: str = self.DEFAULT_IMAGES_RESULT_FIELD\n        if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n            self.IMAGES_URLS_FIELD: str = self.DEFAULT_IMAGES_URLS_FIELD\n\n        self.images_urls_field: str = settings.get(\n            resolve(\"IMAGES_URLS_FIELD\"), self.IMAGES_URLS_FIELD\n        )\n        self.images_result_field: str = settings.get(\n            resolve(\"IMAGES_RESULT_FIELD\"), self.IMAGES_RESULT_FIELD\n        )\n        self.min_width: int = settings.getint(\n            resolve(\"IMAGES_MIN_WIDTH\"), self.MIN_WIDTH\n        )\n        self.min_height: int = settings.getint(\n            resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT\n        )\n        self.thumbs: Dict[str, Tuple[int, int]] = settings.get(\n            resolve(\"IMAGES_THUMBS\"), self.THUMBS\n        )\n\n        self._deprecated_convert_image: Optional[bool] = None\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -> Self:\n        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n\n        gcs_store: Type[GCSFilesStore] = cast(\n            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n        )\n        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n\n        ftp_store: Type[FTPFilesStore] = cast(\n            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n        )\n        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n        store_uri = settings[\"IMAGES_STORE\"]\n        return cls(store_uri, settings=settings)\n\n    def file_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        return self.image_downloaded(response, request, info, item=item)\n\n    def image_downloaded(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> str:\n        checksum: Optional[str] = None\n        for path, image, buf in self.get_images(response, request, info, item=item):\n            if checksum is None:\n                buf.seek(0)\n                checksum = _md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path,\n                buf,\n                info,\n                meta={\"width\": width, \"height\": height},\n                headers={\"Content-Type\": \"image/jpeg\"},\n            )\n        assert checksum is not None\n        return checksum\n\n    def get_images(\n        self,\n        response: Response,\n        request: Request,\n        info: MediaPipeline.SpiderInfo,\n        *,\n        item: Any = None,\n    ) -> Iterable[Tuple[str, Image.Image, BytesIO]]:\n        path = self.file_path(request, response=response, info=info, item=item)\n        orig_image = self._Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\n                \"Image too small \"\n                f\"({width}x{height} < \"\n                f\"{self.min_width}x{self.min_height})\"\n            )\n\n        if self._deprecated_convert_image is None:\n            self._deprecated_convert_image = \"response_body\" not in get_func_args(\n                self.convert_image\n            )\n            if self._deprecated_convert_image:\n                warnings.warn(\n                    f\"{self.__class__.__name__}.convert_image() method overridden in a deprecated way, \"\n                    \"overridden method does not accept response_body argument.\",\n                    category=ScrapyDeprecationWarning,\n                )\n\n        if self._deprecated_convert_image:\n            image, buf = self.convert_image(orig_image)\n        else:\n            image, buf = self.convert_image(\n                orig_image, response_body=BytesIO(response.body)\n            )\n        yield path, image, buf\n\n        for thumb_id, size in self.thumbs.items():\n            thumb_path = self.thumb_path(\n                request, thumb_id, response=response, info=info, item=item\n            )\n            if self._deprecated_convert_image:\n                thumb_image, thumb_buf = self.convert_image(image, size)\n            else:\n                thumb_image, thumb_buf = self.convert_image(image, size, buf)\n            yield thumb_path, thumb_image, thumb_buf\n\n    def convert_image(\n        self,\n        image: Image.Image,\n        size: Optional[Tuple[int, int]] = None,\n        response_body: Optional[BytesIO] = None,\n    ) -> Tuple[Image.Image, BytesIO]:\n        if response_body is None:\n            warnings.warn(\n                f\"{self.__class__.__name__}.convert_image() method called in a deprecated way, \"\n                \"method called without response_body argument.\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n\n        if image.format in (\"PNG\", \"WEBP\") and image.mode == \"RGBA\":\n            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert(\"RGB\")\n        elif image.mode == \"P\":\n            image = image.convert(\"RGBA\")\n            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert(\"RGB\")\n        elif image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        if size:\n            image = image.copy()\n            try:\n                # Image.Resampling.LANCZOS was added in Pillow 9.1.0\n                # remove this try except block,\n                # when updating the minimum requirements for Pillow.\n                resampling_filter = self._Image.Resampling.LANCZOS\n            except AttributeError:\n                resampling_filter = self._Image.ANTIALIAS  # type: ignore[attr-defined]\n            image.thumbnail(size, resampling_filter)\n        elif response_body is not None and image.format == \"JPEG\":\n            return image, response_body\n\n        buf = BytesIO()\n        image.save(buf, \"JPEG\")\n        return image, buf\n\n    def get_media_requests(\n        self, item: Any, info: MediaPipeline.SpiderInfo\n    ) -> List[Request]:\n        urls = ItemAdapter(item).get(self.images_urls_field, [])\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def item_completed(\n        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n    ) -> Any:\n        with suppress(KeyError):\n            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(\n        self,\n        request: Request,\n        response: Optional[Response] = None,\n        info: Optional[MediaPipeline.SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n        return f\"full/{image_guid}.jpg\"\n\n    def thumb_path(\n        self,\n        request: Request,\n        thumb_id: str,\n        response: Optional[Response] = None,\n        info: Optional[MediaPipeline.SpiderInfo] = None,\n        *,\n        item: Any = None,\n    ) -> str:\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n        return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n", "scrapy/spidermiddlewares/httperror.py": "\"\"\"\nHttpError Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Iterable, List, Optional\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Response\nfrom scrapy.settings import BaseSettings\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass HttpError(IgnoreRequest):\n    \"\"\"A non-200 response was filtered\"\"\"\n\n    def __init__(self, response: Response, *args: Any, **kwargs: Any):\n        self.response = response\n        super().__init__(*args, **kwargs)\n\n\nclass HttpErrorMiddleware:\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def __init__(self, settings: BaseSettings):\n        self.handle_httpstatus_all: bool = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n        self.handle_httpstatus_list: List[int] = settings.getlist(\n            \"HTTPERROR_ALLOWED_CODES\"\n        )\n\n    def process_spider_input(self, response: Response, spider: Spider) -> None:\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if meta.get(\"handle_httpstatus_all\", False):\n            return\n        if \"handle_httpstatus_list\" in meta:\n            allowed_statuses = meta[\"handle_httpstatus_list\"]\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(\n                spider, \"handle_httpstatus_list\", self.handle_httpstatus_list\n            )\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, \"Ignoring non-200 response\")\n\n    def process_spider_exception(\n        self, response: Response, exception: Exception, spider: Spider\n    ) -> Optional[Iterable[Any]]:\n        if isinstance(exception, HttpError):\n            assert spider.crawler.stats\n            spider.crawler.stats.inc_value(\"httperror/response_ignored_count\")\n            spider.crawler.stats.inc_value(\n                f\"httperror/response_ignored_status_count/{response.status}\"\n            )\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {\"response\": response},\n                extra={\"spider\": spider},\n            )\n            return []\n        return None\n", "scrapy/spidermiddlewares/offsite.py": "\"\"\"\nOffsite Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport re\nimport warnings\nfrom typing import TYPE_CHECKING, Any, AsyncIterable, Iterable, Set\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.httpobj import urlparse_cached\n\nwarnings.warn(\n    \"The scrapy.spidermiddlewares.offsite module is deprecated, use \"\n    \"scrapy.downloadermiddlewares.offsite instead.\",\n    ScrapyDeprecationWarning,\n)\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        return (r for r in result if self._filter(r, spider))\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        async for r in result:\n            if self._filter(r, spider):\n                yield r\n\n    def _filter(self, request: Any, spider: Spider) -> bool:\n        if not isinstance(request, Request):\n            return True\n        if request.dont_filter or self.should_follow(request, spider):\n            return True\n        domain = urlparse_cached(request).hostname\n        if domain and domain not in self.domains_seen:\n            self.domains_seen.add(domain)\n            logger.debug(\n                \"Filtered offsite request to %(domain)r: %(request)s\",\n                {\"domain\": domain, \"request\": request},\n                extra={\"spider\": spider},\n            )\n            self.stats.inc_value(\"offsite/domains\", spider=spider)\n        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n        return False\n\n    def should_follow(self, request: Request, spider: Spider) -> bool:\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or \"\"\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, \"allowed_domains\", None)\n        if not allowed_domains:\n            return re.compile(\"\")  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            if url_pattern.match(domain):\n                message = (\n                    \"allowed_domains accepts only domains, not URLs. \"\n                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message, URLWarning)\n            elif port_pattern.search(domain):\n                message = (\n                    \"allowed_domains accepts only domains without ports. \"\n                    f\"Ignoring entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message, PortWarning)\n            else:\n                domains.append(re.escape(domain))\n        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n        return re.compile(regex)\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n        self.domains_seen: Set[str] = set()\n\n\nclass URLWarning(Warning):\n    pass\n\n\nclass PortWarning(Warning):\n    pass\n", "scrapy/spidermiddlewares/referer.py": "\"\"\"\nRefererMiddleware: populates Request referer field, based on the Response which\noriginated it.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterable,\n    Dict,\n    Iterable,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.url import strip_url\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nLOCAL_SCHEMES: Tuple[str, ...] = (\n    \"about\",\n    \"blob\",\n    \"data\",\n    \"filesystem\",\n)\n\nPOLICY_NO_REFERRER = \"no-referrer\"\nPOLICY_NO_REFERRER_WHEN_DOWNGRADE = \"no-referrer-when-downgrade\"\nPOLICY_SAME_ORIGIN = \"same-origin\"\nPOLICY_ORIGIN = \"origin\"\nPOLICY_STRICT_ORIGIN = \"strict-origin\"\nPOLICY_ORIGIN_WHEN_CROSS_ORIGIN = \"origin-when-cross-origin\"\nPOLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN = \"strict-origin-when-cross-origin\"\nPOLICY_UNSAFE_URL = \"unsafe-url\"\nPOLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n\n\nclass ReferrerPolicy:\n    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES\n    name: str\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        raise NotImplementedError()\n\n    def stripped_referrer(self, url: str) -> Optional[str]:\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.strip_url(url)\n        return None\n\n    def origin_referrer(self, url: str) -> Optional[str]:\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.origin(url)\n        return None\n\n    def strip_url(self, url: str, origin_only: bool = False) -> Optional[str]:\n        \"\"\"\n        https://www.w3.org/TR/referrer-policy/#strip-url\n\n        If url is null, return no referrer.\n        If url's scheme is a local scheme, then return no referrer.\n        Set url's username to the empty string.\n        Set url's password to null.\n        Set url's fragment to null.\n        If the origin-only flag is true, then:\n            Set url's path to null.\n            Set url's query to null.\n        Return url.\n        \"\"\"\n        if not url:\n            return None\n        return strip_url(\n            url,\n            strip_credentials=True,\n            strip_fragment=True,\n            strip_default_port=True,\n            origin_only=origin_only,\n        )\n\n    def origin(self, url: str) -> Optional[str]:\n        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n        return self.strip_url(url, origin_only=True)\n\n    def potentially_trustworthy(self, url: str) -> bool:\n        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in (\"data\",):\n            return False\n        return self.tls_protected(url)\n\n    def tls_protected(self, url: str) -> bool:\n        return urlparse(url).scheme in (\"https\", \"ftps\")\n\n\nclass NoReferrerPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer\n\n    The simplest policy is \"no-referrer\", which specifies that no referrer information\n    is to be sent along with requests made from a particular request client to any origin.\n    The header will be omitted entirely.\n    \"\"\"\n\n    name: str = POLICY_NO_REFERRER\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        return None\n\n\nclass NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n\n    The \"no-referrer-when-downgrade\" policy sends a full URL along with requests\n    from a TLS-protected environment settings object to a potentially trustworthy URL,\n    and requests from clients which are not TLS-protected to any origin.\n\n    Requests from TLS-protected clients to non-potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n\n    This is a user agent's default behavior, if no policy is otherwise specified.\n    \"\"\"\n\n    name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n            return self.stripped_referrer(response_url)\n        return None\n\n\nclass SameOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin\n\n    The \"same-origin\" policy specifies that a full URL, stripped for use as a referrer,\n    is sent as referrer information when making same-origin requests from a particular request client.\n\n    Cross-origin requests, on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_SAME_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        if self.origin(response_url) == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        return None\n\n\nclass OriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-origin\n\n    The \"origin\" policy specifies that only the ASCII serialization\n    of the origin of the request client is sent as referrer information\n    when making both same-origin requests and cross-origin requests\n    from a particular request client.\n    \"\"\"\n\n    name: str = POLICY_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        return self.origin_referrer(response_url)\n\n\nclass StrictOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin\n\n    The \"strict-origin\" policy sends the ASCII serialization\n    of the origin of the request client when making requests:\n    - from a TLS-protected environment settings object to a potentially trustworthy URL, and\n    - from non-TLS-protected environment settings objects to any origin.\n\n    Requests from TLS-protected request clients to non- potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_STRICT_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        if (\n            self.tls_protected(response_url)\n            and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)\n        return None\n\n\nclass OriginWhenCrossOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin\n\n    The \"origin-when-cross-origin\" policy specifies that a full URL,\n    stripped for use as a referrer, is sent as referrer information\n    when making same-origin requests from a particular request client,\n    and only the ASCII serialization of the origin of the request client\n    is sent as referrer information when making cross-origin requests\n    from a particular request client.\n    \"\"\"\n\n    name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        return origin\n\n\nclass StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin\n\n    The \"strict-origin-when-cross-origin\" policy specifies that a full URL,\n    stripped for use as a referrer, is sent as referrer information\n    when making same-origin requests from a particular request client,\n    and only the ASCII serialization of the origin of the request client\n    when making cross-origin requests:\n\n    - from a TLS-protected environment settings object to a potentially trustworthy URL, and\n    - from non-TLS-protected environment settings objects to any origin.\n\n    Requests from TLS-protected clients to non- potentially trustworthy URLs,\n    on the other hand, will contain no referrer information.\n    A Referer HTTP header will not be sent.\n    \"\"\"\n\n    name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        if (\n            self.tls_protected(response_url)\n            and self.potentially_trustworthy(request_url)\n            or not self.tls_protected(response_url)\n        ):\n            return self.origin_referrer(response_url)\n        return None\n\n\nclass UnsafeUrlPolicy(ReferrerPolicy):\n    \"\"\"\n    https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url\n\n    The \"unsafe-url\" policy specifies that a full URL, stripped for use as a referrer,\n    is sent along with both cross-origin requests\n    and same-origin requests made from a particular request client.\n\n    Note: The policy's name doesn't lie; it is unsafe.\n    This policy will leak origins and paths from TLS-protected resources\n    to insecure origins.\n    Carefully consider the impact of setting such a policy for potentially sensitive documents.\n    \"\"\"\n\n    name: str = POLICY_UNSAFE_URL\n\n    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n        return self.stripped_referrer(response_url)\n\n\nclass DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n    \"\"\"\n    A variant of \"no-referrer-when-downgrade\",\n    with the addition that \"Referer\" is not sent if the parent request was\n    using ``file://`` or ``s3://`` scheme.\n    \"\"\"\n\n    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + (\"file\", \"s3\")\n    name: str = POLICY_SCRAPY_DEFAULT\n\n\n_policy_classes: Dict[str, Type[ReferrerPolicy]] = {\n    p.name: p\n    for p in (\n        NoReferrerPolicy,\n        NoReferrerWhenDowngradePolicy,\n        SameOriginPolicy,\n        OriginPolicy,\n        StrictOriginPolicy,\n        OriginWhenCrossOriginPolicy,\n        StrictOriginWhenCrossOriginPolicy,\n        UnsafeUrlPolicy,\n        DefaultReferrerPolicy,\n    )\n}\n\n# Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string\n_policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n\n\ndef _load_policy_class(\n    policy: str, warning_only: bool = False\n) -> Optional[Type[ReferrerPolicy]]:\n    \"\"\"\n    Expect a string for the path to the policy class,\n    otherwise try to interpret the string as a standard value\n    from https://www.w3.org/TR/referrer-policy/#referrer-policies\n    \"\"\"\n    try:\n        return cast(Type[ReferrerPolicy], load_object(policy))\n    except ValueError:\n        tokens = [token.strip() for token in policy.lower().split(\",\")]\n        # https://www.w3.org/TR/referrer-policy/#parse-referrer-policy-from-header\n        for token in tokens[::-1]:\n            if token in _policy_classes:\n                return _policy_classes[token]\n\n        msg = f\"Could not load referrer policy {policy!r}\"\n        if not warning_only:\n            raise RuntimeError(msg)\n        else:\n            warnings.warn(msg, RuntimeWarning)\n            return None\n\n\nclass RefererMiddleware:\n    def __init__(self, settings: Optional[BaseSettings] = None):\n        self.default_policy: Type[ReferrerPolicy] = DefaultReferrerPolicy\n        if settings is not None:\n            settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n            assert settings_policy\n            self.default_policy = settings_policy\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"REFERER_ENABLED\"):\n            raise NotConfigured\n        mw = cls(crawler.settings)\n\n        # Note: this hook is a bit of a hack to intercept redirections\n        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n\n        return mw\n\n    def policy(\n        self, resp_or_url: Union[Response, str], request: Request\n    ) -> ReferrerPolicy:\n        \"\"\"\n        Determine Referrer-Policy to use from a parent Response (or URL),\n        and a Request to be sent.\n\n        - if a valid policy is set in Request meta, it is used.\n        - if the policy is set in meta but is wrong (e.g. a typo error),\n          the policy from settings is used\n        - if the policy is not set in Request meta,\n          but there is a Referrer-policy header in the parent response,\n          it is used if valid\n        - otherwise, the policy from settings is used.\n        \"\"\"\n        policy_name = request.meta.get(\"referrer_policy\")\n        if policy_name is None:\n            if isinstance(resp_or_url, Response):\n                policy_header = resp_or_url.headers.get(\"Referrer-Policy\")\n                if policy_header is not None:\n                    policy_name = to_unicode(policy_header.decode(\"latin1\"))\n        if policy_name is None:\n            return self.default_policy()\n\n        cls = _load_policy_class(policy_name, warning_only=True)\n        return cls() if cls else self.default_policy()\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        return (self._set_referer(r, response) for r in result)\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        async for r in result:\n            yield self._set_referer(r, response)\n\n    def _set_referer(self, r: Any, response: Response) -> Any:\n        if isinstance(r, Request):\n            referrer = self.policy(response, r).referrer(response.url, r.url)\n            if referrer is not None:\n                r.headers.setdefault(\"Referer\", referrer)\n        return r\n\n    def request_scheduled(self, request: Request, spider: Spider) -> None:\n        # check redirected request to patch \"Referer\" header if necessary\n        redirected_urls = request.meta.get(\"redirect_urls\", [])\n        if redirected_urls:\n            request_referrer = request.headers.get(\"Referer\")\n            # we don't patch the referrer value if there is none\n            if request_referrer is not None:\n                # the request's referrer header value acts as a surrogate\n                # for the parent response URL\n                #\n                # Note: if the 3xx response contained a Referrer-Policy header,\n                #       the information is not available using this hook\n                parent_url = safe_url_string(request_referrer)\n                policy_referrer = self.policy(parent_url, request).referrer(\n                    parent_url, request.url\n                )\n                if policy_referrer != request_referrer.decode(\"latin1\"):\n                    if policy_referrer is None:\n                        request.headers.pop(\"Referer\")\n                    else:\n                        request.headers[\"Referer\"] = policy_referrer\n", "scrapy/spidermiddlewares/__init__.py": "", "scrapy/spidermiddlewares/urllength.py": "\"\"\"\nUrl Length Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n\nfrom scrapy import Spider\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import BaseSettings\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass UrlLengthMiddleware:\n    def __init__(self, maxlength: int):\n        self.maxlength: int = maxlength\n\n    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n        if not maxlength:\n            raise NotConfigured\n        return cls(maxlength)\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        return (r for r in result if self._filter(r, spider))\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        async for r in result:\n            if self._filter(r, spider):\n                yield r\n\n    def _filter(self, request: Any, spider: Spider) -> bool:\n        if isinstance(request, Request) and len(request.url) > self.maxlength:\n            logger.info(\n                \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                {\"maxlength\": self.maxlength, \"url\": request.url},\n                extra={\"spider\": spider},\n            )\n            assert spider.crawler.stats\n            spider.crawler.stats.inc_value(\n                \"urllength/request_ignored_count\", spider=spider\n            )\n            return False\n        return True\n", "scrapy/spidermiddlewares/depth.py": "\"\"\"\nDepth Spider Middleware\n\nSee documentation in docs/topics/spider-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Request, Response\nfrom scrapy.statscollectors import StatsCollector\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass DepthMiddleware:\n    def __init__(\n        self,\n        maxdepth: int,\n        stats: StatsCollector,\n        verbose_stats: bool = False,\n        prio: int = 1,\n    ):\n        self.maxdepth = maxdepth\n        self.stats = stats\n        self.verbose_stats = verbose_stats\n        self.prio = prio\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        settings = crawler.settings\n        maxdepth = settings.getint(\"DEPTH_LIMIT\")\n        verbose = settings.getbool(\"DEPTH_STATS_VERBOSE\")\n        prio = settings.getint(\"DEPTH_PRIORITY\")\n        assert crawler.stats\n        return cls(maxdepth, crawler.stats, verbose, prio)\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        self._init_depth(response, spider)\n        return (r for r in result if self._filter(r, response, spider))\n\n    async def process_spider_output_async(\n        self, response: Response, result: AsyncIterable[Any], spider: Spider\n    ) -> AsyncIterable[Any]:\n        self._init_depth(response, spider)\n        async for r in result:\n            if self._filter(r, response, spider):\n                yield r\n\n    def _init_depth(self, response: Response, spider: Spider) -> None:\n        # base case (depth=0)\n        if \"depth\" not in response.meta:\n            response.meta[\"depth\"] = 0\n            if self.verbose_stats:\n                self.stats.inc_value(\"request_depth_count/0\", spider=spider)\n\n    def _filter(self, request: Any, response: Response, spider: Spider) -> bool:\n        if not isinstance(request, Request):\n            return True\n        depth = response.meta[\"depth\"] + 1\n        request.meta[\"depth\"] = depth\n        if self.prio:\n            request.priority -= depth * self.prio\n        if self.maxdepth and depth > self.maxdepth:\n            logger.debug(\n                \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                {\"maxdepth\": self.maxdepth, \"requrl\": request.url},\n                extra={\"spider\": spider},\n            )\n            return False\n        if self.verbose_stats:\n            self.stats.inc_value(f\"request_depth_count/{depth}\", spider=spider)\n        self.stats.max_value(\"request_depth_max\", depth, spider=spider)\n        return True\n", "scrapy/linkextractors/__init__.py": "\"\"\"\nscrapy.linkextractors\n\nThis package contains a collection of Link Extractors.\n\nFor more info see docs/topics/link-extractors.rst\n\"\"\"\n\nimport re\nfrom typing import Iterable, Pattern\n\n# common file extensions that are not followed if they occur in links\nIGNORED_EXTENSIONS = [\n    # archives\n    \"7z\",\n    \"7zip\",\n    \"bz2\",\n    \"rar\",\n    \"tar\",\n    \"tar.gz\",\n    \"xz\",\n    \"zip\",\n    # images\n    \"mng\",\n    \"pct\",\n    \"bmp\",\n    \"gif\",\n    \"jpg\",\n    \"jpeg\",\n    \"png\",\n    \"pst\",\n    \"psp\",\n    \"tif\",\n    \"tiff\",\n    \"ai\",\n    \"drw\",\n    \"dxf\",\n    \"eps\",\n    \"ps\",\n    \"svg\",\n    \"cdr\",\n    \"ico\",\n    \"webp\",\n    # audio\n    \"mp3\",\n    \"wma\",\n    \"ogg\",\n    \"wav\",\n    \"ra\",\n    \"aac\",\n    \"mid\",\n    \"au\",\n    \"aiff\",\n    # video\n    \"3gp\",\n    \"asf\",\n    \"asx\",\n    \"avi\",\n    \"mov\",\n    \"mp4\",\n    \"mpg\",\n    \"qt\",\n    \"rm\",\n    \"swf\",\n    \"wmv\",\n    \"m4a\",\n    \"m4v\",\n    \"flv\",\n    \"webm\",\n    # office suites\n    \"xls\",\n    \"xlsm\",\n    \"xlsx\",\n    \"xltm\",\n    \"xltx\",\n    \"potm\",\n    \"potx\",\n    \"ppt\",\n    \"pptm\",\n    \"pptx\",\n    \"pps\",\n    \"doc\",\n    \"docb\",\n    \"docm\",\n    \"docx\",\n    \"dotm\",\n    \"dotx\",\n    \"odt\",\n    \"ods\",\n    \"odg\",\n    \"odp\",\n    # other\n    \"css\",\n    \"pdf\",\n    \"exe\",\n    \"bin\",\n    \"rss\",\n    \"dmg\",\n    \"iso\",\n    \"apk\",\n    \"jar\",\n    \"sh\",\n    \"rb\",\n    \"js\",\n    \"hta\",\n    \"bat\",\n    \"cpl\",\n    \"msi\",\n    \"msp\",\n    \"py\",\n]\n\n\ndef _matches(url: str, regexs: Iterable[Pattern[str]]) -> bool:\n    return any(r.search(url) for r in regexs)\n\n\ndef _is_valid_url(url: str) -> bool:\n    return url.split(\"://\", 1)[0] in {\"http\", \"https\", \"file\", \"ftp\"}\n\n\n# Top-level imports\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n", "scrapy/linkextractors/lxmlhtml.py": "\"\"\"\nLink extractor based on lxml.html\n\"\"\"\n\nimport logging\nimport operator\nfrom functools import partial\nfrom typing import (\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import urljoin, urlparse\n\nfrom lxml import etree  # nosec\nfrom lxml.html import HtmlElement  # nosec\nfrom parsel.csstranslator import HTMLTranslator\nfrom w3lib.html import strip_html5_whitespace\nfrom w3lib.url import canonicalize_url, safe_url_string\n\nfrom scrapy import Selector\nfrom scrapy.http import TextResponse\nfrom scrapy.link import Link\nfrom scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches, re\nfrom scrapy.utils.misc import arg_to_iter, rel_has_nofollow\nfrom scrapy.utils.python import unique as unique_list\nfrom scrapy.utils.response import get_base_url\nfrom scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n\nlogger = logging.getLogger(__name__)\n\n# from lxml/src/lxml/html/__init__.py\nXHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n\n_collect_string_content = etree.XPath(\"string()\")\n\n\ndef _nons(tag: Any) -> Any:\n    if isinstance(tag, str):\n        if tag[0] == \"{\" and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n            return tag.split(\"}\")[-1]\n    return tag\n\n\ndef _identity(x: Any) -> Any:\n    return x\n\n\ndef _canonicalize_link_url(link: Link) -> str:\n    return canonicalize_url(link.url, keep_fragments=True)\n\n\nclass LxmlParserLinkExtractor:\n    def __init__(\n        self,\n        tag: Union[str, Callable[[str], bool]] = \"a\",\n        attr: Union[str, Callable[[str], bool]] = \"href\",\n        process: Optional[Callable[[Any], Any]] = None,\n        unique: bool = False,\n        strip: bool = True,\n        canonicalized: bool = False,\n    ):\n        # mypy doesn't infer types for operator.* and also for partial()\n        self.scan_tag: Callable[[str], bool] = (\n            tag\n            if callable(tag)\n            else cast(Callable[[str], bool], partial(operator.eq, tag))\n        )\n        self.scan_attr: Callable[[str], bool] = (\n            attr\n            if callable(attr)\n            else cast(Callable[[str], bool], partial(operator.eq, attr))\n        )\n        self.process_attr: Callable[[Any], Any] = (\n            process if callable(process) else _identity\n        )\n        self.unique: bool = unique\n        self.strip: bool = strip\n        self.link_key: Callable[[Link], str] = (\n            cast(Callable[[Link], str], operator.attrgetter(\"url\"))\n            if canonicalized\n            else _canonicalize_link_url\n        )\n\n    def _iter_links(\n        self, document: HtmlElement\n    ) -> Iterable[Tuple[HtmlElement, str, str]]:\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield el, attrib, attribs[attrib]\n\n    def _extract_links(\n        self,\n        selector: Selector,\n        response_url: str,\n        response_encoding: str,\n        base_url: str,\n    ) -> List[Link]:\n        links: List[Link] = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                if self.strip:\n                    attr_val = strip_html5_whitespace(attr_val)\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue  # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            try:\n                url = safe_url_string(url, encoding=response_encoding)\n            except ValueError:\n                logger.debug(f\"Skipping extraction of link with bad URL {url!r}\")\n                continue\n\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(\n                url,\n                _collect_string_content(el) or \"\",\n                nofollow=rel_has_nofollow(el.get(\"rel\")),\n            )\n            links.append(link)\n        return self._deduplicate_if_needed(links)\n\n    def extract_links(self, response: TextResponse) -> List[Link]:\n        base_url = get_base_url(response)\n        return self._extract_links(\n            response.selector, response.url, response.encoding, base_url\n        )\n\n    def _process_links(self, links: List[Link]) -> List[Link]:\n        \"\"\"Normalize and filter extracted links\n\n        The subclass should override it if necessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)\n\n    def _deduplicate_if_needed(self, links: List[Link]) -> List[Link]:\n        if self.unique:\n            return unique_list(links, key=self.link_key)\n        return links\n\n\n_RegexT = Union[str, Pattern[str]]\n_RegexOrSeveralT = Union[_RegexT, Iterable[_RegexT]]\n\n\nclass LxmlLinkExtractor:\n    _csstranslator = HTMLTranslator()\n\n    def __init__(\n        self,\n        allow: _RegexOrSeveralT = (),\n        deny: _RegexOrSeveralT = (),\n        allow_domains: Union[str, Iterable[str]] = (),\n        deny_domains: Union[str, Iterable[str]] = (),\n        restrict_xpaths: Union[str, Iterable[str]] = (),\n        tags: Union[str, Iterable[str]] = (\"a\", \"area\"),\n        attrs: Union[str, Iterable[str]] = (\"href\",),\n        canonicalize: bool = False,\n        unique: bool = True,\n        process_value: Optional[Callable[[Any], Any]] = None,\n        deny_extensions: Union[str, Iterable[str], None] = None,\n        restrict_css: Union[str, Iterable[str]] = (),\n        strip: bool = True,\n        restrict_text: Optional[_RegexOrSeveralT] = None,\n    ):\n        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n        self.link_extractor = LxmlParserLinkExtractor(\n            tag=partial(operator.contains, tags),\n            attr=partial(operator.contains, attrs),\n            unique=unique,\n            process=process_value,\n            strip=strip,\n            canonicalized=not canonicalize,\n        )\n        self.allow_res: List[Pattern[str]] = self._compile_regexes(allow)\n        self.deny_res: List[Pattern[str]] = self._compile_regexes(deny)\n\n        self.allow_domains: Set[str] = set(arg_to_iter(allow_domains))\n        self.deny_domains: Set[str] = set(arg_to_iter(deny_domains))\n\n        self.restrict_xpaths: Tuple[str, ...] = tuple(arg_to_iter(restrict_xpaths))\n        self.restrict_xpaths += tuple(\n            map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n        )\n\n        if deny_extensions is None:\n            deny_extensions = IGNORED_EXTENSIONS\n        self.canonicalize: bool = canonicalize\n        self.deny_extensions: Set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n        self.restrict_text: List[Pattern[str]] = self._compile_regexes(restrict_text)\n\n    @staticmethod\n    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> List[Pattern[str]]:\n        return [\n            x if isinstance(x, re.Pattern) else re.compile(x)\n            for x in arg_to_iter(value)\n        ]\n\n    def _link_allowed(self, link: Link) -> bool:\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(\n            parsed_url, self.allow_domains\n        ):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(\n            parsed_url, self.deny_extensions\n        ):\n            return False\n        if self.restrict_text and not _matches(link.text, self.restrict_text):\n            return False\n        return True\n\n    def matches(self, url: str) -> bool:\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (\n            (regex.search(url) for regex in self.allow_res)\n            if self.allow_res\n            else [True]\n        )\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)\n\n    def _process_links(self, links: List[Link]) -> List[Link]:\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(link.url)\n        links = self.link_extractor._process_links(links)\n        return links\n\n    def _extract_links(self, *args: Any, **kwargs: Any) -> List[Link]:\n        return self.link_extractor._extract_links(*args, **kwargs)\n\n    def extract_links(self, response: TextResponse) -> List[Link]:\n        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n        specified :class:`response <scrapy.http.Response>`.\n\n        Only links that match the settings passed to the ``__init__`` method of\n        the link extractor are returned.\n\n        Duplicate links are omitted if the ``unique`` attribute is set to ``True``,\n        otherwise they are returned.\n        \"\"\"\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [\n                subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)\n            ]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        if self.link_extractor.unique:\n            return unique_list(all_links, key=self.link_extractor.link_key)\n        return all_links\n", "scrapy/downloadermiddlewares/ajaxcrawl.py": "from __future__ import annotations\n\nimport logging\nimport re\nfrom typing import TYPE_CHECKING, Union\n\nfrom w3lib import html\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.settings import BaseSettings\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass AjaxCrawlMiddleware:\n    \"\"\"\n    Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n    For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(\"AJAXCRAWL_ENABLED\"):\n            raise NotConfigured\n\n        # XXX: Google parses at least first 100k bytes; scrapy's redirect\n        # middleware parses first 4k. 4k turns out to be insufficient\n        # for this middleware, and parsing 100k could be slow.\n        # We use something in between (32K) by default.\n        self.lookup_bytes: int = settings.getint(\"AJAXCRAWL_MAXSIZE\", 32768)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != \"GET\":\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if \"ajax_crawlable\" in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        # scrapy already handles #! links properly\n        ajax_crawl_request = request.replace(url=request.url + \"#!\")\n        logger.debug(\n            \"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n            {\"ajax_crawl_request\": ajax_crawl_request, \"request\": request},\n            extra={\"spider\": spider},\n        )\n\n        ajax_crawl_request.meta[\"ajax_crawlable\"] = True\n        return ajax_crawl_request\n\n    def _has_ajax_crawlable_variant(self, response: Response) -> bool:\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\"\n        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n        \"\"\"\n        body = response.text[: self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)\n\n\n# XXX: move it to w3lib?\n_ajax_crawlable_re: re.Pattern[str] = re.compile(\n    r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'\n)\n\n\ndef _has_ajaxcrawlable_meta(text: str) -> bool:\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if \"fragment\" not in text:\n        return False\n    if \"content\" not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, (\"script\", \"noscript\"))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None\n", "scrapy/downloadermiddlewares/defaultheaders.py": "\"\"\"\nDefaultHeaders downloader middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Iterable, Tuple, Union\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass DefaultHeadersMiddleware:\n    def __init__(self, headers: Iterable[Tuple[str, str]]):\n        self._headers: Iterable[Tuple[str, str]] = headers\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        headers = without_none_values(crawler.settings[\"DEFAULT_REQUEST_HEADERS\"])\n        return cls(headers.items())\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)\n        return None\n", "scrapy/downloadermiddlewares/retry.py": "\"\"\"\nAn extension to retry failed requests that are potentially caused by temporary\nproblems such as a connection timeout or HTTP 500 error.\n\nYou can change the behaviour of this middleware by modifying the scraping settings:\nRETRY_TIMES - how many times to retry a failed page\nRETRY_HTTP_CODES - which HTTP response codes to retry\n\nFailed pages are collected on the scraping process and rescheduled at the end,\nonce the spider has finished crawling all regular (non failed) pages.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom logging import Logger, getLogger\nfrom typing import TYPE_CHECKING, Any, Optional, Tuple, Type, Union\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import Response\nfrom scrapy.http.request import Request\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.python import global_object_name\nfrom scrapy.utils.response import response_status_message\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nretry_logger = getLogger(__name__)\n\n\ndef backwards_compatibility_getattr(self: Any, name: str) -> Tuple[Any, ...]:\n    if name == \"EXCEPTIONS_TO_RETRY\":\n        warnings.warn(\n            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n            \"Use the RETRY_EXCEPTIONS setting instead.\",\n            ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return tuple(\n            load_object(x) if isinstance(x, str) else x\n            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n        )\n    raise AttributeError(\n        f\"{self.__class__.__name__!r} object has no attribute {name!r}\"\n    )\n\n\nclass BackwardsCompatibilityMetaclass(type):\n    __getattr__ = backwards_compatibility_getattr\n\n\ndef get_retry_request(\n    request: Request,\n    *,\n    spider: Spider,\n    reason: Union[str, Exception, Type[Exception]] = \"unspecified\",\n    max_retry_times: Optional[int] = None,\n    priority_adjust: Optional[int] = None,\n    logger: Logger = retry_logger,\n    stats_base_key: str = \"retry\",\n) -> Optional[Request]:\n    \"\"\"\n    Returns a new :class:`~scrapy.Request` object to retry the specified\n    request, or ``None`` if retries of the specified request have been\n    exhausted.\n\n    For example, in a :class:`~scrapy.Spider` callback, you could use it as\n    follows::\n\n        def parse(self, response):\n            if not response.text:\n                new_request_or_none = get_retry_request(\n                    response.request,\n                    spider=self,\n                    reason='empty',\n                )\n                return new_request_or_none\n\n    *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n    retry request. It is used to access the :ref:`settings <topics-settings>`\n    and :ref:`stats <topics-stats>`, and to provide extra logging context (see\n    :func:`logging.debug`).\n\n    *reason* is a string or an :class:`Exception` object that indicates the\n    reason why the request needs to be retried. It is used to name retry stats.\n\n    *max_retry_times* is a number that determines the maximum number of times\n    that *request* can be retried. If not specified or ``None``, the number is\n    read from the :reqmeta:`max_retry_times` meta key of the request. If the\n    :reqmeta:`max_retry_times` meta key is not defined or ``None``, the number\n    is read from the :setting:`RETRY_TIMES` setting.\n\n    *priority_adjust* is a number that determines how the priority of the new\n    request changes in relation to *request*. If not specified, the number is\n    read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n\n    *logger* is the logging.Logger object to be used when logging messages\n\n    *stats_base_key* is a string to be used as the base key for the\n    retry-related job stats\n    \"\"\"\n    settings = spider.crawler.settings\n    assert spider.crawler.stats\n    stats = spider.crawler.stats\n    retry_times = request.meta.get(\"retry_times\", 0) + 1\n    if max_retry_times is None:\n        max_retry_times = request.meta.get(\"max_retry_times\")\n        if max_retry_times is None:\n            max_retry_times = settings.getint(\"RETRY_TIMES\")\n    if retry_times <= max_retry_times:\n        logger.debug(\n            \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n            {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n            extra={\"spider\": spider},\n        )\n        new_request: Request = request.copy()\n        new_request.meta[\"retry_times\"] = retry_times\n        new_request.dont_filter = True\n        if priority_adjust is None:\n            priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n        new_request.priority = request.priority + priority_adjust\n\n        if callable(reason):\n            reason = reason()\n        if isinstance(reason, Exception):\n            reason = global_object_name(reason.__class__)\n\n        stats.inc_value(f\"{stats_base_key}/count\")\n        stats.inc_value(f\"{stats_base_key}/reason_count/{reason}\")\n        return new_request\n    stats.inc_value(f\"{stats_base_key}/max_reached\")\n    logger.error(\n        \"Gave up retrying %(request)s (failed %(retry_times)d times): \" \"%(reason)s\",\n        {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n        extra={\"spider\": spider},\n    )\n    return None\n\n\nclass RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(\"RETRY_ENABLED\"):\n            raise NotConfigured\n        self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n        self.retry_http_codes = {int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")}\n        self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n\n        try:\n            self.exceptions_to_retry = self.__getattribute__(\"EXCEPTIONS_TO_RETRY\")\n        except AttributeError:\n            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n            self.exceptions_to_retry = tuple(\n                load_object(x) if isinstance(x, str) else x\n                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n            )\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.meta.get(\"dont_retry\", False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason, spider) or response\n        return response\n\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n            \"dont_retry\", False\n        ):\n            return self._retry(request, exception, spider)\n        return None\n\n    def _retry(\n        self,\n        request: Request,\n        reason: Union[str, Exception, Type[Exception]],\n        spider: Spider,\n    ) -> Optional[Request]:\n        max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n        priority_adjust = request.meta.get(\"priority_adjust\", self.priority_adjust)\n        return get_retry_request(\n            request,\n            reason=reason,\n            spider=spider,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )\n\n    __getattr__ = backwards_compatibility_getattr\n", "scrapy/downloadermiddlewares/httpcache.py": "from __future__ import annotations\n\nfrom email.utils import formatdate\nfrom typing import TYPE_CHECKING, Optional, Union\n\nfrom twisted.internet import defer\nfrom twisted.internet.error import (\n    ConnectError,\n    ConnectionDone,\n    ConnectionLost,\n    ConnectionRefusedError,\n    DNSLookupError,\n    TCPTimedOutError,\n    TimeoutError,\n)\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http.request import Request\nfrom scrapy.http.response import Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass HttpCacheMiddleware:\n    DOWNLOAD_EXCEPTIONS = (\n        defer.TimeoutError,\n        TimeoutError,\n        DNSLookupError,\n        ConnectionRefusedError,\n        ConnectionDone,\n        ConnectError,\n        ConnectionLost,\n        TCPTimedOutError,\n        ResponseFailed,\n        OSError,\n    )\n\n    def __init__(self, settings: Settings, stats: StatsCollector) -> None:\n        if not settings.getbool(\"HTTPCACHE_ENABLED\"):\n            raise NotConfigured\n        self.policy = load_object(settings[\"HTTPCACHE_POLICY\"])(settings)\n        self.storage = load_object(settings[\"HTTPCACHE_STORAGE\"])(settings)\n        self.ignore_missing = settings.getbool(\"HTTPCACHE_IGNORE_MISSING\")\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.storage.open_spider(spider)\n\n    def spider_closed(self, spider: Spider) -> None:\n        self.storage.close_spider(spider)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if request.meta.get(\"dont_cache\", False):\n            return None\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta[\"_dont_cache\"] = True  # flag as uncacheable\n            return None\n\n        # Look for cached response and check if expired\n        cachedresponse: Optional[Response] = self.storage.retrieve_response(\n            spider, request\n        )\n        if cachedresponse is None:\n            self.stats.inc_value(\"httpcache/miss\", spider=spider)\n            if self.ignore_missing:\n                self.stats.inc_value(\"httpcache/ignore\", spider=spider)\n                raise IgnoreRequest(f\"Ignored request not in cache: {request}\")\n            return None  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append(\"cached\")\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value(\"httpcache/hit\", spider=spider)\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta[\"cached_response\"] = cachedresponse\n\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.meta.get(\"dont_cache\", False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if \"cached\" in response.flags or \"_dont_cache\" in request.meta:\n            request.meta.pop(\"_dont_cache\", None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if \"Date\" not in response.headers:\n            response.headers[\"Date\"] = formatdate(usegmt=True)\n\n        # Do not validate first-hand responses\n        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n        if cachedresponse is None:\n            self.stats.inc_value(\"httpcache/firsthand\", spider=spider)\n            self._cache_response(spider, response, request, cachedresponse)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value(\"httpcache/revalidate\", spider=spider)\n            return cachedresponse\n\n        self.stats.inc_value(\"httpcache/invalidate\", spider=spider)\n        self._cache_response(spider, response, request, cachedresponse)\n        return response\n\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Union[Request, Response, None]:\n        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n        if cachedresponse is not None and isinstance(\n            exception, self.DOWNLOAD_EXCEPTIONS\n        ):\n            self.stats.inc_value(\"httpcache/errorrecovery\", spider=spider)\n            return cachedresponse\n        return None\n\n    def _cache_response(\n        self,\n        spider: Spider,\n        response: Response,\n        request: Request,\n        cachedresponse: Optional[Response],\n    ) -> None:\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value(\"httpcache/store\", spider=spider)\n            self.storage.store_response(spider, request, response)\n        else:\n            self.stats.inc_value(\"httpcache/uncacheable\", spider=spider)\n", "scrapy/downloadermiddlewares/offsite.py": "from __future__ import annotations\n\nimport logging\nimport re\nimport warnings\nfrom typing import TYPE_CHECKING, Set\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.httpobj import urlparse_cached\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffsiteMiddleware:\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        assert crawler.stats\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n        return o\n\n    def __init__(self, stats: StatsCollector):\n        self.stats = stats\n        self.domains_seen: Set[str] = set()\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n\n    def request_scheduled(self, request: Request, spider: Spider) -> None:\n        self.process_request(request, spider)\n\n    def process_request(self, request: Request, spider: Spider) -> None:\n        if request.dont_filter or self.should_follow(request, spider):\n            return None\n        domain = urlparse_cached(request).hostname\n        if domain and domain not in self.domains_seen:\n            self.domains_seen.add(domain)\n            logger.debug(\n                \"Filtered offsite request to %(domain)r: %(request)s\",\n                {\"domain\": domain, \"request\": request},\n                extra={\"spider\": spider},\n            )\n            self.stats.inc_value(\"offsite/domains\", spider=spider)\n        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n        raise IgnoreRequest\n\n    def should_follow(self, request: Request, spider: Spider) -> bool:\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or \"\"\n        return bool(regex.search(host))\n\n    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, \"allowed_domains\", None)\n        if not allowed_domains:\n            return re.compile(\"\")  # allow all by default\n        url_pattern = re.compile(r\"^https?://.*$\")\n        port_pattern = re.compile(r\":\\d+$\")\n        domains = []\n        for domain in allowed_domains:\n            if domain is None:\n                continue\n            if url_pattern.match(domain):\n                message = (\n                    \"allowed_domains accepts only domains, not URLs. \"\n                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message)\n            elif port_pattern.search(domain):\n                message = (\n                    \"allowed_domains accepts only domains without ports. \"\n                    f\"Ignoring entry {domain} in allowed_domains.\"\n                )\n                warnings.warn(message)\n            else:\n                domains.append(re.escape(domain))\n        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n        return re.compile(regex)\n", "scrapy/downloadermiddlewares/httpcompression.py": "from __future__ import annotations\n\nimport warnings\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Response, TextResponse\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils._compression import (\n    _DecompressionMaxSizeExceeded,\n    _inflate,\n    _unbrotli,\n    _unzstd,\n)\nfrom scrapy.utils.deprecate import ScrapyDeprecationWarning\nfrom scrapy.utils.gz import gunzip\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = getLogger(__name__)\n\nACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n\ntry:\n    try:\n        import brotli  # noqa: F401\n    except ImportError:\n        import brotlicffi  # noqa: F401\nexcept ImportError:\n    pass\nelse:\n    ACCEPTED_ENCODINGS.append(b\"br\")\n\ntry:\n    import zstandard  # noqa: F401\nexcept ImportError:\n    pass\nelse:\n    ACCEPTED_ENCODINGS.append(b\"zstd\")\n\n\nclass HttpCompressionMiddleware:\n    \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n    sent/received from web sites\"\"\"\n\n    def __init__(\n        self,\n        stats: Optional[StatsCollector] = None,\n        *,\n        crawler: Optional[Crawler] = None,\n    ):\n        if not crawler:\n            self.stats = stats\n            self._max_size = 1073741824\n            self._warn_size = 33554432\n            return\n        self.stats = crawler.stats\n        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n        crawler.signals.connect(self.open_spider, signals.spider_opened)\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n            raise NotConfigured\n        try:\n            return cls(crawler=crawler)\n        except TypeError:\n            warnings.warn(\n                \"HttpCompressionMiddleware subclasses must either modify \"\n                \"their '__init__' method to support a 'crawler' parameter or \"\n                \"reimplement their 'from_crawler' method.\",\n                ScrapyDeprecationWarning,\n            )\n            mw = cls()\n            mw.stats = crawler.stats\n            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n            crawler.signals.connect(mw.open_spider, signals.spider_opened)\n            return mw\n\n    def open_spider(self, spider):\n        if hasattr(spider, \"download_maxsize\"):\n            self._max_size = spider.download_maxsize\n        if hasattr(spider, \"download_warnsize\"):\n            self._warn_size = spider.download_warnsize\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.method == \"HEAD\":\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist(\"Content-Encoding\")\n            if content_encoding:\n                max_size = request.meta.get(\"download_maxsize\", self._max_size)\n                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\n                try:\n                    decoded_body, content_encoding = self._handle_encoding(\n                        response.body, content_encoding, max_size\n                    )\n                except _DecompressionMaxSizeExceeded:\n                    raise IgnoreRequest(\n                        f\"Ignored response {response} because its body \"\n                        f\"({len(response.body)} B compressed) exceeded \"\n                        f\"DOWNLOAD_MAXSIZE ({max_size} B) during \"\n                        f\"decompression.\"\n                    )\n                if len(response.body) < warn_size <= len(decoded_body):\n                    logger.warning(\n                        f\"{response} body size after decompression \"\n                        f\"({len(decoded_body)} B) is larger than the \"\n                        f\"download warning size ({warn_size} B).\"\n                    )\n                response.headers[\"Content-Encoding\"] = content_encoding\n                if self.stats:\n                    self.stats.inc_value(\n                        \"httpcompression/response_bytes\",\n                        len(decoded_body),\n                        spider=spider,\n                    )\n                    self.stats.inc_value(\n                        \"httpcompression/response_count\", spider=spider\n                    )\n                respcls = responsetypes.from_args(\n                    headers=response.headers, url=response.url, body=decoded_body\n                )\n                kwargs: Dict[str, Any] = {\"body\": decoded_body}\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs[\"encoding\"] = None\n                response = response.replace(cls=respcls, **kwargs)\n                if not content_encoding:\n                    del response.headers[\"Content-Encoding\"]\n\n        return response\n\n    def _handle_encoding(\n        self, body: bytes, content_encoding: List[bytes], max_size: int\n    ) -> Tuple[bytes, List[bytes]]:\n        to_decode, to_keep = self._split_encodings(content_encoding)\n        for encoding in to_decode:\n            body = self._decode(body, encoding, max_size)\n        return body, to_keep\n\n    def _split_encodings(\n        self, content_encoding: List[bytes]\n    ) -> Tuple[List[bytes], List[bytes]]:\n        to_keep: List[bytes] = [\n            encoding.strip().lower()\n            for encoding in chain.from_iterable(\n                encodings.split(b\",\") for encodings in content_encoding\n            )\n        ]\n        to_decode: List[bytes] = []\n        while to_keep:\n            encoding = to_keep.pop()\n            if encoding not in ACCEPTED_ENCODINGS:\n                to_keep.append(encoding)\n                return to_decode, to_keep\n            to_decode.append(encoding)\n        return to_decode, to_keep\n\n    def _decode(self, body: bytes, encoding: bytes, max_size: int) -> bytes:\n        if encoding in {b\"gzip\", b\"x-gzip\"}:\n            return gunzip(body, max_size=max_size)\n        if encoding == b\"deflate\":\n            return _inflate(body, max_size=max_size)\n        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n            return _unbrotli(body, max_size=max_size)\n        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n            return _unzstd(body, max_size=max_size)\n        return body\n", "scrapy/downloadermiddlewares/downloadtimeout.py": "\"\"\"\nDownload timeout middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Union\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass DownloadTimeoutMiddleware:\n    def __init__(self, timeout: float = 180):\n        self._timeout: float = timeout\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings.getfloat(\"DOWNLOAD_TIMEOUT\"))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self._timeout = getattr(spider, \"download_timeout\", self._timeout)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if self._timeout:\n            request.meta.setdefault(\"download_timeout\", self._timeout)\n        return None\n", "scrapy/downloadermiddlewares/httpproxy.py": "from __future__ import annotations\n\nimport base64\nfrom typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\nfrom urllib.parse import unquote, urlunparse\nfrom urllib.request import (  # type: ignore[attr-defined]\n    _parse_proxy,\n    getproxies,\n    proxy_bypass,\n)\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass HttpProxyMiddleware:\n    def __init__(self, auth_encoding: Optional[str] = \"latin-1\"):\n        self.auth_encoding: Optional[str] = auth_encoding\n        self.proxies: Dict[str, Tuple[Optional[bytes], str]] = {}\n        for type_, url in getproxies().items():\n            try:\n                self.proxies[type_] = self._get_proxy(url, type_)\n            # some values such as '/var/run/docker.sock' can't be parsed\n            # by _parse_proxy and as such should be skipped\n            except ValueError:\n                continue\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n            raise NotConfigured\n        auth_encoding: Optional[str] = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n        return cls(auth_encoding)\n\n    def _basic_auth_header(self, username: str, password: str) -> bytes:\n        user_pass = to_bytes(\n            f\"{unquote(username)}:{unquote(password)}\", encoding=self.auth_encoding\n        )\n        return base64.b64encode(user_pass)\n\n    def _get_proxy(self, url: str, orig_type: str) -> Tuple[Optional[bytes], str]:\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n\n        if user:\n            creds = self._basic_auth_header(user, password)\n        else:\n            creds = None\n\n        return creds, proxy_url\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        creds, proxy_url, scheme = None, None, None\n        if \"proxy\" in request.meta:\n            if request.meta[\"proxy\"] is not None:\n                creds, proxy_url = self._get_proxy(request.meta[\"proxy\"], \"\")\n        elif self.proxies:\n            parsed = urlparse_cached(request)\n            _scheme = parsed.scheme\n            if (\n                # 'no_proxy' is only supported by http schemes\n                _scheme not in (\"http\", \"https\")\n                or (parsed.hostname and not proxy_bypass(parsed.hostname))\n            ) and _scheme in self.proxies:\n                scheme = _scheme\n                creds, proxy_url = self.proxies[scheme]\n\n        self._set_proxy_and_creds(request, proxy_url, creds, scheme)\n        return None\n\n    def _set_proxy_and_creds(\n        self,\n        request: Request,\n        proxy_url: Optional[str],\n        creds: Optional[bytes],\n        scheme: Optional[str],\n    ) -> None:\n        if scheme:\n            request.meta[\"_scheme_proxy\"] = True\n        if proxy_url:\n            request.meta[\"proxy\"] = proxy_url\n        elif request.meta.get(\"proxy\") is not None:\n            request.meta[\"proxy\"] = None\n        if creds:\n            request.headers[b\"Proxy-Authorization\"] = b\"Basic \" + creds\n            request.meta[\"_auth_proxy\"] = proxy_url\n        elif \"_auth_proxy\" in request.meta:\n            if proxy_url != request.meta[\"_auth_proxy\"]:\n                if b\"Proxy-Authorization\" in request.headers:\n                    del request.headers[b\"Proxy-Authorization\"]\n                del request.meta[\"_auth_proxy\"]\n        elif b\"Proxy-Authorization\" in request.headers:\n            if proxy_url:\n                request.meta[\"_auth_proxy\"] = proxy_url\n            else:\n                del request.headers[b\"Proxy-Authorization\"]\n", "scrapy/downloadermiddlewares/stats.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Dict, List, Tuple, Union\n\nfrom twisted.web import http\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.python import global_object_name, to_bytes\nfrom scrapy.utils.request import request_httprepr\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\ndef get_header_size(\n    headers: Dict[str, Union[List[Union[str, bytes]], Tuple[Union[str, bytes], ...]]]\n) -> int:\n    size = 0\n    for key, value in headers.items():\n        if isinstance(value, (list, tuple)):\n            for v in value:\n                size += len(b\": \") + len(key) + len(v)\n    return size + len(b\"\\r\\n\") * (len(headers.keys()) - 1)\n\n\ndef get_status_size(response_status: int) -> int:\n    return len(to_bytes(http.RESPONSES.get(response_status, b\"\"))) + 15\n    # resp.status + b\"\\r\\n\" + b\"HTTP/1.1 <100-599> \"\n\n\nclass DownloaderStats:\n    def __init__(self, stats: StatsCollector):\n        self.stats: StatsCollector = stats\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"DOWNLOADER_STATS\"):\n            raise NotConfigured\n        assert crawler.stats\n        return cls(crawler.stats)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        self.stats.inc_value(\"downloader/request_count\", spider=spider)\n        self.stats.inc_value(\n            f\"downloader/request_method_count/{request.method}\", spider=spider\n        )\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value(\"downloader/request_bytes\", reqlen, spider=spider)\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        self.stats.inc_value(\"downloader/response_count\", spider=spider)\n        self.stats.inc_value(\n            f\"downloader/response_status_count/{response.status}\", spider=spider\n        )\n        reslen = (\n            len(response.body)\n            + get_header_size(response.headers)\n            + get_status_size(response.status)\n            + 4\n        )\n        # response.body + b\"\\r\\n\"+ response.header + b\"\\r\\n\" + response.status\n        self.stats.inc_value(\"downloader/response_bytes\", reslen, spider=spider)\n        return response\n\n    def process_exception(\n        self, request: Request, exception: Exception, spider: Spider\n    ) -> Union[Request, Response, None]:\n        ex_class = global_object_name(exception.__class__)\n        self.stats.inc_value(\"downloader/exception_count\", spider=spider)\n        self.stats.inc_value(\n            f\"downloader/exception_type_count/{ex_class}\", spider=spider\n        )\n        return None\n", "scrapy/downloadermiddlewares/robotstxt.py": "\"\"\"\nThis is a middleware to respect robots.txt policies. To activate it you must\nenable this middleware and enable the ROBOTSTXT_OBEY setting.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Union\n\nfrom twisted.internet.defer import Deferred, maybeDeferred\nfrom twisted.python.failure import Failure\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.robotstxt import RobotParser\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass RobotsTxtMiddleware:\n    DOWNLOAD_PRIORITY: int = 1000\n\n    def __init__(self, crawler: Crawler):\n        if not crawler.settings.getbool(\"ROBOTSTXT_OBEY\"):\n            raise NotConfigured\n        self._default_useragent: str = crawler.settings.get(\"USER_AGENT\", \"Scrapy\")\n        self._robotstxt_useragent: Optional[str] = crawler.settings.get(\n            \"ROBOTSTXT_USER_AGENT\", None\n        )\n        self.crawler: Crawler = crawler\n        self._parsers: Dict[str, Union[RobotParser, Deferred, None]] = {}\n        self._parserimpl: RobotParser = load_object(\n            crawler.settings.get(\"ROBOTSTXT_PARSER\")\n        )\n\n        # check if parser dependencies are met, this should throw an error otherwise.\n        self._parserimpl.from_crawler(self.crawler, b\"\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler)\n\n    def process_request(self, request: Request, spider: Spider) -> Optional[Deferred]:\n        if request.meta.get(\"dont_obey_robotstxt\"):\n            return None\n        if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n            return None\n        d: Deferred = maybeDeferred(self.robot_parser, request, spider)\n        d.addCallback(self.process_request_2, request, spider)\n        return d\n\n    def process_request_2(\n        self, rp: Optional[RobotParser], request: Request, spider: Spider\n    ) -> None:\n        if rp is None:\n            return\n\n        useragent: Union[str, bytes, None] = self._robotstxt_useragent\n        if not useragent:\n            useragent = request.headers.get(b\"User-Agent\", self._default_useragent)\n            assert useragent is not None\n        if not rp.allowed(request.url, useragent):\n            logger.debug(\n                \"Forbidden by robots.txt: %(request)s\",\n                {\"request\": request},\n                extra={\"spider\": spider},\n            )\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(\"robotstxt/forbidden\")\n            raise IgnoreRequest(\"Forbidden by robots.txt\")\n\n    def robot_parser(\n        self, request: Request, spider: Spider\n    ) -> Union[RobotParser, Deferred, None]:\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = f\"{url.scheme}://{url.netloc}/robots.txt\"\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={\"dont_obey_robotstxt\": True},\n                callback=NO_CALLBACK,\n            )\n            assert self.crawler.engine\n            assert self.crawler.stats\n            dfd = self.crawler.engine.download(robotsreq)\n            dfd.addCallback(self._parse_robots, netloc, spider)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n            self.crawler.stats.inc_value(\"robotstxt/request_count\")\n\n        parser = self._parsers[netloc]\n        if isinstance(parser, Deferred):\n            d: Deferred = Deferred()\n\n            def cb(result: Any) -> Any:\n                d.callback(result)\n                return result\n\n            parser.addCallback(cb)\n            return d\n        return parser\n\n    def _logerror(self, failure: Failure, request: Request, spider: Spider) -> Failure:\n        if failure.type is not IgnoreRequest:\n            logger.error(\n                \"Error downloading %(request)s: %(f_exception)s\",\n                {\"request\": request, \"f_exception\": failure.value},\n                exc_info=failure_to_exc_info(failure),\n                extra={\"spider\": spider},\n            )\n        return failure\n\n    def _parse_robots(self, response: Response, netloc: str, spider: Spider) -> None:\n        assert self.crawler.stats\n        self.crawler.stats.inc_value(\"robotstxt/response_count\")\n        self.crawler.stats.inc_value(\n            f\"robotstxt/response_status_count/{response.status}\"\n        )\n        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n        rp_dfd = self._parsers[netloc]\n        assert isinstance(rp_dfd, Deferred)\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)\n\n    def _robots_error(self, failure: Failure, netloc: str) -> None:\n        if failure.type is not IgnoreRequest:\n            key = f\"robotstxt/exception_count/{failure.type}\"\n            assert self.crawler.stats\n            self.crawler.stats.inc_value(key)\n        rp_dfd = self._parsers[netloc]\n        assert isinstance(rp_dfd, Deferred)\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)\n", "scrapy/downloadermiddlewares/redirect.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING, Any, List, Union, cast\nfrom urllib.parse import urljoin\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.response import get_meta_refresh\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\ndef _build_redirect_request(\n    source_request: Request, *, url: str, **kwargs: Any\n) -> Request:\n    redirect_request = source_request.replace(\n        url=url,\n        **kwargs,\n        cls=None,\n        cookies=None,\n    )\n    if \"_scheme_proxy\" in redirect_request.meta:\n        source_request_scheme = urlparse_cached(source_request).scheme\n        redirect_request_scheme = urlparse_cached(redirect_request).scheme\n        if source_request_scheme != redirect_request_scheme:\n            redirect_request.meta.pop(\"_scheme_proxy\")\n            redirect_request.meta.pop(\"proxy\", None)\n            redirect_request.meta.pop(\"_auth_proxy\", None)\n            redirect_request.headers.pop(b\"Proxy-Authorization\", None)\n    has_cookie_header = \"Cookie\" in redirect_request.headers\n    has_authorization_header = \"Authorization\" in redirect_request.headers\n    if has_cookie_header or has_authorization_header:\n        default_ports = {\"http\": 80, \"https\": 443}\n\n        parsed_source_request = urlparse_cached(source_request)\n        source_scheme, source_host, source_port = (\n            parsed_source_request.scheme,\n            parsed_source_request.hostname,\n            parsed_source_request.port\n            or default_ports.get(parsed_source_request.scheme),\n        )\n\n        parsed_redirect_request = urlparse_cached(redirect_request)\n        redirect_scheme, redirect_host, redirect_port = (\n            parsed_redirect_request.scheme,\n            parsed_redirect_request.hostname,\n            parsed_redirect_request.port\n            or default_ports.get(parsed_redirect_request.scheme),\n        )\n\n        if has_cookie_header and (\n            redirect_scheme not in {source_scheme, \"https\"}\n            or source_host != redirect_host\n        ):\n            del redirect_request.headers[\"Cookie\"]\n\n        # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name\n        if has_authorization_header and (\n            source_scheme != redirect_scheme\n            or source_host != redirect_host\n            or source_port != redirect_port\n        ):\n            del redirect_request.headers[\"Authorization\"]\n\n    return redirect_request\n\n\nclass BaseRedirectMiddleware:\n    enabled_setting: str = \"REDIRECT_ENABLED\"\n\n    def __init__(self, settings: BaseSettings):\n        if not settings.getbool(self.enabled_setting):\n            raise NotConfigured\n\n        self.max_redirect_times: int = settings.getint(\"REDIRECT_MAX_TIMES\")\n        self.priority_adjust: int = settings.getint(\"REDIRECT_PRIORITY_ADJUST\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        return cls(crawler.settings)\n\n    def _redirect(\n        self, redirected: Request, request: Request, spider: Spider, reason: Any\n    ) -> Request:\n        ttl = request.meta.setdefault(\"redirect_ttl\", self.max_redirect_times)\n        redirects = request.meta.get(\"redirect_times\", 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta[\"redirect_times\"] = redirects\n            redirected.meta[\"redirect_ttl\"] = ttl - 1\n            redirected.meta[\"redirect_urls\"] = request.meta.get(\"redirect_urls\", []) + [\n                request.url\n            ]\n            redirected.meta[\"redirect_reasons\"] = request.meta.get(\n                \"redirect_reasons\", []\n            ) + [reason]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\n                \"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                {\"reason\": reason, \"redirected\": redirected, \"request\": request},\n                extra={\"spider\": spider},\n            )\n            return redirected\n        logger.debug(\n            \"Discarding %(request)s: max redirections reached\",\n            {\"request\": request},\n            extra={\"spider\": spider},\n        )\n        raise IgnoreRequest(\"max redirections reached\")\n\n    def _redirect_request_using_get(\n        self, request: Request, redirect_url: str\n    ) -> Request:\n        redirect_request = _build_redirect_request(\n            request,\n            url=redirect_url,\n            method=\"GET\",\n            body=\"\",\n        )\n        redirect_request.headers.pop(\"Content-Type\", None)\n        redirect_request.headers.pop(\"Content-Length\", None)\n        return redirect_request\n\n\nclass RedirectMiddleware(BaseRedirectMiddleware):\n    \"\"\"\n    Handle redirection of requests based on response status\n    and meta-refresh html tag.\n    \"\"\"\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if (\n            request.meta.get(\"dont_redirect\", False)\n            or response.status in getattr(spider, \"handle_httpstatus_list\", [])\n            or response.status in request.meta.get(\"handle_httpstatus_list\", [])\n            or request.meta.get(\"handle_httpstatus_all\", False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if \"Location\" not in response.headers or response.status not in allowed_status:\n            return response\n\n        assert response.headers[\"Location\"] is not None\n        location = safe_url_string(response.headers[\"Location\"])\n        if response.headers[\"Location\"].startswith(b\"//\"):\n            request_scheme = urlparse_cached(request).scheme\n            location = request_scheme + \"://\" + location.lstrip(\"/\")\n\n        redirected_url = urljoin(request.url, location)\n        redirected = _build_redirect_request(request, url=redirected_url)\n        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n            return response\n\n        if response.status in (301, 307, 308) or request.method == \"HEAD\":\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)\n\n\nclass MetaRefreshMiddleware(BaseRedirectMiddleware):\n    enabled_setting = \"METAREFRESH_ENABLED\"\n\n    def __init__(self, settings: BaseSettings):\n        super().__init__(settings)\n        self._ignore_tags: List[str] = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n        self._maxdelay: int = settings.getint(\"METAREFRESH_MAXDELAY\")\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if (\n            request.meta.get(\"dont_redirect\", False)\n            or request.method == \"HEAD\"\n            or not isinstance(response, HtmlResponse)\n            or urlparse_cached(request).scheme not in {\"http\", \"https\"}\n        ):\n            return response\n\n        interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)\n        if not url:\n            return response\n        redirected = self._redirect_request_using_get(request, url)\n        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n            return response\n        if cast(float, interval) < self._maxdelay:\n            return self._redirect(redirected, request, spider, \"meta refresh\")\n        return response\n", "scrapy/downloadermiddlewares/__init__.py": "", "scrapy/downloadermiddlewares/useragent.py": "\"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Union\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass UserAgentMiddleware:\n    \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n\n    def __init__(self, user_agent: str = \"Scrapy\"):\n        self.user_agent = user_agent\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls(crawler.settings[\"USER_AGENT\"])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        self.user_agent = getattr(spider, \"user_agent\", self.user_agent)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if self.user_agent:\n            request.headers.setdefault(b\"User-Agent\", self.user_agent)\n        return None\n", "scrapy/downloadermiddlewares/cookies.py": "from __future__ import annotations\n\nimport logging\nfrom collections import defaultdict\nfrom http.cookiejar import Cookie\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Iterable, Optional, Sequence, Union\n\nfrom tldextract import TLDExtract\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import Crawler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Response\nfrom scrapy.http.cookies import CookieJar\nfrom scrapy.http.request import VerboseCookie\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\n_split_domain = TLDExtract(include_psl_private_domains=True)\n_UNSET = object()\n\n\ndef _is_public_domain(domain: str) -> bool:\n    parts = _split_domain(domain)\n    return not parts.domain\n\n\nclass CookiesMiddleware:\n    \"\"\"This middleware enables working with sites that need cookies\"\"\"\n\n    def __init__(self, debug: bool = False):\n        self.jars: DefaultDict[Any, CookieJar] = defaultdict(CookieJar)\n        self.debug: bool = debug\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        if not crawler.settings.getbool(\"COOKIES_ENABLED\"):\n            raise NotConfigured\n        return cls(crawler.settings.getbool(\"COOKIES_DEBUG\"))\n\n    def _process_cookies(\n        self, cookies: Iterable[Cookie], *, jar: CookieJar, request: Request\n    ) -> None:\n        for cookie in cookies:\n            cookie_domain = cookie.domain\n            if cookie_domain.startswith(\".\"):\n                cookie_domain = cookie_domain[1:]\n\n            hostname = urlparse_cached(request).hostname\n            assert hostname is not None\n            request_domain = hostname.lower()\n\n            if cookie_domain and _is_public_domain(cookie_domain):\n                if cookie_domain != request_domain:\n                    continue\n                cookie.domain = request_domain\n\n            jar.set_cookie_if_ok(cookie, request)\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        if request.meta.get(\"dont_merge_cookies\", False):\n            return None\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        self._process_cookies(cookies, jar=jar, request=request)\n\n        # set Cookie header\n        request.headers.pop(\"Cookie\", None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)\n        return None\n\n    def process_response(\n        self, request: Request, response: Response, spider: Spider\n    ) -> Union[Request, Response]:\n        if request.meta.get(\"dont_merge_cookies\", False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = jar.make_cookies(response, request)\n        self._process_cookies(cookies, jar=jar, request=request)\n\n        self._debug_set_cookie(response, spider)\n\n        return response\n\n    def _debug_cookie(self, request: Request, spider: Spider) -> None:\n        if self.debug:\n            cl = [\n                to_unicode(c, errors=\"replace\")\n                for c in request.headers.getlist(\"Cookie\")\n            ]\n            if cl:\n                cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n                msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                logger.debug(msg, extra={\"spider\": spider})\n\n    def _debug_set_cookie(self, response: Response, spider: Spider) -> None:\n        if self.debug:\n            cl = [\n                to_unicode(c, errors=\"replace\")\n                for c in response.headers.getlist(\"Set-Cookie\")\n            ]\n            if cl:\n                cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n                msg = f\"Received cookies from: {response}\\n{cookies}\"\n                logger.debug(msg, extra={\"spider\": spider})\n\n    def _format_cookie(self, cookie: VerboseCookie, request: Request) -> Optional[str]:\n        \"\"\"\n        Given a dict consisting of cookie components, return its string representation.\n        Decode from bytes if necessary.\n        \"\"\"\n        decoded = {}\n        flags = set()\n        for key in (\"name\", \"value\", \"path\", \"domain\"):\n            if cookie.get(key) is None:\n                if key in (\"name\", \"value\"):\n                    msg = f\"Invalid cookie found in request {request}: {cookie} ('{key}' is missing)\"\n                    logger.warning(msg)\n                    return None\n                continue\n            # https://github.com/python/mypy/issues/7178, https://github.com/python/mypy/issues/9168\n            if isinstance(cookie[key], (bool, float, int, str)):  # type: ignore[literal-required]\n                decoded[key] = str(cookie[key])  # type: ignore[literal-required]\n            else:\n                try:\n                    decoded[key] = cookie[key].decode(\"utf8\")  # type: ignore[literal-required]\n                except UnicodeDecodeError:\n                    logger.warning(\n                        \"Non UTF-8 encoded cookie found in request %s: %s\",\n                        request,\n                        cookie,\n                    )\n                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")  # type: ignore[literal-required]\n        for flag in (\"secure\",):\n            value = cookie.get(flag, _UNSET)\n            if value is _UNSET or not value:\n                continue\n            flags.add(flag)\n        cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n        for key, value in decoded.items():  # path, domain\n            cookie_str += f\"; {key.capitalize()}={value}\"\n        for flag in flags:  # secure\n            cookie_str += f\"; {flag.capitalize()}\"\n        return cookie_str\n\n    def _get_request_cookies(\n        self, jar: CookieJar, request: Request\n    ) -> Sequence[Cookie]:\n        \"\"\"\n        Extract cookies from the Request.cookies attribute\n        \"\"\"\n        if not request.cookies:\n            return []\n        cookies: Iterable[VerboseCookie]\n        if isinstance(request.cookies, dict):\n            cookies = tuple({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n        else:\n            cookies = request.cookies\n        for cookie in cookies:\n            cookie.setdefault(\"secure\", urlparse_cached(request).scheme == \"https\")\n        formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n        response = Response(request.url, headers={\"Set-Cookie\": formatted})\n        return jar.make_cookies(response, request)\n", "scrapy/downloadermiddlewares/httpauth.py": "\"\"\"\nHTTP basic auth downloader middleware\n\nSee documentation in docs/topics/downloader-middleware.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Union\n\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Response\nfrom scrapy.utils.url import url_is_from_any_domain\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass HttpAuthMiddleware:\n    \"\"\"Set Basic HTTP Authorization header\n    (http_user and http_pass spider class attributes)\"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o\n\n    def spider_opened(self, spider: Spider) -> None:\n        usr = getattr(spider, \"http_user\", \"\")\n        pwd = getattr(spider, \"http_pass\", \"\")\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)\n            self.domain = spider.http_auth_domain  # type: ignore[attr-defined]\n\n    def process_request(\n        self, request: Request, spider: Spider\n    ) -> Union[Request, Response, None]:\n        auth = getattr(self, \"auth\", None)\n        if auth and b\"Authorization\" not in request.headers:\n            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                request.headers[b\"Authorization\"] = auth\n        return None\n", "scrapy/http/headers.py": "from __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom w3lib.http import headers_dict_to_raw\n\nfrom scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_RawValueT = Union[bytes, str, int]\n\n\n# isn't fully compatible typing-wise with either dict or CaselessDict,\n# but it needs refactoring anyway, see also https://github.com/scrapy/scrapy/pull/5146\nclass Headers(CaselessDict):\n    \"\"\"Case insensitive http headers dictionary\"\"\"\n\n    def __init__(\n        self,\n        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        encoding: str = \"utf-8\",\n    ):\n        self.encoding: str = encoding\n        super().__init__(seq)\n\n    def update(  # type: ignore[override]\n        self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]\n    ) -> None:\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq: Dict[bytes, List[bytes]] = {}\n        for k, v in seq:\n            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n        super().update(iseq)\n\n    def normkey(self, key: AnyStr) -> bytes:  # type: ignore[override]\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())\n\n    def normvalue(self, value: Union[_RawValueT, Iterable[_RawValueT]]) -> List[bytes]:\n        \"\"\"Normalize values to bytes\"\"\"\n        _value: Iterable[_RawValueT]\n        if value is None:\n            _value = []\n        elif isinstance(value, (str, bytes)):\n            _value = [value]\n        elif hasattr(value, \"__iter__\"):\n            _value = value\n        else:\n            _value = [value]\n\n        return [self._tobytes(x) for x in _value]\n\n    def _tobytes(self, x: _RawValueT) -> bytes:\n        if isinstance(x, bytes):\n            return x\n        if isinstance(x, str):\n            return x.encode(self.encoding)\n        if isinstance(x, int):\n            return str(x).encode(self.encoding)\n        raise TypeError(f\"Unsupported value type: {type(x)}\")\n\n    def __getitem__(self, key: AnyStr) -> Optional[bytes]:\n        try:\n            return cast(List[bytes], super().__getitem__(key))[-1]\n        except IndexError:\n            return None\n\n    def get(self, key: AnyStr, def_val: Any = None) -> Optional[bytes]:\n        try:\n            return cast(List[bytes], super().get(key, def_val))[-1]\n        except IndexError:\n            return None\n\n    def getlist(self, key: AnyStr, def_val: Any = None) -> List[bytes]:\n        try:\n            return cast(List[bytes], super().__getitem__(key))\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []\n\n    def setlist(self, key: AnyStr, list_: Iterable[_RawValueT]) -> None:\n        self[key] = list_\n\n    def setlistdefault(\n        self, key: AnyStr, default_list: Iterable[_RawValueT] = ()\n    ) -> Any:\n        return self.setdefault(key, default_list)\n\n    def appendlist(self, key: AnyStr, value: Iterable[_RawValueT]) -> None:\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst\n\n    def items(self) -> Iterable[Tuple[bytes, List[bytes]]]:  # type: ignore[override]\n        return ((k, self.getlist(k)) for k in self.keys())\n\n    def values(self) -> List[Optional[bytes]]:  # type: ignore[override]\n        return [\n            self[k] for k in self.keys()  # pylint: disable=consider-using-dict-items\n        ]\n\n    def to_string(self) -> bytes:\n        return headers_dict_to_raw(self)\n\n    def to_unicode_dict(self) -> CaseInsensitiveDict:\n        \"\"\"Return headers as a CaseInsensitiveDict with str keys\n        and str values. Multiple values are joined with ','.\n        \"\"\"\n        return CaseInsensitiveDict(\n            (\n                to_unicode(key, encoding=self.encoding),\n                to_unicode(b\",\".join(value), encoding=self.encoding),\n            )\n            for key, value in self.items()\n        )\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n", "scrapy/http/__init__.py": "\"\"\"\nModule containing all HTTP related classes\n\nUse this module (instead of the more specific ones) when importing Headers,\nRequest and Response outside this module.\n\"\"\"\n\nfrom scrapy.http.headers import Headers\nfrom scrapy.http.request import Request\nfrom scrapy.http.request.form import FormRequest\nfrom scrapy.http.request.json_request import JsonRequest\nfrom scrapy.http.request.rpc import XmlRpcRequest\nfrom scrapy.http.response import Response\nfrom scrapy.http.response.html import HtmlResponse\nfrom scrapy.http.response.json import JsonResponse\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.http.response.xml import XmlResponse\n", "scrapy/http/cookies.py": "from __future__ import annotations\n\nimport re\nimport time\nfrom http.cookiejar import Cookie\nfrom http.cookiejar import CookieJar as _CookieJar\nfrom http.cookiejar import CookiePolicy, DefaultCookiePolicy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    cast,\n)\n\nfrom scrapy import Request\nfrom scrapy.http import Response\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n# Defined in the http.cookiejar module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\nIPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n\n\nclass CookieJar:\n    def __init__(\n        self,\n        policy: Optional[CookiePolicy] = None,\n        check_expired_frequency: int = 10000,\n    ):\n        self.policy: CookiePolicy = policy or DefaultCookiePolicy()\n        self.jar: _CookieJar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()  # type: ignore[attr-defined]\n        self.check_expired_frequency: int = check_expired_frequency\n        self.processed: int = 0\n\n    def extract_cookies(self, response: Response, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        self.jar.extract_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def add_cookie_header(self, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())  # type: ignore[attr-defined]\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if \".\" not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:  # type: ignore[attr-defined]\n                cookies += self.jar._cookies_for_domain(host, wreq)  # type: ignore[attr-defined]\n\n        attrs = self.jar._cookie_attrs(cookies)  # type: ignore[attr-defined]\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()\n\n    @property\n    def _cookies(self) -> Dict[str, Dict[str, Dict[str, Cookie]]]:\n        return self.jar._cookies  # type: ignore[attr-defined,no-any-return]\n\n    def clear_session_cookies(self) -> None:\n        return self.jar.clear_session_cookies()\n\n    def clear(\n        self,\n        domain: Optional[str] = None,\n        path: Optional[str] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        self.jar.clear(domain, path, name)\n\n    def __iter__(self) -> Iterator[Cookie]:\n        return iter(self.jar)\n\n    def __len__(self) -> int:\n        return len(self.jar)\n\n    def set_policy(self, pol: CookiePolicy) -> None:\n        self.jar.set_policy(pol)\n\n    def make_cookies(self, response: Response, request: Request) -> Sequence[Cookie]:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def set_cookie(self, cookie: Cookie) -> None:\n        self.jar.set_cookie(cookie)\n\n    def set_cookie_if_ok(self, cookie: Cookie, request: Request) -> None:\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))  # type: ignore[arg-type]\n\n\ndef potential_domain_matches(domain: str) -> List[str]:\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index(\".\") + 1\n        end = domain.rindex(\".\")\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index(\".\", start) + 1\n    except ValueError:\n        pass\n    return matches + [\".\" + d for d in matches]\n\n\nclass _DummyLock:\n    def acquire(self) -> None:\n        pass\n\n    def release(self) -> None:\n        pass\n\n\nclass WrappedRequest:\n    \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n\n    see http://docs.python.org/library/urllib2.html#urllib2.Request\n    \"\"\"\n\n    def __init__(self, request: Request):\n        self.request = request\n\n    def get_full_url(self) -> str:\n        return self.request.url\n\n    def get_host(self) -> str:\n        return urlparse_cached(self.request).netloc\n\n    def get_type(self) -> str:\n        return urlparse_cached(self.request).scheme\n\n    def is_unverifiable(self) -> bool:\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return cast(bool, self.request.meta.get(\"is_unverifiable\", False))\n\n    @property\n    def full_url(self) -> str:\n        return self.get_full_url()\n\n    @property\n    def host(self) -> str:\n        return self.get_host()\n\n    @property\n    def type(self) -> str:\n        return self.get_type()\n\n    @property\n    def unverifiable(self) -> bool:\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self) -> str:\n        return cast(str, urlparse_cached(self.request).hostname)\n\n    def has_header(self, name: str) -> bool:\n        return name in self.request.headers\n\n    def get_header(self, name: str, default: Optional[str] = None) -> Optional[str]:\n        value = self.request.headers.get(name, default)\n        return to_unicode(value, errors=\"replace\") if value is not None else None\n\n    def header_items(self) -> List[Tuple[str, List[str]]]:\n        return [\n            (\n                to_unicode(k, errors=\"replace\"),\n                [to_unicode(x, errors=\"replace\") for x in v],\n            )\n            for k, v in self.request.headers.items()\n        ]\n\n    def add_unredirected_header(self, name: str, value: str) -> None:\n        self.request.headers.appendlist(name, value)\n\n\nclass WrappedResponse:\n    def __init__(self, response: Response):\n        self.response = response\n\n    def info(self) -> Self:\n        return self\n\n    def get_all(self, name: str, default: Any = None) -> List[str]:\n        return [\n            to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n        ]\n", "scrapy/http/response/html.py": "\"\"\"\nThis module implements the HtmlResponse class which adds encoding\ndiscovering through HTML encoding declarations to the TextResponse class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass HtmlResponse(TextResponse):\n    pass\n", "scrapy/http/response/xml.py": "\"\"\"\nThis module implements the XmlResponse class which adds encoding\ndiscovering through XML encoding declarations to the TextResponse class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass XmlResponse(TextResponse):\n    pass\n", "scrapy/http/response/json.py": "\"\"\"\nThis module implements the JsonResponse class that is used when the response\nhas a JSON MIME type in its Content-Type header.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom scrapy.http.response.text import TextResponse\n\n\nclass JsonResponse(TextResponse):\n    pass\n", "scrapy/http/response/__init__.py": "\"\"\"\nThis module implements the Response class which is used to represent HTTP\nresponses in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom ipaddress import IPv4Address, IPv6Address\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    overload,\n)\nfrom urllib.parse import urljoin\n\nfrom twisted.internet.ssl import Certificate\n\nfrom scrapy.exceptions import NotSupported\nfrom scrapy.http.headers import Headers\nfrom scrapy.http.request import CookiesT, Request\nfrom scrapy.link import Link\nfrom scrapy.utils.trackref import object_ref\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy.selector import SelectorList\n\n\nResponseTypeVar = TypeVar(\"ResponseTypeVar\", bound=\"Response\")\n\n\nclass Response(object_ref):\n    \"\"\"An object that represents an HTTP response, which is usually\n    downloaded (by the Downloader) and fed to the Spiders for processing.\n    \"\"\"\n\n    attributes: Tuple[str, ...] = (\n        \"url\",\n        \"status\",\n        \"headers\",\n        \"body\",\n        \"flags\",\n        \"request\",\n        \"certificate\",\n        \"ip_address\",\n        \"protocol\",\n    )\n    \"\"\"A tuple of :class:`str` objects containing the name of all public\n    attributes of the class that are also keyword parameters of the\n    ``__init__`` method.\n\n    Currently used by :meth:`Response.replace`.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        status: int = 200,\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: bytes = b\"\",\n        flags: Optional[List[str]] = None,\n        request: Optional[Request] = None,\n        certificate: Optional[Certificate] = None,\n        ip_address: Union[IPv4Address, IPv6Address, None] = None,\n        protocol: Optional[str] = None,\n    ):\n        self.headers: Headers = Headers(headers or {})\n        self.status: int = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request: Optional[Request] = request\n        self.flags: List[str] = [] if flags is None else list(flags)\n        self.certificate: Optional[Certificate] = certificate\n        self.ip_address: Union[IPv4Address, IPv6Address, None] = ip_address\n        self.protocol: Optional[str] = protocol\n\n    @property\n    def cb_kwargs(self) -> Dict[str, Any]:\n        try:\n            return self.request.cb_kwargs  # type: ignore[union-attr]\n        except AttributeError:\n            raise AttributeError(\n                \"Response.cb_kwargs not available, this response \"\n                \"is not tied to any request\"\n            )\n\n    @property\n    def meta(self) -> Dict[str, Any]:\n        try:\n            return self.request.meta  # type: ignore[union-attr]\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    def _set_url(self, url: str) -> None:\n        if isinstance(url, str):\n            self._url: str = url\n        else:\n            raise TypeError(\n                f\"{type(self).__name__} url must be str, \" f\"got {type(url).__name__}\"\n            )\n\n    @property\n    def body(self) -> bytes:\n        return self._body\n\n    def _set_body(self, body: Optional[bytes]) -> None:\n        if body is None:\n            self._body = b\"\"\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\"\n            )\n        else:\n            self._body = body\n\n    def __repr__(self) -> str:\n        return f\"<{self.status} {self.url}>\"\n\n    def copy(self) -> Self:\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()\n\n    @overload\n    def replace(\n        self, *args: Any, cls: Type[ResponseTypeVar], **kwargs: Any\n    ) -> ResponseTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: Optional[Type[Response]] = None, **kwargs: Any\n    ) -> Response:\n        \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n        for x in self.attributes:\n            kwargs.setdefault(x, getattr(self, x))\n        if cls is None:\n            cls = self.__class__\n        return cls(*args, **kwargs)\n\n    def urljoin(self, url: str) -> str:\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)\n\n    @property\n    def text(self) -> str:\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as str\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")\n\n    def css(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def jmespath(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def xpath(self, *a: Any, **kw: Any) -> SelectorList:\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def follow(\n        self,\n        url: Union[str, Link],\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n    ) -> Request:\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n        not only an absolute URL.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n\n        .. versionadded:: 2.0\n           The *flags* parameter.\n        \"\"\"\n        if encoding is None:\n            raise ValueError(\"encoding can't be None\")\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n\n        return Request(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n    def follow_all(\n        self,\n        urls: Iterable[Union[str, Link]],\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n    ) -> Iterable[Request]:\n        \"\"\"\n        .. versionadded:: 2.0\n\n        Return an iterable of :class:`~.Request` instances to follow all links\n        in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n        but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n        not only absolute URLs.\n\n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if not hasattr(urls, \"__iter__\"):\n            raise TypeError(\"'urls' argument must be an iterable\")\n        return (\n            self.follow(\n                url=url,\n                callback=callback,\n                method=method,\n                headers=headers,\n                body=body,\n                cookies=cookies,\n                meta=meta,\n                encoding=encoding,\n                priority=priority,\n                dont_filter=dont_filter,\n                errback=errback,\n                cb_kwargs=cb_kwargs,\n                flags=flags,\n            )\n            for url in urls\n        )\n", "scrapy/http/response/text.py": "\"\"\"\nThis module implements the TextResponse class which adds encoding handling and\ndiscovering (through HTTP headers) to base Response class.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom contextlib import suppress\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import urljoin\n\nimport parsel\nfrom w3lib.encoding import (\n    html_body_declared_encoding,\n    html_to_unicode,\n    http_content_type_encoding,\n    read_bom,\n    resolve_encoding,\n)\nfrom w3lib.html import strip_html5_whitespace\n\nfrom scrapy.http.request import CookiesT, Request\nfrom scrapy.http.response import Response\nfrom scrapy.link import Link\nfrom scrapy.utils.python import memoizemethod_noargs, to_unicode\nfrom scrapy.utils.response import get_base_url\n\nif TYPE_CHECKING:\n    from scrapy.selector import Selector, SelectorList\n\n_NONE = object()\n\n\nclass TextResponse(Response):\n    _DEFAULT_ENCODING = \"ascii\"\n    _cached_decoded_json = _NONE\n\n    attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        self._encoding: Optional[str] = kwargs.pop(\"encoding\", None)\n        self._cached_benc: Optional[str] = None\n        self._cached_ubody: Optional[str] = None\n        self._cached_selector: Optional[Selector] = None\n        super().__init__(*args, **kwargs)\n\n    def _set_body(self, body: Union[str, bytes, None]) -> None:\n        self._body: bytes = b\"\"  # used by encoding detection\n        if isinstance(body, str):\n            if self._encoding is None:\n                raise TypeError(\n                    \"Cannot convert unicode body - \"\n                    f\"{type(self).__name__} has no encoding\"\n                )\n            self._body = body.encode(self._encoding)\n        else:\n            super()._set_body(body)\n\n    @property\n    def encoding(self) -> str:\n        return self._declared_encoding() or self._body_inferred_encoding()\n\n    def _declared_encoding(self) -> Optional[str]:\n        return (\n            self._encoding\n            or self._bom_encoding()\n            or self._headers_encoding()\n            or self._body_declared_encoding()\n        )\n\n    def json(self) -> Any:\n        \"\"\"\n        .. versionadded:: 2.2\n\n        Deserialize a JSON document to a Python object.\n        \"\"\"\n        if self._cached_decoded_json is _NONE:\n            self._cached_decoded_json = json.loads(self.body)\n        return self._cached_decoded_json\n\n    @property\n    def text(self) -> str:\n        \"\"\"Body as unicode\"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = f\"charset={benc}\"\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody\n\n    def urljoin(self, url: str) -> str:\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)\n\n    @memoizemethod_noargs\n    def _headers_encoding(self) -> Optional[str]:\n        content_type = cast(bytes, self.headers.get(b\"Content-Type\", b\"\"))\n        return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n\n    def _body_inferred_encoding(self) -> str:\n        if self._cached_benc is None:\n            content_type = to_unicode(\n                cast(bytes, self.headers.get(b\"Content-Type\", b\"\")), encoding=\"latin-1\"\n            )\n            benc, ubody = html_to_unicode(\n                content_type,\n                self.body,\n                auto_detect_fun=self._auto_detect_fun,\n                default_encoding=self._DEFAULT_ENCODING,\n            )\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc\n\n    def _auto_detect_fun(self, text: bytes) -> Optional[str]:\n        for enc in (self._DEFAULT_ENCODING, \"utf-8\", \"cp1252\"):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)\n        return None\n\n    @memoizemethod_noargs\n    def _body_declared_encoding(self) -> Optional[str]:\n        return html_body_declared_encoding(self.body)\n\n    @memoizemethod_noargs\n    def _bom_encoding(self) -> Optional[str]:\n        return read_bom(self.body)[0]\n\n    @property\n    def selector(self) -> Selector:\n        from scrapy.selector import Selector\n\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector\n\n    def jmespath(self, query: str, **kwargs: Any) -> SelectorList:\n        from scrapy.selector import SelectorList\n\n        if not hasattr(self.selector, \"jmespath\"):\n            raise AttributeError(\n                \"Please install parsel >= 1.8.1 to get jmespath support\"\n            )\n\n        return cast(SelectorList, self.selector.jmespath(query, **kwargs))\n\n    def xpath(self, query: str, **kwargs: Any) -> SelectorList:\n        from scrapy.selector import SelectorList\n\n        return cast(SelectorList, self.selector.xpath(query, **kwargs))\n\n    def css(self, query: str) -> SelectorList:\n        from scrapy.selector import SelectorList\n\n        return cast(SelectorList, self.selector.css(query))\n\n    def follow(\n        self,\n        url: Union[str, Link, parsel.Selector],\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = None,\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n    ) -> Request:\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be not only an absolute URL, but also\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        See :ref:`response-follow-example` for usage examples.\n        \"\"\"\n        if isinstance(url, parsel.Selector):\n            url = _url_from_selector(url)\n        elif isinstance(url, parsel.SelectorList):\n            raise ValueError(\"SelectorList is not supported\")\n        encoding = self.encoding if encoding is None else encoding\n        return super().follow(\n            url=url,\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n    def follow_all(\n        self,\n        urls: Union[Iterable[Union[str, Link]], parsel.SelectorList, None] = None,\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: Optional[str] = None,\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n        flags: Optional[List[str]] = None,\n        css: Optional[str] = None,\n        xpath: Optional[str] = None,\n    ) -> Iterable[Request]:\n        \"\"\"\n        A generator that produces :class:`~.Request` instances to follow all\n        links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n        ``__init__`` method, except that each ``urls`` element does not need to be\n        an absolute URL, it can be any of the following:\n\n        * a relative URL\n        * a :class:`~scrapy.link.Link` object, e.g. the result of\n          :ref:`topics-link-extractors`\n        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n          ``response.css('a.my_link')[0]``\n        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``\n\n        In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n\n        Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n        using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n        selectors from which links cannot be obtained (for instance, anchor tags without an\n        ``href`` attribute)\n        \"\"\"\n        arguments = [x for x in (urls, css, xpath) if x is not None]\n        if len(arguments) != 1:\n            raise ValueError(\n                \"Please supply exactly one of the following arguments: urls, css, xpath\"\n            )\n        if not urls:\n            if css:\n                urls = self.css(css)\n            if xpath:\n                urls = self.xpath(xpath)\n        if isinstance(urls, parsel.SelectorList):\n            selectors = urls\n            urls = []\n            for sel in selectors:\n                with suppress(_InvalidSelector):\n                    urls.append(_url_from_selector(sel))\n        return super().follow_all(\n            urls=cast(Iterable[Union[str, Link]], urls),\n            callback=callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n            flags=flags,\n        )\n\n\nclass _InvalidSelector(ValueError):\n    \"\"\"\n    Raised when a URL cannot be obtained from a Selector\n    \"\"\"\n\n\ndef _url_from_selector(sel: parsel.Selector) -> str:\n    if isinstance(sel.root, str):\n        # e.g. ::attr(href) result\n        return strip_html5_whitespace(sel.root)\n    if not hasattr(sel.root, \"tag\"):\n        raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n    if sel.root.tag not in (\"a\", \"link\"):\n        raise _InvalidSelector(\n            \"Only <a> and <link> elements are supported; \" f\"got <{sel.root.tag}>\"\n        )\n    href = sel.root.get(\"href\")\n    if href is None:\n        raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n    return strip_html5_whitespace(href)\n", "scrapy/http/request/form.py": "\"\"\"\nThis module implements the FormRequest class which is a more convenient class\n(than Request) to generate Requests based on form data.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n\nfrom lxml.html import FormElement  # nosec\nfrom lxml.html import InputElement  # nosec\nfrom lxml.html import MultipleSelectOptions  # nosec\nfrom lxml.html import SelectElement  # nosec\nfrom lxml.html import TextareaElement  # nosec\nfrom w3lib.html import strip_html5_whitespace\n\nfrom scrapy.http.request import Request\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.utils.python import is_listlike, to_bytes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nFormdataVType = Union[str, Iterable[str]]\nFormdataKVType = Tuple[str, FormdataVType]\nFormdataType = Optional[Union[Dict[str, FormdataVType], List[FormdataKVType]]]\n\n\nclass FormRequest(Request):\n    valid_form_methods = [\"GET\", \"POST\"]\n\n    def __init__(\n        self, *args: Any, formdata: FormdataType = None, **kwargs: Any\n    ) -> None:\n        if formdata and kwargs.get(\"method\") is None:\n            kwargs[\"method\"] = \"POST\"\n\n        super().__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            form_query_str = _urlencode(items, self.encoding)\n            if self.method == \"POST\":\n                self.headers.setdefault(\n                    b\"Content-Type\", b\"application/x-www-form-urlencoded\"\n                )\n                self._set_body(form_query_str)\n            else:\n                self._set_url(\n                    urlunsplit(urlsplit(self.url)._replace(query=form_query_str))\n                )\n\n    @classmethod\n    def from_response(\n        cls,\n        response: TextResponse,\n        formname: Optional[str] = None,\n        formid: Optional[str] = None,\n        formnumber: int = 0,\n        formdata: FormdataType = None,\n        clickdata: Optional[Dict[str, Union[str, int]]] = None,\n        dont_click: bool = False,\n        formxpath: Optional[str] = None,\n        formcss: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Self:\n        kwargs.setdefault(\"encoding\", response.encoding)\n\n        if formcss is not None:\n            from parsel.csstranslator import HTMLTranslator\n\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata)\n        url = _get_form_url(form, kwargs.pop(\"url\", None))\n\n        method = kwargs.pop(\"method\", form.method)\n        if method is not None:\n            method = method.upper()\n            if method not in cls.valid_form_methods:\n                method = \"GET\"\n\n        return cls(url=url, method=method, formdata=formdata, **kwargs)\n\n\ndef _get_form_url(form: FormElement, url: Optional[str]) -> str:\n    assert form.base_url is not None  # typing\n    if url is None:\n        action = form.get(\"action\")\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n    return urljoin(form.base_url, url)\n\n\ndef _urlencode(seq: Iterable[FormdataKVType], enc: str) -> str:\n    values = [\n        (to_bytes(k, enc), to_bytes(v, enc))\n        for k, vs in seq\n        for v in (cast(Iterable[str], vs) if is_listlike(vs) else [cast(str, vs)])\n    ]\n    return urlencode(values, doseq=True)\n\n\ndef _get_form(\n    response: TextResponse,\n    formname: Optional[str],\n    formid: Optional[str],\n    formnumber: int,\n    formxpath: Optional[str],\n) -> FormElement:\n    \"\"\"Find the wanted form element within the given response.\"\"\"\n    root = response.selector.root\n    forms = root.xpath(\"//form\")\n    if not forms:\n        raise ValueError(f\"No <form> element found in {response}\")\n\n    if formname is not None:\n        f = root.xpath(f'//form[@name=\"{formname}\"]')\n        if f:\n            return cast(FormElement, f[0])\n\n    if formid is not None:\n        f = root.xpath(f'//form[@id=\"{formid}\"]')\n        if f:\n            return cast(FormElement, f[0])\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == \"form\":\n                    return cast(FormElement, el)\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError(f\"No <form> element found with {formxpath}\")\n\n    # If we get here, it means that either formname was None or invalid\n    try:\n        form = forms[formnumber]\n    except IndexError:\n        raise IndexError(f\"Form number {formnumber} not found in {response}\")\n    else:\n        return cast(FormElement, form)\n\n\ndef _get_inputs(\n    form: FormElement,\n    formdata: FormdataType,\n    dont_click: bool,\n    clickdata: Optional[Dict[str, Union[str, int]]],\n) -> List[FormdataKVType]:\n    \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n    try:\n        formdata_keys = dict(formdata or ()).keys()\n    except (ValueError, TypeError):\n        raise ValueError(\"formdata should be a dict or iterable of tuples\")\n\n    if not formdata:\n        formdata = []\n    inputs = form.xpath(\n        \"descendant::textarea\"\n        \"|descendant::select\"\n        \"|descendant::input[not(@type) or @type[\"\n        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n        \" and (../@checked or\"\n        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n        namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n    )\n    values: List[FormdataKVType] = [\n        (k, \"\" if v is None else v)\n        for k, v in (_value(e) for e in inputs)\n        if k and k not in formdata_keys\n    ]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    formdata_items = formdata.items() if isinstance(formdata, dict) else formdata\n    values.extend((k, v) for k, v in formdata_items if v is not None)\n    return values\n\n\ndef _value(\n    ele: Union[InputElement, SelectElement, TextareaElement]\n) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n    n = ele.name\n    v = ele.value\n    if ele.tag == \"select\":\n        return _select_value(cast(SelectElement, ele), n, v)\n    return n, v\n\n\ndef _select_value(\n    ele: SelectElement, n: Optional[str], v: Union[None, str, MultipleSelectOptions]\n) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags without options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    return n, v\n\n\ndef _get_clickable(\n    clickdata: Optional[Dict[str, Union[str, int]]], form: FormElement\n) -> Optional[Tuple[str, str]]:\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = list(\n        form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n            namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n        )\n    )\n    if not clickables:\n        return None\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get(\"name\"), el.get(\"value\") or \"\")\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get(\"nr\", None)\n    if nr is not None:\n        assert isinstance(nr, int)\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get(\"name\"), el.get(\"value\") or \"\")\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = \".//*\" + \"\".join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get(\"name\"), el[0].get(\"value\") or \"\")\n    if len(el) > 1:\n        raise ValueError(\n            f\"Multiple elements found ({el!r}) matching the \"\n            f\"criteria in clickdata: {clickdata!r}\"\n        )\n    else:\n        raise ValueError(f\"No clickable element matching clickdata: {clickdata!r}\")\n", "scrapy/http/request/json_request.py": "\"\"\"\nThis module implements the JsonRequest class which is a more convenient class\n(than Request) to generate JSON Requests.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Type, overload\n\nfrom scrapy.http.request import Request, RequestTypeVar\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nclass JsonRequest(Request):\n    attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n\n    def __init__(\n        self, *args: Any, dumps_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> None:\n        dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n        dumps_kwargs.setdefault(\"sort_keys\", True)\n        self._dumps_kwargs: Dict[str, Any] = dumps_kwargs\n\n        body_passed = kwargs.get(\"body\", None) is not None\n        data: Any = kwargs.pop(\"data\", None)\n        data_passed: bool = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn(\"Both body and data passed. data will be ignored\")\n        elif not body_passed and data_passed:\n            kwargs[\"body\"] = self._dumps(data)\n            if \"method\" not in kwargs:\n                kwargs[\"method\"] = \"POST\"\n\n        super().__init__(*args, **kwargs)\n        self.headers.setdefault(\"Content-Type\", \"application/json\")\n        self.headers.setdefault(\n            \"Accept\", \"application/json, text/javascript, */*; q=0.01\"\n        )\n\n    @property\n    def dumps_kwargs(self) -> Dict[str, Any]:\n        return self._dumps_kwargs\n\n    @overload\n    def replace(\n        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n    ) -> RequestTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n    ) -> Request:\n        body_passed = kwargs.get(\"body\", None) is not None\n        data: Any = kwargs.pop(\"data\", None)\n        data_passed: bool = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn(\"Both body and data passed. data will be ignored\")\n        elif not body_passed and data_passed:\n            kwargs[\"body\"] = self._dumps(data)\n\n        return super().replace(*args, cls=cls, **kwargs)\n\n    def _dumps(self, data: Any) -> str:\n        \"\"\"Convert to JSON\"\"\"\n        return json.dumps(data, **self._dumps_kwargs)\n", "scrapy/http/request/__init__.py": "\"\"\"\nThis module implements the Request class which is used to represent HTTP\nrequests in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    NoReturn,\n    Optional,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    overload,\n)\n\nfrom w3lib.url import safe_url_string\n\nimport scrapy\nfrom scrapy.http.headers import Headers\nfrom scrapy.utils.curl import curl_to_request_kwargs\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.utils.url import escape_ajax\n\nif TYPE_CHECKING:\n    # typing.NotRequired and typing.Self require Python 3.11\n    from typing_extensions import NotRequired, Self\n\n\nclass VerboseCookie(TypedDict):\n    name: str\n    value: str\n    domain: NotRequired[str]\n    path: NotRequired[str]\n    secure: NotRequired[bool]\n\n\nCookiesT = Union[Dict[str, str], List[VerboseCookie]]\n\n\nRequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n\n\ndef NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n    \"\"\"When assigned to the ``callback`` parameter of\n    :class:`~scrapy.http.Request`, it indicates that the request is not meant\n    to have a spider callback at all.\n\n    For example:\n\n    .. code-block:: python\n\n       Request(\"https://example.com\", callback=NO_CALLBACK)\n\n    This value should be used by :ref:`components <topics-components>` that\n    create and handle their own requests, e.g. through\n    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader\n    middlewares handling such requests can treat them differently from requests\n    intended for the :meth:`~scrapy.Spider.parse` callback.\n    \"\"\"\n    raise RuntimeError(\n        \"The NO_CALLBACK callback has been called. This is a special callback \"\n        \"value intended for requests whose callback is never meant to be \"\n        \"called.\"\n    )\n\n\nclass Request(object_ref):\n    \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n    executed by the Downloader, thus generating a :class:`Response`.\n    \"\"\"\n\n    attributes: Tuple[str, ...] = (\n        \"url\",\n        \"callback\",\n        \"method\",\n        \"headers\",\n        \"body\",\n        \"cookies\",\n        \"meta\",\n        \"encoding\",\n        \"priority\",\n        \"dont_filter\",\n        \"errback\",\n        \"flags\",\n        \"cb_kwargs\",\n    )\n    \"\"\"A tuple of :class:`str` objects containing the name of all public\n    attributes of the class that are also keyword parameters of the\n    ``__init__`` method.\n\n    Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n    :func:`~scrapy.utils.request.request_from_dict`.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        callback: Optional[Callable] = None,\n        method: str = \"GET\",\n        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n        body: Optional[Union[bytes, str]] = None,\n        cookies: Optional[CookiesT] = None,\n        meta: Optional[Dict[str, Any]] = None,\n        encoding: str = \"utf-8\",\n        priority: int = 0,\n        dont_filter: bool = False,\n        errback: Optional[Callable] = None,\n        flags: Optional[List[str]] = None,\n        cb_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        self._encoding: str = encoding  # this one has to be set first\n        self.method: str = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        if not isinstance(priority, int):\n            raise TypeError(f\"Request priority not an integer: {priority!r}\")\n        self.priority: int = priority\n\n        if not (callable(callback) or callback is None):\n            raise TypeError(\n                f\"callback must be a callable, got {type(callback).__name__}\"\n            )\n        if not (callable(errback) or errback is None):\n            raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n        self.callback: Optional[Callable] = callback\n        self.errback: Optional[Callable] = errback\n\n        self.cookies: CookiesT = cookies or {}\n        self.headers: Headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter: bool = dont_filter\n\n        self._meta: Optional[Dict[str, Any]] = dict(meta) if meta else None\n        self._cb_kwargs: Optional[Dict[str, Any]] = (\n            dict(cb_kwargs) if cb_kwargs else None\n        )\n        self.flags: List[str] = [] if flags is None else list(flags)\n\n    @property\n    def cb_kwargs(self) -> Dict[str, Any]:\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs\n\n    @property\n    def meta(self) -> Dict[str, Any]:\n        if self._meta is None:\n            self._meta = {}\n        return self._meta\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    def _set_url(self, url: str) -> None:\n        if not isinstance(url, str):\n            raise TypeError(f\"Request url must be str, got {type(url).__name__}\")\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if (\n            \"://\" not in self._url\n            and not self._url.startswith(\"about:\")\n            and not self._url.startswith(\"data:\")\n        ):\n            raise ValueError(f\"Missing scheme in request url: {self._url}\")\n\n    @property\n    def body(self) -> bytes:\n        return self._body\n\n    def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n        self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n\n    @property\n    def encoding(self) -> str:\n        return self._encoding\n\n    def __repr__(self) -> str:\n        return f\"<{self.method} {self.url}>\"\n\n    def copy(self) -> Self:\n        return self.replace()\n\n    @overload\n    def replace(\n        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n    ) -> RequestTypeVar: ...\n\n    @overload\n    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n\n    def replace(\n        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n    ) -> Request:\n        \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n        for x in self.attributes:\n            kwargs.setdefault(x, getattr(self, x))\n        if cls is None:\n            cls = self.__class__\n        return cls(*args, **kwargs)\n\n    @classmethod\n    def from_curl(\n        cls,\n        curl_command: str,\n        ignore_unknown_options: bool = True,\n        **kwargs: Any,\n    ) -> Self:\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JsonRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n        To translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n        \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n\n    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> Dict[str, Any]:\n        \"\"\"Return a dictionary containing the Request's data.\n\n        Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n        If a spider is given, this method will try to find out the name of the spider methods used as callback\n        and errback and include them in the output dict, raising an exception if they cannot be found.\n        \"\"\"\n        d = {\n            \"url\": self.url,  # urls are safe (safe_string_url)\n            \"callback\": (\n                _find_method(spider, self.callback)\n                if callable(self.callback)\n                else self.callback\n            ),\n            \"errback\": (\n                _find_method(spider, self.errback)\n                if callable(self.errback)\n                else self.errback\n            ),\n            \"headers\": dict(self.headers),\n        }\n        for attr in self.attributes:\n            d.setdefault(attr, getattr(self, attr))\n        if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck\n            d[\"_class\"] = self.__module__ + \".\" + self.__class__.__name__\n        return d\n\n\ndef _find_method(obj: Any, func: Callable[..., Any]) -> str:\n    \"\"\"Helper function for Request.to_dict\"\"\"\n    # Only instance methods contain ``__func__``\n    if obj and hasattr(func, \"__func__\"):\n        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n        for name, obj_func in members:\n            # We need to use __func__ to access the original function object because instance\n            # method objects are generated each time attribute is retrieved from instance.\n            #\n            # Reference: The standard type hierarchy\n            # https://docs.python.org/3/reference/datamodel.html\n            if obj_func.__func__ is func.__func__:\n                return name\n    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")\n", "scrapy/http/request/rpc.py": "\"\"\"\nThis module implements the XmlRpcRequest class which is a more convenient class\n(that Request) to generate xml-rpc requests.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\n\nimport xmlrpc.client as xmlrpclib\nfrom typing import Any, Optional\n\nimport defusedxml.xmlrpc\n\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import get_func_args\n\ndefusedxml.xmlrpc.monkey_patch()\n\nDUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n\n\nclass XmlRpcRequest(Request):\n    def __init__(self, *args: Any, encoding: Optional[str] = None, **kwargs: Any):\n        if \"body\" not in kwargs and \"params\" in kwargs:\n            kw = {k: kwargs.pop(k) for k in DUMPS_ARGS if k in kwargs}\n            kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault(\"method\", \"POST\")\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault(\"dont_filter\", True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs[\"encoding\"] = encoding\n\n        super().__init__(*args, **kwargs)\n        self.headers.setdefault(\"Content-Type\", \"text/xml\")\n", "docs/conf.py": "# Scrapy documentation build configuration file, created by\n# sphinx-quickstart on Mon Nov 24 12:02:52 2008.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# The contents of this file are pickled, so don't put values in the namespace\n# that aren't pickleable (module imports are okay, they're removed automatically).\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nfrom pathlib import Path\n\n# If your extensions are in another directory, add it here. If the directory\n# is relative to the documentation root, use Path.absolute to make it absolute.\nsys.path.append(str(Path(__file__).parent / \"_ext\"))\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\n\n# General configuration\n# ---------------------\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = [\n    \"hoverxref.extension\",\n    \"notfound.extension\",\n    \"scrapydocs\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.coverage\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.viewcode\",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix of source filenames.\nsource_suffix = \".rst\"\n\n# The encoding of source files.\n# source_encoding = 'utf-8'\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = \"Scrapy\"\ncopyright = \"Scrapy developers\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\ntry:\n    import scrapy\n\n    version = \".\".join(map(str, scrapy.version_info[:2]))\n    release = scrapy.__version__\nexcept ImportError:\n    version = \"\"\n    release = \"\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\nlanguage = \"en\"\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of documents that shouldn't be included in the build.\n# unused_docs = []\n\nexclude_patterns = [\"build\"]\n\n# List of directories, relative to source directory, that shouldn't be searched\n# for source files.\nexclude_trees = [\".build\"]\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# List of Sphinx warnings that will not be raised\nsuppress_warnings = [\"epub.unknown_project_files\"]\n\n\n# Options for HTML output\n# -----------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# Add path to the RTD explicitly to robustify builds (otherwise might\n# fail in a clean Debian build env)\nimport sphinx_rtd_theme\n\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# The style sheet to use for HTML and HTML Help pages. A file of that name\n# must exist either in Sphinx' static/ path, or in one of the custom paths\n# given in html_static_path.\n# html_style = 'scrapydoc.css'\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = \"%b %d, %Y\"\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_use_modindex = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, the reST sources are included in the HTML build as _sources/<name>.\nhtml_copy_source = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# If nonempty, this is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = ''\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"Scrapydoc\"\n\nhtml_css_files = [\n    \"custom.css\",\n]\n\n\n# Options for LaTeX output\n# ------------------------\n\n# The paper size ('letter' or 'a4').\n# latex_paper_size = 'letter'\n\n# The font size ('10pt', '11pt' or '12pt').\n# latex_font_size = '10pt'\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, document class [howto/manual]).\nlatex_documents = [\n    (\"index\", \"Scrapy.tex\", \"Scrapy Documentation\", \"Scrapy developers\", \"manual\"),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# Additional stuff for the LaTeX preamble.\n# latex_preamble = ''\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_use_modindex = True\n\n\n# Options for the linkcheck builder\n# ---------------------------------\n\n# A list of regular expressions that match URIs that should not be checked when\n# doing a linkcheck build.\nlinkcheck_ignore = [\n    r\"http://localhost:\\d+\",\n    \"http://hg.scrapy.org\",\n    \"http://directory.google.com/\",\n]\n\n\n# Options for the Coverage extension\n# ----------------------------------\ncoverage_ignore_pyobjects = [\n    # Contract\u2019s add_pre_hook and add_post_hook are not documented because\n    # they should be transparent to contract developers, for whom pre_hook and\n    # post_hook should be the actual concern.\n    r\"\\bContract\\.add_(pre|post)_hook$\",\n    # ContractsManager is an internal class, developers are not expected to\n    # interact with it directly in any way.\n    r\"\\bContractsManager\\b$\",\n    # For default contracts we only want to document their general purpose in\n    # their __init__ method, the methods they reimplement to achieve that purpose\n    # should be irrelevant to developers using those contracts.\n    r\"\\w+Contract\\.(adjust_request_args|(pre|post)_process)$\",\n    # Methods of downloader middlewares are not documented, only the classes\n    # themselves, since downloader middlewares are controlled through Scrapy\n    # settings.\n    r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.(\\w*?Middleware|DownloaderStats)\\.\",\n    # Base classes of downloader middlewares are implementation details that\n    # are not meant for users.\n    r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware\",\n    # Private exception used by the command-line interface implementation.\n    r\"^scrapy\\.exceptions\\.UsageError\",\n    # Methods of BaseItemExporter subclasses are only documented in\n    # BaseItemExporter.\n    r\"^scrapy\\.exporters\\.(?!BaseItemExporter\\b)\\w*?\\.\",\n    # Extension behavior is only modified through settings. Methods of\n    # extension classes, as well as helper functions, are implementation\n    # details that are not documented.\n    r\"^scrapy\\.extensions\\.[a-z]\\w*?\\.[A-Z]\\w*?\\.\",  # methods\n    r\"^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]\",  # helper functions\n    # Never documented before, and deprecated now.\n    r\"^scrapy\\.linkextractors\\.FilteringLinkExtractor$\",\n    # Implementation detail of LxmlLinkExtractor\n    r\"^scrapy\\.linkextractors\\.lxmlhtml\\.LxmlParserLinkExtractor\",\n]\n\n\n# Options for the InterSphinx extension\n# -------------------------------------\n\nintersphinx_mapping = {\n    \"attrs\": (\"https://www.attrs.org/en/stable/\", None),\n    \"coverage\": (\"https://coverage.readthedocs.io/en/latest\", None),\n    \"cryptography\": (\"https://cryptography.io/en/latest/\", None),\n    \"cssselect\": (\"https://cssselect.readthedocs.io/en/latest\", None),\n    \"itemloaders\": (\"https://itemloaders.readthedocs.io/en/latest/\", None),\n    \"pytest\": (\"https://docs.pytest.org/en/latest\", None),\n    \"python\": (\"https://docs.python.org/3\", None),\n    \"sphinx\": (\"https://www.sphinx-doc.org/en/master\", None),\n    \"tox\": (\"https://tox.wiki/en/latest/\", None),\n    \"twisted\": (\"https://docs.twisted.org/en/stable/\", None),\n    \"twistedapi\": (\"https://docs.twisted.org/en/stable/api/\", None),\n    \"w3lib\": (\"https://w3lib.readthedocs.io/en/latest\", None),\n}\nintersphinx_disabled_reftypes = []\n\n\n# Options for sphinx-hoverxref options\n# ------------------------------------\n\nhoverxref_auto_ref = True\nhoverxref_role_types = {\n    \"class\": \"tooltip\",\n    \"command\": \"tooltip\",\n    \"confval\": \"tooltip\",\n    \"hoverxref\": \"tooltip\",\n    \"mod\": \"tooltip\",\n    \"ref\": \"tooltip\",\n    \"reqmeta\": \"tooltip\",\n    \"setting\": \"tooltip\",\n    \"signal\": \"tooltip\",\n}\nhoverxref_roles = [\"command\", \"reqmeta\", \"setting\", \"signal\"]\n\n\ndef setup(app):\n    app.connect(\"autodoc-skip-member\", maybe_skip_member)\n\n\ndef maybe_skip_member(app, what, name, obj, skip, options):\n    if not skip:\n        # autodocs was generating a text \"alias of\" for the following members\n        # https://github.com/sphinx-doc/sphinx/issues/4422\n        return name in {\"default_item_class\", \"default_selector_class\"}\n    return skip\n", "docs/conftest.py": "from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\nfrom pathlib import Path\n\nfrom sybil import Sybil\nfrom sybil.parsers.doctest import DocTestParser\nfrom sybil.parsers.skip import skip\n\ntry:\n    # >2.0.1\n    from sybil.parsers.codeblock import PythonCodeBlockParser\nexcept ImportError:\n    from sybil.parsers.codeblock import CodeBlockParser as PythonCodeBlockParser\n\nfrom scrapy.http.response.html import HtmlResponse\n\n\ndef load_response(url: str, filename: str) -> HtmlResponse:\n    input_path = Path(__file__).parent / \"_tests\" / filename\n    return HtmlResponse(url, body=input_path.read_bytes())\n\n\ndef setup(namespace):\n    namespace[\"load_response\"] = load_response\n\n\npytest_collect_file = Sybil(\n    parsers=[\n        DocTestParser(optionflags=ELLIPSIS | NORMALIZE_WHITESPACE),\n        PythonCodeBlockParser(future_imports=[\"print_function\"]),\n        skip,\n    ],\n    pattern=\"*.rst\",\n    setup=setup,\n).pytest()\n", "docs/utils/linkfix.py": "#!/usr/bin/python\n\n\"\"\"\n\nLinkfix - a companion to sphinx's linkcheck builder.\n\nUses the linkcheck's output file to fix links in docs.\n\nOriginally created for this issue:\nhttps://github.com/scrapy/scrapy/issues/606\n\nAuthor: dufferzafar\n\"\"\"\n\nimport re\nimport sys\nfrom pathlib import Path\n\n\ndef main():\n    # Used for remembering the file (and its contents)\n    # so we don't have to open the same file again.\n    _filename = None\n    _contents = None\n\n    # A regex that matches standard linkcheck output lines\n    line_re = re.compile(r\"(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))\")\n\n    # Read lines from the linkcheck output file\n    try:\n        with Path(\"build/linkcheck/output.txt\").open(encoding=\"utf-8\") as out:\n            output_lines = out.readlines()\n    except OSError:\n        print(\"linkcheck output not found; please run linkcheck first.\")\n        sys.exit(1)\n\n    # For every line, fix the respective file\n    for line in output_lines:\n        match = re.match(line_re, line)\n\n        if match:\n            newfilename = match.group(1)\n            errortype = match.group(2)\n\n            # Broken links can't be fixed and\n            # I am not sure what do with the local ones.\n            if errortype.lower() in [\"broken\", \"local\"]:\n                print(\"Not Fixed: \" + line)\n            else:\n                # If this is a new file\n                if newfilename != _filename:\n                    # Update the previous file\n                    if _filename:\n                        Path(_filename).write_text(_contents, encoding=\"utf-8\")\n\n                    _filename = newfilename\n\n                    # Read the new file to memory\n                    _contents = Path(_filename).read_text(encoding=\"utf-8\")\n\n                _contents = _contents.replace(match.group(3), match.group(4))\n        else:\n            # We don't understand what the current line means!\n            print(\"Not Understood: \" + line)\n\n\nif __name__ == \"__main__\":\n    main()\n", "docs/_ext/scrapydocs.py": "from operator import itemgetter\n\nfrom docutils import nodes\nfrom docutils.parsers.rst import Directive\nfrom docutils.parsers.rst.roles import set_classes\nfrom sphinx.util.nodes import make_refnode\n\n\nclass settingslist_node(nodes.General, nodes.Element):\n    pass\n\n\nclass SettingsListDirective(Directive):\n    def run(self):\n        return [settingslist_node(\"\")]\n\n\ndef is_setting_index(node):\n    if node.tagname == \"index\" and node[\"entries\"]:\n        # index entries for setting directives look like:\n        # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n        entry_type, info, refid = node[\"entries\"][0][:3]\n        return entry_type == \"pair\" and info.endswith(\"; setting\")\n    return False\n\n\ndef get_setting_target(node):\n    # target nodes are placed next to the node in the doc tree\n    return node.parent[node.parent.index(node) + 1]\n\n\ndef get_setting_name_and_refid(node):\n    \"\"\"Extract setting name from directive index node\"\"\"\n    entry_type, info, refid = node[\"entries\"][0][:3]\n    return info.replace(\"; setting\", \"\"), refid\n\n\ndef collect_scrapy_settings_refs(app, doctree):\n    env = app.builder.env\n\n    if not hasattr(env, \"scrapy_all_settings\"):\n        env.scrapy_all_settings = []\n\n    for node in doctree.traverse(is_setting_index):\n        targetnode = get_setting_target(node)\n        assert isinstance(targetnode, nodes.target), \"Next node is not a target\"\n\n        setting_name, refid = get_setting_name_and_refid(node)\n\n        env.scrapy_all_settings.append(\n            {\n                \"docname\": env.docname,\n                \"setting_name\": setting_name,\n                \"refid\": refid,\n            }\n        )\n\n\ndef make_setting_element(setting_data, app, fromdocname):\n    refnode = make_refnode(\n        app.builder,\n        fromdocname,\n        todocname=setting_data[\"docname\"],\n        targetid=setting_data[\"refid\"],\n        child=nodes.Text(setting_data[\"setting_name\"]),\n    )\n    p = nodes.paragraph()\n    p += refnode\n\n    item = nodes.list_item()\n    item += p\n    return item\n\n\ndef replace_settingslist_nodes(app, doctree, fromdocname):\n    env = app.builder.env\n\n    for node in doctree.traverse(settingslist_node):\n        settings_list = nodes.bullet_list()\n        settings_list.extend(\n            [\n                make_setting_element(d, app, fromdocname)\n                for d in sorted(env.scrapy_all_settings, key=itemgetter(\"setting_name\"))\n                if fromdocname != d[\"docname\"]\n            ]\n        )\n        node.replace_self(settings_list)\n\n\ndef setup(app):\n    app.add_crossref_type(\n        directivename=\"setting\",\n        rolename=\"setting\",\n        indextemplate=\"pair: %s; setting\",\n    )\n    app.add_crossref_type(\n        directivename=\"signal\",\n        rolename=\"signal\",\n        indextemplate=\"pair: %s; signal\",\n    )\n    app.add_crossref_type(\n        directivename=\"command\",\n        rolename=\"command\",\n        indextemplate=\"pair: %s; command\",\n    )\n    app.add_crossref_type(\n        directivename=\"reqmeta\",\n        rolename=\"reqmeta\",\n        indextemplate=\"pair: %s; reqmeta\",\n    )\n    app.add_role(\"source\", source_role)\n    app.add_role(\"commit\", commit_role)\n    app.add_role(\"issue\", issue_role)\n    app.add_role(\"rev\", rev_role)\n\n    app.add_node(settingslist_node)\n    app.add_directive(\"settingslist\", SettingsListDirective)\n\n    app.connect(\"doctree-read\", collect_scrapy_settings_refs)\n    app.connect(\"doctree-resolved\", replace_settingslist_nodes)\n\n\ndef source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = \"https://github.com/scrapy/scrapy/blob/master/\" + text\n    set_classes(options)\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []\n\n\ndef issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = \"https://github.com/scrapy/scrapy/issues/\" + text\n    set_classes(options)\n    node = nodes.reference(rawtext, \"issue \" + text, refuri=ref, **options)\n    return [node], []\n\n\ndef commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = \"https://github.com/scrapy/scrapy/commit/\" + text\n    set_classes(options)\n    node = nodes.reference(rawtext, \"commit \" + text, refuri=ref, **options)\n    return [node], []\n\n\ndef rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    ref = \"http://hg.scrapy.org/scrapy/changeset/\" + text\n    set_classes(options)\n    node = nodes.reference(rawtext, \"r\" + text, refuri=ref, **options)\n    return [node], []\n", "tests/test_downloadermiddleware_useragent.py": "from unittest import TestCase\n\nfrom scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass UserAgentMiddlewareTest(TestCase):\n    def get_spider_and_mw(self, default_useragent):\n        crawler = get_crawler(Spider, {\"USER_AGENT\": default_useragent})\n        spider = crawler._create_spider(\"foo\")\n        return spider, UserAgentMiddleware.from_crawler(crawler)\n\n    def test_default_agent(self):\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.headers[\"User-Agent\"], b\"default_useragent\")\n\n    def test_remove_agent(self):\n        # settings USER_AGENT to None should remove the user agent\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        spider.user_agent = None\n        mw.spider_opened(spider)\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req, spider) is None\n        assert req.headers.get(\"User-Agent\") is None\n\n    def test_spider_agent(self):\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        spider.user_agent = \"spider_useragent\"\n        mw.spider_opened(spider)\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.headers[\"User-Agent\"], b\"spider_useragent\")\n\n    def test_header_agent(self):\n        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n        spider.user_agent = \"spider_useragent\"\n        mw.spider_opened(spider)\n        req = Request(\n            \"http://scrapytest.org/\", headers={\"User-Agent\": \"header_useragent\"}\n        )\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.headers[\"User-Agent\"], b\"header_useragent\")\n\n    def test_no_agent(self):\n        spider, mw = self.get_spider_and_mw(None)\n        spider.user_agent = None\n        mw.spider_opened(spider)\n        req = Request(\"http://scrapytest.org/\")\n        assert mw.process_request(req, spider) is None\n        assert \"User-Agent\" not in req.headers\n", "tests/test_loader.py": "import dataclasses\nimport unittest\nfrom typing import Optional\n\nimport attr\nfrom itemadapter import ItemAdapter\nfrom itemloaders.processors import Compose, Identity, MapCompose, TakeFirst\n\nfrom scrapy.http import HtmlResponse, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.loader import ItemLoader\nfrom scrapy.selector import Selector\n\n\n# test items\nclass NameItem(Item):\n    name = Field()\n\n\nclass TestItem(NameItem):\n    url = Field()\n    summary = Field()\n\n\nclass TestNestedItem(Item):\n    name = Field()\n    name_div = Field()\n    name_value = Field()\n\n    url = Field()\n    image = Field()\n\n\n@attr.s\nclass AttrsNameItem:\n    name = attr.ib(default=\"\")\n\n\n@dataclasses.dataclass\nclass TestDataClass:\n    name: list = dataclasses.field(default_factory=list)\n\n\n# test item loaders\nclass NameItemLoader(ItemLoader):\n    default_item_class = TestItem\n\n\nclass NestedItemLoader(ItemLoader):\n    default_item_class = TestNestedItem\n\n\nclass TestItemLoader(NameItemLoader):\n    name_in = MapCompose(lambda v: v.title())\n\n\nclass DefaultedItemLoader(NameItemLoader):\n    default_input_processor = MapCompose(lambda v: v[:-1])\n\n\n# test processors\ndef processor_with_args(value, other=None, loader_context=None):\n    if \"key\" in loader_context:\n        return loader_context[\"key\"]\n    return value\n\n\nclass BasicItemLoaderTest(unittest.TestCase):\n    def test_add_value_on_unknown_field(self):\n        il = TestItemLoader()\n        self.assertRaises(KeyError, il.add_value, \"wrong_field\", [\"lala\", \"lolo\"])\n\n    def test_load_item_using_default_loader(self):\n        i = TestItem()\n        i[\"summary\"] = \"lala\"\n        il = ItemLoader(item=i)\n        il.add_value(\"name\", \"marta\")\n        item = il.load_item()\n        assert item is i\n        self.assertEqual(item[\"summary\"], [\"lala\"])\n        self.assertEqual(item[\"name\"], [\"marta\"])\n\n    def test_load_item_using_custom_loader(self):\n        il = TestItemLoader()\n        il.add_value(\"name\", \"marta\")\n        item = il.load_item()\n        self.assertEqual(item[\"name\"], [\"Marta\"])\n\n\nclass InitializationTestMixin:\n    item_class: Optional[type] = None\n\n    def test_keep_single_value(self):\n        \"\"\"Loaded item should contain values from the initial item\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\"]})\n\n    def test_keep_list(self):\n        \"\"\"Loaded item should contain values from the initial item\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\"]})\n\n    def test_add_value_singlevalue_singlevalue(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", \"bar\")\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\"]})\n\n    def test_add_value_singlevalue_list(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", [\"item\", \"loader\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(\n            ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"item\", \"loader\"]}\n        )\n\n    def test_add_value_list_singlevalue(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", \"qwerty\")\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(\n            ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\", \"qwerty\"]}\n        )\n\n    def test_add_value_list_list(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", [\"item\", \"loader\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(\n            ItemAdapter(loaded_item).asdict(),\n            {\"name\": [\"foo\", \"bar\", \"item\", \"loader\"]},\n        )\n\n    def test_get_output_value_singlevalue(self):\n        \"\"\"Getting output value must not remove value from item\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il.get_output_value(\"name\"), [\"foo\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\"]})\n\n    def test_get_output_value_list(self):\n        \"\"\"Getting output value must not remove value from item\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il.get_output_value(\"name\"), [\"foo\", \"bar\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\"]})\n\n    def test_values_single(self):\n        \"\"\"Values from initial item must be added to loader._values\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il._values.get(\"name\"), [\"foo\"])\n\n    def test_values_list(self):\n        \"\"\"Values from initial item must be added to loader._values\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il._values.get(\"name\"), [\"foo\", \"bar\"])\n\n\nclass InitializationFromDictTest(InitializationTestMixin, unittest.TestCase):\n    item_class = dict\n\n\nclass InitializationFromItemTest(InitializationTestMixin, unittest.TestCase):\n    item_class = NameItem\n\n\nclass InitializationFromAttrsItemTest(InitializationTestMixin, unittest.TestCase):\n    item_class = AttrsNameItem\n\n\nclass InitializationFromDataClassTest(InitializationTestMixin, unittest.TestCase):\n    item_class = TestDataClass\n\n\nclass BaseNoInputReprocessingLoader(ItemLoader):\n    title_in = MapCompose(str.upper)\n    title_out = TakeFirst()\n\n\nclass NoInputReprocessingItem(Item):\n    title = Field()\n\n\nclass NoInputReprocessingItemLoader(BaseNoInputReprocessingLoader):\n    default_item_class = NoInputReprocessingItem\n\n\nclass NoInputReprocessingFromItemTest(unittest.TestCase):\n    \"\"\"\n    Loaders initialized from loaded items must not reprocess fields (Item instances)\n    \"\"\"\n\n    def test_avoid_reprocessing_with_initial_values_single(self):\n        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title=\"foo\"))\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n        self.assertEqual(\n            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n        )\n\n    def test_avoid_reprocessing_with_initial_values_list(self):\n        il = NoInputReprocessingItemLoader(\n            item=NoInputReprocessingItem(title=[\"foo\", \"bar\"])\n        )\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n        self.assertEqual(\n            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n        )\n\n    def test_avoid_reprocessing_without_initial_values_single(self):\n        il = NoInputReprocessingItemLoader()\n        il.add_value(\"title\", \"FOO\")\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n        self.assertEqual(\n            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n        )\n\n    def test_avoid_reprocessing_without_initial_values_list(self):\n        il = NoInputReprocessingItemLoader()\n        il.add_value(\"title\", [\"foo\", \"bar\"])\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n        self.assertEqual(\n            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n        )\n\n\nclass TestOutputProcessorItem(unittest.TestCase):\n    def test_output_processor(self):\n        class TempItem(Item):\n            temp = Field()\n\n            def __init__(self, *args, **kwargs):\n                super().__init__(self, *args, **kwargs)\n                self.setdefault(\"temp\", 0.3)\n\n        class TempLoader(ItemLoader):\n            default_item_class = TempItem\n            default_input_processor = Identity()\n            default_output_processor = Compose(TakeFirst())\n\n        loader = TempLoader()\n        item = loader.load_item()\n        self.assertIsInstance(item, TempItem)\n        self.assertEqual(dict(item), {\"temp\": 0.3})\n\n\nclass SelectortemLoaderTest(unittest.TestCase):\n    response = HtmlResponse(\n        url=\"\",\n        encoding=\"utf-8\",\n        body=b\"\"\"\n    <html>\n    <body>\n    <div id=\"id\">marta</div>\n    <p>paragraph</p>\n    <a href=\"http://www.scrapy.org\">homepage</a>\n    <img src=\"/images/logo.png\" width=\"244\" height=\"65\" alt=\"Scrapy\">\n    </body>\n    </html>\n    \"\"\",\n    )\n\n    def test_init_method(self):\n        l = TestItemLoader()\n        self.assertEqual(l.selector, None)\n\n    def test_init_method_errors(self):\n        l = TestItemLoader()\n        self.assertRaises(RuntimeError, l.add_xpath, \"url\", \"//a/@href\")\n        self.assertRaises(RuntimeError, l.replace_xpath, \"url\", \"//a/@href\")\n        self.assertRaises(RuntimeError, l.get_xpath, \"//a/@href\")\n        self.assertRaises(RuntimeError, l.add_css, \"name\", \"#name::text\")\n        self.assertRaises(RuntimeError, l.replace_css, \"name\", \"#name::text\")\n        self.assertRaises(RuntimeError, l.get_css, \"#name::text\")\n\n    def test_init_method_with_selector(self):\n        sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n        l = TestItemLoader(selector=sel)\n        self.assertIs(l.selector, sel)\n\n        l.add_xpath(\"name\", \"//div/text()\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n\n    def test_init_method_with_selector_css(self):\n        sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n        l = TestItemLoader(selector=sel)\n        self.assertIs(l.selector, sel)\n\n        l.add_css(\"name\", \"div::text\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n\n    def test_init_method_with_base_response(self):\n        \"\"\"Selector should be None after initialization\"\"\"\n        response = Response(\"https://scrapy.org\")\n        l = TestItemLoader(response=response)\n        self.assertIs(l.selector, None)\n\n    def test_init_method_with_response(self):\n        l = TestItemLoader(response=self.response)\n        self.assertTrue(l.selector)\n\n        l.add_xpath(\"name\", \"//div/text()\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n\n    def test_init_method_with_response_css(self):\n        l = TestItemLoader(response=self.response)\n        self.assertTrue(l.selector)\n\n        l.add_css(\"name\", \"div::text\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n\n        l.add_css(\"url\", \"a::attr(href)\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n\n        # combining/accumulating CSS selectors and XPath expressions\n        l.add_xpath(\"name\", \"//div/text()\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\", \"Marta\"])\n\n        l.add_xpath(\"url\", \"//img/@src\")\n        self.assertEqual(\n            l.get_output_value(\"url\"), [\"http://www.scrapy.org\", \"/images/logo.png\"]\n        )\n\n    def test_add_xpath_re(self):\n        l = TestItemLoader(response=self.response)\n        l.add_xpath(\"name\", \"//div/text()\", re=\"ma\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n\n    def test_replace_xpath(self):\n        l = TestItemLoader(response=self.response)\n        self.assertTrue(l.selector)\n        l.add_xpath(\"name\", \"//div/text()\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n        l.replace_xpath(\"name\", \"//p/text()\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n\n        l.replace_xpath(\"name\", [\"//p/text()\", \"//div/text()\"])\n        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\", \"Marta\"])\n\n    def test_get_xpath(self):\n        l = TestItemLoader(response=self.response)\n        self.assertEqual(l.get_xpath(\"//p/text()\"), [\"paragraph\"])\n        self.assertEqual(l.get_xpath(\"//p/text()\", TakeFirst()), \"paragraph\")\n        self.assertEqual(l.get_xpath(\"//p/text()\", TakeFirst(), re=\"pa\"), \"pa\")\n\n        self.assertEqual(\n            l.get_xpath([\"//p/text()\", \"//div/text()\"]), [\"paragraph\", \"marta\"]\n        )\n\n    def test_replace_xpath_multi_fields(self):\n        l = TestItemLoader(response=self.response)\n        l.add_xpath(None, \"//div/text()\", TakeFirst(), lambda x: {\"name\": x})\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n        l.replace_xpath(None, \"//p/text()\", TakeFirst(), lambda x: {\"name\": x})\n        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n\n    def test_replace_xpath_re(self):\n        l = TestItemLoader(response=self.response)\n        self.assertTrue(l.selector)\n        l.add_xpath(\"name\", \"//div/text()\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n        l.replace_xpath(\"name\", \"//div/text()\", re=\"ma\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n\n    def test_add_css_re(self):\n        l = TestItemLoader(response=self.response)\n        l.add_css(\"name\", \"div::text\", re=\"ma\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n\n        l.add_css(\"url\", \"a::attr(href)\", re=\"http://(.+)\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"www.scrapy.org\"])\n\n    def test_replace_css(self):\n        l = TestItemLoader(response=self.response)\n        self.assertTrue(l.selector)\n        l.add_css(\"name\", \"div::text\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n        l.replace_css(\"name\", \"p::text\")\n        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n\n        l.replace_css(\"name\", [\"p::text\", \"div::text\"])\n        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\", \"Marta\"])\n\n        l.add_css(\"url\", \"a::attr(href)\", re=\"http://(.+)\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"www.scrapy.org\"])\n        l.replace_css(\"url\", \"img::attr(src)\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n\n    def test_get_css(self):\n        l = TestItemLoader(response=self.response)\n        self.assertEqual(l.get_css(\"p::text\"), [\"paragraph\"])\n        self.assertEqual(l.get_css(\"p::text\", TakeFirst()), \"paragraph\")\n        self.assertEqual(l.get_css(\"p::text\", TakeFirst(), re=\"pa\"), \"pa\")\n\n        self.assertEqual(l.get_css([\"p::text\", \"div::text\"]), [\"paragraph\", \"marta\"])\n        self.assertEqual(\n            l.get_css([\"a::attr(href)\", \"img::attr(src)\"]),\n            [\"http://www.scrapy.org\", \"/images/logo.png\"],\n        )\n\n    def test_replace_css_multi_fields(self):\n        l = TestItemLoader(response=self.response)\n        l.add_css(None, \"div::text\", TakeFirst(), lambda x: {\"name\": x})\n        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n        l.replace_css(None, \"p::text\", TakeFirst(), lambda x: {\"name\": x})\n        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n\n        l.add_css(None, \"a::attr(href)\", TakeFirst(), lambda x: {\"url\": x})\n        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n        l.replace_css(None, \"img::attr(src)\", TakeFirst(), lambda x: {\"url\": x})\n        self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n\n    def test_replace_css_re(self):\n        l = TestItemLoader(response=self.response)\n        self.assertTrue(l.selector)\n        l.add_css(\"url\", \"a::attr(href)\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n        l.replace_css(\"url\", \"a::attr(href)\", re=r\"http://www\\.(.+)\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"scrapy.org\"])\n\n\nclass SubselectorLoaderTest(unittest.TestCase):\n    response = HtmlResponse(\n        url=\"\",\n        encoding=\"utf-8\",\n        body=b\"\"\"\n    <html>\n    <body>\n    <header>\n      <div id=\"id\">marta</div>\n      <p>paragraph</p>\n    </header>\n    <footer class=\"footer\">\n      <a href=\"http://www.scrapy.org\">homepage</a>\n      <img src=\"/images/logo.png\" width=\"244\" height=\"65\" alt=\"Scrapy\">\n    </footer>\n    </body>\n    </html>\n    \"\"\",\n    )\n\n    def test_nested_xpath(self):\n        l = NestedItemLoader(response=self.response)\n\n        nl = l.nested_xpath(\"//header\")\n        nl.add_xpath(\"name\", \"div/text()\")\n        nl.add_css(\"name_div\", \"#id\")\n        nl.add_value(\"name_value\", nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n\n        self.assertEqual(l.get_output_value(\"name\"), [\"marta\"])\n        self.assertEqual(l.get_output_value(\"name_div\"), ['<div id=\"id\">marta</div>'])\n        self.assertEqual(l.get_output_value(\"name_value\"), [\"marta\"])\n\n        self.assertEqual(l.get_output_value(\"name\"), nl.get_output_value(\"name\"))\n        self.assertEqual(\n            l.get_output_value(\"name_div\"), nl.get_output_value(\"name_div\")\n        )\n        self.assertEqual(\n            l.get_output_value(\"name_value\"), nl.get_output_value(\"name_value\")\n        )\n\n    def test_nested_css(self):\n        l = NestedItemLoader(response=self.response)\n        nl = l.nested_css(\"header\")\n        nl.add_xpath(\"name\", \"div/text()\")\n        nl.add_css(\"name_div\", \"#id\")\n        nl.add_value(\"name_value\", nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n\n        self.assertEqual(l.get_output_value(\"name\"), [\"marta\"])\n        self.assertEqual(l.get_output_value(\"name_div\"), ['<div id=\"id\">marta</div>'])\n        self.assertEqual(l.get_output_value(\"name_value\"), [\"marta\"])\n\n        self.assertEqual(l.get_output_value(\"name\"), nl.get_output_value(\"name\"))\n        self.assertEqual(\n            l.get_output_value(\"name_div\"), nl.get_output_value(\"name_div\")\n        )\n        self.assertEqual(\n            l.get_output_value(\"name_value\"), nl.get_output_value(\"name_value\")\n        )\n\n    def test_nested_replace(self):\n        l = NestedItemLoader(response=self.response)\n        nl1 = l.nested_xpath(\"//footer\")\n        nl2 = nl1.nested_xpath(\"a\")\n\n        l.add_xpath(\"url\", \"//footer/a/@href\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n        nl1.replace_xpath(\"url\", \"img/@src\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n        nl2.replace_xpath(\"url\", \"@href\")\n        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n\n    def test_nested_ordering(self):\n        l = NestedItemLoader(response=self.response)\n        nl1 = l.nested_xpath(\"//footer\")\n        nl2 = nl1.nested_xpath(\"a\")\n\n        nl1.add_xpath(\"url\", \"img/@src\")\n        l.add_xpath(\"url\", \"//footer/a/@href\")\n        nl2.add_xpath(\"url\", \"text()\")\n        l.add_xpath(\"url\", \"//footer/a/@href\")\n\n        self.assertEqual(\n            l.get_output_value(\"url\"),\n            [\n                \"/images/logo.png\",\n                \"http://www.scrapy.org\",\n                \"homepage\",\n                \"http://www.scrapy.org\",\n            ],\n        )\n\n    def test_nested_load_item(self):\n        l = NestedItemLoader(response=self.response)\n        nl1 = l.nested_xpath(\"//footer\")\n        nl2 = nl1.nested_xpath(\"img\")\n\n        l.add_xpath(\"name\", \"//header/div/text()\")\n        nl1.add_xpath(\"url\", \"a/@href\")\n        nl2.add_xpath(\"image\", \"@src\")\n\n        item = l.load_item()\n\n        assert item is l.item\n        assert item is nl1.item\n        assert item is nl2.item\n\n        self.assertEqual(item[\"name\"], [\"marta\"])\n        self.assertEqual(item[\"url\"], [\"http://www.scrapy.org\"])\n        self.assertEqual(item[\"image\"], [\"/images/logo.png\"])\n\n\n# Functions as processors\n\n\ndef function_processor_strip(iterable):\n    return [x.strip() for x in iterable]\n\n\ndef function_processor_upper(iterable):\n    return [x.upper() for x in iterable]\n\n\nclass FunctionProcessorItem(Item):\n    foo = Field(\n        input_processor=function_processor_strip,\n        output_processor=function_processor_upper,\n    )\n\n\nclass FunctionProcessorItemLoader(ItemLoader):\n    default_item_class = FunctionProcessorItem\n\n\nclass FunctionProcessorTestCase(unittest.TestCase):\n    def test_processor_defined_in_item(self):\n        lo = FunctionProcessorItemLoader()\n        lo.add_value(\"foo\", \"  bar  \")\n        lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n        self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_engine_stop_download_bytes.py": "from testfixtures import LogCapture\nfrom twisted.internet import defer\n\nfrom scrapy.exceptions import StopDownload\nfrom tests.test_engine import (\n    AttrsItemsSpider,\n    CrawlerRun,\n    DataClassItemsSpider,\n    DictItemsSpider,\n    EngineTest,\n    TestSpider,\n)\n\n\nclass BytesReceivedCrawlerRun(CrawlerRun):\n    def bytes_received(self, data, request, spider):\n        super().bytes_received(data, request, spider)\n        raise StopDownload(fail=False)\n\n\nclass BytesReceivedEngineTest(EngineTest):\n    @defer.inlineCallbacks\n    def test_crawler(self):\n        for spider in (\n            TestSpider,\n            DictItemsSpider,\n            AttrsItemsSpider,\n            DataClassItemsSpider,\n        ):\n            run = BytesReceivedCrawlerRun(spider)\n            with LogCapture() as log:\n                yield run.run()\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET http://localhost:{run.portno}/redirected> \"\n                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET http://localhost:{run.portno}/> \"\n                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET http://localhost:{run.portno}/numbers> \"\n                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n                    )\n                )\n            self._assert_visited_urls(run)\n            self._assert_scheduled_requests(run, count=9)\n            self._assert_downloaded_responses(run, count=9)\n            self._assert_signals_caught(run)\n            self._assert_headers_received(run)\n            self._assert_bytes_received(run)\n\n    def _assert_bytes_received(self, run: CrawlerRun):\n        self.assertEqual(9, len(run.bytes))\n        for request, data in run.bytes.items():\n            joined_data = b\"\".join(data)\n            self.assertTrue(len(data) == 1)  # signal was fired only once\n            if run.getpath(request.url) == \"/numbers\":\n                # Received bytes are not the complete response. The exact amount depends\n                # on the buffer size, which can vary, so we only check that the amount\n                # of received bytes is strictly less than the full response.\n                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n                self.assertTrue(len(joined_data) < len(b\"\".join(numbers)))\n", "tests/mockserver.py": "import argparse\nimport json\nimport os\nimport random\nimport sys\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom subprocess import PIPE, Popen\nfrom tempfile import mkdtemp\nfrom typing import Dict\nfrom urllib.parse import urlencode\n\nfrom OpenSSL import SSL\nfrom twisted.internet import defer, reactor, ssl\nfrom twisted.internet.protocol import ServerFactory\nfrom twisted.internet.task import deferLater\nfrom twisted.names import dns, error\nfrom twisted.names.server import DNSServerFactory\nfrom twisted.web import resource, server\nfrom twisted.web.server import NOT_DONE_YET, GzipEncoderFactory, Site\nfrom twisted.web.static import File\nfrom twisted.web.util import redirectTo\n\nfrom scrapy.utils.python import to_bytes, to_unicode\n\n\ndef getarg(request, name, default=None, type=None):\n    if name in request.args:\n        value = request.args[name][0]\n        if type is not None:\n            value = type(value)\n        return value\n    return default\n\n\ndef get_mockserver_env() -> Dict[str, str]:\n    \"\"\"Return a OS environment dict suitable to run mockserver processes.\"\"\"\n\n    tests_path = Path(__file__).parent.parent\n    pythonpath = str(tests_path) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = pythonpath\n    return env\n\n\n# most of the following resources are copied from twisted.web.test.test_webclient\nclass ForeverTakingResource(resource.Resource):\n    \"\"\"\n    L{ForeverTakingResource} is a resource which never finishes responding\n    to requests.\n    \"\"\"\n\n    def __init__(self, write=False):\n        resource.Resource.__init__(self)\n        self._write = write\n\n    def render(self, request):\n        if self._write:\n            request.write(b\"some bytes\")\n        return server.NOT_DONE_YET\n\n\nclass ErrorResource(resource.Resource):\n    def render(self, request):\n        request.setResponseCode(401)\n        if request.args.get(b\"showlength\"):\n            request.setHeader(b\"content-length\", b\"0\")\n        return b\"\"\n\n\nclass NoLengthResource(resource.Resource):\n    def render(self, request):\n        return b\"nolength\"\n\n\nclass HostHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of the host header\n    from the request.\n    \"\"\"\n\n    def render(self, request):\n        return request.requestHeaders.getRawHeaders(b\"host\")[0]\n\n\nclass PayloadResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the contents of the request body\n    as long as the request body is 100 bytes long, otherwise which renders\n    itself as C{\"ERROR\"}.\n    \"\"\"\n\n    def render(self, request):\n        data = request.content.read()\n        contentLength = request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n        if len(data) != 100 or int(contentLength) != 100:\n            return b\"ERROR\"\n        return data\n\n\nclass BrokenDownloadResource(resource.Resource):\n    def render(self, request):\n        # only sends 3 bytes even though it claims to send 5\n        request.setHeader(b\"content-length\", b\"5\")\n        request.write(b\"abc\")\n        return b\"\"\n\n\nclass LeafResource(resource.Resource):\n    isLeaf = True\n\n    def deferRequest(self, request, delay, f, *a, **kw):\n        def _cancelrequest(_):\n            # silence CancelledError\n            d.addErrback(lambda _: None)\n            d.cancel()\n\n        d = deferLater(reactor, delay, f, *a, **kw)\n        request.notifyFinish().addErrback(_cancelrequest)\n        return d\n\n\nclass Follow(LeafResource):\n    def render(self, request):\n        total = getarg(request, b\"total\", 100, type=int)\n        show = getarg(request, b\"show\", 1, type=int)\n        order = getarg(request, b\"order\", b\"desc\")\n        maxlatency = getarg(request, b\"maxlatency\", 0, type=float)\n        n = getarg(request, b\"n\", total, type=int)\n        if order == b\"rand\":\n            nlist = [random.randint(1, total) for _ in range(show)]\n        else:  # order == \"desc\"\n            nlist = range(n, max(n - show, 0), -1)\n\n        lag = random.random() * maxlatency\n        self.deferRequest(request, lag, self.renderRequest, request, nlist)\n        return NOT_DONE_YET\n\n    def renderRequest(self, request, nlist):\n        s = \"\"\"<html> <head></head> <body>\"\"\"\n        args = request.args.copy()\n        for nl in nlist:\n            args[b\"n\"] = [to_bytes(str(nl))]\n            argstr = urlencode(args, doseq=True)\n            s += f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n        s += \"\"\"</body>\"\"\"\n        request.write(to_bytes(s))\n        request.finish()\n\n\nclass Delay(LeafResource):\n    def render_GET(self, request):\n        n = getarg(request, b\"n\", 1, type=float)\n        b = getarg(request, b\"b\", 1, type=int)\n        if b:\n            # send headers now and delay body\n            request.write(\"\")\n        self.deferRequest(request, n, self._delayedRender, request, n)\n        return NOT_DONE_YET\n\n    def _delayedRender(self, request, n):\n        request.write(to_bytes(f\"Response delayed for {n:.3f} seconds\\n\"))\n        request.finish()\n\n\nclass Status(LeafResource):\n    def render_GET(self, request):\n        n = getarg(request, b\"n\", 200, type=int)\n        request.setResponseCode(n)\n        return b\"\"\n\n\nclass Raw(LeafResource):\n    def render_GET(self, request):\n        request.startedWriting = 1\n        self.deferRequest(request, 0, self._delayedRender, request)\n        return NOT_DONE_YET\n\n    render_POST = render_GET\n\n    def _delayedRender(self, request):\n        raw = getarg(request, b\"raw\", b\"HTTP 1.1 200 OK\\n\")\n        request.startedWriting = 1\n        request.write(raw)\n        request.channel.transport.loseConnection()\n        request.finish()\n\n\nclass Echo(LeafResource):\n    def render_GET(self, request):\n        output = {\n            \"headers\": {\n                to_unicode(k): [to_unicode(v) for v in vs]\n                for k, vs in request.requestHeaders.getAllRawHeaders()\n            },\n            \"body\": to_unicode(request.content.read()),\n        }\n        return to_bytes(json.dumps(output))\n\n    render_POST = render_GET\n\n\nclass RedirectTo(LeafResource):\n    def render(self, request):\n        goto = getarg(request, b\"goto\", b\"/\")\n        # we force the body content, otherwise Twisted redirectTo()\n        # returns HTML with <meta http-equiv=\"refresh\"\n        redirectTo(goto, request)\n        return b\"redirecting...\"\n\n\nclass Partial(LeafResource):\n    def render_GET(self, request):\n        request.setHeader(b\"Content-Length\", b\"1024\")\n        self.deferRequest(request, 0, self._delayedRender, request)\n        return NOT_DONE_YET\n\n    def _delayedRender(self, request):\n        request.write(b\"partial content\\n\")\n        request.finish()\n\n\nclass Drop(Partial):\n    def _delayedRender(self, request):\n        abort = getarg(request, b\"abort\", 0, type=int)\n        request.write(b\"this connection will be dropped\\n\")\n        tr = request.channel.transport\n        try:\n            if abort and hasattr(tr, \"abortConnection\"):\n                tr.abortConnection()\n            else:\n                tr.loseConnection()\n        finally:\n            request.finish()\n\n\nclass ArbitraryLengthPayloadResource(LeafResource):\n    def render(self, request):\n        return request.content.read()\n\n\nclass Root(resource.Resource):\n    def __init__(self):\n        resource.Resource.__init__(self)\n        self.putChild(b\"status\", Status())\n        self.putChild(b\"follow\", Follow())\n        self.putChild(b\"delay\", Delay())\n        self.putChild(b\"partial\", Partial())\n        self.putChild(b\"drop\", Drop())\n        self.putChild(b\"raw\", Raw())\n        self.putChild(b\"echo\", Echo())\n        self.putChild(b\"payload\", PayloadResource())\n        self.putChild(\n            b\"xpayload\",\n            resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]),\n        )\n        self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n        try:\n            from tests import tests_datadir\n\n            self.putChild(b\"files\", File(str(Path(tests_datadir, \"test_site/files/\"))))\n        except Exception:\n            pass\n        self.putChild(b\"redirect-to\", RedirectTo())\n\n    def getChild(self, name, request):\n        return self\n\n    def render(self, request):\n        return b\"Scrapy mock HTTP server\\n\"\n\n\nclass MockServer:\n    def __enter__(self):\n        self.proc = Popen(\n            [sys.executable, \"-u\", \"-m\", \"tests.mockserver\", \"-t\", \"http\"],\n            stdout=PIPE,\n            env=get_mockserver_env(),\n        )\n        http_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n        https_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n\n        self.http_address = http_address\n        self.https_address = https_address\n\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.communicate()\n\n    def url(self, path, is_secure=False):\n        host = self.https_address if is_secure else self.http_address\n        host = host.replace(\"0.0.0.0\", \"127.0.0.1\")\n        return host + path\n\n\nclass MockDNSResolver:\n    \"\"\"\n    Implements twisted.internet.interfaces.IResolver partially\n    \"\"\"\n\n    def _resolve(self, name):\n        record = dns.Record_A(address=b\"127.0.0.1\")\n        answer = dns.RRHeader(name=name, payload=record)\n        return [answer], [], []\n\n    def query(self, query, timeout=None):\n        if query.type == dns.A:\n            return defer.succeed(self._resolve(query.name.name))\n        return defer.fail(error.DomainError())\n\n    def lookupAllRecords(self, name, timeout=None):\n        return defer.succeed(self._resolve(name))\n\n\nclass MockDNSServer:\n    def __enter__(self):\n        self.proc = Popen(\n            [sys.executable, \"-u\", \"-m\", \"tests.mockserver\", \"-t\", \"dns\"],\n            stdout=PIPE,\n            env=get_mockserver_env(),\n        )\n        self.host = \"127.0.0.1\"\n        self.port = int(\n            self.proc.stdout.readline().strip().decode(\"ascii\").split(\":\")[1]\n        )\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.communicate()\n\n\nclass MockFTPServer:\n    \"\"\"Creates an FTP server on port 2121 with a default passwordless user\n    (anonymous) and a temporary root path that you can read from the\n    :attr:`path` attribute.\"\"\"\n\n    def __enter__(self):\n        self.path = Path(mkdtemp())\n        self.proc = Popen(\n            [sys.executable, \"-u\", \"-m\", \"tests.ftpserver\", \"-d\", str(self.path)],\n            stderr=PIPE,\n            env=get_mockserver_env(),\n        )\n        for line in self.proc.stderr:\n            if b\"starting FTP server\" in line:\n                break\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        rmtree(str(self.path))\n        self.proc.kill()\n        self.proc.communicate()\n\n    def url(self, path):\n        return \"ftp://127.0.0.1:2121/\" + path\n\n\ndef ssl_context_factory(\n    keyfile=\"keys/localhost.key\", certfile=\"keys/localhost.crt\", cipher_string=None\n):\n    factory = ssl.DefaultOpenSSLContextFactory(\n        str(Path(__file__).parent / keyfile),\n        str(Path(__file__).parent / certfile),\n    )\n    if cipher_string:\n        ctx = factory.getContext()\n        # disabling TLS1.3 because it unconditionally enables some strong ciphers\n        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_3)\n        ctx.set_cipher_list(to_bytes(cipher_string))\n    return factory\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-t\", \"--type\", type=str, choices=(\"http\", \"dns\"), default=\"http\"\n    )\n    args = parser.parse_args()\n\n    factory: ServerFactory\n\n    if args.type == \"http\":\n        root = Root()\n        factory = Site(root)\n        httpPort = reactor.listenTCP(0, factory)\n        contextFactory = ssl_context_factory()\n        httpsPort = reactor.listenSSL(0, factory, contextFactory)\n\n        def print_listening():\n            httpHost = httpPort.getHost()\n            httpsHost = httpsPort.getHost()\n            httpAddress = f\"http://{httpHost.host}:{httpHost.port}\"\n            httpsAddress = f\"https://{httpsHost.host}:{httpsHost.port}\"\n            print(httpAddress)\n            print(httpsAddress)\n\n    elif args.type == \"dns\":\n        clients = [MockDNSResolver()]\n        factory = DNSServerFactory(clients=clients)\n        protocol = dns.DNSDatagramProtocol(controller=factory)\n        listener = reactor.listenUDP(0, protocol)\n\n        def print_listening():\n            host = listener.getHost()\n            print(f\"{host.host}:{host.port}\")\n\n    reactor.callWhenRunning(print_listening)\n    reactor.run()\n", "tests/test_spidermiddleware_output_chain.py": "from testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy import Request, Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\n\n\nclass LogExceptionMiddleware:\n    def process_spider_exception(self, response, exception, spider):\n        spider.logger.info(\n            \"Middleware: %s exception caught\", exception.__class__.__name__\n        )\n        return None\n\n\n# ================================================================================\n# (0) recover from an exception on a spider callback\nclass RecoveryMiddleware:\n    def process_spider_exception(self, response, exception, spider):\n        spider.logger.info(\n            \"Middleware: %s exception caught\", exception.__class__.__name__\n        )\n        return [\n            {\"from\": \"process_spider_exception\"},\n            Request(response.url, meta={\"dont_fail\": True}, dont_filter=True),\n        ]\n\n\nclass RecoverySpider(Spider):\n    name = \"RecoverySpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES_BASE\": {},\n        \"SPIDER_MIDDLEWARES\": {\n            RecoveryMiddleware: 10,\n        },\n    }\n\n    def start_requests(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        yield {\"test\": 1}\n        self.logger.info(\"DONT_FAIL: %s\", response.meta.get(\"dont_fail\"))\n        if not response.meta.get(\"dont_fail\"):\n            raise TabError()\n\n\nclass RecoveryAsyncGenSpider(RecoverySpider):\n    name = \"RecoveryAsyncGenSpider\"\n\n    async def parse(self, response):\n        for r in super().parse(response):\n            yield r\n\n\n# ================================================================================\n# (1) exceptions from a spider middleware's process_spider_input method\nclass FailProcessSpiderInputMiddleware:\n    def process_spider_input(self, response, spider):\n        spider.logger.info(\"Middleware: will raise IndexError\")\n        raise IndexError()\n\n\nclass ProcessSpiderInputSpiderWithoutErrback(Spider):\n    name = \"ProcessSpiderInputSpiderWithoutErrback\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            # spider\n            FailProcessSpiderInputMiddleware: 8,\n            LogExceptionMiddleware: 6,\n            # engine\n        }\n    }\n\n    def start_requests(self):\n        yield Request(url=self.mockserver.url(\"/status?n=200\"), callback=self.parse)\n\n    def parse(self, response):\n        return {\"from\": \"callback\"}\n\n\nclass ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback):\n    name = \"ProcessSpiderInputSpiderWithErrback\"\n\n    def start_requests(self):\n        yield Request(\n            self.mockserver.url(\"/status?n=200\"), self.parse, errback=self.errback\n        )\n\n    def errback(self, failure):\n        self.logger.info(\"Got a Failure on the Request errback\")\n        return {\"from\": \"errback\"}\n\n\n# ================================================================================\n# (2) exceptions from a spider callback (generator)\nclass GeneratorCallbackSpider(Spider):\n    name = \"GeneratorCallbackSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 10,\n        },\n    }\n\n    def start_requests(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        yield {\"test\": 1}\n        yield {\"test\": 2}\n        raise ImportError()\n\n\nclass AsyncGeneratorCallbackSpider(GeneratorCallbackSpider):\n    async def parse(self, response):\n        yield {\"test\": 1}\n        yield {\"test\": 2}\n        raise ImportError()\n\n\n# ================================================================================\n# (2.1) exceptions from a spider callback (generator, middleware right after callback)\nclass GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider):\n    name = \"GeneratorCallbackSpiderMiddlewareRightAfterSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 100000,\n        },\n    }\n\n\n# ================================================================================\n# (3) exceptions from a spider callback (not a generator)\nclass NotGeneratorCallbackSpider(Spider):\n    name = \"NotGeneratorCallbackSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 10,\n        },\n    }\n\n    def start_requests(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        return [{\"test\": 1}, {\"test\": 1 / 0}]\n\n\n# ================================================================================\n# (3.1) exceptions from a spider callback (not a generator, middleware right after callback)\nclass NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackSpider):\n    name = \"NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            LogExceptionMiddleware: 100000,\n        },\n    }\n\n\n# ================================================================================\n# (4) exceptions from a middleware process_spider_output method (generator)\nclass _GeneratorDoNothingMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            yield r\n\n    def process_spider_exception(self, response, exception, spider):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n        return None\n\n\nclass GeneratorFailMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            yield r\n            raise LookupError()\n\n    def process_spider_exception(self, response, exception, spider):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n        yield {\"processed\": [method]}\n\n\nclass GeneratorDoNothingAfterFailureMiddleware(_GeneratorDoNothingMiddleware):\n    pass\n\n\nclass GeneratorRecoverMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            yield r\n\n    def process_spider_exception(self, response, exception, spider):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n        yield {\"processed\": [method]}\n\n\nclass GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):\n    pass\n\n\nclass GeneratorOutputChainSpider(Spider):\n    name = \"GeneratorOutputChainSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            GeneratorFailMiddleware: 10,\n            GeneratorDoNothingAfterFailureMiddleware: 8,\n            GeneratorRecoverMiddleware: 5,\n            GeneratorDoNothingAfterRecoveryMiddleware: 3,\n        },\n    }\n\n    def start_requests(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        yield {\"processed\": [\"parse-first-item\"]}\n        yield {\"processed\": [\"parse-second-item\"]}\n\n\n# ================================================================================\n# (5) exceptions from a middleware process_spider_output method (not generator)\n\n\nclass _NotGeneratorDoNothingMiddleware:\n    def process_spider_output(self, response, result, spider):\n        out = []\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            out.append(r)\n        return out\n\n    def process_spider_exception(self, response, exception, spider):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n        return None\n\n\nclass NotGeneratorFailMiddleware:\n    def process_spider_output(self, response, result, spider):\n        out = []\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            out.append(r)\n        raise ReferenceError()\n        return out\n\n    def process_spider_exception(self, response, exception, spider):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n        return [{\"processed\": [method]}]\n\n\nclass NotGeneratorDoNothingAfterFailureMiddleware(_NotGeneratorDoNothingMiddleware):\n    pass\n\n\nclass NotGeneratorRecoverMiddleware:\n    def process_spider_output(self, response, result, spider):\n        out = []\n        for r in result:\n            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n            out.append(r)\n        return out\n\n    def process_spider_exception(self, response, exception, spider):\n        method = f\"{self.__class__.__name__}.process_spider_exception\"\n        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n        return [{\"processed\": [method]}]\n\n\nclass NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddleware):\n    pass\n\n\nclass NotGeneratorOutputChainSpider(Spider):\n    name = \"NotGeneratorOutputChainSpider\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\": {\n            NotGeneratorFailMiddleware: 10,\n            NotGeneratorDoNothingAfterFailureMiddleware: 8,\n            NotGeneratorRecoverMiddleware: 5,\n            NotGeneratorDoNothingAfterRecoveryMiddleware: 3,\n        },\n    }\n\n    def start_requests(self):\n        return [Request(self.mockserver.url(\"/status?n=200\"))]\n\n    def parse(self, response):\n        return [\n            {\"processed\": [\"parse-first-item\"]},\n            {\"processed\": [\"parse-second-item\"]},\n        ]\n\n\n# ================================================================================\nclass TestSpiderMiddleware(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.mockserver = MockServer()\n        cls.mockserver.__enter__()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def crawl_log(self, spider):\n        crawler = get_crawler(spider)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        return log\n\n    @defer.inlineCallbacks\n    def test_recovery(self):\n        \"\"\"\n        (0) Recover from an exception in a spider callback. The final item count should be 3\n        (one yielded from the callback method before the exception is raised, one directly\n        from the recovery middleware and one from the spider when processing the request that\n        was enqueued from the recovery middleware)\n        \"\"\"\n        log = yield self.crawl_log(RecoverySpider)\n        self.assertIn(\"Middleware: TabError exception caught\", str(log))\n        self.assertEqual(str(log).count(\"Middleware: TabError exception caught\"), 1)\n        self.assertIn(\"'item_scraped_count': 3\", str(log))\n\n    @defer.inlineCallbacks\n    def test_recovery_asyncgen(self):\n        \"\"\"\n        Same as test_recovery but with an async callback.\n        \"\"\"\n        log = yield self.crawl_log(RecoveryAsyncGenSpider)\n        self.assertIn(\"Middleware: TabError exception caught\", str(log))\n        self.assertEqual(str(log).count(\"Middleware: TabError exception caught\"), 1)\n        self.assertIn(\"'item_scraped_count': 3\", str(log))\n\n    @defer.inlineCallbacks\n    def test_process_spider_input_without_errback(self):\n        \"\"\"\n        (1.1) An exception from the process_spider_input chain should be caught by the\n        process_spider_exception chain from the start if the Request has no errback\n        \"\"\"\n        log1 = yield self.crawl_log(ProcessSpiderInputSpiderWithoutErrback)\n        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n        self.assertIn(\"Middleware: IndexError exception caught\", str(log1))\n\n    @defer.inlineCallbacks\n    def test_process_spider_input_with_errback(self):\n        \"\"\"\n        (1.2) An exception from the process_spider_input chain should not be caught by the\n        process_spider_exception chain if the Request has an errback\n        \"\"\"\n        log1 = yield self.crawl_log(ProcessSpiderInputSpiderWithErrback)\n        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n        self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n        self.assertIn(\"{'from': 'errback'}\", str(log1))\n        self.assertNotIn(\"{'from': 'callback'}\", str(log1))\n        self.assertIn(\"'item_scraped_count': 1\", str(log1))\n\n    @defer.inlineCallbacks\n    def test_generator_callback(self):\n        \"\"\"\n        (2) An exception from a spider callback (returning a generator) should\n        be caught by the process_spider_exception chain. Items yielded before the\n        exception is raised should be processed normally.\n        \"\"\"\n        log2 = yield self.crawl_log(GeneratorCallbackSpider)\n        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n\n    @defer.inlineCallbacks\n    def test_async_generator_callback(self):\n        \"\"\"\n        Same as test_generator_callback but with an async callback.\n        \"\"\"\n        log2 = yield self.crawl_log(AsyncGeneratorCallbackSpider)\n        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n\n    @defer.inlineCallbacks\n    def test_generator_callback_right_after_callback(self):\n        \"\"\"\n        (2.1) Special case of (2): Exceptions should be caught\n        even if the middleware is placed right after the spider\n        \"\"\"\n        log21 = yield self.crawl_log(GeneratorCallbackSpiderMiddlewareRightAfterSpider)\n        self.assertIn(\"Middleware: ImportError exception caught\", str(log21))\n        self.assertIn(\"'item_scraped_count': 2\", str(log21))\n\n    @defer.inlineCallbacks\n    def test_not_a_generator_callback(self):\n        \"\"\"\n        (3) An exception from a spider callback (returning a list) should\n        be caught by the process_spider_exception chain. No items should be processed.\n        \"\"\"\n        log3 = yield self.crawl_log(NotGeneratorCallbackSpider)\n        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log3))\n        self.assertNotIn(\"item_scraped_count\", str(log3))\n\n    @defer.inlineCallbacks\n    def test_not_a_generator_callback_right_after_callback(self):\n        \"\"\"\n        (3.1) Special case of (3): Exceptions should be caught\n        even if the middleware is placed right after the spider\n        \"\"\"\n        log31 = yield self.crawl_log(\n            NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\n        )\n        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log31))\n        self.assertNotIn(\"item_scraped_count\", str(log31))\n\n    @defer.inlineCallbacks\n    def test_generator_output_chain(self):\n        \"\"\"\n        (4) An exception from a middleware's process_spider_output method should be sent\n        to the process_spider_exception method from the next middleware in the chain.\n        The result of the recovery by the process_spider_exception method should be handled\n        by the process_spider_output method from the next middleware.\n        The final item count should be 2 (one from the spider callback and one from the\n        process_spider_exception chain)\n        \"\"\"\n        log4 = yield self.crawl_log(GeneratorOutputChainSpider)\n        self.assertIn(\"'item_scraped_count': 2\", str(log4))\n        self.assertIn(\n            \"GeneratorRecoverMiddleware.process_spider_exception: LookupError caught\",\n            str(log4),\n        )\n        self.assertIn(\n            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: LookupError caught\",\n            str(log4),\n        )\n        self.assertNotIn(\n            \"GeneratorFailMiddleware.process_spider_exception: LookupError caught\",\n            str(log4),\n        )\n        self.assertNotIn(\n            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: LookupError caught\",\n            str(log4),\n        )\n        item_from_callback = {\n            \"processed\": [\n                \"parse-first-item\",\n                \"GeneratorFailMiddleware.process_spider_output\",\n                \"GeneratorDoNothingAfterFailureMiddleware.process_spider_output\",\n                \"GeneratorRecoverMiddleware.process_spider_output\",\n                \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n            ]\n        }\n        item_recovered = {\n            \"processed\": [\n                \"GeneratorRecoverMiddleware.process_spider_exception\",\n                \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n            ]\n        }\n        self.assertIn(str(item_from_callback), str(log4))\n        self.assertIn(str(item_recovered), str(log4))\n        self.assertNotIn(\"parse-second-item\", str(log4))\n\n    @defer.inlineCallbacks\n    def test_not_a_generator_output_chain(self):\n        \"\"\"\n        (5) An exception from a middleware's process_spider_output method should be sent\n        to the process_spider_exception method from the next middleware in the chain.\n        The result of the recovery by the process_spider_exception method should be handled\n        by the process_spider_output method from the next middleware.\n        The final item count should be 1 (from the process_spider_exception chain, the items\n        from the spider callback are lost)\n        \"\"\"\n        log5 = yield self.crawl_log(NotGeneratorOutputChainSpider)\n        self.assertIn(\"'item_scraped_count': 1\", str(log5))\n        self.assertIn(\n            \"GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught\",\n            str(log5),\n        )\n        self.assertIn(\n            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: ReferenceError caught\",\n            str(log5),\n        )\n        self.assertNotIn(\n            \"GeneratorFailMiddleware.process_spider_exception: ReferenceError caught\",\n            str(log5),\n        )\n        self.assertNotIn(\n            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: ReferenceError caught\",\n            str(log5),\n        )\n        item_recovered = {\n            \"processed\": [\n                \"NotGeneratorRecoverMiddleware.process_spider_exception\",\n                \"NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n            ]\n        }\n        self.assertIn(str(item_recovered), str(log5))\n        self.assertNotIn(\"parse-first-item\", str(log5))\n        self.assertNotIn(\"parse-second-item\", str(log5))\n", "tests/test_utils_asyncio.py": "import asyncio\nimport warnings\nfrom unittest import TestCase\n\nfrom pytest import mark\n\nfrom scrapy.utils.defer import deferred_f_from_coro_f\nfrom scrapy.utils.reactor import (\n    install_reactor,\n    is_asyncio_reactor_installed,\n    set_asyncio_event_loop,\n)\n\n\n@mark.usefixtures(\"reactor_pytest\")\nclass AsyncioTest(TestCase):\n    def test_is_asyncio_reactor_installed(self):\n        # the result should depend only on the pytest --reactor argument\n        self.assertEqual(\n            is_asyncio_reactor_installed(), self.reactor_pytest == \"asyncio\"\n        )\n\n    def test_install_asyncio_reactor(self):\n        from twisted.internet import reactor as original_reactor\n\n        with warnings.catch_warnings(record=True) as w:\n            install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n            self.assertEqual(len(w), 0)\n        from twisted.internet import reactor\n\n        assert original_reactor == reactor\n\n    @mark.only_asyncio()\n    @deferred_f_from_coro_f\n    async def test_set_asyncio_event_loop(self):\n        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n        assert set_asyncio_event_loop(None) is asyncio.get_running_loop()\n", "tests/test_utils_project.py": "import contextlib\nimport os\nimport shutil\nimport tempfile\nimport unittest\nimport warnings\nfrom pathlib import Path\n\nfrom scrapy.utils.misc import set_environ\nfrom scrapy.utils.project import data_path, get_project_settings\n\n\n@contextlib.contextmanager\ndef inside_a_project():\n    prev_dir = os.getcwd()\n    project_dir = tempfile.mkdtemp()\n\n    try:\n        os.chdir(project_dir)\n        Path(\"scrapy.cfg\").touch()\n\n        yield project_dir\n    finally:\n        os.chdir(prev_dir)\n        shutil.rmtree(project_dir)\n\n\nclass ProjectUtilsTest(unittest.TestCase):\n    def test_data_path_outside_project(self):\n        self.assertEqual(str(Path(\".scrapy\", \"somepath\")), data_path(\"somepath\"))\n        abspath = str(Path(os.path.sep, \"absolute\", \"path\"))\n        self.assertEqual(abspath, data_path(abspath))\n\n    def test_data_path_inside_project(self):\n        with inside_a_project() as proj_path:\n            expected = Path(proj_path, \".scrapy\", \"somepath\")\n            self.assertEqual(expected.resolve(), Path(data_path(\"somepath\")).resolve())\n            abspath = str(Path(os.path.sep, \"absolute\", \"path\").resolve())\n            self.assertEqual(abspath, data_path(abspath))\n\n\nclass GetProjectSettingsTestCase(unittest.TestCase):\n    def test_valid_envvar(self):\n        value = \"tests.test_cmdline.settings\"\n        envvars = {\n            \"SCRAPY_SETTINGS_MODULE\": value,\n        }\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            with set_environ(**envvars):\n                settings = get_project_settings()\n\n        assert settings.get(\"SETTINGS_MODULE\") == value\n\n    def test_invalid_envvar(self):\n        envvars = {\n            \"SCRAPY_FOO\": \"bar\",\n        }\n        with set_environ(**envvars):\n            settings = get_project_settings()\n\n        assert settings.get(\"SCRAPY_FOO\") is None\n\n    def test_valid_and_invalid_envvars(self):\n        value = \"tests.test_cmdline.settings\"\n        envvars = {\n            \"SCRAPY_FOO\": \"bar\",\n            \"SCRAPY_SETTINGS_MODULE\": value,\n        }\n        with set_environ(**envvars):\n            settings = get_project_settings()\n        assert settings.get(\"SETTINGS_MODULE\") == value\n        assert settings.get(\"SCRAPY_FOO\") is None\n", "tests/test_utils_request.py": "import json\nimport unittest\nimport warnings\nfrom hashlib import sha1\nfrom typing import Dict, Optional, Tuple, Union\nfrom weakref import WeakKeyDictionary\n\nfrom scrapy.http import Request\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.request import (\n    _fingerprint_cache,\n    fingerprint,\n    request_authenticate,\n    request_httprepr,\n    request_to_curl,\n)\nfrom scrapy.utils.test import get_crawler\n\n\nclass UtilsRequestTest(unittest.TestCase):\n    def test_request_authenticate(self):\n        r = Request(\"http://www.example.com\")\n        request_authenticate(r, \"someuser\", \"somepass\")\n        self.assertEqual(r.headers[\"Authorization\"], b\"Basic c29tZXVzZXI6c29tZXBhc3M=\")\n\n    def test_request_httprepr(self):\n        r1 = Request(\"http://www.example.com\")\n        self.assertEqual(\n            request_httprepr(r1), b\"GET / HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n\"\n        )\n\n        r1 = Request(\"http://www.example.com/some/page.html?arg=1\")\n        self.assertEqual(\n            request_httprepr(r1),\n            b\"GET /some/page.html?arg=1 HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n\",\n        )\n\n        r1 = Request(\n            \"http://www.example.com\",\n            method=\"POST\",\n            headers={\"Content-type\": b\"text/html\"},\n            body=b\"Some body\",\n        )\n        self.assertEqual(\n            request_httprepr(r1),\n            b\"POST / HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n        )\n\n    def test_request_httprepr_for_non_http_request(self):\n        # the representation is not important but it must not fail.\n        request_httprepr(Request(\"file:///tmp/foo.txt\"))\n        request_httprepr(Request(\"ftp://localhost/tmp/foo.txt\"))\n\n\nclass FingerprintTest(unittest.TestCase):\n    maxDiff = None\n\n    function: staticmethod = staticmethod(fingerprint)\n    cache: Union[\n        \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\",\n        \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\",\n    ] = _fingerprint_cache\n    default_cache_key = (None, False)\n    known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n        (\n            Request(\"http://example.org\"),\n            b\"xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\"),\n            b\"\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a\"),\n            b\"G\\xad\\xb8Ck\\x19\\x1c\\xed\\x838,\\x01\\xc4\\xde;\\xee\\xa5\\x94a\\x0c\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a=b\"),\n            b\"\\x024MYb\\x8a\\xc2\\x1e\\xbc>\\xd6\\xac*\\xda\\x9cF\\xc1r\\x7f\\x17\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a=b&a\"),\n            b\"t+\\xe8*\\xfb\\x84\\xe3v\\x1a}\\x88p\\xc0\\xccB\\xd7\\x9d\\xfez\\x96\",\n            {},\n        ),\n        (\n            Request(\"https://example.org?a=b&a=c\"),\n            b\"\\xda\\x1ec\\xd0\\x9c\\x08s`\\xb4\\x9b\\xe2\\xb6R\\xf8k\\xef\\xeaQG\\xef\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\", method=\"POST\"),\n            b\"\\x9d\\xcdA\\x0fT\\x02:\\xca\\xa0}\\x90\\xda\\x05B\\xded\\x8aN7\\x1d\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\", body=b\"a\"),\n            b\"\\xc34z>\\xd8\\x99\\x8b\\xda7\\x05r\\x99I\\xa8\\xa0x;\\xa41_\",\n            {},\n        ),\n        (\n            Request(\"https://example.org\", method=\"POST\", body=b\"a\"),\n            b\"5`\\xe2y4\\xd0\\x9d\\xee\\xe0\\xbatw\\x87Q\\xe8O\\xd78\\xfc\\xe7\",\n            {},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l\",\n            {},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"]\\xc7\\x1f\\xf2\\xafG2\\xbc\\xa4\\xfa\\x99\\n33\\xda\\x18\\x94\\x81U.\",\n            {\"include_headers\": [\"A\"]},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"<\\x1a\\xeb\\x85y\\xdeW\\xfb\\xdcq\\x88\\xee\\xaf\\x17\\xdd\\x0c\\xbfH\\x18\\x1f\",\n            {\"keep_fragments\": True},\n        ),\n        (\n            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n            b\"\\xc1\\xef~\\x94\\x9bS\\xc1\\x83\\t\\xdcz8\\x9f\\xdc{\\x11\\x16I.\\x11\",\n            {\"include_headers\": [\"A\"], \"keep_fragments\": True},\n        ),\n        (\n            Request(\"https://example.org/ab\"),\n            b\"N\\xe5l\\xb8\\x12@iw\\xe2\\xf3\\x1bp\\xea\\xffp!u\\xe2\\x8a\\xc6\",\n            {},\n        ),\n        (\n            Request(\"https://example.org/a\", body=b\"b\"),\n            b\"_NOv\\xbco$6\\xfcW\\x9f\\xb24g\\x9f\\xbb\\xdd\\xa82\\xc5\",\n            {},\n        ),\n    )\n\n    def test_query_string_key_order(self):\n        r1 = Request(\"http://www.example.com/query?id=111&cat=222\")\n        r2 = Request(\"http://www.example.com/query?cat=222&id=111\")\n        self.assertEqual(self.function(r1), self.function(r1))\n        self.assertEqual(self.function(r1), self.function(r2))\n\n    def test_query_string_key_without_value(self):\n        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78132,199\")\n        r2 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n        self.assertNotEqual(self.function(r1), self.function(r2))\n\n    def test_caching(self):\n        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n        self.assertEqual(self.function(r1), self.cache[r1][self.default_cache_key])\n\n    def test_header(self):\n        r1 = Request(\"http://www.example.com/members/offers.html\")\n        r2 = Request(\"http://www.example.com/members/offers.html\")\n        r2.headers[\"SESSIONID\"] = b\"somehash\"\n        self.assertEqual(self.function(r1), self.function(r2))\n\n    def test_headers(self):\n        r1 = Request(\"http://www.example.com/\")\n        r2 = Request(\"http://www.example.com/\")\n        r2.headers[\"Accept-Language\"] = b\"en\"\n        r3 = Request(\"http://www.example.com/\")\n        r3.headers[\"Accept-Language\"] = b\"en\"\n        r3.headers[\"SESSIONID\"] = b\"somehash\"\n\n        self.assertEqual(self.function(r1), self.function(r2), self.function(r3))\n\n        self.assertEqual(\n            self.function(r1), self.function(r1, include_headers=[\"Accept-Language\"])\n        )\n\n        self.assertNotEqual(\n            self.function(r1), self.function(r2, include_headers=[\"Accept-Language\"])\n        )\n\n        self.assertEqual(\n            self.function(r3, include_headers=[\"accept-language\", \"sessionid\"]),\n            self.function(r3, include_headers=[\"SESSIONID\", \"Accept-Language\"]),\n        )\n\n    def test_fragment(self):\n        r1 = Request(\"http://www.example.com/test.html\")\n        r2 = Request(\"http://www.example.com/test.html#fragment\")\n        self.assertEqual(self.function(r1), self.function(r2))\n        self.assertEqual(self.function(r1), self.function(r1, keep_fragments=True))\n        self.assertNotEqual(self.function(r2), self.function(r2, keep_fragments=True))\n        self.assertNotEqual(self.function(r1), self.function(r2, keep_fragments=True))\n\n    def test_method_and_body(self):\n        r1 = Request(\"http://www.example.com\")\n        r2 = Request(\"http://www.example.com\", method=\"POST\")\n        r3 = Request(\"http://www.example.com\", method=\"POST\", body=b\"request body\")\n\n        self.assertNotEqual(self.function(r1), self.function(r2))\n        self.assertNotEqual(self.function(r2), self.function(r3))\n\n    def test_request_replace(self):\n        # cached fingerprint must be cleared on request copy\n        r1 = Request(\"http://www.example.com\")\n        fp1 = self.function(r1)\n        r2 = r1.replace(url=\"http://www.example.com/other\")\n        fp2 = self.function(r2)\n        self.assertNotEqual(fp1, fp2)\n\n    def test_part_separation(self):\n        # An old implementation used to serialize request data in a way that\n        # would put the body right after the URL.\n        r1 = Request(\"http://www.example.com/foo\")\n        fp1 = self.function(r1)\n        r2 = Request(\"http://www.example.com/f\", body=b\"oo\")\n        fp2 = self.function(r2)\n        self.assertNotEqual(fp1, fp2)\n\n    def test_hashes(self):\n        \"\"\"Test hardcoded hashes, to make sure future changes to not introduce\n        backward incompatibilities.\"\"\"\n        actual = [\n            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes\n        ]\n        expected = [_fingerprint for _, _fingerprint, _ in self.known_hashes]\n        self.assertEqual(actual, expected)\n\n\nREQUEST_OBJECTS_TO_TEST = (\n    Request(\"http://www.example.com/\"),\n    Request(\"http://www.example.com/query?id=111&cat=222\"),\n    Request(\"http://www.example.com/query?cat=222&id=111\"),\n    Request(\"http://www.example.com/hnnoticiaj1.aspx?78132,199\"),\n    Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\"),\n    Request(\"http://www.example.com/members/offers.html\"),\n    Request(\n        \"http://www.example.com/members/offers.html\",\n        headers={\"SESSIONID\": b\"somehash\"},\n    ),\n    Request(\n        \"http://www.example.com/\",\n        headers={\"Accept-Language\": b\"en\"},\n    ),\n    Request(\n        \"http://www.example.com/\",\n        headers={\n            \"Accept-Language\": b\"en\",\n            \"SESSIONID\": b\"somehash\",\n        },\n    ),\n    Request(\"http://www.example.com/test.html\"),\n    Request(\"http://www.example.com/test.html#fragment\"),\n    Request(\"http://www.example.com\", method=\"POST\"),\n    Request(\"http://www.example.com\", method=\"POST\", body=b\"request body\"),\n)\n\n\nclass RequestFingerprinterTestCase(unittest.TestCase):\n    def test_default_implementation(self):\n        crawler = get_crawler()\n        request = Request(\"https://example.com\")\n        self.assertEqual(\n            crawler.request_fingerprinter.fingerprint(request),\n            fingerprint(request),\n        )\n\n    def test_deprecated_implementation(self):\n        settings = {\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n        }\n        with warnings.catch_warnings(record=True) as logged_warnings:\n            crawler = get_crawler(settings_dict=settings)\n        request = Request(\"https://example.com\")\n        self.assertEqual(\n            crawler.request_fingerprinter.fingerprint(request),\n            fingerprint(request),\n        )\n        self.assertTrue(logged_warnings)\n\n\nclass CustomRequestFingerprinterTestCase(unittest.TestCase):\n    def test_include_headers(self):\n        class RequestFingerprinter:\n            def fingerprint(self, request):\n                return fingerprint(request, include_headers=[\"X-ID\"])\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        r1 = Request(\"http://www.example.com\", headers={\"X-ID\": \"1\"})\n        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n        r2 = Request(\"http://www.example.com\", headers={\"X-ID\": \"2\"})\n        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n        self.assertNotEqual(fp1, fp2)\n\n    def test_dont_canonicalize(self):\n        class RequestFingerprinter:\n            cache = WeakKeyDictionary()\n\n            def fingerprint(self, request):\n                if request not in self.cache:\n                    fp = sha1()\n                    fp.update(to_bytes(request.url))\n                    self.cache[request] = fp.digest()\n                return self.cache[request]\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        r1 = Request(\"http://www.example.com?a=1&a=2\")\n        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n        r2 = Request(\"http://www.example.com?a=2&a=1\")\n        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n        self.assertNotEqual(fp1, fp2)\n\n    def test_meta(self):\n        class RequestFingerprinter:\n            def fingerprint(self, request):\n                if \"fingerprint\" in request.meta:\n                    return request.meta[\"fingerprint\"]\n                return fingerprint(request)\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        r1 = Request(\"http://www.example.com\")\n        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n        r2 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"a\"})\n        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n        r3 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"a\"})\n        fp3 = crawler.request_fingerprinter.fingerprint(r3)\n        r4 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"b\"})\n        fp4 = crawler.request_fingerprinter.fingerprint(r4)\n        self.assertNotEqual(fp1, fp2)\n        self.assertNotEqual(fp1, fp4)\n        self.assertNotEqual(fp2, fp4)\n        self.assertEqual(fp2, fp3)\n\n    def test_from_crawler(self):\n        class RequestFingerprinter:\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler)\n\n            def __init__(self, crawler):\n                self._fingerprint = crawler.settings[\"FINGERPRINT\"]\n\n            def fingerprint(self, request):\n                return self._fingerprint\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n            \"FINGERPRINT\": b\"fingerprint\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        request = Request(\"http://www.example.com\")\n        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n        self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n\n    def test_from_settings(self):\n        class RequestFingerprinter:\n            @classmethod\n            def from_settings(cls, settings):\n                return cls(settings)\n\n            def __init__(self, settings):\n                self._fingerprint = settings[\"FINGERPRINT\"]\n\n            def fingerprint(self, request):\n                return self._fingerprint\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n            \"FINGERPRINT\": b\"fingerprint\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        request = Request(\"http://www.example.com\")\n        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n        self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n\n    def test_from_crawler_and_settings(self):\n        class RequestFingerprinter:\n            # This method is ignored due to the presence of from_crawler\n            @classmethod\n            def from_settings(cls, settings):\n                return cls(settings)\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler)\n\n            def __init__(self, crawler):\n                self._fingerprint = crawler.settings[\"FINGERPRINT\"]\n\n            def fingerprint(self, request):\n                return self._fingerprint\n\n        settings = {\n            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n            \"FINGERPRINT\": b\"fingerprint\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n\n        request = Request(\"http://www.example.com\")\n        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n        self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n\n\nclass RequestToCurlTest(unittest.TestCase):\n    def _test_request(self, request_object, expected_curl_command):\n        curl_command = request_to_curl(request_object)\n        self.assertEqual(curl_command, expected_curl_command)\n\n    def test_get(self):\n        request_object = Request(\"https://www.example.com\")\n        expected_curl_command = \"curl -X GET https://www.example.com\"\n        self._test_request(request_object, expected_curl_command)\n\n    def test_post(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            'curl -X POST https://www.httpbin.org/post --data-raw \\'{\"foo\": \"bar\"}\\''\n        )\n        self._test_request(request_object, expected_curl_command)\n\n    def test_headers(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"},\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            \"curl -X POST https://www.httpbin.org/post\"\n            ' --data-raw \\'{\"foo\": \"bar\"}\\''\n            \" -H 'Content-Type: application/json' -H 'Accept: application/json'\"\n        )\n        self._test_request(request_object, expected_curl_command)\n\n    def test_cookies_dict(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            cookies={\"foo\": \"bar\"},\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            \"curl -X POST https://www.httpbin.org/post\"\n            \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n        )\n        self._test_request(request_object, expected_curl_command)\n\n    def test_cookies_list(self):\n        request_object = Request(\n            \"https://www.httpbin.org/post\",\n            method=\"POST\",\n            cookies=[{\"foo\": \"bar\"}],\n            body=json.dumps({\"foo\": \"bar\"}),\n        )\n        expected_curl_command = (\n            \"curl -X POST https://www.httpbin.org/post\"\n            \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n        )\n        self._test_request(request_object, expected_curl_command)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_linkextractors.py": "import pickle\nimport re\nimport unittest\nfrom typing import Optional\n\nfrom packaging.version import Version\nfrom pytest import mark\nfrom w3lib import __version__ as w3lib_version\n\nfrom scrapy.http import HtmlResponse, XmlResponse\nfrom scrapy.link import Link\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\nfrom tests import get_testdata\n\n\n# a hack to skip base class tests in pytest\nclass Base:\n    class LinkExtractorTestCase(unittest.TestCase):\n        extractor_cls: Optional[type] = None\n\n        def setUp(self):\n            body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n            self.response = HtmlResponse(url=\"http://example.com/index\", body=body)\n\n        def test_urls_type(self):\n            \"\"\"Test that the resulting urls are str objects\"\"\"\n            lx = self.extractor_cls()\n            self.assertTrue(\n                all(\n                    isinstance(link.url, str)\n                    for link in lx.extract_links(self.response)\n                )\n            )\n\n        def test_extract_all_links(self):\n            lx = self.extractor_cls()\n            page4_url = \"http://example.com/page%204.html\"\n\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html#foo\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                    Link(url=\"http://www.google.com/something\", text=\"\"),\n                    Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n                    Link(url=page4_url, text=\"href with whitespaces\"),\n                ],\n            )\n\n        def test_extract_filter_allow(self):\n            lx = self.extractor_cls(allow=(\"sample\",))\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html#foo\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                ],\n            )\n\n        def test_extract_filter_allow_with_duplicates(self):\n            lx = self.extractor_cls(allow=(\"sample\",), unique=False)\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html\",\n                        text=\"sample 3 repetition\",\n                    ),\n                    Link(\n                        url=\"http://example.com/sample3.html\",\n                        text=\"sample 3 repetition\",\n                    ),\n                    Link(\n                        url=\"http://example.com/sample3.html#foo\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                ],\n            )\n\n        def test_extract_filter_allow_with_duplicates_canonicalize(self):\n            lx = self.extractor_cls(allow=(\"sample\",), unique=False, canonicalize=True)\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html\",\n                        text=\"sample 3 repetition\",\n                    ),\n                    Link(\n                        url=\"http://example.com/sample3.html\",\n                        text=\"sample 3 repetition\",\n                    ),\n                    Link(\n                        url=\"http://example.com/sample3.html\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                ],\n            )\n\n        def test_extract_filter_allow_no_duplicates_canonicalize(self):\n            lx = self.extractor_cls(allow=(\"sample\",), unique=True, canonicalize=True)\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                ],\n            )\n\n        def test_extract_filter_allow_and_deny(self):\n            lx = self.extractor_cls(allow=(\"sample\",), deny=(\"3\",))\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                ],\n            )\n\n        def test_extract_filter_allowed_domains(self):\n            lx = self.extractor_cls(allow_domains=(\"google.com\",))\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://www.google.com/something\", text=\"\"),\n                ],\n            )\n\n        def test_extraction_using_single_values(self):\n            \"\"\"Test the extractor's behaviour among different situations\"\"\"\n\n            lx = self.extractor_cls(allow=\"sample\")\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html#foo\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                ],\n            )\n\n            lx = self.extractor_cls(allow=\"sample\", deny=\"3\")\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                ],\n            )\n\n            lx = self.extractor_cls(allow_domains=\"google.com\")\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://www.google.com/something\", text=\"\"),\n                ],\n            )\n\n            lx = self.extractor_cls(deny_domains=\"example.com\")\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://www.google.com/something\", text=\"\"),\n                ],\n            )\n\n        def test_nofollow(self):\n            \"\"\"Test the extractor's behaviour for links with rel='nofollow'\"\"\"\n\n            html = b\"\"\"<html><head><title>Page title<title>\n            <body>\n            <div class='links'>\n            <p><a href=\"/about.html\">About us</a></p>\n            </div>\n            <div>\n            <p><a href=\"/follow.html\">Follow this link</a></p>\n            </div>\n            <div>\n            <p><a href=\"/nofollow.html\" rel=\"nofollow\">Dont follow this one</a></p>\n            </div>\n            <div>\n            <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n            </div>\n            <div>\n            <p><a href=\"http://google.com/something\" rel=\"external nofollow\">External link not to follow</a></p>\n            </div>\n            </body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.org/about.html\", text=\"About us\"),\n                    Link(url=\"http://example.org/follow.html\", text=\"Follow this link\"),\n                    Link(\n                        url=\"http://example.org/nofollow.html\",\n                        text=\"Dont follow this one\",\n                        nofollow=True,\n                    ),\n                    Link(\n                        url=\"http://example.org/nofollow2.html\",\n                        text=\"Choose to follow or not\",\n                    ),\n                    Link(\n                        url=\"http://google.com/something\",\n                        text=\"External link not to follow\",\n                        nofollow=True,\n                    ),\n                ],\n            )\n\n        def test_matches(self):\n            url1 = \"http://lotsofstuff.com/stuff1/index\"\n            url2 = \"http://evenmorestuff.com/uglystuff/index\"\n\n            lx = self.extractor_cls(allow=(r\"stuff1\",))\n            self.assertTrue(lx.matches(url1))\n            self.assertFalse(lx.matches(url2))\n\n            lx = self.extractor_cls(deny=(r\"uglystuff\",))\n            self.assertTrue(lx.matches(url1))\n            self.assertFalse(lx.matches(url2))\n\n            lx = self.extractor_cls(allow_domains=(\"evenmorestuff.com\",))\n            self.assertFalse(lx.matches(url1))\n            self.assertTrue(lx.matches(url2))\n\n            lx = self.extractor_cls(deny_domains=(\"lotsofstuff.com\",))\n            self.assertFalse(lx.matches(url1))\n            self.assertTrue(lx.matches(url2))\n\n            lx = self.extractor_cls(\n                allow=[\"blah1\"],\n                deny=[\"blah2\"],\n                allow_domains=[\"blah1.com\"],\n                deny_domains=[\"blah2.com\"],\n            )\n            self.assertTrue(lx.matches(\"http://blah1.com/blah1\"))\n            self.assertFalse(lx.matches(\"http://blah1.com/blah2\"))\n            self.assertFalse(lx.matches(\"http://blah2.com/blah1\"))\n            self.assertFalse(lx.matches(\"http://blah2.com/blah2\"))\n\n        def test_restrict_xpaths(self):\n            lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                ],\n            )\n\n        def test_restrict_xpaths_encoding(self):\n            \"\"\"Test restrict_xpaths with encodings\"\"\"\n            html = b\"\"\"<html><head><title>Page title<title>\n            <body><p><a href=\"item/12.html\">Item 12</a></p>\n            <div class='links'>\n            <p><a href=\"/about.html\">About us\\xa3</a></p>\n            </div>\n            <div>\n            <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n            </div>\n            </body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://example.org/somepage/index.html\",\n                body=html,\n                encoding=\"windows-1252\",\n            )\n\n            lx = self.extractor_cls(restrict_xpaths=\"//div[@class='links']\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [Link(url=\"http://example.org/about.html\", text=\"About us\\xa3\")],\n            )\n\n        def test_restrict_xpaths_with_html_entities(self):\n            html = b'<html><body><p><a href=\"/&hearts;/you?c=&euro;\">text</a></p></body></html>'\n            response = HtmlResponse(\n                \"http://example.org/somepage/index.html\",\n                body=html,\n                encoding=\"iso8859-15\",\n            )\n            links = self.extractor_cls(restrict_xpaths=\"//p\").extract_links(response)\n            self.assertEqual(\n                links, [Link(url=\"http://example.org/%E2%99%A5/you?c=%A4\", text=\"text\")]\n            )\n\n        def test_restrict_xpaths_concat_in_handle_data(self):\n            \"\"\"html entities cause SGMLParser to call handle_data hook twice\"\"\"\n            body = b\"\"\"<html><body><div><a href=\"/foo\">&gt;\\xbe\\xa9&lt;\\xb6\\xab</a></body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org\", body=body, encoding=\"gb18030\")\n            lx = self.extractor_cls(restrict_xpaths=\"//div\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://example.org/foo\",\n                        text=\">\\u4eac<\\u4e1c\",\n                        fragment=\"\",\n                        nofollow=False,\n                    )\n                ],\n            )\n\n        def test_restrict_css(self):\n            lx = self.extractor_cls(restrict_css=(\"#subwrapper a\",))\n            self.assertEqual(\n                lx.extract_links(self.response),\n                [Link(url=\"http://example.com/sample2.html\", text=\"sample 2\")],\n            )\n\n        def test_restrict_css_and_restrict_xpaths_together(self):\n            lx = self.extractor_cls(\n                restrict_xpaths=('//div[@id=\"subwrapper\"]',),\n                restrict_css=(\"#subwrapper + a\",),\n            )\n            self.assertEqual(\n                list(lx.extract_links(self.response)),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                ],\n            )\n\n        def test_area_tag_with_unicode_present(self):\n            body = b\"\"\"<html><body>\\xbe\\xa9<map><area href=\"http://example.org/foo\" /></map></body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org\", body=body, encoding=\"utf-8\")\n            lx = self.extractor_cls()\n            lx.extract_links(response)\n            lx.extract_links(response)\n            lx.extract_links(response)\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://example.org/foo\",\n                        text=\"\",\n                        fragment=\"\",\n                        nofollow=False,\n                    )\n                ],\n            )\n\n        def test_encoded_url(self):\n            body = b\"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://known.fm/AC%2FDC/\", body=body, encoding=\"utf8\"\n            )\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://known.fm/AC%2FDC/?page=2\",\n                        text=\"BinB\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                ],\n            )\n\n        def test_encoded_url_in_restricted_xpath(self):\n            body = b\"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://known.fm/AC%2FDC/\", body=body, encoding=\"utf8\"\n            )\n            lx = self.extractor_cls(restrict_xpaths=\"//div\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://known.fm/AC%2FDC/?page=2\",\n                        text=\"BinB\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                ],\n            )\n\n        def test_ignored_extensions(self):\n            # jpg is ignored by default\n            html = b\"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n            response = HtmlResponse(\"http://example.org/\", body=html)\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.org/page.html\", text=\"asd\"),\n                ],\n            )\n\n            # override denied extensions\n            lx = self.extractor_cls(deny_extensions=[\"html\"])\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.org/photo.jpg\"),\n                ],\n            )\n\n        def test_process_value(self):\n            \"\"\"Test restrict_xpaths with encodings\"\"\"\n            html = b\"\"\"\n<a href=\"javascript:goToPage('../other/page.html','photo','width=600,height=540,scrollbars'); return false\">Text</a>\n<a href=\"/about.html\">About us</a>\n            \"\"\"\n            response = HtmlResponse(\n                \"http://example.org/somepage/index.html\",\n                body=html,\n                encoding=\"windows-1252\",\n            )\n\n            def process_value(value):\n                m = re.search(r\"javascript:goToPage\\('(.*?)'\", value)\n                if m:\n                    return m.group(1)\n\n            lx = self.extractor_cls(process_value=process_value)\n            self.assertEqual(\n                lx.extract_links(response),\n                [Link(url=\"http://example.org/other/page.html\", text=\"Text\")],\n            )\n\n        def test_base_url_with_restrict_xpaths(self):\n            html = b\"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n            <body><p><a href=\"item/12.html\">Item 12</a></p>\n            </body></html>\"\"\"\n            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n            lx = self.extractor_cls(restrict_xpaths=\"//p\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [Link(url=\"http://otherdomain.com/base/item/12.html\", text=\"Item 12\")],\n            )\n\n        def test_attrs(self):\n            lx = self.extractor_cls(attrs=\"href\")\n            page4_url = \"http://example.com/page%204.html\"\n\n            self.assertEqual(\n                lx.extract_links(self.response),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html#foo\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                    Link(url=\"http://www.google.com/something\", text=\"\"),\n                    Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n                    Link(url=page4_url, text=\"href with whitespaces\"),\n                ],\n            )\n\n            lx = self.extractor_cls(\n                attrs=(\"href\", \"src\"), tags=(\"a\", \"area\", \"img\"), deny_extensions=()\n            )\n            self.assertEqual(\n                lx.extract_links(self.response),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample2.jpg\", text=\"\"),\n                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n                    Link(\n                        url=\"http://example.com/sample3.html#foo\",\n                        text=\"sample 3 repetition with fragment\",\n                    ),\n                    Link(url=\"http://www.google.com/something\", text=\"\"),\n                    Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n                    Link(url=page4_url, text=\"href with whitespaces\"),\n                ],\n            )\n\n            lx = self.extractor_cls(attrs=None)\n            self.assertEqual(lx.extract_links(self.response), [])\n\n        def test_tags(self):\n            html = (\n                b'<html><area href=\"sample1.html\"></area>'\n                b'<a href=\"sample2.html\">sample 2</a><img src=\"sample2.jpg\"/></html>'\n            )\n            response = HtmlResponse(\"http://example.com/index.html\", body=html)\n\n            lx = self.extractor_cls(tags=None)\n            self.assertEqual(lx.extract_links(response), [])\n\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                ],\n            )\n\n            lx = self.extractor_cls(tags=\"area\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                ],\n            )\n\n            lx = self.extractor_cls(tags=\"a\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                ],\n            )\n\n            lx = self.extractor_cls(\n                tags=(\"a\", \"img\"), attrs=(\"href\", \"src\"), deny_extensions=()\n            )\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n                    Link(url=\"http://example.com/sample2.jpg\", text=\"\"),\n                ],\n            )\n\n        def test_tags_attrs(self):\n            html = b\"\"\"\n            <html><body>\n            <div id=\"item1\" data-url=\"get?id=1\"><a href=\"#\">Item 1</a></div>\n            <div id=\"item2\" data-url=\"get?id=2\"><a href=\"#\">Item 2</a></div>\n            </body></html>\n            \"\"\"\n            response = HtmlResponse(\"http://example.com/index.html\", body=html)\n\n            lx = self.extractor_cls(tags=\"div\", attrs=\"data-url\")\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://example.com/get?id=1\",\n                        text=\"Item 1\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.com/get?id=2\",\n                        text=\"Item 2\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                ],\n            )\n\n            lx = self.extractor_cls(tags=(\"div\",), attrs=(\"data-url\",))\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://example.com/get?id=1\",\n                        text=\"Item 1\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.com/get?id=2\",\n                        text=\"Item 2\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                ],\n            )\n\n        def test_xhtml(self):\n            xhtml = b\"\"\"\n    <?xml version=\"1.0\"?>\n    <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n    <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n    <head>\n        <title>XHTML document title</title>\n    </head>\n    <body>\n        <div class='links'>\n        <p><a href=\"/about.html\">About us</a></p>\n        </div>\n        <div>\n        <p><a href=\"/follow.html\">Follow this link</a></p>\n        </div>\n        <div>\n        <p><a href=\"/nofollow.html\" rel=\"nofollow\">Dont follow this one</a></p>\n        </div>\n        <div>\n        <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n        </div>\n        <div>\n        <p><a href=\"http://google.com/something\" rel=\"external nofollow\">External link not to follow</a></p>\n        </div>\n    </body>\n    </html>\n            \"\"\"\n\n            response = HtmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://example.com/about.html\",\n                        text=\"About us\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.com/follow.html\",\n                        text=\"Follow this link\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.com/nofollow.html\",\n                        text=\"Dont follow this one\",\n                        fragment=\"\",\n                        nofollow=True,\n                    ),\n                    Link(\n                        url=\"http://example.com/nofollow2.html\",\n                        text=\"Choose to follow or not\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://google.com/something\",\n                        text=\"External link not to follow\",\n                        nofollow=True,\n                    ),\n                ],\n            )\n\n            response = XmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"http://example.com/about.html\",\n                        text=\"About us\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.com/follow.html\",\n                        text=\"Follow this link\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.com/nofollow.html\",\n                        text=\"Dont follow this one\",\n                        fragment=\"\",\n                        nofollow=True,\n                    ),\n                    Link(\n                        url=\"http://example.com/nofollow2.html\",\n                        text=\"Choose to follow or not\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://google.com/something\",\n                        text=\"External link not to follow\",\n                        nofollow=True,\n                    ),\n                ],\n            )\n\n        def test_link_wrong_href(self):\n            html = b\"\"\"\n            <a href=\"http://example.org/item1.html\">Item 1</a>\n            <a href=\"http://[example.org/item2.html\">Item 2</a>\n            <a href=\"http://example.org/item3.html\">Item 3</a>\n            \"\"\"\n            response = HtmlResponse(\"http://example.org/index.html\", body=html)\n            lx = self.extractor_cls()\n            self.assertEqual(\n                list(lx.extract_links(response)),\n                [\n                    Link(\n                        url=\"http://example.org/item1.html\",\n                        text=\"Item 1\",\n                        nofollow=False,\n                    ),\n                    Link(\n                        url=\"http://example.org/item3.html\",\n                        text=\"Item 3\",\n                        nofollow=False,\n                    ),\n                ],\n            )\n\n        def test_ftp_links(self):\n            body = b\"\"\"\n            <html><body>\n            <div><a href=\"ftp://www.external.com/\">An Item</a></div>\n            </body></html>\"\"\"\n            response = HtmlResponse(\n                \"http://www.example.com/index.html\", body=body, encoding=\"utf8\"\n            )\n            lx = self.extractor_cls()\n            self.assertEqual(\n                lx.extract_links(response),\n                [\n                    Link(\n                        url=\"ftp://www.external.com/\",\n                        text=\"An Item\",\n                        fragment=\"\",\n                        nofollow=False,\n                    ),\n                ],\n            )\n\n        def test_pickle_extractor(self):\n            lx = self.extractor_cls()\n            self.assertIsInstance(pickle.loads(pickle.dumps(lx)), self.extractor_cls)\n\n        def test_link_extractor_aggregation(self):\n            \"\"\"When a parameter like restrict_css is used, the underlying\n            implementation calls its internal link extractor once per selector\n            matching the specified restrictions, and then aggregates the\n            extracted links.\n\n            Test that aggregation respects the unique and canonicalize\n            parameters.\n            \"\"\"\n            # unique=True (default), canonicalize=False (default)\n            lx = self.extractor_cls(restrict_css=(\"div\",))\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            self.assertEqual(\n                actual,\n                [\n                    Link(url=\"https://example.com/a\", text=\"a1\"),\n                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                    Link(url=\"https://example.com/b?b=2&a=1\", text=\"b2\"),\n                ],\n            )\n\n            # unique=True (default), canonicalize=True\n            lx = self.extractor_cls(restrict_css=(\"div\",), canonicalize=True)\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            self.assertEqual(\n                actual,\n                [\n                    Link(url=\"https://example.com/a\", text=\"a1\"),\n                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                ],\n            )\n\n            # unique=False, canonicalize=False (default)\n            lx = self.extractor_cls(restrict_css=(\"div\",), unique=False)\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            self.assertEqual(\n                actual,\n                [\n                    Link(url=\"https://example.com/a\", text=\"a1\"),\n                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                    Link(url=\"https://example.com/a\", text=\"a2\"),\n                    Link(url=\"https://example.com/b?b=2&a=1\", text=\"b2\"),\n                ],\n            )\n\n            # unique=False, canonicalize=True\n            lx = self.extractor_cls(\n                restrict_css=(\"div\",), unique=False, canonicalize=True\n            )\n            response = HtmlResponse(\n                \"https://example.com\",\n                body=b\"\"\"\n                    <div>\n                        <a href=\"/a\">a1</a>\n                        <a href=\"/b?a=1&b=2\">b1</a>\n                    </div>\n                    <div>\n                        <a href=\"/a\">a2</a>\n                        <a href=\"/b?b=2&a=1\">b2</a>\n                    </div>\n                \"\"\",\n            )\n            actual = lx.extract_links(response)\n            self.assertEqual(\n                actual,\n                [\n                    Link(url=\"https://example.com/a\", text=\"a1\"),\n                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n                    Link(url=\"https://example.com/a\", text=\"a2\"),\n                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b2\"),\n                ],\n            )\n\n\nclass LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n    extractor_cls = LxmlLinkExtractor\n\n    def test_link_wrong_href(self):\n        html = b\"\"\"\n        <a href=\"http://example.org/item1.html\">Item 1</a>\n        <a href=\"http://[example.org/item2.html\">Item 2</a>\n        <a href=\"http://example.org/item3.html\">Item 3</a>\n        \"\"\"\n        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n        lx = self.extractor_cls()\n        self.assertEqual(\n            list(lx.extract_links(response)),\n            [\n                Link(\n                    url=\"http://example.org/item1.html\", text=\"Item 1\", nofollow=False\n                ),\n                Link(\n                    url=\"http://example.org/item3.html\", text=\"Item 3\", nofollow=False\n                ),\n            ],\n        )\n\n    def test_link_restrict_text(self):\n        html = b\"\"\"\n        <a href=\"http://example.org/item1.html\">Pic of a cat</a>\n        <a href=\"http://example.org/item2.html\">Pic of a dog</a>\n        <a href=\"http://example.org/item3.html\">Pic of a cow</a>\n        \"\"\"\n        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n        # Simple text inclusion test\n        lx = self.extractor_cls(restrict_text=\"dog\")\n        self.assertEqual(\n            list(lx.extract_links(response)),\n            [\n                Link(\n                    url=\"http://example.org/item2.html\",\n                    text=\"Pic of a dog\",\n                    nofollow=False,\n                ),\n            ],\n        )\n        # Unique regex test\n        lx = self.extractor_cls(restrict_text=r\"of.*dog\")\n        self.assertEqual(\n            list(lx.extract_links(response)),\n            [\n                Link(\n                    url=\"http://example.org/item2.html\",\n                    text=\"Pic of a dog\",\n                    nofollow=False,\n                ),\n            ],\n        )\n        # Multiple regex test\n        lx = self.extractor_cls(restrict_text=[r\"of.*dog\", r\"of.*cat\"])\n        self.assertEqual(\n            list(lx.extract_links(response)),\n            [\n                Link(\n                    url=\"http://example.org/item1.html\",\n                    text=\"Pic of a cat\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.org/item2.html\",\n                    text=\"Pic of a dog\",\n                    nofollow=False,\n                ),\n            ],\n        )\n\n    @mark.skipif(\n        Version(w3lib_version) < Version(\"2.0.0\"),\n        reason=(\n            \"Before w3lib 2.0.0, w3lib.url.safe_url_string would not complain \"\n            \"about an invalid port value.\"\n        ),\n    )\n    def test_skip_bad_links(self):\n        html = b\"\"\"\n        <a href=\"http://example.org:non-port\">Why would you do this?</a>\n        <a href=\"http://example.org/item2.html\">Good Link</a>\n        <a href=\"http://example.org/item3.html\">Good Link 2</a>\n        \"\"\"\n        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n        lx = self.extractor_cls()\n        self.assertEqual(\n            list(lx.extract_links(response)),\n            [\n                Link(\n                    url=\"http://example.org/item2.html\",\n                    text=\"Good Link\",\n                    nofollow=False,\n                ),\n                Link(\n                    url=\"http://example.org/item3.html\",\n                    text=\"Good Link 2\",\n                    nofollow=False,\n                ),\n            ],\n        )\n\n    def test_link_allowed_is_false_with_empty_url(self):\n        bad_link = Link(\"\")\n        self.assertFalse(LxmlLinkExtractor()._link_allowed(bad_link))\n\n    def test_link_allowed_is_false_with_bad_url_prefix(self):\n        bad_link = Link(\"htp://should_be_http.example\")\n        self.assertFalse(LxmlLinkExtractor()._link_allowed(bad_link))\n\n    def test_link_allowed_is_false_with_missing_url_prefix(self):\n        bad_link = Link(\"should_have_prefix.example\")\n        self.assertFalse(LxmlLinkExtractor()._link_allowed(bad_link))\n", "tests/test_engine_stop_download_headers.py": "from testfixtures import LogCapture\nfrom twisted.internet import defer\n\nfrom scrapy.exceptions import StopDownload\nfrom tests.test_engine import (\n    AttrsItemsSpider,\n    CrawlerRun,\n    DataClassItemsSpider,\n    DictItemsSpider,\n    EngineTest,\n    TestSpider,\n)\n\n\nclass HeadersReceivedCrawlerRun(CrawlerRun):\n    def headers_received(self, headers, body_length, request, spider):\n        super().headers_received(headers, body_length, request, spider)\n        raise StopDownload(fail=False)\n\n\nclass HeadersReceivedEngineTest(EngineTest):\n    @defer.inlineCallbacks\n    def test_crawler(self):\n        for spider in (\n            TestSpider,\n            DictItemsSpider,\n            AttrsItemsSpider,\n            DataClassItemsSpider,\n        ):\n            run = HeadersReceivedCrawlerRun(spider)\n            with LogCapture() as log:\n                yield run.run()\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET http://localhost:{run.portno}/redirected> from\"\n                        \" signal handler HeadersReceivedCrawlerRun.headers_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET http://localhost:{run.portno}/> from signal\"\n                        \" handler HeadersReceivedCrawlerRun.headers_received\",\n                    )\n                )\n                log.check_present(\n                    (\n                        \"scrapy.core.downloader.handlers.http11\",\n                        \"DEBUG\",\n                        f\"Download stopped for <GET http://localhost:{run.portno}/numbers> from\"\n                        \" signal handler HeadersReceivedCrawlerRun.headers_received\",\n                    )\n                )\n            self._assert_visited_urls(run)\n            self._assert_downloaded_responses(run, count=6)\n            self._assert_signals_caught(run)\n            self._assert_bytes_received(run)\n            self._assert_headers_received(run)\n\n    def _assert_bytes_received(self, run: CrawlerRun):\n        self.assertEqual(0, len(run.bytes))\n\n    def _assert_visited_urls(self, run: CrawlerRun):\n        must_be_visited = [\"/\", \"/redirect\", \"/redirected\"]\n        urls_visited = {rp[0].url for rp in run.respplug}\n        urls_expected = {run.geturl(p) for p in must_be_visited}\n        assert (\n            urls_expected <= urls_visited\n        ), f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n", "tests/test_downloadermiddleware_defaultheaders.py": "from unittest import TestCase\n\nfrom scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestDefaultHeadersMiddleware(TestCase):\n    def get_defaults_spider_mw(self):\n        crawler = get_crawler(Spider)\n        spider = crawler._create_spider(\"foo\")\n        defaults = {\n            to_bytes(k): [to_bytes(v)]\n            for k, v in crawler.settings.get(\"DEFAULT_REQUEST_HEADERS\").items()\n        }\n        return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)\n\n    def test_process_request(self):\n        defaults, spider, mw = self.get_defaults_spider_mw()\n        req = Request(\"http://www.scrapytest.org\")\n        mw.process_request(req, spider)\n        self.assertEqual(req.headers, defaults)\n\n    def test_update_headers(self):\n        defaults, spider, mw = self.get_defaults_spider_mw()\n        headers = {\"Accept-Language\": [\"es\"], \"Test-Header\": [\"test\"]}\n        bytes_headers = {b\"Accept-Language\": [b\"es\"], b\"Test-Header\": [b\"test\"]}\n        req = Request(\"http://www.scrapytest.org\", headers=headers)\n        self.assertEqual(req.headers, bytes_headers)\n\n        mw.process_request(req, spider)\n        defaults.update(bytes_headers)\n        self.assertEqual(req.headers, defaults)\n", "tests/test_crawl.py": "import json\nimport logging\nimport unittest\nfrom ipaddress import IPv4Address\nfrom socket import gethostbyname\nfrom urllib.parse import urlparse\n\nfrom pytest import mark\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.ssl import Certificate\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.http import Request\nfrom scrapy.http.response import Response\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.test import get_crawler\nfrom tests import NON_EXISTING_RESOLVABLE\nfrom tests.mockserver import MockServer\nfrom tests.spiders import (\n    AsyncDefAsyncioGenComplexSpider,\n    AsyncDefAsyncioGenExcSpider,\n    AsyncDefAsyncioGenLoopSpider,\n    AsyncDefAsyncioGenSpider,\n    AsyncDefAsyncioReqsReturnSpider,\n    AsyncDefAsyncioReturnSingleElementSpider,\n    AsyncDefAsyncioReturnSpider,\n    AsyncDefAsyncioSpider,\n    AsyncDefDeferredDirectSpider,\n    AsyncDefDeferredMaybeWrappedSpider,\n    AsyncDefDeferredWrappedSpider,\n    AsyncDefSpider,\n    BrokenStartRequestsSpider,\n    BytesReceivedCallbackSpider,\n    BytesReceivedErrbackSpider,\n    CrawlSpiderWithAsyncCallback,\n    CrawlSpiderWithAsyncGeneratorCallback,\n    CrawlSpiderWithErrback,\n    CrawlSpiderWithParseMethod,\n    CrawlSpiderWithProcessRequestCallbackKeywordArguments,\n    DelaySpider,\n    DuplicateStartRequestsSpider,\n    FollowAllSpider,\n    HeadersReceivedCallbackSpider,\n    HeadersReceivedErrbackSpider,\n    SimpleSpider,\n    SingleRequestSpider,\n)\n\n\nclass CrawlTestCase(TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_follow_all(self):\n        crawler = get_crawler(FollowAllSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n\n    @defer.inlineCallbacks\n    def test_fixed_delay(self):\n        yield self._test_delay(total=3, delay=0.2)\n\n    @defer.inlineCallbacks\n    def test_randomized_delay(self):\n        yield self._test_delay(total=3, delay=0.1, randomize=True)\n\n    @defer.inlineCallbacks\n    def _test_delay(self, total, delay, randomize=False):\n        crawl_kwargs = {\n            \"maxlatency\": delay * 2,\n            \"mockserver\": self.mockserver,\n            \"total\": total,\n        }\n        tolerance = 1 - (0.6 if randomize else 0.2)\n\n        settings = {\"DOWNLOAD_DELAY\": delay, \"RANDOMIZE_DOWNLOAD_DELAY\": randomize}\n        crawler = get_crawler(FollowAllSpider, settings)\n        yield crawler.crawl(**crawl_kwargs)\n        times = crawler.spider.times\n        total_time = times[-1] - times[0]\n        average = total_time / (len(times) - 1)\n        self.assertTrue(\n            average > delay * tolerance, f\"download delay too small: {average}\"\n        )\n\n        # Ensure that the same test parameters would cause a failure if no\n        # download delay is set. Otherwise, it means we are using a combination\n        # of ``total`` and ``delay`` values that are too small for the test\n        # code above to have any meaning.\n        settings[\"DOWNLOAD_DELAY\"] = 0\n        crawler = get_crawler(FollowAllSpider, settings)\n        yield crawler.crawl(**crawl_kwargs)\n        times = crawler.spider.times\n        total_time = times[-1] - times[0]\n        average = total_time / (len(times) - 1)\n        self.assertFalse(\n            average > delay / tolerance, \"test total or delay values are too small\"\n        )\n\n    @defer.inlineCallbacks\n    def test_timeout_success(self):\n        crawler = get_crawler(DelaySpider)\n        yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n        self.assertTrue(crawler.spider.t1 > 0)\n        self.assertTrue(crawler.spider.t2 > 0)\n        self.assertTrue(crawler.spider.t2 > crawler.spider.t1)\n\n    @defer.inlineCallbacks\n    def test_timeout_failure(self):\n        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n        yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n        self.assertTrue(crawler.spider.t1 > 0)\n        self.assertTrue(crawler.spider.t2 == 0)\n        self.assertTrue(crawler.spider.t2_err > 0)\n        self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n\n        # server hangs after receiving response headers\n        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n        yield crawler.crawl(n=0.5, b=1, mockserver=self.mockserver)\n        self.assertTrue(crawler.spider.t1 > 0)\n        self.assertTrue(crawler.spider.t2 == 0)\n        self.assertTrue(crawler.spider.t2_err > 0)\n        self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n\n    @defer.inlineCallbacks\n    def test_retry_503(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @defer.inlineCallbacks\n    def test_retry_conn_failed(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                \"http://localhost:65432/status?n=503\", mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @defer.inlineCallbacks\n    def test_retry_dns_error(self):\n        if NON_EXISTING_RESOLVABLE:\n            raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            # try to fetch the homepage of a nonexistent domain\n            yield crawler.crawl(\n                \"http://dns.resolution.invalid./\", mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @defer.inlineCallbacks\n    def test_start_requests_bug_before_yield(self):\n        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n            crawler = get_crawler(BrokenStartRequestsSpider)\n            yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n\n        self.assertEqual(len(log.records), 1)\n        record = log.records[0]\n        self.assertIsNotNone(record.exc_info)\n        self.assertIs(record.exc_info[0], ZeroDivisionError)\n\n    @defer.inlineCallbacks\n    def test_start_requests_bug_yielding(self):\n        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n            crawler = get_crawler(BrokenStartRequestsSpider)\n            yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n\n        self.assertEqual(len(log.records), 1)\n        record = log.records[0]\n        self.assertIsNotNone(record.exc_info)\n        self.assertIs(record.exc_info[0], ZeroDivisionError)\n\n    @defer.inlineCallbacks\n    def test_start_requests_laziness(self):\n        settings = {\"CONCURRENT_REQUESTS\": 1}\n        crawler = get_crawler(BrokenStartRequestsSpider, settings)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertTrue(\n            crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n            crawler.spider.seedsseen,\n        )\n\n    @defer.inlineCallbacks\n    def test_start_requests_dupes(self):\n        settings = {\"CONCURRENT_REQUESTS\": 1}\n        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n        yield crawler.crawl(\n            dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver\n        )\n        self.assertEqual(crawler.spider.visited, 6)\n\n        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n        yield crawler.crawl(\n            dont_filter=False,\n            distinct_urls=3,\n            dupe_factor=4,\n            mockserver=self.mockserver,\n        )\n        self.assertEqual(crawler.spider.visited, 3)\n\n    @defer.inlineCallbacks\n    def test_unbounded_response(self):\n        # Completeness of responses without Content-Length or Transfer-Encoding\n        # can not be determined, we treat them as valid but flagged as \"partial\"\n        from urllib.parse import urlencode\n\n        query = urlencode(\n            {\n                \"raw\": \"\"\"\\\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nX-Powered-By: Servlet 2.4; JBoss-4.2.3.GA (build: SVNTag=JBoss_4_2_3_GA date=200807181417)/JBossWeb-2.0\nSet-Cookie: JSESSIONID=08515F572832D0E659FD2B0D8031D75F; Path=/\nPragma: no-cache\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nCache-Control: no-cache\nCache-Control: no-store\nContent-Type: text/html;charset=UTF-8\nContent-Language: en\nDate: Tue, 27 Aug 2013 13:05:05 GMT\nConnection: close\n\nfoo body\nwith multiples lines\n\"\"\"\n            }\n        )\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver\n            )\n        self.assertEqual(str(log).count(\"Got response 200\"), 1)\n\n    @defer.inlineCallbacks\n    def test_retry_conn_lost(self):\n        # connection lost after receiving data\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    @defer.inlineCallbacks\n    def test_retry_conn_aborted(self):\n        # connection lost before receiving data\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver\n            )\n        self._assert_retried(log)\n\n    def _assert_retried(self, log):\n        self.assertEqual(str(log).count(\"Retrying\"), 2)\n        self.assertEqual(str(log).count(\"Gave up retrying\"), 1)\n\n    @defer.inlineCallbacks\n    def test_referer_header(self):\n        \"\"\"Referer header is set by RefererMiddleware unless it is already set\"\"\"\n        req0 = Request(self.mockserver.url(\"/echo?headers=1&body=0\"), dont_filter=1)\n        req1 = req0.replace()\n        req2 = req0.replace(headers={\"Referer\": None})\n        req3 = req0.replace(headers={\"Referer\": \"http://example.com\"})\n        req0.meta[\"next\"] = req1\n        req1.meta[\"next\"] = req2\n        req2.meta[\"next\"] = req3\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(seed=req0, mockserver=self.mockserver)\n        # basic asserts in case of weird communication errors\n        self.assertIn(\"responses\", crawler.spider.meta)\n        self.assertNotIn(\"failures\", crawler.spider.meta)\n        # start requests doesn't set Referer header\n        echo0 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n        self.assertNotIn(\"Referer\", echo0[\"headers\"])\n        # following request sets Referer to start request url\n        echo1 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][1].body))\n        self.assertEqual(echo1[\"headers\"].get(\"Referer\"), [req0.url])\n        # next request avoids Referer header\n        echo2 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n        self.assertNotIn(\"Referer\", echo2[\"headers\"])\n        # last request explicitly sets a Referer header\n        echo3 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][3].body))\n        self.assertEqual(echo3[\"headers\"].get(\"Referer\"), [\"http://example.com\"])\n\n    @defer.inlineCallbacks\n    def test_engine_status(self):\n        from scrapy.utils.engine import get_engine_status\n\n        est = []\n\n        def cb(response):\n            est.append(get_engine_status(crawler.engine))\n\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(\n            seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n        )\n        self.assertEqual(len(est), 1, est)\n        s = dict(est[0])\n        self.assertEqual(s[\"engine.spider.name\"], crawler.spider.name)\n        self.assertEqual(s[\"len(engine.scraper.slot.active)\"], 1)\n\n    @defer.inlineCallbacks\n    def test_format_engine_status(self):\n        from scrapy.utils.engine import format_engine_status\n\n        est = []\n\n        def cb(response):\n            est.append(format_engine_status(crawler.engine))\n\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(\n            seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n        )\n        self.assertEqual(len(est), 1, est)\n        est = est[0].split(\"\\n\")[2:-2]  # remove header & footer\n        # convert to dict\n        est = [x.split(\":\") for x in est]\n        est = [x for sublist in est for x in sublist]  # flatten\n        est = [x.lstrip().rstrip() for x in est]\n        it = iter(est)\n        s = dict(zip(it, it))\n\n        self.assertEqual(s[\"engine.spider.name\"], crawler.spider.name)\n        self.assertEqual(s[\"len(engine.scraper.slot.active)\"], \"1\")\n\n    @defer.inlineCallbacks\n    def test_graceful_crawl_error_handling(self):\n        \"\"\"\n        Test whether errors happening anywhere in Crawler.crawl() are properly\n        reported (and not somehow swallowed) after a graceful engine shutdown.\n        The errors should not come from within Scrapy's core but from within\n        spiders/middlewares/etc., e.g. raised in Spider.start_requests(),\n        SpiderMiddleware.process_start_requests(), etc.\n        \"\"\"\n\n        class TestError(Exception):\n            pass\n\n        class FaultySpider(SimpleSpider):\n            def start_requests(self):\n                raise TestError\n\n        crawler = get_crawler(FaultySpider)\n        yield self.assertFailure(crawler.crawl(mockserver=self.mockserver), TestError)\n        self.assertFalse(crawler.crawling)\n\n    @defer.inlineCallbacks\n    def test_open_spider_error_on_faulty_pipeline(self):\n        settings = {\n            \"ITEM_PIPELINES\": {\n                \"tests.pipelines.ZeroDivisionErrorPipeline\": 300,\n            }\n        }\n        crawler = get_crawler(SimpleSpider, settings)\n        yield self.assertFailure(\n            crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            ),\n            ZeroDivisionError,\n        )\n        self.assertFalse(crawler.crawling)\n\n    @defer.inlineCallbacks\n    def test_crawlerrunner_accepts_crawler(self):\n        crawler = get_crawler(SimpleSpider)\n        runner = CrawlerRunner()\n        with LogCapture() as log:\n            yield runner.crawl(\n                crawler,\n                self.mockserver.url(\"/status?n=200\"),\n                mockserver=self.mockserver,\n            )\n        self.assertIn(\"Got response 200\", str(log))\n\n    @defer.inlineCallbacks\n    def test_crawl_multiple(self):\n        runner = CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n        runner.crawl(\n            SimpleSpider,\n            self.mockserver.url(\"/status?n=200\"),\n            mockserver=self.mockserver,\n        )\n        runner.crawl(\n            SimpleSpider,\n            self.mockserver.url(\"/status?n=503\"),\n            mockserver=self.mockserver,\n        )\n\n        with LogCapture() as log:\n            yield runner.join()\n\n        self._assert_retried(log)\n        self.assertIn(\"Got response 200\", str(log))\n\n\nclass CrawlSpiderTestCase(TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def _run_spider(self, spider_cls):\n        items = []\n\n        def _on_item_scraped(item):\n            items.append(item)\n\n        crawler = get_crawler(spider_cls)\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        return log, items, crawler.stats\n\n    @defer.inlineCallbacks\n    def test_crawlspider_with_parse(self):\n        crawler = get_crawler(CrawlSpiderWithParseMethod)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n        self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_with_async_callback(self):\n        crawler = get_crawler(CrawlSpiderWithAsyncCallback)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        self.assertIn(\"[parse_async] status 200 (foo: None)\", str(log))\n        self.assertIn(\"[parse_async] status 201 (foo: None)\", str(log))\n        self.assertIn(\"[parse_async] status 202 (foo: bar)\", str(log))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_with_async_generator_callback(self):\n        crawler = get_crawler(CrawlSpiderWithAsyncGeneratorCallback)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        self.assertIn(\"[parse_async_gen] status 200 (foo: None)\", str(log))\n        self.assertIn(\"[parse_async_gen] status 201 (foo: None)\", str(log))\n        self.assertIn(\"[parse_async_gen] status 202 (foo: bar)\", str(log))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_with_errback(self):\n        crawler = get_crawler(CrawlSpiderWithErrback)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n        self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n        self.assertIn(\"[errback] status 404\", str(log))\n        self.assertIn(\"[errback] status 500\", str(log))\n        self.assertIn(\"[errback] status 501\", str(log))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_process_request_cb_kwargs(self):\n        crawler = get_crawler(CrawlSpiderWithProcessRequestCallbackKeywordArguments)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n\n        self.assertIn(\"[parse] status 200 (foo: process_request)\", str(log))\n        self.assertIn(\"[parse] status 201 (foo: process_request)\", str(log))\n        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n\n    @defer.inlineCallbacks\n    def test_async_def_parse(self):\n        crawler = get_crawler(AsyncDefSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        self.assertIn(\"Got response 200\", str(log))\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse(self):\n        crawler = get_crawler(\n            AsyncDefAsyncioSpider,\n            {\n                \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n            },\n        )\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        self.assertIn(\"Got response 200\", str(log))\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse_items_list(self):\n        log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)\n        self.assertIn(\"Got response 200\", str(log))\n        self.assertIn({\"id\": 1}, items)\n        self.assertIn({\"id\": 2}, items)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse_items_single_element(self):\n        items = []\n\n        def _on_item_scraped(item):\n            items.append(item)\n\n        crawler = get_crawler(AsyncDefAsyncioReturnSingleElementSpider)\n        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n            )\n        self.assertIn(\"Got response 200\", str(log))\n        self.assertIn({\"foo\": 42}, items)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncgen_parse(self):\n        log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)\n        self.assertIn(\"Got response 200\", str(log))\n        itemcount = stats.get_value(\"item_scraped_count\")\n        self.assertEqual(itemcount, 1)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncgen_parse_loop(self):\n        log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)\n        self.assertIn(\"Got response 200\", str(log))\n        itemcount = stats.get_value(\"item_scraped_count\")\n        self.assertEqual(itemcount, 10)\n        for i in range(10):\n            self.assertIn({\"foo\": i}, items)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncgen_parse_exc(self):\n        log, items, stats = yield self._run_spider(AsyncDefAsyncioGenExcSpider)\n        log = str(log)\n        self.assertIn(\"Spider error processing\", log)\n        self.assertIn(\"ValueError\", log)\n        itemcount = stats.get_value(\"item_scraped_count\")\n        self.assertEqual(itemcount, 7)\n        for i in range(7):\n            self.assertIn({\"foo\": i}, items)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncgen_parse_complex(self):\n        _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)\n        itemcount = stats.get_value(\"item_scraped_count\")\n        self.assertEqual(itemcount, 156)\n        # some random items\n        for i in [1, 4, 21, 22, 207, 311]:\n            self.assertIn({\"index\": i}, items)\n        for i in [10, 30, 122]:\n            self.assertIn({\"index2\": i}, items)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse_reqs_list(self):\n        log, *_ = yield self._run_spider(AsyncDefAsyncioReqsReturnSpider)\n        for req_id in range(3):\n            self.assertIn(f\"Got response 200, req_id {req_id}\", str(log))\n\n    @mark.only_not_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_deferred_direct(self):\n        _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)\n        self.assertEqual(items, [{\"code\": 200}])\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_async_def_deferred_wrapped(self):\n        log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)\n        self.assertEqual(items, [{\"code\": 200}])\n\n    @defer.inlineCallbacks\n    def test_async_def_deferred_maybe_wrapped(self):\n        _, items, _ = yield self._run_spider(AsyncDefDeferredMaybeWrappedSpider)\n        self.assertEqual(items, [{\"code\": 200}])\n\n    @defer.inlineCallbacks\n    def test_response_ssl_certificate_none(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        self.assertIsNone(crawler.spider.meta[\"responses\"][0].certificate)\n\n    @defer.inlineCallbacks\n    def test_response_ssl_certificate(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        cert = crawler.spider.meta[\"responses\"][0].certificate\n        self.assertIsInstance(cert, Certificate)\n        self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n        self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n\n    @mark.xfail(reason=\"Responses with no body return early and contain no certificate\")\n    @defer.inlineCallbacks\n    def test_response_ssl_certificate_empty_response(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        cert = crawler.spider.meta[\"responses\"][0].certificate\n        self.assertIsInstance(cert, Certificate)\n        self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n        self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n\n    @defer.inlineCallbacks\n    def test_dns_server_ip_address_none(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/status?n=200\")\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n        self.assertIsNone(ip_address)\n\n    @defer.inlineCallbacks\n    def test_dns_server_ip_address(self):\n        crawler = get_crawler(SingleRequestSpider)\n        url = self.mockserver.url(\"/echo?body=test\")\n        expected_netloc, _ = urlparse(url).netloc.split(\":\")\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n        self.assertIsInstance(ip_address, IPv4Address)\n        self.assertEqual(str(ip_address), gethostbyname(expected_netloc))\n\n    @defer.inlineCallbacks\n    def test_bytes_received_stop_download_callback(self):\n        crawler = get_crawler(BytesReceivedCallbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n        self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n        self.assertEqual(\n            crawler.spider.meta[\"response\"].body,\n            crawler.spider.meta.get(\"bytes_received\"),\n        )\n        self.assertLess(\n            len(crawler.spider.meta[\"response\"].body),\n            crawler.spider.full_response_length,\n        )\n\n    @defer.inlineCallbacks\n    def test_bytes_received_stop_download_errback(self):\n        crawler = get_crawler(BytesReceivedErrbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertIsNone(crawler.spider.meta.get(\"response\"))\n        self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n        self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n        self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n        self.assertEqual(\n            crawler.spider.meta[\"failure\"].value.response.body,\n            crawler.spider.meta.get(\"bytes_received\"),\n        )\n        self.assertLess(\n            len(crawler.spider.meta[\"failure\"].value.response.body),\n            crawler.spider.full_response_length,\n        )\n\n    @defer.inlineCallbacks\n    def test_headers_received_stop_download_callback(self):\n        crawler = get_crawler(HeadersReceivedCallbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n        self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n        self.assertEqual(\n            crawler.spider.meta[\"response\"].headers,\n            crawler.spider.meta.get(\"headers_received\"),\n        )\n\n    @defer.inlineCallbacks\n    def test_headers_received_stop_download_errback(self):\n        crawler = get_crawler(HeadersReceivedErrbackSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertIsNone(crawler.spider.meta.get(\"response\"))\n        self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n        self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n        self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n        self.assertEqual(\n            crawler.spider.meta[\"failure\"].value.response.headers,\n            crawler.spider.meta.get(\"headers_received\"),\n        )\n", "tests/test_request_left.py": "from twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.signals import request_left_downloader\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\n\n\nclass SignalCatcherSpider(Spider):\n    name = \"signal_catcher\"\n\n    def __init__(self, crawler, url, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        crawler.signals.connect(self.on_request_left, signal=request_left_downloader)\n        self.caught_times = 0\n        self.start_urls = [url]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(crawler, *args, **kwargs)\n        return spider\n\n    def on_request_left(self, request, spider):\n        self.caught_times += 1\n\n\nclass TestCatching(TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_success(self):\n        crawler = get_crawler(SignalCatcherSpider)\n        yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n        self.assertEqual(crawler.spider.caught_times, 1)\n\n    @defer.inlineCallbacks\n    def test_timeout(self):\n        crawler = get_crawler(SignalCatcherSpider, {\"DOWNLOAD_TIMEOUT\": 0.1})\n        yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n        self.assertEqual(crawler.spider.caught_times, 1)\n\n    @defer.inlineCallbacks\n    def test_disconnect(self):\n        crawler = get_crawler(SignalCatcherSpider)\n        yield crawler.crawl(self.mockserver.url(\"/drop\"))\n        self.assertEqual(crawler.spider.caught_times, 1)\n\n    @defer.inlineCallbacks\n    def test_noconnect(self):\n        crawler = get_crawler(SignalCatcherSpider)\n        yield crawler.crawl(\"http://thereisdefinetelynosuchdomain.com\")\n        self.assertEqual(crawler.spider.caught_times, 1)\n", "tests/test_dupefilters.py": "import hashlib\nimport shutil\nimport sys\nimport tempfile\nimport unittest\nfrom pathlib import Path\n\nfrom testfixtures import LogCapture\n\nfrom scrapy.core.scheduler import Scheduler\nfrom scrapy.dupefilters import RFPDupeFilter\nfrom scrapy.http import Request\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import SimpleSpider\n\n\ndef _get_dupefilter(*, crawler=None, settings=None, open=True):\n    if crawler is None:\n        crawler = get_crawler(settings_dict=settings)\n    scheduler = Scheduler.from_crawler(crawler)\n    dupefilter = scheduler.df\n    if open:\n        dupefilter.open()\n    return dupefilter\n\n\nclass FromCrawlerRFPDupeFilter(RFPDupeFilter):\n    @classmethod\n    def from_crawler(cls, crawler):\n        df = super().from_crawler(crawler)\n        df.method = \"from_crawler\"\n        return df\n\n\nclass FromSettingsRFPDupeFilter(RFPDupeFilter):\n    @classmethod\n    def from_settings(cls, settings, *, fingerprinter=None):\n        df = super().from_settings(settings, fingerprinter=fingerprinter)\n        df.method = \"from_settings\"\n        return df\n\n\nclass DirectDupeFilter:\n    method = \"n/a\"\n\n\nclass RFPDupeFilterTest(unittest.TestCase):\n    def test_df_from_crawler_scheduler(self):\n        settings = {\n            \"DUPEFILTER_DEBUG\": True,\n            \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        self.assertTrue(scheduler.df.debug)\n        self.assertEqual(scheduler.df.method, \"from_crawler\")\n\n    def test_df_from_settings_scheduler(self):\n        settings = {\n            \"DUPEFILTER_DEBUG\": True,\n            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        self.assertTrue(scheduler.df.debug)\n        self.assertEqual(scheduler.df.method, \"from_settings\")\n\n    def test_df_direct_scheduler(self):\n        settings = {\n            \"DUPEFILTER_CLASS\": DirectDupeFilter,\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        self.assertEqual(scheduler.df.method, \"n/a\")\n\n    def test_filter(self):\n        dupefilter = _get_dupefilter()\n        r1 = Request(\"http://scrapytest.org/1\")\n        r2 = Request(\"http://scrapytest.org/2\")\n        r3 = Request(\"http://scrapytest.org/2\")\n\n        assert not dupefilter.request_seen(r1)\n        assert dupefilter.request_seen(r1)\n\n        assert not dupefilter.request_seen(r2)\n        assert dupefilter.request_seen(r3)\n\n        dupefilter.close(\"finished\")\n\n    def test_dupefilter_path(self):\n        r1 = Request(\"http://scrapytest.org/1\")\n        r2 = Request(\"http://scrapytest.org/2\")\n\n        path = tempfile.mkdtemp()\n        try:\n            df = _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n            try:\n                df.open()\n                assert not df.request_seen(r1)\n                assert df.request_seen(r1)\n            finally:\n                df.close(\"finished\")\n\n            df2 = _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n            assert df != df2\n            try:\n                df2.open()\n                assert df2.request_seen(r1)\n                assert not df2.request_seen(r2)\n                assert df2.request_seen(r2)\n            finally:\n                df2.close(\"finished\")\n        finally:\n            shutil.rmtree(path)\n\n    def test_request_fingerprint(self):\n        \"\"\"Test if customization of request_fingerprint method will change\n        output of request_seen.\n\n        \"\"\"\n        dupefilter = _get_dupefilter()\n        r1 = Request(\"http://scrapytest.org/index.html\")\n        r2 = Request(\"http://scrapytest.org/INDEX.html\")\n\n        assert not dupefilter.request_seen(r1)\n        assert not dupefilter.request_seen(r2)\n\n        dupefilter.close(\"finished\")\n\n        class RequestFingerprinter:\n            def fingerprint(self, request):\n                fp = hashlib.sha1()\n                fp.update(to_bytes(request.url.lower()))\n                return fp.digest()\n\n        settings = {\"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter}\n        case_insensitive_dupefilter = _get_dupefilter(settings=settings)\n\n        assert not case_insensitive_dupefilter.request_seen(r1)\n        assert case_insensitive_dupefilter.request_seen(r2)\n\n        case_insensitive_dupefilter.close(\"finished\")\n\n    def test_seenreq_newlines(self):\n        r\"\"\"Checks against adding duplicate \\r to\n        line endings on Windows platforms.\"\"\"\n\n        r1 = Request(\"http://scrapytest.org/1\")\n\n        path = tempfile.mkdtemp()\n        crawler = get_crawler(settings_dict={\"JOBDIR\": path})\n        try:\n            scheduler = Scheduler.from_crawler(crawler)\n            df = scheduler.df\n            df.open()\n            df.request_seen(r1)\n            df.close(\"finished\")\n\n            with Path(path, \"requests.seen\").open(\"rb\") as seen_file:\n                line = next(seen_file).decode()\n                assert not line.endswith(\"\\r\\r\\n\")\n                if sys.platform == \"win32\":\n                    assert line.endswith(\"\\r\\n\")\n                else:\n                    assert line.endswith(\"\\n\")\n\n        finally:\n            shutil.rmtree(path)\n\n    def test_log(self):\n        with LogCapture() as log:\n            settings = {\n                \"DUPEFILTER_DEBUG\": False,\n                \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n            }\n            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n            spider = SimpleSpider.from_crawler(crawler)\n            dupefilter = _get_dupefilter(crawler=crawler)\n\n            r1 = Request(\"http://scrapytest.org/index.html\")\n            r2 = Request(\"http://scrapytest.org/index.html\")\n\n            dupefilter.log(r1, spider)\n            dupefilter.log(r2, spider)\n\n            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more\"\n                    \" duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\",\n                )\n            )\n\n            dupefilter.close(\"finished\")\n\n    def test_log_debug(self):\n        with LogCapture() as log:\n            settings = {\n                \"DUPEFILTER_DEBUG\": True,\n                \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n            }\n            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n            spider = SimpleSpider.from_crawler(crawler)\n            dupefilter = _get_dupefilter(crawler=crawler)\n\n            r1 = Request(\"http://scrapytest.org/index.html\")\n            r2 = Request(\n                \"http://scrapytest.org/index.html\",\n                headers={\"Referer\": \"http://scrapytest.org/INDEX.html\"},\n            )\n\n            dupefilter.log(r1, spider)\n            dupefilter.log(r2, spider)\n\n            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)\",\n                )\n            )\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html>\"\n                    \" (referer: http://scrapytest.org/INDEX.html)\",\n                )\n            )\n\n            dupefilter.close(\"finished\")\n\n    def test_log_debug_default_dupefilter(self):\n        with LogCapture() as log:\n            settings = {\n                \"DUPEFILTER_DEBUG\": True,\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n            }\n            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n            spider = SimpleSpider.from_crawler(crawler)\n            dupefilter = _get_dupefilter(crawler=crawler)\n\n            r1 = Request(\"http://scrapytest.org/index.html\")\n            r2 = Request(\n                \"http://scrapytest.org/index.html\",\n                headers={\"Referer\": \"http://scrapytest.org/INDEX.html\"},\n            )\n\n            dupefilter.log(r1, spider)\n            dupefilter.log(r2, spider)\n\n            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)\",\n                )\n            )\n            log.check_present(\n                (\n                    \"scrapy.dupefilters\",\n                    \"DEBUG\",\n                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html>\"\n                    \" (referer: http://scrapytest.org/INDEX.html)\",\n                )\n            )\n\n            dupefilter.close(\"finished\")\n", "tests/test_urlparse_monkeypatches.py": "import unittest\nfrom urllib.parse import urlparse\n\n\nclass UrlparseTestCase(unittest.TestCase):\n    def test_s3_url(self):\n        p = urlparse(\"s3://bucket/key/name?param=value\")\n        self.assertEqual(p.scheme, \"s3\")\n        self.assertEqual(p.hostname, \"bucket\")\n        self.assertEqual(p.path, \"/key/name\")\n        self.assertEqual(p.query, \"param=value\")\n", "tests/test_contracts.py": "from unittest import TextTestResult\n\nfrom twisted.internet import defer\nfrom twisted.python import failure\nfrom twisted.trial import unittest\n\nfrom scrapy import FormRequest\nfrom scrapy.contracts import Contract, ContractsManager\nfrom scrapy.contracts.default import (\n    CallbackKeywordArgumentsContract,\n    ReturnsContract,\n    ScrapesContract,\n    UrlContract,\n)\nfrom scrapy.http import Request\nfrom scrapy.item import Field, Item\nfrom scrapy.spidermiddlewares.httperror import HttpError\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\n\n\nclass TestItem(Item):\n    name = Field()\n    url = Field()\n\n\nclass ResponseMock:\n    url = \"http://scrapy.org\"\n\n\nclass CustomSuccessContract(Contract):\n    name = \"custom_success_contract\"\n\n    def adjust_request_args(self, args):\n        args[\"url\"] = \"http://scrapy.org\"\n        return args\n\n\nclass CustomFailContract(Contract):\n    name = \"custom_fail_contract\"\n\n    def adjust_request_args(self, args):\n        raise TypeError(\"Error in adjust_request_args\")\n\n\nclass CustomFormContract(Contract):\n    name = \"custom_form\"\n    request_cls = FormRequest\n\n    def adjust_request_args(self, args):\n        args[\"formdata\"] = {\"name\": \"scrapy\"}\n        return args\n\n\nclass TestSpider(Spider):\n    name = \"demo_spider\"\n\n    def returns_request(self, response):\n        \"\"\"method which returns request\n        @url http://scrapy.org\n        @returns requests 1\n        \"\"\"\n        return Request(\"http://scrapy.org\", callback=self.returns_item)\n\n    async def returns_request_async(self, response):\n        \"\"\"async method which returns request\n        @url http://scrapy.org\n        @returns requests 1\n        \"\"\"\n        return Request(\"http://scrapy.org\", callback=self.returns_item)\n\n    def returns_item(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return TestItem(url=response.url)\n\n    def returns_request_cb_kwargs(self, response, url):\n        \"\"\"method which returns request\n        @url https://example.org\n        @cb_kwargs {\"url\": \"http://scrapy.org\"}\n        @returns requests 1\n        \"\"\"\n        return Request(url, callback=self.returns_item_cb_kwargs)\n\n    def returns_item_cb_kwargs(self, response, name):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @cb_kwargs {\"name\": \"Scrapy\"}\n        @returns items 1 1\n        \"\"\"\n        return TestItem(name=name, url=response.url)\n\n    def returns_item_cb_kwargs_error_unexpected_keyword(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @cb_kwargs {\"arg\": \"value\"}\n        @returns items 1 1\n        \"\"\"\n        return TestItem(url=response.url)\n\n    def returns_item_cb_kwargs_error_missing_argument(self, response, arg):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return TestItem(url=response.url)\n\n    def returns_dict_item(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return {\"url\": response.url}\n\n    def returns_fail(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 0 0\n        \"\"\"\n        return TestItem(url=response.url)\n\n    def returns_dict_fail(self, response):\n        \"\"\"method which returns item\n        @url http://scrapy.org\n        @returns items 0 0\n        \"\"\"\n        return {\"url\": response.url}\n\n    def scrapes_item_ok(self, response):\n        \"\"\"returns item with name and url\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return TestItem(name=\"test\", url=response.url)\n\n    def scrapes_dict_item_ok(self, response):\n        \"\"\"returns item with name and url\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return {\"name\": \"test\", \"url\": response.url}\n\n    def scrapes_item_fail(self, response):\n        \"\"\"returns item with no name\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return TestItem(url=response.url)\n\n    def scrapes_dict_item_fail(self, response):\n        \"\"\"returns item with no name\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return {\"url\": response.url}\n\n    def scrapes_multiple_missing_fields(self, response):\n        \"\"\"returns item with no name\n        @url http://scrapy.org\n        @returns items 1 1\n        @scrapes name url\n        \"\"\"\n        return {}\n\n    def parse_no_url(self, response):\n        \"\"\"method with no url\n        @returns items 1 1\n        \"\"\"\n        pass\n\n    def custom_form(self, response):\n        \"\"\"\n        @url http://scrapy.org\n        @custom_form\n        \"\"\"\n        pass\n\n    def invalid_regex(self, response):\n        \"\"\"method with invalid regex\n        @ Scrapy is awsome\n        \"\"\"\n        pass\n\n    def invalid_regex_with_valid_contract(self, response):\n        \"\"\"method with invalid regex\n        @ scrapy is awsome\n        @url http://scrapy.org\n        \"\"\"\n        pass\n\n\nclass CustomContractSuccessSpider(Spider):\n    name = \"custom_contract_success_spider\"\n\n    def parse(self, response):\n        \"\"\"\n        @custom_success_contract\n        \"\"\"\n        pass\n\n\nclass CustomContractFailSpider(Spider):\n    name = \"custom_contract_fail_spider\"\n\n    def parse(self, response):\n        \"\"\"\n        @custom_fail_contract\n        \"\"\"\n        pass\n\n\nclass InheritsTestSpider(TestSpider):\n    name = \"inherits_demo_spider\"\n\n\nclass ContractsManagerTest(unittest.TestCase):\n    contracts = [\n        UrlContract,\n        CallbackKeywordArgumentsContract,\n        ReturnsContract,\n        ScrapesContract,\n        CustomFormContract,\n        CustomSuccessContract,\n        CustomFailContract,\n    ]\n\n    def setUp(self):\n        self.conman = ContractsManager(self.contracts)\n        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n\n    def should_succeed(self):\n        self.assertFalse(self.results.failures)\n        self.assertFalse(self.results.errors)\n\n    def should_fail(self):\n        self.assertTrue(self.results.failures)\n        self.assertFalse(self.results.errors)\n\n    def should_error(self):\n        self.assertTrue(self.results.errors)\n\n    def test_contracts(self):\n        spider = TestSpider()\n\n        # extract contracts correctly\n        contracts = self.conman.extract_contracts(spider.returns_request)\n        self.assertEqual(len(contracts), 2)\n        self.assertEqual(\n            frozenset(type(x) for x in contracts),\n            frozenset([UrlContract, ReturnsContract]),\n        )\n\n        # returns request for valid method\n        request = self.conman.from_method(spider.returns_request, self.results)\n        self.assertNotEqual(request, None)\n\n        # no request for missing url\n        request = self.conman.from_method(spider.parse_no_url, self.results)\n        self.assertEqual(request, None)\n\n    def test_cb_kwargs(self):\n        spider = TestSpider()\n        response = ResponseMock()\n\n        # extract contracts correctly\n        contracts = self.conman.extract_contracts(spider.returns_request_cb_kwargs)\n        self.assertEqual(len(contracts), 3)\n        self.assertEqual(\n            frozenset(type(x) for x in contracts),\n            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n        )\n\n        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs)\n        self.assertEqual(len(contracts), 3)\n        self.assertEqual(\n            frozenset(type(x) for x in contracts),\n            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n        )\n\n        contracts = self.conman.extract_contracts(\n            spider.returns_item_cb_kwargs_error_unexpected_keyword\n        )\n        self.assertEqual(len(contracts), 3)\n        self.assertEqual(\n            frozenset(type(x) for x in contracts),\n            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n        )\n\n        contracts = self.conman.extract_contracts(\n            spider.returns_item_cb_kwargs_error_missing_argument\n        )\n        self.assertEqual(len(contracts), 2)\n        self.assertEqual(\n            frozenset(type(x) for x in contracts),\n            frozenset([UrlContract, ReturnsContract]),\n        )\n\n        # returns_request\n        request = self.conman.from_method(\n            spider.returns_request_cb_kwargs, self.results\n        )\n        request.callback(response, **request.cb_kwargs)\n        self.should_succeed()\n\n        # returns_item\n        request = self.conman.from_method(spider.returns_item_cb_kwargs, self.results)\n        request.callback(response, **request.cb_kwargs)\n        self.should_succeed()\n\n        # returns_item (error, callback doesn't take keyword arguments)\n        request = self.conman.from_method(\n            spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results\n        )\n        request.callback(response, **request.cb_kwargs)\n        self.should_error()\n\n        # returns_item (error, contract doesn't provide keyword arguments)\n        request = self.conman.from_method(\n            spider.returns_item_cb_kwargs_error_missing_argument, self.results\n        )\n        request.callback(response, **request.cb_kwargs)\n        self.should_error()\n\n    def test_returns(self):\n        spider = TestSpider()\n        response = ResponseMock()\n\n        # returns_item\n        request = self.conman.from_method(spider.returns_item, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # returns_dict_item\n        request = self.conman.from_method(spider.returns_dict_item, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # returns_request\n        request = self.conman.from_method(spider.returns_request, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # returns_fail\n        request = self.conman.from_method(spider.returns_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n        # returns_dict_fail\n        request = self.conman.from_method(spider.returns_dict_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n    def test_returns_async(self):\n        spider = TestSpider()\n        response = ResponseMock()\n\n        request = self.conman.from_method(spider.returns_request_async, self.results)\n        request.callback(response)\n        self.should_error()\n\n    def test_scrapes(self):\n        spider = TestSpider()\n        response = ResponseMock()\n\n        # scrapes_item_ok\n        request = self.conman.from_method(spider.scrapes_item_ok, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # scrapes_dict_item_ok\n        request = self.conman.from_method(spider.scrapes_dict_item_ok, self.results)\n        request.callback(response)\n        self.should_succeed()\n\n        # scrapes_item_fail\n        request = self.conman.from_method(spider.scrapes_item_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n        # scrapes_dict_item_fail\n        request = self.conman.from_method(spider.scrapes_dict_item_fail, self.results)\n        request.callback(response)\n        self.should_fail()\n\n        # scrapes_multiple_missing_fields\n        request = self.conman.from_method(\n            spider.scrapes_multiple_missing_fields, self.results\n        )\n        request.callback(response)\n        self.should_fail()\n        message = \"ContractFail: Missing fields: name, url\"\n        assert message in self.results.failures[-1][-1]\n\n    def test_regex(self):\n        spider = TestSpider()\n        response = ResponseMock()\n\n        # invalid regex\n        request = self.conman.from_method(spider.invalid_regex, self.results)\n        self.should_succeed()\n\n        # invalid regex with valid contract\n        request = self.conman.from_method(\n            spider.invalid_regex_with_valid_contract, self.results\n        )\n        self.should_succeed()\n        request.callback(response)\n\n    def test_custom_contracts(self):\n        self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n        self.should_succeed()\n\n        self.conman.from_spider(CustomContractFailSpider(), self.results)\n        self.should_error()\n\n    def test_errback(self):\n        spider = TestSpider()\n        response = ResponseMock()\n\n        try:\n            raise HttpError(response, \"Ignoring non-200 response\")\n        except HttpError:\n            failure_mock = failure.Failure()\n\n        request = self.conman.from_method(spider.returns_request, self.results)\n        request.errback(failure_mock)\n\n        self.assertFalse(self.results.failures)\n        self.assertTrue(self.results.errors)\n\n    @defer.inlineCallbacks\n    def test_same_url(self):\n        class TestSameUrlSpider(Spider):\n            name = \"test_same_url\"\n\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.visited = 0\n\n            def start_requests(s):\n                return self.conman.from_spider(s, self.results)\n\n            def parse_first(self, response):\n                self.visited += 1\n                return TestItem()\n\n            def parse_second(self, response):\n                self.visited += 1\n                return TestItem()\n\n        with MockServer() as mockserver:\n            contract_doc = f'@url {mockserver.url(\"/status?n=200\")}'\n\n            TestSameUrlSpider.parse_first.__doc__ = contract_doc\n            TestSameUrlSpider.parse_second.__doc__ = contract_doc\n\n            crawler = get_crawler(TestSameUrlSpider)\n            yield crawler.crawl()\n\n        self.assertEqual(crawler.spider.visited, 2)\n\n    def test_form_contract(self):\n        spider = TestSpider()\n        request = self.conman.from_method(spider.custom_form, self.results)\n        self.assertEqual(request.method, \"POST\")\n        self.assertIsInstance(request, FormRequest)\n\n    def test_inherited_contracts(self):\n        spider = InheritsTestSpider()\n\n        requests = self.conman.from_spider(spider, self.results)\n        self.assertTrue(requests)\n", "tests/test_spidermiddleware_referer.py": "import warnings\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom unittest import TestCase\nfrom urllib.parse import urlparse\n\nfrom scrapy.downloadermiddlewares.redirect import RedirectMiddleware\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spidermiddlewares.referer import (\n    POLICY_NO_REFERRER,\n    POLICY_NO_REFERRER_WHEN_DOWNGRADE,\n    POLICY_ORIGIN,\n    POLICY_ORIGIN_WHEN_CROSS_ORIGIN,\n    POLICY_SAME_ORIGIN,\n    POLICY_SCRAPY_DEFAULT,\n    POLICY_STRICT_ORIGIN,\n    POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n    POLICY_UNSAFE_URL,\n    DefaultReferrerPolicy,\n    NoReferrerPolicy,\n    NoReferrerWhenDowngradePolicy,\n    OriginPolicy,\n    OriginWhenCrossOriginPolicy,\n    RefererMiddleware,\n    ReferrerPolicy,\n    SameOriginPolicy,\n    StrictOriginPolicy,\n    StrictOriginWhenCrossOriginPolicy,\n    UnsafeUrlPolicy,\n)\nfrom scrapy.spiders import Spider\n\n\nclass TestRefererMiddleware(TestCase):\n    req_meta: Dict[str, Any] = {}\n    resp_headers: Dict[str, str] = {}\n    settings: Dict[str, Any] = {}\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n    ]\n\n    def setUp(self):\n        self.spider = Spider(\"foo\")\n        settings = Settings(self.settings)\n        self.mw = RefererMiddleware(settings)\n\n    def get_request(self, target):\n        return Request(target, meta=self.req_meta)\n\n    def get_response(self, origin):\n        return Response(origin, headers=self.resp_headers)\n\n    def test(self):\n        for origin, target, referrer in self.scenarii:\n            response = self.get_response(origin)\n            request = self.get_request(target)\n            out = list(self.mw.process_spider_output(response, [request], self.spider))\n            self.assertEqual(out[0].headers.get(\"Referer\"), referrer)\n\n\nclass MixinDefault:\n    \"\"\"\n    Based on https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n\n    with some additional filtering of s3://\n    \"\"\"\n\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        (\"https://example.com/\", \"https://scrapy.org/\", b\"https://example.com/\"),\n        (\"http://example.com/\", \"http://scrapy.org/\", b\"http://example.com/\"),\n        (\"http://example.com/\", \"https://scrapy.org/\", b\"http://example.com/\"),\n        (\"https://example.com/\", \"http://scrapy.org/\", None),\n        # no credentials leak\n        (\n            \"http://user:password@example.com/\",\n            \"https://scrapy.org/\",\n            b\"http://example.com/\",\n        ),\n        # no referrer leak for local schemes\n        (\"file:///home/path/to/somefile.html\", \"https://scrapy.org/\", None),\n        (\"file:///home/path/to/somefile.html\", \"http://scrapy.org/\", None),\n        # no referrer leak for s3 origins\n        (\"s3://mybucket/path/to/data.csv\", \"https://scrapy.org/\", None),\n        (\"s3://mybucket/path/to/data.csv\", \"http://scrapy.org/\", None),\n    ]\n\n\nclass MixinNoReferrer:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        (\"https://example.com/page.html\", \"https://example.com/\", None),\n        (\"http://www.example.com/\", \"https://scrapy.org/\", None),\n        (\"http://www.example.com/\", \"http://scrapy.org/\", None),\n        (\"https://www.example.com/\", \"http://scrapy.org/\", None),\n        (\"file:///home/path/to/somefile.html\", \"http://scrapy.org/\", None),\n    ]\n\n\nclass MixinNoReferrerWhenDowngrade:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # TLS to TLS: send non-empty referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://not.example.com/\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"https://example.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:444/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example.com:444/page.html\",\n        ),\n        (\n            \"ftps://example.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftps://example.com/urls.zip\",\n        ),\n        # TLS to non-TLS: do not send referrer\n        (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n        (\"https://example.com/page.html\", \"http://scrapy.org/\", None),\n        (\"ftps://example.com/urls.zip\", \"http://scrapy.org/\", None),\n        # non-TLS to TLS or non-TLS: send referrer\n        (\n            \"http://example.com/page.html\",\n            \"https://not.example.com/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8080/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example.com:8080/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://not.example.com/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:443/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example.com:443/page.html\",\n        ),\n        (\n            \"ftp://example.com/urls.zip\",\n            \"http://scrapy.org/\",\n            b\"ftp://example.com/urls.zip\",\n        ),\n        (\n            \"ftp://example.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftp://example.com/urls.zip\",\n        ),\n        # test for user/password stripping\n        (\n            \"http://user:password@example.com/page.html\",\n            \"https://not.example.com/\",\n            b\"http://example.com/page.html\",\n        ),\n    ]\n\n\nclass MixinSameOrigin:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # Same origin (protocol, host, port): send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com:80/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8888/page.html\",\n            \"http://example.com:8888/not-page.html\",\n            b\"http://example.com:8888/page.html\",\n        ),\n        # Different host: do NOT send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://not.example.com/otherpage.html\",\n            None,\n        ),\n        (\"http://example.com/page.html\", \"http://not.example.com/otherpage.html\", None),\n        (\"http://example.com/page.html\", \"http://www.example.com/otherpage.html\", None),\n        # Different port: do NOT send referrer\n        (\n            \"https://example.com:444/page.html\",\n            \"https://example.com/not-page.html\",\n            None,\n        ),\n        (\"http://example.com:81/page.html\", \"http://example.com/not-page.html\", None),\n        (\"http://example.com/page.html\", \"http://example.com:81/not-page.html\", None),\n        # Different protocols: do NOT send referrer\n        (\"https://example.com/page.html\", \"http://example.com/not-page.html\", None),\n        (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n        (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n        (\"ftp://example.com/urls.zip\", \"http://example.com/not-page.html\", None),\n        (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n        # test for user/password stripping\n        (\n            \"https://user:password@example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            None,\n        ),\n        (\n            \"https://user:password@example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n    ]\n\n\nclass MixinOrigin:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/\",\n        ),\n        (\n            \"https://example.com/page.html\",\n            \"https://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n        (\"https://example.com/page.html\", \"http://scrapy.org\", b\"https://example.com/\"),\n        (\"http://example.com/page.html\", \"http://scrapy.org\", b\"http://example.com/\"),\n        # test for user/password stripping\n        (\n            \"https://user:password@example.com/page.html\",\n            \"http://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n    ]\n\n\nclass MixinStrictOrigin:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/\",\n        ),\n        (\n            \"https://example.com/page.html\",\n            \"https://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n        (\"http://example.com/page.html\", \"http://scrapy.org\", b\"http://example.com/\"),\n        # downgrade: send nothing\n        (\"https://example.com/page.html\", \"http://scrapy.org\", None),\n        # upgrade: send origin\n        (\"http://example.com/page.html\", \"https://scrapy.org\", b\"http://example.com/\"),\n        # test for user/password stripping\n        (\n            \"https://user:password@example.com/page.html\",\n            \"https://scrapy.org\",\n            b\"https://example.com/\",\n        ),\n        (\"https://user:password@example.com/page.html\", \"http://scrapy.org\", None),\n    ]\n\n\nclass MixinOriginWhenCrossOrigin:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # Same origin (protocol, host, port): send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com:80/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8888/page.html\",\n            \"http://example.com:8888/not-page.html\",\n            b\"http://example.com:8888/page.html\",\n        ),\n        # Different host: send origin as referrer\n        (\n            \"https://example2.com/page.html\",\n            \"https://scrapy.org/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"https://example2.com/page.html\",\n            \"https://not.example2.com/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"http://example2.com/page.html\",\n            \"http://not.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # exact match required\n        (\n            \"http://example2.com/page.html\",\n            \"http://www.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # Different port: send origin as referrer\n        (\n            \"https://example3.com:444/page.html\",\n            \"https://example3.com/not-page.html\",\n            b\"https://example3.com:444/\",\n        ),\n        (\n            \"http://example3.com:81/page.html\",\n            \"http://example3.com/not-page.html\",\n            b\"http://example3.com:81/\",\n        ),\n        # Different protocols: send origin as referrer\n        (\n            \"https://example4.com/page.html\",\n            \"http://example4.com/not-page.html\",\n            b\"https://example4.com/\",\n        ),\n        (\n            \"https://example4.com/page.html\",\n            \"http://not.example4.com/\",\n            b\"https://example4.com/\",\n        ),\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        (\n            \"ftp://example4.com/urls.zip\",\n            \"http://example4.com/not-page.html\",\n            b\"ftp://example4.com/\",\n        ),\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        # test for user/password stripping\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"https://example5.com/not-page.html\",\n            b\"https://example5.com/page.html\",\n        ),\n        # TLS to non-TLS downgrade: send origin\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"http://example5.com/not-page.html\",\n            b\"https://example5.com/\",\n        ),\n    ]\n\n\nclass MixinStrictOriginWhenCrossOrigin:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # Same origin (protocol, host, port): send referrer\n        (\n            \"https://example.com/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"https://example.com:443/page.html\",\n            \"https://example.com/not-page.html\",\n            b\"https://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:80/page.html\",\n            \"http://example.com/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com/page.html\",\n            \"http://example.com:80/not-page.html\",\n            b\"http://example.com/page.html\",\n        ),\n        (\n            \"http://example.com:8888/page.html\",\n            \"http://example.com:8888/not-page.html\",\n            b\"http://example.com:8888/page.html\",\n        ),\n        # Different host: send origin as referrer\n        (\n            \"https://example2.com/page.html\",\n            \"https://scrapy.org/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"https://example2.com/page.html\",\n            \"https://not.example2.com/otherpage.html\",\n            b\"https://example2.com/\",\n        ),\n        (\n            \"http://example2.com/page.html\",\n            \"http://not.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # exact match required\n        (\n            \"http://example2.com/page.html\",\n            \"http://www.example2.com/otherpage.html\",\n            b\"http://example2.com/\",\n        ),\n        # Different port: send origin as referrer\n        (\n            \"https://example3.com:444/page.html\",\n            \"https://example3.com/not-page.html\",\n            b\"https://example3.com:444/\",\n        ),\n        (\n            \"http://example3.com:81/page.html\",\n            \"http://example3.com/not-page.html\",\n            b\"http://example3.com:81/\",\n        ),\n        # downgrade\n        (\"https://example4.com/page.html\", \"http://example4.com/not-page.html\", None),\n        (\"https://example4.com/page.html\", \"http://not.example4.com/\", None),\n        # non-TLS to non-TLS\n        (\n            \"ftp://example4.com/urls.zip\",\n            \"http://example4.com/not-page.html\",\n            b\"ftp://example4.com/\",\n        ),\n        # upgrade\n        (\n            \"http://example4.com/page.html\",\n            \"https://example4.com/not-page.html\",\n            b\"http://example4.com/\",\n        ),\n        (\n            \"http://example4.com/page.html\",\n            \"https://not.example4.com/\",\n            b\"http://example4.com/\",\n        ),\n        # Different protocols: send origin as referrer\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        (\n            \"ftps://example4.com/urls.zip\",\n            \"https://example4.com/not-page.html\",\n            b\"ftps://example4.com/\",\n        ),\n        # test for user/password stripping\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"https://example5.com/not-page.html\",\n            b\"https://example5.com/page.html\",\n        ),\n        # TLS to non-TLS downgrade: send nothing\n        (\n            \"https://user:password@example5.com/page.html\",\n            \"http://example5.com/not-page.html\",\n            None,\n        ),\n    ]\n\n\nclass MixinUnsafeUrl:\n    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n        # TLS to TLS: send referrer\n        (\n            \"https://example.com/sekrit.html\",\n            \"http://not.example.com/\",\n            b\"https://example.com/sekrit.html\",\n        ),\n        (\n            \"https://example1.com/page.html\",\n            \"https://not.example1.com/\",\n            b\"https://example1.com/page.html\",\n        ),\n        (\n            \"https://example1.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example1.com/page.html\",\n        ),\n        (\n            \"https://example1.com:443/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example1.com/page.html\",\n        ),\n        (\n            \"https://example1.com:444/page.html\",\n            \"https://scrapy.org/\",\n            b\"https://example1.com:444/page.html\",\n        ),\n        (\n            \"ftps://example1.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftps://example1.com/urls.zip\",\n        ),\n        # TLS to non-TLS: send referrer (yes, it's unsafe)\n        (\n            \"https://example2.com/page.html\",\n            \"http://not.example2.com/\",\n            b\"https://example2.com/page.html\",\n        ),\n        (\n            \"https://example2.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"https://example2.com/page.html\",\n        ),\n        (\n            \"ftps://example2.com/urls.zip\",\n            \"http://scrapy.org/\",\n            b\"ftps://example2.com/urls.zip\",\n        ),\n        # non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)\n        (\n            \"http://example3.com/page.html\",\n            \"https://not.example3.com/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com:8080/page.html\",\n            \"https://scrapy.org/\",\n            b\"http://example3.com:8080/page.html\",\n        ),\n        (\n            \"http://example3.com:80/page.html\",\n            \"http://not.example3.com/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example3.com/page.html\",\n        ),\n        (\n            \"http://example3.com:443/page.html\",\n            \"http://scrapy.org/\",\n            b\"http://example3.com:443/page.html\",\n        ),\n        (\n            \"ftp://example3.com/urls.zip\",\n            \"http://scrapy.org/\",\n            b\"ftp://example3.com/urls.zip\",\n        ),\n        (\n            \"ftp://example3.com/urls.zip\",\n            \"https://scrapy.org/\",\n            b\"ftp://example3.com/urls.zip\",\n        ),\n        # test for user/password stripping\n        (\n            \"http://user:password@example4.com/page.html\",\n            \"https://not.example4.com/\",\n            b\"http://example4.com/page.html\",\n        ),\n        (\n            \"https://user:password@example4.com/page.html\",\n            \"http://scrapy.org/\",\n            b\"https://example4.com/page.html\",\n        ),\n    ]\n\n\nclass TestRefererMiddlewareDefault(MixinDefault, TestRefererMiddleware):\n    pass\n\n\n# --- Tests using settings to set policy using class path\nclass TestSettingsNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerPolicy\"}\n\n\nclass TestSettingsNoReferrerWhenDowngrade(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n    }\n\n\nclass TestSettingsSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n\n\nclass TestSettingsOrigin(MixinOrigin, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginPolicy\"}\n\n\nclass TestSettingsStrictOrigin(MixinStrictOrigin, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.StrictOriginPolicy\"\n    }\n\n\nclass TestSettingsOriginWhenCrossOrigin(\n    MixinOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n\n\nclass TestSettingsStrictOriginWhenCrossOrigin(\n    MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy\"\n    }\n\n\nclass TestSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n\n\nclass CustomPythonOrgPolicy(ReferrerPolicy):\n    \"\"\"\n    A dummy policy that returns referrer as http(s)://python.org\n    depending on the scheme of the target URL.\n    \"\"\"\n\n    def referrer(self, response, request):\n        scheme = urlparse(request).scheme\n        if scheme == \"https\":\n            return b\"https://python.org/\"\n        if scheme == \"http\":\n            return b\"http://python.org/\"\n\n\nclass TestSettingsCustomPolicy(TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": CustomPythonOrgPolicy}\n    scenarii = [\n        (\"https://example.com/\", \"https://scrapy.org/\", b\"https://python.org/\"),\n        (\"http://example.com/\", \"http://scrapy.org/\", b\"http://python.org/\"),\n        (\"http://example.com/\", \"https://scrapy.org/\", b\"https://python.org/\"),\n        (\"https://example.com/\", \"http://scrapy.org/\", b\"http://python.org/\"),\n        (\n            \"file:///home/path/to/somefile.html\",\n            \"https://scrapy.org/\",\n            b\"https://python.org/\",\n        ),\n        (\n            \"file:///home/path/to/somefile.html\",\n            \"http://scrapy.org/\",\n            b\"http://python.org/\",\n        ),\n    ]\n\n\n# --- Tests using Request meta dict to set policy\nclass TestRequestMetaDefault(MixinDefault, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_SCRAPY_DEFAULT}\n\n\nclass TestRequestMetaNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n\n\nclass TestRequestMetaNoReferrerWhenDowngrade(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE}\n\n\nclass TestRequestMetaSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_SAME_ORIGIN}\n\n\nclass TestRequestMetaOrigin(MixinOrigin, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_ORIGIN}\n\n\nclass TestRequestMetaSrictOrigin(MixinStrictOrigin, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_STRICT_ORIGIN}\n\n\nclass TestRequestMetaOriginWhenCrossOrigin(\n    MixinOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    req_meta = {\"referrer_policy\": POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n\n\nclass TestRequestMetaStrictOriginWhenCrossOrigin(\n    MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware\n):\n    req_meta = {\"referrer_policy\": POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n\n\nclass TestRequestMetaUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n\n\nclass TestRequestMetaPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n\n\nclass TestRequestMetaPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n    }\n    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n\n\nclass TestRequestMetaPrecedence003(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n\n\nclass TestRequestMetaSettingFallback(TestCase):\n    params = [\n        (\n            # When an unknown policy is referenced in Request.meta\n            # (here, a typo error),\n            # the policy defined in settings takes precedence\n            {\n                \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n            },\n            {},\n            {\"referrer_policy\": \"ssscrapy-default\"},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n        (\n            # same as above but with string value for settings policy\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {},\n            {\"referrer_policy\": \"ssscrapy-default\"},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n        (\n            # request meta references a wrong policy but it is set,\n            # so the Referrer-Policy header in response is not used,\n            # and the settings' policy is applied\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {\"Referrer-Policy\": \"unsafe-url\"},\n            {\"referrer_policy\": \"ssscrapy-default\"},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n        (\n            # here, request meta does not set the policy\n            # so response headers take precedence\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {\"Referrer-Policy\": \"unsafe-url\"},\n            {},\n            UnsafeUrlPolicy,\n            False,\n        ),\n        (\n            # here, request meta does not set the policy,\n            # but response headers also use an unknown policy,\n            # so the settings' policy is used\n            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n            {\"Referrer-Policy\": \"unknown\"},\n            {},\n            OriginWhenCrossOriginPolicy,\n            True,\n        ),\n    ]\n\n    def test(self):\n        origin = \"http://www.scrapy.org\"\n        target = \"http://www.example.com\"\n\n        for (\n            settings,\n            response_headers,\n            request_meta,\n            policy_class,\n            check_warning,\n        ) in self.params[3:]:\n            mw = RefererMiddleware(Settings(settings))\n\n            response = Response(origin, headers=response_headers)\n            request = Request(target, meta=request_meta)\n\n            with warnings.catch_warnings(record=True) as w:\n                policy = mw.policy(response, request)\n                self.assertIsInstance(policy, policy_class)\n\n                if check_warning:\n                    self.assertEqual(len(w), 1)\n                    self.assertEqual(w[0].category, RuntimeWarning, w[0].message)\n\n\nclass TestSettingsPolicyByName(TestCase):\n    def test_valid_name(self):\n        for s, p in [\n            (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n            (POLICY_NO_REFERRER, NoReferrerPolicy),\n            (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),\n            (POLICY_SAME_ORIGIN, SameOriginPolicy),\n            (POLICY_ORIGIN, OriginPolicy),\n            (POLICY_STRICT_ORIGIN, StrictOriginPolicy),\n            (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),\n            (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),\n            (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n        ]:\n            settings = Settings({\"REFERRER_POLICY\": s})\n            mw = RefererMiddleware(settings)\n            self.assertEqual(mw.default_policy, p)\n\n    def test_valid_name_casevariants(self):\n        for s, p in [\n            (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n            (POLICY_NO_REFERRER, NoReferrerPolicy),\n            (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),\n            (POLICY_SAME_ORIGIN, SameOriginPolicy),\n            (POLICY_ORIGIN, OriginPolicy),\n            (POLICY_STRICT_ORIGIN, StrictOriginPolicy),\n            (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),\n            (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),\n            (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n        ]:\n            settings = Settings({\"REFERRER_POLICY\": s.upper()})\n            mw = RefererMiddleware(settings)\n            self.assertEqual(mw.default_policy, p)\n\n    def test_invalid_name(self):\n        settings = Settings({\"REFERRER_POLICY\": \"some-custom-unknown-policy\"})\n        with self.assertRaises(RuntimeError):\n            RefererMiddleware(settings)\n\n    def test_multiple_policy_tokens(self):\n        # test parsing without space(s) after the comma\n        settings1 = Settings(\n            {\n                \"REFERRER_POLICY\": \",\".join(\n                    [\n                        \"some-custom-unknown-policy\",\n                        POLICY_SAME_ORIGIN,\n                        POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n                        \"another-custom-unknown-policy\",\n                    ]\n                )\n            }\n        )\n        mw1 = RefererMiddleware(settings1)\n        self.assertEqual(mw1.default_policy, StrictOriginWhenCrossOriginPolicy)\n\n        # test parsing with space(s) after the comma\n        settings2 = Settings(\n            {\n                \"REFERRER_POLICY\": \",    \".join(\n                    [\n                        POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n                        \"another-custom-unknown-policy\",\n                        POLICY_UNSAFE_URL,\n                    ]\n                )\n            }\n        )\n        mw2 = RefererMiddleware(settings2)\n        self.assertEqual(mw2.default_policy, UnsafeUrlPolicy)\n\n    def test_multiple_policy_tokens_all_invalid(self):\n        settings = Settings(\n            {\n                \"REFERRER_POLICY\": \",\".join(\n                    [\n                        \"some-custom-unknown-policy\",\n                        \"another-custom-unknown-policy\",\n                        \"yet-another-custom-unknown-policy\",\n                    ]\n                )\n            }\n        )\n        with self.assertRaises(RuntimeError):\n            RefererMiddleware(settings)\n\n\nclass TestPolicyHeaderPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n    resp_headers = {\"Referrer-Policy\": POLICY_UNSAFE_URL.upper()}\n\n\nclass TestPolicyHeaderPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n    }\n    resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER.swapcase()}\n\n\nclass TestPolicyHeaderPrecedence003(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n    resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n\n\nclass TestPolicyHeaderPrecedence004(\n    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n):\n    \"\"\"\n    The empty string means \"no-referrer-when-downgrade\"\n    \"\"\"\n\n    settings = {\n        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n    }\n    resp_headers = {\"Referrer-Policy\": \"\"}\n\n\nclass TestReferrerOnRedirect(TestRefererMiddleware):\n    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n    scenarii: List[\n        Tuple[str, str, Tuple[Tuple[int, str], ...], Optional[bytes], Optional[bytes]]\n    ] = [  # type: ignore[assignment]\n        (\n            \"http://scrapytest.org/1\",  # parent\n            \"http://scrapytest.org/2\",  # target\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/3\"),\n                (301, \"http://scrapytest.org/4\"),\n            ),\n            b\"http://scrapytest.org/1\",  # expected initial referer\n            b\"http://scrapytest.org/1\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://scrapytest.org/2\",\n            (\n                # redirecting to non-secure URL\n                (301, \"http://scrapytest.org/3\"),\n            ),\n            b\"https://scrapytest.org/1\",\n            b\"https://scrapytest.org/1\",\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://scrapytest.com/2\",\n            (\n                # redirecting to non-secure URL: different origin\n                (301, \"http://scrapytest.com/3\"),\n            ),\n            b\"https://scrapytest.org/1\",\n            b\"https://scrapytest.org/1\",\n        ),\n    ]\n\n    def setUp(self):\n        self.spider = Spider(\"foo\")\n        settings = Settings(self.settings)\n        self.referrermw = RefererMiddleware(settings)\n        self.redirectmw = RedirectMiddleware(settings)\n\n    def test(self):\n        for (\n            parent,\n            target,\n            redirections,\n            init_referrer,\n            final_referrer,\n        ) in self.scenarii:\n            response = self.get_response(parent)\n            request = self.get_request(target)\n\n            out = list(\n                self.referrermw.process_spider_output(response, [request], self.spider)\n            )\n            self.assertEqual(out[0].headers.get(\"Referer\"), init_referrer)\n\n            for status, url in redirections:\n                response = Response(\n                    request.url, headers={\"Location\": url}, status=status\n                )\n                request = self.redirectmw.process_response(\n                    request, response, self.spider\n                )\n                self.referrermw.request_scheduled(request, self.spider)\n\n            assert isinstance(request, Request)\n            self.assertEqual(request.headers.get(\"Referer\"), final_referrer)\n\n\nclass TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n    \"\"\"\n    No Referrer policy never sets the \"Referer\" header.\n    HTTP redirections should not change that.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": \"no-referrer\"}\n    scenarii = [\n        (\n            \"http://scrapytest.org/1\",  # parent\n            \"http://scrapytest.org/2\",  # target\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/3\"),\n                (301, \"http://scrapytest.org/4\"),\n            ),\n            None,  # expected initial \"Referer\"\n            None,  # expected \"Referer\" for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://scrapytest.org/2\",\n            ((301, \"http://scrapytest.org/3\"),),\n            None,\n            None,\n        ),\n        (\n            \"https://scrapytest.org/1\",\n            \"https://example.com/2\",  # different origin\n            ((301, \"http://scrapytest.com/3\"),),\n            None,\n            None,\n        ),\n    ]\n\n\nclass TestReferrerOnRedirectSameOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Same Origin policy sends the full URL as \"Referer\" if the target origin\n    is the same as the parent response (same protocol, same domain, same port).\n\n    HTTP redirections to a different domain or a lower secure level\n    should have the \"Referer\" removed.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": \"same-origin\"}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",  # origin\n            \"http://scrapytest.org/102\",  # target\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/101\",  # expected initial \"Referer\"\n            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting from secure to non-secure URL == different origin\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/201\",\n            None,\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # different domain == different origin\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/301\",\n            None,\n        ),\n    ]\n\n\nclass TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Strict Origin policy will always send the \"origin\" as referrer\n    (think of it as the parent URL without the path part),\n    unless the security level is lower and no \"Referer\" is sent.\n\n    Redirections from secure to non-secure URLs should have the\n    \"Referrer\" header removed if necessary.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": POLICY_STRICT_ORIGIN}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",\n            \"http://scrapytest.org/102\",\n            (\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/\",  # send origin\n            b\"http://scrapytest.org/\",  # redirects to same origin: send origin\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting to non-secure URL: no referrer\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/\",\n            None,\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # redirecting to non-secure URL (different domain): no referrer\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/\",\n            None,\n        ),\n        (\n            \"http://scrapy.org/401\",\n            \"http://example.com/402\",\n            ((301, \"http://scrapytest.org/403\"),),\n            b\"http://scrapy.org/\",\n            b\"http://scrapy.org/\",\n        ),\n        (\n            \"https://scrapy.org/501\",\n            \"https://example.com/502\",\n            (\n                # HTTPS all along, so origin referrer is kept as-is\n                (301, \"https://google.com/503\"),\n                (301, \"https://facebook.com/504\"),\n            ),\n            b\"https://scrapy.org/\",\n            b\"https://scrapy.org/\",\n        ),\n        (\n            \"https://scrapytest.org/601\",\n            \"http://scrapytest.org/602\",  # TLS to non-TLS: no referrer\n            (\n                (\n                    301,\n                    \"https://scrapytest.org/603\",\n                ),  # TLS URL again: (still) no referrer\n            ),\n            None,\n            None,\n        ),\n    ]\n\n\nclass TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Origin When Cross-Origin policy sends the full URL as \"Referer\",\n    unless the target's origin is different (different domain, different protocol)\n    in which case only the origin is sent.\n\n    Redirections to a different origin should strip the \"Referer\"\n    to the parent origin.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",  # origin\n            \"http://scrapytest.org/102\",  # target + redirection\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/101\",  # expected initial referer\n            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting to non-secure URL: send origin\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/201\",\n            b\"https://scrapytest.org/\",\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # redirecting to non-secure URL (different domain): send origin\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/301\",\n            b\"https://scrapytest.org/\",\n        ),\n        (\n            \"http://scrapy.org/401\",\n            \"http://example.com/402\",\n            ((301, \"http://scrapytest.org/403\"),),\n            b\"http://scrapy.org/\",\n            b\"http://scrapy.org/\",\n        ),\n        (\n            \"https://scrapy.org/501\",\n            \"https://example.com/502\",\n            (\n                # all different domains: send origin\n                (301, \"https://google.com/503\"),\n                (301, \"https://facebook.com/504\"),\n            ),\n            b\"https://scrapy.org/\",\n            b\"https://scrapy.org/\",\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"http://scrapytest.org/302\",  # TLS to non-TLS: send origin\n            ((301, \"https://scrapytest.org/303\"),),  # TLS URL again: send origin (also)\n            b\"https://scrapytest.org/\",\n            b\"https://scrapytest.org/\",\n        ),\n    ]\n\n\nclass TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n    \"\"\"\n    Strict Origin When Cross-Origin policy sends the full URL as \"Referer\",\n    unless the target's origin is different (different domain, different protocol)\n    in which case only the origin is sent...\n    Unless there's also a downgrade in security and then the \"Referer\" header\n    is not sent.\n\n    Redirections to a different origin should strip the \"Referer\" to the parent origin,\n    and from https:// to http:// will remove the \"Referer\" header.\n    \"\"\"\n\n    settings = {\"REFERRER_POLICY\": POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n    scenarii = [\n        (\n            \"http://scrapytest.org/101\",  # origin\n            \"http://scrapytest.org/102\",  # target + redirection\n            (\n                # redirections: code, URL\n                (301, \"http://scrapytest.org/103\"),\n                (301, \"http://scrapytest.org/104\"),\n            ),\n            b\"http://scrapytest.org/101\",  # expected initial referer\n            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n        ),\n        (\n            \"https://scrapytest.org/201\",\n            \"https://scrapytest.org/202\",\n            (\n                # redirecting to non-secure URL: do not send the \"Referer\" header\n                (301, \"http://scrapytest.org/203\"),\n            ),\n            b\"https://scrapytest.org/201\",\n            None,\n        ),\n        (\n            \"https://scrapytest.org/301\",\n            \"https://scrapytest.org/302\",\n            (\n                # redirecting to non-secure URL (different domain): send origin\n                (301, \"http://example.com/303\"),\n            ),\n            b\"https://scrapytest.org/301\",\n            None,\n        ),\n        (\n            \"http://scrapy.org/401\",\n            \"http://example.com/402\",\n            ((301, \"http://scrapytest.org/403\"),),\n            b\"http://scrapy.org/\",\n            b\"http://scrapy.org/\",\n        ),\n        (\n            \"https://scrapy.org/501\",\n            \"https://example.com/502\",\n            (\n                # all different domains: send origin\n                (301, \"https://google.com/503\"),\n                (301, \"https://facebook.com/504\"),\n            ),\n            b\"https://scrapy.org/\",\n            b\"https://scrapy.org/\",\n        ),\n        (\n            \"https://scrapytest.org/601\",\n            \"http://scrapytest.org/602\",  # TLS to non-TLS: do not send \"Referer\"\n            (\n                (\n                    301,\n                    \"https://scrapytest.org/603\",\n                ),  # TLS URL again: (still) send nothing\n            ),\n            None,\n            None,\n        ),\n    ]\n", "tests/test_utils_log.py": "import json\nimport logging\nimport re\nimport sys\nimport unittest\nfrom io import StringIO\nfrom typing import Any, Dict, Mapping, MutableMapping\nfrom unittest import TestCase\n\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.python.failure import Failure\n\nfrom scrapy.extensions import telnet\nfrom scrapy.utils.log import (\n    LogCounterHandler,\n    SpiderLoggerAdapter,\n    StreamLogger,\n    TopLevelFormatter,\n    failure_to_exc_info,\n)\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import LogSpider\n\n\nclass FailureToExcInfoTest(unittest.TestCase):\n    def test_failure(self):\n        try:\n            0 / 0\n        except ZeroDivisionError:\n            exc_info = sys.exc_info()\n            failure = Failure()\n\n        self.assertTupleEqual(exc_info, failure_to_exc_info(failure))\n\n    def test_non_failure(self):\n        self.assertIsNone(failure_to_exc_info(\"test\"))\n\n\nclass TopLevelFormatterTest(unittest.TestCase):\n    def setUp(self):\n        self.handler = LogCapture()\n        self.handler.addFilter(TopLevelFormatter([\"test\"]))\n\n    def test_top_level_logger(self):\n        logger = logging.getLogger(\"test\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"test\", \"WARNING\", \"test log msg\"))\n\n    def test_children_logger(self):\n        logger = logging.getLogger(\"test.test1\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"test\", \"WARNING\", \"test log msg\"))\n\n    def test_overlapping_name_logger(self):\n        logger = logging.getLogger(\"test2\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"test2\", \"WARNING\", \"test log msg\"))\n\n    def test_different_name_logger(self):\n        logger = logging.getLogger(\"different\")\n        with self.handler as log:\n            logger.warning(\"test log msg\")\n        log.check((\"different\", \"WARNING\", \"test log msg\"))\n\n\nclass LogCounterHandlerTest(unittest.TestCase):\n    def setUp(self):\n        settings = {\"LOG_LEVEL\": \"WARNING\"}\n        if not telnet.TWISTED_CONCH_AVAILABLE:\n            # disable it to avoid the extra warning\n            settings[\"TELNETCONSOLE_ENABLED\"] = False\n        self.logger = logging.getLogger(\"test\")\n        self.logger.setLevel(logging.NOTSET)\n        self.logger.propagate = False\n        self.crawler = get_crawler(settings_dict=settings)\n        self.handler = LogCounterHandler(self.crawler)\n        self.logger.addHandler(self.handler)\n\n    def tearDown(self):\n        self.logger.propagate = True\n        self.logger.removeHandler(self.handler)\n\n    def test_init(self):\n        self.assertIsNone(self.crawler.stats.get_value(\"log_count/DEBUG\"))\n        self.assertIsNone(self.crawler.stats.get_value(\"log_count/INFO\"))\n        self.assertIsNone(self.crawler.stats.get_value(\"log_count/WARNING\"))\n        self.assertIsNone(self.crawler.stats.get_value(\"log_count/ERROR\"))\n        self.assertIsNone(self.crawler.stats.get_value(\"log_count/CRITICAL\"))\n\n    def test_accepted_level(self):\n        self.logger.error(\"test log msg\")\n        self.assertEqual(self.crawler.stats.get_value(\"log_count/ERROR\"), 1)\n\n    def test_filtered_out_level(self):\n        self.logger.debug(\"test log msg\")\n        self.assertIsNone(self.crawler.stats.get_value(\"log_count/INFO\"))\n\n\nclass StreamLoggerTest(unittest.TestCase):\n    def setUp(self):\n        self.stdout = sys.stdout\n        logger = logging.getLogger(\"test\")\n        logger.setLevel(logging.WARNING)\n        sys.stdout = StreamLogger(logger, logging.ERROR)\n\n    def tearDown(self):\n        sys.stdout = self.stdout\n\n    def test_redirect(self):\n        with LogCapture() as log:\n            print(\"test log msg\")\n        log.check((\"test\", \"ERROR\", \"test log msg\"))\n\n\n@pytest.mark.parametrize(\n    (\"base_extra\", \"log_extra\", \"expected_extra\"),\n    (\n        (\n            {\"spider\": \"test\"},\n            {\"extra\": {\"log_extra\": \"info\"}},\n            {\"extra\": {\"log_extra\": \"info\", \"spider\": \"test\"}},\n        ),\n        (\n            {\"spider\": \"test\"},\n            {\"extra\": None},\n            {\"extra\": {\"spider\": \"test\"}},\n        ),\n        (\n            {\"spider\": \"test\"},\n            {\"extra\": {\"spider\": \"test2\"}},\n            {\"extra\": {\"spider\": \"test\"}},\n        ),\n    ),\n)\ndef test_spider_logger_adapter_process(\n    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: Dict\n):\n    logger = logging.getLogger(\"test\")\n    spider_logger_adapter = SpiderLoggerAdapter(logger, base_extra)\n\n    log_message = \"test_log_message\"\n    result_message, result_kwargs = spider_logger_adapter.process(\n        log_message, log_extra\n    )\n\n    assert result_message == log_message\n    assert result_kwargs == expected_extra\n\n\nclass LoggingTestCase(TestCase):\n    def setUp(self):\n        self.log_stream = StringIO()\n        handler = logging.StreamHandler(self.log_stream)\n        logger = logging.getLogger(\"log_spider\")\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n        self.handler = handler\n        self.logger = logger\n        self.spider = LogSpider()\n\n    def tearDown(self):\n        self.logger.removeHandler(self.handler)\n\n    def test_debug_logging(self):\n        log_message = \"Foo message\"\n        self.spider.log_debug(log_message)\n        log_contents = self.log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_info_logging(self):\n        log_message = \"Bar message\"\n        self.spider.log_info(log_message)\n        log_contents = self.log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_warning_logging(self):\n        log_message = \"Baz message\"\n        self.spider.log_warning(log_message)\n        log_contents = self.log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_error_logging(self):\n        log_message = \"Foo bar message\"\n        self.spider.log_error(log_message)\n        log_contents = self.log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n    def test_critical_logging(self):\n        log_message = \"Foo bar baz message\"\n        self.spider.log_critical(log_message)\n        log_contents = self.log_stream.getvalue()\n\n        assert log_contents == f\"{log_message}\\n\"\n\n\nclass LoggingWithExtraTestCase(TestCase):\n    def setUp(self):\n        self.log_stream = StringIO()\n        handler = logging.StreamHandler(self.log_stream)\n        formatter = logging.Formatter(\n            '{\"levelname\": \"%(levelname)s\", \"message\": \"%(message)s\", \"spider\": \"%(spider)s\", \"important_info\": \"%(important_info)s\"}'\n        )\n        handler.setFormatter(formatter)\n        logger = logging.getLogger(\"log_spider\")\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n        self.handler = handler\n        self.logger = logger\n        self.spider = LogSpider()\n        self.regex_pattern = re.compile(r\"^<LogSpider\\s'log_spider'\\sat\\s[^>]+>$\")\n\n    def tearDown(self):\n        self.logger.removeHandler(self.handler)\n\n    def test_debug_logging(self):\n        log_message = \"Foo message\"\n        extra = {\"important_info\": \"foo\"}\n        self.spider.log_debug(log_message, extra)\n        log_contents = self.log_stream.getvalue()\n        log_contents = json.loads(log_contents)\n\n        assert log_contents[\"levelname\"] == \"DEBUG\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_info_logging(self):\n        log_message = \"Bar message\"\n        extra = {\"important_info\": \"bar\"}\n        self.spider.log_info(log_message, extra)\n        log_contents = self.log_stream.getvalue()\n        log_contents = json.loads(log_contents)\n\n        assert log_contents[\"levelname\"] == \"INFO\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_warning_logging(self):\n        log_message = \"Baz message\"\n        extra = {\"important_info\": \"baz\"}\n        self.spider.log_warning(log_message, extra)\n        log_contents = self.log_stream.getvalue()\n        log_contents = json.loads(log_contents)\n\n        assert log_contents[\"levelname\"] == \"WARNING\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_error_logging(self):\n        log_message = \"Foo bar message\"\n        extra = {\"important_info\": \"foo bar\"}\n        self.spider.log_error(log_message, extra)\n        log_contents = self.log_stream.getvalue()\n        log_contents = json.loads(log_contents)\n\n        assert log_contents[\"levelname\"] == \"ERROR\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_critical_logging(self):\n        log_message = \"Foo bar baz message\"\n        extra = {\"important_info\": \"foo bar baz\"}\n        self.spider.log_critical(log_message, extra)\n        log_contents = self.log_stream.getvalue()\n        log_contents = json.loads(log_contents)\n\n        assert log_contents[\"levelname\"] == \"CRITICAL\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n    def test_overwrite_spider_extra(self):\n        log_message = \"Foo message\"\n        extra = {\"important_info\": \"foo\", \"spider\": \"shouldn't change\"}\n        self.spider.log_error(log_message, extra)\n        log_contents = self.log_stream.getvalue()\n        log_contents = json.loads(log_contents)\n\n        assert log_contents[\"levelname\"] == \"ERROR\"\n        assert log_contents[\"message\"] == log_message\n        assert self.regex_pattern.match(log_contents[\"spider\"])\n        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n", "tests/test_downloader_handlers_http2.py": "import json\nfrom unittest import mock, skipIf\n\nfrom pytest import mark\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer, error, reactor\nfrom twisted.trial import unittest\nfrom twisted.web import server\nfrom twisted.web.error import SchemeNotSupported\nfrom twisted.web.http import H2_ENABLED\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import ssl_context_factory\nfrom tests.test_downloader_handlers import (\n    Http11MockServerTestCase,\n    Http11ProxyTestCase,\n    Https11CustomCiphers,\n    Https11TestCase,\n    UriResource,\n)\n\n\n@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\nclass Https2TestCase(Https11TestCase):\n    scheme = \"https\"\n    HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n\n    @classmethod\n    def setUpClass(cls):\n        from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n\n        cls.download_handler_cls = H2DownloadHandler\n\n    def test_protocol(self):\n        request = Request(self.getURL(\"host\"), method=\"GET\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.protocol)\n        d.addCallback(self.assertEqual, \"h2\")\n        return d\n\n    @defer.inlineCallbacks\n    def test_download_with_maxsize_very_large_file(self):\n        with mock.patch(\"scrapy.core.http2.stream.logger\") as logger:\n            request = Request(self.getURL(\"largechunkedfile\"))\n\n            def check(logger):\n                logger.error.assert_called_once_with(mock.ANY)\n\n            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n\n            # As the error message is logged in the dataReceived callback, we\n            # have to give a bit of time to the reactor to process the queue\n            # after closing the connection.\n            d = defer.Deferred()\n            d.addCallback(check)\n            reactor.callLater(0.1, d.callback, logger)\n            yield d\n\n    @defer.inlineCallbacks\n    def test_unsupported_scheme(self):\n        request = Request(\"ftp://unsupported.scheme\")\n        d = self.download_request(request, Spider(\"foo\"))\n        yield self.assertFailure(d, SchemeNotSupported)\n\n    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_broken_chunked_content_cause_data_loss(self):\n        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_broken_chunked_content_allow_data_loss(self):\n        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n\n    def test_concurrent_requests_same_domain(self):\n        spider = Spider(\"foo\")\n\n        request1 = Request(self.getURL(\"file\"))\n        d1 = self.download_request(request1, spider)\n        d1.addCallback(lambda r: r.body)\n        d1.addCallback(self.assertEqual, b\"0123456789\")\n\n        request2 = Request(self.getURL(\"echo\"), method=\"POST\")\n        d2 = self.download_request(request2, spider)\n        d2.addCallback(lambda r: r.headers[\"Content-Length\"])\n        d2.addCallback(self.assertEqual, b\"79\")\n\n        return defer.DeferredList([d1, d2])\n\n    @mark.xfail(reason=\"https://github.com/python-hyper/h2/issues/1247\")\n    def test_connect_request(self):\n        request = Request(self.getURL(\"file\"), method=\"CONNECT\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"\")\n        return d\n\n    def test_custom_content_length_good(self):\n        request = Request(self.getURL(\"contentlength\"))\n        custom_content_length = str(len(request.body))\n        request.headers[\"Content-Length\"] = custom_content_length\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.text)\n        d.addCallback(self.assertEqual, custom_content_length)\n        return d\n\n    def test_custom_content_length_bad(self):\n        request = Request(self.getURL(\"contentlength\"))\n        actual_content_length = str(len(request.body))\n        bad_content_length = str(len(request.body) + 1)\n        request.headers[\"Content-Length\"] = bad_content_length\n        log = LogCapture()\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.text)\n        d.addCallback(self.assertEqual, actual_content_length)\n        d.addCallback(\n            lambda _: log.check_present(\n                (\n                    \"scrapy.core.http2.stream\",\n                    \"WARNING\",\n                    f\"Ignoring bad Content-Length header \"\n                    f\"{bad_content_length!r} of request {request}, sending \"\n                    f\"{actual_content_length!r} instead\",\n                )\n            )\n        )\n        d.addCallback(lambda _: log.uninstall())\n        return d\n\n    def test_duplicate_header(self):\n        request = Request(self.getURL(\"echo\"))\n        header, value1, value2 = \"Custom-Header\", \"foo\", \"bar\"\n        request.headers.appendlist(header, value1)\n        request.headers.appendlist(header, value2)\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: json.loads(r.text)[\"headers\"][header])\n        d.addCallback(self.assertEqual, [value1, value2])\n        return d\n\n\nclass Https2WrongHostnameTestCase(Https2TestCase):\n    tls_log_message = (\n        'SSL connection certificate: issuer \"/C=XW/ST=XW/L=The '\n        'Internet/O=Scrapy/CN=www.example.com/emailAddress=test@example.com\", '\n        'subject \"/C=XW/ST=XW/L=The '\n        'Internet/O=Scrapy/CN=www.example.com/emailAddress=test@example.com\"'\n    )\n\n    # above tests use a server certificate for \"localhost\",\n    # client connection to \"localhost\" too.\n    # here we test that even if the server certificate is for another domain,\n    # \"www.example.com\" in this case,\n    # the tests still pass\n    keyfile = \"keys/example-com.key.pem\"\n    certfile = \"keys/example-com.cert.pem\"\n\n\nclass Https2InvalidDNSId(Https2TestCase):\n    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n\n    def setUp(self):\n        super().setUp()\n        self.host = \"127.0.0.1\"\n\n\nclass Https2InvalidDNSPattern(Https2TestCase):\n    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n\n    keyfile = \"keys/localhost.ip.key\"\n    certfile = \"keys/localhost.ip.crt\"\n\n    def setUp(self):\n        try:\n            from service_identity.exceptions import CertificateError  # noqa: F401\n        except ImportError:\n            raise unittest.SkipTest(\"cryptography lib is too old\")\n        self.tls_log_message = (\n            'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", '\n            'subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n        )\n        super().setUp()\n\n\n@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\nclass Https2CustomCiphers(Https11CustomCiphers):\n    scheme = \"https\"\n\n    @classmethod\n    def setUpClass(cls):\n        from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n\n        cls.download_handler_cls = H2DownloadHandler\n\n\nclass Http2MockServerTestCase(Http11MockServerTestCase):\n    \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n\n    settings_dict = {\n        \"DOWNLOAD_HANDLERS\": {\n            \"https\": \"scrapy.core.downloader.handlers.http2.H2DownloadHandler\"\n        }\n    }\n\n\n@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\nclass Https2ProxyTestCase(Http11ProxyTestCase):\n    # only used for HTTPS tests\n    keyfile = \"keys/localhost.key\"\n    certfile = \"keys/localhost.crt\"\n\n    scheme = \"https\"\n    host = \"127.0.0.1\"\n\n    expected_http_proxy_request_body = b\"/\"\n\n    @classmethod\n    def setUpClass(cls):\n        from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n\n        cls.download_handler_cls = H2DownloadHandler\n\n    def setUp(self):\n        site = server.Site(UriResource(), timeout=None)\n        self.port = reactor.listenSSL(\n            0,\n            site,\n            ssl_context_factory(self.keyfile, self.certfile),\n            interface=self.host,\n        )\n        self.portno = self.port.getHost().port\n        self.download_handler = build_from_crawler(\n            self.download_handler_cls, get_crawler()\n        )\n        self.download_request = self.download_handler.download_request\n\n    def getURL(self, path):\n        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n\n    @defer.inlineCallbacks\n    def test_download_with_proxy_https_timeout(self):\n        with self.assertRaises(NotImplementedError):\n            yield super().test_download_with_proxy_https_timeout()\n", "tests/test_downloadermiddleware_cookies.py": "import logging\nfrom unittest import TestCase\n\nimport pytest\nfrom testfixtures import LogCapture\n\nfrom scrapy.downloadermiddlewares.cookies import CookiesMiddleware\nfrom scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\nfrom scrapy.downloadermiddlewares.redirect import RedirectMiddleware\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler\n\nUNSET = object()\n\n\ndef _cookie_to_set_cookie_value(cookie):\n    \"\"\"Given a cookie defined as a dictionary with name and value keys, and\n    optional path and domain keys, return the equivalent string that can be\n    associated to a ``Set-Cookie`` header.\"\"\"\n    decoded = {}\n    for key in (\"name\", \"value\", \"path\", \"domain\"):\n        if cookie.get(key) is None:\n            if key in (\"name\", \"value\"):\n                return\n            continue\n        if isinstance(cookie[key], (bool, float, int, str)):\n            decoded[key] = str(cookie[key])\n        else:\n            try:\n                decoded[key] = cookie[key].decode(\"utf8\")\n            except UnicodeDecodeError:\n                decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n\n    cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n    for key, value in decoded.items():  # path, domain\n        cookie_str += f\"; {key.capitalize()}={value}\"\n    return cookie_str\n\n\ndef _cookies_to_set_cookie_list(cookies):\n    \"\"\"Given a group of cookie defined either as a dictionary or as a list of\n    dictionaries (i.e. in a format supported by the cookies parameter of\n    Request), return the equivalen list of strings that can be associated to a\n    ``Set-Cookie`` header.\"\"\"\n    if not cookies:\n        return []\n    if isinstance(cookies, dict):\n        cookies = ({\"name\": k, \"value\": v} for k, v in cookies.items())\n    return filter(None, (_cookie_to_set_cookie_value(cookie) for cookie in cookies))\n\n\nclass CookiesMiddlewareTest(TestCase):\n    def assertCookieValEqual(self, first, second, msg=None):\n        def split_cookies(cookies):\n            return sorted([s.strip() for s in to_bytes(cookies).split(b\";\")])\n\n        return self.assertEqual(split_cookies(first), split_cookies(second), msg=msg)\n\n    def setUp(self):\n        self.spider = Spider(\"foo\")\n        self.mw = CookiesMiddleware()\n        self.redirect_middleware = RedirectMiddleware(settings=Settings())\n\n    def tearDown(self):\n        del self.mw\n        del self.redirect_middleware\n\n    def test_basic(self):\n        req = Request(\"http://scrapytest.org/\")\n        assert self.mw.process_request(req, self.spider) is None\n        assert \"Cookie\" not in req.headers\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res, self.spider) is res\n\n        req2 = Request(\"http://scrapytest.org/sub1/\")\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertEqual(req2.headers.get(\"Cookie\"), b\"C1=value1\")\n\n    def test_setting_false_cookies_enabled(self):\n        self.assertRaises(\n            NotConfigured,\n            CookiesMiddleware.from_crawler,\n            get_crawler(settings_dict={\"COOKIES_ENABLED\": False}),\n        )\n\n    def test_setting_default_cookies_enabled(self):\n        self.assertIsInstance(\n            CookiesMiddleware.from_crawler(get_crawler()), CookiesMiddleware\n        )\n\n    def test_setting_true_cookies_enabled(self):\n        self.assertIsInstance(\n            CookiesMiddleware.from_crawler(\n                get_crawler(settings_dict={\"COOKIES_ENABLED\": True})\n            ),\n            CookiesMiddleware,\n        )\n\n    def test_setting_enabled_cookies_debug(self):\n        crawler = get_crawler(settings_dict={\"COOKIES_DEBUG\": True})\n        mw = CookiesMiddleware.from_crawler(crawler)\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.cookies\",\n            propagate=False,\n            level=logging.DEBUG,\n        ) as log:\n            req = Request(\"http://scrapytest.org/\")\n            res = Response(\n                \"http://scrapytest.org/\", headers={\"Set-Cookie\": \"C1=value1; path=/\"}\n            )\n            mw.process_response(req, res, crawler.spider)\n            req2 = Request(\"http://scrapytest.org/sub1/\")\n            mw.process_request(req2, crawler.spider)\n\n            log.check(\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"DEBUG\",\n                    \"Received cookies from: <200 http://scrapytest.org/>\\n\"\n                    \"Set-Cookie: C1=value1; path=/\\n\",\n                ),\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"DEBUG\",\n                    \"Sending cookies to: <GET http://scrapytest.org/sub1/>\\n\"\n                    \"Cookie: C1=value1\\n\",\n                ),\n            )\n\n    def test_setting_disabled_cookies_debug(self):\n        crawler = get_crawler(settings_dict={\"COOKIES_DEBUG\": False})\n        mw = CookiesMiddleware.from_crawler(crawler)\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.cookies\",\n            propagate=False,\n            level=logging.DEBUG,\n        ) as log:\n            req = Request(\"http://scrapytest.org/\")\n            res = Response(\n                \"http://scrapytest.org/\", headers={\"Set-Cookie\": \"C1=value1; path=/\"}\n            )\n            mw.process_response(req, res, crawler.spider)\n            req2 = Request(\"http://scrapytest.org/sub1/\")\n            mw.process_request(req2, crawler.spider)\n\n            log.check()\n\n    def test_do_not_break_on_non_utf8_header(self):\n        req = Request(\"http://scrapytest.org/\")\n        assert self.mw.process_request(req, self.spider) is None\n        assert \"Cookie\" not in req.headers\n\n        headers = {\"Set-Cookie\": b\"C1=in\\xa3valid; path=/\", \"Other\": b\"ignore\\xa3me\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res, self.spider) is res\n\n        req2 = Request(\"http://scrapytest.org/sub1/\")\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertIn(\"Cookie\", req2.headers)\n\n    def test_dont_merge_cookies(self):\n        # merge some cookies into jar\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        req = Request(\"http://scrapytest.org/\")\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res, self.spider) is res\n\n        # test Cookie header is not seted to request\n        req = Request(\"http://scrapytest.org/dontmerge\", meta={\"dont_merge_cookies\": 1})\n        assert self.mw.process_request(req, self.spider) is None\n        assert \"Cookie\" not in req.headers\n\n        # check that returned cookies are not merged back to jar\n        res = Response(\n            \"http://scrapytest.org/dontmerge\",\n            headers={\"Set-Cookie\": \"dont=mergeme; path=/\"},\n        )\n        assert self.mw.process_response(req, res, self.spider) is res\n\n        # check that cookies are merged back\n        req = Request(\"http://scrapytest.org/mergeme\")\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers.get(\"Cookie\"), b\"C1=value1\")\n\n        # check that cookies are merged when dont_merge_cookies is passed as 0\n        req = Request(\"http://scrapytest.org/mergeme\", meta={\"dont_merge_cookies\": 0})\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers.get(\"Cookie\"), b\"C1=value1\")\n\n    def test_complex_cookies(self):\n        # merge some cookies into jar\n        cookies = [\n            {\n                \"name\": \"C1\",\n                \"value\": \"value1\",\n                \"path\": \"/foo\",\n                \"domain\": \"scrapytest.org\",\n            },\n            {\n                \"name\": \"C2\",\n                \"value\": \"value2\",\n                \"path\": \"/bar\",\n                \"domain\": \"scrapytest.org\",\n            },\n            {\n                \"name\": \"C3\",\n                \"value\": \"value3\",\n                \"path\": \"/foo\",\n                \"domain\": \"scrapytest.org\",\n            },\n            {\"name\": \"C4\", \"value\": \"value4\", \"path\": \"/foo\", \"domain\": \"scrapy.org\"},\n        ]\n\n        req = Request(\"http://scrapytest.org/\", cookies=cookies)\n        self.mw.process_request(req, self.spider)\n\n        # embed C1 and C3 for scrapytest.org/foo\n        req = Request(\"http://scrapytest.org/foo\")\n        self.mw.process_request(req, self.spider)\n        assert req.headers.get(\"Cookie\") in (\n            b\"C1=value1; C3=value3\",\n            b\"C3=value3; C1=value1\",\n        )\n\n        # embed C2 for scrapytest.org/bar\n        req = Request(\"http://scrapytest.org/bar\")\n        self.mw.process_request(req, self.spider)\n        self.assertEqual(req.headers.get(\"Cookie\"), b\"C2=value2\")\n\n        # embed nothing for scrapytest.org/baz\n        req = Request(\"http://scrapytest.org/baz\")\n        self.mw.process_request(req, self.spider)\n        assert \"Cookie\" not in req.headers\n\n    def test_merge_request_cookies(self):\n        req = Request(\"http://scrapytest.org/\", cookies={\"galleta\": \"salada\"})\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers.get(\"Cookie\"), b\"galleta=salada\")\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers)\n        assert self.mw.process_response(req, res, self.spider) is res\n\n        req2 = Request(\"http://scrapytest.org/sub1/\")\n        assert self.mw.process_request(req2, self.spider) is None\n\n        self.assertCookieValEqual(\n            req2.headers.get(\"Cookie\"), b\"C1=value1; galleta=salada\"\n        )\n\n    def test_cookiejar_key(self):\n        req = Request(\n            \"http://scrapytest.org/\",\n            cookies={\"galleta\": \"salada\"},\n            meta={\"cookiejar\": \"store1\"},\n        )\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers.get(\"Cookie\"), b\"galleta=salada\")\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res = Response(\"http://scrapytest.org/\", headers=headers, request=req)\n        assert self.mw.process_response(req, res, self.spider) is res\n\n        req2 = Request(\"http://scrapytest.org/\", meta=res.meta)\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertCookieValEqual(\n            req2.headers.get(\"Cookie\"), b\"C1=value1; galleta=salada\"\n        )\n\n        req3 = Request(\n            \"http://scrapytest.org/\",\n            cookies={\"galleta\": \"dulce\"},\n            meta={\"cookiejar\": \"store2\"},\n        )\n        assert self.mw.process_request(req3, self.spider) is None\n        self.assertEqual(req3.headers.get(\"Cookie\"), b\"galleta=dulce\")\n\n        headers = {\"Set-Cookie\": \"C2=value2; path=/\"}\n        res2 = Response(\"http://scrapytest.org/\", headers=headers, request=req3)\n        assert self.mw.process_response(req3, res2, self.spider) is res2\n\n        req4 = Request(\"http://scrapytest.org/\", meta=res2.meta)\n        assert self.mw.process_request(req4, self.spider) is None\n        self.assertCookieValEqual(\n            req4.headers.get(\"Cookie\"), b\"C2=value2; galleta=dulce\"\n        )\n\n        # cookies from hosts with port\n        req5_1 = Request(\"http://scrapytest.org:1104/\")\n        assert self.mw.process_request(req5_1, self.spider) is None\n\n        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n        res5_1 = Response(\n            \"http://scrapytest.org:1104/\", headers=headers, request=req5_1\n        )\n        assert self.mw.process_response(req5_1, res5_1, self.spider) is res5_1\n\n        req5_2 = Request(\"http://scrapytest.org:1104/some-redirected-path\")\n        assert self.mw.process_request(req5_2, self.spider) is None\n        self.assertEqual(req5_2.headers.get(\"Cookie\"), b\"C1=value1\")\n\n        req5_3 = Request(\"http://scrapytest.org/some-redirected-path\")\n        assert self.mw.process_request(req5_3, self.spider) is None\n        self.assertEqual(req5_3.headers.get(\"Cookie\"), b\"C1=value1\")\n\n        # skip cookie retrieval for not http request\n        req6 = Request(\"file:///scrapy/sometempfile\")\n        assert self.mw.process_request(req6, self.spider) is None\n        self.assertEqual(req6.headers.get(\"Cookie\"), None)\n\n    def test_local_domain(self):\n        request = Request(\"http://example-host/\", cookies={\"currencyCookie\": \"USD\"})\n        assert self.mw.process_request(request, self.spider) is None\n        self.assertIn(\"Cookie\", request.headers)\n        self.assertEqual(b\"currencyCookie=USD\", request.headers[\"Cookie\"])\n\n    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n    def test_keep_cookie_from_default_request_headers_middleware(self):\n        DEFAULT_REQUEST_HEADERS = {\"Cookie\": \"default=value; asdf=qwerty\"}\n        mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n        # overwrite with values from 'cookies' request argument\n        req1 = Request(\"http://example.org\", cookies={\"default\": \"something\"})\n        assert mw_default_headers.process_request(req1, self.spider) is None\n        assert self.mw.process_request(req1, self.spider) is None\n        self.assertCookieValEqual(\n            req1.headers[\"Cookie\"], b\"default=something; asdf=qwerty\"\n        )\n        # keep both\n        req2 = Request(\"http://example.com\", cookies={\"a\": \"b\"})\n        assert mw_default_headers.process_request(req2, self.spider) is None\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertCookieValEqual(\n            req2.headers[\"Cookie\"], b\"default=value; a=b; asdf=qwerty\"\n        )\n\n    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n    def test_keep_cookie_header(self):\n        # keep only cookies from 'Cookie' request header\n        req1 = Request(\"http://scrapytest.org\", headers={\"Cookie\": \"a=b; c=d\"})\n        assert self.mw.process_request(req1, self.spider) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], \"a=b; c=d\")\n        # keep cookies from both 'Cookie' request header and 'cookies' keyword\n        req2 = Request(\n            \"http://scrapytest.org\", headers={\"Cookie\": \"a=b; c=d\"}, cookies={\"e\": \"f\"}\n        )\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], \"a=b; c=d; e=f\")\n        # overwrite values from 'Cookie' request header with 'cookies' keyword\n        req3 = Request(\n            \"http://scrapytest.org\",\n            headers={\"Cookie\": \"a=b; c=d\"},\n            cookies={\"a\": \"new\", \"e\": \"f\"},\n        )\n        assert self.mw.process_request(req3, self.spider) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], \"a=new; c=d; e=f\")\n\n    def test_request_cookies_encoding(self):\n        # 1) UTF8-encoded bytes\n        req1 = Request(\"http://example.org\", cookies={\"a\": \"\u00e1\".encode()})\n        assert self.mw.process_request(req1, self.spider) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 2) Non UTF8-encoded bytes\n        req2 = Request(\"http://example.org\", cookies={\"a\": \"\u00e1\".encode(\"latin1\")})\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 3) String\n        req3 = Request(\"http://example.org\", cookies={\"a\": \"\u00e1\"})\n        assert self.mw.process_request(req3, self.spider) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n    def test_request_headers_cookie_encoding(self):\n        # 1) UTF8-encoded bytes\n        req1 = Request(\"http://example.org\", headers={\"Cookie\": \"a=\u00e1\".encode()})\n        assert self.mw.process_request(req1, self.spider) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 2) Non UTF8-encoded bytes\n        req2 = Request(\"http://example.org\", headers={\"Cookie\": \"a=\u00e1\".encode(\"latin1\")})\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n        # 3) String\n        req3 = Request(\"http://example.org\", headers={\"Cookie\": \"a=\u00e1\"})\n        assert self.mw.process_request(req3, self.spider) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n\n    def test_invalid_cookies(self):\n        \"\"\"\n        Invalid cookies are logged as warnings and discarded\n        \"\"\"\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.cookies\",\n            propagate=False,\n            level=logging.INFO,\n        ) as lc:\n            cookies1 = [{\"value\": \"bar\"}, {\"name\": \"key\", \"value\": \"value1\"}]\n            req1 = Request(\"http://example.org/1\", cookies=cookies1)\n            assert self.mw.process_request(req1, self.spider) is None\n            cookies2 = [{\"name\": \"foo\"}, {\"name\": \"key\", \"value\": \"value2\"}]\n            req2 = Request(\"http://example.org/2\", cookies=cookies2)\n            assert self.mw.process_request(req2, self.spider) is None\n            cookies3 = [{\"name\": \"foo\", \"value\": None}, {\"name\": \"key\", \"value\": \"\"}]\n            req3 = Request(\"http://example.org/3\", cookies=cookies3)\n            assert self.mw.process_request(req3, self.spider) is None\n            lc.check(\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"WARNING\",\n                    \"Invalid cookie found in request <GET http://example.org/1>:\"\n                    \" {'value': 'bar', 'secure': False} ('name' is missing)\",\n                ),\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"WARNING\",\n                    \"Invalid cookie found in request <GET http://example.org/2>:\"\n                    \" {'name': 'foo', 'secure': False} ('value' is missing)\",\n                ),\n                (\n                    \"scrapy.downloadermiddlewares.cookies\",\n                    \"WARNING\",\n                    \"Invalid cookie found in request <GET http://example.org/3>:\"\n                    \" {'name': 'foo', 'value': None, 'secure': False} ('value' is missing)\",\n                ),\n            )\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], \"key=value1\")\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], \"key=value2\")\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], \"key=\")\n\n    def test_primitive_type_cookies(self):\n        # Boolean\n        req1 = Request(\"http://example.org\", cookies={\"a\": True})\n        assert self.mw.process_request(req1, self.spider) is None\n        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=True\")\n\n        # Float\n        req2 = Request(\"http://example.org\", cookies={\"a\": 9.5})\n        assert self.mw.process_request(req2, self.spider) is None\n        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=9.5\")\n\n        # Integer\n        req3 = Request(\"http://example.org\", cookies={\"a\": 10})\n        assert self.mw.process_request(req3, self.spider) is None\n        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=10\")\n\n        # String\n        req4 = Request(\"http://example.org\", cookies={\"a\": \"b\"})\n        assert self.mw.process_request(req4, self.spider) is None\n        self.assertCookieValEqual(req4.headers[\"Cookie\"], b\"a=b\")\n\n    def _test_cookie_redirect(\n        self,\n        source,\n        target,\n        *,\n        cookies1,\n        cookies2,\n    ):\n        input_cookies = {\"a\": \"b\"}\n\n        if not isinstance(source, dict):\n            source = {\"url\": source}\n        if not isinstance(target, dict):\n            target = {\"url\": target}\n        target.setdefault(\"status\", 301)\n\n        request1 = Request(cookies=input_cookies, **source)\n        self.mw.process_request(request1, self.spider)\n        cookies = request1.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n\n        response = Response(\n            headers={\n                \"Location\": target[\"url\"],\n            },\n            **target,\n        )\n        self.assertEqual(\n            self.mw.process_response(request1, response, self.spider),\n            response,\n        )\n\n        request2 = self.redirect_middleware.process_response(\n            request1,\n            response,\n            self.spider,\n        )\n        self.assertIsInstance(request2, Request)\n\n        self.mw.process_request(request2, self.spider)\n        cookies = request2.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n\n    def test_cookie_redirect_same_domain(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            \"https://toscrape.com\",\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def test_cookie_redirect_same_domain_forcing_get(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://toscrape.com\", \"status\": 302},\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def test_cookie_redirect_different_domain(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            \"https://example.com\",\n            cookies1=True,\n            cookies2=False,\n        )\n\n    def test_cookie_redirect_different_domain_forcing_get(self):\n        self._test_cookie_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://example.com\", \"status\": 302},\n            cookies1=True,\n            cookies2=False,\n        )\n\n    def _test_cookie_header_redirect(\n        self,\n        source,\n        target,\n        *,\n        cookies2,\n    ):\n        \"\"\"Test the handling of a user-defined Cookie header when building a\n        redirect follow-up request.\n\n        We follow RFC 6265 for cookie handling. The Cookie header can only\n        contain a list of key-value pairs (i.e. no additional cookie\n        parameters like Domain or Path). Because of that, we follow the same\n        rules that we would follow for the handling of the Set-Cookie response\n        header when the Domain is not set: the cookies must be limited to the\n        target URL domain (not even subdomains can receive those cookies).\n\n        .. note:: This method tests the scenario where the cookie middleware is\n                  disabled. Because of known issue #1992, when the cookies\n                  middleware is enabled we do not need to be concerned about\n                  the Cookie header getting leaked to unintended domains,\n                  because the middleware empties the header from every request.\n        \"\"\"\n        if not isinstance(source, dict):\n            source = {\"url\": source}\n        if not isinstance(target, dict):\n            target = {\"url\": target}\n        target.setdefault(\"status\", 301)\n\n        request1 = Request(headers={\"Cookie\": b\"a=b\"}, **source)\n\n        response = Response(\n            headers={\n                \"Location\": target[\"url\"],\n            },\n            **target,\n        )\n\n        request2 = self.redirect_middleware.process_response(\n            request1,\n            response,\n            self.spider,\n        )\n        self.assertIsInstance(request2, Request)\n\n        cookies = request2.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n\n    def test_cookie_header_redirect_same_domain(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            \"https://toscrape.com\",\n            cookies2=True,\n        )\n\n    def test_cookie_header_redirect_same_domain_forcing_get(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://toscrape.com\", \"status\": 302},\n            cookies2=True,\n        )\n\n    def test_cookie_header_redirect_different_domain(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            \"https://example.com\",\n            cookies2=False,\n        )\n\n    def test_cookie_header_redirect_different_domain_forcing_get(self):\n        self._test_cookie_header_redirect(\n            \"https://toscrape.com\",\n            {\"url\": \"https://example.com\", \"status\": 302},\n            cookies2=False,\n        )\n\n    def _test_user_set_cookie_domain_followup(\n        self,\n        url1,\n        url2,\n        domain,\n        *,\n        cookies1,\n        cookies2,\n    ):\n        input_cookies = [\n            {\n                \"name\": \"a\",\n                \"value\": \"b\",\n                \"domain\": domain,\n            }\n        ]\n\n        request1 = Request(url1, cookies=input_cookies)\n        self.mw.process_request(request1, self.spider)\n        cookies = request1.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n\n        request2 = Request(url2)\n        self.mw.process_request(request2, self.spider)\n        cookies = request2.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n\n    def test_user_set_cookie_domain_suffix_private(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://books.toscrape.com\",\n            \"https://quotes.toscrape.com\",\n            \"toscrape.com\",\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def test_user_set_cookie_domain_suffix_public_period(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://foo.co.uk\",\n            \"https://bar.co.uk\",\n            \"co.uk\",\n            cookies1=False,\n            cookies2=False,\n        )\n\n    def test_user_set_cookie_domain_suffix_public_private(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://foo.blogspot.com\",\n            \"https://bar.blogspot.com\",\n            \"blogspot.com\",\n            cookies1=False,\n            cookies2=False,\n        )\n\n    def test_user_set_cookie_domain_public_period(self):\n        self._test_user_set_cookie_domain_followup(\n            \"https://co.uk\",\n            \"https://co.uk\",\n            \"co.uk\",\n            cookies1=True,\n            cookies2=True,\n        )\n\n    def _test_server_set_cookie_domain_followup(\n        self,\n        url1,\n        url2,\n        domain,\n        *,\n        cookies,\n    ):\n        request1 = Request(url1)\n        self.mw.process_request(request1, self.spider)\n\n        input_cookies = [\n            {\n                \"name\": \"a\",\n                \"value\": \"b\",\n                \"domain\": domain,\n            }\n        ]\n\n        headers = {\n            \"Set-Cookie\": _cookies_to_set_cookie_list(input_cookies),\n        }\n        response = Response(url1, status=200, headers=headers)\n        self.assertEqual(\n            self.mw.process_response(request1, response, self.spider),\n            response,\n        )\n\n        request2 = Request(url2)\n        self.mw.process_request(request2, self.spider)\n        actual_cookies = request2.headers.get(\"Cookie\")\n        self.assertEqual(actual_cookies, b\"a=b\" if cookies else None)\n\n    def test_server_set_cookie_domain_suffix_private(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://books.toscrape.com\",\n            \"https://quotes.toscrape.com\",\n            \"toscrape.com\",\n            cookies=True,\n        )\n\n    def test_server_set_cookie_domain_suffix_public_period(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://foo.co.uk\",\n            \"https://bar.co.uk\",\n            \"co.uk\",\n            cookies=False,\n        )\n\n    def test_server_set_cookie_domain_suffix_public_private(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://foo.blogspot.com\",\n            \"https://bar.blogspot.com\",\n            \"blogspot.com\",\n            cookies=False,\n        )\n\n    def test_server_set_cookie_domain_public_period(self):\n        self._test_server_set_cookie_domain_followup(\n            \"https://co.uk\",\n            \"https://co.uk\",\n            \"co.uk\",\n            cookies=True,\n        )\n\n    def _test_cookie_redirect_scheme_change(\n        self, secure, from_scheme, to_scheme, cookies1, cookies2, cookies3\n    ):\n        \"\"\"When a redirect causes the URL scheme to change from *from_scheme*\n        to *to_scheme*, while domain and port remain the same, and given a\n        cookie on the initial request with its secure attribute set to\n        *secure*, check if the cookie should be set on the Cookie header of the\n        initial request (*cookies1*), if it should be kept by the redirect\n        middleware (*cookies2*), and if it should be present on the Cookie\n        header in the redirected request (*cookie3*).\"\"\"\n        cookie_kwargs = {}\n        if secure is not UNSET:\n            cookie_kwargs[\"secure\"] = secure\n        input_cookies = [{\"name\": \"a\", \"value\": \"b\", **cookie_kwargs}]\n\n        request1 = Request(f\"{from_scheme}://a.example\", cookies=input_cookies)\n        self.mw.process_request(request1, self.spider)\n        cookies = request1.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n\n        response = Response(\n            f\"{from_scheme}://a.example\",\n            headers={\"Location\": f\"{to_scheme}://a.example\"},\n            status=301,\n        )\n        self.assertEqual(\n            self.mw.process_response(request1, response, self.spider),\n            response,\n        )\n\n        request2 = self.redirect_middleware.process_response(\n            request1,\n            response,\n            self.spider,\n        )\n        self.assertIsInstance(request2, Request)\n        cookies = request2.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n\n        self.mw.process_request(request2, self.spider)\n        cookies = request2.headers.get(\"Cookie\")\n        self.assertEqual(cookies, b\"a=b\" if cookies3 else None)\n\n    def test_cookie_redirect_secure_undefined_downgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=UNSET,\n            from_scheme=\"https\",\n            to_scheme=\"http\",\n            cookies1=True,\n            cookies2=False,\n            cookies3=False,\n        )\n\n    def test_cookie_redirect_secure_undefined_upgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=UNSET,\n            from_scheme=\"http\",\n            to_scheme=\"https\",\n            cookies1=True,\n            cookies2=True,\n            cookies3=True,\n        )\n\n    def test_cookie_redirect_secure_false_downgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=False,\n            from_scheme=\"https\",\n            to_scheme=\"http\",\n            cookies1=True,\n            cookies2=False,\n            cookies3=True,\n        )\n\n    def test_cookie_redirect_secure_false_upgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=False,\n            from_scheme=\"http\",\n            to_scheme=\"https\",\n            cookies1=True,\n            cookies2=True,\n            cookies3=True,\n        )\n\n    def test_cookie_redirect_secure_true_downgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=True,\n            from_scheme=\"https\",\n            to_scheme=\"http\",\n            cookies1=True,\n            cookies2=False,\n            cookies3=False,\n        )\n\n    def test_cookie_redirect_secure_true_upgrade(self):\n        self._test_cookie_redirect_scheme_change(\n            secure=True,\n            from_scheme=\"http\",\n            to_scheme=\"https\",\n            cookies1=False,\n            cookies2=False,\n            cookies3=True,\n        )\n", "tests/test_webclient.py": "\"\"\"\nfrom twisted.internet import defer\nTests borrowed from the twisted.web.client tests.\n\"\"\"\n\nimport shutil\nfrom pathlib import Path\nfrom tempfile import mkdtemp\n\nimport OpenSSL.SSL\nfrom twisted.internet import defer, reactor\nfrom twisted.trial import unittest\nfrom twisted.web import resource, server, static, util\n\ntry:\n    from twisted.internet.testing import StringTransport\nexcept ImportError:\n    # deprecated in Twisted 19.7.0\n    # (remove once we bump our requirement past that version)\n    from twisted.test.proto_helpers import StringTransport\n\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.protocols.policies import WrappingFactory\n\nfrom scrapy.core.downloader import webclient as client\nfrom scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\nfrom scrapy.http import Headers, Request\nfrom scrapy.settings import Settings\nfrom scrapy.utils.misc import build_from_settings\nfrom scrapy.utils.python import to_bytes, to_unicode\nfrom tests.mockserver import (\n    BrokenDownloadResource,\n    ErrorResource,\n    ForeverTakingResource,\n    HostHeaderResource,\n    NoLengthResource,\n    PayloadResource,\n    ssl_context_factory,\n)\n\n\ndef getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n    \"\"\"Adapted version of twisted.web.client.getPage\"\"\"\n\n    def _clientfactory(url, *args, **kwargs):\n        url = to_unicode(url)\n        timeout = kwargs.pop(\"timeout\", 0)\n        f = client.ScrapyHTTPClientFactory(\n            Request(url, *args, **kwargs), timeout=timeout\n        )\n        f.deferred.addCallback(response_transform or (lambda r: r.body))\n        return f\n\n    from twisted.web.client import _makeGetterFactory\n\n    return _makeGetterFactory(\n        to_bytes(url),\n        _clientfactory,\n        contextFactory=contextFactory,\n        *args,\n        **kwargs,\n    ).deferred\n\n\nclass ParseUrlTestCase(unittest.TestCase):\n    \"\"\"Test URL parsing facility and defaults values.\"\"\"\n\n    def _parse(self, url):\n        f = client.ScrapyHTTPClientFactory(Request(url))\n        return (f.scheme, f.netloc, f.host, f.port, f.path)\n\n    def testParse(self):\n        lip = \"127.0.0.1\"\n        tests = (\n            (\n                \"http://127.0.0.1?c=v&c2=v2#fragment\",\n                (\"http\", lip, lip, 80, \"/?c=v&c2=v2\"),\n            ),\n            (\n                \"http://127.0.0.1/?c=v&c2=v2#fragment\",\n                (\"http\", lip, lip, 80, \"/?c=v&c2=v2\"),\n            ),\n            (\n                \"http://127.0.0.1/foo?c=v&c2=v2#frag\",\n                (\"http\", lip, lip, 80, \"/foo?c=v&c2=v2\"),\n            ),\n            (\n                \"http://127.0.0.1:100?c=v&c2=v2#fragment\",\n                (\"http\", lip + \":100\", lip, 100, \"/?c=v&c2=v2\"),\n            ),\n            (\n                \"http://127.0.0.1:100/?c=v&c2=v2#frag\",\n                (\"http\", lip + \":100\", lip, 100, \"/?c=v&c2=v2\"),\n            ),\n            (\n                \"http://127.0.0.1:100/foo?c=v&c2=v2#frag\",\n                (\"http\", lip + \":100\", lip, 100, \"/foo?c=v&c2=v2\"),\n            ),\n            (\"http://127.0.0.1\", (\"http\", lip, lip, 80, \"/\")),\n            (\"http://127.0.0.1/\", (\"http\", lip, lip, 80, \"/\")),\n            (\"http://127.0.0.1/foo\", (\"http\", lip, lip, 80, \"/foo\")),\n            (\"http://127.0.0.1?param=value\", (\"http\", lip, lip, 80, \"/?param=value\")),\n            (\"http://127.0.0.1/?param=value\", (\"http\", lip, lip, 80, \"/?param=value\")),\n            (\n                \"http://127.0.0.1:12345/foo\",\n                (\"http\", lip + \":12345\", lip, 12345, \"/foo\"),\n            ),\n            (\"http://spam:12345/foo\", (\"http\", \"spam:12345\", \"spam\", 12345, \"/foo\")),\n            (\n                \"http://spam.test.org/foo\",\n                (\"http\", \"spam.test.org\", \"spam.test.org\", 80, \"/foo\"),\n            ),\n            (\"https://127.0.0.1/foo\", (\"https\", lip, lip, 443, \"/foo\")),\n            (\n                \"https://127.0.0.1/?param=value\",\n                (\"https\", lip, lip, 443, \"/?param=value\"),\n            ),\n            (\"https://127.0.0.1:12345/\", (\"https\", lip + \":12345\", lip, 12345, \"/\")),\n            (\n                \"http://scrapytest.org/foo \",\n                (\"http\", \"scrapytest.org\", \"scrapytest.org\", 80, \"/foo\"),\n            ),\n            (\"http://egg:7890 \", (\"http\", \"egg:7890\", \"egg\", 7890, \"/\")),\n        )\n\n        for url, test in tests:\n            test = tuple(to_bytes(x) if not isinstance(x, int) else x for x in test)\n            self.assertEqual(client._parse(url), test, url)\n\n\nclass ScrapyHTTPPageGetterTests(unittest.TestCase):\n    def test_earlyHeaders(self):\n        # basic test stolen from twisted HTTPageGetter\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                url=\"http://foo/bar\",\n                body=\"some data\",\n                headers={\n                    \"Host\": \"example.net\",\n                    \"User-Agent\": \"fooble\",\n                    \"Cookie\": \"blah blah\",\n                    \"Content-Length\": \"12981\",\n                    \"Useful\": \"value\",\n                },\n            )\n        )\n\n        self._test(\n            factory,\n            b\"GET /bar HTTP/1.0\\r\\n\"\n            b\"Content-Length: 9\\r\\n\"\n            b\"Useful: value\\r\\n\"\n            b\"Connection: close\\r\\n\"\n            b\"User-Agent: fooble\\r\\n\"\n            b\"Host: example.net\\r\\n\"\n            b\"Cookie: blah blah\\r\\n\"\n            b\"\\r\\n\"\n            b\"some data\",\n        )\n\n        # test minimal sent headers\n        factory = client.ScrapyHTTPClientFactory(Request(\"http://foo/bar\"))\n        self._test(factory, b\"GET /bar HTTP/1.0\\r\\n\" b\"Host: foo\\r\\n\" b\"\\r\\n\")\n\n        # test a simple POST with body and content-type\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                method=\"POST\",\n                url=\"http://foo/bar\",\n                body=\"name=value\",\n                headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n            )\n        )\n\n        self._test(\n            factory,\n            b\"POST /bar HTTP/1.0\\r\\n\"\n            b\"Host: foo\\r\\n\"\n            b\"Connection: close\\r\\n\"\n            b\"Content-Type: application/x-www-form-urlencoded\\r\\n\"\n            b\"Content-Length: 10\\r\\n\"\n            b\"\\r\\n\"\n            b\"name=value\",\n        )\n\n        # test a POST method with no body provided\n        factory = client.ScrapyHTTPClientFactory(\n            Request(method=\"POST\", url=\"http://foo/bar\")\n        )\n\n        self._test(\n            factory,\n            b\"POST /bar HTTP/1.0\\r\\n\" b\"Host: foo\\r\\n\" b\"Content-Length: 0\\r\\n\" b\"\\r\\n\",\n        )\n\n        # test with single and multivalued headers\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                url=\"http://foo/bar\",\n                headers={\n                    \"X-Meta-Single\": \"single\",\n                    \"X-Meta-Multivalued\": [\"value1\", \"value2\"],\n                },\n            )\n        )\n\n        self._test(\n            factory,\n            b\"GET /bar HTTP/1.0\\r\\n\"\n            b\"Host: foo\\r\\n\"\n            b\"X-Meta-Multivalued: value1\\r\\n\"\n            b\"X-Meta-Multivalued: value2\\r\\n\"\n            b\"X-Meta-Single: single\\r\\n\"\n            b\"\\r\\n\",\n        )\n\n        # same test with single and multivalued headers but using Headers class\n        factory = client.ScrapyHTTPClientFactory(\n            Request(\n                url=\"http://foo/bar\",\n                headers=Headers(\n                    {\n                        \"X-Meta-Single\": \"single\",\n                        \"X-Meta-Multivalued\": [\"value1\", \"value2\"],\n                    }\n                ),\n            )\n        )\n\n        self._test(\n            factory,\n            b\"GET /bar HTTP/1.0\\r\\n\"\n            b\"Host: foo\\r\\n\"\n            b\"X-Meta-Multivalued: value1\\r\\n\"\n            b\"X-Meta-Multivalued: value2\\r\\n\"\n            b\"X-Meta-Single: single\\r\\n\"\n            b\"\\r\\n\",\n        )\n\n    def _test(self, factory, testvalue):\n        transport = StringTransport()\n        protocol = client.ScrapyHTTPPageGetter()\n        protocol.factory = factory\n        protocol.makeConnection(transport)\n        self.assertEqual(\n            set(transport.value().splitlines()), set(testvalue.splitlines())\n        )\n        return testvalue\n\n    def test_non_standard_line_endings(self):\n        # regression test for: http://dev.scrapy.org/ticket/258\n        factory = client.ScrapyHTTPClientFactory(Request(url=\"http://foo/bar\"))\n        protocol = client.ScrapyHTTPPageGetter()\n        protocol.factory = factory\n        protocol.headers = Headers()\n        protocol.dataReceived(b\"HTTP/1.0 200 OK\\n\")\n        protocol.dataReceived(b\"Hello: World\\n\")\n        protocol.dataReceived(b\"Foo: Bar\\n\")\n        protocol.dataReceived(b\"\\n\")\n        self.assertEqual(\n            protocol.headers, Headers({\"Hello\": [\"World\"], \"Foo\": [\"Bar\"]})\n        )\n\n\nclass EncodingResource(resource.Resource):\n    out_encoding = \"cp1251\"\n\n    def render(self, request):\n        body = to_unicode(request.content.read())\n        request.setHeader(b\"content-encoding\", self.out_encoding)\n        return body.encode(self.out_encoding)\n\n\nclass WebClientTestCase(unittest.TestCase):\n    def _listen(self, site):\n        return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n\n    def setUp(self):\n        self.tmpname = Path(mkdtemp())\n        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n        r = static.File(str(self.tmpname))\n        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n        r.putChild(b\"wait\", ForeverTakingResource())\n        r.putChild(b\"error\", ErrorResource())\n        r.putChild(b\"nolength\", NoLengthResource())\n        r.putChild(b\"host\", HostHeaderResource())\n        r.putChild(b\"payload\", PayloadResource())\n        r.putChild(b\"broken\", BrokenDownloadResource())\n        r.putChild(b\"encoding\", EncodingResource())\n        self.site = server.Site(r, timeout=None)\n        self.wrapper = WrappingFactory(self.site)\n        self.port = self._listen(self.wrapper)\n        self.portno = self.port.getHost().port\n\n    @inlineCallbacks\n    def tearDown(self):\n        yield self.port.stopListening()\n        shutil.rmtree(self.tmpname)\n\n    def getURL(self, path):\n        return f\"http://127.0.0.1:{self.portno}/{path}\"\n\n    def testPayload(self):\n        s = \"0123456789\" * 10\n        return getPage(self.getURL(\"payload\"), body=s).addCallback(\n            self.assertEqual, to_bytes(s)\n        )\n\n    def testHostHeader(self):\n        # if we pass Host header explicitly, it should be used, otherwise\n        # it should extract from url\n        return defer.gatherResults(\n            [\n                getPage(self.getURL(\"host\")).addCallback(\n                    self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\")\n                ),\n                getPage(\n                    self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}\n                ).addCallback(self.assertEqual, to_bytes(\"www.example.com\")),\n            ]\n        )\n\n    def test_getPage(self):\n        \"\"\"\n        L{client.getPage} returns a L{Deferred} which is called back with\n        the body of the response if the default method B{GET} is used.\n        \"\"\"\n        d = getPage(self.getURL(\"file\"))\n        d.addCallback(self.assertEqual, b\"0123456789\")\n        return d\n\n    def test_getPageHead(self):\n        \"\"\"\n        L{client.getPage} returns a L{Deferred} which is called back with\n        the empty string if the method is C{HEAD} and there is a successful\n        response code.\n        \"\"\"\n\n        def _getPage(method):\n            return getPage(self.getURL(\"file\"), method=method)\n\n        return defer.gatherResults(\n            [\n                _getPage(\"head\").addCallback(self.assertEqual, b\"\"),\n                _getPage(\"HEAD\").addCallback(self.assertEqual, b\"\"),\n            ]\n        )\n\n    def test_timeoutNotTriggering(self):\n        \"\"\"\n        When a non-zero timeout is passed to L{getPage} and the page is\n        retrieved before the timeout period elapses, the L{Deferred} is\n        called back with the contents of the page.\n        \"\"\"\n        d = getPage(self.getURL(\"host\"), timeout=100)\n        d.addCallback(self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\"))\n        return d\n\n    def test_timeoutTriggering(self):\n        \"\"\"\n        When a non-zero timeout is passed to L{getPage} and that many\n        seconds elapse before the server responds to the request. the\n        L{Deferred} is errbacked with a L{error.TimeoutError}.\n        \"\"\"\n        finished = self.assertFailure(\n            getPage(self.getURL(\"wait\"), timeout=0.000001), defer.TimeoutError\n        )\n\n        def cleanup(passthrough):\n            # Clean up the server which is hanging around not doing\n            # anything.\n            connected = list(self.wrapper.protocols.keys())\n            # There might be nothing here if the server managed to already see\n            # that the connection was lost.\n            if connected:\n                connected[0].transport.loseConnection()\n            return passthrough\n\n        finished.addBoth(cleanup)\n        return finished\n\n    def testNotFound(self):\n        return getPage(self.getURL(\"notsuchfile\")).addCallback(self._cbNoSuchFile)\n\n    def _cbNoSuchFile(self, pageData):\n        self.assertIn(b\"404 - No Such Resource\", pageData)\n\n    def testFactoryInfo(self):\n        url = self.getURL(\"file\")\n        _, _, host, port, _ = client._parse(url)\n        factory = client.ScrapyHTTPClientFactory(Request(url))\n        reactor.connectTCP(to_unicode(host), port, factory)\n        return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n\n    def _cbFactoryInfo(self, ignoredResult, factory):\n        self.assertEqual(factory.status, b\"200\")\n        self.assertTrue(factory.version.startswith(b\"HTTP/\"))\n        self.assertEqual(factory.message, b\"OK\")\n        self.assertEqual(factory.response_headers[b\"content-length\"], b\"10\")\n\n    def testRedirect(self):\n        return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n\n    def _cbRedirect(self, pageData):\n        self.assertEqual(\n            pageData,\n            b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n            b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n            b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n',\n        )\n\n    def test_encoding(self):\n        \"\"\"Test that non-standart body encoding matches\n        Content-Encoding header\"\"\"\n        body = b\"\\xd0\\x81\\xd1\\x8e\\xd0\\xaf\"\n        dfd = getPage(\n            self.getURL(\"encoding\"), body=body, response_transform=lambda r: r\n        )\n        return dfd.addCallback(self._check_Encoding, body)\n\n    def _check_Encoding(self, response, original_body):\n        content_encoding = to_unicode(response.headers[b\"Content-Encoding\"])\n        self.assertEqual(content_encoding, EncodingResource.out_encoding)\n        self.assertEqual(\n            response.body.decode(content_encoding), to_unicode(original_body)\n        )\n\n\nclass WebClientSSLTestCase(unittest.TestCase):\n    context_factory = None\n\n    def _listen(self, site):\n        return reactor.listenSSL(\n            0,\n            site,\n            contextFactory=self.context_factory or ssl_context_factory(),\n            interface=\"127.0.0.1\",\n        )\n\n    def getURL(self, path):\n        return f\"https://127.0.0.1:{self.portno}/{path}\"\n\n    def setUp(self):\n        self.tmpname = Path(mkdtemp())\n        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n        r = static.File(str(self.tmpname))\n        r.putChild(b\"payload\", PayloadResource())\n        self.site = server.Site(r, timeout=None)\n        self.wrapper = WrappingFactory(self.site)\n        self.port = self._listen(self.wrapper)\n        self.portno = self.port.getHost().port\n\n    @inlineCallbacks\n    def tearDown(self):\n        yield self.port.stopListening()\n        shutil.rmtree(self.tmpname)\n\n    def testPayload(self):\n        s = \"0123456789\" * 10\n        return getPage(self.getURL(\"payload\"), body=s).addCallback(\n            self.assertEqual, to_bytes(s)\n        )\n\n\nclass WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n    # we try to use a cipher that is not enabled by default in OpenSSL\n    custom_ciphers = \"CAMELLIA256-SHA\"\n    context_factory = ssl_context_factory(cipher_string=custom_ciphers)\n\n    def testPayload(self):\n        s = \"0123456789\" * 10\n        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n        client_context_factory = build_from_settings(\n            ScrapyClientContextFactory, settings\n        )\n        return getPage(\n            self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n        ).addCallback(self.assertEqual, to_bytes(s))\n\n    def testPayloadDisabledCipher(self):\n        s = \"0123456789\" * 10\n        settings = Settings(\n            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n        )\n        client_context_factory = build_from_settings(\n            ScrapyClientContextFactory, settings\n        )\n        d = getPage(\n            self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n        )\n        return self.assertFailure(d, OpenSSL.SSL.Error)\n", "tests/test_feedexport.py": "import bz2\nimport csv\nimport gzip\nimport json\nimport lzma\nimport random\nimport shutil\nimport string\nimport sys\nimport tempfile\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom contextlib import ExitStack\nfrom io import BytesIO\nfrom logging import getLogger\nfrom os import PathLike\nfrom pathlib import Path\nfrom string import ascii_letters, digits\nfrom typing import Union\nfrom unittest import mock\nfrom urllib.parse import quote, urljoin\nfrom urllib.request import pathname2url\n\nimport lxml.etree\nimport pytest\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\nfrom w3lib.url import file_uri_to_path, path_to_file_uri\nfrom zope.interface import implementer\nfrom zope.interface.verify import verifyObject\n\nimport scrapy\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.exporters import CsvItemExporter, JsonItemExporter\nfrom scrapy.extensions.feedexport import (\n    IS_BOTO3_AVAILABLE,\n    BlockingFeedStorage,\n    FeedExporter,\n    FeedSlot,\n    FileFeedStorage,\n    FTPFeedStorage,\n    GCSFeedStorage,\n    IFeedStorage,\n    S3FeedStorage,\n    StdoutFeedStorage,\n)\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.test import get_crawler, mock_google_cloud_storage, skip_if_no_boto\nfrom tests.mockserver import MockFTPServer, MockServer\nfrom tests.spiders import ItemSpider\n\n\ndef path_to_url(path):\n    return urljoin(\"file:\", pathname2url(str(path)))\n\n\ndef printf_escape(string):\n    return string.replace(\"%\", \"%%\")\n\n\ndef build_url(path: Union[str, PathLike]) -> str:\n    path_str = str(path)\n    if path_str[0] != \"/\":\n        path_str = \"/\" + path_str\n    return urljoin(\"file:\", path_str)\n\n\nclass FileFeedStorageTest(unittest.TestCase):\n    def test_store_file_uri(self):\n        path = Path(self.mktemp()).resolve()\n        uri = path_to_file_uri(str(path))\n        return self._assert_stores(FileFeedStorage(uri), path)\n\n    def test_store_file_uri_makedirs(self):\n        path = Path(self.mktemp()).resolve() / \"more\" / \"paths\" / \"file.txt\"\n        uri = path_to_file_uri(str(path))\n        return self._assert_stores(FileFeedStorage(uri), path)\n\n    def test_store_direct_path(self):\n        path = Path(self.mktemp()).resolve()\n        return self._assert_stores(FileFeedStorage(str(path)), path)\n\n    def test_store_direct_path_relative(self):\n        path = Path(self.mktemp())\n        return self._assert_stores(FileFeedStorage(str(path)), path)\n\n    def test_interface(self):\n        path = self.mktemp()\n        st = FileFeedStorage(path)\n        verifyObject(IFeedStorage, st)\n\n    def _store(self, feed_options=None) -> Path:\n        path = Path(self.mktemp()).resolve()\n        storage = FileFeedStorage(str(path), feed_options=feed_options)\n        spider = scrapy.Spider(\"default\")\n        file = storage.open(spider)\n        file.write(b\"content\")\n        storage.store(file)\n        return path\n\n    def test_append(self):\n        path = self._store()\n        return self._assert_stores(FileFeedStorage(str(path)), path, b\"contentcontent\")\n\n    def test_overwrite(self):\n        path = self._store({\"overwrite\": True})\n        return self._assert_stores(\n            FileFeedStorage(str(path), feed_options={\"overwrite\": True}), path\n        )\n\n    @defer.inlineCallbacks\n    def _assert_stores(self, storage, path: Path, expected_content=b\"content\"):\n        spider = scrapy.Spider(\"default\")\n        file = storage.open(spider)\n        file.write(b\"content\")\n        yield storage.store(file)\n        self.assertTrue(path.exists())\n        try:\n            self.assertEqual(path.read_bytes(), expected_content)\n        finally:\n            path.unlink()\n\n\nclass FTPFeedStorageTest(unittest.TestCase):\n    def get_test_spider(self, settings=None):\n        class TestSpider(scrapy.Spider):\n            name = \"test_spider\"\n\n        crawler = get_crawler(settings_dict=settings)\n        spider = TestSpider.from_crawler(crawler)\n        return spider\n\n    def _store(self, uri, content, feed_options=None, settings=None):\n        crawler = get_crawler(settings_dict=settings or {})\n        storage = FTPFeedStorage.from_crawler(\n            crawler,\n            uri,\n            feed_options=feed_options,\n        )\n        verifyObject(IFeedStorage, storage)\n        spider = self.get_test_spider()\n        file = storage.open(spider)\n        file.write(content)\n        return storage.store(file)\n\n    def _assert_stored(self, path: Path, content):\n        self.assertTrue(path.exists())\n        try:\n            self.assertEqual(path.read_bytes(), content)\n        finally:\n            path.unlink()\n\n    @defer.inlineCallbacks\n    def test_append(self):\n        with MockFTPServer() as ftp_server:\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            feed_options = {\"overwrite\": False}\n            yield self._store(url, b\"foo\", feed_options=feed_options)\n            yield self._store(url, b\"bar\", feed_options=feed_options)\n            self._assert_stored(ftp_server.path / filename, b\"foobar\")\n\n    @defer.inlineCallbacks\n    def test_overwrite(self):\n        with MockFTPServer() as ftp_server:\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            yield self._store(url, b\"foo\")\n            yield self._store(url, b\"bar\")\n            self._assert_stored(ftp_server.path / filename, b\"bar\")\n\n    @defer.inlineCallbacks\n    def test_append_active_mode(self):\n        with MockFTPServer() as ftp_server:\n            settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            feed_options = {\"overwrite\": False}\n            yield self._store(url, b\"foo\", feed_options=feed_options, settings=settings)\n            yield self._store(url, b\"bar\", feed_options=feed_options, settings=settings)\n            self._assert_stored(ftp_server.path / filename, b\"foobar\")\n\n    @defer.inlineCallbacks\n    def test_overwrite_active_mode(self):\n        with MockFTPServer() as ftp_server:\n            settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n            filename = \"file\"\n            url = ftp_server.url(filename)\n            yield self._store(url, b\"foo\", settings=settings)\n            yield self._store(url, b\"bar\", settings=settings)\n            self._assert_stored(ftp_server.path / filename, b\"bar\")\n\n    def test_uri_auth_quote(self):\n        # RFC3986: 3.2.1. User Information\n        pw_quoted = quote(string.punctuation, safe=\"\")\n        st = FTPFeedStorage(f\"ftp://foo:{pw_quoted}@example.com/some_path\", {})\n        self.assertEqual(st.password, string.punctuation)\n\n\nclass BlockingFeedStorageTest(unittest.TestCase):\n    def get_test_spider(self, settings=None):\n        class TestSpider(scrapy.Spider):\n            name = \"test_spider\"\n\n        crawler = get_crawler(settings_dict=settings)\n        spider = TestSpider.from_crawler(crawler)\n        return spider\n\n    def test_default_temp_dir(self):\n        b = BlockingFeedStorage()\n\n        tmp = b.open(self.get_test_spider())\n        tmp_path = Path(tmp.name).parent\n        self.assertEqual(str(tmp_path), tempfile.gettempdir())\n\n    def test_temp_file(self):\n        b = BlockingFeedStorage()\n\n        tests_path = Path(__file__).resolve().parent\n        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(tests_path)})\n        tmp = b.open(spider)\n        tmp_path = Path(tmp.name).parent\n        self.assertEqual(tmp_path, tests_path)\n\n    def test_invalid_folder(self):\n        b = BlockingFeedStorage()\n\n        tests_path = Path(__file__).resolve().parent\n        invalid_path = tests_path / \"invalid_path\"\n        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(invalid_path)})\n\n        self.assertRaises(OSError, b.open, spider=spider)\n\n\nclass S3FeedStorageTest(unittest.TestCase):\n    def setUp(self):\n        skip_if_no_boto()\n\n    def test_parse_credentials(self):\n        aws_credentials = {\n            \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"settings_secret\",\n            \"AWS_SESSION_TOKEN\": \"settings_token\",\n        }\n        crawler = get_crawler(settings_dict=aws_credentials)\n        # Instantiate with crawler\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        self.assertEqual(storage.access_key, \"settings_key\")\n        self.assertEqual(storage.secret_key, \"settings_secret\")\n        self.assertEqual(storage.session_token, \"settings_token\")\n        # Instantiate directly\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n            aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n            session_token=aws_credentials[\"AWS_SESSION_TOKEN\"],\n        )\n        self.assertEqual(storage.access_key, \"settings_key\")\n        self.assertEqual(storage.secret_key, \"settings_secret\")\n        self.assertEqual(storage.session_token, \"settings_token\")\n        # URI priority > settings priority\n        storage = S3FeedStorage(\n            \"s3://uri_key:uri_secret@mybucket/export.csv\",\n            aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n            aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n        )\n        self.assertEqual(storage.access_key, \"uri_key\")\n        self.assertEqual(storage.secret_key, \"uri_secret\")\n\n    @defer.inlineCallbacks\n    def test_store(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        storage = S3FeedStorage.from_crawler(crawler, f\"s3://{bucket}/{key}\")\n        verifyObject(IFeedStorage, storage)\n\n        file = mock.MagicMock()\n\n        if IS_BOTO3_AVAILABLE:\n            storage.s3_client = mock.MagicMock()\n            yield storage.store(file)\n            self.assertEqual(\n                storage.s3_client.upload_fileobj.call_args,\n                mock.call(Bucket=bucket, Key=key, Fileobj=file),\n            )\n        else:\n            from botocore.stub import Stubber\n\n            with Stubber(storage.s3_client) as stub:\n                stub.add_response(\n                    \"put_object\",\n                    expected_params={\n                        \"Body\": file,\n                        \"Bucket\": bucket,\n                        \"Key\": key,\n                    },\n                    service_response={},\n                )\n\n                yield storage.store(file)\n\n                stub.assert_no_pending_responses()\n                self.assertEqual(\n                    file.method_calls,\n                    [\n                        mock.call.seek(0),\n                        # The call to read does not happen with Stubber\n                        mock.call.close(),\n                    ],\n                )\n\n    def test_init_without_acl(self):\n        storage = S3FeedStorage(\"s3://mybucket/export.csv\", \"access_key\", \"secret_key\")\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.acl, None)\n\n    def test_init_with_acl(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.acl, \"custom-acl\")\n\n    def test_init_with_endpoint_url(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            \"access_key\",\n            \"secret_key\",\n            endpoint_url=\"https://example.com\",\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.endpoint_url, \"https://example.com\")\n\n    def test_init_with_region_name(self):\n        region_name = \"ap-east-1\"\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            \"access_key\",\n            \"secret_key\",\n            region_name=region_name,\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.region_name, region_name)\n        self.assertEqual(storage.s3_client._client_config.region_name, region_name)\n\n    def test_from_crawler_without_acl(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.acl, None)\n\n    def test_without_endpoint_url(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.endpoint_url, None)\n\n    def test_without_region_name(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.s3_client._client_config.region_name, \"us-east-1\")\n\n    def test_from_crawler_with_acl(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"FEED_STORAGE_S3_ACL\": \"custom-acl\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(\n            crawler,\n            \"s3://mybucket/export.csv\",\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.acl, \"custom-acl\")\n\n    def test_from_crawler_with_endpoint_url(self):\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"AWS_ENDPOINT_URL\": \"https://example.com\",\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(crawler, \"s3://mybucket/export.csv\")\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.endpoint_url, \"https://example.com\")\n\n    def test_from_crawler_with_region_name(self):\n        region_name = \"ap-east-1\"\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"AWS_REGION_NAME\": region_name,\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(crawler, \"s3://mybucket/export.csv\")\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.region_name, region_name)\n        self.assertEqual(storage.s3_client._client_config.region_name, region_name)\n\n    @defer.inlineCallbacks\n    def test_store_without_acl(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\",\n            \"access_key\",\n            \"secret_key\",\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.acl, None)\n\n        storage.s3_client = mock.MagicMock()\n        yield storage.store(BytesIO(b\"test file\"))\n        if IS_BOTO3_AVAILABLE:\n            acl = (\n                storage.s3_client.upload_fileobj.call_args[1]\n                .get(\"ExtraArgs\", {})\n                .get(\"ACL\")\n            )\n        else:\n            acl = storage.s3_client.put_object.call_args[1].get(\"ACL\")\n        self.assertIsNone(acl)\n\n    @defer.inlineCallbacks\n    def test_store_with_acl(self):\n        storage = S3FeedStorage(\n            \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n        )\n        self.assertEqual(storage.access_key, \"access_key\")\n        self.assertEqual(storage.secret_key, \"secret_key\")\n        self.assertEqual(storage.acl, \"custom-acl\")\n\n        storage.s3_client = mock.MagicMock()\n        yield storage.store(BytesIO(b\"test file\"))\n        if IS_BOTO3_AVAILABLE:\n            acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n        else:\n            acl = storage.s3_client.put_object.call_args[1][\"ACL\"]\n        self.assertEqual(acl, \"custom-acl\")\n\n    def test_overwrite_default(self):\n        with LogCapture() as log:\n            S3FeedStorage(\n                \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n            )\n        self.assertNotIn(\"S3 does not support appending to files\", str(log))\n\n    def test_overwrite_false(self):\n        with LogCapture() as log:\n            S3FeedStorage(\n                \"s3://mybucket/export.csv\",\n                \"access_key\",\n                \"secret_key\",\n                \"custom-acl\",\n                feed_options={\"overwrite\": False},\n            )\n        self.assertIn(\"S3 does not support appending to files\", str(log))\n\n\nclass GCSFeedStorageTest(unittest.TestCase):\n    def test_parse_settings(self):\n        try:\n            from google.cloud.storage import Client  # noqa\n        except ImportError:\n            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n\n        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"publicRead\"}\n        crawler = get_crawler(settings_dict=settings)\n        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n        assert storage.project_id == \"123\"\n        assert storage.acl == \"publicRead\"\n        assert storage.bucket_name == \"mybucket\"\n        assert storage.blob_name == \"export.csv\"\n\n    def test_parse_empty_acl(self):\n        try:\n            from google.cloud.storage import Client  # noqa\n        except ImportError:\n            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n\n        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"\"}\n        crawler = get_crawler(settings_dict=settings)\n        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n        assert storage.acl is None\n\n        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": None}\n        crawler = get_crawler(settings_dict=settings)\n        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n        assert storage.acl is None\n\n    @defer.inlineCallbacks\n    def test_store(self):\n        try:\n            from google.cloud.storage import Client  # noqa\n        except ImportError:\n            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n\n        uri = \"gs://mybucket/export.csv\"\n        project_id = \"myproject-123\"\n        acl = \"publicRead\"\n        (client_mock, bucket_mock, blob_mock) = mock_google_cloud_storage()\n        with mock.patch(\"google.cloud.storage.Client\") as m:\n            m.return_value = client_mock\n\n            f = mock.Mock()\n            storage = GCSFeedStorage(uri, project_id, acl)\n            yield storage.store(f)\n\n            f.seek.assert_called_once_with(0)\n            m.assert_called_once_with(project=project_id)\n            client_mock.get_bucket.assert_called_once_with(\"mybucket\")\n            bucket_mock.blob.assert_called_once_with(\"export.csv\")\n            blob_mock.upload_from_file.assert_called_once_with(f, predefined_acl=acl)\n\n\nclass StdoutFeedStorageTest(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_store(self):\n        out = BytesIO()\n        storage = StdoutFeedStorage(\"stdout:\", _stdout=out)\n        file = storage.open(scrapy.Spider(\"default\"))\n        file.write(b\"content\")\n        yield storage.store(file)\n        self.assertEqual(out.getvalue(), b\"content\")\n\n    def test_overwrite_default(self):\n        with LogCapture() as log:\n            StdoutFeedStorage(\"stdout:\")\n        self.assertNotIn(\n            \"Standard output (stdout) storage does not support overwriting\", str(log)\n        )\n\n    def test_overwrite_true(self):\n        with LogCapture() as log:\n            StdoutFeedStorage(\"stdout:\", feed_options={\"overwrite\": True})\n        self.assertIn(\n            \"Standard output (stdout) storage does not support overwriting\", str(log)\n        )\n\n\nclass FromCrawlerMixin:\n    init_with_crawler = False\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n        cls.init_with_crawler = True\n        return cls(*args, **kwargs)\n\n\nclass FromCrawlerCsvItemExporter(CsvItemExporter, FromCrawlerMixin):\n    pass\n\n\nclass FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n    @classmethod\n    def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n        cls.init_with_crawler = True\n        return cls(*args, feed_options=feed_options, **kwargs)\n\n\nclass DummyBlockingFeedStorage(BlockingFeedStorage):\n    def __init__(self, uri, *args, feed_options=None):\n        self.path = Path(file_uri_to_path(uri))\n\n    def _store_in_thread(self, file):\n        dirname = self.path.parent\n        if dirname and not dirname.exists():\n            dirname.mkdir(parents=True)\n        with self.path.open(\"ab\") as output_file:\n            output_file.write(file.read())\n\n        file.close()\n\n\nclass FailingBlockingFeedStorage(DummyBlockingFeedStorage):\n    def _store_in_thread(self, file):\n        raise OSError(\"Cannot store\")\n\n\n@implementer(IFeedStorage)\nclass LogOnStoreFileStorage:\n    \"\"\"\n    This storage logs inside `store` method.\n    It can be used to make sure `store` method is invoked.\n    \"\"\"\n\n    def __init__(self, uri, feed_options=None):\n        self.path = file_uri_to_path(uri)\n        self.logger = getLogger()\n\n    def open(self, spider):\n        return tempfile.NamedTemporaryFile(prefix=\"feed-\")\n\n    def store(self, file):\n        self.logger.info(\"Storage.store is called\")\n        file.close()\n\n\nclass FeedExportTestBase(ABC, unittest.TestCase):\n    __test__ = False\n\n    class MyItem(scrapy.Item):\n        foo = scrapy.Field()\n        egg = scrapy.Field()\n        baz = scrapy.Field()\n\n    class MyItem2(scrapy.Item):\n        foo = scrapy.Field()\n        hello = scrapy.Field()\n\n    def _random_temp_filename(self, inter_dir=\"\") -> Path:\n        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n        filename = \"\".join(chars)\n        return Path(self.temp_dir, inter_dir, filename)\n\n    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    @defer.inlineCallbacks\n    def exported_data(self, items, settings):\n        \"\"\"\n        Return exported data which a spider yielding ``items`` would return.\n        \"\"\"\n\n        class TestSpider(scrapy.Spider):\n            name = \"testspider\"\n\n            def parse(self, response):\n                yield from items\n\n        data = yield self.run_and_export(TestSpider, settings)\n        return data\n\n    @defer.inlineCallbacks\n    def exported_no_data(self, settings):\n        \"\"\"\n        Return exported data which a spider yielding no ``items`` would return.\n        \"\"\"\n\n        class TestSpider(scrapy.Spider):\n            name = \"testspider\"\n\n            def parse(self, response):\n                pass\n\n        data = yield self.run_and_export(TestSpider, settings)\n        return data\n\n    @defer.inlineCallbacks\n    def assertExported(self, items, header, rows, settings=None):\n        yield self.assertExportedCsv(items, header, rows, settings)\n        yield self.assertExportedJsonLines(items, rows, settings)\n        yield self.assertExportedXml(items, rows, settings)\n        yield self.assertExportedPickle(items, rows, settings)\n        yield self.assertExportedMarshal(items, rows, settings)\n        yield self.assertExportedMultiple(items, rows, settings)\n\n    @abstractmethod\n    def run_and_export(self, spider_cls, settings):\n        pass\n\n    def _load_until_eof(self, data, load_func):\n        result = []\n        with tempfile.TemporaryFile() as temp:\n            temp.write(data)\n            temp.seek(0)\n            while True:\n                try:\n                    result.append(load_func(temp))\n                except EOFError:\n                    break\n        return result\n\n\nclass InstrumentedFeedSlot(FeedSlot):\n    \"\"\"Instrumented FeedSlot subclass for keeping track of calls to\n    start_exporting and finish_exporting.\"\"\"\n\n    def start_exporting(self):\n        self.update_listener(\"start\")\n        super().start_exporting()\n\n    def finish_exporting(self):\n        self.update_listener(\"finish\")\n        super().finish_exporting()\n\n    @classmethod\n    def subscribe__listener(cls, listener):\n        cls.update_listener = listener.update\n\n\nclass IsExportingListener:\n    \"\"\"When subscribed to InstrumentedFeedSlot, keeps track of when\n    a call to start_exporting has been made without a closing call to\n    finish_exporting and when a call to finish_exporting has been made\n    before a call to start_exporting.\"\"\"\n\n    def __init__(self):\n        self.start_without_finish = False\n        self.finish_without_start = False\n\n    def update(self, method):\n        if method == \"start\":\n            self.start_without_finish = True\n        elif method == \"finish\":\n            if self.start_without_finish:\n                self.start_without_finish = False\n            else:\n                self.finish_before_start = True\n\n\nclass ExceptionJsonItemExporter(JsonItemExporter):\n    \"\"\"JsonItemExporter that throws an exception every time export_item is called.\"\"\"\n\n    def export_item(self, _):\n        raise Exception(\"foo\")\n\n\nclass FeedExportTest(FeedExportTestBase):\n    __test__ = True\n\n    @defer.inlineCallbacks\n    def run_and_export(self, spider_cls, settings):\n        \"\"\"Run spider with specified settings; return exported data.\"\"\"\n\n        FEEDS = settings.get(\"FEEDS\") or {}\n        settings[\"FEEDS\"] = {\n            printf_escape(path_to_url(file_path)): feed_options\n            for file_path, feed_options in FEEDS.items()\n        }\n\n        content = {}\n        try:\n            with MockServer() as s:\n                spider_cls.start_urls = [s.url(\"/\")]\n                crawler = get_crawler(spider_cls, settings)\n                yield crawler.crawl()\n\n            for file_path, feed_options in FEEDS.items():\n                content[feed_options[\"format\"]] = (\n                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n                )\n\n        finally:\n            for file_path in FEEDS.keys():\n                if not Path(file_path).exists():\n                    continue\n\n                Path(file_path).unlink()\n\n        return content\n\n    @defer.inlineCallbacks\n    def assertExportedCsv(self, items, header, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"csv\"},\n                },\n            }\n        )\n        data = yield self.exported_data(items, settings)\n        reader = csv.DictReader(to_unicode(data[\"csv\"]).splitlines())\n        self.assertEqual(reader.fieldnames, list(header))\n        self.assertEqual(rows, list(reader))\n\n    @defer.inlineCallbacks\n    def assertExportedJsonLines(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"jl\"},\n                },\n            }\n        )\n        data = yield self.exported_data(items, settings)\n        parsed = [json.loads(to_unicode(line)) for line in data[\"jl\"].splitlines()]\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        self.assertEqual(rows, parsed)\n\n    @defer.inlineCallbacks\n    def assertExportedXml(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"xml\"},\n                },\n            }\n        )\n        data = yield self.exported_data(items, settings)\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        root = lxml.etree.fromstring(data[\"xml\"])\n        got_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n        self.assertEqual(rows, got_rows)\n\n    @defer.inlineCallbacks\n    def assertExportedMultiple(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"xml\"},\n                    self._random_temp_filename(): {\"format\": \"json\"},\n                },\n            }\n        )\n        data = yield self.exported_data(items, settings)\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        # XML\n        root = lxml.etree.fromstring(data[\"xml\"])\n        xml_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n        self.assertEqual(rows, xml_rows)\n        # JSON\n        json_rows = json.loads(to_unicode(data[\"json\"]))\n        self.assertEqual(rows, json_rows)\n\n    @defer.inlineCallbacks\n    def assertExportedPickle(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"pickle\"},\n                },\n            }\n        )\n        data = yield self.exported_data(items, settings)\n        expected = [{k: v for k, v in row.items() if v} for row in rows]\n        import pickle\n\n        result = self._load_until_eof(data[\"pickle\"], load_func=pickle.load)\n        self.assertEqual(expected, result)\n\n    @defer.inlineCallbacks\n    def assertExportedMarshal(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": \"marshal\"},\n                },\n            }\n        )\n        data = yield self.exported_data(items, settings)\n        expected = [{k: v for k, v in row.items() if v} for row in rows]\n        import marshal\n\n        result = self._load_until_eof(data[\"marshal\"], load_func=marshal.load)\n        self.assertEqual(expected, result)\n\n    @defer.inlineCallbacks\n    def test_stats_file_success(self):\n        settings = {\n            \"FEEDS\": {\n                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                    \"format\": \"json\",\n                }\n            },\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        with MockServer() as mockserver:\n            yield crawler.crawl(mockserver=mockserver)\n        self.assertIn(\n            \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n        )\n        self.assertEqual(\n            crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n        )\n\n    @defer.inlineCallbacks\n    def test_stats_file_failed(self):\n        settings = {\n            \"FEEDS\": {\n                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                    \"format\": \"json\",\n                }\n            },\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        with ExitStack() as stack:\n            mockserver = stack.enter_context(MockServer())\n            stack.enter_context(\n                mock.patch(\n                    \"scrapy.extensions.feedexport.FileFeedStorage.store\",\n                    side_effect=KeyError(\"foo\"),\n                )\n            )\n            yield crawler.crawl(mockserver=mockserver)\n        self.assertIn(\n            \"feedexport/failed_count/FileFeedStorage\", crawler.stats.get_stats()\n        )\n        self.assertEqual(\n            crawler.stats.get_value(\"feedexport/failed_count/FileFeedStorage\"), 1\n        )\n\n    @defer.inlineCallbacks\n    def test_stats_multiple_file(self):\n        settings = {\n            \"FEEDS\": {\n                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                    \"format\": \"json\",\n                },\n                \"stdout:\": {\n                    \"format\": \"xml\",\n                },\n            },\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        with MockServer() as mockserver, mock.patch.object(S3FeedStorage, \"store\"):\n            yield crawler.crawl(mockserver=mockserver)\n        self.assertIn(\n            \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n        )\n        self.assertIn(\n            \"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats()\n        )\n        self.assertEqual(\n            crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n        )\n        self.assertEqual(\n            crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1\n        )\n\n    @defer.inlineCallbacks\n    def test_export_items(self):\n        # feed exporters use field names from Item\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n        ]\n        rows = [\n            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n            {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n        ]\n        header = self.MyItem.fields.keys()\n        yield self.assertExported(items, header, rows)\n\n    @defer.inlineCallbacks\n    def test_export_no_items_not_store_empty(self):\n        for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_STORE_EMPTY\": False,\n            }\n            data = yield self.exported_no_data(settings)\n            self.assertEqual(None, data[fmt])\n\n    @defer.inlineCallbacks\n    def test_start_finish_exporting_items(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n        ]\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            _ = yield self.exported_data(items, settings)\n            self.assertFalse(listener.start_without_finish)\n            self.assertFalse(listener.finish_without_start)\n\n    @defer.inlineCallbacks\n    def test_start_finish_exporting_no_items(self):\n        items = []\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            _ = yield self.exported_data(items, settings)\n            self.assertFalse(listener.start_without_finish)\n            self.assertFalse(listener.finish_without_start)\n\n    @defer.inlineCallbacks\n    def test_start_finish_exporting_items_exception(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n        ]\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORTERS\": {\"json\": ExceptionJsonItemExporter},\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            _ = yield self.exported_data(items, settings)\n            self.assertFalse(listener.start_without_finish)\n            self.assertFalse(listener.finish_without_start)\n\n    @defer.inlineCallbacks\n    def test_start_finish_exporting_no_items_exception(self):\n        items = []\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n            },\n            \"FEED_EXPORTERS\": {\"json\": ExceptionJsonItemExporter},\n            \"FEED_EXPORT_INDENT\": None,\n        }\n\n        listener = IsExportingListener()\n        InstrumentedFeedSlot.subscribe__listener(listener)\n\n        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n            _ = yield self.exported_data(items, settings)\n            self.assertFalse(listener.start_without_finish)\n            self.assertFalse(listener.finish_without_start)\n\n    @defer.inlineCallbacks\n    def test_export_no_items_store_empty(self):\n        formats = (\n            (\"json\", b\"[]\"),\n            (\"jsonlines\", b\"\"),\n            (\"xml\", b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n            (\"csv\", b\"\"),\n        )\n\n        for fmt, expctd in formats:\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_EXPORT_INDENT\": None,\n            }\n            data = yield self.exported_no_data(settings)\n            self.assertEqual(expctd, data[fmt])\n\n    @defer.inlineCallbacks\n    def test_export_no_items_multiple_feeds(self):\n        \"\"\"Make sure that `storage.store` is called for every feed.\"\"\"\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n                self._random_temp_filename(): {\"format\": \"xml\"},\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n            \"FEED_STORAGES\": {\"file\": LogOnStoreFileStorage},\n            \"FEED_STORE_EMPTY\": False,\n        }\n\n        with LogCapture() as log:\n            yield self.exported_no_data(settings)\n\n        self.assertEqual(str(log).count(\"Storage.store is called\"), 0)\n\n    @defer.inlineCallbacks\n    def test_export_multiple_item_classes(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"egg\": \"spam3\", \"baz\": \"quux3\"}),\n            {\"hello\": \"world4\", \"egg\": \"spam4\"},\n        ]\n\n        # by default, Scrapy uses fields of the first Item for CSV and\n        # all fields for JSON Lines\n        header = self.MyItem.fields.keys()\n        rows_csv = [\n            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n            {\"egg\": \"\", \"foo\": \"bar2\", \"baz\": \"\"},\n            {\"egg\": \"spam3\", \"foo\": \"bar3\", \"baz\": \"quux3\"},\n            {\"egg\": \"spam4\", \"foo\": \"\", \"baz\": \"\"},\n        ]\n        rows_jl = [dict(row) for row in items]\n        yield self.assertExportedCsv(items, header, rows_csv)\n        yield self.assertExportedJsonLines(items, rows_jl)\n\n    @defer.inlineCallbacks\n    def test_export_items_empty_field_list(self):\n        # FEED_EXPORT_FIELDS==[] means the same as default None\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\"]\n        rows = [{\"foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": []}\n        yield self.assertExportedCsv(items, header, rows)\n        yield self.assertExportedJsonLines(items, rows, settings)\n\n    @defer.inlineCallbacks\n    def test_export_items_field_list(self):\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\", \"baz\"]\n        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": header}\n        yield self.assertExported(items, header, rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_items_comma_separated_field_list(self):\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\", \"baz\"]\n        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": \",\".join(header)}\n        yield self.assertExported(items, header, rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_items_json_field_list(self):\n        items = [{\"foo\": \"bar\"}]\n        header = [\"foo\", \"baz\"]\n        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n        yield self.assertExported(items, header, rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_items_field_names(self):\n        items = [{\"foo\": \"bar\"}]\n        header = {\"foo\": \"Foo\"}\n        rows = [{\"Foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": header}\n        yield self.assertExported(items, list(header.values()), rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_items_dict_field_names(self):\n        items = [{\"foo\": \"bar\"}]\n        header = {\n            \"baz\": \"Baz\",\n            \"foo\": \"Foo\",\n        }\n        rows = [{\"Baz\": \"\", \"Foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": header}\n        yield self.assertExported(items, [\"Baz\", \"Foo\"], rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_items_json_field_names(self):\n        items = [{\"foo\": \"bar\"}]\n        header = {\"foo\": \"Foo\"}\n        rows = [{\"Foo\": \"bar\"}]\n        settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n        yield self.assertExported(items, list(header.values()), rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_based_on_item_classes(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n            {\"hello\": \"world3\", \"egg\": \"spam3\"},\n        ]\n\n        formats = {\n            \"csv\": b\"baz,egg,foo\\r\\n,spam1,bar1\\r\\n\",\n            \"json\": b'[\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n]',\n            \"jsonlines\": (\n                b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n'\n                b'{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n'\n            ),\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n                b\"<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>\"\n                b\"world2</hello><foo>bar2</foo></item>\\n<item><hello>world3\"\n                b\"</hello><egg>spam3</egg></item>\\n</items>\"\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"csv\",\n                    \"item_classes\": [self.MyItem],\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"json\",\n                    \"item_classes\": [self.MyItem2],\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"jsonlines\",\n                    \"item_classes\": [self.MyItem, self.MyItem2],\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"xml\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            self.assertEqual(expected, data[fmt])\n\n    @defer.inlineCallbacks\n    def test_export_based_on_custom_filters(self):\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n            {\"hello\": \"world3\", \"egg\": \"spam3\"},\n        ]\n\n        MyItem = self.MyItem\n\n        class CustomFilter1:\n            def __init__(self, feed_options):\n                pass\n\n            def accepts(self, item):\n                return isinstance(item, MyItem)\n\n        class CustomFilter2(scrapy.extensions.feedexport.ItemFilter):\n            def accepts(self, item):\n                if \"foo\" not in item.fields:\n                    return False\n                return True\n\n        class CustomFilter3(scrapy.extensions.feedexport.ItemFilter):\n            def accepts(self, item):\n                if isinstance(item, tuple(self.item_classes)) and item[\"foo\"] == \"bar1\":\n                    return True\n                return False\n\n        formats = {\n            \"json\": b'[\\n{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n]',\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n                b\"<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>\"\n                b\"world2</hello><foo>bar2</foo></item>\\n</items>\"\n            ),\n            \"jsonlines\": b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n',\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"json\",\n                    \"item_filter\": CustomFilter1,\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"xml\",\n                    \"item_filter\": CustomFilter2,\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"jsonlines\",\n                    \"item_classes\": [self.MyItem, self.MyItem2],\n                    \"item_filter\": CustomFilter3,\n                },\n            },\n        }\n\n        data = yield self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            self.assertEqual(expected, data[fmt])\n\n    @defer.inlineCallbacks\n    def test_export_dicts(self):\n        # When dicts are used, only keys from the first row are used as\n        # a header for CSV, and all fields are used for JSON Lines.\n        items = [\n            {\"foo\": \"bar\", \"egg\": \"spam\"},\n            {\"foo\": \"bar\", \"egg\": \"spam\", \"baz\": \"quux\"},\n        ]\n        rows_csv = [{\"egg\": \"spam\", \"foo\": \"bar\"}, {\"egg\": \"spam\", \"foo\": \"bar\"}]\n        rows_jl = items\n        yield self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n        yield self.assertExportedJsonLines(items, rows_jl)\n\n    @defer.inlineCallbacks\n    def test_export_tuple(self):\n        items = [\n            {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n            {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux\"},\n        ]\n\n        settings = {\"FEED_EXPORT_FIELDS\": (\"foo\", \"baz\")}\n        rows = [{\"foo\": \"bar1\", \"baz\": \"\"}, {\"foo\": \"bar2\", \"baz\": \"quux\"}]\n        yield self.assertExported(items, [\"foo\", \"baz\"], rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_feed_export_fields(self):\n        # FEED_EXPORT_FIELDS option allows to order export fields\n        # and to select a subset of fields to export, both for Items and dicts.\n\n        for item_cls in [self.MyItem, dict]:\n            items = [\n                item_cls({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n                item_cls({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            ]\n\n            # export all columns\n            settings = {\"FEED_EXPORT_FIELDS\": \"foo,baz,egg\"}\n            rows = [\n                {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n                {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n            ]\n            yield self.assertExported(\n                items, [\"foo\", \"baz\", \"egg\"], rows, settings=settings\n            )\n\n            # export a subset of columns\n            settings = {\"FEED_EXPORT_FIELDS\": \"egg,baz\"}\n            rows = [{\"egg\": \"spam1\", \"baz\": \"\"}, {\"egg\": \"spam2\", \"baz\": \"quux2\"}]\n            yield self.assertExported(items, [\"egg\", \"baz\"], rows, settings=settings)\n\n    @defer.inlineCallbacks\n    def test_export_encoding(self):\n        items = [{\"foo\": \"Test\\xd6\"}]\n\n        formats = {\n            \"json\": b'[{\"foo\": \"Test\\\\u00d6\"}]',\n            \"jsonlines\": b'{\"foo\": \"Test\\\\u00d6\"}\\n',\n            \"xml\": (\n                '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n                \"<items><item><foo>Test\\xd6</foo></item></items>\"\n            ).encode(),\n            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(),\n        }\n\n        for fmt, expected in formats.items():\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_EXPORT_INDENT\": None,\n            }\n            data = yield self.exported_data(items, settings)\n            self.assertEqual(expected, data[fmt])\n\n        formats = {\n            \"json\": b'[{\"foo\": \"Test\\xd6\"}]',\n            \"jsonlines\": b'{\"foo\": \"Test\\xd6\"}\\n',\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                b\"<items><item><foo>Test\\xd6</foo></item></items>\"\n            ),\n            \"csv\": b\"foo\\r\\nTest\\xd6\\r\\n\",\n        }\n\n        for fmt, expected in formats.items():\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\"format\": fmt},\n                },\n                \"FEED_EXPORT_INDENT\": None,\n                \"FEED_EXPORT_ENCODING\": \"latin-1\",\n            }\n            data = yield self.exported_data(items, settings)\n            self.assertEqual(expected, data[fmt])\n\n    @defer.inlineCallbacks\n    def test_export_multiple_configs(self):\n        items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n\n        formats = {\n            \"json\": b'[\\n{\"bar\": \"BAR\"}\\n]',\n            \"xml\": (\n                b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                b\"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n            ),\n            \"csv\": b\"bar,foo\\r\\nBAR,FOO\\r\\n\",\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"json\",\n                    \"indent\": 0,\n                    \"fields\": [\"bar\"],\n                    \"encoding\": \"utf-8\",\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"xml\",\n                    \"indent\": 2,\n                    \"fields\": [\"foo\"],\n                    \"encoding\": \"latin-1\",\n                },\n                self._random_temp_filename(): {\n                    \"format\": \"csv\",\n                    \"indent\": None,\n                    \"fields\": [\"bar\", \"foo\"],\n                    \"encoding\": \"utf-8\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            self.assertEqual(expected, data[fmt])\n\n    @defer.inlineCallbacks\n    def test_export_indentation(self):\n        items = [\n            {\"foo\": [\"bar\"]},\n            {\"key\": \"value\"},\n        ]\n\n        test_cases = [\n            # JSON\n            {\n                \"format\": \"json\",\n                \"indent\": None,\n                \"expected\": b'[{\"foo\": [\"bar\"]},{\"key\": \"value\"}]',\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": -1,\n                \"expected\": b\"\"\"[\n{\"foo\": [\"bar\"]},\n{\"key\": \"value\"}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 0,\n                \"expected\": b\"\"\"[\n{\"foo\": [\"bar\"]},\n{\"key\": \"value\"}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 2,\n                \"expected\": b\"\"\"[\n{\n  \"foo\": [\n    \"bar\"\n  ]\n},\n{\n  \"key\": \"value\"\n}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 4,\n                \"expected\": b\"\"\"[\n{\n    \"foo\": [\n        \"bar\"\n    ]\n},\n{\n    \"key\": \"value\"\n}\n]\"\"\",\n            },\n            {\n                \"format\": \"json\",\n                \"indent\": 5,\n                \"expected\": b\"\"\"[\n{\n     \"foo\": [\n          \"bar\"\n     ]\n},\n{\n     \"key\": \"value\"\n}\n]\"\"\",\n            },\n            # XML\n            {\n                \"format\": \"xml\",\n                \"indent\": None,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": -1,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n<item><foo><value>bar</value></foo></item>\n<item><key>value</key></item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 0,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n<item><foo><value>bar</value></foo></item>\n<item><key>value</key></item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 2,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <foo>\n      <value>bar</value>\n    </foo>\n  </item>\n  <item>\n    <key>value</key>\n  </item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 4,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n    <item>\n        <foo>\n            <value>bar</value>\n        </foo>\n    </item>\n    <item>\n        <key>value</key>\n    </item>\n</items>\"\"\",\n            },\n            {\n                \"format\": \"xml\",\n                \"indent\": 5,\n                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n     <item>\n          <foo>\n               <value>bar</value>\n          </foo>\n     </item>\n     <item>\n          <key>value</key>\n     </item>\n</items>\"\"\",\n            },\n        ]\n\n        for row in test_cases:\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): {\n                        \"format\": row[\"format\"],\n                        \"indent\": row[\"indent\"],\n                    },\n                },\n            }\n            data = yield self.exported_data(items, settings)\n            self.assertEqual(row[\"expected\"], data[row[\"format\"]])\n\n    @defer.inlineCallbacks\n    def test_init_exporters_storages_with_crawler(self):\n        settings = {\n            \"FEED_EXPORTERS\": {\"csv\": FromCrawlerCsvItemExporter},\n            \"FEED_STORAGES\": {\"file\": FromCrawlerFileFeedStorage},\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n        }\n        yield self.exported_data(items=[], settings=settings)\n        self.assertTrue(FromCrawlerCsvItemExporter.init_with_crawler)\n        self.assertTrue(FromCrawlerFileFeedStorage.init_with_crawler)\n\n    @defer.inlineCallbacks\n    def test_str_uri(self):\n        settings = {\n            \"FEED_STORE_EMPTY\": True,\n            \"FEEDS\": {str(self._random_temp_filename()): {\"format\": \"csv\"}},\n        }\n        data = yield self.exported_no_data(settings)\n        self.assertEqual(data[\"csv\"], b\"\")\n\n    @defer.inlineCallbacks\n    def test_multiple_feeds_success_logs_blocking_feed_storage(self):\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n                self._random_temp_filename(): {\"format\": \"xml\"},\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n            \"FEED_STORAGES\": {\"file\": DummyBlockingFeedStorage},\n        }\n        items = [\n            {\"foo\": \"bar1\", \"baz\": \"\"},\n            {\"foo\": \"bar2\", \"baz\": \"quux\"},\n        ]\n        with LogCapture() as log:\n            yield self.exported_data(items, settings)\n\n        print(log)\n        for fmt in [\"json\", \"xml\", \"csv\"]:\n            self.assertIn(f\"Stored {fmt} feed (2 items)\", str(log))\n\n    @defer.inlineCallbacks\n    def test_multiple_feeds_failing_logs_blocking_feed_storage(self):\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"json\"},\n                self._random_temp_filename(): {\"format\": \"xml\"},\n                self._random_temp_filename(): {\"format\": \"csv\"},\n            },\n            \"FEED_STORAGES\": {\"file\": FailingBlockingFeedStorage},\n        }\n        items = [\n            {\"foo\": \"bar1\", \"baz\": \"\"},\n            {\"foo\": \"bar2\", \"baz\": \"quux\"},\n        ]\n        with LogCapture() as log:\n            yield self.exported_data(items, settings)\n\n        print(log)\n        for fmt in [\"json\", \"xml\", \"csv\"]:\n            self.assertIn(f\"Error storing {fmt} feed (2 items)\", str(log))\n\n    @defer.inlineCallbacks\n    def test_extend_kwargs(self):\n        items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n\n        expected_with_title_csv = b\"foo,bar\\r\\nFOO,BAR\\r\\n\"\n        expected_without_title_csv = b\"FOO,BAR\\r\\n\"\n        test_cases = [\n            # with title\n            {\n                \"options\": {\n                    \"format\": \"csv\",\n                    \"item_export_kwargs\": {\"include_headers_line\": True},\n                },\n                \"expected\": expected_with_title_csv,\n            },\n            # without title\n            {\n                \"options\": {\n                    \"format\": \"csv\",\n                    \"item_export_kwargs\": {\"include_headers_line\": False},\n                },\n                \"expected\": expected_without_title_csv,\n            },\n        ]\n\n        for row in test_cases:\n            feed_options = row[\"options\"]\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename(): feed_options,\n                },\n                \"FEED_EXPORT_INDENT\": None,\n            }\n\n            data = yield self.exported_data(items, settings)\n            self.assertEqual(row[\"expected\"], data[feed_options[\"format\"]])\n\n    @defer.inlineCallbacks\n    def test_storage_file_no_postprocessing(self):\n        @implementer(IFeedStorage)\n        class Storage:\n            def __init__(self, uri, *, feed_options=None):\n                pass\n\n            def open(self, spider):\n                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n                return Storage.open_file\n\n            def store(self, file):\n                Storage.store_file = file\n                file.close()\n\n        settings = {\n            \"FEEDS\": {self._random_temp_filename(): {\"format\": \"jsonlines\"}},\n            \"FEED_STORAGES\": {\"file\": Storage},\n        }\n        yield self.exported_no_data(settings)\n        self.assertIs(Storage.open_file, Storage.store_file)\n\n    @defer.inlineCallbacks\n    def test_storage_file_postprocessing(self):\n        @implementer(IFeedStorage)\n        class Storage:\n            def __init__(self, uri, *, feed_options=None):\n                pass\n\n            def open(self, spider):\n                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n                return Storage.open_file\n\n            def store(self, file):\n                Storage.store_file = file\n                Storage.file_was_closed = file.closed\n                file.close()\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\n                    \"format\": \"jsonlines\",\n                    \"postprocessing\": [\n                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n                    ],\n                },\n            },\n            \"FEED_STORAGES\": {\"file\": Storage},\n        }\n        yield self.exported_no_data(settings)\n        self.assertIs(Storage.open_file, Storage.store_file)\n        self.assertFalse(Storage.file_was_closed)\n\n\nclass FeedPostProcessedExportsTest(FeedExportTestBase):\n    __test__ = True\n\n    items = [{\"foo\": \"bar\"}]\n    expected = b\"foo\\r\\nbar\\r\\n\"\n\n    class MyPlugin1:\n        def __init__(self, file, feed_options):\n            self.file = file\n            self.feed_options = feed_options\n            self.char = self.feed_options.get(\"plugin1_char\", b\"\")\n\n        def write(self, data):\n            written_count = self.file.write(data)\n            written_count += self.file.write(self.char)\n            return written_count\n\n        def close(self):\n            self.file.close()\n\n    def _named_tempfile(self, name) -> str:\n        return str(Path(self.temp_dir, name))\n\n    @defer.inlineCallbacks\n    def run_and_export(self, spider_cls, settings):\n        \"\"\"Run spider with specified settings; return exported data with filename.\"\"\"\n\n        FEEDS = settings.get(\"FEEDS\") or {}\n        settings[\"FEEDS\"] = {\n            printf_escape(path_to_url(file_path)): feed_options\n            for file_path, feed_options in FEEDS.items()\n        }\n\n        content = {}\n        try:\n            with MockServer() as s:\n                spider_cls.start_urls = [s.url(\"/\")]\n                crawler = get_crawler(spider_cls, settings)\n                yield crawler.crawl()\n\n            for file_path, feed_options in FEEDS.items():\n                content[str(file_path)] = (\n                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n                )\n\n        finally:\n            for file_path in FEEDS.keys():\n                if not Path(file_path).exists():\n                    continue\n\n                Path(file_path).unlink()\n\n        return content\n\n    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=\"\"):\n        data_stream = BytesIO()\n        gzipf = gzip.GzipFile(\n            fileobj=data_stream,\n            filename=filename,\n            mtime=mtime,\n            compresslevel=compresslevel,\n            mode=\"wb\",\n        )\n        gzipf.write(data)\n        gzipf.close()\n        data_stream.seek(0)\n        return data_stream.read()\n\n    @defer.inlineCallbacks\n    def test_gzip_plugin(self):\n        filename = self._named_tempfile(\"gzip_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n        try:\n            gzip.decompress(data[filename])\n        except OSError:\n            self.fail(\"Received invalid gzip data.\")\n\n    @defer.inlineCallbacks\n    def test_gzip_plugin_compresslevel(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"compresslevel_0\"): self.get_gzip_compressed(\n                self.expected, compresslevel=0\n            ),\n            self._named_tempfile(\"compresslevel_9\"): self.get_gzip_compressed(\n                self.expected, compresslevel=9\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"compresslevel_0\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_compresslevel\": 0,\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"\",\n                },\n                self._named_tempfile(\"compresslevel_9\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_compresslevel\": 9,\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = gzip.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_gzip_plugin_mtime(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"mtime_123\"): self.get_gzip_compressed(\n                self.expected, mtime=123\n            ),\n            self._named_tempfile(\"mtime_123456789\"): self.get_gzip_compressed(\n                self.expected, mtime=123456789\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"mtime_123\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 123,\n                    \"gzip_filename\": \"\",\n                },\n                self._named_tempfile(\"mtime_123456789\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 123456789,\n                    \"gzip_filename\": \"\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = gzip.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_gzip_plugin_filename(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"filename_FILE1\"): self.get_gzip_compressed(\n                self.expected, filename=\"FILE1\"\n            ),\n            self._named_tempfile(\"filename_FILE2\"): self.get_gzip_compressed(\n                self.expected, filename=\"FILE2\"\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"filename_FILE1\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"FILE1\",\n                },\n                self._named_tempfile(\"filename_FILE2\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                    \"gzip_mtime\": 0,\n                    \"gzip_filename\": \"FILE2\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = gzip.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_lzma_plugin(self):\n        filename = self._named_tempfile(\"lzma_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n        try:\n            lzma.decompress(data[filename])\n        except lzma.LZMAError:\n            self.fail(\"Received invalid lzma data.\")\n\n    @defer.inlineCallbacks\n    def test_lzma_plugin_format(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"format_FORMAT_XZ\"): lzma.compress(\n                self.expected, format=lzma.FORMAT_XZ\n            ),\n            self._named_tempfile(\"format_FORMAT_ALONE\"): lzma.compress(\n                self.expected, format=lzma.FORMAT_ALONE\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"format_FORMAT_XZ\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_format\": lzma.FORMAT_XZ,\n                },\n                self._named_tempfile(\"format_FORMAT_ALONE\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_format\": lzma.FORMAT_ALONE,\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = lzma.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_lzma_plugin_check(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"check_CHECK_NONE\"): lzma.compress(\n                self.expected, check=lzma.CHECK_NONE\n            ),\n            self._named_tempfile(\"check_CHECK_CRC256\"): lzma.compress(\n                self.expected, check=lzma.CHECK_SHA256\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"check_CHECK_NONE\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_check\": lzma.CHECK_NONE,\n                },\n                self._named_tempfile(\"check_CHECK_CRC256\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_check\": lzma.CHECK_SHA256,\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = lzma.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_lzma_plugin_preset(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"preset_PRESET_0\"): lzma.compress(\n                self.expected, preset=0\n            ),\n            self._named_tempfile(\"preset_PRESET_9\"): lzma.compress(\n                self.expected, preset=9\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"preset_PRESET_0\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_preset\": 0,\n                },\n                self._named_tempfile(\"preset_PRESET_9\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_preset\": 9,\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = lzma.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_lzma_plugin_filters(self):\n        if \"PyPy\" in sys.version:\n            # https://foss.heptapod.net/pypy/pypy/-/issues/3527\n            raise unittest.SkipTest(\"lzma filters doesn't work in PyPy\")\n\n        filters = [{\"id\": lzma.FILTER_LZMA2}]\n        compressed = lzma.compress(self.expected, filters=filters)\n        filename = self._named_tempfile(\"filters\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                    \"lzma_filters\": filters,\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n        self.assertEqual(compressed, data[filename])\n        result = lzma.decompress(data[filename])\n        self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_bz2_plugin(self):\n        filename = self._named_tempfile(\"bz2_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n        try:\n            bz2.decompress(data[filename])\n        except OSError:\n            self.fail(\"Received invalid bz2 data.\")\n\n    @defer.inlineCallbacks\n    def test_bz2_plugin_compresslevel(self):\n        filename_to_compressed = {\n            self._named_tempfile(\"compresslevel_1\"): bz2.compress(\n                self.expected, compresslevel=1\n            ),\n            self._named_tempfile(\"compresslevel_9\"): bz2.compress(\n                self.expected, compresslevel=9\n            ),\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"compresslevel_1\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                    \"bz2_compresslevel\": 1,\n                },\n                self._named_tempfile(\"compresslevel_9\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                    \"bz2_compresslevel\": 9,\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, compressed in filename_to_compressed.items():\n            result = bz2.decompress(data[filename])\n            self.assertEqual(compressed, data[filename])\n            self.assertEqual(self.expected, result)\n\n    @defer.inlineCallbacks\n    def test_custom_plugin(self):\n        filename = self._named_tempfile(\"csv_file\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n        self.assertEqual(self.expected, data[filename])\n\n    @defer.inlineCallbacks\n    def test_custom_plugin_with_parameter(self):\n        expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n        filename = self._named_tempfile(\"newline\")\n\n        settings = {\n            \"FEEDS\": {\n                filename: {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [self.MyPlugin1],\n                    \"plugin1_char\": b\"\\n\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n        self.assertEqual(expected, data[filename])\n\n    @defer.inlineCallbacks\n    def test_custom_plugin_with_compression(self):\n        expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n\n        filename_to_decompressor = {\n            self._named_tempfile(\"bz2\"): bz2.decompress,\n            self._named_tempfile(\"lzma\"): lzma.decompress,\n            self._named_tempfile(\"gzip\"): gzip.decompress,\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"bz2\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\n                        self.MyPlugin1,\n                        \"scrapy.extensions.postprocessing.Bz2Plugin\",\n                    ],\n                    \"plugin1_char\": b\"\\n\",\n                },\n                self._named_tempfile(\"lzma\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\n                        self.MyPlugin1,\n                        \"scrapy.extensions.postprocessing.LZMAPlugin\",\n                    ],\n                    \"plugin1_char\": b\"\\n\",\n                },\n                self._named_tempfile(\"gzip\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [\n                        self.MyPlugin1,\n                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n                    ],\n                    \"plugin1_char\": b\"\\n\",\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, decompressor in filename_to_decompressor.items():\n            result = decompressor(data[filename])\n            self.assertEqual(expected, result)\n\n    @defer.inlineCallbacks\n    def test_exports_compatibility_with_postproc(self):\n        import marshal\n        import pickle\n\n        filename_to_expected = {\n            self._named_tempfile(\"csv\"): b\"foo\\r\\nbar\\r\\n\",\n            self._named_tempfile(\"json\"): b'[\\n{\"foo\": \"bar\"}\\n]',\n            self._named_tempfile(\"jsonlines\"): b'{\"foo\": \"bar\"}\\n',\n            self._named_tempfile(\"xml\"): b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n            b\"<items>\\n<item><foo>bar</foo></item>\\n</items>\",\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._named_tempfile(\"csv\"): {\n                    \"format\": \"csv\",\n                    \"postprocessing\": [self.MyPlugin1],\n                    # empty plugin to activate postprocessing.PostProcessingManager\n                },\n                self._named_tempfile(\"json\"): {\n                    \"format\": \"json\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"jsonlines\"): {\n                    \"format\": \"jsonlines\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"xml\"): {\n                    \"format\": \"xml\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"marshal\"): {\n                    \"format\": \"marshal\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n                self._named_tempfile(\"pickle\"): {\n                    \"format\": \"pickle\",\n                    \"postprocessing\": [self.MyPlugin1],\n                },\n            },\n        }\n\n        data = yield self.exported_data(self.items, settings)\n\n        for filename, result in data.items():\n            if \"pickle\" in filename:\n                expected, result = self.items[0], pickle.loads(result)\n            elif \"marshal\" in filename:\n                expected, result = self.items[0], marshal.loads(result)\n            else:\n                expected = filename_to_expected[filename]\n            self.assertEqual(expected, result)\n\n\nclass BatchDeliveriesTest(FeedExportTestBase):\n    __test__ = True\n    _file_mark = \"_%(batch_time)s_#%(batch_id)02d_\"\n\n    @defer.inlineCallbacks\n    def run_and_export(self, spider_cls, settings):\n        \"\"\"Run spider with specified settings; return exported data.\"\"\"\n\n        FEEDS = settings.get(\"FEEDS\") or {}\n        settings[\"FEEDS\"] = {\n            build_url(file_path): feed for file_path, feed in FEEDS.items()\n        }\n        content = defaultdict(list)\n        try:\n            with MockServer() as s:\n                spider_cls.start_urls = [s.url(\"/\")]\n                crawler = get_crawler(spider_cls, settings)\n                yield crawler.crawl()\n\n            for path, feed in FEEDS.items():\n                dir_name = Path(path).parent\n                if not dir_name.exists():\n                    content[feed[\"format\"]] = []\n                    continue\n                for file in sorted(dir_name.iterdir()):\n                    content[feed[\"format\"]].append(file.read_bytes())\n        finally:\n            self.tearDown()\n        return content\n\n    @defer.inlineCallbacks\n    def assertExportedJsonLines(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / \"jl\"\n                    / self._file_mark: {\"format\": \"jl\"},\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = yield self.exported_data(items, settings)\n        for batch in data[\"jl\"]:\n            got_batch = [\n                json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()\n            ]\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def assertExportedCsv(self, items, header, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / \"csv\"\n                    / self._file_mark: {\"format\": \"csv\"},\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        data = yield self.exported_data(items, settings)\n        for batch in data[\"csv\"]:\n            got_batch = csv.DictReader(to_unicode(batch).splitlines())\n            self.assertEqual(list(header), got_batch.fieldnames)\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            self.assertEqual(expected_batch, list(got_batch))\n\n    @defer.inlineCallbacks\n    def assertExportedXml(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / \"xml\"\n                    / self._file_mark: {\"format\": \"xml\"},\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = yield self.exported_data(items, settings)\n        for batch in data[\"xml\"]:\n            root = lxml.etree.fromstring(batch)\n            got_batch = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def assertExportedMultiple(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / \"xml\"\n                    / self._file_mark: {\"format\": \"xml\"},\n                    self._random_temp_filename()\n                    / \"json\"\n                    / self._file_mark: {\"format\": \"json\"},\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = yield self.exported_data(items, settings)\n        # XML\n        xml_rows = rows.copy()\n        for batch in data[\"xml\"]:\n            root = lxml.etree.fromstring(batch)\n            got_batch = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n            expected_batch, xml_rows = xml_rows[:batch_size], xml_rows[batch_size:]\n            self.assertEqual(expected_batch, got_batch)\n        # JSON\n        json_rows = rows.copy()\n        for batch in data[\"json\"]:\n            got_batch = json.loads(batch.decode(\"utf-8\"))\n            expected_batch, json_rows = json_rows[:batch_size], json_rows[batch_size:]\n            self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def assertExportedPickle(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / \"pickle\"\n                    / self._file_mark: {\"format\": \"pickle\"},\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = yield self.exported_data(items, settings)\n        import pickle\n\n        for batch in data[\"pickle\"]:\n            got_batch = self._load_until_eof(batch, load_func=pickle.load)\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def assertExportedMarshal(self, items, rows, settings=None):\n        settings = settings or {}\n        settings.update(\n            {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / \"marshal\"\n                    / self._file_mark: {\"format\": \"marshal\"},\n                },\n            }\n        )\n        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n        rows = [{k: v for k, v in row.items() if v} for row in rows]\n        data = yield self.exported_data(items, settings)\n        import marshal\n\n        for batch in data[\"marshal\"]:\n            got_batch = self._load_until_eof(batch, load_func=marshal.load)\n            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n            self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def test_export_items(self):\n        \"\"\"Test partial deliveries in all supported formats\"\"\"\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n        ]\n        rows = [\n            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n            {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n            {\"foo\": \"bar3\", \"baz\": \"quux3\", \"egg\": \"\"},\n        ]\n        settings = {\"FEED_EXPORT_BATCH_ITEM_COUNT\": 2}\n        header = self.MyItem.fields.keys()\n        yield self.assertExported(items, header, rows, settings=settings)\n\n    def test_wrong_path(self):\n        \"\"\"If path is without %(batch_time)s and %(batch_id) an exception must be raised\"\"\"\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename(): {\"format\": \"xml\"},\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        crawler = get_crawler(settings_dict=settings)\n        self.assertRaises(NotConfigured, FeedExporter, crawler)\n\n    @defer.inlineCallbacks\n    def test_export_no_items_not_store_empty(self):\n        for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / fmt\n                    / self._file_mark: {\"format\": fmt},\n                },\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n                \"FEED_STORE_EMPTY\": False,\n            }\n            data = yield self.exported_no_data(settings)\n            data = dict(data)\n            self.assertEqual(0, len(data[fmt]))\n\n    @defer.inlineCallbacks\n    def test_export_no_items_store_empty(self):\n        formats = (\n            (\"json\", b\"[]\"),\n            (\"jsonlines\", b\"\"),\n            (\"xml\", b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n            (\"csv\", b\"\"),\n        )\n\n        for fmt, expctd in formats:\n            settings = {\n                \"FEEDS\": {\n                    self._random_temp_filename()\n                    / fmt\n                    / self._file_mark: {\"format\": fmt},\n                },\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_EXPORT_INDENT\": None,\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n            }\n            data = yield self.exported_no_data(settings)\n            data = dict(data)\n            self.assertEqual(expctd, data[fmt][0])\n\n    @defer.inlineCallbacks\n    def test_export_multiple_configs(self):\n        items = [\n            {\"foo\": \"FOO\", \"bar\": \"BAR\"},\n            {\"foo\": \"FOO1\", \"bar\": \"BAR1\"},\n        ]\n\n        formats = {\n            \"json\": [\n                b'[\\n{\"bar\": \"BAR\"}\\n]',\n                b'[\\n{\"bar\": \"BAR1\"}\\n]',\n            ],\n            \"xml\": [\n                (\n                    b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                    b\"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n                ),\n                (\n                    b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n                    b\"<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>\"\n                ),\n            ],\n            \"csv\": [\n                b\"foo,bar\\r\\nFOO,BAR\\r\\n\",\n                b\"foo,bar\\r\\nFOO1,BAR1\\r\\n\",\n            ],\n        }\n\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename()\n                / \"json\"\n                / self._file_mark: {\n                    \"format\": \"json\",\n                    \"indent\": 0,\n                    \"fields\": [\"bar\"],\n                    \"encoding\": \"utf-8\",\n                },\n                self._random_temp_filename()\n                / \"xml\"\n                / self._file_mark: {\n                    \"format\": \"xml\",\n                    \"indent\": 2,\n                    \"fields\": [\"foo\"],\n                    \"encoding\": \"latin-1\",\n                },\n                self._random_temp_filename()\n                / \"csv\"\n                / self._file_mark: {\n                    \"format\": \"csv\",\n                    \"indent\": None,\n                    \"fields\": [\"foo\", \"bar\"],\n                    \"encoding\": \"utf-8\",\n                },\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        data = yield self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            for expected_batch, got_batch in zip(expected, data[fmt]):\n                self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def test_batch_item_count_feeds_setting(self):\n        items = [{\"foo\": \"FOO\"}, {\"foo\": \"FOO1\"}]\n        formats = {\n            \"json\": [\n                b'[{\"foo\": \"FOO\"}]',\n                b'[{\"foo\": \"FOO1\"}]',\n            ],\n        }\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename()\n                / \"json\"\n                / self._file_mark: {\n                    \"format\": \"json\",\n                    \"indent\": None,\n                    \"encoding\": \"utf-8\",\n                    \"batch_item_count\": 1,\n                },\n            },\n        }\n        data = yield self.exported_data(items, settings)\n        for fmt, expected in formats.items():\n            for expected_batch, got_batch in zip(expected, data[fmt]):\n                self.assertEqual(expected_batch, got_batch)\n\n    @defer.inlineCallbacks\n    def test_batch_path_differ(self):\n        \"\"\"\n        Test that the name of all batch files differ from each other.\n        So %(batch_id)d replaced with the current id.\n        \"\"\"\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n        ]\n        settings = {\n            \"FEEDS\": {\n                self._random_temp_filename()\n                / \"%(batch_id)d\": {\n                    \"format\": \"json\",\n                },\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        data = yield self.exported_data(items, settings)\n        self.assertEqual(len(items), len(data[\"json\"]))\n\n    @defer.inlineCallbacks\n    def test_stats_batch_file_success(self):\n        settings = {\n            \"FEEDS\": {\n                build_url(\n                    str(self._random_temp_filename() / \"json\" / self._file_mark)\n                ): {\n                    \"format\": \"json\",\n                }\n            },\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        with MockServer() as mockserver:\n            yield crawler.crawl(total=2, mockserver=mockserver)\n        self.assertIn(\n            \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n        )\n        self.assertEqual(\n            crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 12\n        )\n\n    @defer.inlineCallbacks\n    def test_s3_export(self):\n        skip_if_no_boto()\n        bucket = \"mybucket\"\n        items = [\n            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n        ]\n\n        class CustomS3FeedStorage(S3FeedStorage):\n            stubs = []\n\n            def open(self, *args, **kwargs):\n                from botocore.stub import ANY, Stubber\n\n                stub = Stubber(self.s3_client)\n                stub.activate()\n                CustomS3FeedStorage.stubs.append(stub)\n                stub.add_response(\n                    \"put_object\",\n                    expected_params={\n                        \"Body\": ANY,\n                        \"Bucket\": bucket,\n                        \"Key\": ANY,\n                    },\n                    service_response={},\n                )\n                return super().open(*args, **kwargs)\n\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}/%(batch_id)d.json\"\n        batch_item_count = 1\n        settings = {\n            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n            \"FEED_EXPORT_BATCH_ITEM_COUNT\": batch_item_count,\n            \"FEED_STORAGES\": {\n                \"s3\": CustomS3FeedStorage,\n            },\n            \"FEEDS\": {\n                uri: {\n                    \"format\": \"json\",\n                },\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        storage = S3FeedStorage.from_crawler(crawler, uri)\n        verifyObject(IFeedStorage, storage)\n\n        class TestSpider(scrapy.Spider):\n            name = \"testspider\"\n\n            def parse(self, response):\n                yield from items\n\n        with MockServer() as server:\n            TestSpider.start_urls = [server.url(\"/\")]\n            crawler = get_crawler(TestSpider, settings)\n            yield crawler.crawl()\n\n        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items))\n        for stub in CustomS3FeedStorage.stubs[:-1]:\n            stub.assert_no_pending_responses()\n\n\n# Test that the FeedExporer sends the feed_exporter_closed and feed_slot_closed signals\nclass FeedExporterSignalsTest(unittest.TestCase):\n    items = [\n        {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n        {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"},\n        {\"foo\": \"bar3\", \"baz\": \"quux3\"},\n    ]\n\n    with tempfile.NamedTemporaryFile(suffix=\"json\") as tmp:\n        settings = {\n            \"FEEDS\": {\n                f\"file:///{tmp.name}\": {\n                    \"format\": \"json\",\n                },\n            },\n        }\n\n    def feed_exporter_closed_signal_handler(self):\n        self.feed_exporter_closed_received = True\n\n    def feed_slot_closed_signal_handler(self, slot):\n        self.feed_slot_closed_received = True\n\n    def feed_exporter_closed_signal_handler_deferred(self):\n        d = defer.Deferred()\n        d.addCallback(lambda _: setattr(self, \"feed_exporter_closed_received\", True))\n        d.callback(None)\n        return d\n\n    def feed_slot_closed_signal_handler_deferred(self, slot):\n        d = defer.Deferred()\n        d.addCallback(lambda _: setattr(self, \"feed_slot_closed_received\", True))\n        d.callback(None)\n        return d\n\n    def run_signaled_feed_exporter(\n        self, feed_exporter_signal_handler, feed_slot_signal_handler\n    ):\n        crawler = get_crawler(settings_dict=self.settings)\n        feed_exporter = FeedExporter.from_crawler(crawler)\n        spider = scrapy.Spider(\"default\")\n        spider.crawler = crawler\n        crawler.signals.connect(\n            feed_exporter_signal_handler,\n            signal=signals.feed_exporter_closed,\n        )\n        crawler.signals.connect(\n            feed_slot_signal_handler, signal=signals.feed_slot_closed\n        )\n        feed_exporter.open_spider(spider)\n        for item in self.items:\n            feed_exporter.item_scraped(item, spider)\n        defer.ensureDeferred(feed_exporter.close_spider(spider))\n\n    def test_feed_exporter_signals_sent(self):\n        self.feed_exporter_closed_received = False\n        self.feed_slot_closed_received = False\n\n        self.run_signaled_feed_exporter(\n            self.feed_exporter_closed_signal_handler,\n            self.feed_slot_closed_signal_handler,\n        )\n        self.assertTrue(self.feed_slot_closed_received)\n        self.assertTrue(self.feed_exporter_closed_received)\n\n    def test_feed_exporter_signals_sent_deferred(self):\n        self.feed_exporter_closed_received = False\n        self.feed_slot_closed_received = False\n\n        self.run_signaled_feed_exporter(\n            self.feed_exporter_closed_signal_handler_deferred,\n            self.feed_slot_closed_signal_handler_deferred,\n        )\n        self.assertTrue(self.feed_slot_closed_received)\n        self.assertTrue(self.feed_exporter_closed_received)\n\n\nclass FeedExportInitTest(unittest.TestCase):\n    def test_unsupported_storage(self):\n        settings = {\n            \"FEEDS\": {\n                \"unsupported://uri\": {},\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        with self.assertRaises(NotConfigured):\n            FeedExporter.from_crawler(crawler)\n\n    def test_unsupported_format(self):\n        settings = {\n            \"FEEDS\": {\n                \"file://path\": {\n                    \"format\": \"unsupported_format\",\n                },\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        with self.assertRaises(NotConfigured):\n            FeedExporter.from_crawler(crawler)\n\n    def test_absolute_pathlib_as_uri(self):\n        with tempfile.NamedTemporaryFile(suffix=\"json\") as tmp:\n            settings = {\n                \"FEEDS\": {\n                    Path(tmp.name).resolve(): {\n                        \"format\": \"json\",\n                    },\n                },\n            }\n            crawler = get_crawler(settings_dict=settings)\n            exporter = FeedExporter.from_crawler(crawler)\n            self.assertIsInstance(exporter, FeedExporter)\n\n    def test_relative_pathlib_as_uri(self):\n        settings = {\n            \"FEEDS\": {\n                Path(\"./items.json\"): {\n                    \"format\": \"json\",\n                },\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        exporter = FeedExporter.from_crawler(crawler)\n        self.assertIsInstance(exporter, FeedExporter)\n\n\nclass URIParamsTest:\n    spider_name = \"uri_params_spider\"\n    deprecated_options = False\n\n    def build_settings(self, uri=\"file:///tmp/foobar\", uri_params=None):\n        raise NotImplementedError\n\n    def _crawler_feed_exporter(self, settings):\n        if self.deprecated_options:\n            with pytest.warns(\n                ScrapyDeprecationWarning,\n                match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n            ):\n                crawler = get_crawler(settings_dict=settings)\n                feed_exporter = FeedExporter.from_crawler(crawler)\n        else:\n            crawler = get_crawler(settings_dict=settings)\n            feed_exporter = FeedExporter.from_crawler(crawler)\n        return crawler, feed_exporter\n\n    def test_default(self):\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            feed_exporter.open_spider(spider)\n\n        self.assertEqual(feed_exporter.slots[0].uri, f\"file:///tmp/{self.spider_name}\")\n\n    def test_none(self):\n        def uri_params(params, spider):\n            pass\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n\n        feed_exporter.open_spider(spider)\n\n        self.assertEqual(feed_exporter.slots[0].uri, f\"file:///tmp/{self.spider_name}\")\n\n    def test_empty_dict(self):\n        def uri_params(params, spider):\n            return {}\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            with self.assertRaises(KeyError):\n                feed_exporter.open_spider(spider)\n\n    def test_params_as_is(self):\n        def uri_params(params, spider):\n            return params\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(name)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            feed_exporter.open_spider(spider)\n\n        self.assertEqual(feed_exporter.slots[0].uri, f\"file:///tmp/{self.spider_name}\")\n\n    def test_custom_param(self):\n        def uri_params(params, spider):\n            return {**params, \"foo\": self.spider_name}\n\n        settings = self.build_settings(\n            uri=\"file:///tmp/%(foo)s\",\n            uri_params=uri_params,\n        )\n        crawler, feed_exporter = self._crawler_feed_exporter(settings)\n        spider = scrapy.Spider(self.spider_name)\n        spider.crawler = crawler\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", ScrapyDeprecationWarning)\n            feed_exporter.open_spider(spider)\n\n        self.assertEqual(feed_exporter.slots[0].uri, f\"file:///tmp/{self.spider_name}\")\n\n\nclass URIParamsSettingTest(URIParamsTest, unittest.TestCase):\n    deprecated_options = True\n\n    def build_settings(self, uri=\"file:///tmp/foobar\", uri_params=None):\n        extra_settings = {}\n        if uri_params:\n            extra_settings[\"FEED_URI_PARAMS\"] = uri_params\n        return {\n            \"FEED_URI\": uri,\n            **extra_settings,\n        }\n\n\nclass URIParamsFeedOptionTest(URIParamsTest, unittest.TestCase):\n    deprecated_options = False\n\n    def build_settings(self, uri=\"file:///tmp/foobar\", uri_params=None):\n        options = {\n            \"format\": \"jl\",\n        }\n        if uri_params:\n            options[\"uri_params\"] = uri_params\n        return {\n            \"FEEDS\": {\n                uri: options,\n            },\n        }\n", "tests/test_pipelines.py": "import asyncio\n\nfrom pytest import mark\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\nfrom twisted.trial import unittest\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\nfrom tests.mockserver import MockServer\n\n\nclass SimplePipeline:\n    def process_item(self, item, spider):\n        item[\"pipeline_passed\"] = True\n        return item\n\n\nclass DeferredPipeline:\n    def cb(self, item):\n        item[\"pipeline_passed\"] = True\n        return item\n\n    def process_item(self, item, spider):\n        d = Deferred()\n        d.addCallback(self.cb)\n        d.callback(item)\n        return d\n\n\nclass AsyncDefPipeline:\n    async def process_item(self, item, spider):\n        d = Deferred()\n        from twisted.internet import reactor\n\n        reactor.callLater(0, d.callback, None)\n        await maybe_deferred_to_future(d)\n        item[\"pipeline_passed\"] = True\n        return item\n\n\nclass AsyncDefAsyncioPipeline:\n    async def process_item(self, item, spider):\n        d = Deferred()\n        from twisted.internet import reactor\n\n        reactor.callLater(0, d.callback, None)\n        await deferred_to_future(d)\n        await asyncio.sleep(0.2)\n        item[\"pipeline_passed\"] = await get_from_asyncio_queue(True)\n        return item\n\n\nclass AsyncDefNotAsyncioPipeline:\n    async def process_item(self, item, spider):\n        d1 = Deferred()\n        from twisted.internet import reactor\n\n        reactor.callLater(0, d1.callback, None)\n        await d1\n        d2 = Deferred()\n        reactor.callLater(0, d2.callback, None)\n        await maybe_deferred_to_future(d2)\n        item[\"pipeline_passed\"] = True\n        return item\n\n\nclass ItemSpider(Spider):\n    name = \"itemspider\"\n\n    def start_requests(self):\n        yield Request(self.mockserver.url(\"/status?n=200\"))\n\n    def parse(self, response):\n        return {\"field\": 42}\n\n\nclass PipelineTestCase(unittest.TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    def _on_item_scraped(self, item):\n        self.assertIsInstance(item, dict)\n        self.assertTrue(item.get(\"pipeline_passed\"))\n        self.items.append(item)\n\n    def _create_crawler(self, pipeline_class):\n        settings = {\n            \"ITEM_PIPELINES\": {pipeline_class: 1},\n        }\n        crawler = get_crawler(ItemSpider, settings)\n        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n        self.items = []\n        return crawler\n\n    @defer.inlineCallbacks\n    def test_simple_pipeline(self):\n        crawler = self._create_crawler(SimplePipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(self.items), 1)\n\n    @defer.inlineCallbacks\n    def test_deferred_pipeline(self):\n        crawler = self._create_crawler(DeferredPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(self.items), 1)\n\n    @defer.inlineCallbacks\n    def test_asyncdef_pipeline(self):\n        crawler = self._create_crawler(AsyncDefPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(self.items), 1)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_asyncdef_asyncio_pipeline(self):\n        crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(self.items), 1)\n\n    @mark.only_not_asyncio()\n    @defer.inlineCallbacks\n    def test_asyncdef_not_asyncio_pipeline(self):\n        crawler = self._create_crawler(AsyncDefNotAsyncioPipeline)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(self.items), 1)\n", "tests/test_downloader_handlers.py": "import contextlib\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom tempfile import mkdtemp, mkstemp\nfrom typing import Optional, Type\nfrom unittest import SkipTest, mock\n\nfrom testfixtures import LogCapture\nfrom twisted.cred import checkers, credentials, portal\nfrom twisted.internet import defer, error, reactor\nfrom twisted.protocols.policies import WrappingFactory\nfrom twisted.trial import unittest\nfrom twisted.web import resource, server, static, util\nfrom twisted.web._newclient import ResponseFailed\nfrom twisted.web.http import _DataLoss\nfrom w3lib.url import path_to_file_uri\n\nfrom scrapy.core.downloader.handlers import DownloadHandlers\nfrom scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler\nfrom scrapy.core.downloader.handlers.file import FileDownloadHandler\nfrom scrapy.core.downloader.handlers.http import HTTPDownloadHandler\nfrom scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\nfrom scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\nfrom scrapy.core.downloader.handlers.s3 import S3DownloadHandler\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Headers, HtmlResponse, Request\nfrom scrapy.http.response.text import TextResponse\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler, skip_if_no_boto\nfrom tests import NON_EXISTING_RESOLVABLE\nfrom tests.mockserver import (\n    Echo,\n    ForeverTakingResource,\n    HostHeaderResource,\n    MockServer,\n    NoLengthResource,\n    PayloadResource,\n    ssl_context_factory,\n)\nfrom tests.spiders import SingleRequestSpider\n\n\nclass DummyDH:\n    lazy = False\n\n\nclass DummyLazyDH:\n    # Default is lazy for backward compatibility\n    pass\n\n\nclass OffDH:\n    lazy = False\n\n    def __init__(self, crawler):\n        raise NotConfigured\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n\nclass LoadTestCase(unittest.TestCase):\n    def test_enabled_handler(self):\n        handlers = {\"scheme\": DummyDH}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        self.assertIn(\"scheme\", dh._schemes)\n        self.assertIn(\"scheme\", dh._handlers)\n        self.assertNotIn(\"scheme\", dh._notconfigured)\n\n    def test_not_configured_handler(self):\n        handlers = {\"scheme\": OffDH}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        self.assertIn(\"scheme\", dh._schemes)\n        self.assertNotIn(\"scheme\", dh._handlers)\n        self.assertIn(\"scheme\", dh._notconfigured)\n\n    def test_disabled_handler(self):\n        handlers = {\"scheme\": None}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        self.assertNotIn(\"scheme\", dh._schemes)\n        for scheme in handlers:  # force load handlers\n            dh._get_handler(scheme)\n        self.assertNotIn(\"scheme\", dh._handlers)\n        self.assertIn(\"scheme\", dh._notconfigured)\n\n    def test_lazy_handlers(self):\n        handlers = {\"scheme\": DummyLazyDH}\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n        dh = DownloadHandlers(crawler)\n        self.assertIn(\"scheme\", dh._schemes)\n        self.assertNotIn(\"scheme\", dh._handlers)\n        for scheme in handlers:  # force load lazy handler\n            dh._get_handler(scheme)\n        self.assertIn(\"scheme\", dh._handlers)\n        self.assertNotIn(\"scheme\", dh._notconfigured)\n\n\nclass FileTestCase(unittest.TestCase):\n    def setUp(self):\n        # add a special char to check that they are handled correctly\n        self.fd, self.tmpname = mkstemp(suffix=\"^\")\n        Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n        handler = build_from_crawler(FileDownloadHandler, get_crawler())\n        self.download_request = handler.download_request\n\n    def tearDown(self):\n        os.close(self.fd)\n        os.remove(self.tmpname)\n\n    def test_download(self):\n        def _test(response):\n            self.assertEqual(response.url, request.url)\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.body, b\"0123456789\")\n            self.assertEqual(response.protocol, None)\n\n        request = Request(path_to_file_uri(self.tmpname))\n        assert request.url.upper().endswith(\"%5E\")\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n    def test_non_existent(self):\n        request = Request(path_to_file_uri(mkdtemp()))\n        d = self.download_request(request, Spider(\"foo\"))\n        return self.assertFailure(d, OSError)\n\n\nclass ContentLengthHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of the Content-Length\n    header from the request.\n    \"\"\"\n\n    def render(self, request):\n        return request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n\n\nclass ChunkedResource(resource.Resource):\n    def render(self, request):\n        def response():\n            request.write(b\"chunked \")\n            request.write(b\"content\\n\")\n            request.finish()\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass BrokenChunkedResource(resource.Resource):\n    def render(self, request):\n        def response():\n            request.write(b\"chunked \")\n            request.write(b\"content\\n\")\n            # Disable terminating chunk on finish.\n            request.chunked = False\n            closeConnection(request)\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass BrokenDownloadResource(resource.Resource):\n    def render(self, request):\n        def response():\n            request.setHeader(b\"Content-Length\", b\"20\")\n            request.write(b\"partial\")\n            closeConnection(request)\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\ndef closeConnection(request):\n    # We have to force a disconnection for HTTP/1.1 clients. Otherwise\n    # client keeps the connection open waiting for more data.\n    if hasattr(request.channel, \"loseConnection\"):  # twisted >=16.3.0\n        request.channel.loseConnection()\n    else:\n        request.channel.transport.loseConnection()\n    request.finish()\n\n\nclass EmptyContentTypeHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of request body\n    without content-type header in response.\n    \"\"\"\n\n    def render(self, request):\n        request.setHeader(\"content-type\", \"\")\n        return request.content.read()\n\n\nclass LargeChunkedFileResource(resource.Resource):\n    def render(self, request):\n        def response():\n            for i in range(1024):\n                request.write(b\"x\" * 1024)\n            request.finish()\n\n        reactor.callLater(0, response)\n        return server.NOT_DONE_YET\n\n\nclass DuplicateHeaderResource(resource.Resource):\n    def render(self, request):\n        request.responseHeaders.setRawHeaders(b\"Set-Cookie\", [b\"a=b\", b\"c=d\"])\n        return b\"\"\n\n\nclass HttpTestCase(unittest.TestCase):\n    scheme = \"http\"\n    download_handler_cls: Type = HTTPDownloadHandler\n\n    # only used for HTTPS tests\n    keyfile = \"keys/localhost.key\"\n    certfile = \"keys/localhost.crt\"\n\n    def setUp(self):\n        self.tmpname = Path(mkdtemp())\n        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n        r = static.File(str(self.tmpname))\n        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n        r.putChild(b\"wait\", ForeverTakingResource())\n        r.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n        r.putChild(b\"nolength\", NoLengthResource())\n        r.putChild(b\"host\", HostHeaderResource())\n        r.putChild(b\"payload\", PayloadResource())\n        r.putChild(b\"broken\", BrokenDownloadResource())\n        r.putChild(b\"chunked\", ChunkedResource())\n        r.putChild(b\"broken-chunked\", BrokenChunkedResource())\n        r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n        r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n        r.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n        r.putChild(b\"duplicate-header\", DuplicateHeaderResource())\n        r.putChild(b\"echo\", Echo())\n        self.site = server.Site(r, timeout=None)\n        self.wrapper = WrappingFactory(self.site)\n        self.host = \"localhost\"\n        if self.scheme == \"https\":\n            # Using WrappingFactory do not enable HTTP/2 failing all the\n            # tests with H2DownloadHandler\n            self.port = reactor.listenSSL(\n                0,\n                self.site,\n                ssl_context_factory(self.keyfile, self.certfile),\n                interface=self.host,\n            )\n        else:\n            self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n        self.portno = self.port.getHost().port\n        self.download_handler = build_from_crawler(\n            self.download_handler_cls, get_crawler()\n        )\n        self.download_request = self.download_handler.download_request\n\n    @defer.inlineCallbacks\n    def tearDown(self):\n        yield self.port.stopListening()\n        if hasattr(self.download_handler, \"close\"):\n            yield self.download_handler.close()\n        shutil.rmtree(self.tmpname)\n\n    def getURL(self, path):\n        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n\n    def test_download(self):\n        request = Request(self.getURL(\"file\"))\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"0123456789\")\n        return d\n\n    def test_download_head(self):\n        request = Request(self.getURL(\"file\"), method=\"HEAD\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"\")\n        return d\n\n    def test_redirect_status(self):\n        request = Request(self.getURL(\"redirect\"))\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.status)\n        d.addCallback(self.assertEqual, 302)\n        return d\n\n    def test_redirect_status_head(self):\n        request = Request(self.getURL(\"redirect\"), method=\"HEAD\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.status)\n        d.addCallback(self.assertEqual, 302)\n        return d\n\n    @defer.inlineCallbacks\n    def test_timeout_download_from_spider_nodata_rcvd(self):\n        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n            # https://twistedmatrix.com/trac/ticket/10279\n            raise unittest.SkipTest(\n                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n            )\n\n        # client connects but no data is received\n        spider = Spider(\"foo\")\n        meta = {\"download_timeout\": 0.5}\n        request = Request(self.getURL(\"wait\"), meta=meta)\n        d = self.download_request(request, spider)\n        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n\n    @defer.inlineCallbacks\n    def test_timeout_download_from_spider_server_hangs(self):\n        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n            # https://twistedmatrix.com/trac/ticket/10279\n            raise unittest.SkipTest(\n                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n            )\n        # client connects, server send headers and some body bytes but hangs\n        spider = Spider(\"foo\")\n        meta = {\"download_timeout\": 0.5}\n        request = Request(self.getURL(\"hang-after-headers\"), meta=meta)\n        d = self.download_request(request, spider)\n        yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n\n    def test_host_header_not_in_request_headers(self):\n        def _test(response):\n            self.assertEqual(response.body, to_bytes(f\"{self.host}:{self.portno}\"))\n            self.assertEqual(request.headers, {})\n\n        request = Request(self.getURL(\"host\"))\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n    def test_host_header_seted_in_request_headers(self):\n        host = self.host + \":\" + str(self.portno)\n\n        def _test(response):\n            self.assertEqual(response.body, host.encode())\n            self.assertEqual(request.headers.get(\"Host\"), host.encode())\n\n        request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"localhost\")\n        return d\n\n    def test_content_length_zero_bodyless_post_request_headers(self):\n        \"\"\"Tests if \"Content-Length: 0\" is sent for bodyless POST requests.\n\n        This is not strictly required by HTTP RFCs but can cause trouble\n        for some web servers.\n        See:\n        https://github.com/scrapy/scrapy/issues/823\n        https://issues.apache.org/jira/browse/TS-2902\n        https://github.com/kennethreitz/requests/issues/405\n        https://bugs.python.org/issue14721\n        \"\"\"\n\n        def _test(response):\n            self.assertEqual(response.body, b\"0\")\n\n        request = Request(self.getURL(\"contentlength\"), method=\"POST\")\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n    def test_content_length_zero_bodyless_post_only_one(self):\n        def _test(response):\n            import json\n\n            headers = Headers(json.loads(response.text)[\"headers\"])\n            contentlengths = headers.getlist(\"Content-Length\")\n            self.assertEqual(len(contentlengths), 1)\n            self.assertEqual(contentlengths, [b\"0\"])\n\n        request = Request(self.getURL(\"echo\"), method=\"POST\")\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n    def test_payload(self):\n        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n        request = Request(self.getURL(\"payload\"), method=\"POST\", body=body)\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, body)\n        return d\n\n    def test_response_header_content_length(self):\n        request = Request(self.getURL(\"file\"), method=b\"GET\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.headers[b\"content-length\"])\n        d.addCallback(self.assertEqual, b\"159\")\n        return d\n\n    def _test_response_class(self, filename, body, response_class):\n        def _test(response):\n            self.assertEqual(type(response), response_class)\n\n        request = Request(self.getURL(filename), body=body)\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n    def test_response_class_from_url(self):\n        return self._test_response_class(\"foo.html\", b\"\", HtmlResponse)\n\n    def test_response_class_from_body(self):\n        return self._test_response_class(\n            \"foo\",\n            b\"<!DOCTYPE html>\\n<title>.</title>\",\n            HtmlResponse,\n        )\n\n    def test_get_duplicate_header(self):\n        def _test(response):\n            self.assertEqual(\n                response.headers.getlist(b\"Set-Cookie\"),\n                [b\"a=b\", b\"c=d\"],\n            )\n\n        request = Request(self.getURL(\"duplicate-header\"))\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n\nclass Http10TestCase(HttpTestCase):\n    \"\"\"HTTP 1.0 test case\"\"\"\n\n    download_handler_cls: Type = HTTP10DownloadHandler\n\n    def test_protocol(self):\n        request = Request(self.getURL(\"host\"), method=\"GET\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.protocol)\n        d.addCallback(self.assertEqual, \"HTTP/1.0\")\n        return d\n\n\nclass Https10TestCase(Http10TestCase):\n    scheme = \"https\"\n\n\nclass Http11TestCase(HttpTestCase):\n    \"\"\"HTTP 1.1 test case\"\"\"\n\n    download_handler_cls: Type = HTTP11DownloadHandler\n\n    def test_download_without_maxsize_limit(self):\n        request = Request(self.getURL(\"file\"))\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"0123456789\")\n        return d\n\n    def test_response_class_choosing_request(self):\n        \"\"\"Tests choosing of correct response type\n        in case of Content-Type is empty but body contains text.\n        \"\"\"\n        body = b\"Some plain text\\ndata with tabs\\t and null bytes\\0\"\n\n        def _test_type(response):\n            self.assertEqual(type(response), TextResponse)\n\n        request = Request(self.getURL(\"nocontenttype\"), body=body)\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(_test_type)\n        return d\n\n    @defer.inlineCallbacks\n    def test_download_with_maxsize(self):\n        request = Request(self.getURL(\"file\"))\n\n        # 10 is minimal size for this request and the limit is only counted on\n        # response body. (regardless of headers)\n        d = self.download_request(request, Spider(\"foo\", download_maxsize=10))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"0123456789\")\n        yield d\n\n        d = self.download_request(request, Spider(\"foo\", download_maxsize=9))\n        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n\n    @defer.inlineCallbacks\n    def test_download_with_maxsize_very_large_file(self):\n        with mock.patch(\"scrapy.core.downloader.handlers.http11.logger\") as logger:\n            request = Request(self.getURL(\"largechunkedfile\"))\n\n            def check(logger):\n                logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n\n            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n\n            # As the error message is logged in the dataReceived callback, we\n            # have to give a bit of time to the reactor to process the queue\n            # after closing the connection.\n            d = defer.Deferred()\n            d.addCallback(check)\n            reactor.callLater(0.1, d.callback, logger)\n            yield d\n\n    @defer.inlineCallbacks\n    def test_download_with_maxsize_per_req(self):\n        meta = {\"download_maxsize\": 2}\n        request = Request(self.getURL(\"file\"), meta=meta)\n        d = self.download_request(request, Spider(\"foo\"))\n        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n\n    @defer.inlineCallbacks\n    def test_download_with_small_maxsize_per_spider(self):\n        request = Request(self.getURL(\"file\"))\n        d = self.download_request(request, Spider(\"foo\", download_maxsize=2))\n        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n\n    def test_download_with_large_maxsize_per_spider(self):\n        request = Request(self.getURL(\"file\"))\n        d = self.download_request(request, Spider(\"foo\", download_maxsize=100))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"0123456789\")\n        return d\n\n    def test_download_chunked_content(self):\n        request = Request(self.getURL(\"chunked\"))\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"chunked content\\n\")\n        return d\n\n    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n        request = Request(self.getURL(url))\n        d = self.download_request(request, Spider(\"foo\"))\n\n        def checkDataLoss(failure):\n            if failure.check(ResponseFailed):\n                if any(r.check(_DataLoss) for r in failure.value.reasons):\n                    return None\n            return failure\n\n        d.addCallback(lambda _: self.fail(\"No DataLoss exception\"))\n        d.addErrback(checkDataLoss)\n        return d\n\n    def test_download_broken_chunked_content_cause_data_loss(self):\n        return self.test_download_broken_content_cause_data_loss(\"broken-chunked\")\n\n    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n        request = Request(self.getURL(url), meta={\"download_fail_on_dataloss\": False})\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.flags)\n        d.addCallback(self.assertEqual, [\"dataloss\"])\n        return d\n\n    def test_download_broken_chunked_content_allow_data_loss(self):\n        return self.test_download_broken_content_allow_data_loss(\"broken-chunked\")\n\n    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n        crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n        request = Request(self.getURL(url))\n        d = download_handler.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.flags)\n        d.addCallback(self.assertEqual, [\"dataloss\"])\n        return d\n\n    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n        return self.test_download_broken_content_allow_data_loss_via_setting(\n            \"broken-chunked\"\n        )\n\n    def test_protocol(self):\n        request = Request(self.getURL(\"host\"), method=\"GET\")\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.protocol)\n        d.addCallback(self.assertEqual, \"HTTP/1.1\")\n        return d\n\n\nclass Https11TestCase(Http11TestCase):\n    scheme = \"https\"\n\n    tls_log_message = (\n        'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=localhost\", '\n        'subject \"/C=IE/O=Scrapy/CN=localhost\"'\n    )\n\n    @defer.inlineCallbacks\n    def test_tls_logging(self):\n        crawler = get_crawler(\n            settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n        )\n        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n        try:\n            with LogCapture() as log_capture:\n                request = Request(self.getURL(\"file\"))\n                d = download_handler.download_request(request, Spider(\"foo\"))\n                d.addCallback(lambda r: r.body)\n                d.addCallback(self.assertEqual, b\"0123456789\")\n                yield d\n                log_capture.check_present(\n                    (\"scrapy.core.downloader.tls\", \"DEBUG\", self.tls_log_message)\n                )\n        finally:\n            yield download_handler.close()\n\n\nclass Https11WrongHostnameTestCase(Http11TestCase):\n    scheme = \"https\"\n\n    # above tests use a server certificate for \"localhost\",\n    # client connection to \"localhost\" too.\n    # here we test that even if the server certificate is for another domain,\n    # \"www.example.com\" in this case,\n    # the tests still pass\n    keyfile = \"keys/example-com.key.pem\"\n    certfile = \"keys/example-com.cert.pem\"\n\n\nclass Https11InvalidDNSId(Https11TestCase):\n    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n\n    def setUp(self):\n        super().setUp()\n        self.host = \"127.0.0.1\"\n\n\nclass Https11InvalidDNSPattern(Https11TestCase):\n    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n\n    keyfile = \"keys/localhost.ip.key\"\n    certfile = \"keys/localhost.ip.crt\"\n\n    def setUp(self):\n        try:\n            from service_identity.exceptions import CertificateError  # noqa: F401\n        except ImportError:\n            raise unittest.SkipTest(\"cryptography lib is too old\")\n        self.tls_log_message = (\n            'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", '\n            'subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n        )\n        super().setUp()\n\n\nclass Https11CustomCiphers(unittest.TestCase):\n    scheme = \"https\"\n    download_handler_cls: Type = HTTP11DownloadHandler\n\n    keyfile = \"keys/localhost.key\"\n    certfile = \"keys/localhost.crt\"\n\n    def setUp(self):\n        self.tmpname = Path(mkdtemp())\n        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n        r = static.File(str(self.tmpname))\n        self.site = server.Site(r, timeout=None)\n        self.host = \"localhost\"\n        self.port = reactor.listenSSL(\n            0,\n            self.site,\n            ssl_context_factory(\n                self.keyfile, self.certfile, cipher_string=\"CAMELLIA256-SHA\"\n            ),\n            interface=self.host,\n        )\n        self.portno = self.port.getHost().port\n        crawler = get_crawler(\n            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"CAMELLIA256-SHA\"}\n        )\n        self.download_handler = build_from_crawler(self.download_handler_cls, crawler)\n        self.download_request = self.download_handler.download_request\n\n    @defer.inlineCallbacks\n    def tearDown(self):\n        yield self.port.stopListening()\n        if hasattr(self.download_handler, \"close\"):\n            yield self.download_handler.close()\n        shutil.rmtree(self.tmpname)\n\n    def getURL(self, path):\n        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n\n    def test_download(self):\n        request = Request(self.getURL(\"file\"))\n        d = self.download_request(request, Spider(\"foo\"))\n        d.addCallback(lambda r: r.body)\n        d.addCallback(self.assertEqual, b\"0123456789\")\n        return d\n\n\nclass Http11MockServerTestCase(unittest.TestCase):\n    \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n\n    settings_dict: Optional[dict] = None\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_download_with_content_length(self):\n        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n        # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n        # download it\n        yield crawler.crawl(\n            seed=Request(\n                url=self.mockserver.url(\"/partial\"), meta={\"download_maxsize\": 1000}\n            )\n        )\n        failure = crawler.spider.meta[\"failure\"]\n        self.assertIsInstance(failure.value, defer.CancelledError)\n\n    @defer.inlineCallbacks\n    def test_download(self):\n        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n        yield crawler.crawl(seed=Request(url=self.mockserver.url(\"\")))\n        failure = crawler.spider.meta.get(\"failure\")\n        self.assertTrue(failure is None)\n        reason = crawler.spider.meta[\"close_reason\"]\n        self.assertTrue(reason, \"finished\")\n\n\nclass UriResource(resource.Resource):\n    \"\"\"Return the full uri that was requested\"\"\"\n\n    def getChild(self, path, request):\n        return self\n\n    def render(self, request):\n        # Note: this is an ugly hack for CONNECT request timeout test.\n        #       Returning some data here fail SSL/TLS handshake\n        # ToDo: implement proper HTTPS proxy tests, not faking them.\n        if request.method != b\"CONNECT\":\n            return request.uri\n        return b\"\"\n\n\nclass HttpProxyTestCase(unittest.TestCase):\n    download_handler_cls: Type = HTTPDownloadHandler\n    expected_http_proxy_request_body = b\"http://example.com\"\n\n    def setUp(self):\n        site = server.Site(UriResource(), timeout=None)\n        wrapper = WrappingFactory(site)\n        self.port = reactor.listenTCP(0, wrapper, interface=\"127.0.0.1\")\n        self.portno = self.port.getHost().port\n        self.download_handler = build_from_crawler(\n            self.download_handler_cls, get_crawler()\n        )\n        self.download_request = self.download_handler.download_request\n\n    @defer.inlineCallbacks\n    def tearDown(self):\n        yield self.port.stopListening()\n        if hasattr(self.download_handler, \"close\"):\n            yield self.download_handler.close()\n\n    def getURL(self, path):\n        return f\"http://127.0.0.1:{self.portno}/{path}\"\n\n    def test_download_with_proxy(self):\n        def _test(response):\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.url, request.url)\n            self.assertEqual(response.body, self.expected_http_proxy_request_body)\n\n        http_proxy = self.getURL(\"\")\n        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n    def test_download_without_proxy(self):\n        def _test(response):\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.url, request.url)\n            self.assertEqual(response.body, b\"/path/to/resource\")\n\n        request = Request(self.getURL(\"path/to/resource\"))\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n\nclass Http10ProxyTestCase(HttpProxyTestCase):\n    download_handler_cls: Type = HTTP10DownloadHandler\n\n    def test_download_with_proxy_https_noconnect(self):\n        raise unittest.SkipTest(\"noconnect is not supported in HTTP10DownloadHandler\")\n\n\nclass Http11ProxyTestCase(HttpProxyTestCase):\n    download_handler_cls: Type = HTTP11DownloadHandler\n\n    @defer.inlineCallbacks\n    def test_download_with_proxy_https_timeout(self):\n        \"\"\"Test TunnelingTCP4ClientEndpoint\"\"\"\n        if NON_EXISTING_RESOLVABLE:\n            raise SkipTest(\"Non-existing hosts are resolvable\")\n        http_proxy = self.getURL(\"\")\n        domain = \"https://no-such-domain.nosuch\"\n        request = Request(domain, meta={\"proxy\": http_proxy, \"download_timeout\": 0.2})\n        d = self.download_request(request, Spider(\"foo\"))\n        timeout = yield self.assertFailure(d, error.TimeoutError)\n        self.assertIn(domain, timeout.osError)\n\n    def test_download_with_proxy_without_http_scheme(self):\n        def _test(response):\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.url, request.url)\n            self.assertEqual(response.body, self.expected_http_proxy_request_body)\n\n        http_proxy = self.getURL(\"\").replace(\"http://\", \"\")\n        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n\n\nclass HttpDownloadHandlerMock:\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def download_request(self, request, spider):\n        return request\n\n\nclass S3AnonTestCase(unittest.TestCase):\n    def setUp(self):\n        skip_if_no_boto()\n        crawler = get_crawler()\n        self.s3reqh = build_from_crawler(\n            S3DownloadHandler,\n            crawler,\n            httpdownloadhandler=HttpDownloadHandlerMock,\n            # anon=True, # implicit\n        )\n        self.download_request = self.s3reqh.download_request\n        self.spider = Spider(\"foo\")\n\n    def test_anon_request(self):\n        req = Request(\"s3://aws-publicdatasets/\")\n        httpreq = self.download_request(req, self.spider)\n        self.assertEqual(hasattr(self.s3reqh, \"anon\"), True)\n        self.assertEqual(self.s3reqh.anon, True)\n        self.assertEqual(httpreq.url, \"http://aws-publicdatasets.s3.amazonaws.com/\")\n\n\nclass S3TestCase(unittest.TestCase):\n    download_handler_cls: Type = S3DownloadHandler\n\n    # test use same example keys than amazon developer guide\n    # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n    # and the tests described here are the examples from that manual\n\n    AWS_ACCESS_KEY_ID = \"0PN5J17HBGZHT7JJ3X82\"\n    AWS_SECRET_ACCESS_KEY = \"uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o\"\n\n    def setUp(self):\n        skip_if_no_boto()\n        crawler = get_crawler()\n        s3reqh = build_from_crawler(\n            S3DownloadHandler,\n            crawler,\n            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n            httpdownloadhandler=HttpDownloadHandlerMock,\n        )\n        self.download_request = s3reqh.download_request\n        self.spider = Spider(\"foo\")\n\n    @contextlib.contextmanager\n    def _mocked_date(self, date):\n        try:\n            import botocore.auth  # noqa: F401\n        except ImportError:\n            yield\n        else:\n            # We need to mock botocore.auth.formatdate, because otherwise\n            # botocore overrides Date header with current date and time\n            # and Authorization header is different each time\n            with mock.patch(\"botocore.auth.formatdate\") as mock_formatdate:\n                mock_formatdate.return_value = date\n                yield\n\n    def test_extra_kw(self):\n        try:\n            crawler = get_crawler()\n            build_from_crawler(\n                S3DownloadHandler,\n                crawler,\n                extra_kw=True,\n            )\n        except Exception as e:\n            self.assertIsInstance(e, (TypeError, NotConfigured))\n        else:\n            raise AssertionError()\n\n    def test_request_signing1(self):\n        # gets an object from the johnsmith bucket.\n        date = \"Tue, 27 Mar 2007 19:36:42 +0000\"\n        req = Request(\"s3://johnsmith/photos/puppy.jpg\", headers={\"Date\": date})\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        self.assertEqual(\n            httpreq.headers[\"Authorization\"],\n            b\"AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=\",\n        )\n\n    def test_request_signing2(self):\n        # puts an object into the johnsmith bucket.\n        date = \"Tue, 27 Mar 2007 21:15:45 +0000\"\n        req = Request(\n            \"s3://johnsmith/photos/puppy.jpg\",\n            method=\"PUT\",\n            headers={\n                \"Content-Type\": \"image/jpeg\",\n                \"Date\": date,\n                \"Content-Length\": \"94328\",\n            },\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        self.assertEqual(\n            httpreq.headers[\"Authorization\"],\n            b\"AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=\",\n        )\n\n    def test_request_signing3(self):\n        # lists the content of the johnsmith bucket.\n        date = \"Tue, 27 Mar 2007 19:42:41 +0000\"\n        req = Request(\n            \"s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy\",\n            method=\"GET\",\n            headers={\n                \"User-Agent\": \"Mozilla/5.0\",\n                \"Date\": date,\n            },\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        self.assertEqual(\n            httpreq.headers[\"Authorization\"],\n            b\"AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=\",\n        )\n\n    def test_request_signing4(self):\n        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n        date = \"Tue, 27 Mar 2007 19:44:46 +0000\"\n        req = Request(\"s3://johnsmith/?acl\", method=\"GET\", headers={\"Date\": date})\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        self.assertEqual(\n            httpreq.headers[\"Authorization\"],\n            b\"AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=\",\n        )\n\n    def test_request_signing6(self):\n        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n        date = \"Tue, 27 Mar 2007 21:06:08 +0000\"\n        req = Request(\n            \"s3://static.johnsmith.net:8080/db-backup.dat.gz\",\n            method=\"PUT\",\n            headers={\n                \"User-Agent\": \"curl/7.15.5\",\n                \"Host\": \"static.johnsmith.net:8080\",\n                \"Date\": date,\n                \"x-amz-acl\": \"public-read\",\n                \"content-type\": \"application/x-download\",\n                \"Content-MD5\": \"4gJE4saaMU4BqNR0kLY+lw==\",\n                \"X-Amz-Meta-ReviewedBy\": \"joe@johnsmith.net,jane@johnsmith.net\",\n                \"X-Amz-Meta-FileChecksum\": \"0x02661779\",\n                \"X-Amz-Meta-ChecksumAlgorithm\": \"crc32\",\n                \"Content-Disposition\": \"attachment; filename=database.dat\",\n                \"Content-Encoding\": \"gzip\",\n                \"Content-Length\": \"5913339\",\n            },\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        self.assertEqual(\n            httpreq.headers[\"Authorization\"],\n            b\"AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=\",\n        )\n\n    def test_request_signing7(self):\n        # ensure that spaces are quoted properly before signing\n        date = \"Tue, 27 Mar 2007 19:42:41 +0000\"\n        req = Request(\n            \"s3://johnsmith/photos/my puppy.jpg?response-content-disposition=my puppy.jpg\",\n            method=\"GET\",\n            headers={\"Date\": date},\n        )\n        with self._mocked_date(date):\n            httpreq = self.download_request(req, self.spider)\n        self.assertEqual(\n            httpreq.headers[\"Authorization\"],\n            b\"AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=\",\n        )\n\n\nclass BaseFTPTestCase(unittest.TestCase):\n    username = \"scrapy\"\n    password = \"passwd\"\n    req_meta = {\"ftp_user\": username, \"ftp_password\": password}\n\n    test_files = (\n        (\"file.txt\", b\"I have the power!\"),\n        (\"file with spaces.txt\", b\"Moooooooooo power!\"),\n        (\"html-file-without-extension\", b\"<!DOCTYPE html>\\n<title>.</title>\"),\n    )\n\n    def setUp(self):\n        from twisted.protocols.ftp import FTPFactory, FTPRealm\n\n        from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n\n        # setup dirs and test file\n        self.directory = Path(mkdtemp())\n        userdir = self.directory / self.username\n        userdir.mkdir()\n        for filename, content in self.test_files:\n            (userdir / filename).write_bytes(content)\n\n        # setup server\n        realm = FTPRealm(\n            anonymousRoot=str(self.directory), userHome=str(self.directory)\n        )\n        p = portal.Portal(realm)\n        users_checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()\n        users_checker.addUser(self.username, self.password)\n        p.registerChecker(users_checker, credentials.IUsernamePassword)\n        self.factory = FTPFactory(portal=p)\n        self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n        self.portNum = self.port.getHost().port\n        crawler = get_crawler()\n        self.download_handler = build_from_crawler(FTPDownloadHandler, crawler)\n        self.addCleanup(self.port.stopListening)\n\n    def tearDown(self):\n        shutil.rmtree(self.directory)\n\n    def _add_test_callbacks(self, deferred, callback=None, errback=None):\n        def _clean(data):\n            self.download_handler.client.transport.loseConnection()\n            return data\n\n        deferred.addCallback(_clean)\n        if callback:\n            deferred.addCallback(callback)\n        if errback:\n            deferred.addErrback(errback)\n        return deferred\n\n    def test_ftp_download_success(self):\n        request = Request(\n            url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\", meta=self.req_meta\n        )\n        d = self.download_handler.download_request(request, None)\n\n        def _test(r):\n            self.assertEqual(r.status, 200)\n            self.assertEqual(r.body, b\"I have the power!\")\n            self.assertEqual(r.headers, {b\"Local Filename\": [b\"\"], b\"Size\": [b\"17\"]})\n            self.assertIsNone(r.protocol)\n\n        return self._add_test_callbacks(d, _test)\n\n    def test_ftp_download_path_with_spaces(self):\n        request = Request(\n            url=f\"ftp://127.0.0.1:{self.portNum}/file with spaces.txt\",\n            meta=self.req_meta,\n        )\n        d = self.download_handler.download_request(request, None)\n\n        def _test(r):\n            self.assertEqual(r.status, 200)\n            self.assertEqual(r.body, b\"Moooooooooo power!\")\n            self.assertEqual(r.headers, {b\"Local Filename\": [b\"\"], b\"Size\": [b\"18\"]})\n\n        return self._add_test_callbacks(d, _test)\n\n    def test_ftp_download_nonexistent(self):\n        request = Request(\n            url=f\"ftp://127.0.0.1:{self.portNum}/nonexistent.txt\", meta=self.req_meta\n        )\n        d = self.download_handler.download_request(request, None)\n\n        def _test(r):\n            self.assertEqual(r.status, 404)\n\n        return self._add_test_callbacks(d, _test)\n\n    def test_ftp_local_filename(self):\n        f, local_fname = mkstemp()\n        fname_bytes = to_bytes(local_fname)\n        local_fname = Path(local_fname)\n        os.close(f)\n        meta = {\"ftp_local_filename\": fname_bytes}\n        meta.update(self.req_meta)\n        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\", meta=meta)\n        d = self.download_handler.download_request(request, None)\n\n        def _test(r):\n            self.assertEqual(r.body, fname_bytes)\n            self.assertEqual(\n                r.headers, {b\"Local Filename\": [fname_bytes], b\"Size\": [b\"17\"]}\n            )\n            self.assertTrue(local_fname.exists())\n            self.assertEqual(local_fname.read_bytes(), b\"I have the power!\")\n            local_fname.unlink()\n\n        return self._add_test_callbacks(d, _test)\n\n    def _test_response_class(self, filename, response_class):\n        f, local_fname = mkstemp()\n        local_fname = Path(local_fname)\n        os.close(f)\n        meta = {}\n        meta.update(self.req_meta)\n        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/{filename}\", meta=meta)\n        d = self.download_handler.download_request(request, None)\n\n        def _test(r):\n            self.assertEqual(type(r), response_class)\n            local_fname.unlink()\n\n        return self._add_test_callbacks(d, _test)\n\n    def test_response_class_from_url(self):\n        return self._test_response_class(\"file.txt\", TextResponse)\n\n    def test_response_class_from_body(self):\n        return self._test_response_class(\"html-file-without-extension\", HtmlResponse)\n\n\nclass FTPTestCase(BaseFTPTestCase):\n    def test_invalid_credentials(self):\n        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n            raise unittest.SkipTest(\n                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n            )\n        from twisted.protocols.ftp import ConnectionLost\n\n        meta = dict(self.req_meta)\n        meta.update({\"ftp_password\": \"invalid\"})\n        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\", meta=meta)\n        d = self.download_handler.download_request(request, None)\n\n        def _test(r):\n            self.assertEqual(r.type, ConnectionLost)\n\n        return self._add_test_callbacks(d, errback=_test)\n\n\nclass AnonymousFTPTestCase(BaseFTPTestCase):\n    username = \"anonymous\"\n    req_meta = {}\n\n    def setUp(self):\n        from twisted.protocols.ftp import FTPFactory, FTPRealm\n\n        from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n\n        # setup dir and test file\n        self.directory = Path(mkdtemp())\n        for filename, content in self.test_files:\n            (self.directory / filename).write_bytes(content)\n\n        # setup server for anonymous access\n        realm = FTPRealm(anonymousRoot=str(self.directory))\n        p = portal.Portal(realm)\n        p.registerChecker(checkers.AllowAnonymousAccess(), credentials.IAnonymous)\n\n        self.factory = FTPFactory(portal=p, userAnonymous=self.username)\n        self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n        self.portNum = self.port.getHost().port\n        crawler = get_crawler()\n        self.download_handler = build_from_crawler(FTPDownloadHandler, crawler)\n        self.addCleanup(self.port.stopListening)\n\n    def tearDown(self):\n        shutil.rmtree(self.directory)\n\n\nclass DataURITestCase(unittest.TestCase):\n    def setUp(self):\n        crawler = get_crawler()\n        self.download_handler = build_from_crawler(DataURIDownloadHandler, crawler)\n        self.download_request = self.download_handler.download_request\n        self.spider = Spider(\"foo\")\n\n    def test_response_attrs(self):\n        uri = \"data:,A%20brief%20note\"\n\n        def _test(response):\n            self.assertEqual(response.url, uri)\n            self.assertFalse(response.headers)\n\n        request = Request(uri)\n        return self.download_request(request, self.spider).addCallback(_test)\n\n    def test_default_mediatype_encoding(self):\n        def _test(response):\n            self.assertEqual(response.text, \"A brief note\")\n            self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n            self.assertEqual(response.encoding, \"US-ASCII\")\n\n        request = Request(\"data:,A%20brief%20note\")\n        return self.download_request(request, self.spider).addCallback(_test)\n\n    def test_default_mediatype(self):\n        def _test(response):\n            self.assertEqual(response.text, \"\\u038e\\u03a3\\u038e\")\n            self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n            self.assertEqual(response.encoding, \"iso-8859-7\")\n\n        request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n        return self.download_request(request, self.spider).addCallback(_test)\n\n    def test_text_charset(self):\n        def _test(response):\n            self.assertEqual(response.text, \"\\u038e\\u03a3\\u038e\")\n            self.assertEqual(response.body, b\"\\xbe\\xd3\\xbe\")\n            self.assertEqual(response.encoding, \"iso-8859-7\")\n\n        request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n        return self.download_request(request, self.spider).addCallback(_test)\n\n    def test_mediatype_parameters(self):\n        def _test(response):\n            self.assertEqual(response.text, \"\\u038e\\u03a3\\u038e\")\n            self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n            self.assertEqual(response.encoding, \"utf-8\")\n\n        request = Request(\n            \"data:text/plain;foo=%22foo;bar%5C%22%22;\"\n            \"charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22\"\n            \",%CE%8E%CE%A3%CE%8E\"\n        )\n        return self.download_request(request, self.spider).addCallback(_test)\n\n    def test_base64(self):\n        def _test(response):\n            self.assertEqual(response.text, \"Hello, world.\")\n\n        request = Request(\"data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D\")\n        return self.download_request(request, self.spider).addCallback(_test)\n\n    def test_protocol(self):\n        def _test(response):\n            self.assertIsNone(response.protocol)\n\n        request = Request(\"data:,\")\n        return self.download_request(request, self.spider).addCallback(_test)\n", "tests/test_command_check.py": "import sys\nfrom io import StringIO\nfrom unittest.mock import Mock, PropertyMock, call, patch\n\nfrom scrapy.commands.check import Command, TextTestResult\nfrom tests.test_commands import CommandTest\n\n\nclass CheckCommandTest(CommandTest):\n    command = \"check\"\n\n    def setUp(self):\n        super().setUp()\n        self.spider_name = \"check_spider\"\n        self.spider = (self.proj_mod_path / \"spiders\" / \"checkspider.py\").resolve()\n\n    def _write_contract(self, contracts, parse_def):\n        self.spider.write_text(\n            f\"\"\"\nimport scrapy\n\nclass CheckSpider(scrapy.Spider):\n    name = '{self.spider_name}'\n    start_urls = ['data:,']\n\n    def parse(self, response, **cb_kwargs):\n        \\\"\\\"\\\"\n        @url data:,\n        {contracts}\n        \\\"\\\"\\\"\n        {parse_def}\n        \"\"\",\n            encoding=\"utf-8\",\n        )\n\n    def _test_contract(self, contracts=\"\", parse_def=\"pass\"):\n        self._write_contract(contracts, parse_def)\n        p, out, err = self.proc(\"check\")\n        self.assertNotIn(\"F\", out)\n        self.assertIn(\"OK\", err)\n        self.assertEqual(p.returncode, 0)\n\n    def test_check_returns_requests_contract(self):\n        contracts = \"\"\"\n        @returns requests 1\n        \"\"\"\n        parse_def = \"\"\"\n        yield scrapy.Request(url='http://next-url.com')\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_returns_items_contract(self):\n        contracts = \"\"\"\n        @returns items 1\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_cb_kwargs_contract(self):\n        contracts = \"\"\"\n        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n        \"\"\"\n        parse_def = \"\"\"\n        if len(cb_kwargs.items()) == 0:\n            raise Exception(\"Callback args not set\")\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_scrapes_contract(self):\n        contracts = \"\"\"\n        @scrapes key1 key2\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_all_default_contracts(self):\n        contracts = \"\"\"\n        @returns items 1\n        @returns requests 1\n        @scrapes key1 key2\n        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        yield scrapy.Request(url='http://next-url.com')\n        if len(cb_kwargs.items()) == 0:\n            raise Exception(\"Callback args not set\")\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_SCRAPY_CHECK_set(self):\n        parse_def = \"\"\"\n        import os\n        if not os.environ.get('SCRAPY_CHECK'):\n            raise Exception('SCRAPY_CHECK not set')\n        \"\"\"\n        self._test_contract(parse_def=parse_def)\n\n    def test_printSummary_with_unsuccessful_test_result_without_errors_and_without_failures(\n        self,\n    ):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = []\n        result.errors = []\n        result.unexpectedSuccesses = [\"a\", \"b\"]\n        with patch.object(result.stream, \"write\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_has_calls([call(\"FAILED\"), call(\"\\n\")])\n\n    def test_printSummary_with_unsuccessful_test_result_with_only_failures(self):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = [(self, \"failure\")]\n        result.errors = []\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (failures=1)\")\n\n    def test_printSummary_with_unsuccessful_test_result_with_only_errors(self):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = []\n        result.errors = [(self, \"error\")]\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (errors=1)\")\n\n    def test_printSummary_with_unsuccessful_test_result_with_both_failures_and_errors(\n        self,\n    ):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = [(self, \"failure\")]\n        result.errors = [(self, \"error\")]\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (failures=1, errors=1)\")\n\n    @patch(\"scrapy.commands.check.ContractsManager\")\n    def test_run_with_opts_list_prints_spider(self, cm_cls_mock):\n        output = StringIO()\n        sys.stdout = output\n        cmd = Command()\n        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n        cm_cls_mock.return_value = cm_mock = Mock()\n        spider_loader_mock = Mock()\n        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n        spider_name = \"FakeSpider\"\n        spider_cls_mock = Mock()\n        type(spider_cls_mock).name = PropertyMock(return_value=spider_name)\n        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n            x\n        ]\n        tested_methods = [\"fakeMethod1\", \"fakeMethod2\"]\n        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n            spider_cls_mock: tested_methods\n        }[x]\n\n        cmd.run([spider_name], Mock(list=True))\n\n        self.assertEqual(\n            \"FakeSpider\\n  * fakeMethod1\\n  * fakeMethod2\\n\", output.getvalue()\n        )\n        sys.stdout = sys.__stdout__\n\n    @patch(\"scrapy.commands.check.ContractsManager\")\n    def test_run_without_opts_list_does_not_crawl_spider_with_no_tested_methods(\n        self, cm_cls_mock\n    ):\n        cmd = Command()\n        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n        cm_cls_mock.return_value = cm_mock = Mock()\n        spider_loader_mock = Mock()\n        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n        spider_name = \"FakeSpider\"\n        spider_cls_mock = Mock()\n        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n            x\n        ]\n        tested_methods = []\n        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n            spider_cls_mock: tested_methods\n        }[x]\n\n        cmd.run([spider_name], Mock(list=False))\n\n        cmd.crawler_process.crawl.assert_not_called()\n", "tests/test_pqueues.py": "import tempfile\nimport unittest\n\nimport queuelib\n\nfrom scrapy.http.request import Request\nfrom scrapy.pqueues import DownloaderAwarePriorityQueue, ScrapyPriorityQueue\nfrom scrapy.spiders import Spider\nfrom scrapy.squeues import FifoMemoryQueue\nfrom scrapy.utils.test import get_crawler\nfrom tests.test_scheduler import MockDownloader, MockEngine\n\n\nclass PriorityQueueTest(unittest.TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"foo\")\n\n    def test_queue_push_pop_one(self):\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir\n        )\n        self.assertIsNone(queue.pop())\n        self.assertEqual(len(queue), 0)\n        req1 = Request(\"https://example.org/1\", priority=1)\n        queue.push(req1)\n        self.assertEqual(len(queue), 1)\n        dequeued = queue.pop()\n        self.assertEqual(len(queue), 0)\n        self.assertEqual(dequeued.url, req1.url)\n        self.assertEqual(dequeued.priority, req1.priority)\n        self.assertEqual(queue.close(), [])\n\n    def test_no_peek_raises(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir\n        )\n        queue.push(Request(\"https://example.org\"))\n        with self.assertRaises(\n            NotImplementedError,\n            msg=\"The underlying queue class does not implement 'peek'\",\n        ):\n            queue.peek()\n        queue.close()\n\n    def test_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir\n        )\n        self.assertEqual(len(queue), 0)\n        self.assertIsNone(queue.peek())\n        req1 = Request(\"https://example.org/1\")\n        req2 = Request(\"https://example.org/2\")\n        req3 = Request(\"https://example.org/3\")\n        queue.push(req1)\n        queue.push(req2)\n        queue.push(req3)\n        self.assertEqual(len(queue), 3)\n        self.assertEqual(queue.peek().url, req1.url)\n        self.assertEqual(queue.pop().url, req1.url)\n        self.assertEqual(len(queue), 2)\n        self.assertEqual(queue.peek().url, req2.url)\n        self.assertEqual(queue.pop().url, req2.url)\n        self.assertEqual(len(queue), 1)\n        self.assertEqual(queue.peek().url, req3.url)\n        self.assertEqual(queue.pop().url, req3.url)\n        self.assertEqual(queue.close(), [])\n\n    def test_queue_push_pop_priorities(self):\n        temp_dir = tempfile.mkdtemp()\n        queue = ScrapyPriorityQueue.from_crawler(\n            self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3]\n        )\n        self.assertIsNone(queue.pop())\n        self.assertEqual(len(queue), 0)\n        req1 = Request(\"https://example.org/1\", priority=1)\n        req2 = Request(\"https://example.org/2\", priority=2)\n        req3 = Request(\"https://example.org/3\", priority=3)\n        queue.push(req1)\n        queue.push(req2)\n        queue.push(req3)\n        self.assertEqual(len(queue), 3)\n        dequeued = queue.pop()\n        self.assertEqual(len(queue), 2)\n        self.assertEqual(dequeued.url, req3.url)\n        self.assertEqual(dequeued.priority, req3.priority)\n        self.assertEqual(queue.close(), [-1, -2])\n\n\nclass DownloaderAwarePriorityQueueTest(unittest.TestCase):\n    def setUp(self):\n        crawler = get_crawler(Spider)\n        crawler.engine = MockEngine(downloader=MockDownloader())\n        self.queue = DownloaderAwarePriorityQueue.from_crawler(\n            crawler=crawler,\n            downstream_queue_cls=FifoMemoryQueue,\n            key=\"foo/bar\",\n        )\n\n    def tearDown(self):\n        self.queue.close()\n\n    def test_push_pop(self):\n        self.assertEqual(len(self.queue), 0)\n        self.assertIsNone(self.queue.pop())\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        self.queue.push(req1)\n        self.queue.push(req2)\n        self.queue.push(req3)\n        self.assertEqual(len(self.queue), 3)\n        self.assertEqual(self.queue.pop().url, req1.url)\n        self.assertEqual(len(self.queue), 2)\n        self.assertEqual(self.queue.pop().url, req2.url)\n        self.assertEqual(len(self.queue), 1)\n        self.assertEqual(self.queue.pop().url, req3.url)\n        self.assertEqual(len(self.queue), 0)\n        self.assertIsNone(self.queue.pop())\n\n    def test_no_peek_raises(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n        self.queue.push(Request(\"https://example.org\"))\n        with self.assertRaises(\n            NotImplementedError,\n            msg=\"The underlying queue class does not implement 'peek'\",\n        ):\n            self.queue.peek()\n\n    def test_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n        self.assertEqual(len(self.queue), 0)\n        req1 = Request(\"https://example.org/1\")\n        req2 = Request(\"https://example.org/2\")\n        req3 = Request(\"https://example.org/3\")\n        self.queue.push(req1)\n        self.queue.push(req2)\n        self.queue.push(req3)\n        self.assertEqual(len(self.queue), 3)\n        self.assertEqual(self.queue.peek().url, req1.url)\n        self.assertEqual(self.queue.pop().url, req1.url)\n        self.assertEqual(len(self.queue), 2)\n        self.assertEqual(self.queue.peek().url, req2.url)\n        self.assertEqual(self.queue.pop().url, req2.url)\n        self.assertEqual(len(self.queue), 1)\n        self.assertEqual(self.queue.peek().url, req3.url)\n        self.assertEqual(self.queue.pop().url, req3.url)\n        self.assertIsNone(self.queue.peek())\n", "tests/test_link.py": "import unittest\n\nfrom scrapy.link import Link\n\n\nclass LinkTest(unittest.TestCase):\n    def _assert_same_links(self, link1, link2):\n        self.assertEqual(link1, link2)\n        self.assertEqual(hash(link1), hash(link2))\n\n    def _assert_different_links(self, link1, link2):\n        self.assertNotEqual(link1, link2)\n        self.assertNotEqual(hash(link1), hash(link2))\n\n    def test_eq_and_hash(self):\n        l1 = Link(\"http://www.example.com\")\n        l2 = Link(\"http://www.example.com/other\")\n        l3 = Link(\"http://www.example.com\")\n\n        self._assert_same_links(l1, l1)\n        self._assert_different_links(l1, l2)\n        self._assert_same_links(l1, l3)\n\n        l4 = Link(\"http://www.example.com\", text=\"test\")\n        l5 = Link(\"http://www.example.com\", text=\"test2\")\n        l6 = Link(\"http://www.example.com\", text=\"test\")\n\n        self._assert_same_links(l4, l4)\n        self._assert_different_links(l4, l5)\n        self._assert_same_links(l4, l6)\n\n        l7 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n        )\n        l8 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n        )\n        l9 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n        )\n        l10 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"other\", nofollow=False\n        )\n        self._assert_same_links(l7, l8)\n        self._assert_different_links(l7, l9)\n        self._assert_different_links(l7, l10)\n\n    def test_repr(self):\n        l1 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n        )\n        l2 = eval(repr(l1))\n        self._assert_same_links(l1, l2)\n\n    def test_bytes_url(self):\n        with self.assertRaises(TypeError):\n            Link(b\"http://www.example.com/\\xc2\\xa3\")\n", "tests/test_utils_python.py": "import functools\nimport operator\nimport platform\n\nfrom twisted.trial import unittest\n\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.defer import aiter_errback, deferred_f_from_coro_f\nfrom scrapy.utils.python import (\n    MutableAsyncChain,\n    MutableChain,\n    binary_is_text,\n    equal_attributes,\n    get_func_args,\n    memoizemethod_noargs,\n    to_bytes,\n    to_unicode,\n    without_none_values,\n)\n\n__doctests__ = [\"scrapy.utils.python\"]\n\n\nclass MutableChainTest(unittest.TestCase):\n    def test_mutablechain(self):\n        m = MutableChain(range(2), [2, 3], (4, 5))\n        m.extend(range(6, 7))\n        m.extend([7, 8])\n        m.extend([9, 10], (11, 12))\n        self.assertEqual(next(m), 0)\n        self.assertEqual(m.__next__(), 1)\n        self.assertEqual(list(m), list(range(2, 13)))\n\n\nclass MutableAsyncChainTest(unittest.TestCase):\n    @staticmethod\n    async def g1():\n        for i in range(3):\n            yield i\n\n    @staticmethod\n    async def g2():\n        return\n        yield\n\n    @staticmethod\n    async def g3():\n        for i in range(7, 10):\n            yield i\n\n    @staticmethod\n    async def g4():\n        for i in range(3, 5):\n            yield i\n        1 / 0\n        for i in range(5, 7):\n            yield i\n\n    @staticmethod\n    async def collect_asyncgen_exc(asyncgen):\n        results = []\n        async for x in asyncgen:\n            results.append(x)\n        return results\n\n    @deferred_f_from_coro_f\n    async def test_mutableasyncchain(self):\n        m = MutableAsyncChain(self.g1(), as_async_generator(range(3, 7)))\n        m.extend(self.g2())\n        m.extend(self.g3())\n\n        self.assertEqual(await m.__anext__(), 0)\n        results = await collect_asyncgen(m)\n        self.assertEqual(results, list(range(1, 10)))\n\n    @deferred_f_from_coro_f\n    async def test_mutableasyncchain_exc(self):\n        m = MutableAsyncChain(self.g1())\n        m.extend(self.g4())\n        m.extend(self.g3())\n\n        results = await collect_asyncgen(aiter_errback(m, lambda _: None))\n        self.assertEqual(results, list(range(5)))\n\n\nclass ToUnicodeTest(unittest.TestCase):\n    def test_converting_an_utf8_encoded_string_to_unicode(self):\n        self.assertEqual(to_unicode(b\"lel\\xc3\\xb1e\"), \"lel\\xf1e\")\n\n    def test_converting_a_latin_1_encoded_string_to_unicode(self):\n        self.assertEqual(to_unicode(b\"lel\\xf1e\", \"latin-1\"), \"lel\\xf1e\")\n\n    def test_converting_a_unicode_to_unicode_should_return_the_same_object(self):\n        self.assertEqual(to_unicode(\"\\xf1e\\xf1e\\xf1e\"), \"\\xf1e\\xf1e\\xf1e\")\n\n    def test_converting_a_strange_object_should_raise_TypeError(self):\n        self.assertRaises(TypeError, to_unicode, 423)\n\n    def test_errors_argument(self):\n        self.assertEqual(to_unicode(b\"a\\xedb\", \"utf-8\", errors=\"replace\"), \"a\\ufffdb\")\n\n\nclass ToBytesTest(unittest.TestCase):\n    def test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):\n        self.assertEqual(to_bytes(\"\\xa3 49\"), b\"\\xc2\\xa3 49\")\n\n    def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):\n        self.assertEqual(to_bytes(\"\\xa3 49\", \"latin-1\"), b\"\\xa3 49\")\n\n    def test_converting_a_regular_bytes_to_bytes_should_return_the_same_object(self):\n        self.assertEqual(to_bytes(b\"lel\\xf1e\"), b\"lel\\xf1e\")\n\n    def test_converting_a_strange_object_should_raise_TypeError(self):\n        self.assertRaises(TypeError, to_bytes, unittest)\n\n    def test_errors_argument(self):\n        self.assertEqual(to_bytes(\"a\\ufffdb\", \"latin-1\", errors=\"replace\"), b\"a?b\")\n\n\nclass MemoizedMethodTest(unittest.TestCase):\n    def test_memoizemethod_noargs(self):\n        class A:\n            @memoizemethod_noargs\n            def cached(self):\n                return object()\n\n            def noncached(self):\n                return object()\n\n        a = A()\n        one = a.cached()\n        two = a.cached()\n        three = a.noncached()\n        assert one is two\n        assert one is not three\n\n\nclass BinaryIsTextTest(unittest.TestCase):\n    def test_binaryistext(self):\n        assert binary_is_text(b\"hello\")\n\n    def test_utf_16_strings_contain_null_bytes(self):\n        assert binary_is_text(\"hello\".encode(\"utf-16\"))\n\n    def test_one_with_encoding(self):\n        assert binary_is_text(b\"<div>Price \\xa3</div>\")\n\n    def test_real_binary_bytes(self):\n        assert not binary_is_text(b\"\\x02\\xa3\")\n\n\nclass UtilsPythonTestCase(unittest.TestCase):\n    def test_equal_attributes(self):\n        class Obj:\n            pass\n\n        a = Obj()\n        b = Obj()\n        # no attributes given return False\n        self.assertFalse(equal_attributes(a, b, []))\n        # nonexistent attributes\n        self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n\n        a.x = 1\n        b.x = 1\n        # equal attribute\n        self.assertTrue(equal_attributes(a, b, [\"x\"]))\n\n        b.y = 2\n        # obj1 has no attribute y\n        self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n\n        a.y = 2\n        # equal attributes\n        self.assertTrue(equal_attributes(a, b, [\"x\", \"y\"]))\n\n        a.y = 1\n        # different attributes\n        self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n\n        # test callable\n        a.meta = {}\n        b.meta = {}\n        self.assertTrue(equal_attributes(a, b, [\"meta\"]))\n\n        # compare ['meta']['a']\n        a.meta[\"z\"] = 1\n        b.meta[\"z\"] = 1\n\n        get_z = operator.itemgetter(\"z\")\n        get_meta = operator.attrgetter(\"meta\")\n\n        def compare_z(obj):\n            return get_z(get_meta(obj))\n\n        self.assertTrue(equal_attributes(a, b, [compare_z, \"x\"]))\n        # fail z equality\n        a.meta[\"z\"] = 2\n        self.assertFalse(equal_attributes(a, b, [compare_z, \"x\"]))\n\n    def test_get_func_args(self):\n        def f1(a, b, c):\n            pass\n\n        def f2(a, b=None, c=None):\n            pass\n\n        def f3(a, b=None, *, c=None):\n            pass\n\n        class A:\n            def __init__(self, a, b, c):\n                pass\n\n            def method(self, a, b, c):\n                pass\n\n        class Callable:\n            def __call__(self, a, b, c):\n                pass\n\n        a = A(1, 2, 3)\n        cal = Callable()\n        partial_f1 = functools.partial(f1, None)\n        partial_f2 = functools.partial(f1, b=None)\n        partial_f3 = functools.partial(partial_f2, None)\n\n        self.assertEqual(get_func_args(f1), [\"a\", \"b\", \"c\"])\n        self.assertEqual(get_func_args(f2), [\"a\", \"b\", \"c\"])\n        self.assertEqual(get_func_args(f3), [\"a\", \"b\", \"c\"])\n        self.assertEqual(get_func_args(A), [\"a\", \"b\", \"c\"])\n        self.assertEqual(get_func_args(a.method), [\"a\", \"b\", \"c\"])\n        self.assertEqual(get_func_args(partial_f1), [\"b\", \"c\"])\n        self.assertEqual(get_func_args(partial_f2), [\"a\", \"c\"])\n        self.assertEqual(get_func_args(partial_f3), [\"c\"])\n        self.assertEqual(get_func_args(cal), [\"a\", \"b\", \"c\"])\n        self.assertEqual(get_func_args(object), [])\n        self.assertEqual(get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"])\n        self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n\n        if platform.python_implementation() == \"CPython\":\n            # This didn't work on older versions of CPython: https://github.com/python/cpython/issues/86951\n            self.assertIn(\n                get_func_args(operator.itemgetter(2), stripself=True),\n                [[], [\"args\", \"kwargs\"]],\n            )\n        elif platform.python_implementation() == \"PyPy\":\n            self.assertEqual(\n                get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n            )\n\n    def test_without_none_values(self):\n        self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n        self.assertEqual(without_none_values((1, None, 3, 4)), (1, 3, 4))\n        self.assertEqual(\n            without_none_values({\"one\": 1, \"none\": None, \"three\": 3, \"four\": 4}),\n            {\"one\": 1, \"three\": 3, \"four\": 4},\n        )\n", "tests/test_utils_deprecate.py": "import inspect\nimport unittest\nimport warnings\nfrom unittest import mock\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.deprecate import create_deprecated_class, update_classpath\n\n\nclass MyWarning(UserWarning):\n    pass\n\n\nclass SomeBaseClass:\n    pass\n\n\nclass NewName(SomeBaseClass):\n    pass\n\n\nclass WarnWhenSubclassedTest(unittest.TestCase):\n    def _mywarnings(self, w, category=MyWarning):\n        return [x for x in w if x.category is MyWarning]\n\n    def test_no_warning_on_definition(self):\n        with warnings.catch_warnings(record=True) as w:\n            create_deprecated_class(\"Deprecated\", NewName)\n\n        w = self._mywarnings(w)\n        self.assertEqual(w, [])\n\n    def test_subclassing_warning_message(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_category=MyWarning\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 1)\n        self.assertEqual(\n            str(w[0].message),\n            \"tests.test_utils_deprecate.UserClass inherits from \"\n            \"deprecated class tests.test_utils_deprecate.Deprecated, \"\n            \"please inherit from tests.test_utils_deprecate.NewName.\"\n            \" (warning only on first subclass, there may be others)\",\n        )\n        self.assertEqual(w[0].lineno, inspect.getsourcelines(UserClass)[1])\n\n    def test_custom_class_paths(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\",\n            NewName,\n            new_class_path=\"foo.NewClass\",\n            old_class_path=\"bar.OldClass\",\n            warn_category=MyWarning,\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n            _ = Deprecated()\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 2)\n        self.assertIn(\"foo.NewClass\", str(w[0].message))\n        self.assertIn(\"bar.OldClass\", str(w[0].message))\n        self.assertIn(\"foo.NewClass\", str(w[1].message))\n        self.assertIn(\"bar.OldClass\", str(w[1].message))\n\n    def test_subclassing_warns_only_on_direct_children(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_once=False, warn_category=MyWarning\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n            class NoWarnOnMe(UserClass):\n                pass\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 1)\n        self.assertIn(\"UserClass\", str(w[0].message))\n\n    def test_subclassing_warns_once_by_default(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_category=MyWarning\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n\n            class UserClass(Deprecated):\n                pass\n\n            class FooClass(Deprecated):\n                pass\n\n            class BarClass(Deprecated):\n                pass\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 1)\n        self.assertIn(\"UserClass\", str(w[0].message))\n\n    def test_warning_on_instance(self):\n        Deprecated = create_deprecated_class(\n            \"Deprecated\", NewName, warn_category=MyWarning\n        )\n\n        # ignore subclassing warnings\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", MyWarning)\n\n            class UserClass(Deprecated):\n                pass\n\n        with warnings.catch_warnings(record=True) as w:\n            _, lineno = Deprecated(), inspect.getlineno(inspect.currentframe())\n            _ = UserClass()  # subclass instances don't warn\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 1)\n        self.assertEqual(\n            str(w[0].message),\n            \"tests.test_utils_deprecate.Deprecated is deprecated, \"\n            \"instantiate tests.test_utils_deprecate.NewName instead.\",\n        )\n        self.assertEqual(w[0].lineno, lineno)\n\n    def test_warning_auto_message(self):\n        with warnings.catch_warnings(record=True) as w:\n            Deprecated = create_deprecated_class(\"Deprecated\", NewName)\n\n            class UserClass2(Deprecated):\n                pass\n\n        msg = str(w[0].message)\n        self.assertIn(\"tests.test_utils_deprecate.NewName\", msg)\n        self.assertIn(\"tests.test_utils_deprecate.Deprecated\", msg)\n\n    def test_issubclass(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n\n            class UpdatedUserClass1(NewName):\n                pass\n\n            class UpdatedUserClass1a(NewName):\n                pass\n\n            class OutdatedUserClass1(DeprecatedName):\n                pass\n\n            class OutdatedUserClass1a(DeprecatedName):\n                pass\n\n            class UnrelatedClass:\n                pass\n\n            class OldStyleClass:\n                pass\n\n        assert issubclass(UpdatedUserClass1, NewName)\n        assert issubclass(UpdatedUserClass1a, NewName)\n        assert issubclass(UpdatedUserClass1, DeprecatedName)\n        assert issubclass(UpdatedUserClass1a, DeprecatedName)\n        assert issubclass(OutdatedUserClass1, DeprecatedName)\n        assert not issubclass(UnrelatedClass, DeprecatedName)\n        assert not issubclass(OldStyleClass, DeprecatedName)\n        assert not issubclass(OldStyleClass, DeprecatedName)\n        assert not issubclass(OutdatedUserClass1, OutdatedUserClass1a)\n        assert not issubclass(OutdatedUserClass1a, OutdatedUserClass1)\n\n        self.assertRaises(TypeError, issubclass, object(), DeprecatedName)\n\n    def test_isinstance(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n\n            class UpdatedUserClass2(NewName):\n                pass\n\n            class UpdatedUserClass2a(NewName):\n                pass\n\n            class OutdatedUserClass2(DeprecatedName):\n                pass\n\n            class OutdatedUserClass2a(DeprecatedName):\n                pass\n\n            class UnrelatedClass:\n                pass\n\n            class OldStyleClass:\n                pass\n\n        assert isinstance(UpdatedUserClass2(), NewName)\n        assert isinstance(UpdatedUserClass2a(), NewName)\n        assert isinstance(UpdatedUserClass2(), DeprecatedName)\n        assert isinstance(UpdatedUserClass2a(), DeprecatedName)\n        assert isinstance(OutdatedUserClass2(), DeprecatedName)\n        assert isinstance(OutdatedUserClass2a(), DeprecatedName)\n        assert not isinstance(OutdatedUserClass2a(), OutdatedUserClass2)\n        assert not isinstance(OutdatedUserClass2(), OutdatedUserClass2a)\n        assert not isinstance(UnrelatedClass(), DeprecatedName)\n        assert not isinstance(OldStyleClass(), DeprecatedName)\n\n    def test_clsdict(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            Deprecated = create_deprecated_class(\"Deprecated\", NewName, {\"foo\": \"bar\"})\n\n        self.assertEqual(Deprecated.foo, \"bar\")\n\n    def test_deprecate_a_class_with_custom_metaclass(self):\n        Meta1 = type(\"Meta1\", (type,), {})\n        New = Meta1(\"New\", (), {})\n        create_deprecated_class(\"Deprecated\", New)\n\n    def test_deprecate_subclass_of_deprecated_class(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            Deprecated = create_deprecated_class(\n                \"Deprecated\", NewName, warn_category=MyWarning\n            )\n            AlsoDeprecated = create_deprecated_class(\n                \"AlsoDeprecated\",\n                Deprecated,\n                new_class_path=\"foo.Bar\",\n                warn_category=MyWarning,\n            )\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 0, str(map(str, w)))\n\n        with warnings.catch_warnings(record=True) as w:\n            AlsoDeprecated()\n\n            class UserClass(AlsoDeprecated):\n                pass\n\n        w = self._mywarnings(w)\n        self.assertEqual(len(w), 2)\n        self.assertIn(\"AlsoDeprecated\", str(w[0].message))\n        self.assertIn(\"foo.Bar\", str(w[0].message))\n        self.assertIn(\"AlsoDeprecated\", str(w[1].message))\n        self.assertIn(\"foo.Bar\", str(w[1].message))\n\n    def test_inspect_stack(self):\n        with mock.patch(\"inspect.stack\", side_effect=IndexError):\n            with warnings.catch_warnings(record=True) as w:\n                DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n\n                class SubClass(DeprecatedName):\n                    pass\n\n        self.assertIn(\"Error detecting parent module\", str(w[0].message))\n\n\n@mock.patch(\n    \"scrapy.utils.deprecate.DEPRECATION_RULES\",\n    [\n        (\"scrapy.contrib.pipeline.\", \"scrapy.pipelines.\"),\n        (\"scrapy.contrib.\", \"scrapy.extensions.\"),\n    ],\n)\nclass UpdateClassPathTest(unittest.TestCase):\n    def test_old_path_gets_fixed(self):\n        with warnings.catch_warnings(record=True) as w:\n            output = update_classpath(\"scrapy.contrib.debug.Debug\")\n        self.assertEqual(output, \"scrapy.extensions.debug.Debug\")\n        self.assertEqual(len(w), 1)\n        self.assertIn(\"scrapy.contrib.debug.Debug\", str(w[0].message))\n        self.assertIn(\"scrapy.extensions.debug.Debug\", str(w[0].message))\n\n    def test_sorted_replacement(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            output = update_classpath(\"scrapy.contrib.pipeline.Pipeline\")\n        self.assertEqual(output, \"scrapy.pipelines.Pipeline\")\n\n    def test_unmatched_path_stays_the_same(self):\n        with warnings.catch_warnings(record=True) as w:\n            output = update_classpath(\"scrapy.unmatched.Path\")\n        self.assertEqual(output, \"scrapy.unmatched.Path\")\n        self.assertEqual(len(w), 0)\n\n    def test_returns_nonstring(self):\n        for notastring in [None, True, [1, 2, 3], object()]:\n            self.assertEqual(update_classpath(notastring), notastring)\n", "tests/test_utils_asyncgen.py": "from twisted.trial import unittest\n\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.defer import deferred_f_from_coro_f\n\n\nclass AsyncgenUtilsTest(unittest.TestCase):\n    @deferred_f_from_coro_f\n    async def test_as_async_generator(self):\n        ag = as_async_generator(range(42))\n        results = []\n        async for i in ag:\n            results.append(i)\n        self.assertEqual(results, list(range(42)))\n\n    @deferred_f_from_coro_f\n    async def test_collect_asyncgen(self):\n        ag = as_async_generator(range(42))\n        results = await collect_asyncgen(ag)\n        self.assertEqual(results, list(range(42)))\n", "tests/test_spider.py": "import gzip\nimport inspect\nimport warnings\nfrom io import BytesIO\nfrom logging import WARNING\nfrom pathlib import Path\nfrom typing import Any\nfrom unittest import mock\n\nfrom testfixtures import LogCapture\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.trial import unittest\nfrom w3lib.url import safe_url_string\n\nfrom scrapy import signals\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import HtmlResponse, Request, Response, TextResponse, XmlResponse\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import (\n    CrawlSpider,\n    CSVFeedSpider,\n    Rule,\n    SitemapSpider,\n    Spider,\n    XMLFeedSpider,\n)\nfrom scrapy.spiders.init import InitSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests import get_testdata, tests_datadir\n\n\nclass SpiderTest(unittest.TestCase):\n    spider_class = Spider\n\n    def setUp(self):\n        warnings.simplefilter(\"always\")\n\n    def tearDown(self):\n        warnings.resetwarnings()\n\n    def test_base_spider(self):\n        spider = self.spider_class(\"example.com\")\n        self.assertEqual(spider.name, \"example.com\")\n        self.assertEqual(spider.start_urls, [])\n\n    def test_start_requests(self):\n        spider = self.spider_class(\"example.com\")\n        start_requests = spider.start_requests()\n        self.assertTrue(inspect.isgenerator(start_requests))\n        self.assertEqual(list(start_requests), [])\n\n    def test_spider_args(self):\n        \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n        spider = self.spider_class(\"example.com\", foo=\"bar\")\n        self.assertEqual(spider.foo, \"bar\")\n\n    def test_spider_without_name(self):\n        \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n        self.assertRaises(ValueError, self.spider_class)\n        self.assertRaises(ValueError, self.spider_class, somearg=\"foo\")\n\n    def test_from_crawler_crawler_and_settings_population(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        self.assertTrue(hasattr(spider, \"crawler\"))\n        self.assertIs(spider.crawler, crawler)\n        self.assertTrue(hasattr(spider, \"settings\"))\n        self.assertIs(spider.settings, crawler.settings)\n\n    def test_from_crawler_init_call(self):\n        with mock.patch.object(\n            self.spider_class, \"__init__\", return_value=None\n        ) as mock_init:\n            self.spider_class.from_crawler(get_crawler(), \"example.com\", foo=\"bar\")\n            mock_init.assert_called_once_with(\"example.com\", foo=\"bar\")\n\n    def test_closed_signal_call(self):\n        class TestSpider(self.spider_class):\n            closed_called = False\n\n            def closed(self, reason):\n                self.closed_called = True\n\n        crawler = get_crawler()\n        spider = TestSpider.from_crawler(crawler, \"example.com\")\n        crawler.signals.send_catch_log(signal=signals.spider_opened, spider=spider)\n        crawler.signals.send_catch_log(\n            signal=signals.spider_closed, spider=spider, reason=None\n        )\n        self.assertTrue(spider.closed_called)\n\n    def test_update_settings(self):\n        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n        self.spider_class.custom_settings = spider_settings\n        settings = Settings(project_settings, priority=\"project\")\n\n        self.spider_class.update_settings(settings)\n        self.assertEqual(settings.get(\"TEST1\"), \"spider\")\n        self.assertEqual(settings.get(\"TEST2\"), \"spider\")\n        self.assertEqual(settings.get(\"TEST3\"), \"project\")\n\n    @inlineCallbacks\n    def test_settings_in_from_crawler(self):\n        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n\n        class TestSpider(self.spider_class):\n            name = \"test\"\n            custom_settings = spider_settings\n\n            @classmethod\n            def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n                spider = super().from_crawler(crawler, *args, **kwargs)\n                spider.settings.set(\"TEST1\", \"spider_instance\", priority=\"spider\")\n                return spider\n\n        crawler = Crawler(TestSpider, project_settings)\n        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n        yield crawler.crawl()\n        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider_instance\")\n\n    def test_logger(self):\n        spider = self.spider_class(\"example.com\")\n        with LogCapture() as lc:\n            spider.logger.info(\"test log msg\")\n        lc.check((\"example.com\", \"INFO\", \"test log msg\"))\n\n        record = lc.records[0]\n        self.assertIn(\"spider\", record.__dict__)\n        self.assertIs(record.spider, spider)\n\n    def test_log(self):\n        spider = self.spider_class(\"example.com\")\n        with mock.patch(\"scrapy.spiders.Spider.logger\") as mock_logger:\n            spider.log(\"test log msg\", \"INFO\")\n        mock_logger.log.assert_called_once_with(\"INFO\", \"test log msg\")\n\n\nclass InitSpiderTest(SpiderTest):\n    spider_class = InitSpider\n\n\nclass XMLFeedSpiderTest(SpiderTest):\n    spider_class = XMLFeedSpider\n\n    def test_register_namespace(self):\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n        <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"\n                xmlns:y=\"http://www.example.com/schemas/extras/1.0\">\n        <url><x:loc>http://www.example.com/Special-Offers.html</x:loc><y:updated>2009-08-16</y:updated>\n            <other value=\"bar\" y:custom=\"fuu\"/>\n        </url>\n        <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</y:updated><other value=\"foo\"/></url>\n        </urlset>\"\"\"\n        response = XmlResponse(url=\"http://example.com/sitemap.xml\", body=body)\n\n        class _XMLSpider(self.spider_class):\n            itertag = \"url\"\n            namespaces = (\n                (\"a\", \"http://www.google.com/schemas/sitemap/0.84\"),\n                (\"b\", \"http://www.example.com/schemas/extras/1.0\"),\n            )\n\n            def parse_node(self, response, selector):\n                yield {\n                    \"loc\": selector.xpath(\"a:loc/text()\").getall(),\n                    \"updated\": selector.xpath(\"b:updated/text()\").getall(),\n                    \"other\": selector.xpath(\"other/@value\").getall(),\n                    \"custom\": selector.xpath(\"other/@b:custom\").getall(),\n                }\n\n        for iterator in (\"iternodes\", \"xml\"):\n            spider = _XMLSpider(\"example\", iterator=iterator)\n            output = list(spider._parse(response))\n            self.assertEqual(len(output), 2, iterator)\n            self.assertEqual(\n                output,\n                [\n                    {\n                        \"loc\": [\"http://www.example.com/Special-Offers.html\"],\n                        \"updated\": [\"2009-08-16\"],\n                        \"custom\": [\"fuu\"],\n                        \"other\": [\"bar\"],\n                    },\n                    {\n                        \"loc\": [],\n                        \"updated\": [\"2009-08-16\"],\n                        \"other\": [\"foo\"],\n                        \"custom\": [],\n                    },\n                ],\n                iterator,\n            )\n\n\nclass CSVFeedSpiderTest(SpiderTest):\n    spider_class = CSVFeedSpider\n\n    def test_parse_rows(self):\n        body = get_testdata(\"feeds\", \"feed-sample6.csv\")\n        response = Response(\"http://example.org/dummy.csv\", body=body)\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            delimiter = \",\"\n            quotechar = \"'\"\n\n            def parse_row(self, response, row):\n                return row\n\n        spider = _CrawlSpider()\n        rows = list(spider.parse_rows(response))\n        assert rows[0] == {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"}\n        assert len(rows) == 4\n\n\nclass CrawlSpiderTest(SpiderTest):\n    test_body = b\"\"\"<html><head><title>Page title<title>\n    <body>\n    <p><a href=\"item/12.html\">Item 12</a></p>\n    <div class='links'>\n    <p><a href=\"/about.html\">About us</a></p>\n    </div>\n    <div>\n    <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n    </div>\n    </body></html>\"\"\"\n    spider_class = CrawlSpider\n\n    def test_rule_without_link_extractor(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(),)\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.org/somepage/item/12.html\",\n                \"http://example.org/about.html\",\n                \"http://example.org/nofollow.html\",\n            ],\n        )\n\n    def test_process_links(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n\n            def dummy_process_links(self, links):\n                return links\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.org/somepage/item/12.html\",\n                \"http://example.org/about.html\",\n                \"http://example.org/nofollow.html\",\n            ],\n        )\n\n    def test_process_links_filter(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            import re\n\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_links=\"filter_process_links\"),)\n            _test_regex = re.compile(\"nofollow\")\n\n            def filter_process_links(self, links):\n                return [link for link in links if not self._test_regex.search(link.url)]\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 2)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.org/somepage/item/12.html\",\n                \"http://example.org/about.html\",\n            ],\n        )\n\n    def test_process_links_generator(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n\n            def dummy_process_links(self, links):\n                yield from links\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.org/somepage/item/12.html\",\n                \"http://example.org/about.html\",\n                \"http://example.org/nofollow.html\",\n            ],\n        )\n\n    def test_process_request(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        def process_request_change_domain(request, response):\n            return request.replace(url=request.url.replace(\".org\", \".com\"))\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (\n                Rule(LinkExtractor(), process_request=process_request_change_domain),\n            )\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.com/somepage/item/12.html\",\n                \"http://example.com/about.html\",\n                \"http://example.com/nofollow.html\",\n            ],\n        )\n\n    def test_process_request_with_response(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        def process_request_meta_response_class(request, response):\n            request.meta[\"response_class\"] = response.__class__.__name__\n            return request\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (\n                Rule(\n                    LinkExtractor(), process_request=process_request_meta_response_class\n                ),\n            )\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.org/somepage/item/12.html\",\n                \"http://example.org/about.html\",\n                \"http://example.org/nofollow.html\",\n            ],\n        )\n        self.assertEqual(\n            [r.meta[\"response_class\"] for r in output],\n            [\"HtmlResponse\", \"HtmlResponse\", \"HtmlResponse\"],\n        )\n\n    def test_process_request_instance_method(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (Rule(LinkExtractor(), process_request=\"process_request_upper\"),)\n\n            def process_request_upper(self, request, response):\n                return request.replace(url=request.url.upper())\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                safe_url_string(\"http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML\"),\n                safe_url_string(\"http://EXAMPLE.ORG/ABOUT.HTML\"),\n                safe_url_string(\"http://EXAMPLE.ORG/NOFOLLOW.HTML\"),\n            ],\n        )\n\n    def test_process_request_instance_method_with_response(self):\n        response = HtmlResponse(\n            \"http://example.org/somepage/index.html\", body=self.test_body\n        )\n\n        class _CrawlSpider(self.spider_class):\n            name = \"test\"\n            allowed_domains = [\"example.org\"]\n            rules = (\n                Rule(\n                    LinkExtractor(),\n                    process_request=\"process_request_meta_response_class\",\n                ),\n            )\n\n            def process_request_meta_response_class(self, request, response):\n                request.meta[\"response_class\"] = response.__class__.__name__\n                return request\n\n        spider = _CrawlSpider()\n        output = list(spider._requests_to_follow(response))\n        self.assertEqual(len(output), 3)\n        self.assertTrue(all(isinstance(r, Request) for r in output))\n        self.assertEqual(\n            [r.url for r in output],\n            [\n                \"http://example.org/somepage/item/12.html\",\n                \"http://example.org/about.html\",\n                \"http://example.org/nofollow.html\",\n            ],\n        )\n        self.assertEqual(\n            [r.meta[\"response_class\"] for r in output],\n            [\"HtmlResponse\", \"HtmlResponse\", \"HtmlResponse\"],\n        )\n\n    def test_follow_links_attribute_population(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        self.assertTrue(hasattr(spider, \"_follow_links\"))\n        self.assertTrue(spider._follow_links)\n\n        settings_dict = {\"CRAWLSPIDER_FOLLOW_LINKS\": False}\n        crawler = get_crawler(settings_dict=settings_dict)\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        self.assertTrue(hasattr(spider, \"_follow_links\"))\n        self.assertFalse(spider._follow_links)\n\n    def test_start_url(self):\n        spider = self.spider_class(\"example.com\")\n        spider.start_url = \"https://www.example.com\"\n\n        with self.assertRaisesRegex(AttributeError, r\"^Crawling could not start.*$\"):\n            list(spider.start_requests())\n\n\nclass SitemapSpiderTest(SpiderTest):\n    spider_class = SitemapSpider\n\n    BODY = b\"SITEMAP\"\n    f = BytesIO()\n    g = gzip.GzipFile(fileobj=f, mode=\"w+b\")\n    g.write(BODY)\n    g.close()\n    GZBODY = f.getvalue()\n\n    def assertSitemapBody(self, response, body):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        self.assertEqual(spider._get_sitemap_body(response), body)\n\n    def test_get_sitemap_body(self):\n        r = XmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n        self.assertSitemapBody(r, self.BODY)\n\n        r = HtmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n        self.assertSitemapBody(r, None)\n\n        r = Response(url=\"http://www.example.com/favicon.ico\", body=self.BODY)\n        self.assertSitemapBody(r, None)\n\n    def test_get_sitemap_body_gzip_headers(self):\n        r = Response(\n            url=\"http://www.example.com/sitemap\",\n            body=self.GZBODY,\n            headers={\"content-type\": \"application/gzip\"},\n            request=Request(\"http://www.example.com/sitemap\"),\n        )\n        self.assertSitemapBody(r, self.BODY)\n\n    def test_get_sitemap_body_xml_url(self):\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=self.BODY)\n        self.assertSitemapBody(r, self.BODY)\n\n    def test_get_sitemap_body_xml_url_compressed(self):\n        r = Response(\n            url=\"http://www.example.com/sitemap.xml.gz\",\n            body=self.GZBODY,\n            request=Request(\"http://www.example.com/sitemap\"),\n        )\n        self.assertSitemapBody(r, self.BODY)\n\n        # .xml.gz but body decoded by HttpCompression middleware already\n        r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.BODY)\n        self.assertSitemapBody(r, self.BODY)\n\n    def test_get_sitemap_urls_from_robotstxt(self):\n        robots = b\"\"\"# Sitemap files\nSitemap: http://example.com/sitemap.xml\nSitemap: http://example.com/sitemap-product-index.xml\nSitemap: HTTP://example.com/sitemap-uppercase.xml\nSitemap: /sitemap-relative-url.xml\n\"\"\"\n\n        r = TextResponse(url=\"http://www.example.com/robots.txt\", body=robots)\n        spider = self.spider_class(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\n                \"http://example.com/sitemap.xml\",\n                \"http://example.com/sitemap-product-index.xml\",\n                \"http://example.com/sitemap-uppercase.xml\",\n                \"http://www.example.com/sitemap-relative-url.xml\",\n            ],\n        )\n\n    def test_alternate_url_locs(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/</loc>\n            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n                href=\"http://www.example.com/deutsch/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"de-ch\"\n                href=\"http://www.example.com/schweiz-deutsch/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"it\"\n                href=\"http://www.example.com/italiano/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"it\"/><!-- wrong tag without href -->\n        </url>\n    </urlset>\"\"\"\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\"http://www.example.com/english/\"],\n        )\n\n        spider.sitemap_alternate_links = True\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\n                \"http://www.example.com/english/\",\n                \"http://www.example.com/deutsch/\",\n                \"http://www.example.com/schweiz-deutsch/\",\n                \"http://www.example.com/italiano/\",\n            ],\n        )\n\n    def test_sitemap_filter(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/</loc>\n            <lastmod>2010-01-01</lastmod>\n        </url>\n        <url>\n            <loc>http://www.example.com/portuguese/</loc>\n            <lastmod>2005-01-01</lastmod>\n        </url>\n    </urlset>\"\"\"\n\n        class FilteredSitemapSpider(self.spider_class):\n            def sitemap_filter(self, entries):\n                from datetime import datetime\n\n                for entry in entries:\n                    date_time = datetime.strptime(entry[\"lastmod\"], \"%Y-%m-%d\")\n                    if date_time.year > 2008:\n                        yield entry\n\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\"http://www.example.com/english/\", \"http://www.example.com/portuguese/\"],\n        )\n\n        spider = FilteredSitemapSpider(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\"http://www.example.com/english/\"],\n        )\n\n    def test_sitemap_filter_with_alternate_links(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/article_1/</loc>\n            <lastmod>2010-01-01</lastmod>\n            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n                href=\"http://www.example.com/deutsch/article_1/\"/>\n        </url>\n        <url>\n            <loc>http://www.example.com/english/article_2/</loc>\n            <lastmod>2015-01-01</lastmod>\n        </url>\n    </urlset>\"\"\"\n\n        class FilteredSitemapSpider(self.spider_class):\n            def sitemap_filter(self, entries):\n                for entry in entries:\n                    alternate_links = entry.get(\"alternate\", ())\n                    for link in alternate_links:\n                        if \"/deutsch/\" in link:\n                            entry[\"loc\"] = link\n                            yield entry\n\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\n                \"http://www.example.com/english/article_1/\",\n                \"http://www.example.com/english/article_2/\",\n            ],\n        )\n\n        spider = FilteredSitemapSpider(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\"http://www.example.com/deutsch/article_1/\"],\n        )\n\n    def test_sitemapindex_filter(self):\n        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n        <sitemap>\n            <loc>http://www.example.com/sitemap1.xml</loc>\n            <lastmod>2004-01-01T20:00:00+00:00</lastmod>\n        </sitemap>\n        <sitemap>\n            <loc>http://www.example.com/sitemap2.xml</loc>\n            <lastmod>2005-01-01</lastmod>\n        </sitemap>\n    </sitemapindex>\"\"\"\n\n        class FilteredSitemapSpider(self.spider_class):\n            def sitemap_filter(self, entries):\n                from datetime import datetime\n\n                for entry in entries:\n                    date_time = datetime.strptime(\n                        entry[\"lastmod\"].split(\"T\")[0], \"%Y-%m-%d\"\n                    )\n                    if date_time.year > 2004:\n                        yield entry\n\n        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n        spider = self.spider_class(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\n                \"http://www.example.com/sitemap1.xml\",\n                \"http://www.example.com/sitemap2.xml\",\n            ],\n        )\n\n        spider = FilteredSitemapSpider(\"example.com\")\n        self.assertEqual(\n            [req.url for req in spider._parse_sitemap(r)],\n            [\"http://www.example.com/sitemap2.xml\"],\n        )\n\n    def test_compression_bomb_setting(self):\n        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n        crawler = get_crawler(settings_dict=settings)\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(url=\"https://example.com\")\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        self.assertIsNone(spider._get_sitemap_body(response))\n\n    def test_compression_bomb_spider_attr(self):\n        class DownloadMaxSizeSpider(self.spider_class):\n            download_maxsize = 10_000_000\n\n        crawler = get_crawler()\n        spider = DownloadMaxSizeSpider.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(url=\"https://example.com\")\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        self.assertIsNone(spider._get_sitemap_body(response))\n\n    def test_compression_bomb_request_meta(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(\n            url=\"https://example.com\", meta={\"download_maxsize\": 10_000_000}\n        )\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        self.assertIsNone(spider._get_sitemap_body(response))\n\n    def test_download_warnsize_setting(self):\n        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n        crawler = get_crawler(settings_dict=settings)\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(url=\"https://example.com\")\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        with LogCapture(\n            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n        ) as log:\n            spider._get_sitemap_body(response)\n        log.check(\n            (\n                \"scrapy.spiders.sitemap\",\n                \"WARNING\",\n                (\n                    \"<200 https://example.com> body size after decompression \"\n                    \"(11511612 B) is larger than the download warning size \"\n                    \"(10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_spider_attr(self):\n        class DownloadWarnSizeSpider(self.spider_class):\n            download_warnsize = 10_000_000\n\n        crawler = get_crawler()\n        spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(\n            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n        )\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        with LogCapture(\n            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n        ) as log:\n            spider._get_sitemap_body(response)\n        log.check(\n            (\n                \"scrapy.spiders.sitemap\",\n                \"WARNING\",\n                (\n                    \"<200 https://example.com> body size after decompression \"\n                    \"(11511612 B) is larger than the download warning size \"\n                    \"(10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_request_meta(self):\n        crawler = get_crawler()\n        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n        body = body_path.read_bytes()\n        request = Request(\n            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n        )\n        response = Response(url=\"https://example.com\", body=body, request=request)\n        with LogCapture(\n            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n        ) as log:\n            spider._get_sitemap_body(response)\n        log.check(\n            (\n                \"scrapy.spiders.sitemap\",\n                \"WARNING\",\n                (\n                    \"<200 https://example.com> body size after decompression \"\n                    \"(11511612 B) is larger than the download warning size \"\n                    \"(10000000 B).\"\n                ),\n            ),\n        )\n\n\nclass DeprecationTest(unittest.TestCase):\n    def test_crawl_spider(self):\n        assert issubclass(CrawlSpider, Spider)\n        assert isinstance(CrawlSpider(name=\"foo\"), Spider)\n\n\nclass NoParseMethodSpiderTest(unittest.TestCase):\n    spider_class = Spider\n\n    def test_undefined_parse_method(self):\n        spider = self.spider_class(\"example.com\")\n        text = b\"Random text\"\n        resp = TextResponse(url=\"http://www.example.com/random_url\", body=text)\n\n        exc_msg = \"Spider.parse callback is not defined\"\n        with self.assertRaisesRegex(NotImplementedError, exc_msg):\n            spider.parse(resp)\n", "tests/test_downloadermiddleware_downloadtimeout.py": "import unittest\n\nfrom scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass DownloadTimeoutMiddlewareTest(unittest.TestCase):\n    def get_request_spider_mw(self, settings=None):\n        crawler = get_crawler(Spider, settings)\n        spider = crawler._create_spider(\"foo\")\n        request = Request(\"http://scrapytest.org/\")\n        return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n\n    def test_default_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw()\n        mw.spider_opened(spider)\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta.get(\"download_timeout\"), 180)\n\n    def test_string_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw({\"DOWNLOAD_TIMEOUT\": \"20.1\"})\n        mw.spider_opened(spider)\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta.get(\"download_timeout\"), 20.1)\n\n    def test_spider_has_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw()\n        spider.download_timeout = 2\n        mw.spider_opened(spider)\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta.get(\"download_timeout\"), 2)\n\n    def test_request_has_download_timeout(self):\n        req, spider, mw = self.get_request_spider_mw()\n        spider.download_timeout = 2\n        mw.spider_opened(spider)\n        req.meta[\"download_timeout\"] = 1\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta.get(\"download_timeout\"), 1)\n", "tests/test_pipeline_crawl.py": "import shutil\nfrom pathlib import Path\nfrom tempfile import mkdtemp\nfrom typing import Optional, Set\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\nfrom w3lib.url import add_or_replace_parameter\n\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.misc import load_object\nfrom tests.mockserver import MockServer\nfrom tests.spiders import SimpleSpider\n\n\nclass MediaDownloadSpider(SimpleSpider):\n    name = \"mediadownload\"\n\n    def _process_url(self, url):\n        return url\n\n    def parse(self, response):\n        self.logger.info(response.headers)\n        self.logger.info(response.text)\n        item = {\n            self.media_key: [],\n            self.media_urls_key: [\n                self._process_url(response.urljoin(href))\n                for href in response.xpath(\n                    '//table[thead/tr/th=\"Filename\"]/tbody//a/@href'\n                ).getall()\n            ],\n        }\n        yield item\n\n\nclass BrokenLinksMediaDownloadSpider(MediaDownloadSpider):\n    name = \"brokenmedia\"\n\n    def _process_url(self, url):\n        return url + \".foo\"\n\n\nclass RedirectedMediaDownloadSpider(MediaDownloadSpider):\n    name = \"redirectedmedia\"\n\n    def _process_url(self, url):\n        return add_or_replace_parameter(\n            self.mockserver.url(\"/redirect-to\"), \"goto\", url\n        )\n\n\nclass FileDownloadCrawlTestCase(TestCase):\n    pipeline_class = \"scrapy.pipelines.files.FilesPipeline\"\n    store_setting_key = \"FILES_STORE\"\n    media_key = \"files\"\n    media_urls_key = \"file_urls\"\n    expected_checksums: Optional[Set[str]] = {\n        \"5547178b89448faf0015a13f904c936e\",\n        \"c2281c83670e31d8aaab7cb642b824db\",\n        \"ed3f6538dc15d4d9179dae57319edc5f\",\n    }\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n        # prepare a directory for storing files\n        self.tmpmediastore = Path(mkdtemp())\n        self.settings = {\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n            \"ITEM_PIPELINES\": {self.pipeline_class: 1},\n            self.store_setting_key: str(self.tmpmediastore),\n        }\n        self.runner = CrawlerRunner(self.settings)\n        self.items = []\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpmediastore)\n        self.items = []\n        self.mockserver.__exit__(None, None, None)\n\n    def _on_item_scraped(self, item):\n        self.items.append(item)\n\n    def _create_crawler(self, spider_class, runner=None, **kwargs):\n        if runner is None:\n            runner = self.runner\n        crawler = runner.create_crawler(spider_class, **kwargs)\n        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n        return crawler\n\n    def _assert_files_downloaded(self, items, logs):\n        self.assertEqual(len(items), 1)\n        self.assertIn(self.media_key, items[0])\n\n        # check that logs show the expected number of successful file downloads\n        file_dl_success = \"File (downloaded): Downloaded file from\"\n        self.assertEqual(logs.count(file_dl_success), 3)\n\n        # check that the images/files status is `downloaded`\n        for item in items:\n            for i in item[self.media_key]:\n                self.assertEqual(i[\"status\"], \"downloaded\")\n\n        # check that the images/files checksums are what we know they should be\n        if self.expected_checksums is not None:\n            checksums = {i[\"checksum\"] for item in items for i in item[self.media_key]}\n            self.assertEqual(checksums, self.expected_checksums)\n\n        # check that the image files where actually written to the media store\n        for item in items:\n            for i in item[self.media_key]:\n                self.assertTrue((self.tmpmediastore / i[\"path\"]).exists())\n\n    def _assert_files_download_failure(self, crawler, items, code, logs):\n        # check that the item does NOT have the \"images/files\" field populated\n        self.assertEqual(len(items), 1)\n        self.assertIn(self.media_key, items[0])\n        self.assertFalse(items[0][self.media_key])\n\n        # check that there was 1 successful fetch and 3 other responses with non-200 code\n        self.assertEqual(\n            crawler.stats.get_value(\"downloader/request_method_count/GET\"), 4\n        )\n        self.assertEqual(crawler.stats.get_value(\"downloader/response_count\"), 4)\n        self.assertEqual(\n            crawler.stats.get_value(\"downloader/response_status_count/200\"), 1\n        )\n        self.assertEqual(\n            crawler.stats.get_value(f\"downloader/response_status_count/{code}\"), 3\n        )\n\n        # check that logs do show the failure on the file downloads\n        file_dl_failure = f\"File (code: {code}): Error downloading file from\"\n        self.assertEqual(logs.count(file_dl_failure), 3)\n\n        # check that no files were written to the media store\n        self.assertEqual(list(self.tmpmediastore.iterdir()), [])\n\n    @defer.inlineCallbacks\n    def test_download_media(self):\n        crawler = self._create_crawler(MediaDownloadSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n            )\n        self._assert_files_downloaded(self.items, str(log))\n\n    @defer.inlineCallbacks\n    def test_download_media_wrong_urls(self):\n        crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n            )\n        self._assert_files_download_failure(crawler, self.items, 404, str(log))\n\n    @defer.inlineCallbacks\n    def test_download_media_redirected_default_failure(self):\n        crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n                mockserver=self.mockserver,\n            )\n        self._assert_files_download_failure(crawler, self.items, 302, str(log))\n\n    @defer.inlineCallbacks\n    def test_download_media_redirected_allowed(self):\n        settings = dict(self.settings)\n        settings.update({\"MEDIA_ALLOW_REDIRECTS\": True})\n        runner = CrawlerRunner(settings)\n        crawler = self._create_crawler(RedirectedMediaDownloadSpider, runner=runner)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n                mockserver=self.mockserver,\n            )\n        self._assert_files_downloaded(self.items, str(log))\n        self.assertEqual(\n            crawler.stats.get_value(\"downloader/response_status_count/302\"), 3\n        )\n\n    @defer.inlineCallbacks\n    def test_download_media_file_path_error(self):\n        cls = load_object(self.pipeline_class)\n\n        class ExceptionRaisingMediaPipeline(cls):\n            def file_path(self, request, response=None, info=None, *, item=None):\n                return 1 / 0\n\n        settings = {\n            **self.settings,\n            \"ITEM_PIPELINES\": {ExceptionRaisingMediaPipeline: 1},\n        }\n        runner = CrawlerRunner(settings)\n        crawler = self._create_crawler(MediaDownloadSpider, runner=runner)\n        with LogCapture() as log:\n            yield crawler.crawl(\n                self.mockserver.url(\"/files/images/\"),\n                media_key=self.media_key,\n                media_urls_key=self.media_urls_key,\n                mockserver=self.mockserver,\n            )\n        self.assertIn(\"ZeroDivisionError\", str(log))\n\n\nskip_pillow: Optional[str]\ntry:\n    from PIL import Image  # noqa: imported just to check for the import error\nexcept ImportError:\n    skip_pillow = (\n        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n    )\nelse:\n    skip_pillow = None\n\n\nclass ImageDownloadCrawlTestCase(FileDownloadCrawlTestCase):\n    skip = skip_pillow\n\n    pipeline_class = \"scrapy.pipelines.images.ImagesPipeline\"\n    store_setting_key = \"IMAGES_STORE\"\n    media_key = \"images\"\n    media_urls_key = \"image_urls\"\n\n    # somehow checksums for images are different for Python 3.3\n    expected_checksums = None\n", "tests/test_downloaderslotssettings.py": "import time\n\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy import Request\nfrom scrapy.core.downloader import Downloader, Slot\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import MetaSpider\n\n\nclass DownloaderSlotsSettingsTestSpider(MetaSpider):\n    name = \"downloader_slots\"\n\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 1,\n        \"RANDOMIZE_DOWNLOAD_DELAY\": False,\n        \"DOWNLOAD_SLOTS\": {\n            \"quotes.toscrape.com\": {\n                \"concurrency\": 1,\n                \"delay\": 2,\n                \"randomize_delay\": False,\n                \"throttle\": False,\n            },\n            \"books.toscrape.com\": {\"delay\": 3, \"randomize_delay\": False},\n        },\n    }\n\n    def start_requests(self):\n        self.times = {None: []}\n\n        slots = list(self.custom_settings.get(\"DOWNLOAD_SLOTS\", {}).keys()) + [None]\n\n        for slot in slots:\n            url = self.mockserver.url(f\"/?downloader_slot={slot}\")\n            self.times[slot] = []\n            yield Request(url, callback=self.parse, meta={\"download_slot\": slot})\n\n    def parse(self, response):\n        slot = response.meta.get(\"download_slot\", None)\n        self.times[slot].append(time.time())\n        url = self.mockserver.url(f\"/?downloader_slot={slot}&req=2\")\n        yield Request(url, callback=self.not_parse, meta={\"download_slot\": slot})\n\n    def not_parse(self, response):\n        slot = response.meta.get(\"download_slot\", None)\n        self.times[slot].append(time.time())\n\n\nclass CrawlTestCase(TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self.runner = CrawlerRunner()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_delay(self):\n        crawler = CrawlerRunner().create_crawler(DownloaderSlotsSettingsTestSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        slots = crawler.engine.downloader.slots\n        times = crawler.spider.times\n        tolerance = 0.3\n\n        delays_real = {k: v[1] - v[0] for k, v in times.items()}\n        error_delta = {\n            k: 1 - min(delays_real[k], v.delay) / max(delays_real[k], v.delay)\n            for k, v in slots.items()\n        }\n\n        self.assertTrue(max(list(error_delta.values())) < tolerance)\n\n\ndef test_params():\n    params = {\n        \"concurrency\": 1,\n        \"delay\": 2,\n        \"randomize_delay\": False,\n        \"throttle\": False,\n    }\n    settings = {\n        \"DOWNLOAD_SLOTS\": {\n            \"example.com\": params,\n        },\n    }\n    crawler = get_crawler(settings_dict=settings)\n    downloader = Downloader(crawler)\n    downloader._slot_gc_loop.stop()  # Prevent an unclean reactor.\n    request = Request(\"https://example.com\")\n    _, actual = downloader._get_slot(request, spider=None)\n    expected = Slot(**params)\n    for param in params:\n        assert getattr(expected, param) == getattr(\n            actual, param\n        ), f\"Slot.{param}: {getattr(expected, param)!r} != {getattr(actual, param)!r}\"\n", "tests/test_downloadermiddleware_robotstxt.py": "from unittest import mock\n\nfrom twisted.internet import error, reactor\nfrom twisted.internet.defer import Deferred, DeferredList, maybeDeferred\nfrom twisted.python import failure\nfrom twisted.trial import unittest\n\nfrom scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware\nfrom scrapy.downloadermiddlewares.robotstxt import logger as mw_module_logger\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request, Response, TextResponse\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.settings import Settings\nfrom tests.test_robotstxt_interface import reppy_available, rerp_available\n\n\nclass RobotsTxtMiddlewareTest(unittest.TestCase):\n    def setUp(self):\n        self.crawler = mock.MagicMock()\n        self.crawler.settings = Settings()\n        self.crawler.engine.download = mock.MagicMock()\n\n    def tearDown(self):\n        del self.crawler\n\n    def test_robotstxt_settings(self):\n        self.crawler.settings = Settings()\n        self.crawler.settings.set(\"USER_AGENT\", \"CustomAgent\")\n        self.assertRaises(NotConfigured, RobotsTxtMiddleware, self.crawler)\n\n    def _get_successful_crawler(self):\n        crawler = self.crawler\n        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        ROBOTS = \"\"\"\nUser-Agent: *\nDisallow: /admin/\nDisallow: /static/\n# taken from https://en.wikipedia.org/robots.txt\nDisallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\nDisallow: /wiki/K\u00e4ytt\u00e4j\u00e4:\nUser-Agent: Unic\u00f6deB\u00f6t\nDisallow: /some/randome/page.html\n\"\"\".encode()\n        response = TextResponse(\"http://site.local/robots.txt\", body=ROBOTS)\n\n        def return_response(request):\n            deferred = Deferred()\n            reactor.callFromThread(deferred.callback, response)\n            return deferred\n\n        crawler.engine.download.side_effect = return_response\n        return crawler\n\n    def test_robotstxt(self):\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        return DeferredList(\n            [\n                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n                maybeDeferred(self.assertRobotsTxtRequested, \"http://site.local\"),\n                self.assertIgnored(Request(\"http://site.local/admin/main\"), middleware),\n                self.assertIgnored(Request(\"http://site.local/static/\"), middleware),\n                self.assertIgnored(\n                    Request(\"http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\"),\n                    middleware,\n                ),\n                self.assertIgnored(\n                    Request(\"http://site.local/wiki/K\u00e4ytt\u00e4j\u00e4:\"), middleware\n                ),\n            ],\n            fireOnOneErrback=True,\n        )\n\n    def test_robotstxt_ready_parser(self):\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n        d.addCallback(\n            lambda _: self.assertNotIgnored(\n                Request(\"http://site.local/allowed\"), middleware\n            )\n        )\n        return d\n\n    def test_robotstxt_meta(self):\n        middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n        meta = {\"dont_obey_robotstxt\": True}\n        return DeferredList(\n            [\n                self.assertNotIgnored(\n                    Request(\"http://site.local/allowed\", meta=meta), middleware\n                ),\n                self.assertNotIgnored(\n                    Request(\"http://site.local/admin/main\", meta=meta), middleware\n                ),\n                self.assertNotIgnored(\n                    Request(\"http://site.local/static/\", meta=meta), middleware\n                ),\n            ],\n            fireOnOneErrback=True,\n        )\n\n    def _get_garbage_crawler(self):\n        crawler = self.crawler\n        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        response = Response(\n            \"http://site.local/robots.txt\", body=b\"GIF89a\\xd3\\x00\\xfe\\x00\\xa2\"\n        )\n\n        def return_response(request):\n            deferred = Deferred()\n            reactor.callFromThread(deferred.callback, response)\n            return deferred\n\n        crawler.engine.download.side_effect = return_response\n        return crawler\n\n    def test_robotstxt_garbage(self):\n        # garbage response should be discarded, equal 'allow all'\n        middleware = RobotsTxtMiddleware(self._get_garbage_crawler())\n        deferred = DeferredList(\n            [\n                self.assertNotIgnored(Request(\"http://site.local\"), middleware),\n                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n                self.assertNotIgnored(\n                    Request(\"http://site.local/admin/main\"), middleware\n                ),\n                self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware),\n            ],\n            fireOnOneErrback=True,\n        )\n        return deferred\n\n    def _get_emptybody_crawler(self):\n        crawler = self.crawler\n        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        response = Response(\"http://site.local/robots.txt\")\n\n        def return_response(request):\n            deferred = Deferred()\n            reactor.callFromThread(deferred.callback, response)\n            return deferred\n\n        crawler.engine.download.side_effect = return_response\n        return crawler\n\n    def test_robotstxt_empty_response(self):\n        # empty response should equal 'allow all'\n        middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n        return DeferredList(\n            [\n                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n                self.assertNotIgnored(\n                    Request(\"http://site.local/admin/main\"), middleware\n                ),\n                self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware),\n            ],\n            fireOnOneErrback=True,\n        )\n\n    def test_robotstxt_error(self):\n        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        err = error.DNSLookupError(\"Robotstxt address not found\")\n\n        def return_failure(request):\n            deferred = Deferred()\n            reactor.callFromThread(deferred.errback, failure.Failure(err))\n            return deferred\n\n        self.crawler.engine.download.side_effect = return_failure\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)\n        deferred = middleware.process_request(Request(\"http://site.local\"), None)\n        deferred.addCallback(lambda _: self.assertTrue(middleware._logerror.called))\n        return deferred\n\n    def test_robotstxt_immediate_error(self):\n        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n        err = error.DNSLookupError(\"Robotstxt address not found\")\n\n        def immediate_failure(request):\n            deferred = Deferred()\n            deferred.errback(failure.Failure(err))\n            return deferred\n\n        self.crawler.engine.download.side_effect = immediate_failure\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        return self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n\n    def test_ignore_robotstxt_request(self):\n        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n\n        def ignore_request(request):\n            deferred = Deferred()\n            reactor.callFromThread(deferred.errback, failure.Failure(IgnoreRequest()))\n            return deferred\n\n        self.crawler.engine.download.side_effect = ignore_request\n\n        middleware = RobotsTxtMiddleware(self.crawler)\n        mw_module_logger.error = mock.MagicMock()\n\n        d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n        d.addCallback(lambda _: self.assertFalse(mw_module_logger.error.called))\n        return d\n\n    def test_robotstxt_user_agent_setting(self):\n        crawler = self._get_successful_crawler()\n        crawler.settings.set(\"ROBOTSTXT_USER_AGENT\", \"Examplebot\")\n        crawler.settings.set(\"USER_AGENT\", \"Mozilla/5.0 (X11; Linux x86_64)\")\n        middleware = RobotsTxtMiddleware(crawler)\n        rp = mock.MagicMock(return_value=True)\n        middleware.process_request_2(rp, Request(\"http://site.local/allowed\"), None)\n        rp.allowed.assert_called_once_with(\"http://site.local/allowed\", \"Examplebot\")\n\n    def test_robotstxt_local_file(self):\n        middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n        assert not middleware.process_request(\n            Request(\"data:text/plain,Hello World data\"), None\n        )\n        assert not middleware.process_request(\n            Request(\"file:///tests/sample_data/test_site/nothinghere.html\"), None\n        )\n        assert isinstance(\n            middleware.process_request(Request(\"http://site.local/allowed\"), None),\n            Deferred,\n        )\n\n    def assertNotIgnored(self, request, middleware):\n        spider = None  # not actually used\n        dfd = maybeDeferred(middleware.process_request, request, spider)\n        dfd.addCallback(self.assertIsNone)\n        return dfd\n\n    def assertIgnored(self, request, middleware):\n        spider = None  # not actually used\n        return self.assertFailure(\n            maybeDeferred(middleware.process_request, request, spider), IgnoreRequest\n        )\n\n    def assertRobotsTxtRequested(self, base_url):\n        calls = self.crawler.engine.download.call_args_list\n        request = calls[0][0][0]\n        self.assertEqual(request.url, f\"{base_url}/robots.txt\")\n        self.assertEqual(request.callback, NO_CALLBACK)\n\n\nclass RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n    if not rerp_available():\n        skip = \"Rerp parser is not installed\"\n\n    def setUp(self):\n        super().setUp()\n        self.crawler.settings.set(\n            \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.RerpRobotParser\"\n        )\n\n\nclass RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):\n    if not reppy_available():\n        skip = \"Reppy parser is not installed\"\n\n    def setUp(self):\n        super().setUp()\n        self.crawler.settings.set(\n            \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.ReppyRobotParser\"\n        )\n", "tests/test_spidermiddleware.py": "import collections.abc\nfrom typing import Optional\nfrom unittest import mock\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.core.spidermw import SpiderMiddlewareManager\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.asyncgen import collect_asyncgen\nfrom scrapy.utils.defer import deferred_from_coro, maybe_deferred_to_future\nfrom scrapy.utils.test import get_crawler\n\n\nclass SpiderMiddlewareTestCase(TestCase):\n    def setUp(self):\n        self.request = Request(\"http://example.com/index.html\")\n        self.response = Response(self.request.url, request=self.request)\n        self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}})\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n\n    def _scrape_response(self):\n        \"\"\"Execute spider mw manager's scrape_response method and return the result.\n        Raise exception in case of failure.\n        \"\"\"\n        scrape_func = mock.MagicMock()\n        dfd = self.mwman.scrape_response(\n            scrape_func, self.response, self.request, self.spider\n        )\n        # catch deferred result and return the value\n        results = []\n        dfd.addBoth(results.append)\n        self._wait(dfd)\n        ret = results[0]\n        return ret\n\n\nclass ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):\n    \"\"\"Invalid return value for process_spider_input method\"\"\"\n\n    def test_invalid_process_spider_input(self):\n        class InvalidProcessSpiderInputMiddleware:\n            def process_spider_input(self, response, spider):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessSpiderInputMiddleware())\n        result = self._scrape_response()\n        self.assertIsInstance(result, Failure)\n        self.assertIsInstance(result.value, _InvalidOutput)\n\n\nclass ProcessSpiderOutputInvalidOutput(SpiderMiddlewareTestCase):\n    \"\"\"Invalid return value for process_spider_output method\"\"\"\n\n    def test_invalid_process_spider_output(self):\n        class InvalidProcessSpiderOutputMiddleware:\n            def process_spider_output(self, response, result, spider):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessSpiderOutputMiddleware())\n        result = self._scrape_response()\n        self.assertIsInstance(result, Failure)\n        self.assertIsInstance(result.value, _InvalidOutput)\n\n\nclass ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n    \"\"\"Invalid return value for process_spider_exception method\"\"\"\n\n    def test_invalid_process_spider_exception(self):\n        class InvalidProcessSpiderOutputExceptionMiddleware:\n            def process_spider_exception(self, response, exception, spider):\n                return 1\n\n        class RaiseExceptionProcessSpiderOutputMiddleware:\n            def process_spider_output(self, response, result, spider):\n                raise Exception()\n\n        self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n        result = self._scrape_response()\n        self.assertIsInstance(result, Failure)\n        self.assertIsInstance(result.value, _InvalidOutput)\n\n\nclass ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n    \"\"\"Re raise the exception by returning None\"\"\"\n\n    def test_process_spider_exception_return_none(self):\n        class ProcessSpiderExceptionReturnNoneMiddleware:\n            def process_spider_exception(self, response, exception, spider):\n                return None\n\n        class RaiseExceptionProcessSpiderOutputMiddleware:\n            def process_spider_output(self, response, result, spider):\n                1 / 0\n\n        self.mwman._add_middleware(ProcessSpiderExceptionReturnNoneMiddleware())\n        self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n        result = self._scrape_response()\n        self.assertIsInstance(result, Failure)\n        self.assertIsInstance(result.value, ZeroDivisionError)\n\n\nclass BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n    \"\"\"Helpers for testing sync, async and mixed middlewares.\n\n    Should work for process_spider_output and, when it's supported, process_start_requests.\n    \"\"\"\n\n    ITEM_TYPE: type\n    RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects\n\n    @staticmethod\n    def _construct_mw_setting(*mw_classes, start_index: Optional[int] = None):\n        if start_index is None:\n            start_index = 10\n        return {i: c for c, i in enumerate(mw_classes, start=start_index)}\n\n    def _scrape_func(self, *args, **kwargs):\n        yield {\"foo\": 1}\n        yield {\"foo\": 2}\n        yield {\"foo\": 3}\n\n    @defer.inlineCallbacks\n    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n        self.crawler = get_crawler(\n            Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n        )\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n        result = yield self.mwman.scrape_response(\n            self._scrape_func, self.response, self.request, self.spider\n        )\n        return result\n\n    @defer.inlineCallbacks\n    def _test_simple_base(\n        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None\n    ):\n        with LogCapture() as log:\n            result = yield self._get_middleware_result(\n                *mw_classes, start_index=start_index\n            )\n        self.assertIsInstance(result, collections.abc.Iterable)\n        result_list = list(result)\n        self.assertEqual(len(result_list), self.RESULT_COUNT)\n        self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n        self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n\n    @defer.inlineCallbacks\n    def _test_asyncgen_base(\n        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None\n    ):\n        with LogCapture() as log:\n            result = yield self._get_middleware_result(\n                *mw_classes, start_index=start_index\n            )\n        self.assertIsInstance(result, collections.abc.AsyncIterator)\n        result_list = yield deferred_from_coro(collect_asyncgen(result))\n        self.assertEqual(len(result_list), self.RESULT_COUNT)\n        self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n        self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n\n\nclass ProcessSpiderOutputSimpleMiddleware:\n    def process_spider_output(self, response, result, spider):\n        yield from result\n\n\nclass ProcessSpiderOutputAsyncGenMiddleware:\n    async def process_spider_output(self, response, result, spider):\n        async for r in result:\n            yield r\n\n\nclass ProcessSpiderOutputUniversalMiddleware:\n    def process_spider_output(self, response, result, spider):\n        yield from result\n\n    async def process_spider_output_async(self, response, result, spider):\n        async for r in result:\n            yield r\n\n\nclass ProcessSpiderExceptionSimpleIterableMiddleware:\n    def process_spider_exception(self, response, exception, spider):\n        yield {\"foo\": 1}\n        yield {\"foo\": 2}\n        yield {\"foo\": 3}\n\n\nclass ProcessSpiderExceptionAsyncIterableMiddleware:\n    async def process_spider_exception(self, response, exception, spider):\n        yield {\"foo\": 1}\n        d = defer.Deferred()\n        from twisted.internet import reactor\n\n        reactor.callLater(0, d.callback, None)\n        await maybe_deferred_to_future(d)\n        yield {\"foo\": 2}\n        yield {\"foo\": 3}\n\n\nclass ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n    \"\"\"process_spider_output tests for simple callbacks\"\"\"\n\n    ITEM_TYPE = dict\n    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n\n    def test_simple(self):\n        \"\"\"Simple mw\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE)\n\n    def test_asyncgen(self):\n        \"\"\"Asyncgen mw; upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN)\n\n    def test_simple_asyncgen(self):\n        \"\"\"Simple mw -> asyncgen mw; upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_SIMPLE)\n\n    def test_asyncgen_simple(self):\n        \"\"\"Asyncgen mw -> simple mw; upgrade then downgrade\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, self.MW_ASYNCGEN, downgrade=True)\n\n    def test_universal(self):\n        \"\"\"Universal mw\"\"\"\n        return self._test_simple_base(self.MW_UNIVERSAL)\n\n    def test_universal_simple(self):\n        \"\"\"Universal mw -> simple mw\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL)\n\n    def test_simple_universal(self):\n        \"\"\"Simple mw -> universal mw\"\"\"\n        return self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE)\n\n    def test_universal_asyncgen(self):\n        \"\"\"Universal mw -> asyncgen mw; upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_UNIVERSAL)\n\n    def test_asyncgen_universal(self):\n        \"\"\"Asyncgen mw -> universal mw; upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_UNIVERSAL, self.MW_ASYNCGEN)\n\n\nclass ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n    \"\"\"process_spider_output tests for async generator callbacks\"\"\"\n\n    async def _scrape_func(self, *args, **kwargs):\n        for item in super()._scrape_func():\n            yield item\n\n    def test_simple(self):\n        \"\"\"Simple mw; downgrade\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, downgrade=True)\n\n    def test_simple_asyncgen(self):\n        \"\"\"Simple mw -> asyncgen mw; downgrade then upgrade\"\"\"\n        return self._test_asyncgen_base(\n            self.MW_ASYNCGEN, self.MW_SIMPLE, downgrade=True\n        )\n\n    def test_universal(self):\n        \"\"\"Universal mw\"\"\"\n        return self._test_asyncgen_base(self.MW_UNIVERSAL)\n\n    def test_universal_simple(self):\n        \"\"\"Universal mw -> simple mw; downgrade\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL, downgrade=True)\n\n    def test_simple_universal(self):\n        \"\"\"Simple mw -> universal mw; downgrade\"\"\"\n        return self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE, downgrade=True)\n\n\nclass ProcessSpiderOutputNonIterableMiddleware:\n    def process_spider_output(self, response, result, spider):\n        return\n\n\nclass ProcessSpiderOutputCoroutineMiddleware:\n    async def process_spider_output(self, response, result, spider):\n        results = []\n        for r in result:\n            results.append(r)\n        return results\n\n\nclass ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n    @defer.inlineCallbacks\n    def test_non_iterable(self):\n        with self.assertRaisesRegex(\n            _InvalidOutput,\n            (\n                r\"\\.process_spider_output must return an iterable, got <class \"\n                r\"'NoneType'>\"\n            ),\n        ):\n            yield self._get_middleware_result(\n                ProcessSpiderOutputNonIterableMiddleware,\n            )\n\n    @defer.inlineCallbacks\n    def test_coroutine(self):\n        with self.assertRaisesRegex(\n            _InvalidOutput,\n            r\"\\.process_spider_output must be an asynchronous generator\",\n        ):\n            yield self._get_middleware_result(\n                ProcessSpiderOutputCoroutineMiddleware,\n            )\n\n\nclass ProcessStartRequestsSimpleMiddleware:\n    def process_start_requests(self, start_requests, spider):\n        yield from start_requests\n\n\nclass ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n    \"\"\"process_start_requests tests for simple start_requests\"\"\"\n\n    ITEM_TYPE = Request\n    MW_SIMPLE = ProcessStartRequestsSimpleMiddleware\n\n    def _start_requests(self):\n        for i in range(3):\n            yield Request(f\"https://example.com/{i}\", dont_filter=True)\n\n    @defer.inlineCallbacks\n    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n        self.crawler = get_crawler(\n            Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n        )\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n        start_requests = iter(self._start_requests())\n        results = yield self.mwman.process_start_requests(start_requests, self.spider)\n        return results\n\n    def test_simple(self):\n        \"\"\"Simple mw\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE)\n\n\nclass UniversalMiddlewareNoSync:\n    async def process_spider_output_async(self, response, result, spider):\n        yield\n\n\nclass UniversalMiddlewareBothSync:\n    def process_spider_output(self, response, result, spider):\n        yield\n\n    def process_spider_output_async(self, response, result, spider):\n        yield\n\n\nclass UniversalMiddlewareBothAsync:\n    async def process_spider_output(self, response, result, spider):\n        yield\n\n    async def process_spider_output_async(self, response, result, spider):\n        yield\n\n\nclass UniversalMiddlewareManagerTest(TestCase):\n    def setUp(self):\n        self.mwman = SpiderMiddlewareManager()\n\n    def test_simple_mw(self):\n        mw = ProcessSpiderOutputSimpleMiddleware\n        self.mwman._add_middleware(mw)\n        self.assertEqual(\n            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n        )\n\n    def test_async_mw(self):\n        mw = ProcessSpiderOutputAsyncGenMiddleware\n        self.mwman._add_middleware(mw)\n        self.assertEqual(\n            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n        )\n\n    def test_universal_mw(self):\n        mw = ProcessSpiderOutputUniversalMiddleware\n        self.mwman._add_middleware(mw)\n        self.assertEqual(\n            self.mwman.methods[\"process_spider_output\"][0],\n            (mw.process_spider_output, mw.process_spider_output_async),\n        )\n\n    def test_universal_mw_no_sync(self):\n        with LogCapture() as log:\n            self.mwman._add_middleware(UniversalMiddlewareNoSync)\n        self.assertIn(\n            \"UniversalMiddlewareNoSync has process_spider_output_async\"\n            \" without process_spider_output\",\n            str(log),\n        )\n        self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n\n    def test_universal_mw_both_sync(self):\n        mw = UniversalMiddlewareBothSync\n        with LogCapture() as log:\n            self.mwman._add_middleware(mw)\n        self.assertIn(\n            \"UniversalMiddlewareBothSync.process_spider_output_async \"\n            \"is not an async generator function\",\n            str(log),\n        )\n        self.assertEqual(\n            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n        )\n\n    def test_universal_mw_both_async(self):\n        with LogCapture() as log:\n            self.mwman._add_middleware(UniversalMiddlewareBothAsync)\n        self.assertIn(\n            \"UniversalMiddlewareBothAsync.process_spider_output \"\n            \"is an async generator function while process_spider_output_async exists\",\n            str(log),\n        )\n        self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n\n\nclass BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n    ITEM_TYPE = dict\n    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n\n    @defer.inlineCallbacks\n    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n        self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES\": setting})\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n        result = yield self.mwman.scrape_response(\n            self._scrape_func, self.response, self.request, self.spider\n        )\n        return result\n\n    def test_just_builtin(self):\n        return self._test_simple_base()\n\n    def test_builtin_simple(self):\n        return self._test_simple_base(self.MW_SIMPLE, start_index=1000)\n\n    def test_builtin_async(self):\n        \"\"\"Upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)\n\n    def test_builtin_universal(self):\n        return self._test_simple_base(self.MW_UNIVERSAL, start_index=1000)\n\n    def test_simple_builtin(self):\n        return self._test_simple_base(self.MW_SIMPLE)\n\n    def test_async_builtin(self):\n        \"\"\"Upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN)\n\n    def test_universal_builtin(self):\n        return self._test_simple_base(self.MW_UNIVERSAL)\n\n\nclass BuiltinMiddlewareAsyncGenTest(BuiltinMiddlewareSimpleTest):\n    async def _scrape_func(self, *args, **kwargs):\n        for item in super()._scrape_func():\n            yield item\n\n    def test_just_builtin(self):\n        return self._test_asyncgen_base()\n\n    def test_builtin_simple(self):\n        \"\"\"Downgrade\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, downgrade=True, start_index=1000)\n\n    def test_builtin_async(self):\n        return self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)\n\n    def test_builtin_universal(self):\n        return self._test_asyncgen_base(self.MW_UNIVERSAL, start_index=1000)\n\n    def test_simple_builtin(self):\n        \"\"\"Downgrade\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, downgrade=True)\n\n    def test_async_builtin(self):\n        return self._test_asyncgen_base(self.MW_ASYNCGEN)\n\n    def test_universal_builtin(self):\n        return self._test_asyncgen_base(self.MW_UNIVERSAL)\n\n\nclass ProcessSpiderExceptionTest(BaseAsyncSpiderMiddlewareTestCase):\n    ITEM_TYPE = dict\n    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n    MW_EXC_SIMPLE = ProcessSpiderExceptionSimpleIterableMiddleware\n    MW_EXC_ASYNCGEN = ProcessSpiderExceptionAsyncIterableMiddleware\n\n    def _scrape_func(self, *args, **kwargs):\n        1 / 0\n\n    @defer.inlineCallbacks\n    def _test_asyncgen_nodowngrade(self, *mw_classes):\n        with self.assertRaisesRegex(\n            _InvalidOutput, \"Async iterable returned from .+ cannot be downgraded\"\n        ):\n            yield self._get_middleware_result(*mw_classes)\n\n    def test_exc_simple(self):\n        \"\"\"Simple exc mw\"\"\"\n        return self._test_simple_base(self.MW_EXC_SIMPLE)\n\n    def test_exc_async(self):\n        \"\"\"Async exc mw\"\"\"\n        return self._test_asyncgen_base(self.MW_EXC_ASYNCGEN)\n\n    def test_exc_simple_simple(self):\n        \"\"\"Simple exc mw -> simple output mw\"\"\"\n        return self._test_simple_base(self.MW_SIMPLE, self.MW_EXC_SIMPLE)\n\n    def test_exc_async_async(self):\n        \"\"\"Async exc mw -> async output mw\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_ASYNCGEN)\n\n    def test_exc_simple_async(self):\n        \"\"\"Simple exc mw -> async output mw; upgrade\"\"\"\n        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_SIMPLE)\n\n    def test_exc_async_simple(self):\n        \"\"\"Async exc mw -> simple output mw; cannot work as downgrading is not supported\"\"\"\n        return self._test_asyncgen_nodowngrade(self.MW_SIMPLE, self.MW_EXC_ASYNCGEN)\n", "tests/test_item.py": "import unittest\nfrom unittest import mock\n\nfrom scrapy.item import ABCMeta, Field, Item, ItemMeta\n\n\nclass ItemTest(unittest.TestCase):\n    def assertSortedEqual(self, first, second, msg=None):\n        return self.assertEqual(sorted(first), sorted(second), msg)\n\n    def test_simple(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"name\"\n        self.assertEqual(i[\"name\"], \"name\")\n\n    def test_init(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        self.assertRaises(KeyError, i.__getitem__, \"name\")\n\n        i2 = TestItem(name=\"john doe\")\n        self.assertEqual(i2[\"name\"], \"john doe\")\n\n        i3 = TestItem({\"name\": \"john doe\"})\n        self.assertEqual(i3[\"name\"], \"john doe\")\n\n        i4 = TestItem(i3)\n        self.assertEqual(i4[\"name\"], \"john doe\")\n\n        self.assertRaises(KeyError, TestItem, {\"name\": \"john doe\", \"other\": \"foo\"})\n\n    def test_invalid_field(self):\n        class TestItem(Item):\n            pass\n\n        i = TestItem()\n        self.assertRaises(KeyError, i.__setitem__, \"field\", \"text\")\n        self.assertRaises(KeyError, i.__getitem__, \"field\")\n\n    def test_repr(self):\n        class TestItem(Item):\n            name = Field()\n            number = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John Doe\"\n        i[\"number\"] = 123\n        itemrepr = repr(i)\n\n        self.assertEqual(itemrepr, \"{'name': 'John Doe', 'number': 123}\")\n\n        i2 = eval(itemrepr)\n        self.assertEqual(i2[\"name\"], \"John Doe\")\n        self.assertEqual(i2[\"number\"], 123)\n\n    def test_private_attr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i._private = \"test\"\n        self.assertEqual(i._private, \"test\")\n\n    def test_raise_getattr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        self.assertRaises(AttributeError, getattr, i, \"name\")\n\n    def test_raise_setattr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        self.assertRaises(AttributeError, setattr, i, \"name\", \"john\")\n\n    def test_custom_methods(self):\n        class TestItem(Item):\n            name = Field()\n\n            def get_name(self):\n                return self[\"name\"]\n\n            def change_name(self, name):\n                self[\"name\"] = name\n\n        i = TestItem()\n        self.assertRaises(KeyError, i.get_name)\n        i[\"name\"] = \"lala\"\n        self.assertEqual(i.get_name(), \"lala\")\n        i.change_name(\"other\")\n        self.assertEqual(i.get_name(), \"other\")\n\n    def test_metaclass(self):\n        class TestItem(Item):\n            name = Field()\n            keys = Field()\n            values = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John\"\n        self.assertEqual(list(i.keys()), [\"name\"])\n        self.assertEqual(list(i.values()), [\"John\"])\n\n        i[\"keys\"] = \"Keys\"\n        i[\"values\"] = \"Values\"\n        self.assertSortedEqual(list(i.keys()), [\"keys\", \"values\", \"name\"])\n        self.assertSortedEqual(list(i.values()), [\"Keys\", \"Values\", \"John\"])\n\n    def test_metaclass_with_fields_attribute(self):\n        class TestItem(Item):\n            fields = {\"new\": Field(default=\"X\")}\n\n        item = TestItem(new=\"New\")\n        self.assertSortedEqual(list(item.keys()), [\"new\"])\n        self.assertSortedEqual(list(item.values()), [\"New\"])\n\n    def test_metaclass_inheritance(self):\n        class ParentItem(Item):\n            name = Field()\n            keys = Field()\n            values = Field()\n\n        class TestItem(ParentItem):\n            keys = Field()\n\n        i = TestItem()\n        i[\"keys\"] = 3\n        self.assertEqual(list(i.keys()), [\"keys\"])\n        self.assertEqual(list(i.values()), [3])\n\n    def test_metaclass_multiple_inheritance_simple(self):\n        class A(Item):\n            fields = {\"load\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C(Item):\n            fields = {\"load\": Field(default=\"C\")}\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            pass\n\n        item = D(save=\"X\", load=\"Y\")\n        self.assertEqual(item[\"save\"], \"X\")\n        self.assertEqual(item[\"load\"], \"Y\")\n        self.assertEqual(D.fields, {\"load\": {\"default\": \"A\"}, \"save\": {\"default\": \"A\"}})\n\n        # D class inverted\n        class E(C, B):\n            pass\n\n        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(E(load=\"X\")[\"load\"], \"X\")\n        self.assertEqual(E.fields, {\"load\": {\"default\": \"C\"}, \"save\": {\"default\": \"C\"}})\n\n    def test_metaclass_multiple_inheritance_diamond(self):\n        class A(Item):\n            fields = {\"update\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n            load = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C(A):\n            fields = {\"update\": Field(default=\"C\")}\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            fields = {\"update\": Field(default=\"D\")}\n            load = Field(default=\"D\")\n\n        self.assertEqual(D(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(D(load=\"X\")[\"load\"], \"X\")\n        self.assertEqual(\n            D.fields,\n            {\n                \"save\": {\"default\": \"C\"},\n                \"load\": {\"default\": \"D\"},\n                \"update\": {\"default\": \"D\"},\n            },\n        )\n\n        # D class inverted\n        class E(C, B):\n            load = Field(default=\"E\")\n\n        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(E(load=\"X\")[\"load\"], \"X\")\n        self.assertEqual(\n            E.fields,\n            {\n                \"save\": {\"default\": \"C\"},\n                \"load\": {\"default\": \"E\"},\n                \"update\": {\"default\": \"C\"},\n            },\n        )\n\n    def test_metaclass_multiple_inheritance_without_metaclass(self):\n        class A(Item):\n            fields = {\"load\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C:\n            fields = {\"load\": Field(default=\"C\")}\n            not_allowed = Field(default=\"not_allowed\")\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            pass\n\n        self.assertRaises(KeyError, D, not_allowed=\"value\")\n        self.assertEqual(D(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(D.fields, {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}})\n\n        # D class inverted\n        class E(C, B):\n            pass\n\n        self.assertRaises(KeyError, E, not_allowed=\"value\")\n        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(E.fields, {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}})\n\n    def test_to_dict(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John\"\n        self.assertEqual(dict(i), {\"name\": \"John\"})\n\n    def test_copy(self):\n        class TestItem(Item):\n            name = Field()\n\n        item = TestItem({\"name\": \"lower\"})\n        copied_item = item.copy()\n        self.assertNotEqual(id(item), id(copied_item))\n        copied_item[\"name\"] = copied_item[\"name\"].upper()\n        self.assertNotEqual(item[\"name\"], copied_item[\"name\"])\n\n    def test_deepcopy(self):\n        class TestItem(Item):\n            tags = Field()\n\n        item = TestItem({\"tags\": [\"tag1\"]})\n        copied_item = item.deepcopy()\n        item[\"tags\"].append(\"tag2\")\n        assert item[\"tags\"] != copied_item[\"tags\"]\n\n\nclass ItemMetaTest(unittest.TestCase):\n    def test_new_method_propagates_classcell(self):\n        new_mock = mock.Mock(side_effect=ABCMeta.__new__)\n        base = ItemMeta.__bases__[0]\n\n        with mock.patch.object(base, \"__new__\", new_mock):\n\n            class MyItem(Item):\n                def f(self):\n                    # For rationale of this see:\n                    # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n                    return (\n                        __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836\n                    )\n\n            MyItem()\n\n        (first_call, second_call) = new_mock.call_args_list[-2:]\n\n        mcs, class_name, bases, attrs = first_call[0]\n        assert \"__classcell__\" not in attrs\n        mcs, class_name, bases, attrs = second_call[0]\n        assert \"__classcell__\" in attrs\n\n\nclass ItemMetaClassCellRegression(unittest.TestCase):\n    def test_item_meta_classcell_regression(self):\n        class MyItem(Item, metaclass=ItemMeta):\n            def __init__(\n                self, *args, **kwargs\n            ):  # pylint: disable=useless-parent-delegation\n                # This call to super() trigger the __classcell__ propagation\n                # requirement. When not done properly raises an error:\n                # TypeError: __class__ set to <class '__main__.MyItem'>\n                # defining 'MyItem' as <class '__main__.MyItem'>\n                super().__init__(*args, **kwargs)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_engine.py": "\"\"\"\nScrapy engine tests\n\nThis starts a testing web server (using twisted.server.Site) and then crawls it\nwith the Scrapy crawler.\n\nTo view the testing web server in a browser you can start it by running this\nmodule with the ``runserver`` argument::\n\n    python test_engine.py runserver\n\"\"\"\n\nimport re\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom logging import DEBUG\nfrom pathlib import Path\nfrom threading import Timer\nfrom unittest.mock import Mock\nfrom urllib.parse import urlparse\n\nimport attr\nfrom itemadapter import ItemAdapter\nfrom pydispatch import dispatcher\nfrom twisted.internet import defer, reactor\nfrom twisted.trial import unittest\nfrom twisted.web import server, static, util\n\nfrom scrapy import signals\nfrom scrapy.core.engine import ExecutionEngine, Slot\nfrom scrapy.core.scheduler import BaseScheduler\nfrom scrapy.exceptions import CloseSpider, IgnoreRequest\nfrom scrapy.http import Request\nfrom scrapy.item import Field, Item\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.signals import request_scheduled\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.signal import disconnect_all\nfrom scrapy.utils.test import get_crawler\nfrom tests import get_testdata, tests_datadir\n\n\nclass TestItem(Item):\n    name = Field()\n    url = Field()\n    price = Field()\n\n\n@attr.s\nclass AttrsItem:\n    name = attr.ib(default=\"\")\n    url = attr.ib(default=\"\")\n    price = attr.ib(default=0)\n\n\n@dataclass\nclass DataClassItem:\n    name: str = \"\"\n    url: str = \"\"\n    price: int = 0\n\n\nclass TestSpider(Spider):\n    name = \"scrapytest.org\"\n    allowed_domains = [\"scrapytest.org\", \"localhost\"]\n\n    itemurl_re = re.compile(r\"item\\d+.html\")\n    name_re = re.compile(r\"<h1>(.*?)</h1>\", re.M)\n    price_re = re.compile(r\">Price: \\$(.*?)<\", re.M)\n\n    item_cls: type = TestItem\n\n    def parse(self, response):\n        xlink = LinkExtractor()\n        itemre = re.compile(self.itemurl_re)\n        for link in xlink.extract_links(response):\n            if itemre.search(link.url):\n                yield Request(url=link.url, callback=self.parse_item)\n\n    def parse_item(self, response):\n        adapter = ItemAdapter(self.item_cls())\n        m = self.name_re.search(response.text)\n        if m:\n            adapter[\"name\"] = m.group(1)\n        adapter[\"url\"] = response.url\n        m = self.price_re.search(response.text)\n        if m:\n            adapter[\"price\"] = m.group(1)\n        return adapter.item\n\n\nclass TestDupeFilterSpider(TestSpider):\n    def start_requests(self):\n        return (Request(url) for url in self.start_urls)  # no dont_filter=True\n\n\nclass DictItemsSpider(TestSpider):\n    item_cls = dict\n\n\nclass AttrsItemsSpider(TestSpider):\n    item_cls = AttrsItem\n\n\nclass DataClassItemsSpider(TestSpider):\n    item_cls = DataClassItem\n\n\nclass ItemZeroDivisionErrorSpider(TestSpider):\n    custom_settings = {\n        \"ITEM_PIPELINES\": {\n            \"tests.pipelines.ProcessWithZeroDivisionErrorPipeline\": 300,\n        }\n    }\n\n\nclass ChangeCloseReasonSpider(TestSpider):\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        crawler.signals.connect(spider.spider_idle, signals.spider_idle)\n        return spider\n\n    def spider_idle(self):\n        raise CloseSpider(reason=\"custom_reason\")\n\n\ndef start_test_site(debug=False):\n    root_dir = Path(tests_datadir, \"test_site\")\n    r = static.File(str(root_dir))\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n    r.putChild(b\"numbers\", static.Data(b\"\".join(numbers), \"text/plain\"))\n\n    port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n    if debug:\n        print(\n            f\"Test server running at http://localhost:{port.getHost().port}/ \"\n            \"- hit Ctrl-C to finish.\"\n        )\n    return port\n\n\nclass CrawlerRun:\n    \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n\n    def __init__(self, spider_class):\n        self.spider = None\n        self.respplug = []\n        self.reqplug = []\n        self.reqdropped = []\n        self.reqreached = []\n        self.itemerror = []\n        self.itemresp = []\n        self.headers = {}\n        self.bytes = defaultdict(list)\n        self.signals_caught = {}\n        self.spider_class = spider_class\n\n    def run(self):\n        self.port = start_test_site()\n        self.portno = self.port.getHost().port\n\n        start_urls = [\n            self.geturl(\"/\"),\n            self.geturl(\"/redirect\"),\n            self.geturl(\"/redirect\"),  # duplicate\n            self.geturl(\"/numbers\"),\n        ]\n\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                dispatcher.connect(self.record_signal, signal)\n\n        self.crawler = get_crawler(self.spider_class)\n        self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n        self.crawler.signals.connect(self.item_error, signals.item_error)\n        self.crawler.signals.connect(self.headers_received, signals.headers_received)\n        self.crawler.signals.connect(self.bytes_received, signals.bytes_received)\n        self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n        self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n        self.crawler.signals.connect(\n            self.request_reached, signals.request_reached_downloader\n        )\n        self.crawler.signals.connect(\n            self.response_downloaded, signals.response_downloaded\n        )\n        self.crawler.crawl(start_urls=start_urls)\n        self.spider = self.crawler.spider\n\n        self.deferred = defer.Deferred()\n        dispatcher.connect(self.stop, signals.engine_stopped)\n        return self.deferred\n\n    def stop(self):\n        self.port.stopListening()  # FIXME: wait for this Deferred\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                disconnect_all(signal)\n        self.deferred.callback(None)\n        return self.crawler.stop()\n\n    def geturl(self, path):\n        return f\"http://localhost:{self.portno}{path}\"\n\n    def getpath(self, url):\n        u = urlparse(url)\n        return u.path\n\n    def item_error(self, item, response, spider, failure):\n        self.itemerror.append((item, response, spider, failure))\n\n    def item_scraped(self, item, spider, response):\n        self.itemresp.append((item, response))\n\n    def headers_received(self, headers, body_length, request, spider):\n        self.headers[request] = headers\n\n    def bytes_received(self, data, request, spider):\n        self.bytes[request].append(data)\n\n    def request_scheduled(self, request, spider):\n        self.reqplug.append((request, spider))\n\n    def request_reached(self, request, spider):\n        self.reqreached.append((request, spider))\n\n    def request_dropped(self, request, spider):\n        self.reqdropped.append((request, spider))\n\n    def response_downloaded(self, response, spider):\n        self.respplug.append((response, spider))\n\n    def record_signal(self, *args, **kwargs):\n        \"\"\"Record a signal and its parameters\"\"\"\n        signalargs = kwargs.copy()\n        sig = signalargs.pop(\"signal\")\n        signalargs.pop(\"sender\", None)\n        self.signals_caught[sig] = signalargs\n\n\nclass EngineTest(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_crawler(self):\n        for spider in (\n            TestSpider,\n            DictItemsSpider,\n            AttrsItemsSpider,\n            DataClassItemsSpider,\n        ):\n            run = CrawlerRun(spider)\n            yield run.run()\n            self._assert_visited_urls(run)\n            self._assert_scheduled_requests(run, count=9)\n            self._assert_downloaded_responses(run, count=9)\n            self._assert_scraped_items(run)\n            self._assert_signals_caught(run)\n            self._assert_bytes_received(run)\n\n    @defer.inlineCallbacks\n    def test_crawler_dupefilter(self):\n        run = CrawlerRun(TestDupeFilterSpider)\n        yield run.run()\n        self._assert_scheduled_requests(run, count=8)\n        self._assert_dropped_requests(run)\n\n    @defer.inlineCallbacks\n    def test_crawler_itemerror(self):\n        run = CrawlerRun(ItemZeroDivisionErrorSpider)\n        yield run.run()\n        self._assert_items_error(run)\n\n    @defer.inlineCallbacks\n    def test_crawler_change_close_reason_on_idle(self):\n        run = CrawlerRun(ChangeCloseReasonSpider)\n        yield run.run()\n        self.assertEqual(\n            {\"spider\": run.spider, \"reason\": \"custom_reason\"},\n            run.signals_caught[signals.spider_closed],\n        )\n\n    def _assert_visited_urls(self, run: CrawlerRun):\n        must_be_visited = [\n            \"/\",\n            \"/redirect\",\n            \"/redirected\",\n            \"/item1.html\",\n            \"/item2.html\",\n            \"/item999.html\",\n        ]\n        urls_visited = {rp[0].url for rp in run.respplug}\n        urls_expected = {run.geturl(p) for p in must_be_visited}\n        assert (\n            urls_expected <= urls_visited\n        ), f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n\n    def _assert_scheduled_requests(self, run: CrawlerRun, count=None):\n        self.assertEqual(count, len(run.reqplug))\n\n        paths_expected = [\"/item999.html\", \"/item2.html\", \"/item1.html\"]\n\n        urls_requested = {rq[0].url for rq in run.reqplug}\n        urls_expected = {run.geturl(p) for p in paths_expected}\n        assert urls_expected <= urls_requested\n        scheduled_requests_count = len(run.reqplug)\n        dropped_requests_count = len(run.reqdropped)\n        responses_count = len(run.respplug)\n        self.assertEqual(\n            scheduled_requests_count, dropped_requests_count + responses_count\n        )\n        self.assertEqual(len(run.reqreached), responses_count)\n\n    def _assert_dropped_requests(self, run: CrawlerRun):\n        self.assertEqual(len(run.reqdropped), 1)\n\n    def _assert_downloaded_responses(self, run: CrawlerRun, count):\n        # response tests\n        self.assertEqual(count, len(run.respplug))\n        self.assertEqual(count, len(run.reqreached))\n\n        for response, _ in run.respplug:\n            if run.getpath(response.url) == \"/item999.html\":\n                self.assertEqual(404, response.status)\n            if run.getpath(response.url) == \"/redirect\":\n                self.assertEqual(302, response.status)\n\n    def _assert_items_error(self, run: CrawlerRun):\n        self.assertEqual(2, len(run.itemerror))\n        for item, response, spider, failure in run.itemerror:\n            self.assertEqual(failure.value.__class__, ZeroDivisionError)\n            self.assertEqual(spider, run.spider)\n\n            self.assertEqual(item[\"url\"], response.url)\n            if \"item1.html\" in item[\"url\"]:\n                self.assertEqual(\"Item 1 name\", item[\"name\"])\n                self.assertEqual(\"100\", item[\"price\"])\n            if \"item2.html\" in item[\"url\"]:\n                self.assertEqual(\"Item 2 name\", item[\"name\"])\n                self.assertEqual(\"200\", item[\"price\"])\n\n    def _assert_scraped_items(self, run: CrawlerRun):\n        self.assertEqual(2, len(run.itemresp))\n        for item, response in run.itemresp:\n            item = ItemAdapter(item)\n            self.assertEqual(item[\"url\"], response.url)\n            if \"item1.html\" in item[\"url\"]:\n                self.assertEqual(\"Item 1 name\", item[\"name\"])\n                self.assertEqual(\"100\", item[\"price\"])\n            if \"item2.html\" in item[\"url\"]:\n                self.assertEqual(\"Item 2 name\", item[\"name\"])\n                self.assertEqual(\"200\", item[\"price\"])\n\n    def _assert_headers_received(self, run: CrawlerRun):\n        for headers in run.headers.values():\n            self.assertIn(b\"Server\", headers)\n            self.assertIn(b\"TwistedWeb\", headers[b\"Server\"])\n            self.assertIn(b\"Date\", headers)\n            self.assertIn(b\"Content-Type\", headers)\n\n    def _assert_bytes_received(self, run: CrawlerRun):\n        self.assertEqual(9, len(run.bytes))\n        for request, data in run.bytes.items():\n            joined_data = b\"\".join(data)\n            if run.getpath(request.url) == \"/\":\n                self.assertEqual(joined_data, get_testdata(\"test_site\", \"index.html\"))\n            elif run.getpath(request.url) == \"/item1.html\":\n                self.assertEqual(joined_data, get_testdata(\"test_site\", \"item1.html\"))\n            elif run.getpath(request.url) == \"/item2.html\":\n                self.assertEqual(joined_data, get_testdata(\"test_site\", \"item2.html\"))\n            elif run.getpath(request.url) == \"/redirected\":\n                self.assertEqual(joined_data, b\"Redirected here\")\n            elif run.getpath(request.url) == \"/redirect\":\n                self.assertEqual(\n                    joined_data,\n                    b\"\\n<html>\\n\"\n                    b\"    <head>\\n\"\n                    b'        <meta http-equiv=\"refresh\" content=\"0;URL=/redirected\">\\n'\n                    b\"    </head>\\n\"\n                    b'    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n'\n                    b'    <a href=\"/redirected\">click here</a>\\n'\n                    b\"    </body>\\n\"\n                    b\"</html>\\n\",\n                )\n            elif run.getpath(request.url) == \"/tem999.html\":\n                self.assertEqual(\n                    joined_data,\n                    b\"\\n<html>\\n\"\n                    b\"  <head><title>404 - No Such Resource</title></head>\\n\"\n                    b\"  <body>\\n\"\n                    b\"    <h1>No Such Resource</h1>\\n\"\n                    b\"    <p>File not found.</p>\\n\"\n                    b\"  </body>\\n\"\n                    b\"</html>\\n\",\n                )\n            elif run.getpath(request.url) == \"/numbers\":\n                # signal was fired multiple times\n                self.assertTrue(len(data) > 1)\n                # bytes were received in order\n                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n                self.assertEqual(joined_data, b\"\".join(numbers))\n\n    def _assert_signals_caught(self, run: CrawlerRun):\n        assert signals.engine_started in run.signals_caught\n        assert signals.engine_stopped in run.signals_caught\n        assert signals.spider_opened in run.signals_caught\n        assert signals.spider_idle in run.signals_caught\n        assert signals.spider_closed in run.signals_caught\n        assert signals.headers_received in run.signals_caught\n\n        self.assertEqual(\n            {\"spider\": run.spider}, run.signals_caught[signals.spider_opened]\n        )\n        self.assertEqual(\n            {\"spider\": run.spider}, run.signals_caught[signals.spider_idle]\n        )\n        self.assertEqual(\n            {\"spider\": run.spider, \"reason\": \"finished\"},\n            run.signals_caught[signals.spider_closed],\n        )\n\n    @defer.inlineCallbacks\n    def test_close_downloader(self):\n        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n        yield e.close()\n\n    @defer.inlineCallbacks\n    def test_start_already_running_exception(self):\n        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n        yield e.open_spider(TestSpider(), [])\n        e.start()\n        try:\n            yield self.assertFailure(e.start(), RuntimeError).addBoth(\n                lambda exc: self.assertEqual(str(exc), \"Engine already running\")\n            )\n        finally:\n            yield e.stop()\n\n    def test_short_timeout(self):\n        args = (\n            sys.executable,\n            \"-m\",\n            \"scrapy.cmdline\",\n            \"fetch\",\n            \"-s\",\n            \"CLOSESPIDER_TIMEOUT=0.001\",\n            \"-s\",\n            \"LOG_LEVEL=DEBUG\",\n            \"http://toscrape.com\",\n        )\n        p = subprocess.Popen(\n            args,\n            stderr=subprocess.PIPE,\n        )\n\n        def kill_proc():\n            p.kill()\n            p.communicate()\n            raise AssertionError(\"Command took too much time to complete\")\n\n        timer = Timer(15, kill_proc)\n        try:\n            timer.start()\n            _, stderr = p.communicate()\n        finally:\n            timer.cancel()\n\n        self.assertNotIn(b\"Traceback\", stderr)\n\n\ndef test_request_scheduled_signal(caplog):\n    class TestScheduler(BaseScheduler):\n        def __init__(self):\n            self.enqueued = []\n\n        def enqueue_request(self, request: Request) -> bool:\n            self.enqueued.append(request)\n            return True\n\n    def signal_handler(request: Request, spider: Spider) -> None:\n        if \"drop\" in request.url:\n            raise IgnoreRequest\n\n    spider = TestSpider()\n    crawler = get_crawler(spider.__class__)\n    engine = ExecutionEngine(crawler, lambda _: None)\n    engine.downloader._slot_gc_loop.stop()\n    scheduler = TestScheduler()\n    engine.slot = Slot((), None, Mock(), scheduler)\n    crawler.signals.connect(signal_handler, request_scheduled)\n    keep_request = Request(\"https://keep.example\")\n    engine._schedule_request(keep_request, spider)\n    drop_request = Request(\"https://drop.example\")\n    caplog.set_level(DEBUG)\n    engine._schedule_request(drop_request, spider)\n    assert scheduler.enqueued == [\n        keep_request\n    ], f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n    assert \"dropped request <GET https://drop.example>\" in caplog.text\n    crawler.signals.disconnect(signal_handler, request_scheduled)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1 and sys.argv[1] == \"runserver\":\n        start_test_site(debug=True)\n        reactor.run()\n", "tests/test_robotstxt_interface.py": "from twisted.trial import unittest\n\nfrom scrapy.robotstxt import decode_robotstxt\n\n\ndef reppy_available():\n    # check if reppy parser is installed\n    try:\n        from reppy.robots import Robots  # noqa: F401\n    except ImportError:\n        return False\n    return True\n\n\ndef rerp_available():\n    # check if robotexclusionrulesparser is installed\n    try:\n        from robotexclusionrulesparser import RobotExclusionRulesParser  # noqa: F401\n    except ImportError:\n        return False\n    return True\n\n\ndef protego_available():\n    # check if protego parser is installed\n    try:\n        from protego import Protego  # noqa: F401\n    except ImportError:\n        return False\n    return True\n\n\nclass BaseRobotParserTest:\n    def _setUp(self, parser_cls):\n        self.parser_cls = parser_cls\n\n    def test_allowed(self):\n        robotstxt_robotstxt_body = (\n            b\"User-agent: * \\n\"\n            b\"Disallow: /disallowed \\n\"\n            b\"Allow: /allowed \\n\"\n            b\"Crawl-delay: 10\"\n        )\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"*\"))\n        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed\", \"*\"))\n\n    def test_allowed_wildcards(self):\n        robotstxt_robotstxt_body = b\"\"\"User-agent: first\n                                Disallow: /disallowed/*/end$\n\n                                User-agent: second\n                                Allow: /*allowed\n                                Disallow: /\n                                \"\"\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n\n        self.assertTrue(rp.allowed(\"https://www.site.local/disallowed\", \"first\"))\n        self.assertFalse(\n            rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\")\n        )\n        self.assertFalse(\n            rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\")\n        )\n        self.assertTrue(\n            rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\")\n        )\n\n        self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"second\"))\n        self.assertTrue(rp.allowed(\"https://www.site.local/is_still_allowed\", \"second\"))\n        self.assertTrue(rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\"))\n\n    def test_length_based_precedence(self):\n        robotstxt_robotstxt_body = b\"User-agent: * \\n\" b\"Disallow: / \\n\" b\"Allow: /page\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        self.assertTrue(rp.allowed(\"https://www.site.local/page\", \"*\"))\n\n    def test_order_based_precedence(self):\n        robotstxt_robotstxt_body = b\"User-agent: * \\n\" b\"Disallow: / \\n\" b\"Allow: /page\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        self.assertFalse(rp.allowed(\"https://www.site.local/page\", \"*\"))\n\n    def test_empty_response(self):\n        \"\"\"empty response should equal 'allow all'\"\"\"\n        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b\"\")\n        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n        self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n        self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n        self.assertTrue(rp.allowed(\"https://site.local/disallowed\", \"*\"))\n\n    def test_garbage_response(self):\n        \"\"\"garbage response should be discarded, equal 'allow all'\"\"\"\n        robotstxt_robotstxt_body = b\"GIF89a\\xd3\\x00\\xfe\\x00\\xa2\"\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n        self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n        self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n        self.assertTrue(rp.allowed(\"https://site.local/disallowed\", \"*\"))\n\n    def test_unicode_url_and_useragent(self):\n        robotstxt_robotstxt_body = \"\"\"\n        User-Agent: *\n        Disallow: /admin/\n        Disallow: /static/\n        # taken from https://en.wikipedia.org/robots.txt\n        Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n        Disallow: /wiki/K\u00e4ytt\u00e4j\u00e4:\n\n        User-Agent: Unic\u00f6deB\u00f6t\n        Disallow: /some/randome/page.html\"\"\".encode()\n        rp = self.parser_cls.from_crawler(\n            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n        )\n        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n        self.assertFalse(rp.allowed(\"https://site.local/admin/\", \"*\"))\n        self.assertFalse(rp.allowed(\"https://site.local/static/\", \"*\"))\n        self.assertTrue(rp.allowed(\"https://site.local/admin/\", \"Unic\u00f6deB\u00f6t\"))\n        self.assertFalse(\n            rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\")\n        )\n        self.assertFalse(rp.allowed(\"https://site.local/wiki/K\u00e4ytt\u00e4j\u00e4:\", \"*\"))\n        self.assertTrue(rp.allowed(\"https://site.local/some/randome/page.html\", \"*\"))\n        self.assertFalse(\n            rp.allowed(\"https://site.local/some/randome/page.html\", \"Unic\u00f6deB\u00f6t\")\n        )\n\n\nclass DecodeRobotsTxtTest(unittest.TestCase):\n    def test_native_string_conversion(self):\n        robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n        decoded_content = decode_robotstxt(\n            robotstxt_body, spider=None, to_native_str_type=True\n        )\n        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n\n    def test_decode_utf8(self):\n        robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n        decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n\n    def test_decode_non_utf8(self):\n        robotstxt_body = b\"User-agent: *\\n\\xFFDisallow: /\\n\"\n        decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n\n\nclass PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n    def setUp(self):\n        from scrapy.robotstxt import PythonRobotParser\n\n        super()._setUp(PythonRobotParser)\n\n    def test_length_based_precedence(self):\n        raise unittest.SkipTest(\n            \"RobotFileParser does not support length based directives precedence.\"\n        )\n\n    def test_allowed_wildcards(self):\n        raise unittest.SkipTest(\"RobotFileParser does not support wildcards.\")\n\n\nclass ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n    if not reppy_available():\n        skip = \"Reppy parser is not installed\"\n\n    def setUp(self):\n        from scrapy.robotstxt import ReppyRobotParser\n\n        super()._setUp(ReppyRobotParser)\n\n    def test_order_based_precedence(self):\n        raise unittest.SkipTest(\n            \"Reppy does not support order based directives precedence.\"\n        )\n\n\nclass RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n    if not rerp_available():\n        skip = \"Rerp parser is not installed\"\n\n    def setUp(self):\n        from scrapy.robotstxt import RerpRobotParser\n\n        super()._setUp(RerpRobotParser)\n\n    def test_length_based_precedence(self):\n        raise unittest.SkipTest(\n            \"Rerp does not support length based directives precedence.\"\n        )\n\n\nclass ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n    if not protego_available():\n        skip = \"Protego parser is not installed\"\n\n    def setUp(self):\n        from scrapy.robotstxt import ProtegoRobotParser\n\n        super()._setUp(ProtegoRobotParser)\n\n    def test_order_based_precedence(self):\n        raise unittest.SkipTest(\n            \"Protego does not support order based directives precedence.\"\n        )\n", "tests/test_utils_trackref.py": "import unittest\nfrom io import StringIO\nfrom time import sleep, time\nfrom unittest import mock\n\nfrom twisted.trial.unittest import SkipTest\n\nfrom scrapy.utils import trackref\n\n\nclass Foo(trackref.object_ref):\n    pass\n\n\nclass Bar(trackref.object_ref):\n    pass\n\n\nclass TrackrefTestCase(unittest.TestCase):\n    def setUp(self):\n        trackref.live_refs.clear()\n\n    def test_format_live_refs(self):\n        o1 = Foo()  # NOQA\n        o2 = Bar()  # NOQA\n        o3 = Foo()  # NOQA\n        self.assertEqual(\n            trackref.format_live_refs(),\n            \"\"\"\\\nLive References\n\nBar                                 1   oldest: 0s ago\nFoo                                 2   oldest: 0s ago\n\"\"\",\n        )\n\n        self.assertEqual(\n            trackref.format_live_refs(ignore=Foo),\n            \"\"\"\\\nLive References\n\nBar                                 1   oldest: 0s ago\n\"\"\",\n        )\n\n    @mock.patch(\"sys.stdout\", new_callable=StringIO)\n    def test_print_live_refs_empty(self, stdout):\n        trackref.print_live_refs()\n        self.assertEqual(stdout.getvalue(), \"Live References\\n\\n\\n\")\n\n    @mock.patch(\"sys.stdout\", new_callable=StringIO)\n    def test_print_live_refs_with_objects(self, stdout):\n        o1 = Foo()  # NOQA\n        trackref.print_live_refs()\n        self.assertEqual(\n            stdout.getvalue(),\n            \"\"\"\\\nLive References\n\nFoo                                 1   oldest: 0s ago\\n\\n\"\"\",\n        )\n\n    def test_get_oldest(self):\n        o1 = Foo()  # NOQA\n\n        o1_time = time()\n\n        o2 = Bar()  # NOQA\n\n        o3_time = time()\n        if o3_time <= o1_time:\n            sleep(0.01)\n            o3_time = time()\n        if o3_time <= o1_time:\n            raise SkipTest(\"time.time is not precise enough\")\n\n        o3 = Foo()  # NOQA\n        self.assertIs(trackref.get_oldest(\"Foo\"), o1)\n        self.assertIs(trackref.get_oldest(\"Bar\"), o2)\n        self.assertIsNone(trackref.get_oldest(\"XXX\"))\n\n    def test_iter_all(self):\n        o1 = Foo()  # NOQA\n        o2 = Bar()  # NOQA\n        o3 = Foo()  # NOQA\n        self.assertEqual(\n            set(trackref.iter_all(\"Foo\")),\n            {o1, o3},\n        )\n", "tests/test_utils_httpobj.py": "import unittest\nfrom urllib.parse import urlparse\n\nfrom scrapy.http import Request\nfrom scrapy.utils.httpobj import urlparse_cached\n\n\nclass HttpobjUtilsTest(unittest.TestCase):\n    def test_urlparse_cached(self):\n        url = \"http://www.example.com/index.html\"\n        request1 = Request(url)\n        request2 = Request(url)\n        req1a = urlparse_cached(request1)\n        req1b = urlparse_cached(request1)\n        req2 = urlparse_cached(request2)\n        urlp = urlparse(url)\n\n        assert req1a == req2\n        assert req1a == urlp\n        assert req1a is req1b\n        assert req1a is not req2\n        assert req1a is not req2\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_signal.py": "import asyncio\n\nfrom pydispatch import dispatcher\nfrom pytest import mark\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer, reactor\nfrom twisted.python.failure import Failure\nfrom twisted.trial import unittest\n\nfrom scrapy.utils.signal import send_catch_log, send_catch_log_deferred\nfrom scrapy.utils.test import get_from_asyncio_queue\n\n\nclass SendCatchLogTest(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_send_catch_log(self):\n        test_signal = object()\n        handlers_called = set()\n\n        dispatcher.connect(self.error_handler, signal=test_signal)\n        dispatcher.connect(self.ok_handler, signal=test_signal)\n        with LogCapture() as log:\n            result = yield defer.maybeDeferred(\n                self._get_result,\n                test_signal,\n                arg=\"test\",\n                handlers_called=handlers_called,\n            )\n\n        assert self.error_handler in handlers_called\n        assert self.ok_handler in handlers_called\n        self.assertEqual(len(log.records), 1)\n        record = log.records[0]\n        self.assertIn(\"error_handler\", record.getMessage())\n        self.assertEqual(record.levelname, \"ERROR\")\n        self.assertEqual(result[0][0], self.error_handler)\n        self.assertIsInstance(result[0][1], Failure)\n        self.assertEqual(result[1], (self.ok_handler, \"OK\"))\n\n        dispatcher.disconnect(self.error_handler, signal=test_signal)\n        dispatcher.disconnect(self.ok_handler, signal=test_signal)\n\n    def _get_result(self, signal, *a, **kw):\n        return send_catch_log(signal, *a, **kw)\n\n    def error_handler(self, arg, handlers_called):\n        handlers_called.add(self.error_handler)\n        1 / 0\n\n    def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        return \"OK\"\n\n\nclass SendCatchLogDeferredTest(SendCatchLogTest):\n    def _get_result(self, signal, *a, **kw):\n        return send_catch_log_deferred(signal, *a, **kw)\n\n\nclass SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n    def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        d = defer.Deferred()\n        reactor.callLater(0, d.callback, \"OK\")\n        return d\n\n\n@mark.usefixtures(\"reactor_pytest\")\nclass SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n    async def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        await defer.succeed(42)\n        return \"OK\"\n\n\n@mark.only_asyncio()\nclass SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n    async def ok_handler(self, arg, handlers_called):\n        handlers_called.add(self.ok_handler)\n        assert arg == \"test\"\n        await asyncio.sleep(0.2)\n        return await get_from_asyncio_queue(\"OK\")\n\n\nclass SendCatchLogTest2(unittest.TestCase):\n    def test_error_logged_if_deferred_not_supported(self):\n        def test_handler():\n            return defer.Deferred()\n\n        test_signal = object()\n        dispatcher.connect(test_handler, test_signal)\n        with LogCapture() as log:\n            send_catch_log(test_signal)\n        self.assertEqual(len(log.records), 1)\n        self.assertIn(\"Cannot return deferreds from signal handler\", str(log))\n        dispatcher.disconnect(test_handler, test_signal)\n", "tests/test_downloadermiddleware_stats.py": "from unittest import TestCase\n\nfrom scrapy.downloadermiddlewares.stats import DownloaderStats\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass MyException(Exception):\n    pass\n\n\nclass TestDownloaderStats(TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"scrapytest.org\")\n        self.mw = DownloaderStats(self.crawler.stats)\n\n        self.crawler.stats.open_spider(self.spider)\n\n        self.req = Request(\"http://scrapytest.org\")\n        self.res = Response(\"scrapytest.org\", status=400)\n\n    def assertStatsEqual(self, key, value):\n        self.assertEqual(\n            self.crawler.stats.get_value(key, spider=self.spider),\n            value,\n            str(self.crawler.stats.get_stats(self.spider)),\n        )\n\n    def test_process_request(self):\n        self.mw.process_request(self.req, self.spider)\n        self.assertStatsEqual(\"downloader/request_count\", 1)\n\n    def test_process_response(self):\n        self.mw.process_response(self.req, self.res, self.spider)\n        self.assertStatsEqual(\"downloader/response_count\", 1)\n\n    def test_process_exception(self):\n        self.mw.process_exception(self.req, MyException(), self.spider)\n        self.assertStatsEqual(\"downloader/exception_count\", 1)\n        self.assertStatsEqual(\n            \"downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException\",\n            1,\n        )\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n", "tests/test_pipeline_files.py": "import dataclasses\nimport os\nimport random\nimport time\nfrom datetime import datetime\nfrom io import BytesIO\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\nfrom typing import Dict, List\nfrom unittest import mock\nfrom urllib.parse import urlparse\n\nimport attr\nfrom itemadapter import ItemAdapter\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\n\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.pipelines.files import (\n    FilesPipeline,\n    FSFilesStore,\n    FTPFilesStore,\n    GCSFilesStore,\n    S3FilesStore,\n)\nfrom scrapy.settings import Settings\nfrom scrapy.utils.test import (\n    assert_gcs_environ,\n    get_crawler,\n    get_ftp_content_and_delete,\n    get_gcs_content_and_delete,\n    skip_if_no_boto,\n)\nfrom tests.mockserver import MockFTPServer\n\nfrom .test_pipeline_media import _mocked_download_func\n\n\nclass FilesPipelineTestCase(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = mkdtemp()\n        settings_dict = {\"FILES_STORE\": self.tempdir}\n        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n        self.pipeline = FilesPipeline.from_crawler(crawler)\n        self.pipeline.download_func = _mocked_download_func\n        self.pipeline.open_spider(None)\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def test_file_path(self):\n        file_path = self.pipeline.file_path\n        self.assertEqual(\n            file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n            \"full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\"\n                )\n            ),\n            \"full/4ce274dd83db0368bafd7e406f382ae088e39219.txt\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")\n            ),\n            \"full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n                )\n            ),\n            \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\",\n        )\n        self.assertEqual(\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n            \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2\",\n        )\n        self.assertEqual(\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                info=object(),\n            ),\n            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\"\n                )\n            ),\n            \"full/76c00cef2ef669ae65052661f68d451162829507\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\"\n                )\n            ),\n            \"full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png\",\n        )\n\n    def test_fs_store(self):\n        assert isinstance(self.pipeline.store, FSFilesStore)\n        self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n\n        path = \"some/image/key.jpg\"\n        fullpath = Path(self.tempdir, \"some\", \"image\", \"key.jpg\")\n        self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)\n\n    @defer.inlineCallbacks\n    def test_file_not_expired(self):\n        item_url = \"http://example.com/file.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\"checksum\": \"abc\", \"last_modified\": time.time()},\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url)],\n            ),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n        self.assertEqual(result[\"files\"][0][\"status\"], \"uptodate\")\n\n        for p in patchers:\n            p.stop()\n\n    @defer.inlineCallbacks\n    def test_file_expired(self):\n        item_url = \"http://example.com/file2.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\n                    \"checksum\": \"abc\",\n                    \"last_modified\": time.time()\n                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n                },\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url)],\n            ),\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertNotEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n        self.assertEqual(result[\"files\"][0][\"status\"], \"downloaded\")\n\n        for p in patchers:\n            p.stop()\n\n    @defer.inlineCallbacks\n    def test_file_cached(self):\n        item_url = \"http://example.com/file3.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\n                    \"checksum\": \"abc\",\n                    \"last_modified\": time.time()\n                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n                },\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url, flags=[\"cached\"])],\n            ),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertNotEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n        self.assertEqual(result[\"files\"][0][\"status\"], \"cached\")\n\n        for p in patchers:\n            p.stop()\n\n    def test_file_path_from_item(self):\n        \"\"\"\n        Custom file path based on item data, overriding default implementation\n        \"\"\"\n\n        class CustomFilesPipeline(FilesPipeline):\n            def file_path(self, request, response=None, info=None, item=None):\n                return f'full/{item.get(\"path\")}'\n\n        file_path = CustomFilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": self.tempdir})\n        ).file_path\n        item = {\"path\": \"path-to-store-file\"}\n        request = Request(\"http://example.com\")\n        self.assertEqual(file_path(request, item=item), \"full/path-to-store-file\")\n\n\nclass FilesPipelineTestCaseFieldsMixin:\n    def setUp(self):\n        self.tempdir = mkdtemp()\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def test_item_fields_default(self):\n        url = \"http://www.example.com/files/1.txt\"\n        item = self.item_class(name=\"item1\", file_urls=[url])\n        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n        requests = list(pipeline.get_media_requests(item, None))\n        self.assertEqual(requests[0].url, url)\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        files = ItemAdapter(item).get(\"files\")\n        self.assertEqual(files, [results[0][1]])\n        self.assertIsInstance(item, self.item_class)\n\n    def test_item_fields_override_settings(self):\n        url = \"http://www.example.com/files/1.txt\"\n        item = self.item_class(name=\"item1\", custom_file_urls=[url])\n        pipeline = FilesPipeline.from_settings(\n            Settings(\n                {\n                    \"FILES_STORE\": self.tempdir,\n                    \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                    \"FILES_RESULT_FIELD\": \"custom_files\",\n                }\n            )\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        self.assertEqual(requests[0].url, url)\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        custom_files = ItemAdapter(item).get(\"custom_files\")\n        self.assertEqual(custom_files, [results[0][1]])\n        self.assertIsInstance(item, self.item_class)\n\n\nclass FilesPipelineTestCaseFieldsDict(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = dict\n\n\nclass FilesPipelineTestItem(Item):\n    name = Field()\n    # default fields\n    file_urls = Field()\n    files = Field()\n    # overridden fields\n    custom_file_urls = Field()\n    custom_files = Field()\n\n\nclass FilesPipelineTestCaseFieldsItem(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = FilesPipelineTestItem\n\n\n@dataclasses.dataclass\nclass FilesPipelineTestDataClass:\n    name: str\n    # default fields\n    file_urls: list = dataclasses.field(default_factory=list)\n    files: list = dataclasses.field(default_factory=list)\n    # overridden fields\n    custom_file_urls: list = dataclasses.field(default_factory=list)\n    custom_files: list = dataclasses.field(default_factory=list)\n\n\nclass FilesPipelineTestCaseFieldsDataClass(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = FilesPipelineTestDataClass\n\n\n@attr.s\nclass FilesPipelineTestAttrsItem:\n    name = attr.ib(default=\"\")\n    # default fields\n    file_urls: List[str] = attr.ib(default=lambda: [])\n    files: List[Dict[str, str]] = attr.ib(default=lambda: [])\n    # overridden fields\n    custom_file_urls: List[str] = attr.ib(default=lambda: [])\n    custom_files: List[Dict[str, str]] = attr.ib(default=lambda: [])\n\n\nclass FilesPipelineTestCaseFieldsAttrsItem(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = FilesPipelineTestAttrsItem\n\n\nclass FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n    default_cls_settings = {\n        \"EXPIRES\": 90,\n        \"FILES_URLS_FIELD\": \"file_urls\",\n        \"FILES_RESULT_FIELD\": \"files\",\n    }\n    file_cls_attr_settings_map = {\n        (\"EXPIRES\", \"FILES_EXPIRES\", \"expires\"),\n        (\"FILES_URLS_FIELD\", \"FILES_URLS_FIELD\", \"files_urls_field\"),\n        (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\"),\n    }\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def _generate_fake_settings(self, prefix=None):\n        def random_string():\n            return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n\n        settings = {\n            \"FILES_EXPIRES\": random.randint(100, 1000),\n            \"FILES_URLS_FIELD\": random_string(),\n            \"FILES_RESULT_FIELD\": random_string(),\n            \"FILES_STORE\": self.tempdir,\n        }\n        if not prefix:\n            return settings\n\n        return {\n            prefix.upper() + \"_\" + k if k != \"FILES_STORE\" else k: v\n            for k, v in settings.items()\n        }\n\n    def _generate_fake_pipeline(self):\n        class UserDefinedFilePipeline(FilesPipeline):\n            EXPIRES = 1001\n            FILES_URLS_FIELD = \"alfa\"\n            FILES_RESULT_FIELD = \"beta\"\n\n        return UserDefinedFilePipeline\n\n    def test_different_settings_for_different_instances(self):\n        \"\"\"\n        If there are different instances with different settings they should keep\n        different settings.\n        \"\"\"\n        custom_settings = self._generate_fake_settings()\n        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n        one_pipeline = FilesPipeline(self.tempdir)\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            default_value = self.default_cls_settings[pipe_attr]\n            self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n            custom_value = custom_settings[settings_attr]\n            self.assertNotEqual(default_value, custom_value)\n            self.assertEqual(getattr(another_pipeline, pipe_ins_attr), custom_value)\n\n    def test_subclass_attributes_preserved_if_no_settings(self):\n        \"\"\"\n        If subclasses override class attributes and there are no special settings those values should be kept.\n        \"\"\"\n        pipe_cls = self._generate_fake_pipeline()\n        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            custom_value = getattr(pipe, pipe_ins_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(getattr(pipe, pipe_ins_attr), getattr(pipe, pipe_attr))\n\n    def test_subclass_attrs_preserved_custom_settings(self):\n        \"\"\"\n        If file settings are defined but they are not defined for subclass\n        settings should be preserved.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        settings = self._generate_fake_settings()\n        pipeline = pipeline_cls.from_settings(Settings(settings))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            value = getattr(pipeline, pipe_ins_attr)\n            setting_value = settings.get(settings_attr)\n            self.assertNotEqual(value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(value, setting_value)\n\n    def test_no_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are no settings for subclass and no subclass attributes, pipeline should use\n        attributes of base class.\n        \"\"\"\n\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        user_pipeline = UserDefinedFilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": self.tempdir})\n        )\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = self.default_cls_settings.get(pipe_attr.upper())\n            self.assertEqual(getattr(user_pipeline, pipe_ins_attr), custom_value)\n\n    def test_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass and NO class attributes, pipeline should use custom\n        settings.\n        \"\"\"\n\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        prefix = UserDefinedFilesPipeline.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n\n    def test_custom_settings_and_class_attrs_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass AND class attributes\n        setting keys are preferred and override attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        prefix = pipeline_cls.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n        for (\n            pipe_cls_attr,\n            settings_attr,\n            pipe_inst_attr,\n        ) in self.file_cls_attr_settings_map:\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_cls_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n\n    def test_cls_attrs_with_DEFAULT_prefix(self):\n        class UserDefinedFilesPipeline(FilesPipeline):\n            DEFAULT_FILES_RESULT_FIELD = \"this\"\n            DEFAULT_FILES_URLS_FIELD = \"that\"\n\n        pipeline = UserDefinedFilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": self.tempdir})\n        )\n        self.assertEqual(\n            pipeline.files_result_field,\n            UserDefinedFilesPipeline.DEFAULT_FILES_RESULT_FIELD,\n        )\n        self.assertEqual(\n            pipeline.files_urls_field, UserDefinedFilesPipeline.DEFAULT_FILES_URLS_FIELD\n        )\n\n    def test_user_defined_subclass_default_key_names(self):\n        \"\"\"Test situation when user defines subclass of FilesPipeline,\n        but uses attribute names for default pipeline (without prefixing\n        them with pipeline class name).\n        \"\"\"\n        settings = self._generate_fake_settings()\n\n        class UserPipe(FilesPipeline):\n            pass\n\n        pipeline_cls = UserPipe.from_settings(Settings(settings))\n\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            expected_value = settings.get(settings_attr)\n            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr), expected_value)\n\n    def test_file_pipeline_using_pathlike_objects(self):\n        class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n            def file_path(self, request, response=None, info=None, *, item=None):\n                return Path(\"subdir\") / Path(request.url).name\n\n        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n        )\n        request = Request(\"http://example.com/image01.jpg\")\n        self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n\n    def test_files_store_constructor_with_pathlike_object(self):\n        path = Path(\"./FileDir\")\n        fs_store = FSFilesStore(path)\n        self.assertEqual(fs_store.basedir, str(path))\n\n\nclass TestS3FilesStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        skip_if_no_boto()\n\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}\"\n        buffer = mock.MagicMock()\n        meta = {\"foo\": \"bar\"}\n        path = \"\"\n        content_type = \"image/png\"\n\n        store = S3FilesStore(uri)\n        from botocore.stub import Stubber\n\n        with Stubber(store.s3_client) as stub:\n            stub.add_response(\n                \"put_object\",\n                expected_params={\n                    \"ACL\": S3FilesStore.POLICY,\n                    \"Body\": buffer,\n                    \"Bucket\": bucket,\n                    \"CacheControl\": S3FilesStore.HEADERS[\"Cache-Control\"],\n                    \"ContentType\": content_type,\n                    \"Key\": key,\n                    \"Metadata\": meta,\n                },\n                service_response={},\n            )\n\n            yield store.persist_file(\n                path,\n                buffer,\n                info=None,\n                meta=meta,\n                headers={\"Content-Type\": content_type},\n            )\n\n            stub.assert_no_pending_responses()\n            self.assertEqual(\n                buffer.method_calls,\n                [\n                    mock.call.seek(0),\n                    # The call to read does not happen with Stubber\n                ],\n            )\n\n    @defer.inlineCallbacks\n    def test_stat(self):\n        skip_if_no_boto()\n\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}\"\n        checksum = \"3187896a9657a28163abb31667df64c8\"\n        last_modified = datetime(2019, 12, 1)\n\n        store = S3FilesStore(uri)\n        from botocore.stub import Stubber\n\n        with Stubber(store.s3_client) as stub:\n            stub.add_response(\n                \"head_object\",\n                expected_params={\n                    \"Bucket\": bucket,\n                    \"Key\": key,\n                },\n                service_response={\n                    \"ETag\": f'\"{checksum}\"',\n                    \"LastModified\": last_modified,\n                },\n            )\n\n            file_stats = yield store.stat_file(\"\", info=None)\n            self.assertEqual(\n                file_stats,\n                {\n                    \"checksum\": checksum,\n                    \"last_modified\": last_modified.timestamp(),\n                },\n            )\n\n            stub.assert_no_pending_responses()\n\n\nclass TestGCSFilesStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        assert_gcs_environ()\n        uri = os.environ.get(\"GCS_TEST_FILE_URI\")\n        if not uri:\n            raise unittest.SkipTest(\"No GCS URI available for testing\")\n        data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {\"foo\": \"bar\"}\n        path = \"full/filename\"\n        store = GCSFilesStore(uri)\n        store.POLICY = \"authenticatedRead\"\n        expected_policy = {\"role\": \"READER\", \"entity\": \"allAuthenticatedUsers\"}\n        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n        s = yield store.stat_file(path, info=None)\n        self.assertIn(\"last_modified\", s)\n        self.assertIn(\"checksum\", s)\n        self.assertEqual(s[\"checksum\"], \"cdcda85605e46d0af6110752770dce3c\")\n        u = urlparse(uri)\n        content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:] + path)\n        self.assertEqual(content, data)\n        self.assertEqual(blob.metadata, {\"foo\": \"bar\"})\n        self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)\n        self.assertEqual(blob.content_type, \"application/octet-stream\")\n        self.assertIn(expected_policy, acl)\n\n    @defer.inlineCallbacks\n    def test_blob_path_consistency(self):\n        \"\"\"Test to make sure that paths used to store files is the same as the one used to get\n        already uploaded files.\n        \"\"\"\n        assert_gcs_environ()\n        try:\n            import google.cloud.storage  # noqa\n        except ModuleNotFoundError:\n            raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n        else:\n            with mock.patch(\"google.cloud.storage\") as _:\n                with mock.patch(\"scrapy.pipelines.files.time\") as _:\n                    uri = \"gs://my_bucket/my_prefix/\"\n                    store = GCSFilesStore(uri)\n                    store.bucket = mock.Mock()\n                    path = \"full/my_data.txt\"\n                    yield store.persist_file(\n                        path, mock.Mock(), info=None, meta=None, headers=None\n                    )\n                    yield store.stat_file(path, info=None)\n                    expected_blob_path = store.prefix + path\n                    store.bucket.blob.assert_called_with(expected_blob_path)\n                    store.bucket.get_blob.assert_called_with(expected_blob_path)\n\n\nclass TestFTPFileStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {\"foo\": \"bar\"}\n        path = \"full/filename\"\n        with MockFTPServer() as ftp_server:\n            store = FTPFilesStore(ftp_server.url(\"/\"))\n            empty_dict = yield store.stat_file(path, info=None)\n            self.assertEqual(empty_dict, {})\n            yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n            stat = yield store.stat_file(path, info=None)\n            self.assertIn(\"last_modified\", stat)\n            self.assertIn(\"checksum\", stat)\n            self.assertEqual(stat[\"checksum\"], \"d113d66b2ec7258724a268bd88eef6b6\")\n            path = f\"{store.basedir}/{path}\"\n            content = get_ftp_content_and_delete(\n                path,\n                store.host,\n                store.port,\n                store.username,\n                store.password,\n                store.USE_ACTIVE_MODE,\n            )\n        self.assertEqual(data, content)\n\n\nclass ItemWithFiles(Item):\n    file_urls = Field()\n    files = Field()\n\n\ndef _create_item_with_files(*files):\n    item = ItemWithFiles()\n    item[\"file_urls\"] = files\n    return item\n\n\ndef _prepare_request_object(item_url, flags=None):\n    return Request(\n        item_url,\n        meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n    )\n", "tests/test_command_shell.py": "import os\nimport sys\nfrom io import BytesIO\nfrom pathlib import Path\n\nfrom pexpect.popen_spawn import PopenSpawn\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\n\nfrom scrapy.utils.testproc import ProcessTest\nfrom scrapy.utils.testsite import SiteTest\nfrom tests import NON_EXISTING_RESOLVABLE, tests_datadir\nfrom tests.mockserver import MockServer\n\n\nclass ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n    command = \"shell\"\n\n    @defer.inlineCallbacks\n    def test_empty(self):\n        _, out, _ = yield self.execute([\"-c\", \"item\"])\n        assert b\"{}\" in out\n\n    @defer.inlineCallbacks\n    def test_response_body(self):\n        _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"response.body\"])\n        assert b\"Works\" in out\n\n    @defer.inlineCallbacks\n    def test_response_type_text(self):\n        _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"type(response)\"])\n        assert b\"TextResponse\" in out\n\n    @defer.inlineCallbacks\n    def test_response_type_html(self):\n        _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", \"type(response)\"])\n        assert b\"HtmlResponse\" in out\n\n    @defer.inlineCallbacks\n    def test_response_selector_html(self):\n        xpath = \"response.xpath(\\\"//p[@class='one']/text()\\\").get()\"\n        _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", xpath])\n        self.assertEqual(out.strip(), b\"Works\")\n\n    @defer.inlineCallbacks\n    def test_response_encoding_gb18030(self):\n        _, out, _ = yield self.execute(\n            [self.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"]\n        )\n        self.assertEqual(out.strip(), b\"gb18030\")\n\n    @defer.inlineCallbacks\n    def test_redirect(self):\n        _, out, _ = yield self.execute([self.url(\"/redirect\"), \"-c\", \"response.url\"])\n        assert out.strip().endswith(b\"/redirected\")\n\n    @defer.inlineCallbacks\n    def test_redirect_follow_302(self):\n        _, out, _ = yield self.execute(\n            [self.url(\"/redirect-no-meta-refresh\"), \"-c\", \"response.status\"]\n        )\n        assert out.strip().endswith(b\"200\")\n\n    @defer.inlineCallbacks\n    def test_redirect_not_follow_302(self):\n        _, out, _ = yield self.execute(\n            [\n                \"--no-redirect\",\n                self.url(\"/redirect-no-meta-refresh\"),\n                \"-c\",\n                \"response.status\",\n            ]\n        )\n        assert out.strip().endswith(b\"302\")\n\n    @defer.inlineCallbacks\n    def test_fetch_redirect_follow_302(self):\n        \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n        url = self.url(\"/redirect-no-meta-refresh\")\n        code = f\"fetch('{url}')\"\n        errcode, out, errout = yield self.execute([\"-c\", code])\n        self.assertEqual(errcode, 0, out)\n        assert b\"Redirecting (302)\" in errout\n        assert b\"Crawled (200)\" in errout\n\n    @defer.inlineCallbacks\n    def test_fetch_redirect_not_follow_302(self):\n        \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n        url = self.url(\"/redirect-no-meta-refresh\")\n        code = f\"fetch('{url}', redirect=False)\"\n        errcode, out, errout = yield self.execute([\"-c\", code])\n        self.assertEqual(errcode, 0, out)\n        assert b\"Crawled (302)\" in errout\n\n    @defer.inlineCallbacks\n    def test_request_replace(self):\n        url = self.url(\"/text\")\n        code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n        errcode, out, _ = yield self.execute([\"-c\", code])\n        self.assertEqual(errcode, 0, out)\n\n    @defer.inlineCallbacks\n    def test_scrapy_import(self):\n        url = self.url(\"/text\")\n        code = f\"fetch(scrapy.Request('{url}'))\"\n        errcode, out, _ = yield self.execute([\"-c\", code])\n        self.assertEqual(errcode, 0, out)\n\n    @defer.inlineCallbacks\n    def test_local_file(self):\n        filepath = Path(tests_datadir, \"test_site\", \"index.html\")\n        _, out, _ = yield self.execute([str(filepath), \"-c\", \"item\"])\n        assert b\"{}\" in out\n\n    @defer.inlineCallbacks\n    def test_local_nofile(self):\n        filepath = \"file:///tests/sample_data/test_site/nothinghere.html\"\n        errcode, out, err = yield self.execute(\n            [filepath, \"-c\", \"item\"], check_code=False\n        )\n        self.assertEqual(errcode, 1, out or err)\n        self.assertIn(b\"No such file or directory\", err)\n\n    @defer.inlineCallbacks\n    def test_dns_failures(self):\n        if NON_EXISTING_RESOLVABLE:\n            raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n        url = \"www.somedomainthatdoesntexi.st\"\n        errcode, out, err = yield self.execute([url, \"-c\", \"item\"], check_code=False)\n        self.assertEqual(errcode, 1, out or err)\n        self.assertIn(b\"DNS lookup failed\", err)\n\n    @defer.inlineCallbacks\n    def test_shell_fetch_async(self):\n        reactor_path = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n        url = self.url(\"/html\")\n        code = f\"fetch('{url}')\"\n        args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n        _, _, err = yield self.execute(args, check_code=True)\n        self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)\n\n\nclass InteractiveShellTest(unittest.TestCase):\n    def test_fetch(self):\n        args = (\n            sys.executable,\n            \"-m\",\n            \"scrapy.cmdline\",\n            \"shell\",\n        )\n        env = os.environ.copy()\n        env[\"SCRAPY_PYTHON_SHELL\"] = \"python\"\n        logfile = BytesIO()\n        p = PopenSpawn(args, env=env, timeout=5)\n        p.logfile_read = logfile\n        p.expect_exact(\"Available Scrapy objects\")\n        with MockServer() as mockserver:\n            p.sendline(f\"fetch('{mockserver.url('/')}')\")\n            p.sendline(\"type(response)\")\n            p.expect_exact(\"HtmlResponse\")\n        p.sendeof()\n        p.wait()\n        logfile.seek(0)\n        self.assertNotIn(\"Traceback\", logfile.read().decode())\n", "tests/test_http_cookies.py": "from unittest import TestCase\n\nfrom scrapy.http import Request, Response\nfrom scrapy.http.cookies import WrappedRequest, WrappedResponse\nfrom scrapy.utils.httpobj import urlparse_cached\n\n\nclass WrappedRequestTest(TestCase):\n    def setUp(self):\n        self.request = Request(\n            \"http://www.example.com/page.html\", headers={\"Content-Type\": \"text/html\"}\n        )\n        self.wrapped = WrappedRequest(self.request)\n\n    def test_get_full_url(self):\n        self.assertEqual(self.wrapped.get_full_url(), self.request.url)\n        self.assertEqual(self.wrapped.full_url, self.request.url)\n\n    def test_get_host(self):\n        self.assertEqual(self.wrapped.get_host(), urlparse_cached(self.request).netloc)\n        self.assertEqual(self.wrapped.host, urlparse_cached(self.request).netloc)\n\n    def test_get_type(self):\n        self.assertEqual(self.wrapped.get_type(), urlparse_cached(self.request).scheme)\n        self.assertEqual(self.wrapped.type, urlparse_cached(self.request).scheme)\n\n    def test_is_unverifiable(self):\n        self.assertFalse(self.wrapped.is_unverifiable())\n        self.assertFalse(self.wrapped.unverifiable)\n\n    def test_is_unverifiable2(self):\n        self.request.meta[\"is_unverifiable\"] = True\n        self.assertTrue(self.wrapped.is_unverifiable())\n        self.assertTrue(self.wrapped.unverifiable)\n\n    def test_get_origin_req_host(self):\n        self.assertEqual(self.wrapped.origin_req_host, \"www.example.com\")\n\n    def test_has_header(self):\n        self.assertTrue(self.wrapped.has_header(\"content-type\"))\n        self.assertFalse(self.wrapped.has_header(\"xxxxx\"))\n\n    def test_get_header(self):\n        self.assertEqual(self.wrapped.get_header(\"content-type\"), \"text/html\")\n        self.assertEqual(self.wrapped.get_header(\"xxxxx\", \"def\"), \"def\")\n        self.assertEqual(self.wrapped.get_header(\"xxxxx\"), None)\n        wrapped = WrappedRequest(\n            Request(\n                \"http://www.example.com/page.html\", headers={\"empty-binary-header\": b\"\"}\n            )\n        )\n        self.assertEqual(wrapped.get_header(\"empty-binary-header\"), \"\")\n\n    def test_header_items(self):\n        self.assertEqual(self.wrapped.header_items(), [(\"Content-Type\", [\"text/html\"])])\n\n    def test_add_unredirected_header(self):\n        self.wrapped.add_unredirected_header(\"hello\", \"world\")\n        self.assertEqual(self.request.headers[\"hello\"], b\"world\")\n\n\nclass WrappedResponseTest(TestCase):\n    def setUp(self):\n        self.response = Response(\n            \"http://www.example.com/page.html\", headers={\"Content-TYpe\": \"text/html\"}\n        )\n        self.wrapped = WrappedResponse(self.response)\n\n    def test_info(self):\n        self.assertIs(self.wrapped.info(), self.wrapped)\n\n    def test_get_all(self):\n        # get_all result must be native string\n        self.assertEqual(self.wrapped.get_all(\"content-type\"), [\"text/html\"])\n", "tests/test_http_headers.py": "import copy\nimport unittest\n\nfrom scrapy.http import Headers\n\n\nclass HeadersTest(unittest.TestCase):\n    def assertSortedEqual(self, first, second, msg=None):\n        return self.assertEqual(sorted(first), sorted(second), msg)\n\n    def test_basics(self):\n        h = Headers({\"Content-Type\": \"text/html\", \"Content-Length\": 1234})\n        assert h[\"Content-Type\"]\n        assert h[\"Content-Length\"]\n\n        self.assertRaises(KeyError, h.__getitem__, \"Accept\")\n        self.assertEqual(h.get(\"Accept\"), None)\n        self.assertEqual(h.getlist(\"Accept\"), [])\n\n        self.assertEqual(h.get(\"Accept\", \"*/*\"), b\"*/*\")\n        self.assertEqual(h.getlist(\"Accept\", \"*/*\"), [b\"*/*\"])\n        self.assertEqual(\n            h.getlist(\"Accept\", [\"text/html\", \"images/jpeg\"]),\n            [b\"text/html\", b\"images/jpeg\"],\n        )\n\n    def test_single_value(self):\n        h = Headers()\n        h[\"Content-Type\"] = \"text/html\"\n        self.assertEqual(h[\"Content-Type\"], b\"text/html\")\n        self.assertEqual(h.get(\"Content-Type\"), b\"text/html\")\n        self.assertEqual(h.getlist(\"Content-Type\"), [b\"text/html\"])\n\n    def test_multivalue(self):\n        h = Headers()\n        h[\"X-Forwarded-For\"] = hlist = [\"ip1\", \"ip2\"]\n        self.assertEqual(h[\"X-Forwarded-For\"], b\"ip2\")\n        self.assertEqual(h.get(\"X-Forwarded-For\"), b\"ip2\")\n        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\", b\"ip2\"])\n        assert h.getlist(\"X-Forwarded-For\") is not hlist\n\n    def test_multivalue_for_one_header(self):\n        h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n        self.assertEqual(h[\"a\"], b\"c\")\n        self.assertEqual(h.get(\"a\"), b\"c\")\n        self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n\n    def test_encode_utf8(self):\n        h = Headers({\"key\": \"\\xa3\"}, encoding=\"utf-8\")\n        key, val = dict(h).popitem()\n        assert isinstance(key, bytes), key\n        assert isinstance(val[0], bytes), val[0]\n        self.assertEqual(val[0], b\"\\xc2\\xa3\")\n\n    def test_encode_latin1(self):\n        h = Headers({\"key\": \"\\xa3\"}, encoding=\"latin1\")\n        key, val = dict(h).popitem()\n        self.assertEqual(val[0], b\"\\xa3\")\n\n    def test_encode_multiple(self):\n        h = Headers({\"key\": [\"\\xa3\"]}, encoding=\"utf-8\")\n        key, val = dict(h).popitem()\n        self.assertEqual(val[0], b\"\\xc2\\xa3\")\n\n    def test_delete_and_contains(self):\n        h = Headers()\n        h[\"Content-Type\"] = \"text/html\"\n        assert \"Content-Type\" in h\n        del h[\"Content-Type\"]\n        assert \"Content-Type\" not in h\n\n    def test_setdefault(self):\n        h = Headers()\n        hlist = [\"ip1\", \"ip2\"]\n        olist = h.setdefault(\"X-Forwarded-For\", hlist)\n        assert h.getlist(\"X-Forwarded-For\") is not hlist\n        assert h.getlist(\"X-Forwarded-For\") is olist\n\n        h = Headers()\n        olist = h.setdefault(\"X-Forwarded-For\", \"ip1\")\n        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\"])\n        assert h.getlist(\"X-Forwarded-For\") is olist\n\n    def test_iterables(self):\n        idict = {\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]}\n\n        h = Headers(idict)\n        self.assertDictEqual(\n            dict(h),\n            {b\"Content-Type\": [b\"text/html\"], b\"X-Forwarded-For\": [b\"ip1\", b\"ip2\"]},\n        )\n        self.assertSortedEqual(h.keys(), [b\"X-Forwarded-For\", b\"Content-Type\"])\n        self.assertSortedEqual(\n            h.items(),\n            [(b\"X-Forwarded-For\", [b\"ip1\", b\"ip2\"]), (b\"Content-Type\", [b\"text/html\"])],\n        )\n        self.assertSortedEqual(h.values(), [b\"ip2\", b\"text/html\"])\n\n    def test_update(self):\n        h = Headers()\n        h.update({\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]})\n        self.assertEqual(h.getlist(\"Content-Type\"), [b\"text/html\"])\n        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\", b\"ip2\"])\n\n    def test_copy(self):\n        h1 = Headers({\"header1\": [\"value1\", \"value2\"]})\n        h2 = copy.copy(h1)\n        self.assertEqual(h1, h2)\n        self.assertEqual(h1.getlist(\"header1\"), h2.getlist(\"header1\"))\n        assert h1.getlist(\"header1\") is not h2.getlist(\"header1\")\n        assert isinstance(h2, Headers)\n\n    def test_appendlist(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        h1.appendlist(\"header1\", \"value3\")\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\", b\"value3\"])\n\n        h1 = Headers()\n        h1.appendlist(\"header1\", \"value1\")\n        h1.appendlist(\"header1\", \"value3\")\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\", b\"value3\"])\n\n    def test_setlist(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\"])\n        h1.setlist(\"header1\", [b\"value2\", b\"value3\"])\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value2\", b\"value3\"])\n\n    def test_setlistdefault(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        h1.setlistdefault(\"header1\", [\"value2\", \"value3\"])\n        h1.setlistdefault(\"header2\", [\"value2\", \"value3\"])\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\"])\n        self.assertEqual(h1.getlist(\"header2\"), [b\"value2\", b\"value3\"])\n\n    def test_none_value(self):\n        h1 = Headers()\n        h1[\"foo\"] = \"bar\"\n        h1[\"foo\"] = None\n        h1.setdefault(\"foo\", \"bar\")\n        self.assertEqual(h1.get(\"foo\"), None)\n        self.assertEqual(h1.getlist(\"foo\"), [])\n\n    def test_int_value(self):\n        h1 = Headers({\"hey\": 5})\n        h1[\"foo\"] = 1\n        h1.setdefault(\"bar\", 2)\n        h1.setlist(\"buz\", [1, \"dos\", 3])\n        self.assertEqual(h1.getlist(\"foo\"), [b\"1\"])\n        self.assertEqual(h1.getlist(\"bar\"), [b\"2\"])\n        self.assertEqual(h1.getlist(\"buz\"), [b\"1\", b\"dos\", b\"3\"])\n        self.assertEqual(h1.getlist(\"hey\"), [b\"5\"])\n\n    def test_invalid_value(self):\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers, {\"foo\": object()}\n        )\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers().__setitem__, \"foo\", object()\n        )\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers().setdefault, \"foo\", object()\n        )\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers().setlist, \"foo\", [object()]\n        )\n", "tests/test_core_downloader.py": "from twisted.trial import unittest\n\nfrom scrapy.core.downloader import Slot\n\n\nclass SlotTest(unittest.TestCase):\n    def test_repr(self):\n        slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n        self.assertEqual(\n            repr(slot),\n            \"Slot(concurrency=8, delay=0.10, randomize_delay=True, throttle=None)\",\n        )\n", "tests/test_logstats.py": "import unittest\nfrom datetime import datetime\n\nfrom scrapy.extensions.logstats import LogStats\nfrom scrapy.utils.test import get_crawler\nfrom tests.spiders import SimpleSpider\n\n\nclass TestLogStats(unittest.TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(SimpleSpider)\n        self.spider = self.crawler._create_spider(\"spidey\")\n        self.stats = self.crawler.stats\n\n        self.stats.set_value(\"response_received_count\", 4802)\n        self.stats.set_value(\"item_scraped_count\", 3201)\n\n    def test_stats_calculations(self):\n        logstats = LogStats.from_crawler(self.crawler)\n\n        with self.assertRaises(AttributeError):\n            logstats.pagesprev\n            logstats.itemsprev\n\n        logstats.spider_opened(self.spider)\n        self.assertEqual(logstats.pagesprev, 4802)\n        self.assertEqual(logstats.itemsprev, 3201)\n\n        logstats.calculate_stats()\n        self.assertEqual(logstats.items, 3201)\n        self.assertEqual(logstats.pages, 4802)\n        self.assertEqual(logstats.irate, 0.0)\n        self.assertEqual(logstats.prate, 0.0)\n        self.assertEqual(logstats.pagesprev, 4802)\n        self.assertEqual(logstats.itemsprev, 3201)\n\n        # Simulate what happens after a minute\n        self.stats.set_value(\"response_received_count\", 5187)\n        self.stats.set_value(\"item_scraped_count\", 3492)\n        logstats.calculate_stats()\n        self.assertEqual(logstats.items, 3492)\n        self.assertEqual(logstats.pages, 5187)\n        self.assertEqual(logstats.irate, 291.0)\n        self.assertEqual(logstats.prate, 385.0)\n        self.assertEqual(logstats.pagesprev, 5187)\n        self.assertEqual(logstats.itemsprev, 3492)\n\n        # Simulate when spider closes after running for 30 mins\n        self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n        self.stats.set_value(\"finished_time\", datetime.fromtimestamp(1655101972))\n        logstats.spider_closed(self.spider, \"test reason\")\n        self.assertEqual(self.stats.get_value(\"responses_per_minute\"), 172.9)\n        self.assertEqual(self.stats.get_value(\"items_per_minute\"), 116.4)\n\n    def test_stats_calculations_no_time(self):\n        \"\"\"The stat values should be None since the start and finish time are\n        not available.\n        \"\"\"\n        logstats = LogStats.from_crawler(self.crawler)\n        logstats.spider_closed(self.spider, \"test reason\")\n        self.assertIsNone(self.stats.get_value(\"responses_per_minute\"))\n        self.assertIsNone(self.stats.get_value(\"items_per_minute\"))\n", "tests/test_utils_url.py": "import unittest\n\nfrom scrapy.linkextractors import IGNORED_EXTENSIONS\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.url import (\n    _is_filesystem_path,\n    add_http_if_no_scheme,\n    guess_scheme,\n    strip_url,\n    url_has_any_extension,\n    url_is_from_any_domain,\n    url_is_from_spider,\n)\n\n__doctests__ = [\"scrapy.utils.url\"]\n\n\nclass UrlUtilsTest(unittest.TestCase):\n    def test_url_is_from_any_domain(self):\n        url = \"http://www.wheele-bin-art.co.uk/get/product/123\"\n        self.assertTrue(url_is_from_any_domain(url, [\"wheele-bin-art.co.uk\"]))\n        self.assertFalse(url_is_from_any_domain(url, [\"art.co.uk\"]))\n\n        url = \"http://wheele-bin-art.co.uk/get/product/123\"\n        self.assertTrue(url_is_from_any_domain(url, [\"wheele-bin-art.co.uk\"]))\n        self.assertFalse(url_is_from_any_domain(url, [\"art.co.uk\"]))\n\n        url = \"http://www.Wheele-Bin-Art.co.uk/get/product/123\"\n        self.assertTrue(url_is_from_any_domain(url, [\"wheele-bin-art.CO.UK\"]))\n        self.assertTrue(url_is_from_any_domain(url, [\"WHEELE-BIN-ART.CO.UK\"]))\n\n        url = \"http://192.169.0.15:8080/mypage.html\"\n        self.assertTrue(url_is_from_any_domain(url, [\"192.169.0.15:8080\"]))\n        self.assertFalse(url_is_from_any_domain(url, [\"192.169.0.15\"]))\n\n        url = (\n            \"javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20\"\n            \"javascript:%20document.orderform_2581_1190810811.submit%28%29\"\n        )\n        self.assertFalse(url_is_from_any_domain(url, [\"testdomain.com\"]))\n        self.assertFalse(\n            url_is_from_any_domain(url + \".testdomain.com\", [\"testdomain.com\"])\n        )\n\n    def test_url_is_from_spider(self):\n        spider = Spider(name=\"example.com\")\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://sub.example.com/some/page.html\", spider)\n        )\n        self.assertFalse(\n            url_is_from_spider(\"http://www.example.org/some/page.html\", spider)\n        )\n        self.assertFalse(\n            url_is_from_spider(\"http://www.example.net/some/page.html\", spider)\n        )\n\n    def test_url_is_from_spider_class_attributes(self):\n        class MySpider(Spider):\n            name = \"example.com\"\n\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n        )\n        self.assertFalse(\n            url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n        )\n        self.assertFalse(\n            url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n        )\n\n    def test_url_is_from_spider_with_allowed_domains(self):\n        spider = Spider(\n            name=\"example.com\", allowed_domains=[\"example.org\", \"example.net\"]\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://sub.example.com/some/page.html\", spider)\n        )\n        self.assertTrue(url_is_from_spider(\"http://example.com/some/page.html\", spider))\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.org/some/page.html\", spider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.net/some/page.html\", spider)\n        )\n        self.assertFalse(\n            url_is_from_spider(\"http://www.example.us/some/page.html\", spider)\n        )\n\n        spider = Spider(\n            name=\"example.com\", allowed_domains={\"example.com\", \"example.net\"}\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n        )\n\n        spider = Spider(\n            name=\"example.com\", allowed_domains=(\"example.com\", \"example.net\")\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n        )\n\n    def test_url_is_from_spider_with_allowed_domains_class_attributes(self):\n        class MySpider(Spider):\n            name = \"example.com\"\n            allowed_domains = (\"example.org\", \"example.net\")\n\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://example.com/some/page.html\", MySpider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n        )\n        self.assertTrue(\n            url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n        )\n        self.assertFalse(\n            url_is_from_spider(\"http://www.example.us/some/page.html\", MySpider)\n        )\n\n    def test_url_has_any_extension(self):\n        deny_extensions = {\".\" + e for e in arg_to_iter(IGNORED_EXTENSIONS)}\n        self.assertTrue(\n            url_has_any_extension(\n                \"http://www.example.com/archive.tar.gz\", deny_extensions\n            )\n        )\n        self.assertTrue(\n            url_has_any_extension(\"http://www.example.com/page.doc\", deny_extensions)\n        )\n        self.assertTrue(\n            url_has_any_extension(\"http://www.example.com/page.pdf\", deny_extensions)\n        )\n        self.assertFalse(\n            url_has_any_extension(\"http://www.example.com/page.htm\", deny_extensions)\n        )\n        self.assertFalse(\n            url_has_any_extension(\"http://www.example.com/\", deny_extensions)\n        )\n        self.assertFalse(\n            url_has_any_extension(\n                \"http://www.example.com/page.doc.html\", deny_extensions\n            )\n        )\n\n\nclass AddHttpIfNoScheme(unittest.TestCase):\n    def test_add_scheme(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"www.example.com\"), \"http://www.example.com\"\n        )\n\n    def test_without_subdomain(self):\n        self.assertEqual(add_http_if_no_scheme(\"example.com\"), \"http://example.com\")\n\n    def test_path(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"www.example.com/some/page.html\"),\n            \"http://www.example.com/some/page.html\",\n        )\n\n    def test_port(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"www.example.com:80\"), \"http://www.example.com:80\"\n        )\n\n    def test_fragment(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"www.example.com/some/page#frag\"),\n            \"http://www.example.com/some/page#frag\",\n        )\n\n    def test_query(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"www.example.com/do?a=1&b=2&c=3\"),\n            \"http://www.example.com/do?a=1&b=2&c=3\",\n        )\n\n    def test_username_password(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"username:password@www.example.com\"),\n            \"http://username:password@www.example.com\",\n        )\n\n    def test_complete_url(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\n                \"username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\"\n            ),\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n        )\n\n    def test_preserve_http(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://www.example.com\"), \"http://www.example.com\"\n        )\n\n    def test_preserve_http_without_subdomain(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://example.com\"), \"http://example.com\"\n        )\n\n    def test_preserve_http_path(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://www.example.com/some/page.html\"),\n            \"http://www.example.com/some/page.html\",\n        )\n\n    def test_preserve_http_port(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://www.example.com:80\"),\n            \"http://www.example.com:80\",\n        )\n\n    def test_preserve_http_fragment(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://www.example.com/some/page#frag\"),\n            \"http://www.example.com/some/page#frag\",\n        )\n\n    def test_preserve_http_query(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://www.example.com/do?a=1&b=2&c=3\"),\n            \"http://www.example.com/do?a=1&b=2&c=3\",\n        )\n\n    def test_preserve_http_username_password(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"http://username:password@www.example.com\"),\n            \"http://username:password@www.example.com\",\n        )\n\n    def test_preserve_http_complete_url(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\n                \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\"\n            ),\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n        )\n\n    def test_protocol_relative(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"//www.example.com\"), \"http://www.example.com\"\n        )\n\n    def test_protocol_relative_without_subdomain(self):\n        self.assertEqual(add_http_if_no_scheme(\"//example.com\"), \"http://example.com\")\n\n    def test_protocol_relative_path(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"//www.example.com/some/page.html\"),\n            \"http://www.example.com/some/page.html\",\n        )\n\n    def test_protocol_relative_port(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"//www.example.com:80\"), \"http://www.example.com:80\"\n        )\n\n    def test_protocol_relative_fragment(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"//www.example.com/some/page#frag\"),\n            \"http://www.example.com/some/page#frag\",\n        )\n\n    def test_protocol_relative_query(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"//www.example.com/do?a=1&b=2&c=3\"),\n            \"http://www.example.com/do?a=1&b=2&c=3\",\n        )\n\n    def test_protocol_relative_username_password(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"//username:password@www.example.com\"),\n            \"http://username:password@www.example.com\",\n        )\n\n    def test_protocol_relative_complete_url(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\n                \"//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\"\n            ),\n            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n        )\n\n    def test_preserve_https(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"https://www.example.com\"), \"https://www.example.com\"\n        )\n\n    def test_preserve_ftp(self):\n        self.assertEqual(\n            add_http_if_no_scheme(\"ftp://www.example.com\"), \"ftp://www.example.com\"\n        )\n\n\nclass GuessSchemeTest(unittest.TestCase):\n    pass\n\n\ndef create_guess_scheme_t(args):\n    def do_expected(self):\n        url = guess_scheme(args[0])\n        assert url.startswith(\n            args[1]\n        ), f\"Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`\"\n\n    return do_expected\n\n\ndef create_skipped_scheme_t(args):\n    def do_expected(self):\n        raise unittest.SkipTest(args[2])\n        url = guess_scheme(args[0])\n        assert url.startswith(args[1])\n\n    return do_expected\n\n\nfor k, args in enumerate(\n    [\n        (\"/index\", \"file://\"),\n        (\"/index.html\", \"file://\"),\n        (\"./index.html\", \"file://\"),\n        (\"../index.html\", \"file://\"),\n        (\"../../index.html\", \"file://\"),\n        (\"./data/index.html\", \"file://\"),\n        (\".hidden/data/index.html\", \"file://\"),\n        (\"/home/user/www/index.html\", \"file://\"),\n        (\"//home/user/www/index.html\", \"file://\"),\n        (\"file:///home/user/www/index.html\", \"file://\"),\n        (\"index.html\", \"http://\"),\n        (\"example.com\", \"http://\"),\n        (\"www.example.com\", \"http://\"),\n        (\"www.example.com/index.html\", \"http://\"),\n        (\"http://example.com\", \"http://\"),\n        (\"http://example.com/index.html\", \"http://\"),\n        (\"localhost\", \"http://\"),\n        (\"localhost/index.html\", \"http://\"),\n        # some corner cases (default to http://)\n        (\"/\", \"http://\"),\n        (\".../test\", \"http://\"),\n    ],\n    start=1,\n):\n    t_method = create_guess_scheme_t(args)\n    t_method.__name__ = f\"test_uri_{k:03}\"\n    setattr(GuessSchemeTest, t_method.__name__, t_method)\n\n# TODO: the following tests do not pass with current implementation\nfor k, skip_args in enumerate(\n    [\n        (\n            r\"C:\\absolute\\path\\to\\a\\file.html\",\n            \"file://\",\n            \"Windows filepath are not supported for scrapy shell\",\n        ),\n    ],\n    start=1,\n):\n    t_method = create_skipped_scheme_t(skip_args)\n    t_method.__name__ = f\"test_uri_skipped_{k:03}\"\n    setattr(GuessSchemeTest, t_method.__name__, t_method)\n\n\nclass StripUrl(unittest.TestCase):\n    def test_noop(self):\n        self.assertEqual(\n            strip_url(\"http://www.example.com/index.html\"),\n            \"http://www.example.com/index.html\",\n        )\n\n    def test_noop_query_string(self):\n        self.assertEqual(\n            strip_url(\"http://www.example.com/index.html?somekey=somevalue\"),\n            \"http://www.example.com/index.html?somekey=somevalue\",\n        )\n\n    def test_fragments(self):\n        self.assertEqual(\n            strip_url(\n                \"http://www.example.com/index.html?somekey=somevalue#section\",\n                strip_fragment=False,\n            ),\n            \"http://www.example.com/index.html?somekey=somevalue#section\",\n        )\n\n    def test_path(self):\n        for input_url, origin, output_url in [\n            (\"http://www.example.com/\", False, \"http://www.example.com/\"),\n            (\"http://www.example.com\", False, \"http://www.example.com\"),\n            (\"http://www.example.com\", True, \"http://www.example.com/\"),\n        ]:\n            self.assertEqual(strip_url(input_url, origin_only=origin), output_url)\n\n    def test_credentials(self):\n        for i, o in [\n            (\n                \"http://username@www.example.com/index.html?somekey=somevalue#section\",\n                \"http://www.example.com/index.html?somekey=somevalue\",\n            ),\n            (\n                \"https://username:@www.example.com/index.html?somekey=somevalue#section\",\n                \"https://www.example.com/index.html?somekey=somevalue\",\n            ),\n            (\n                \"ftp://username:password@www.example.com/index.html?somekey=somevalue#section\",\n                \"ftp://www.example.com/index.html?somekey=somevalue\",\n            ),\n        ]:\n            self.assertEqual(strip_url(i, strip_credentials=True), o)\n\n    def test_credentials_encoded_delims(self):\n        for i, o in [\n            # user: \"username@\"\n            # password: none\n            (\n                \"http://username%40@www.example.com/index.html?somekey=somevalue#section\",\n                \"http://www.example.com/index.html?somekey=somevalue\",\n            ),\n            # user: \"username:pass\"\n            # password: \"\"\n            (\n                \"https://username%3Apass:@www.example.com/index.html?somekey=somevalue#section\",\n                \"https://www.example.com/index.html?somekey=somevalue\",\n            ),\n            # user: \"me\"\n            # password: \"user@domain.com\"\n            (\n                \"ftp://me:user%40domain.com@www.example.com/index.html?somekey=somevalue#section\",\n                \"ftp://www.example.com/index.html?somekey=somevalue\",\n            ),\n        ]:\n            self.assertEqual(strip_url(i, strip_credentials=True), o)\n\n    def test_default_ports_creds_off(self):\n        for i, o in [\n            (\n                \"http://username:password@www.example.com:80/index.html?somekey=somevalue#section\",\n                \"http://www.example.com/index.html?somekey=somevalue\",\n            ),\n            (\n                \"http://username:password@www.example.com:8080/index.html#section\",\n                \"http://www.example.com:8080/index.html\",\n            ),\n            (\n                \"http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section\",\n                \"http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://www.example.com/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:442/index.html\",\n                \"https://www.example.com:442/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:80/index.html\",\n                \"https://www.example.com:80/index.html\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:21/file.txt\",\n                \"ftp://www.example.com/file.txt\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:221/file.txt\",\n                \"ftp://www.example.com:221/file.txt\",\n            ),\n        ]:\n            self.assertEqual(strip_url(i), o)\n\n    def test_default_ports(self):\n        for i, o in [\n            (\n                \"http://username:password@www.example.com:80/index.html\",\n                \"http://username:password@www.example.com/index.html\",\n            ),\n            (\n                \"http://username:password@www.example.com:8080/index.html\",\n                \"http://username:password@www.example.com:8080/index.html\",\n            ),\n            (\n                \"http://username:password@www.example.com:443/index.html\",\n                \"http://username:password@www.example.com:443/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://username:password@www.example.com/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:442/index.html\",\n                \"https://username:password@www.example.com:442/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:80/index.html\",\n                \"https://username:password@www.example.com:80/index.html\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:21/file.txt\",\n                \"ftp://username:password@www.example.com/file.txt\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:221/file.txt\",\n                \"ftp://username:password@www.example.com:221/file.txt\",\n            ),\n        ]:\n            self.assertEqual(\n                strip_url(i, strip_default_port=True, strip_credentials=False), o\n            )\n\n    def test_default_ports_keep(self):\n        for i, o in [\n            (\n                \"http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section\",\n                \"http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov\",\n            ),\n            (\n                \"http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section\",\n                \"http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov\",\n            ),\n            (\n                \"http://username:password@www.example.com:443/index.html\",\n                \"http://username:password@www.example.com:443/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://username:password@www.example.com:443/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:442/index.html\",\n                \"https://username:password@www.example.com:442/index.html\",\n            ),\n            (\n                \"https://username:password@www.example.com:80/index.html\",\n                \"https://username:password@www.example.com:80/index.html\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:21/file.txt\",\n                \"ftp://username:password@www.example.com:21/file.txt\",\n            ),\n            (\n                \"ftp://username:password@www.example.com:221/file.txt\",\n                \"ftp://username:password@www.example.com:221/file.txt\",\n            ),\n        ]:\n            self.assertEqual(\n                strip_url(i, strip_default_port=False, strip_credentials=False), o\n            )\n\n    def test_origin_only(self):\n        for i, o in [\n            (\n                \"http://username:password@www.example.com/index.html\",\n                \"http://www.example.com/\",\n            ),\n            (\n                \"http://username:password@www.example.com:80/foo/bar?query=value#somefrag\",\n                \"http://www.example.com/\",\n            ),\n            (\n                \"http://username:password@www.example.com:8008/foo/bar?query=value#somefrag\",\n                \"http://www.example.com:8008/\",\n            ),\n            (\n                \"https://username:password@www.example.com:443/index.html\",\n                \"https://www.example.com/\",\n            ),\n        ]:\n            self.assertEqual(strip_url(i, origin_only=True), o)\n\n\nclass IsPathTestCase(unittest.TestCase):\n    def test_path(self):\n        for input_value, output_value in (\n            # https://en.wikipedia.org/wiki/Path_(computing)#Representations_of_paths_by_operating_system_and_shell\n            # Unix-like OS, Microsoft Windows / cmd.exe\n            (\"/home/user/docs/Letter.txt\", True),\n            (\"./inthisdir\", True),\n            (\"../../greatgrandparent\", True),\n            (\"~/.rcinfo\", True),\n            (r\"C:\\user\\docs\\Letter.txt\", True),\n            (\"/user/docs/Letter.txt\", True),\n            (r\"C:\\Letter.txt\", True),\n            (r\"\\\\Server01\\user\\docs\\Letter.txt\", True),\n            (r\"\\\\?\\UNC\\Server01\\user\\docs\\Letter.txt\", True),\n            (r\"\\\\?\\C:\\user\\docs\\Letter.txt\", True),\n            (r\"C:\\user\\docs\\somefile.ext:alternate_stream_name\", True),\n            (r\"https://example.com\", False),\n        ):\n            self.assertEqual(\n                _is_filesystem_path(input_value), output_value, input_value\n            )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_downloadermiddleware.py": "import asyncio\nfrom unittest import mock\n\nfrom pytest import mark\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\n\n\nclass ManagerTestCase(TestCase):\n    settings_dict = None\n\n    def setUp(self):\n        self.crawler = get_crawler(Spider, self.settings_dict)\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n        self.crawler.engine = self.crawler._create_engine()\n        return self.crawler.engine.open_spider(self.spider, start_requests=())\n\n    def tearDown(self):\n        return self.crawler.engine.close_spider(self.spider)\n\n    def _download(self, request, response=None):\n        \"\"\"Executes downloader mw manager's download method and returns\n        the result (Request or Response) or raise exception in case of\n        failure.\n        \"\"\"\n        if not response:\n            response = Response(request.url)\n\n        def download_func(request, spider):\n            return response\n\n        dfd = self.mwman.download(download_func, request, self.spider)\n        # catch deferred result and return the value\n        results = []\n        dfd.addBoth(results.append)\n        self._wait(dfd)\n        ret = results[0]\n        if isinstance(ret, Failure):\n            ret.raiseException()\n        return ret\n\n\nclass DefaultsTest(ManagerTestCase):\n    \"\"\"Tests default behavior with default settings\"\"\"\n\n    def test_request_response(self):\n        req = Request(\"http://example.com/index.html\")\n        resp = Response(req.url, status=200)\n        ret = self._download(req, resp)\n        self.assertTrue(isinstance(ret, Response), \"Non-response returned\")\n\n    def test_3xx_and_invalid_gzipped_body_must_redirect(self):\n        \"\"\"Regression test for a failure when redirecting a compressed\n        request.\n\n        This happens when httpcompression middleware is executed before redirect\n        middleware and attempts to decompress a non-compressed body.\n        In particular when some website returns a 30x response with header\n        'Content-Encoding: gzip' giving as result the error below:\n\n            BadGzipFile: Not a gzipped file (...)\n\n        \"\"\"\n        req = Request(\"http://example.com\")\n        body = b\"<p>You are being redirected</p>\"\n        resp = Response(\n            req.url,\n            status=302,\n            body=body,\n            headers={\n                \"Content-Length\": str(len(body)),\n                \"Content-Type\": \"text/html\",\n                \"Content-Encoding\": \"gzip\",\n                \"Location\": \"http://example.com/login\",\n            },\n        )\n        ret = self._download(request=req, response=resp)\n        self.assertTrue(isinstance(ret, Request), f\"Not redirected: {ret!r}\")\n        self.assertEqual(\n            to_bytes(ret.url),\n            resp.headers[\"Location\"],\n            \"Not redirected to location header\",\n        )\n\n    def test_200_and_invalid_gzipped_body_must_fail(self):\n        req = Request(\"http://example.com\")\n        body = b\"<p>You are being redirected</p>\"\n        resp = Response(\n            req.url,\n            status=200,\n            body=body,\n            headers={\n                \"Content-Length\": str(len(body)),\n                \"Content-Type\": \"text/html\",\n                \"Content-Encoding\": \"gzip\",\n                \"Location\": \"http://example.com/login\",\n            },\n        )\n        self.assertRaises(OSError, self._download, request=req, response=resp)\n\n\nclass ResponseFromProcessRequestTest(ManagerTestCase):\n    \"\"\"Tests middleware returning a response from process_request.\"\"\"\n\n    def test_download_func_not_called(self):\n        resp = Response(\"http://example.com/index.html\")\n\n        class ResponseMiddleware:\n            def process_request(self, request, spider):\n                return resp\n\n        self.mwman._add_middleware(ResponseMiddleware())\n\n        req = Request(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self._wait(dfd)\n\n        self.assertIs(results[0], resp)\n        self.assertFalse(download_func.called)\n\n\nclass ProcessRequestInvalidOutput(ManagerTestCase):\n    \"\"\"Invalid return value for process_request method should raise an exception\"\"\"\n\n    def test_invalid_process_request(self):\n        req = Request(\"http://example.com/index.html\")\n\n        class InvalidProcessRequestMiddleware:\n            def process_request(self, request, spider):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessRequestMiddleware())\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self.assertIsInstance(results[0], Failure)\n        self.assertIsInstance(results[0].value, _InvalidOutput)\n\n\nclass ProcessResponseInvalidOutput(ManagerTestCase):\n    \"\"\"Invalid return value for process_response method should raise an exception\"\"\"\n\n    def test_invalid_process_response(self):\n        req = Request(\"http://example.com/index.html\")\n\n        class InvalidProcessResponseMiddleware:\n            def process_response(self, request, response, spider):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessResponseMiddleware())\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self.assertIsInstance(results[0], Failure)\n        self.assertIsInstance(results[0].value, _InvalidOutput)\n\n\nclass ProcessExceptionInvalidOutput(ManagerTestCase):\n    \"\"\"Invalid return value for process_exception method should raise an exception\"\"\"\n\n    def test_invalid_process_exception(self):\n        req = Request(\"http://example.com/index.html\")\n\n        class InvalidProcessExceptionMiddleware:\n            def process_request(self, request, spider):\n                raise Exception()\n\n            def process_exception(self, request, exception, spider):\n                return 1\n\n        self.mwman._add_middleware(InvalidProcessExceptionMiddleware())\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self.assertIsInstance(results[0], Failure)\n        self.assertIsInstance(results[0].value, _InvalidOutput)\n\n\nclass MiddlewareUsingDeferreds(ManagerTestCase):\n    \"\"\"Middlewares using Deferreds should work\"\"\"\n\n    def test_deferred(self):\n        resp = Response(\"http://example.com/index.html\")\n\n        class DeferredMiddleware:\n            def cb(self, result):\n                return result\n\n            def process_request(self, request, spider):\n                d = Deferred()\n                d.addCallback(self.cb)\n                d.callback(resp)\n                return d\n\n        self.mwman._add_middleware(DeferredMiddleware())\n        req = Request(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self._wait(dfd)\n\n        self.assertIs(results[0], resp)\n        self.assertFalse(download_func.called)\n\n\n@mark.usefixtures(\"reactor_pytest\")\nclass MiddlewareUsingCoro(ManagerTestCase):\n    \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n\n    def test_asyncdef(self):\n        resp = Response(\"http://example.com/index.html\")\n\n        class CoroMiddleware:\n            async def process_request(self, request, spider):\n                await defer.succeed(42)\n                return resp\n\n        self.mwman._add_middleware(CoroMiddleware())\n        req = Request(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self._wait(dfd)\n\n        self.assertIs(results[0], resp)\n        self.assertFalse(download_func.called)\n\n    @mark.only_asyncio()\n    def test_asyncdef_asyncio(self):\n        resp = Response(\"http://example.com/index.html\")\n\n        class CoroMiddleware:\n            async def process_request(self, request, spider):\n                await asyncio.sleep(0.1)\n                result = await get_from_asyncio_queue(resp)\n                return result\n\n        self.mwman._add_middleware(CoroMiddleware())\n        req = Request(\"http://example.com/index.html\")\n        download_func = mock.MagicMock()\n        dfd = self.mwman.download(download_func, req, self.spider)\n        results = []\n        dfd.addBoth(results.append)\n        self._wait(dfd)\n\n        self.assertIs(results[0], resp)\n        self.assertFalse(download_func.called)\n", "tests/test_request_cb_kwargs.py": "from typing import List\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.http import Request\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import MockServerSpider\n\n\nclass InjectArgumentsDownloaderMiddleware:\n    \"\"\"\n    Make sure downloader middlewares are able to update the keyword arguments\n    \"\"\"\n\n    def process_request(self, request, spider):\n        if request.callback.__name__ == \"parse_downloader_mw\":\n            request.cb_kwargs[\"from_process_request\"] = True\n        return None\n\n    def process_response(self, request, response, spider):\n        if request.callback.__name__ == \"parse_downloader_mw\":\n            request.cb_kwargs[\"from_process_response\"] = True\n        return response\n\n\nclass InjectArgumentsSpiderMiddleware:\n    \"\"\"\n    Make sure spider middlewares are able to update the keyword arguments\n    \"\"\"\n\n    def process_start_requests(self, start_requests, spider):\n        for request in start_requests:\n            if request.callback.__name__ == \"parse_spider_mw\":\n                request.cb_kwargs[\"from_process_start_requests\"] = True\n            yield request\n\n    def process_spider_input(self, response, spider):\n        request = response.request\n        if request.callback.__name__ == \"parse_spider_mw\":\n            request.cb_kwargs[\"from_process_spider_input\"] = True\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        for element in result:\n            if (\n                isinstance(element, Request)\n                and element.callback.__name__ == \"parse_spider_mw_2\"\n            ):\n                element.cb_kwargs[\"from_process_spider_output\"] = True\n            yield element\n\n\nclass KeywordArgumentsSpider(MockServerSpider):\n    name = \"kwargs\"\n    custom_settings = {\n        \"DOWNLOADER_MIDDLEWARES\": {\n            InjectArgumentsDownloaderMiddleware: 750,\n        },\n        \"SPIDER_MIDDLEWARES\": {\n            InjectArgumentsSpiderMiddleware: 750,\n        },\n    }\n\n    checks: List[bool] = []\n\n    def start_requests(self):\n        data = {\"key\": \"value\", \"number\": 123, \"callback\": \"some_callback\"}\n        yield Request(self.mockserver.url(\"/first\"), self.parse_first, cb_kwargs=data)\n        yield Request(\n            self.mockserver.url(\"/general_with\"), self.parse_general, cb_kwargs=data\n        )\n        yield Request(self.mockserver.url(\"/general_without\"), self.parse_general)\n        yield Request(self.mockserver.url(\"/no_kwargs\"), self.parse_no_kwargs)\n        yield Request(\n            self.mockserver.url(\"/default\"), self.parse_default, cb_kwargs=data\n        )\n        yield Request(\n            self.mockserver.url(\"/takes_less\"), self.parse_takes_less, cb_kwargs=data\n        )\n        yield Request(\n            self.mockserver.url(\"/takes_more\"), self.parse_takes_more, cb_kwargs=data\n        )\n        yield Request(self.mockserver.url(\"/downloader_mw\"), self.parse_downloader_mw)\n        yield Request(self.mockserver.url(\"/spider_mw\"), self.parse_spider_mw)\n\n    def parse_first(self, response, key, number):\n        self.checks.append(key == \"value\")\n        self.checks.append(number == 123)\n        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n        yield response.follow(\n            self.mockserver.url(\"/two\"),\n            self.parse_second,\n            cb_kwargs={\"new_key\": \"new_value\"},\n        )\n\n    def parse_second(self, response, new_key):\n        self.checks.append(new_key == \"new_value\")\n        self.crawler.stats.inc_value(\"boolean_checks\")\n\n    def parse_general(self, response, **kwargs):\n        if response.url.endswith(\"/general_with\"):\n            self.checks.append(kwargs[\"key\"] == \"value\")\n            self.checks.append(kwargs[\"number\"] == 123)\n            self.checks.append(kwargs[\"callback\"] == \"some_callback\")\n            self.crawler.stats.inc_value(\"boolean_checks\", 3)\n        elif response.url.endswith(\"/general_without\"):\n            self.checks.append(\n                kwargs == {}  # pylint: disable=use-implicit-booleaness-not-comparison\n            )\n            self.crawler.stats.inc_value(\"boolean_checks\")\n\n    def parse_no_kwargs(self, response):\n        self.checks.append(response.url.endswith(\"/no_kwargs\"))\n        self.crawler.stats.inc_value(\"boolean_checks\")\n\n    def parse_default(self, response, key, number=None, default=99):\n        self.checks.append(response.url.endswith(\"/default\"))\n        self.checks.append(key == \"value\")\n        self.checks.append(number == 123)\n        self.checks.append(default == 99)\n        self.crawler.stats.inc_value(\"boolean_checks\", 4)\n\n    def parse_takes_less(self, response, key, callback):\n        \"\"\"\n        Should raise\n        TypeError: parse_takes_less() got an unexpected keyword argument 'number'\n        \"\"\"\n\n    def parse_takes_more(self, response, key, number, callback, other):\n        \"\"\"\n        Should raise\n        TypeError: parse_takes_more() missing 1 required positional argument: 'other'\n        \"\"\"\n\n    def parse_downloader_mw(\n        self, response, from_process_request, from_process_response\n    ):\n        self.checks.append(bool(from_process_request))\n        self.checks.append(bool(from_process_response))\n        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n\n    def parse_spider_mw(\n        self, response, from_process_spider_input, from_process_start_requests\n    ):\n        self.checks.append(bool(from_process_spider_input))\n        self.checks.append(bool(from_process_start_requests))\n        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n        return Request(self.mockserver.url(\"/spider_mw_2\"), self.parse_spider_mw_2)\n\n    def parse_spider_mw_2(self, response, from_process_spider_output):\n        self.checks.append(bool(from_process_spider_output))\n        self.crawler.stats.inc_value(\"boolean_checks\", 1)\n\n\nclass CallbackKeywordArgumentsTestCase(TestCase):\n    maxDiff = None\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_callback_kwargs(self):\n        crawler = get_crawler(KeywordArgumentsSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        self.assertTrue(all(crawler.spider.checks))\n        self.assertEqual(\n            len(crawler.spider.checks), crawler.stats.get_value(\"boolean_checks\")\n        )\n        # check exceptions for argument mismatch\n        exceptions = {}\n        for line in log.records:\n            for key in (\"takes_less\", \"takes_more\"):\n                if key in line.getMessage():\n                    exceptions[key] = line\n        self.assertEqual(exceptions[\"takes_less\"].exc_info[0], TypeError)\n        self.assertTrue(\n            str(exceptions[\"takes_less\"].exc_info[1]).endswith(\n                \"parse_takes_less() got an unexpected keyword argument 'number'\"\n            ),\n            msg=\"Exception message: \" + str(exceptions[\"takes_less\"].exc_info[1]),\n        )\n        self.assertEqual(exceptions[\"takes_more\"].exc_info[0], TypeError)\n        self.assertTrue(\n            str(exceptions[\"takes_more\"].exc_info[1]).endswith(\n                \"parse_takes_more() missing 1 required positional argument: 'other'\"\n            ),\n            msg=\"Exception message: \" + str(exceptions[\"takes_more\"].exc_info[1]),\n        )\n", "tests/test_utils_serialize.py": "import dataclasses\nimport datetime\nimport json\nimport unittest\nfrom decimal import Decimal\n\nimport attr\nfrom twisted.internet import defer\n\nfrom scrapy.http import Request, Response\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\n\nclass JsonEncoderTestCase(unittest.TestCase):\n    def setUp(self):\n        self.encoder = ScrapyJSONEncoder(sort_keys=True)\n\n    def test_encode_decode(self):\n        dt = datetime.datetime(2010, 1, 2, 10, 11, 12)\n        dts = \"2010-01-02 10:11:12\"\n        d = datetime.date(2010, 1, 2)\n        ds = \"2010-01-02\"\n        t = datetime.time(10, 11, 12)\n        ts = \"10:11:12\"\n        dec = Decimal(\"1000.12\")\n        decs = \"1000.12\"\n        s = {\"foo\"}\n        ss = [\"foo\"]\n        dt_set = {dt}\n        dt_sets = [dts]\n\n        for input, output in [\n            (\"foo\", \"foo\"),\n            (d, ds),\n            (t, ts),\n            (dt, dts),\n            (dec, decs),\n            ([\"foo\", d], [\"foo\", ds]),\n            (s, ss),\n            (dt_set, dt_sets),\n        ]:\n            self.assertEqual(\n                self.encoder.encode(input), json.dumps(output, sort_keys=True)\n            )\n\n    def test_encode_deferred(self):\n        self.assertIn(\"Deferred\", self.encoder.encode(defer.Deferred()))\n\n    def test_encode_request(self):\n        r = Request(\"http://www.example.com/lala\")\n        rs = self.encoder.encode(r)\n        self.assertIn(r.method, rs)\n        self.assertIn(r.url, rs)\n\n    def test_encode_response(self):\n        r = Response(\"http://www.example.com/lala\")\n        rs = self.encoder.encode(r)\n        self.assertIn(r.url, rs)\n        self.assertIn(str(r.status), rs)\n\n    def test_encode_dataclass_item(self) -> None:\n        @dataclasses.dataclass\n        class TestDataClass:\n            name: str\n            url: str\n            price: int\n\n        item = TestDataClass(name=\"Product\", url=\"http://product.org\", price=1)\n        encoded = self.encoder.encode(item)\n        self.assertEqual(\n            encoded, '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n        )\n\n    def test_encode_attrs_item(self):\n        @attr.s\n        class AttrsItem:\n            name = attr.ib(type=str)\n            url = attr.ib(type=str)\n            price = attr.ib(type=int)\n\n        item = AttrsItem(name=\"Product\", url=\"http://product.org\", price=1)\n        encoded = self.encoder.encode(item)\n        self.assertEqual(\n            encoded, '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n        )\n", "tests/ftpserver.py": "from argparse import ArgumentParser\n\nfrom pyftpdlib.authorizers import DummyAuthorizer\nfrom pyftpdlib.handlers import FTPHandler\nfrom pyftpdlib.servers import FTPServer\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\"-d\", \"--directory\")\n    args = parser.parse_args()\n\n    authorizer = DummyAuthorizer()\n    full_permissions = \"elradfmwMT\"\n    authorizer.add_anonymous(args.directory, perm=full_permissions)\n    handler = FTPHandler\n    handler.authorizer = authorizer\n    address = (\"127.0.0.1\", 2121)\n    server = FTPServer(address, handler)\n    server.serve_forever()\n\n\nif __name__ == \"__main__\":\n    main()\n", "tests/test_middleware.py": "from twisted.trial import unittest\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.middleware import MiddlewareManager\nfrom scrapy.settings import Settings\n\n\nclass M1:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n    def process(self, response, request, spider):\n        pass\n\n\nclass M2:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n    pass\n\n\nclass M3:\n    def process(self, response, request, spider):\n        pass\n\n\nclass MOff:\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider):\n        pass\n\n    def __init__(self):\n        raise NotConfigured(\"foo\")\n\n\nclass TestMiddlewareManager(MiddlewareManager):\n    @classmethod\n    def _get_mwlist_from_settings(cls, settings):\n        return [M1, MOff, M3]\n\n    def _add_middleware(self, mw):\n        super()._add_middleware(mw)\n        if hasattr(mw, \"process\"):\n            self.methods[\"process\"].append(mw.process)\n\n\nclass MiddlewareManagerTest(unittest.TestCase):\n    def test_init(self):\n        m1, m2, m3 = M1(), M2(), M3()\n        mwman = TestMiddlewareManager(m1, m2, m3)\n        self.assertEqual(\n            list(mwman.methods[\"open_spider\"]), [m1.open_spider, m2.open_spider]\n        )\n        self.assertEqual(\n            list(mwman.methods[\"close_spider\"]), [m2.close_spider, m1.close_spider]\n        )\n        self.assertEqual(list(mwman.methods[\"process\"]), [m1.process, m3.process])\n\n    def test_methods(self):\n        mwman = TestMiddlewareManager(M1(), M2(), M3())\n        self.assertEqual(\n            [x.__self__.__class__ for x in mwman.methods[\"open_spider\"]], [M1, M2]\n        )\n        self.assertEqual(\n            [x.__self__.__class__ for x in mwman.methods[\"close_spider\"]], [M2, M1]\n        )\n        self.assertEqual(\n            [x.__self__.__class__ for x in mwman.methods[\"process\"]], [M1, M3]\n        )\n\n    def test_enabled(self):\n        m1, m2, m3 = M1(), M2(), M3()\n        mwman = MiddlewareManager(m1, m2, m3)\n        self.assertEqual(mwman.middlewares, (m1, m2, m3))\n\n    def test_enabled_from_settings(self):\n        settings = Settings()\n        mwman = TestMiddlewareManager.from_settings(settings)\n        classes = [x.__class__ for x in mwman.middlewares]\n        self.assertEqual(classes, [M1, M3])\n", "tests/test_scheduler_base.py": "from typing import Dict, Optional\nfrom unittest import TestCase\nfrom urllib.parse import urljoin\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase as TwistedTestCase\n\nfrom scrapy.core.scheduler import BaseScheduler\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.request import fingerprint\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\n\nPATHS = [\"/a\", \"/b\", \"/c\"]\nURLS = [urljoin(\"https://example.org\", p) for p in PATHS]\n\n\nclass MinimalScheduler:\n    def __init__(self) -> None:\n        self.requests: Dict[bytes, Request] = {}\n\n    def has_pending_requests(self) -> bool:\n        return bool(self.requests)\n\n    def enqueue_request(self, request: Request) -> bool:\n        fp = fingerprint(request)\n        if fp not in self.requests:\n            self.requests[fp] = request\n            return True\n        return False\n\n    def next_request(self) -> Optional[Request]:\n        if self.has_pending_requests():\n            fp, request = self.requests.popitem()\n            return request\n        return None\n\n\nclass SimpleScheduler(MinimalScheduler):\n    def open(self, spider: Spider) -> defer.Deferred:\n        return defer.succeed(\"open\")\n\n    def close(self, reason: str) -> defer.Deferred:\n        return defer.succeed(\"close\")\n\n    def __len__(self) -> int:\n        return len(self.requests)\n\n\nclass TestSpider(Spider):\n    name = \"test\"\n\n    def __init__(self, mockserver, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = map(mockserver.url, PATHS)\n\n    def parse(self, response):\n        return {\"path\": urlparse_cached(response).path}\n\n\nclass InterfaceCheckMixin:\n    def test_scheduler_class(self):\n        self.assertTrue(isinstance(self.scheduler, BaseScheduler))\n        self.assertTrue(issubclass(self.scheduler.__class__, BaseScheduler))\n\n\nclass BaseSchedulerTest(TestCase, InterfaceCheckMixin):\n    def setUp(self):\n        self.scheduler = BaseScheduler()\n\n    def test_methods(self):\n        self.assertIsNone(self.scheduler.open(Spider(\"foo\")))\n        self.assertIsNone(self.scheduler.close(\"finished\"))\n        self.assertRaises(NotImplementedError, self.scheduler.has_pending_requests)\n        self.assertRaises(\n            NotImplementedError,\n            self.scheduler.enqueue_request,\n            Request(\"https://example.org\"),\n        )\n        self.assertRaises(NotImplementedError, self.scheduler.next_request)\n\n\nclass MinimalSchedulerTest(TestCase, InterfaceCheckMixin):\n    def setUp(self):\n        self.scheduler = MinimalScheduler()\n\n    def test_open_close(self):\n        with self.assertRaises(AttributeError):\n            self.scheduler.open(Spider(\"foo\"))\n        with self.assertRaises(AttributeError):\n            self.scheduler.close(\"finished\")\n\n    def test_len(self):\n        with self.assertRaises(AttributeError):\n            self.scheduler.__len__()\n        with self.assertRaises(TypeError):\n            len(self.scheduler)\n\n    def test_enqueue_dequeue(self):\n        self.assertFalse(self.scheduler.has_pending_requests())\n        for url in URLS:\n            self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n            self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n        self.assertTrue(self.scheduler.has_pending_requests)\n\n        dequeued = []\n        while self.scheduler.has_pending_requests():\n            request = self.scheduler.next_request()\n            dequeued.append(request.url)\n        self.assertEqual(set(dequeued), set(URLS))\n        self.assertFalse(self.scheduler.has_pending_requests())\n\n\nclass SimpleSchedulerTest(TwistedTestCase, InterfaceCheckMixin):\n    def setUp(self):\n        self.scheduler = SimpleScheduler()\n\n    @defer.inlineCallbacks\n    def test_enqueue_dequeue(self):\n        open_result = yield self.scheduler.open(Spider(\"foo\"))\n        self.assertEqual(open_result, \"open\")\n        self.assertFalse(self.scheduler.has_pending_requests())\n\n        for url in URLS:\n            self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n            self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n\n        self.assertTrue(self.scheduler.has_pending_requests())\n        self.assertEqual(len(self.scheduler), len(URLS))\n\n        dequeued = []\n        while self.scheduler.has_pending_requests():\n            request = self.scheduler.next_request()\n            dequeued.append(request.url)\n        self.assertEqual(set(dequeued), set(URLS))\n\n        self.assertFalse(self.scheduler.has_pending_requests())\n        self.assertEqual(len(self.scheduler), 0)\n\n        close_result = yield self.scheduler.close(\"\")\n        self.assertEqual(close_result, \"close\")\n\n\nclass MinimalSchedulerCrawlTest(TwistedTestCase):\n    scheduler_cls = MinimalScheduler\n\n    @defer.inlineCallbacks\n    def test_crawl(self):\n        with MockServer() as mockserver:\n            settings = {\n                \"SCHEDULER\": self.scheduler_cls,\n            }\n            with LogCapture() as log:\n                crawler = get_crawler(TestSpider, settings)\n                yield crawler.crawl(mockserver)\n            for path in PATHS:\n                self.assertIn(f\"{{'path': '{path}'}}\", str(log))\n            self.assertIn(f\"'item_scraped_count': {len(PATHS)}\", str(log))\n\n\nclass SimpleSchedulerCrawlTest(MinimalSchedulerCrawlTest):\n    scheduler_cls = SimpleScheduler\n", "tests/test_spidermiddleware_offsite.py": "import warnings\nfrom unittest import TestCase\nfrom urllib.parse import urlparse\n\nfrom scrapy.http import Request, Response\nfrom scrapy.spidermiddlewares.offsite import OffsiteMiddleware, PortWarning, URLWarning\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestOffsiteMiddleware(TestCase):\n    def setUp(self):\n        crawler = get_crawler(Spider)\n        self.spider = crawler._create_spider(**self._get_spiderargs())\n        self.mw = OffsiteMiddleware.from_crawler(crawler)\n        self.mw.spider_opened(self.spider)\n\n    def _get_spiderargs(self):\n        return {\n            \"name\": \"foo\",\n            \"allowed_domains\": [\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n        }\n\n    def test_process_spider_output(self):\n        res = Response(\"http://scrapytest.org\")\n\n        onsite_reqs = [\n            Request(\"http://scrapytest.org/1\"),\n            Request(\"http://scrapy.org/1\"),\n            Request(\"http://sub.scrapy.org/1\"),\n            Request(\"http://offsite.tld/letmepass\", dont_filter=True),\n            Request(\"http://scrapy.test.org/\"),\n            Request(\"http://scrapy.test.org:8000/\"),\n        ]\n        offsite_reqs = [\n            Request(\"http://scrapy2.org\"),\n            Request(\"http://offsite.tld/\"),\n            Request(\"http://offsite.tld/scrapytest.org\"),\n            Request(\"http://offsite.tld/rogue.scrapytest.org\"),\n            Request(\"http://rogue.scrapytest.org.haha.com\"),\n            Request(\"http://roguescrapytest.org\"),\n            Request(\"http://test.org/\"),\n            Request(\"http://notscrapy.test.org/\"),\n        ]\n        reqs = onsite_reqs + offsite_reqs\n\n        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n        self.assertEqual(out, onsite_reqs)\n\n\nclass TestOffsiteMiddleware2(TestOffsiteMiddleware):\n    def _get_spiderargs(self):\n        return {\"name\": \"foo\", \"allowed_domains\": None}\n\n    def test_process_spider_output(self):\n        res = Response(\"http://scrapytest.org\")\n        reqs = [Request(\"http://a.com/b.html\"), Request(\"http://b.com/1\")]\n        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n        self.assertEqual(out, reqs)\n\n\nclass TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n    def _get_spiderargs(self):\n        return {\"name\": \"foo\"}\n\n\nclass TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n    def _get_spiderargs(self):\n        bad_hostname = urlparse(\"http:////scrapytest.org\").hostname\n        return {\n            \"name\": \"foo\",\n            \"allowed_domains\": [\"scrapytest.org\", None, bad_hostname],\n        }\n\n    def test_process_spider_output(self):\n        res = Response(\"http://scrapytest.org\")\n        reqs = [Request(\"http://scrapytest.org/1\")]\n        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n        self.assertEqual(out, reqs)\n\n\nclass TestOffsiteMiddleware5(TestOffsiteMiddleware4):\n    def test_get_host_regex(self):\n        self.spider.allowed_domains = [\n            \"http://scrapytest.org\",\n            \"scrapy.org\",\n            \"scrapy.test.org\",\n        ]\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            self.mw.get_host_regex(self.spider)\n            assert issubclass(w[-1].category, URLWarning)\n\n\nclass TestOffsiteMiddleware6(TestOffsiteMiddleware4):\n    def test_get_host_regex(self):\n        self.spider.allowed_domains = [\n            \"scrapytest.org:8000\",\n            \"scrapy.org\",\n            \"scrapy.test.org\",\n        ]\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            self.mw.get_host_regex(self.spider)\n            assert issubclass(w[-1].category, PortWarning)\n", "tests/test_exporters.py": "import dataclasses\nimport json\nimport marshal\nimport pickle\nimport re\nimport tempfile\nimport unittest\nfrom datetime import datetime\nfrom io import BytesIO\nfrom typing import Any\n\nimport lxml.etree\nfrom itemadapter import ItemAdapter\n\nfrom scrapy.exporters import (\n    BaseItemExporter,\n    CsvItemExporter,\n    JsonItemExporter,\n    JsonLinesItemExporter,\n    MarshalItemExporter,\n    PickleItemExporter,\n    PprintItemExporter,\n    PythonItemExporter,\n    XmlItemExporter,\n)\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.python import to_unicode\n\n\ndef custom_serializer(value):\n    return str(int(value) + 2)\n\n\nclass TestItem(Item):\n    name = Field()\n    age = Field()\n\n\nclass CustomFieldItem(Item):\n    name = Field()\n    age = Field(serializer=custom_serializer)\n\n\n@dataclasses.dataclass\nclass TestDataClass:\n    name: str\n    age: int\n\n\n@dataclasses.dataclass\nclass CustomFieldDataclass:\n    name: str\n    age: int = dataclasses.field(metadata={\"serializer\": custom_serializer})\n\n\nclass BaseItemExporterTest(unittest.TestCase):\n    item_class: type = TestItem\n    custom_field_item_class: type = CustomFieldItem\n\n    def setUp(self):\n        self.i = self.item_class(name=\"John\\xa3\", age=\"22\")\n        self.output = BytesIO()\n        self.ie = self._get_exporter()\n\n    def _get_exporter(self, **kwargs):\n        return BaseItemExporter(**kwargs)\n\n    def _check_output(self):\n        pass\n\n    def _assert_expected_item(self, exported_dict):\n        for k, v in exported_dict.items():\n            exported_dict[k] = to_unicode(v)\n        self.assertEqual(self.i, self.item_class(**exported_dict))\n\n    def _get_nonstring_types_item(self):\n        return {\n            \"boolean\": False,\n            \"number\": 22,\n            \"time\": datetime(2015, 1, 1, 1, 1, 1),\n            \"float\": 3.14,\n        }\n\n    def assertItemExportWorks(self, item):\n        self.ie.start_exporting()\n        try:\n            self.ie.export_item(item)\n        except NotImplementedError:\n            if self.ie.__class__ is not BaseItemExporter:\n                raise\n        self.ie.finish_exporting()\n        # Delete the item exporter object, so that if it causes the output\n        # file handle to be closed, which should not be the case, follow-up\n        # interactions with the output file handle will surface the issue.\n        del self.ie\n        self._check_output()\n\n    def test_export_item(self):\n        self.assertItemExportWorks(self.i)\n\n    def test_export_dict_item(self):\n        self.assertItemExportWorks(ItemAdapter(self.i).asdict())\n\n    def test_serialize_field(self):\n        a = ItemAdapter(self.i)\n        res = self.ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"])\n        self.assertEqual(res, \"John\\xa3\")\n\n        res = self.ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"])\n        self.assertEqual(res, \"22\")\n\n    def test_fields_to_export(self):\n        ie = self._get_exporter(fields_to_export=[\"name\"])\n        self.assertEqual(\n            list(ie._get_serialized_fields(self.i)), [(\"name\", \"John\\xa3\")]\n        )\n\n        ie = self._get_exporter(fields_to_export=[\"name\"], encoding=\"latin-1\")\n        _, name = list(ie._get_serialized_fields(self.i))[0]\n        assert isinstance(name, str)\n        self.assertEqual(name, \"John\\xa3\")\n\n        ie = self._get_exporter(fields_to_export={\"name\": \"\u540d\u7a31\"})\n        self.assertEqual(\n            list(ie._get_serialized_fields(self.i)), [(\"\u540d\u7a31\", \"John\\xa3\")]\n        )\n\n    def test_field_custom_serializer(self):\n        i = self.custom_field_item_class(name=\"John\\xa3\", age=\"22\")\n        a = ItemAdapter(i)\n        ie = self._get_exporter()\n        self.assertEqual(\n            ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"]), \"John\\xa3\"\n        )\n        self.assertEqual(\n            ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"]), \"24\"\n        )\n\n\nclass BaseItemExporterDataclassTest(BaseItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass PythonItemExporterTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        return PythonItemExporter(**kwargs)\n\n    def test_invalid_option(self):\n        with self.assertRaisesRegex(TypeError, \"Unexpected options: invalid_option\"):\n            PythonItemExporter(invalid_option=\"something\")\n\n    def test_nested_item(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = {\"name\": \"Maria\", \"age\": i1}\n        i3 = self.item_class(name=\"Jesus\", age=i2)\n        ie = self._get_exporter()\n        exported = ie.export_item(i3)\n        self.assertEqual(type(exported), dict)\n        self.assertEqual(\n            exported,\n            {\n                \"age\": {\"age\": {\"age\": \"22\", \"name\": \"Joseph\"}, \"name\": \"Maria\"},\n                \"name\": \"Jesus\",\n            },\n        )\n        self.assertEqual(type(exported[\"age\"]), dict)\n        self.assertEqual(type(exported[\"age\"][\"age\"]), dict)\n\n    def test_export_list(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = self.item_class(name=\"Maria\", age=[i1])\n        i3 = self.item_class(name=\"Jesus\", age=[i2])\n        ie = self._get_exporter()\n        exported = ie.export_item(i3)\n        self.assertEqual(\n            exported,\n            {\n                \"age\": [{\"age\": [{\"age\": \"22\", \"name\": \"Joseph\"}], \"name\": \"Maria\"}],\n                \"name\": \"Jesus\",\n            },\n        )\n        self.assertEqual(type(exported[\"age\"][0]), dict)\n        self.assertEqual(type(exported[\"age\"][0][\"age\"][0]), dict)\n\n    def test_export_item_dict_list(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = {\"name\": \"Maria\", \"age\": [i1]}\n        i3 = self.item_class(name=\"Jesus\", age=[i2])\n        ie = self._get_exporter()\n        exported = ie.export_item(i3)\n        self.assertEqual(\n            exported,\n            {\n                \"age\": [{\"age\": [{\"age\": \"22\", \"name\": \"Joseph\"}], \"name\": \"Maria\"}],\n                \"name\": \"Jesus\",\n            },\n        )\n        self.assertEqual(type(exported[\"age\"][0]), dict)\n        self.assertEqual(type(exported[\"age\"][0][\"age\"][0]), dict)\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        ie = self._get_exporter()\n        exported = ie.export_item(item)\n        self.assertEqual(exported, item)\n\n\nclass PythonItemExporterDataclassTest(PythonItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass PprintItemExporterTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        return PprintItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        self._assert_expected_item(eval(self.output.getvalue()))\n\n\nclass PprintItemExporterDataclassTest(PprintItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass PickleItemExporterTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        return PickleItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        self._assert_expected_item(pickle.loads(self.output.getvalue()))\n\n    def test_export_multiple_items(self):\n        i1 = self.item_class(name=\"hello\", age=\"world\")\n        i2 = self.item_class(name=\"bye\", age=\"world\")\n        f = BytesIO()\n        ie = PickleItemExporter(f)\n        ie.start_exporting()\n        ie.export_item(i1)\n        ie.export_item(i2)\n        ie.finish_exporting()\n        del ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        f.seek(0)\n        self.assertEqual(self.item_class(**pickle.load(f)), i1)\n        self.assertEqual(self.item_class(**pickle.load(f)), i2)\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        fp = BytesIO()\n        ie = PickleItemExporter(fp)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        self.assertEqual(pickle.loads(fp.getvalue()), item)\n\n\nclass PickleItemExporterDataclassTest(PickleItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass MarshalItemExporterTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        self.output = tempfile.TemporaryFile()\n        return MarshalItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        self.output.seek(0)\n        self._assert_expected_item(marshal.load(self.output))\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        item.pop(\"time\")  # datetime is not marshallable\n        fp = tempfile.TemporaryFile()\n        ie = MarshalItemExporter(fp)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        fp.seek(0)\n        self.assertEqual(marshal.load(fp), item)\n\n\nclass MarshalItemExporterDataclassTest(MarshalItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass CsvItemExporterTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        self.output = tempfile.TemporaryFile()\n        return CsvItemExporter(self.output, **kwargs)\n\n    def assertCsvEqual(self, first, second, msg=None):\n        def split_csv(csv):\n            return [\n                sorted(re.split(r\"(,|\\s+)\", line))\n                for line in to_unicode(csv).splitlines(True)\n            ]\n\n        return self.assertEqual(split_csv(first), split_csv(second), msg=msg)\n\n    def _check_output(self):\n        self.output.seek(0)\n        self.assertCsvEqual(\n            to_unicode(self.output.read()), \"age,name\\r\\n22,John\\xa3\\r\\n\"\n        )\n\n    def assertExportResult(self, item, expected, **kwargs):\n        fp = BytesIO()\n        ie = CsvItemExporter(fp, **kwargs)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        self.assertCsvEqual(fp.getvalue(), expected)\n\n    def test_header_export_all(self):\n        self.assertExportResult(\n            item=self.i,\n            fields_to_export=ItemAdapter(self.i).field_names(),\n            expected=b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n\",\n        )\n\n    def test_header_export_all_dict(self):\n        self.assertExportResult(\n            item=ItemAdapter(self.i).asdict(),\n            expected=b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n\",\n        )\n\n    def test_header_export_single_field(self):\n        for item in [self.i, ItemAdapter(self.i).asdict()]:\n            self.assertExportResult(\n                item=item,\n                fields_to_export=[\"age\"],\n                expected=b\"age\\r\\n22\\r\\n\",\n            )\n\n    def test_header_export_two_items(self):\n        for item in [self.i, ItemAdapter(self.i).asdict()]:\n            output = BytesIO()\n            ie = CsvItemExporter(output)\n            ie.start_exporting()\n            ie.export_item(item)\n            ie.export_item(item)\n            ie.finish_exporting()\n            del ie  # See the first \u201cdel self.ie\u201d in this file for context.\n            self.assertCsvEqual(\n                output.getvalue(), b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n\"\n            )\n\n    def test_header_no_header_line(self):\n        for item in [self.i, ItemAdapter(self.i).asdict()]:\n            self.assertExportResult(\n                item=item,\n                include_headers_line=False,\n                expected=b\"22,John\\xc2\\xa3\\r\\n\",\n            )\n\n    def test_join_multivalue(self):\n        class TestItem2(Item):\n            name = Field()\n            friends = Field()\n\n        for cls in TestItem2, dict:\n            self.assertExportResult(\n                item=cls(name=\"John\", friends=[\"Mary\", \"Paul\"]),\n                include_headers_line=False,\n                expected='\"Mary,Paul\",John\\r\\n',\n            )\n\n    def test_join_multivalue_not_strings(self):\n        self.assertExportResult(\n            item={\"name\": \"John\", \"friends\": [4, 8]},\n            include_headers_line=False,\n            expected='\"[4, 8]\",John\\r\\n',\n        )\n\n    def test_nonstring_types_item(self):\n        self.assertExportResult(\n            item=self._get_nonstring_types_item(),\n            include_headers_line=False,\n            expected=\"22,False,3.14,2015-01-01 01:01:01\\r\\n\",\n        )\n\n    def test_errors_default(self):\n        with self.assertRaises(UnicodeEncodeError):\n            self.assertExportResult(\n                item={\"text\": \"W\\u0275\\u200Brd\"},\n                expected=None,\n                encoding=\"windows-1251\",\n            )\n\n    def test_errors_xmlcharrefreplace(self):\n        self.assertExportResult(\n            item={\"text\": \"W\\u0275\\u200Brd\"},\n            include_headers_line=False,\n            expected=\"W&#629;&#8203;rd\\r\\n\",\n            encoding=\"windows-1251\",\n            errors=\"xmlcharrefreplace\",\n        )\n\n\nclass CsvItemExporterDataclassTest(CsvItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass XmlItemExporterTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        return XmlItemExporter(self.output, **kwargs)\n\n    def assertXmlEquivalent(self, first, second, msg=None):\n        def xmltuple(elem):\n            children = list(elem.iterchildren())\n            if children:\n                return [(child.tag, sorted(xmltuple(child))) for child in children]\n            return [(elem.tag, [(elem.text, ())])]\n\n        def xmlsplit(xmlcontent):\n            doc = lxml.etree.fromstring(xmlcontent)\n            return xmltuple(doc)\n\n        return self.assertEqual(xmlsplit(first), xmlsplit(second), msg)\n\n    def assertExportResult(self, item, expected_value):\n        fp = BytesIO()\n        ie = XmlItemExporter(fp)\n        ie.start_exporting()\n        ie.export_item(item)\n        ie.finish_exporting()\n        del ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        self.assertXmlEquivalent(fp.getvalue(), expected_value)\n\n    def _check_output(self):\n        expected_value = (\n            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n            b\"<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>\"\n        )\n        self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n\n    def test_multivalued_fields(self):\n        self.assertExportResult(\n            self.item_class(name=[\"John\\xa3\", \"Doe\"], age=[1, 2, 3]),\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n            <items>\n                <item>\n                    <name><value>John\\xc2\\xa3</value><value>Doe</value></name>\n                    <age><value>1</value><value>2</value><value>3</value></age>\n                </item>\n            </items>\n            \"\"\",\n        )\n\n    def test_nested_item(self):\n        i1 = {\"name\": \"foo\\xa3hoo\", \"age\": \"22\"}\n        i2 = {\"name\": \"bar\", \"age\": i1}\n        i3 = self.item_class(name=\"buz\", age=i2)\n\n        self.assertExportResult(\n            i3,\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n                <items>\n                    <item>\n                        <age>\n                            <age>\n                                <age>22</age>\n                                <name>foo\\xc2\\xa3hoo</name>\n                            </age>\n                            <name>bar</name>\n                        </age>\n                        <name>buz</name>\n                    </item>\n                </items>\n            \"\"\",\n        )\n\n    def test_nested_list_item(self):\n        i1 = {\"name\": \"foo\"}\n        i2 = {\"name\": \"bar\", \"v2\": {\"egg\": [\"spam\"]}}\n        i3 = self.item_class(name=\"buz\", age=[i1, i2])\n\n        self.assertExportResult(\n            i3,\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n                <items>\n                    <item>\n                        <age>\n                            <value><name>foo</name></value>\n                            <value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>\n                        </age>\n                        <name>buz</name>\n                    </item>\n                </items>\n            \"\"\",\n        )\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        self.assertExportResult(\n            item,\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n                <items>\n                   <item>\n                       <float>3.14</float>\n                       <boolean>False</boolean>\n                       <number>22</number>\n                       <time>2015-01-01 01:01:01</time>\n                   </item>\n                </items>\n            \"\"\",\n        )\n\n\nclass XmlItemExporterDataclassTest(XmlItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass JsonLinesItemExporterTest(BaseItemExporterTest):\n    _expected_nested: Any = {\n        \"name\": \"Jesus\",\n        \"age\": {\"name\": \"Maria\", \"age\": {\"name\": \"Joseph\", \"age\": \"22\"}},\n    }\n\n    def _get_exporter(self, **kwargs):\n        return JsonLinesItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        exported = json.loads(to_unicode(self.output.getvalue().strip()))\n        self.assertEqual(exported, ItemAdapter(self.i).asdict())\n\n    def test_nested_item(self):\n        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n        i2 = {\"name\": \"Maria\", \"age\": i1}\n        i3 = self.item_class(name=\"Jesus\", age=i2)\n        self.ie.start_exporting()\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        del self.ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        self.assertEqual(exported, self._expected_nested)\n\n    def test_extra_keywords(self):\n        self.ie = self._get_exporter(sort_keys=True)\n        self.test_export_item()\n        self._check_output()\n        self.assertRaises(TypeError, self._get_exporter, foo_unknown_keyword_bar=True)\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        del self.ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        item[\"time\"] = str(item[\"time\"])\n        self.assertEqual(exported, item)\n\n\nclass JsonLinesItemExporterDataclassTest(JsonLinesItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass JsonItemExporterTest(JsonLinesItemExporterTest):\n    _expected_nested = [JsonLinesItemExporterTest._expected_nested]\n\n    def _get_exporter(self, **kwargs):\n        return JsonItemExporter(self.output, **kwargs)\n\n    def _check_output(self):\n        exported = json.loads(to_unicode(self.output.getvalue().strip()))\n        self.assertEqual(exported, [ItemAdapter(self.i).asdict()])\n\n    def assertTwoItemsExported(self, item):\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        del self.ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        self.assertEqual(\n            exported, [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()]\n        )\n\n    def test_two_items(self):\n        self.assertTwoItemsExported(self.i)\n\n    def test_two_dict_items(self):\n        self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n\n    def test_two_items_with_failure_between(self):\n        i1 = TestItem(name=\"Joseph\\xa3\", age=\"22\")\n        i2 = TestItem(\n            name=\"Maria\", age=1j\n        )  # Invalid datetimes didn't consistently fail between Python versions\n        i3 = TestItem(name=\"Jesus\", age=\"44\")\n        self.ie.start_exporting()\n        self.ie.export_item(i1)\n        self.assertRaises(TypeError, self.ie.export_item, i2)\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        self.assertEqual(exported, [dict(i1), dict(i3)])\n\n    def test_nested_item(self):\n        i1 = self.item_class(name=\"Joseph\\xa3\", age=\"22\")\n        i2 = self.item_class(name=\"Maria\", age=i1)\n        i3 = self.item_class(name=\"Jesus\", age=i2)\n        self.ie.start_exporting()\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        del self.ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        expected = {\n            \"name\": \"Jesus\",\n            \"age\": {\"name\": \"Maria\", \"age\": ItemAdapter(i1).asdict()},\n        }\n        self.assertEqual(exported, [expected])\n\n    def test_nested_dict_item(self):\n        i1 = {\"name\": \"Joseph\\xa3\", \"age\": \"22\"}\n        i2 = self.item_class(name=\"Maria\", age=i1)\n        i3 = {\"name\": \"Jesus\", \"age\": i2}\n        self.ie.start_exporting()\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        del self.ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        expected = {\"name\": \"Jesus\", \"age\": {\"name\": \"Maria\", \"age\": i1}}\n        self.assertEqual(exported, [expected])\n\n    def test_nonstring_types_item(self):\n        item = self._get_nonstring_types_item()\n        self.ie.start_exporting()\n        self.ie.export_item(item)\n        self.ie.finish_exporting()\n        del self.ie  # See the first \u201cdel self.ie\u201d in this file for context.\n        exported = json.loads(to_unicode(self.output.getvalue()))\n        item[\"time\"] = str(item[\"time\"])\n        self.assertEqual(exported, [item])\n\n\nclass JsonItemExporterToBytesTest(BaseItemExporterTest):\n    def _get_exporter(self, **kwargs):\n        kwargs[\"encoding\"] = \"latin\"\n        return JsonItemExporter(self.output, **kwargs)\n\n    def test_two_items_with_failure_between(self):\n        i1 = TestItem(name=\"Joseph\", age=\"22\")\n        i2 = TestItem(name=\"\\u263a\", age=\"11\")\n        i3 = TestItem(name=\"Jesus\", age=\"44\")\n        self.ie.start_exporting()\n        self.ie.export_item(i1)\n        self.assertRaises(UnicodeEncodeError, self.ie.export_item, i2)\n        self.ie.export_item(i3)\n        self.ie.finish_exporting()\n        exported = json.loads(to_unicode(self.output.getvalue(), encoding=\"latin\"))\n        self.assertEqual(exported, [dict(i1), dict(i3)])\n\n\nclass JsonItemExporterDataclassTest(JsonItemExporterTest):\n    item_class = TestDataClass\n    custom_field_item_class = CustomFieldDataclass\n\n\nclass CustomExporterItemTest(unittest.TestCase):\n    item_class: type = TestItem\n\n    def setUp(self):\n        if self.item_class is None:\n            raise unittest.SkipTest(\"item class is None\")\n\n    def test_exporter_custom_serializer(self):\n        class CustomItemExporter(BaseItemExporter):\n            def serialize_field(self, field, name, value):\n                if name == \"age\":\n                    return str(int(value) + 1)\n                return super().serialize_field(field, name, value)\n\n        i = self.item_class(name=\"John\", age=\"22\")\n        a = ItemAdapter(i)\n        ie = CustomItemExporter()\n\n        self.assertEqual(\n            ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"]), \"John\"\n        )\n        self.assertEqual(\n            ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"]), \"23\"\n        )\n\n        i2 = {\"name\": \"John\", \"age\": \"22\"}\n        self.assertEqual(ie.serialize_field({}, \"name\", i2[\"name\"]), \"John\")\n        self.assertEqual(ie.serialize_field({}, \"age\", i2[\"age\"]), \"23\")\n\n\nclass CustomExporterDataclassTest(CustomExporterItemTest):\n    item_class = TestDataClass\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_defer.py": "import random\n\nfrom pytest import mark\nfrom twisted.internet import defer, reactor\nfrom twisted.python.failure import Failure\nfrom twisted.trial import unittest\n\nfrom scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\nfrom scrapy.utils.defer import (\n    aiter_errback,\n    deferred_f_from_coro_f,\n    iter_errback,\n    maybe_deferred_to_future,\n    mustbe_deferred,\n    parallel_async,\n    process_chain,\n    process_parallel,\n)\n\n\nclass MustbeDeferredTest(unittest.TestCase):\n    def test_success_function(self):\n        steps = []\n\n        def _append(v):\n            steps.append(v)\n            return steps\n\n        dfd = mustbe_deferred(_append, 1)\n        dfd.addCallback(self.assertEqual, [1, 2])  # it is [1] with maybeDeferred\n        steps.append(2)  # add another value, that should be caught by assertEqual\n        return dfd\n\n    def test_unfired_deferred(self):\n        steps = []\n\n        def _append(v):\n            steps.append(v)\n            dfd = defer.Deferred()\n            reactor.callLater(0, dfd.callback, steps)\n            return dfd\n\n        dfd = mustbe_deferred(_append, 1)\n        dfd.addCallback(self.assertEqual, [1, 2])  # it is [1] with maybeDeferred\n        steps.append(2)  # add another value, that should be caught by assertEqual\n        return dfd\n\n\ndef cb1(value, arg1, arg2):\n    return f\"(cb1 {value} {arg1} {arg2})\"\n\n\ndef cb2(value, arg1, arg2):\n    return defer.succeed(f\"(cb2 {value} {arg1} {arg2})\")\n\n\ndef cb3(value, arg1, arg2):\n    return f\"(cb3 {value} {arg1} {arg2})\"\n\n\ndef cb_fail(value, arg1, arg2):\n    return Failure(TypeError())\n\n\ndef eb1(failure, arg1, arg2):\n    return f\"(eb1 {failure.value.__class__.__name__} {arg1} {arg2})\"\n\n\nclass DeferUtilsTest(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_process_chain(self):\n        x = yield process_chain([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n        self.assertEqual(x, \"(cb3 (cb2 (cb1 res v1 v2) v1 v2) v1 v2)\")\n\n        gotexc = False\n        try:\n            yield process_chain([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n        except TypeError:\n            gotexc = True\n        self.assertTrue(gotexc)\n\n    @defer.inlineCallbacks\n    def test_process_parallel(self):\n        x = yield process_parallel([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n        self.assertEqual(x, [\"(cb1 res v1 v2)\", \"(cb2 res v1 v2)\", \"(cb3 res v1 v2)\"])\n\n    def test_process_parallel_failure(self):\n        d = process_parallel([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n        self.failUnlessFailure(d, TypeError)\n        return d\n\n\nclass IterErrbackTest(unittest.TestCase):\n    def test_iter_errback_good(self):\n        def itergood():\n            yield from range(10)\n\n        errors = []\n        out = list(iter_errback(itergood(), errors.append))\n        self.assertEqual(out, list(range(10)))\n        self.assertFalse(errors)\n\n    def test_iter_errback_bad(self):\n        def iterbad():\n            for x in range(10):\n                if x == 5:\n                    1 / 0\n                yield x\n\n        errors = []\n        out = list(iter_errback(iterbad(), errors.append))\n        self.assertEqual(out, [0, 1, 2, 3, 4])\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0].value, ZeroDivisionError)\n\n\nclass AiterErrbackTest(unittest.TestCase):\n    @deferred_f_from_coro_f\n    async def test_aiter_errback_good(self):\n        async def itergood():\n            for x in range(10):\n                yield x\n\n        errors = []\n        out = await collect_asyncgen(aiter_errback(itergood(), errors.append))\n        self.assertEqual(out, list(range(10)))\n        self.assertFalse(errors)\n\n    @deferred_f_from_coro_f\n    async def test_iter_errback_bad(self):\n        async def iterbad():\n            for x in range(10):\n                if x == 5:\n                    1 / 0\n                yield x\n\n        errors = []\n        out = await collect_asyncgen(aiter_errback(iterbad(), errors.append))\n        self.assertEqual(out, [0, 1, 2, 3, 4])\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0].value, ZeroDivisionError)\n\n\nclass AsyncDefTestsuiteTest(unittest.TestCase):\n    @deferred_f_from_coro_f\n    async def test_deferred_f_from_coro_f(self):\n        pass\n\n    @deferred_f_from_coro_f\n    async def test_deferred_f_from_coro_f_generator(self):\n        yield\n\n    @mark.xfail(reason=\"Checks that the test is actually executed\", strict=True)\n    @deferred_f_from_coro_f\n    async def test_deferred_f_from_coro_f_xfail(self):\n        raise Exception(\"This is expected to be raised\")\n\n\nclass AsyncCooperatorTest(unittest.TestCase):\n    \"\"\"This tests _AsyncCooperatorAdapter by testing parallel_async which is its only usage.\n\n    parallel_async is called with the results of a callback (so an iterable of items, requests and None,\n    with arbitrary delays between values), and it uses Scraper._process_spidermw_output as the callable\n    (so a callable that returns a Deferred for an item, which will fire after pipelines process it, and\n    None for everything else). The concurrent task count is the CONCURRENT_ITEMS setting.\n\n    We want to test different concurrency values compared to the iterable length.\n    We also want to simulate the real usage, with arbitrary delays between getting the values\n    from the iterable. We also want to simulate sync and async results from the callable.\n    \"\"\"\n\n    CONCURRENT_ITEMS = 50\n\n    @staticmethod\n    def callable(o, results):\n        if random.random() < 0.4:\n            # simulate async processing\n            dfd = defer.Deferred()\n            dfd.addCallback(lambda _: results.append(o))\n            delay = random.random() / 8\n            reactor.callLater(delay, dfd.callback, None)\n            return dfd\n        # simulate trivial sync processing\n        results.append(o)\n\n    @staticmethod\n    def get_async_iterable(length):\n        # simulate a simple callback without delays between results\n        return as_async_generator(range(length))\n\n    @staticmethod\n    async def get_async_iterable_with_delays(length):\n        # simulate a callback with delays between some of the results\n        for i in range(length):\n            if random.random() < 0.1:\n                dfd = defer.Deferred()\n                delay = random.random() / 20\n                reactor.callLater(delay, dfd.callback, None)\n                await maybe_deferred_to_future(dfd)\n            yield i\n\n    @defer.inlineCallbacks\n    def test_simple(self):\n        for length in [20, 50, 100]:\n            results = []\n            ait = self.get_async_iterable(length)\n            dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n            yield dl\n            self.assertEqual(list(range(length)), sorted(results))\n\n    @defer.inlineCallbacks\n    def test_delays(self):\n        for length in [20, 50, 100]:\n            results = []\n            ait = self.get_async_iterable_with_delays(length)\n            dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n            yield dl\n            self.assertEqual(list(range(length)), sorted(results))\n", "tests/test_signals.py": "from pytest import mark\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\n\nfrom scrapy import Request, Spider, signals\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\nfrom tests.mockserver import MockServer\n\n\nclass ItemSpider(Spider):\n    name = \"itemspider\"\n\n    def start_requests(self):\n        for index in range(10):\n            yield Request(\n                self.mockserver.url(f\"/status?n=200&id={index}\"), meta={\"index\": index}\n            )\n\n    def parse(self, response):\n        return {\"index\": response.meta[\"index\"]}\n\n\nclass AsyncSignalTestCase(unittest.TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self.items = []\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    async def _on_item_scraped(self, item):\n        item = await get_from_asyncio_queue(item)\n        self.items.append(item)\n\n    @mark.only_asyncio()\n    @defer.inlineCallbacks\n    def test_simple_pipeline(self):\n        crawler = get_crawler(ItemSpider)\n        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n        yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(len(self.items), 10)\n        for index in range(10):\n            self.assertIn({\"index\": index}, self.items)\n", "tests/test_downloadermiddleware_httpcache.py": "import email.utils\nimport shutil\nimport tempfile\nimport time\nimport unittest\nfrom contextlib import contextmanager\n\nfrom scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass _BaseTest(unittest.TestCase):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n    def setUp(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"example.com\")\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n        self.response = Response(\n            \"http://www.example.com\",\n            headers={\"Content-Type\": \"text/html\"},\n            body=b\"test body\",\n            status=202,\n        )\n        self.crawler.stats.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        shutil.rmtree(self.tmpdir)\n\n    def _get_settings(self, **new_settings):\n        settings = {\n            \"HTTPCACHE_ENABLED\": True,\n            \"HTTPCACHE_DIR\": self.tmpdir,\n            \"HTTPCACHE_EXPIRATION_SECS\": 1,\n            \"HTTPCACHE_IGNORE_HTTP_CODES\": [],\n            \"HTTPCACHE_POLICY\": self.policy_class,\n            \"HTTPCACHE_STORAGE\": self.storage_class,\n        }\n        settings.update(new_settings)\n        return Settings(settings)\n\n    @contextmanager\n    def _storage(self, **new_settings):\n        with self._middleware(**new_settings) as mw:\n            yield mw.storage\n\n    @contextmanager\n    def _policy(self, **new_settings):\n        with self._middleware(**new_settings) as mw:\n            yield mw.policy\n\n    @contextmanager\n    def _middleware(self, **new_settings):\n        settings = self._get_settings(**new_settings)\n        mw = HttpCacheMiddleware(settings, self.crawler.stats)\n        mw.spider_opened(self.spider)\n        try:\n            yield mw\n        finally:\n            mw.spider_closed(self.spider)\n\n    def assertEqualResponse(self, response1, response2):\n        self.assertEqual(response1.url, response2.url)\n        self.assertEqual(response1.status, response2.status)\n        self.assertEqual(response1.headers, response2.headers)\n        self.assertEqual(response1.body, response2.body)\n\n    def assertEqualRequest(self, request1, request2):\n        self.assertEqual(request1.url, request2.url)\n        self.assertEqual(request1.headers, request2.headers)\n        self.assertEqual(request1.body, request2.body)\n\n    def assertEqualRequestButWithCacheValidators(self, request1, request2):\n        self.assertEqual(request1.url, request2.url)\n        assert b\"If-None-Match\" not in request1.headers\n        assert b\"If-Modified-Since\" not in request1.headers\n        assert any(\n            h in request2.headers for h in (b\"If-None-Match\", b\"If-Modified-Since\")\n        )\n        self.assertEqual(request1.body, request2.body)\n\n    def test_dont_cache(self):\n        with self._middleware() as mw:\n            self.request.meta[\"dont_cache\"] = True\n            mw.process_response(self.request, self.response, self.spider)\n            self.assertEqual(\n                mw.storage.retrieve_response(self.spider, self.request), None\n            )\n\n        with self._middleware() as mw:\n            self.request.meta[\"dont_cache\"] = False\n            mw.process_response(self.request, self.response, self.spider)\n            if mw.policy.should_cache_response(self.response, self.request):\n                self.assertIsInstance(\n                    mw.storage.retrieve_response(self.spider, self.request),\n                    self.response.__class__,\n                )\n\n\nclass DefaultStorageTest(_BaseTest):\n    def test_storage(self):\n        with self._storage() as storage:\n            request2 = self.request.copy()\n            assert storage.retrieve_response(self.spider, request2) is None\n\n            storage.store_response(self.spider, self.request, self.response)\n            response2 = storage.retrieve_response(self.spider, request2)\n            assert isinstance(response2, HtmlResponse)  # content-type header\n            self.assertEqualResponse(self.response, response2)\n\n            time.sleep(2)  # wait for cache to expire\n            assert storage.retrieve_response(self.spider, request2) is None\n\n    def test_storage_never_expire(self):\n        with self._storage(HTTPCACHE_EXPIRATION_SECS=0) as storage:\n            assert storage.retrieve_response(self.spider, self.request) is None\n            storage.store_response(self.spider, self.request, self.response)\n            time.sleep(0.5)  # give the chance to expire\n            assert storage.retrieve_response(self.spider, self.request)\n\n    def test_storage_no_content_type_header(self):\n        \"\"\"Test that the response body is used to get the right response class\n        even if there is no Content-Type header\"\"\"\n        with self._storage() as storage:\n            assert storage.retrieve_response(self.spider, self.request) is None\n            response = Response(\n                \"http://www.example.com\",\n                body=b\"<!DOCTYPE html>\\n<title>.</title>\",\n                status=202,\n            )\n            storage.store_response(self.spider, self.request, response)\n            cached_response = storage.retrieve_response(self.spider, self.request)\n            self.assertIsInstance(cached_response, HtmlResponse)\n            self.assertEqualResponse(response, cached_response)\n\n\nclass DbmStorageTest(DefaultStorageTest):\n    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n\n\nclass DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n    dbm_module = \"tests.mocks.dummydbm\"\n\n    def _get_settings(self, **new_settings):\n        new_settings.setdefault(\"HTTPCACHE_DBM_MODULE\", self.dbm_module)\n        return super()._get_settings(**new_settings)\n\n    def test_custom_dbm_module_loaded(self):\n        # make sure our dbm module has been loaded\n        with self._storage() as storage:\n            self.assertEqual(storage.dbmodule.__name__, self.dbm_module)\n\n\nclass FilesystemStorageTest(DefaultStorageTest):\n    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n\n\nclass FilesystemStorageGzipTest(FilesystemStorageTest):\n    def _get_settings(self, **new_settings):\n        new_settings.setdefault(\"HTTPCACHE_GZIP\", True)\n        return super()._get_settings(**new_settings)\n\n\nclass DummyPolicyTest(_BaseTest):\n    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n\n    def test_middleware(self):\n        with self._middleware() as mw:\n            assert mw.process_request(self.request, self.spider) is None\n            mw.process_response(self.request, self.response, self.spider)\n            response = mw.process_request(self.request, self.spider)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert \"cached\" in response.flags\n\n    def test_different_request_response_urls(self):\n        with self._middleware() as mw:\n            req = Request(\"http://host.com/path\")\n            res = Response(\"http://host2.net/test.html\")\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n            cached = mw.process_request(req, self.spider)\n            assert isinstance(cached, Response)\n            self.assertEqualResponse(res, cached)\n            assert \"cached\" in cached.flags\n\n    def test_middleware_ignore_missing(self):\n        with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:\n            self.assertRaises(\n                IgnoreRequest, mw.process_request, self.request, self.spider\n            )\n            mw.process_response(self.request, self.response, self.spider)\n            response = mw.process_request(self.request, self.spider)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert \"cached\" in response.flags\n\n    def test_middleware_ignore_schemes(self):\n        # http responses are cached by default\n        req, res = Request(\"http://test.com/\"), Response(\"http://test.com/\")\n        with self._middleware() as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            cached = mw.process_request(req, self.spider)\n            assert isinstance(cached, Response), type(cached)\n            self.assertEqualResponse(res, cached)\n            assert \"cached\" in cached.flags\n\n        # file response is not cached by default\n        req, res = Request(\"file:///tmp/t.txt\"), Response(\"file:///tmp/t.txt\")\n        with self._middleware() as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            assert mw.storage.retrieve_response(self.spider, req) is None\n            assert mw.process_request(req, self.spider) is None\n\n        # s3 scheme response is cached by default\n        req, res = Request(\"s3://bucket/key\"), Response(\"http://bucket/key\")\n        with self._middleware() as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            cached = mw.process_request(req, self.spider)\n            assert isinstance(cached, Response), type(cached)\n            self.assertEqualResponse(res, cached)\n            assert \"cached\" in cached.flags\n\n        # ignore s3 scheme\n        req, res = Request(\"s3://bucket/key2\"), Response(\"http://bucket/key2\")\n        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=[\"s3\"]) as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            assert mw.storage.retrieve_response(self.spider, req) is None\n            assert mw.process_request(req, self.spider) is None\n\n    def test_middleware_ignore_http_codes(self):\n        # test response is not cached\n        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202]) as mw:\n            assert mw.process_request(self.request, self.spider) is None\n            mw.process_response(self.request, self.response, self.spider)\n\n            assert mw.storage.retrieve_response(self.spider, self.request) is None\n            assert mw.process_request(self.request, self.spider) is None\n\n        # test response is cached\n        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203]) as mw:\n            mw.process_response(self.request, self.response, self.spider)\n            response = mw.process_request(self.request, self.spider)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert \"cached\" in response.flags\n\n\nclass RFC2616PolicyTest(DefaultStorageTest):\n    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n    def _process_requestresponse(self, mw, request, response):\n        result = None\n        try:\n            result = mw.process_request(request, self.spider)\n            if result:\n                assert isinstance(result, (Request, Response))\n                return result\n            result = mw.process_response(request, response, self.spider)\n            assert isinstance(result, Response)\n            return result\n        except Exception:\n            print(\"Request\", request)\n            print(\"Response\", response)\n            print(\"Result\", result)\n            raise\n\n    def test_request_cacheability(self):\n        res0 = Response(\n            self.request.url, status=200, headers={\"Expires\": self.tomorrow}\n        )\n        req0 = Request(\"http://example.com\")\n        req1 = req0.replace(headers={\"Cache-Control\": \"no-store\"})\n        req2 = req0.replace(headers={\"Cache-Control\": \"no-cache\"})\n        with self._middleware() as mw:\n            # response for a request with no-store must not be cached\n            res1 = self._process_requestresponse(mw, req1, res0)\n            self.assertEqualResponse(res1, res0)\n            assert mw.storage.retrieve_response(self.spider, req1) is None\n            # Re-do request without no-store and expect it to be cached\n            res2 = self._process_requestresponse(mw, req0, res0)\n            assert \"cached\" not in res2.flags\n            res3 = mw.process_request(req0, self.spider)\n            assert \"cached\" in res3.flags\n            self.assertEqualResponse(res2, res3)\n            # request with no-cache directive must not return cached response\n            # but it allows new response to be stored\n            res0b = res0.replace(body=b\"foo\")\n            res4 = self._process_requestresponse(mw, req2, res0b)\n            self.assertEqualResponse(res4, res0b)\n            assert \"cached\" not in res4.flags\n            res5 = self._process_requestresponse(mw, req0, None)\n            self.assertEqualResponse(res5, res0b)\n            assert \"cached\" in res5.flags\n\n    def test_response_cacheability(self):\n        responses = [\n            # 304 is not cacheable no matter what servers sends\n            (False, 304, {}),\n            (False, 304, {\"Last-Modified\": self.yesterday}),\n            (False, 304, {\"Expires\": self.tomorrow}),\n            (False, 304, {\"Etag\": \"bar\"}),\n            (False, 304, {\"Cache-Control\": \"max-age=3600\"}),\n            # Always obey no-store cache control\n            (False, 200, {\"Cache-Control\": \"no-store\"}),\n            (False, 200, {\"Cache-Control\": \"no-store, max-age=300\"}),  # invalid\n            (\n                False,\n                200,\n                {\"Cache-Control\": \"no-store\", \"Expires\": self.tomorrow},\n            ),  # invalid\n            # Ignore responses missing expiration and/or validation headers\n            (False, 200, {}),\n            (False, 302, {}),\n            (False, 307, {}),\n            (False, 404, {}),\n            # Cache responses with expiration and/or validation headers\n            (True, 200, {\"Last-Modified\": self.yesterday}),\n            (True, 203, {\"Last-Modified\": self.yesterday}),\n            (True, 300, {\"Last-Modified\": self.yesterday}),\n            (True, 301, {\"Last-Modified\": self.yesterday}),\n            (True, 308, {\"Last-Modified\": self.yesterday}),\n            (True, 401, {\"Last-Modified\": self.yesterday}),\n            (True, 404, {\"Cache-Control\": \"public, max-age=600\"}),\n            (True, 302, {\"Expires\": self.tomorrow}),\n            (True, 200, {\"Etag\": \"foo\"}),\n        ]\n        with self._middleware() as mw:\n            for idx, (shouldcache, status, headers) in enumerate(responses):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                res1 = self._process_requestresponse(mw, req0, res0)\n                res304 = res0.replace(status=304)\n                res2 = self._process_requestresponse(\n                    mw, req0, res304 if shouldcache else res0\n                )\n                self.assertEqualResponse(res1, res0)\n                self.assertEqualResponse(res2, res0)\n                resc = mw.storage.retrieve_response(self.spider, req0)\n                if shouldcache:\n                    self.assertEqualResponse(resc, res1)\n                    assert \"cached\" in res2.flags and res2.status != 304\n                else:\n                    self.assertFalse(resc)\n                    assert \"cached\" not in res2.flags\n\n        # cache unconditionally unless response contains no-store or is a 304\n        with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:\n            for idx, (_, status, headers) in enumerate(responses):\n                shouldcache = (\n                    \"no-store\" not in headers.get(\"Cache-Control\", \"\") and status != 304\n                )\n                req0 = Request(f\"http://example2-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                res1 = self._process_requestresponse(mw, req0, res0)\n                res304 = res0.replace(status=304)\n                res2 = self._process_requestresponse(\n                    mw, req0, res304 if shouldcache else res0\n                )\n                self.assertEqualResponse(res1, res0)\n                self.assertEqualResponse(res2, res0)\n                resc = mw.storage.retrieve_response(self.spider, req0)\n                if shouldcache:\n                    self.assertEqualResponse(resc, res1)\n                    assert \"cached\" in res2.flags and res2.status != 304\n                else:\n                    self.assertFalse(resc)\n                    assert \"cached\" not in res2.flags\n\n    def test_cached_and_fresh(self):\n        sampledata = [\n            (200, {\"Date\": self.yesterday, \"Expires\": self.tomorrow}),\n            (200, {\"Date\": self.yesterday, \"Cache-Control\": \"max-age=86405\"}),\n            (200, {\"Age\": \"299\", \"Cache-Control\": \"max-age=300\"}),\n            # Obey max-age if present over any others\n            (\n                200,\n                {\n                    \"Date\": self.today,\n                    \"Age\": \"86405\",\n                    \"Cache-Control\": \"max-age=\" + str(86400 * 3),\n                    \"Expires\": self.yesterday,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            # obey Expires if max-age is not present\n            (\n                200,\n                {\n                    \"Date\": self.yesterday,\n                    \"Age\": \"86400\",\n                    \"Cache-Control\": \"public\",\n                    \"Expires\": self.tomorrow,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            # Default missing Date header to right now\n            (200, {\"Expires\": self.tomorrow}),\n            # Firefox - Expires if age is greater than 10% of (Date - Last-Modified)\n            (\n                200,\n                {\n                    \"Date\": self.today,\n                    \"Last-Modified\": self.yesterday,\n                    \"Age\": str(86400 / 10 - 1),\n                },\n            ),\n            # Firefox - Set one year maxage to permanent redirects missing expiration info\n            (300, {}),\n            (301, {}),\n            (308, {}),\n        ]\n        with self._middleware() as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                # cache fresh response\n                res1 = self._process_requestresponse(mw, req0, res0)\n                self.assertEqualResponse(res1, res0)\n                assert \"cached\" not in res1.flags\n                # return fresh cached response without network interaction\n                res2 = self._process_requestresponse(mw, req0, None)\n                self.assertEqualResponse(res1, res2)\n                assert \"cached\" in res2.flags\n                # validate cached response if request max-age set as 0\n                req1 = req0.replace(headers={\"Cache-Control\": \"max-age=0\"})\n                res304 = res0.replace(status=304)\n                assert mw.process_request(req1, self.spider) is None\n                res3 = self._process_requestresponse(mw, req1, res304)\n                self.assertEqualResponse(res1, res3)\n                assert \"cached\" in res3.flags\n\n    def test_cached_and_stale(self):\n        sampledata = [\n            (200, {\"Date\": self.today, \"Expires\": self.yesterday}),\n            (\n                200,\n                {\n                    \"Date\": self.today,\n                    \"Expires\": self.yesterday,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            (200, {\"Expires\": self.yesterday}),\n            (200, {\"Expires\": self.yesterday, \"ETag\": \"foo\"}),\n            (200, {\"Expires\": self.yesterday, \"Last-Modified\": self.yesterday}),\n            (200, {\"Expires\": self.tomorrow, \"Age\": \"86405\"}),\n            (200, {\"Cache-Control\": \"max-age=86400\", \"Age\": \"86405\"}),\n            # no-cache forces expiration, also revalidation if validators exists\n            (200, {\"Cache-Control\": \"no-cache\"}),\n            (200, {\"Cache-Control\": \"no-cache\", \"ETag\": \"foo\"}),\n            (200, {\"Cache-Control\": \"no-cache\", \"Last-Modified\": self.yesterday}),\n            (\n                200,\n                {\n                    \"Cache-Control\": \"no-cache,must-revalidate\",\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            (\n                200,\n                {\n                    \"Cache-Control\": \"must-revalidate\",\n                    \"Expires\": self.yesterday,\n                    \"Last-Modified\": self.yesterday,\n                },\n            ),\n            (200, {\"Cache-Control\": \"max-age=86400,must-revalidate\", \"Age\": \"86405\"}),\n        ]\n        with self._middleware() as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0a = Response(req0.url, status=status, headers=headers)\n                # cache expired response\n                res1 = self._process_requestresponse(mw, req0, res0a)\n                self.assertEqualResponse(res1, res0a)\n                assert \"cached\" not in res1.flags\n                # Same request but as cached response is stale a new response must\n                # be returned\n                res0b = res0a.replace(body=b\"bar\")\n                res2 = self._process_requestresponse(mw, req0, res0b)\n                self.assertEqualResponse(res2, res0b)\n                assert \"cached\" not in res2.flags\n                cc = headers.get(\"Cache-Control\", \"\")\n                # Previous response expired too, subsequent request to same\n                # resource must revalidate and succeed on 304 if validators\n                # are present\n                if \"ETag\" in headers or \"Last-Modified\" in headers:\n                    res0c = res0b.replace(status=304)\n                    res3 = self._process_requestresponse(mw, req0, res0c)\n                    self.assertEqualResponse(res3, res0b)\n                    assert \"cached\" in res3.flags\n                    # get cached response on server errors unless must-revalidate\n                    # in cached response\n                    res0d = res0b.replace(status=500)\n                    res4 = self._process_requestresponse(mw, req0, res0d)\n                    if \"must-revalidate\" in cc:\n                        assert \"cached\" not in res4.flags\n                        self.assertEqualResponse(res4, res0d)\n                    else:\n                        assert \"cached\" in res4.flags\n                        self.assertEqualResponse(res4, res0b)\n                # Requests with max-stale can fetch expired cached responses\n                # unless cached response has must-revalidate\n                req1 = req0.replace(headers={\"Cache-Control\": \"max-stale\"})\n                res5 = self._process_requestresponse(mw, req1, res0b)\n                self.assertEqualResponse(res5, res0b)\n                if \"no-cache\" in cc or \"must-revalidate\" in cc:\n                    assert \"cached\" not in res5.flags\n                else:\n                    assert \"cached\" in res5.flags\n\n    def test_process_exception(self):\n        with self._middleware() as mw:\n            res0 = Response(self.request.url, headers={\"Expires\": self.yesterday})\n            req0 = Request(self.request.url)\n            self._process_requestresponse(mw, req0, res0)\n            for e in mw.DOWNLOAD_EXCEPTIONS:\n                # Simulate encountering an error on download attempts\n                assert mw.process_request(req0, self.spider) is None\n                res1 = mw.process_exception(req0, e(\"foo\"), self.spider)\n                # Use cached response as recovery\n                assert \"cached\" in res1.flags\n                self.assertEqualResponse(res0, res1)\n            # Do not use cached response for unhandled exceptions\n            mw.process_request(req0, self.spider)\n            assert mw.process_exception(req0, Exception(\"foo\"), self.spider) is None\n\n    def test_ignore_response_cache_controls(self):\n        sampledata = [\n            (200, {\"Date\": self.yesterday, \"Expires\": self.tomorrow}),\n            (200, {\"Date\": self.yesterday, \"Cache-Control\": \"no-store,max-age=86405\"}),\n            (200, {\"Age\": \"299\", \"Cache-Control\": \"max-age=300,no-cache\"}),\n            (300, {\"Cache-Control\": \"no-cache\"}),\n            (200, {\"Expires\": self.tomorrow, \"Cache-Control\": \"no-store\"}),\n        ]\n        with self._middleware(\n            HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=[\"no-cache\", \"no-store\"]\n        ) as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request(f\"http://example-{idx}.com\")\n                res0 = Response(req0.url, status=status, headers=headers)\n                # cache fresh response\n                res1 = self._process_requestresponse(mw, req0, res0)\n                self.assertEqualResponse(res1, res0)\n                assert \"cached\" not in res1.flags\n                # return fresh cached response without network interaction\n                res2 = self._process_requestresponse(mw, req0, None)\n                self.assertEqualResponse(res1, res2)\n                assert \"cached\" in res2.flags\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_http_request.py": "import json\nimport re\nimport unittest\nimport warnings\nimport xmlrpc.client\nfrom typing import Any, Dict, List\nfrom unittest import mock\nfrom urllib.parse import parse_qs, unquote_to_bytes\n\nfrom scrapy.http import (\n    FormRequest,\n    Headers,\n    HtmlResponse,\n    JsonRequest,\n    Request,\n    XmlRpcRequest,\n)\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_bytes, to_unicode\n\n\nclass RequestTest(unittest.TestCase):\n    request_class = Request\n    default_method = \"GET\"\n    default_headers: Dict[bytes, List[bytes]] = {}\n    default_meta: Dict[str, Any] = {}\n\n    def test_init(self):\n        # Request requires url in the __init__ method\n        self.assertRaises(Exception, self.request_class)\n\n        # url argument must be basestring\n        self.assertRaises(TypeError, self.request_class, 123)\n        r = self.request_class(\"http://www.example.com\")\n\n        r = self.request_class(\"http://www.example.com\")\n        assert isinstance(r.url, str)\n        self.assertEqual(r.url, \"http://www.example.com\")\n        self.assertEqual(r.method, self.default_method)\n\n        assert isinstance(r.headers, Headers)\n        self.assertEqual(r.headers, self.default_headers)\n        self.assertEqual(r.meta, self.default_meta)\n\n        meta = {\"lala\": \"lolo\"}\n        headers = {b\"caca\": b\"coco\"}\n        r = self.request_class(\n            \"http://www.example.com\", meta=meta, headers=headers, body=\"a body\"\n        )\n\n        assert r.meta is not meta\n        self.assertEqual(r.meta, meta)\n        assert r.headers is not headers\n        self.assertEqual(r.headers[b\"caca\"], b\"coco\")\n\n    def test_url_scheme(self):\n        # This test passes by not raising any (ValueError) exception\n        self.request_class(\"http://example.org\")\n        self.request_class(\"https://example.org\")\n        self.request_class(\"s3://example.org\")\n        self.request_class(\"ftp://example.org\")\n        self.request_class(\"about:config\")\n        self.request_class(\"data:,Hello%2C%20World!\")\n\n    def test_url_no_scheme(self):\n        self.assertRaises(ValueError, self.request_class, \"foo\")\n        self.assertRaises(ValueError, self.request_class, \"/foo/\")\n        self.assertRaises(ValueError, self.request_class, \"/foo:bar\")\n\n    def test_headers(self):\n        # Different ways of setting headers attribute\n        url = \"http://www.scrapy.org\"\n        headers = {b\"Accept\": \"gzip\", b\"Custom-Header\": \"nothing to tell you\"}\n        r = self.request_class(url=url, headers=headers)\n        p = self.request_class(url=url, headers=r.headers)\n\n        self.assertEqual(r.headers, p.headers)\n        self.assertFalse(r.headers is headers)\n        self.assertFalse(p.headers is r.headers)\n\n        # headers must not be unicode\n        h = Headers({\"key1\": \"val1\", \"key2\": \"val2\"})\n        h[\"newkey\"] = \"newval\"\n        for k, v in h.items():\n            self.assertIsInstance(k, bytes)\n            for s in v:\n                self.assertIsInstance(s, bytes)\n\n    def test_eq(self):\n        url = \"http://www.scrapy.org\"\n        r1 = self.request_class(url=url)\n        r2 = self.request_class(url=url)\n        self.assertNotEqual(r1, r2)\n\n        set_ = set()\n        set_.add(r1)\n        set_.add(r2)\n        self.assertEqual(len(set_), 2)\n\n    def test_url(self):\n        r = self.request_class(url=\"http://www.scrapy.org/path\")\n        self.assertEqual(r.url, \"http://www.scrapy.org/path\")\n\n    def test_url_quoting(self):\n        r = self.request_class(url=\"http://www.scrapy.org/blank%20space\")\n        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n        r = self.request_class(url=\"http://www.scrapy.org/blank space\")\n        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n\n    def test_url_encoding(self):\n        r = self.request_class(url=\"http://www.scrapy.org/price/\u00a3\")\n        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n\n    def test_url_encoding_other(self):\n        # encoding affects only query part of URI, not path\n        # path part should always be UTF-8 encoded before percent-escaping\n        r = self.request_class(url=\"http://www.scrapy.org/price/\u00a3\", encoding=\"utf-8\")\n        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n\n        r = self.request_class(url=\"http://www.scrapy.org/price/\u00a3\", encoding=\"latin1\")\n        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n\n    def test_url_encoding_query(self):\n        r1 = self.request_class(url=\"http://www.scrapy.org/price/\u00a3?unit=\u00b5\")\n        self.assertEqual(r1.url, \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\")\n\n        # should be same as above\n        r2 = self.request_class(\n            url=\"http://www.scrapy.org/price/\u00a3?unit=\u00b5\", encoding=\"utf-8\"\n        )\n        self.assertEqual(r2.url, \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\")\n\n    def test_url_encoding_query_latin1(self):\n        # encoding is used for encoding query-string before percent-escaping;\n        # path is still UTF-8 encoded before percent-escaping\n        r3 = self.request_class(\n            url=\"http://www.scrapy.org/price/\u00b5?currency=\u00a3\", encoding=\"latin1\"\n        )\n        self.assertEqual(r3.url, \"http://www.scrapy.org/price/%C2%B5?currency=%A3\")\n\n    def test_url_encoding_nonutf8_untouched(self):\n        # percent-escaping sequences that do not match valid UTF-8 sequences\n        # should be kept untouched (just upper-cased perhaps)\n        #\n        # See https://tools.ietf.org/html/rfc3987#section-3.2\n        #\n        # \"Conversions from URIs to IRIs MUST NOT use any character encoding\n        # other than UTF-8 in steps 3 and 4, even if it might be possible to\n        # guess from the context that another character encoding than UTF-8 was\n        # used in the URI.  For example, the URI\n        # \"http://www.example.org/r%E9sum%E9.html\" might with some guessing be\n        # interpreted to contain two e-acute characters encoded as iso-8859-1.\n        # It must not be converted to an IRI containing these e-acute\n        # characters.  Otherwise, in the future the IRI will be mapped to\n        # \"http://www.example.org/r%C3%A9sum%C3%A9.html\", which is a different\n        # URI from \"http://www.example.org/r%E9sum%E9.html\".\n        r1 = self.request_class(url=\"http://www.scrapy.org/price/%a3\")\n        self.assertEqual(r1.url, \"http://www.scrapy.org/price/%a3\")\n\n        r2 = self.request_class(url=\"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n        self.assertEqual(r2.url, \"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n\n        r3 = self.request_class(url=\"http://www.scrapy.org/r\u00e9sum\u00e9/%a3\")\n        self.assertEqual(r3.url, \"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n\n        r4 = self.request_class(url=\"http://www.example.org/r%E9sum%E9.html\")\n        self.assertEqual(r4.url, \"http://www.example.org/r%E9sum%E9.html\")\n\n    def test_body(self):\n        r1 = self.request_class(url=\"http://www.example.com/\")\n        assert r1.body == b\"\"\n\n        r2 = self.request_class(url=\"http://www.example.com/\", body=b\"\")\n        assert isinstance(r2.body, bytes)\n        self.assertEqual(r2.encoding, \"utf-8\")  # default encoding\n\n        r3 = self.request_class(\n            url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding=\"utf-8\"\n        )\n        assert isinstance(r3.body, bytes)\n        self.assertEqual(r3.body, b\"Price: \\xc2\\xa3100\")\n\n        r4 = self.request_class(\n            url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding=\"latin1\"\n        )\n        assert isinstance(r4.body, bytes)\n        self.assertEqual(r4.body, b\"Price: \\xa3100\")\n\n    def test_ajax_url(self):\n        # ascii url\n        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n        self.assertEqual(\n            r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\"\n        )\n        # unicode url\n        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n        self.assertEqual(\n            r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\"\n        )\n\n    def test_copy(self):\n        \"\"\"Test Request copy\"\"\"\n\n        def somecallback():\n            pass\n\n        r1 = self.request_class(\n            \"http://www.example.com\",\n            flags=[\"f1\", \"f2\"],\n            callback=somecallback,\n            errback=somecallback,\n        )\n        r1.meta[\"foo\"] = \"bar\"\n        r1.cb_kwargs[\"key\"] = \"value\"\n        r2 = r1.copy()\n\n        # make sure copy does not propagate callbacks\n        assert r1.callback is somecallback\n        assert r1.errback is somecallback\n        assert r2.callback is r1.callback\n        assert r2.errback is r2.errback\n\n        # make sure flags list is shallow copied\n        assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n        self.assertEqual(r1.flags, r2.flags)\n\n        # make sure cb_kwargs dict is shallow copied\n        assert (\n            r1.cb_kwargs is not r2.cb_kwargs\n        ), \"cb_kwargs must be a shallow copy, not identical\"\n        self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n\n        # make sure meta dict is shallow copied\n        assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n        self.assertEqual(r1.meta, r2.meta)\n\n        # make sure headers attribute is shallow copied\n        assert (\n            r1.headers is not r2.headers\n        ), \"headers must be a shallow copy, not identical\"\n        self.assertEqual(r1.headers, r2.headers)\n        self.assertEqual(r1.encoding, r2.encoding)\n        self.assertEqual(r1.dont_filter, r2.dont_filter)\n\n        # Request.body can be identical since it's an immutable object (str)\n\n    def test_copy_inherited_classes(self):\n        \"\"\"Test Request children copies preserve their class\"\"\"\n\n        class CustomRequest(self.request_class):\n            pass\n\n        r1 = CustomRequest(\"http://www.example.com\")\n        r2 = r1.copy()\n\n        assert isinstance(r2, CustomRequest)\n\n    def test_replace(self):\n        \"\"\"Test Request.replace() method\"\"\"\n        r1 = self.request_class(\"http://www.example.com\", method=\"GET\")\n        hdrs = Headers(r1.headers)\n        hdrs[b\"key\"] = b\"value\"\n        r2 = r1.replace(method=\"POST\", body=\"New body\", headers=hdrs)\n        self.assertEqual(r1.url, r2.url)\n        self.assertEqual((r1.method, r2.method), (\"GET\", \"POST\"))\n        self.assertEqual((r1.body, r2.body), (b\"\", b\"New body\"))\n        self.assertEqual((r1.headers, r2.headers), (self.default_headers, hdrs))\n\n        # Empty attributes (which may fail if not compared properly)\n        r3 = self.request_class(\n            \"http://www.example.com\", meta={\"a\": 1}, dont_filter=True\n        )\n        r4 = r3.replace(\n            url=\"http://www.example.com/2\", body=b\"\", meta={}, dont_filter=False\n        )\n        self.assertEqual(r4.url, \"http://www.example.com/2\")\n        self.assertEqual(r4.body, b\"\")\n        self.assertEqual(r4.meta, {})\n        assert r4.dont_filter is False\n\n    def test_method_always_str(self):\n        r = self.request_class(\"http://www.example.com\", method=\"POST\")\n        assert isinstance(r.method, str)\n\n    def test_immutable_attributes(self):\n        r = self.request_class(\"http://example.com\")\n        self.assertRaises(AttributeError, setattr, r, \"url\", \"http://example2.com\")\n        self.assertRaises(AttributeError, setattr, r, \"body\", \"xxx\")\n\n    def test_callback_and_errback(self):\n        def a_function():\n            pass\n\n        r1 = self.request_class(\"http://example.com\")\n        self.assertIsNone(r1.callback)\n        self.assertIsNone(r1.errback)\n\n        r2 = self.request_class(\"http://example.com\", callback=a_function)\n        self.assertIs(r2.callback, a_function)\n        self.assertIsNone(r2.errback)\n\n        r3 = self.request_class(\"http://example.com\", errback=a_function)\n        self.assertIsNone(r3.callback)\n        self.assertIs(r3.errback, a_function)\n\n        r4 = self.request_class(\n            url=\"http://example.com\",\n            callback=a_function,\n            errback=a_function,\n        )\n        self.assertIs(r4.callback, a_function)\n        self.assertIs(r4.errback, a_function)\n\n        r5 = self.request_class(\n            url=\"http://example.com\",\n            callback=NO_CALLBACK,\n            errback=NO_CALLBACK,\n        )\n        self.assertIs(r5.callback, NO_CALLBACK)\n        self.assertIs(r5.errback, NO_CALLBACK)\n\n    def test_callback_and_errback_type(self):\n        with self.assertRaises(TypeError):\n            self.request_class(\"http://example.com\", callback=\"a_function\")\n        with self.assertRaises(TypeError):\n            self.request_class(\"http://example.com\", errback=\"a_function\")\n        with self.assertRaises(TypeError):\n            self.request_class(\n                url=\"http://example.com\",\n                callback=\"a_function\",\n                errback=\"a_function\",\n            )\n\n    def test_no_callback(self):\n        with self.assertRaises(RuntimeError):\n            NO_CALLBACK()\n\n    def test_from_curl(self):\n        # Note: more curated tests regarding curl conversion are in\n        # `test_utils_curl.py`\n        curl_command = (\n            \"curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique\"\n            \"_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique\"\n            \"_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H\"\n            \" 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q\"\n            \"=0.9,ru;q=0.8,es;q=0.7' -H 'Upgrade-Insecure-Requests: 1' -H 'Use\"\n            \"r-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTM\"\n            \"L, like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.3202.75 S\"\n            \"afari/537.36' -H 'Content-Type: application /x-www-form-urlencode\"\n            \"d' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=\"\n            \"0.9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0\"\n            \"' -H 'Referer: http://httpbin.org/forms/post' -H 'Connection: kee\"\n            \"p-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith\"\n            \"%40example.org&size=small&topping=cheese&topping=onion&delivery=1\"\n            \"2%3A15&comments=' --compressed\"\n        )\n        r = self.request_class.from_curl(curl_command)\n        self.assertEqual(r.method, \"POST\")\n        self.assertEqual(r.url, \"http://httpbin.org/post\")\n        self.assertEqual(\n            r.body,\n            b\"custname=John+Smith&custtel=500&custemail=jsmith%40\"\n            b\"example.org&size=small&topping=cheese&topping=onion\"\n            b\"&delivery=12%3A15&comments=\",\n        )\n        self.assertEqual(\n            r.cookies,\n            {\n                \"_gauges_unique_year\": \"1\",\n                \"_gauges_unique\": \"1\",\n                \"_gauges_unique_month\": \"1\",\n                \"_gauges_unique_hour\": \"1\",\n                \"_gauges_unique_day\": \"1\",\n            },\n        )\n        self.assertEqual(\n            r.headers,\n            {\n                b\"Origin\": [b\"http://httpbin.org\"],\n                b\"Accept-Encoding\": [b\"gzip, deflate\"],\n                b\"Accept-Language\": [b\"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"],\n                b\"Upgrade-Insecure-Requests\": [b\"1\"],\n                b\"User-Agent\": [\n                    b\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.\"\n                    b\"36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202\"\n                    b\".75 Chrome/62.0.3202.75 Safari/537.36\"\n                ],\n                b\"Content-Type\": [b\"application /x-www-form-urlencoded\"],\n                b\"Accept\": [\n                    b\"text/html,application/xhtml+xml,application/xml;q=0.\"\n                    b\"9,image/webp,image/apng,*/*;q=0.8\"\n                ],\n                b\"Cache-Control\": [b\"max-age=0\"],\n                b\"Referer\": [b\"http://httpbin.org/forms/post\"],\n                b\"Connection\": [b\"keep-alive\"],\n            },\n        )\n\n    def test_from_curl_with_kwargs(self):\n        r = self.request_class.from_curl(\n            'curl -X PATCH \"http://example.org\"', method=\"POST\", meta={\"key\": \"value\"}\n        )\n        self.assertEqual(r.method, \"POST\")\n        self.assertEqual(r.meta, {\"key\": \"value\"})\n\n    def test_from_curl_ignore_unknown_options(self):\n        # By default: it works and ignores the unknown options: --foo and -z\n        with warnings.catch_warnings():  # avoid warning when executing tests\n            warnings.simplefilter(\"ignore\")\n            r = self.request_class.from_curl(\n                'curl -X DELETE \"http://example.org\" --foo -z',\n            )\n            self.assertEqual(r.method, \"DELETE\")\n\n        # If `ignore_unknown_options` is set to `False` it raises an error with\n        # the unknown options: --foo and -z\n        self.assertRaises(\n            ValueError,\n            lambda: self.request_class.from_curl(\n                'curl -X PATCH \"http://example.org\" --foo -z',\n                ignore_unknown_options=False,\n            ),\n        )\n\n\nclass FormRequestTest(RequestTest):\n    request_class = FormRequest\n\n    def assertQueryEqual(self, first, second, msg=None):\n        first = to_unicode(first).split(\"&\")\n        second = to_unicode(second).split(\"&\")\n        return self.assertEqual(sorted(first), sorted(second), msg)\n\n    def test_empty_formdata(self):\n        r1 = self.request_class(\"http://www.example.com\", formdata={})\n        self.assertEqual(r1.body, b\"\")\n\n    def test_formdata_overrides_querystring(self):\n        data = ((\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"2\"))\n        url = self.request_class(\n            \"http://www.example.com/?a=0&b=1&c=3#fragment\", method=\"GET\", formdata=data\n        ).url.split(\"#\")[0]\n        fs = _qs(self.request_class(url, method=\"GET\", formdata=data))\n        self.assertEqual(set(fs[b\"a\"]), {b\"one\", b\"two\"})\n        self.assertEqual(fs[b\"b\"], [b\"2\"])\n        self.assertIsNone(fs.get(b\"c\"))\n\n        data = {\"a\": \"1\", \"b\": \"2\"}\n        fs = _qs(\n            self.request_class(\"http://www.example.com/\", method=\"GET\", formdata=data)\n        )\n        self.assertEqual(fs[b\"a\"], [b\"1\"])\n        self.assertEqual(fs[b\"b\"], [b\"2\"])\n\n    def test_default_encoding_bytes(self):\n        # using default encoding (utf-8)\n        data = {b\"one\": b\"two\", b\"price\": b\"\\xc2\\xa3 100\"}\n        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n        self.assertEqual(r2.method, \"POST\")\n        self.assertEqual(r2.encoding, \"utf-8\")\n        self.assertQueryEqual(r2.body, b\"price=%C2%A3+100&one=two\")\n        self.assertEqual(\n            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n        )\n\n    def test_default_encoding_textual_data(self):\n        # using default encoding (utf-8)\n        data = {\"\u00b5 one\": \"two\", \"price\": \"\u00a3 100\"}\n        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n        self.assertEqual(r2.method, \"POST\")\n        self.assertEqual(r2.encoding, \"utf-8\")\n        self.assertQueryEqual(r2.body, b\"price=%C2%A3+100&%C2%B5+one=two\")\n        self.assertEqual(\n            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n        )\n\n    def test_default_encoding_mixed_data(self):\n        # using default encoding (utf-8)\n        data = {\"\\u00b5one\": b\"two\", b\"price\\xc2\\xa3\": \"\\u00a3 100\"}\n        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n        self.assertEqual(r2.method, \"POST\")\n        self.assertEqual(r2.encoding, \"utf-8\")\n        self.assertQueryEqual(r2.body, b\"%C2%B5one=two&price%C2%A3=%C2%A3+100\")\n        self.assertEqual(\n            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n        )\n\n    def test_custom_encoding_bytes(self):\n        data = {b\"\\xb5 one\": b\"two\", b\"price\": b\"\\xa3 100\"}\n        r2 = self.request_class(\n            \"http://www.example.com\", formdata=data, encoding=\"latin1\"\n        )\n        self.assertEqual(r2.method, \"POST\")\n        self.assertEqual(r2.encoding, \"latin1\")\n        self.assertQueryEqual(r2.body, b\"price=%A3+100&%B5+one=two\")\n        self.assertEqual(\n            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n        )\n\n    def test_custom_encoding_textual_data(self):\n        data = {\"price\": \"\u00a3 100\"}\n        r3 = self.request_class(\n            \"http://www.example.com\", formdata=data, encoding=\"latin1\"\n        )\n        self.assertEqual(r3.encoding, \"latin1\")\n        self.assertEqual(r3.body, b\"price=%A3+100\")\n\n    def test_multi_key_values(self):\n        # using multiples values for a single key\n        data = {\"price\": \"\\xa3 100\", \"colours\": [\"red\", \"blue\", \"green\"]}\n        r3 = self.request_class(\"http://www.example.com\", formdata=data)\n        self.assertQueryEqual(\n            r3.body, b\"colours=red&colours=blue&colours=green&price=%C2%A3+100\"\n        )\n\n    def test_from_response_post(self):\n        response = _buildresponse(\n            b\"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req)\n        self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n        self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\n        self.assertEqual(fs[b\"test2\"], [b\"xxx\"])\n        self.assertEqual(fs[b\"six\"], [b\"seven\"])\n\n    def test_from_response_post_nonascii_bytes_utf8(self):\n        response = _buildresponse(\n            b\"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val1\">\n            <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx \\xc2\\xb5\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req, to_unicode=True)\n        self.assertEqual(set(fs[\"test \u00a3\"]), {\"val1\", \"val2\"})\n        self.assertEqual(set(fs[\"one\"]), {\"two\", \"three\"})\n        self.assertEqual(fs[\"test2\"], [\"xxx \u00b5\"])\n        self.assertEqual(fs[\"six\"], [\"seven\"])\n\n    def test_from_response_post_nonascii_bytes_latin1(self):\n        response = _buildresponse(\n            b\"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test \\xa3\" value=\"val1\">\n            <input type=\"hidden\" name=\"test \\xa3\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx \\xb5\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n            encoding=\"latin1\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req, to_unicode=True, encoding=\"latin1\")\n        self.assertEqual(set(fs[\"test \u00a3\"]), {\"val1\", \"val2\"})\n        self.assertEqual(set(fs[\"one\"]), {\"two\", \"three\"})\n        self.assertEqual(fs[\"test2\"], [\"xxx \u00b5\"])\n        self.assertEqual(fs[\"six\"], [\"seven\"])\n\n    def test_from_response_post_nonascii_unicode(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test \u00a3\" value=\"val1\">\n            <input type=\"hidden\" name=\"test \u00a3\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx \u00b5\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req, to_unicode=True)\n        self.assertEqual(set(fs[\"test \u00a3\"]), {\"val1\", \"val2\"})\n        self.assertEqual(set(fs[\"one\"]), {\"two\", \"three\"})\n        self.assertEqual(fs[\"test2\"], [\"xxx \u00b5\"])\n        self.assertEqual(fs[\"six\"], [\"seven\"])\n\n    def test_from_response_duplicate_form_key(self):\n        response = _buildresponse(\"<form></form>\", url=\"http://www.example.com\")\n        req = self.request_class.from_response(\n            response=response,\n            method=\"GET\",\n            formdata=((\"foo\", \"bar\"), (\"foo\", \"baz\")),\n        )\n        self.assertEqual(urlparse_cached(req).hostname, \"www.example.com\")\n        self.assertEqual(urlparse_cached(req).query, \"foo=bar&foo=baz\")\n\n    def test_from_response_override_duplicate_form_key(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formdata=((\"two\", \"2\"), (\"two\", \"4\"))\n        )\n        fs = _qs(req)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n        self.assertEqual(fs[b\"two\"], [b\"2\", b\"4\"])\n\n    def test_from_response_extra_headers(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response=response,\n            formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"},\n            headers={\"Accept-Encoding\": \"gzip,deflate\"},\n        )\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.headers[\"Accept-Encoding\"], b\"gzip,deflate\")\n\n    def test_from_response_get(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        r1 = self.request_class.from_response(\n            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n        )\n        self.assertEqual(r1.method, \"GET\")\n        self.assertEqual(urlparse_cached(r1).hostname, \"www.example.com\")\n        self.assertEqual(urlparse_cached(r1).path, \"/this/get.php\")\n        fs = _qs(r1)\n        self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n        self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\n        self.assertEqual(fs[b\"test2\"], [b\"xxx\"])\n        self.assertEqual(fs[b\"six\"], [b\"seven\"])\n\n    def test_from_response_override_params(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"two\": \"2\"})\n        fs = _qs(req)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n        self.assertEqual(fs[b\"two\"], [b\"2\"])\n\n    def test_from_response_drop_params(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"two\": None})\n        fs = _qs(req)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n        self.assertNotIn(b\"two\", fs)\n\n    def test_from_response_override_method(self):\n        response = _buildresponse(\n            \"\"\"<html><body>\n            <form action=\"/app\"></form>\n            </body></html>\"\"\"\n        )\n        request = FormRequest.from_response(response)\n        self.assertEqual(request.method, \"GET\")\n        request = FormRequest.from_response(response, method=\"POST\")\n        self.assertEqual(request.method, \"POST\")\n\n    def test_from_response_override_url(self):\n        response = _buildresponse(\n            \"\"\"<html><body>\n            <form action=\"/app\"></form>\n            </body></html>\"\"\"\n        )\n        request = FormRequest.from_response(response)\n        self.assertEqual(request.url, \"http://example.com/app\")\n        request = FormRequest.from_response(response, url=\"http://foo.bar/absolute\")\n        self.assertEqual(request.url, \"http://foo.bar/absolute\")\n        request = FormRequest.from_response(response, url=\"/relative\")\n        self.assertEqual(request.url, \"http://example.com/relative\")\n\n    def test_from_response_case_insensitive(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"SuBmIt\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"iMaGe\" name=\"i1\" src=\"http://my.image.org/1.jpg\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response)\n        fs = _qs(req)\n        self.assertEqual(fs[b\"clickable1\"], [b\"clicked1\"])\n        self.assertFalse(b\"i1\" in fs, fs)  # xpath in _get_inputs()\n        self.assertFalse(b\"clickable2\" in fs, fs)  # xpath in _get_clickable()\n\n    def test_from_response_submit_first_clickable(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"two\": \"2\"})\n        fs = _qs(req)\n        self.assertEqual(fs[b\"clickable1\"], [b\"clicked1\"])\n        self.assertFalse(b\"clickable2\" in fs, fs)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n        self.assertEqual(fs[b\"two\"], [b\"2\"])\n\n    def test_from_response_submit_not_first_clickable(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"two\": \"2\"}, clickdata={\"name\": \"clickable2\"}\n        )\n        fs = _qs(req)\n        self.assertEqual(fs[b\"clickable2\"], [b\"clicked2\"])\n        self.assertFalse(b\"clickable1\" in fs, fs)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n        self.assertEqual(fs[b\"two\"], [b\"2\"])\n\n    def test_from_response_dont_submit_image_as_input(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"hidden\" name=\"i1\" value=\"i1v\">\n            <input type=\"image\" name=\"i2\" src=\"http://my.image.org/1.jpg\">\n            <input type=\"submit\" name=\"i3\" value=\"i3v\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, dont_click=True)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"i1v\"]})\n\n    def test_from_response_dont_submit_reset_as_input(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"hidden\" name=\"i1\" value=\"i1v\">\n            <input type=\"text\" name=\"i2\" value=\"i2v\">\n            <input type=\"reset\" name=\"resetme\">\n            <input type=\"submit\" name=\"i3\" value=\"i3v\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response, dont_click=True)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"i1v\"], b\"i2\": [b\"i2v\"]})\n\n    def test_from_response_clickdata_does_not_ignore_image(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"text\" name=\"i1\" value=\"i1v\">\n            <input id=\"image\" name=\"i2\" type=\"image\" value=\"i2v\" alt=\"Login\" src=\"http://my.image.org/1.jpg\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(response)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"i1v\"], b\"i2\": [b\"i2v\"]})\n\n    def test_from_response_multiple_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked1\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked2\">\n            <input type=\"hidden\" name=\"one\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"two\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, clickdata={\"name\": \"clickable\", \"value\": \"clicked2\"}\n        )\n        fs = _qs(req)\n        self.assertEqual(fs[b\"clickable\"], [b\"clicked2\"])\n        self.assertEqual(fs[b\"one\"], [b\"clicked1\"])\n        self.assertEqual(fs[b\"two\"], [b\"clicked2\"])\n\n    def test_from_response_unicode_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"price in \\u00a3\" value=\"\\u00a3 1000\">\n            <input type=\"submit\" name=\"price in \\u20ac\" value=\"\\u20ac 2000\">\n            <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n            <input type=\"hidden\" name=\"eurosign\" value=\"\\u20ac\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, clickdata={\"name\": \"price in \\u00a3\"}\n        )\n        fs = _qs(req, to_unicode=True)\n        self.assertTrue(fs[\"price in \\u00a3\"])\n\n    def test_from_response_unicode_clickdata_latin1(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"price in \\u00a3\" value=\"\\u00a3 1000\">\n            <input type=\"submit\" name=\"price in \\u00a5\" value=\"\\u00a5 2000\">\n            <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n            <input type=\"hidden\" name=\"yensign\" value=\"\\u00a5\">\n            </form>\"\"\",\n            encoding=\"latin1\",\n        )\n        req = self.request_class.from_response(\n            response, clickdata={\"name\": \"price in \\u00a5\"}\n        )\n        fs = _qs(req, to_unicode=True, encoding=\"latin1\")\n        self.assertTrue(fs[\"price in \\u00a5\"])\n\n    def test_from_response_multiple_forms_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form name=\"form1\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"field1\" value=\"value1\">\n            </form>\n            <form name=\"form2\">\n            <input type=\"submit\" name=\"clickable\" value=\"clicked2\">\n            <input type=\"hidden\" name=\"field2\" value=\"value2\">\n            </form>\n            \"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formname=\"form2\", clickdata={\"name\": \"clickable\"}\n        )\n        fs = _qs(req)\n        self.assertEqual(fs[b\"clickable\"], [b\"clicked2\"])\n        self.assertEqual(fs[b\"field2\"], [b\"value2\"])\n        self.assertFalse(b\"field1\" in fs, fs)\n\n    def test_from_response_override_clickable(self):\n        response = _buildresponse(\n            \"\"\"<form><input type=\"submit\" name=\"clickme\" value=\"one\"> </form>\"\"\"\n        )\n        req = self.request_class.from_response(\n            response, formdata={\"clickme\": \"two\"}, clickdata={\"name\": \"clickme\"}\n        )\n        fs = _qs(req)\n        self.assertEqual(fs[b\"clickme\"], [b\"two\"])\n\n    def test_from_response_dont_click(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, dont_click=True)\n        fs = _qs(r1)\n        self.assertFalse(b\"clickable1\" in fs, fs)\n        self.assertFalse(b\"clickable2\" in fs, fs)\n\n    def test_from_response_ambiguous_clickdata(self):\n        response = _buildresponse(\n            \"\"\"\n            <form action=\"get.php\" method=\"GET\">\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"3\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\"\"\"\n        )\n        self.assertRaises(\n            ValueError,\n            self.request_class.from_response,\n            response,\n            clickdata={\"type\": \"submit\"},\n        )\n\n    def test_from_response_non_matching_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n            </form>\"\"\"\n        )\n        self.assertRaises(\n            ValueError,\n            self.request_class.from_response,\n            response,\n            clickdata={\"nonexistent\": \"notme\"},\n        )\n\n    def test_from_response_nr_index_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n            </form>\n            \"\"\"\n        )\n        req = self.request_class.from_response(response, clickdata={\"nr\": 1})\n        fs = _qs(req)\n        self.assertIn(b\"clickable2\", fs)\n        self.assertNotIn(b\"clickable1\", fs)\n\n    def test_from_response_invalid_nr_index_clickdata(self):\n        response = _buildresponse(\n            \"\"\"<form>\n            <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n            </form>\n            \"\"\"\n        )\n        self.assertRaises(\n            ValueError, self.request_class.from_response, response, clickdata={\"nr\": 1}\n        )\n\n    def test_from_response_errors_noform(self):\n        response = _buildresponse(\"\"\"<html></html>\"\"\")\n        self.assertRaises(ValueError, self.request_class.from_response, response)\n\n    def test_from_response_invalid_html5(self):\n        response = _buildresponse(\n            \"\"\"<!DOCTYPE html><body></html><form>\"\"\"\n            \"\"\"<input type=\"text\" name=\"foo\" value=\"xxx\">\"\"\"\n            \"\"\"</form></body></html>\"\"\"\n        )\n        req = self.request_class.from_response(response, formdata={\"bar\": \"buz\"})\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"foo\": [b\"xxx\"], b\"bar\": [b\"buz\"]})\n\n    def test_from_response_errors_formnumber(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"get.php\" method=\"GET\">\n            <input type=\"hidden\" name=\"test\" value=\"val1\">\n            <input type=\"hidden\" name=\"test\" value=\"val2\">\n            <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n            </form>\"\"\"\n        )\n        self.assertRaises(\n            IndexError, self.request_class.from_response, response, formnumber=1\n        )\n\n    def test_from_response_noformname(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formdata={\"two\": \"3\"})\n        self.assertEqual(r1.method, \"POST\")\n        self.assertEqual(\n            r1.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        fs = _qs(r1)\n        self.assertEqual(fs, {b\"one\": [b\"1\"], b\"two\": [b\"3\"]})\n\n    def test_from_response_formname_exists(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formname=\"form2\")\n        self.assertEqual(r1.method, \"POST\")\n        fs = _qs(r1)\n        self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n\n    def test_from_response_formname_nonexistent(self):\n        response = _buildresponse(\n            \"\"\"<form name=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formname=\"form3\")\n        self.assertEqual(r1.method, \"POST\")\n        fs = _qs(r1)\n        self.assertEqual(fs, {b\"one\": [b\"1\"]})\n\n    def test_from_response_formname_errors_formnumber(self):\n        response = _buildresponse(\n            \"\"\"<form name=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        self.assertRaises(\n            IndexError,\n            self.request_class.from_response,\n            response,\n            formname=\"form3\",\n            formnumber=2,\n        )\n\n    def test_from_response_formid_exists(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form id=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formid=\"form2\")\n        self.assertEqual(r1.method, \"POST\")\n        fs = _qs(r1)\n        self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n\n    def test_from_response_formname_nonexistent_fallback_formid(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(\n            response, formname=\"form3\", formid=\"form2\"\n        )\n        self.assertEqual(r1.method, \"POST\")\n        fs = _qs(r1)\n        self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n\n    def test_from_response_formid_nonexistent(self):\n        response = _buildresponse(\n            \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form id=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(response, formid=\"form3\")\n        self.assertEqual(r1.method, \"POST\")\n        fs = _qs(r1)\n        self.assertEqual(fs, {b\"one\": [b\"1\"]})\n\n    def test_from_response_formid_errors_formnumber(self):\n        response = _buildresponse(\n            \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\n            <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\"\"\"\n        )\n        self.assertRaises(\n            IndexError,\n            self.request_class.from_response,\n            response,\n            formid=\"form3\",\n            formnumber=2,\n        )\n\n    def test_from_response_select(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <select name=\"i1\">\n                <option value=\"i1v1\">option 1</option>\n                <option value=\"i1v2\" selected>option 2</option>\n            </select>\n            <select name=\"i2\">\n                <option value=\"i2v1\">option 1</option>\n                <option value=\"i2v2\">option 2</option>\n            </select>\n            <select>\n                <option value=\"i3v1\">option 1</option>\n                <option value=\"i3v2\">option 2</option>\n            </select>\n            <select name=\"i4\" multiple>\n                <option value=\"i4v1\">option 1</option>\n                <option value=\"i4v2\" selected>option 2</option>\n                <option value=\"i4v3\" selected>option 3</option>\n            </select>\n            <select name=\"i5\" multiple>\n                <option value=\"i5v1\">option 1</option>\n                <option value=\"i5v2\">option 2</option>\n            </select>\n            <select name=\"i6\"></select>\n            <select name=\"i7\"/>\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req, to_unicode=True)\n        self.assertEqual(fs, {\"i1\": [\"i1v2\"], \"i2\": [\"i2v1\"], \"i4\": [\"i4v2\", \"i4v3\"]})\n\n    def test_from_response_radio(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"radio\" name=\"i1\" value=\"i1v1\">\n            <input type=\"radio\" name=\"i1\" value=\"iv2\" checked>\n            <input type=\"radio\" name=\"i2\" checked>\n            <input type=\"radio\" name=\"i2\">\n            <input type=\"radio\" name=\"i3\" value=\"i3v1\">\n            <input type=\"radio\" name=\"i3\">\n            <input type=\"radio\" value=\"i4v1\">\n            <input type=\"radio\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"iv2\"], b\"i2\": [b\"on\"]})\n\n    def test_from_response_checkbox(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"checkbox\" name=\"i1\" value=\"i1v1\">\n            <input type=\"checkbox\" name=\"i1\" value=\"iv2\" checked>\n            <input type=\"checkbox\" name=\"i2\" checked>\n            <input type=\"checkbox\" name=\"i2\">\n            <input type=\"checkbox\" name=\"i3\" value=\"i3v1\">\n            <input type=\"checkbox\" name=\"i3\">\n            <input type=\"checkbox\" value=\"i4v1\">\n            <input type=\"checkbox\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"iv2\"], b\"i2\": [b\"on\"]})\n\n    def test_from_response_input_text(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"text\" name=\"i1\" value=\"i1v1\">\n            <input type=\"text\" name=\"i2\">\n            <input type=\"text\" value=\"i3v1\">\n            <input type=\"text\">\n            <input name=\"i4\" value=\"i4v1\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"i1v1\"], b\"i2\": [b\"\"], b\"i4\": [b\"i4v1\"]})\n\n    def test_from_response_input_hidden(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <input type=\"hidden\" name=\"i1\" value=\"i1v1\">\n            <input type=\"hidden\" name=\"i2\">\n            <input type=\"hidden\" value=\"i3v1\">\n            <input type=\"hidden\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"i1v1\"], b\"i2\": [b\"\"]})\n\n    def test_from_response_input_textarea(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <textarea name=\"i1\">i1v</textarea>\n            <textarea name=\"i2\"></textarea>\n            <textarea name=\"i3\"/>\n            <textarea>i4v</textarea>\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        self.assertEqual(fs, {b\"i1\": [b\"i1v\"], b\"i2\": [b\"\"], b\"i3\": [b\"\"]})\n\n    def test_from_response_descendants(self):\n        res = _buildresponse(\n            \"\"\"<form>\n            <div>\n              <fieldset>\n                <input type=\"text\" name=\"i1\">\n                <select name=\"i2\">\n                    <option value=\"v1\" selected>\n                </select>\n              </fieldset>\n              <input type=\"radio\" name=\"i3\" value=\"i3v2\" checked>\n              <input type=\"checkbox\" name=\"i4\" value=\"i4v2\" checked>\n              <textarea name=\"i5\"></textarea>\n              <input type=\"hidden\" name=\"h1\" value=\"h1v\">\n              </div>\n            <input type=\"hidden\" name=\"h2\" value=\"h2v\">\n            </form>\"\"\"\n        )\n        req = self.request_class.from_response(res)\n        fs = _qs(req)\n        self.assertEqual(set(fs), {b\"h2\", b\"i2\", b\"i1\", b\"i3\", b\"h1\", b\"i5\", b\"i4\"})\n\n    def test_from_response_xpath(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form action=\"post2.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(\n            response, formxpath=\"//form[@action='post.php']\"\n        )\n        fs = _qs(r1)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n\n        r1 = self.request_class.from_response(\n            response, formxpath=\"//form/input[@name='four']\"\n        )\n        fs = _qs(r1)\n        self.assertEqual(fs[b\"three\"], [b\"3\"])\n\n        self.assertRaises(\n            ValueError,\n            self.request_class.from_response,\n            response,\n            formxpath=\"//form/input[@name='abc']\",\n        )\n\n    def test_from_response_unicode_xpath(self):\n        response = _buildresponse(b'<form name=\"\\xd1\\x8a\"></form>')\n        r = self.request_class.from_response(\n            response, formxpath=\"//form[@name='\\u044a']\"\n        )\n        fs = _qs(r)\n        self.assertEqual(fs, {})\n\n        xpath = \"//form[@name='\\u03b1']\"\n        self.assertRaisesRegex(\n            ValueError,\n            re.escape(xpath),\n            self.request_class.from_response,\n            response,\n            formxpath=xpath,\n        )\n\n    def test_from_response_button_submit(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <button type=\"submit\" name=\"button1\" value=\"submit1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req)\n        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n        self.assertEqual(fs[b\"button1\"], [b\"submit1\"])\n\n    def test_from_response_button_notype(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <button name=\"button1\" value=\"submit1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req)\n        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n        self.assertEqual(fs[b\"button1\"], [b\"submit1\"])\n\n    def test_from_response_submit_novalue(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <input type=\"submit\" name=\"button1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req)\n        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n        self.assertEqual(fs[b\"button1\"], [b\"\"])\n\n    def test_from_response_button_novalue(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"test1\" value=\"val1\">\n            <input type=\"hidden\" name=\"test2\" value=\"val2\">\n            <button type=\"submit\" name=\"button1\">Submit</button>\n            </form>\"\"\",\n            url=\"http://www.example.com/this/list.html\",\n        )\n        req = self.request_class.from_response(response)\n        self.assertEqual(req.method, \"POST\")\n        self.assertEqual(\n            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n        )\n        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n        fs = _qs(req)\n        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n        self.assertEqual(fs[b\"button1\"], [b\"\"])\n\n    def test_html_base_form_action(self):\n        response = _buildresponse(\n            \"\"\"\n            <html>\n                <head>\n                    <base href=\" http://b.com/\">\n                </head>\n                <body>\n                    <form action=\"test_form\">\n                    </form>\n                </body>\n            </html>\n            \"\"\",\n            url=\"http://a.com/\",\n        )\n        req = self.request_class.from_response(response)\n        self.assertEqual(req.url, \"http://b.com/test_form\")\n\n    def test_spaces_in_action(self):\n        resp = _buildresponse('<body><form action=\" path\\n\"></form></body>')\n        req = self.request_class.from_response(resp)\n        self.assertEqual(req.url, \"http://example.com/path\")\n\n    def test_from_response_css(self):\n        response = _buildresponse(\n            \"\"\"<form action=\"post.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            <input type=\"hidden\" name=\"two\" value=\"2\">\n            </form>\n            <form action=\"post2.php\" method=\"POST\">\n            <input type=\"hidden\" name=\"three\" value=\"3\">\n            <input type=\"hidden\" name=\"four\" value=\"4\">\n            </form>\"\"\"\n        )\n        r1 = self.request_class.from_response(\n            response, formcss=\"form[action='post.php']\"\n        )\n        fs = _qs(r1)\n        self.assertEqual(fs[b\"one\"], [b\"1\"])\n\n        r1 = self.request_class.from_response(response, formcss=\"input[name='four']\")\n        fs = _qs(r1)\n        self.assertEqual(fs[b\"three\"], [b\"3\"])\n\n        self.assertRaises(\n            ValueError,\n            self.request_class.from_response,\n            response,\n            formcss=\"input[name='abc']\",\n        )\n\n    def test_from_response_valid_form_methods(self):\n        form_methods = [\n            [method, method] for method in self.request_class.valid_form_methods\n        ]\n        form_methods.append([\"UNKNOWN\", \"GET\"])\n\n        for method, expected in form_methods:\n            response = _buildresponse(\n                f'<form action=\"post.php\" method=\"{method}\">'\n                '<input type=\"hidden\" name=\"one\" value=\"1\">'\n                \"</form>\"\n            )\n            r = self.request_class.from_response(response)\n            self.assertEqual(r.method, expected)\n\n    def test_form_response_with_invalid_formdata_type_error(self):\n        \"\"\"Test that a ValueError is raised for non-iterable and non-dict formdata input\"\"\"\n        response = _buildresponse(\n            \"\"\"<html><body>\n            <form action=\"/submit\" method=\"post\">\n                <input type=\"text\" name=\"test\" value=\"value\">\n            </form>\n            </body></html>\"\"\"\n        )\n        with self.assertRaises(ValueError) as context:\n            FormRequest.from_response(response, formdata=123)\n\n        self.assertIn(\n            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n        )\n\n    def test_form_response_with_custom_invalid_formdata_value_error(self):\n        \"\"\"Test that a ValueError is raised for fault-inducing iterable formdata input\"\"\"\n        response = _buildresponse(\n            \"\"\"<html><body>\n                <form action=\"/submit\" method=\"post\">\n                    <input type=\"text\" name=\"test\" value=\"value\">\n                </form>\n            </body></html>\"\"\"\n        )\n\n        with self.assertRaises(ValueError) as context:\n            FormRequest.from_response(response, formdata=(\"a\",))\n\n        self.assertIn(\n            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n        )\n\n    def test_get_form_with_xpath_no_form_parent(self):\n        \"\"\"Test that _get_from raised a ValueError when an XPath selects an element\n        not nested within a <form> and no <form> parent is found\"\"\"\n        response = _buildresponse(\n            \"\"\"<html><body>\n                <div id=\"outside-form\">\n                    <p>This paragraph is not inside a form.</p>\n                </div>\n                <form action=\"/submit\" method=\"post\">\n                    <input type=\"text\" name=\"inside-form\" value=\"\">\n                </form>\n            </body></html>\"\"\"\n        )\n\n        with self.assertRaises(ValueError) as context:\n            FormRequest.from_response(response, formxpath='//div[@id=\"outside-form\"]/p')\n\n        self.assertIn(\"No <form> element found with\", str(context.exception))\n\n\ndef _buildresponse(body, **kwargs):\n    kwargs.setdefault(\"body\", body)\n    kwargs.setdefault(\"url\", \"http://example.com\")\n    kwargs.setdefault(\"encoding\", \"utf-8\")\n    return HtmlResponse(**kwargs)\n\n\ndef _qs(req, encoding=\"utf-8\", to_unicode=False):\n    if req.method == \"POST\":\n        qs = req.body\n    else:\n        qs = req.url.partition(\"?\")[2]\n    uqs = unquote_to_bytes(qs)\n    if to_unicode:\n        uqs = uqs.decode(encoding)\n    return parse_qs(uqs, True)\n\n\nclass XmlRpcRequestTest(RequestTest):\n    request_class = XmlRpcRequest\n    default_method = \"POST\"\n    default_headers = {b\"Content-Type\": [b\"text/xml\"]}\n\n    def _test_request(self, **kwargs):\n        r = self.request_class(\"http://scrapytest.org/rpc2\", **kwargs)\n        self.assertEqual(r.headers[b\"Content-Type\"], b\"text/xml\")\n        self.assertEqual(\n            r.body,\n            to_bytes(\n                xmlrpc.client.dumps(**kwargs), encoding=kwargs.get(\"encoding\", \"utf-8\")\n            ),\n        )\n        self.assertEqual(r.method, \"POST\")\n        self.assertEqual(r.encoding, kwargs.get(\"encoding\", \"utf-8\"))\n        self.assertTrue(r.dont_filter, True)\n\n    def test_xmlrpc_dumps(self):\n        self._test_request(params=(\"value\",))\n        self._test_request(params=(\"username\", \"password\"), methodname=\"login\")\n        self._test_request(params=(\"response\",), methodresponse=\"login\")\n        self._test_request(params=(\"pas\u00a3\",), encoding=\"utf-8\")\n        self._test_request(params=(None,), allow_none=1)\n        self.assertRaises(TypeError, self._test_request)\n        self.assertRaises(TypeError, self._test_request, params=(None,))\n\n    def test_latin1(self):\n        self._test_request(params=(\"pas\u00a3\",), encoding=\"latin1\")\n\n\nclass JsonRequestTest(RequestTest):\n    request_class = JsonRequest\n    default_method = \"GET\"\n    default_headers = {\n        b\"Content-Type\": [b\"application/json\"],\n        b\"Accept\": [b\"application/json, text/javascript, */*; q=0.01\"],\n    }\n\n    def setUp(self):\n        warnings.simplefilter(\"always\")\n        super().setUp()\n\n    def test_data(self):\n        r1 = self.request_class(url=\"http://www.example.com/\")\n        self.assertEqual(r1.body, b\"\")\n\n        body = b\"body\"\n        r2 = self.request_class(url=\"http://www.example.com/\", body=body)\n        self.assertEqual(r2.body, body)\n\n        data = {\n            \"name\": \"value\",\n        }\n        r3 = self.request_class(url=\"http://www.example.com/\", data=data)\n        self.assertEqual(r3.body, to_bytes(json.dumps(data)))\n\n        # empty data\n        r4 = self.request_class(url=\"http://www.example.com/\", data=[])\n        self.assertEqual(r4.body, to_bytes(json.dumps([])))\n\n    def test_data_method(self):\n        # data is not passed\n        r1 = self.request_class(url=\"http://www.example.com/\")\n        self.assertEqual(r1.method, \"GET\")\n\n        body = b\"body\"\n        r2 = self.request_class(url=\"http://www.example.com/\", body=body)\n        self.assertEqual(r2.method, \"GET\")\n\n        data = {\n            \"name\": \"value\",\n        }\n        r3 = self.request_class(url=\"http://www.example.com/\", data=data)\n        self.assertEqual(r3.method, \"POST\")\n\n        # method passed explicitly\n        r4 = self.request_class(url=\"http://www.example.com/\", data=data, method=\"GET\")\n        self.assertEqual(r4.method, \"GET\")\n\n        r5 = self.request_class(url=\"http://www.example.com/\", data=[])\n        self.assertEqual(r5.method, \"POST\")\n\n    def test_body_data(self):\n        \"\"\"passing both body and data should result a warning\"\"\"\n        body = b\"body\"\n        data = {\n            \"name\": \"value\",\n        }\n        with warnings.catch_warnings(record=True) as _warnings:\n            r5 = self.request_class(url=\"http://www.example.com/\", body=body, data=data)\n            self.assertEqual(r5.body, body)\n            self.assertEqual(r5.method, \"GET\")\n            self.assertEqual(len(_warnings), 1)\n            self.assertIn(\"data will be ignored\", str(_warnings[0].message))\n\n    def test_empty_body_data(self):\n        \"\"\"passing any body value and data should result a warning\"\"\"\n        data = {\n            \"name\": \"value\",\n        }\n        with warnings.catch_warnings(record=True) as _warnings:\n            r6 = self.request_class(url=\"http://www.example.com/\", body=b\"\", data=data)\n            self.assertEqual(r6.body, b\"\")\n            self.assertEqual(r6.method, \"GET\")\n            self.assertEqual(len(_warnings), 1)\n            self.assertIn(\"data will be ignored\", str(_warnings[0].message))\n\n    def test_body_none_data(self):\n        data = {\n            \"name\": \"value\",\n        }\n        with warnings.catch_warnings(record=True) as _warnings:\n            r7 = self.request_class(url=\"http://www.example.com/\", body=None, data=data)\n            self.assertEqual(r7.body, to_bytes(json.dumps(data)))\n            self.assertEqual(r7.method, \"POST\")\n            self.assertEqual(len(_warnings), 0)\n\n    def test_body_data_none(self):\n        with warnings.catch_warnings(record=True) as _warnings:\n            r8 = self.request_class(url=\"http://www.example.com/\", body=None, data=None)\n            self.assertEqual(r8.method, \"GET\")\n            self.assertEqual(len(_warnings), 0)\n\n    def test_dumps_sort_keys(self):\n        \"\"\"Test that sort_keys=True is passed to json.dumps by default\"\"\"\n        data = {\n            \"name\": \"value\",\n        }\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            self.request_class(url=\"http://www.example.com/\", data=data)\n            kwargs = mock_dumps.call_args[1]\n            self.assertEqual(kwargs[\"sort_keys\"], True)\n\n    def test_dumps_kwargs(self):\n        \"\"\"Test that dumps_kwargs are passed to json.dumps\"\"\"\n        data = {\n            \"name\": \"value\",\n        }\n        dumps_kwargs = {\n            \"ensure_ascii\": True,\n            \"allow_nan\": True,\n        }\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            self.request_class(\n                url=\"http://www.example.com/\", data=data, dumps_kwargs=dumps_kwargs\n            )\n            kwargs = mock_dumps.call_args[1]\n            self.assertEqual(kwargs[\"ensure_ascii\"], True)\n            self.assertEqual(kwargs[\"allow_nan\"], True)\n\n    def test_replace_data(self):\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        r1 = self.request_class(url=\"http://www.example.com/\", data=data1)\n        r2 = r1.replace(data=data2)\n        self.assertEqual(r2.body, to_bytes(json.dumps(data2)))\n\n    def test_replace_sort_keys(self):\n        \"\"\"Test that replace provides sort_keys=True to json.dumps\"\"\"\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        r1 = self.request_class(url=\"http://www.example.com/\", data=data1)\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            r1.replace(data=data2)\n            kwargs = mock_dumps.call_args[1]\n            self.assertEqual(kwargs[\"sort_keys\"], True)\n\n    def test_replace_dumps_kwargs(self):\n        \"\"\"Test that dumps_kwargs are provided to json.dumps when replace is called\"\"\"\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        dumps_kwargs = {\n            \"ensure_ascii\": True,\n            \"allow_nan\": True,\n        }\n        r1 = self.request_class(\n            url=\"http://www.example.com/\", data=data1, dumps_kwargs=dumps_kwargs\n        )\n        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n            r1.replace(data=data2)\n            kwargs = mock_dumps.call_args[1]\n            self.assertEqual(kwargs[\"ensure_ascii\"], True)\n            self.assertEqual(kwargs[\"allow_nan\"], True)\n\n    def test_replacement_both_body_and_data_warns(self):\n        \"\"\"Test that we get a warning if both body and data are passed\"\"\"\n        body1 = None\n        body2 = b\"body\"\n        data1 = {\n            \"name1\": \"value1\",\n        }\n        data2 = {\n            \"name2\": \"value2\",\n        }\n        r1 = self.request_class(url=\"http://www.example.com/\", data=data1, body=body1)\n\n        with warnings.catch_warnings(record=True) as _warnings:\n            r1.replace(data=data2, body=body2)\n            self.assertIn(\n                \"Both body and data passed. data will be ignored\",\n                str(_warnings[0].message),\n            )\n\n    def tearDown(self):\n        warnings.resetwarnings()\n        super().tearDown()\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_extension_periodic_log.py": "import datetime\nimport typing\nimport unittest\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.extensions.periodic_log import PeriodicLog\n\nfrom .spiders import MetaSpider\n\nstats_dump_1 = {\n    \"log_count/INFO\": 10,\n    \"log_count/WARNING\": 1,\n    \"start_time\": datetime.datetime(2023, 6, 16, 8, 59, 18, 993170),\n    \"scheduler/enqueued/memory\": 190,\n    \"scheduler/enqueued\": 190,\n    \"scheduler/dequeued/memory\": 166,\n    \"scheduler/dequeued\": 166,\n    \"downloader/request_count\": 166,\n    \"downloader/request_method_count/GET\": 166,\n    \"downloader/request_bytes\": 56803,\n    \"downloader/response_count\": 150,\n    \"downloader/response_status_count/200\": 150,\n    \"downloader/response_bytes\": 595698,\n    \"httpcompression/response_bytes\": 3186068,\n    \"httpcompression/response_count\": 150,\n    \"response_received_count\": 150,\n    \"request_depth_max\": 9,\n    \"dupefilter/filtered\": 180,\n    \"item_scraped_count\": 140,\n}\nstats_dump_2 = {\n    \"log_count/INFO\": 12,\n    \"log_count/WARNING\": 1,\n    \"start_time\": datetime.datetime(2023, 6, 16, 8, 59, 18, 993170),\n    \"scheduler/enqueued/memory\": 337,\n    \"scheduler/enqueued\": 337,\n    \"scheduler/dequeued/memory\": 280,\n    \"scheduler/dequeued\": 280,\n    \"downloader/request_count\": 280,\n    \"downloader/request_method_count/GET\": 280,\n    \"downloader/request_bytes\": 95754,\n    \"downloader/response_count\": 264,\n    \"downloader/response_status_count/200\": 264,\n    \"downloader/response_bytes\": 1046274,\n    \"httpcompression/response_bytes\": 5614484,\n    \"httpcompression/response_count\": 264,\n    \"response_received_count\": 264,\n    \"request_depth_max\": 16,\n    \"dupefilter/filtered\": 320,\n    \"item_scraped_count\": 248,\n}\n\n\nclass TestExtPeriodicLog(PeriodicLog):\n    def set_a(self):\n        self.stats._stats = stats_dump_1\n\n    def set_b(self):\n        self.stats._stats = stats_dump_2\n\n\ndef extension(settings=None):\n    crawler = Crawler(MetaSpider, settings=settings)\n    crawler._apply_settings()\n    return TestExtPeriodicLog.from_crawler(crawler)\n\n\nclass TestPeriodicLog(unittest.TestCase):\n    def test_extension_enabled(self):\n        # Expected that settings for this extension loaded successfully\n        # And on certain conditions - extension raising NotConfigured\n\n        # \"PERIODIC_LOG_STATS\": True -> set to {\"enabled\": True}\n        # due to TypeError exception from settings.getdict\n        assert extension({\"PERIODIC_LOG_STATS\": True, \"LOGSTATS_INTERVAL\": 60})\n\n        # \"PERIODIC_LOG_STATS\": \"True\" -> set to {\"enabled\": True}\n        # due to JSONDecodeError(ValueError) exception from settings.getdict\n        assert extension({\"PERIODIC_LOG_STATS\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n\n        # The ame for PERIODIC_LOG_DELTA:\n        assert extension({\"PERIODIC_LOG_DELTA\": True, \"LOGSTATS_INTERVAL\": 60})\n        assert extension({\"PERIODIC_LOG_DELTA\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n\n    def test_log_delta(self):\n        def emulate(settings=None):\n            spider = MetaSpider()\n            ext = extension(settings)\n            ext.spider_opened(spider)\n            ext.set_a()\n            a = ext.log_delta()\n            ext.set_a()\n            b = ext.log_delta()\n            ext.spider_closed(spider, reason=\"finished\")\n            return ext, a, b\n\n        def check(settings: dict, condition: typing.Callable):\n            ext, a, b = emulate(settings)\n            assert list(a[\"delta\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n            assert list(b[\"delta\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n\n        # Including all\n        check({\"PERIODIC_LOG_DELTA\": True}, lambda k, v: isinstance(v, (int, float)))\n\n        # include:\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]}},\n            lambda k, v: isinstance(v, (int, float)) and \"downloader/\" in k,\n        )\n\n        # include multiple\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: isinstance(v, (int, float))\n            and (\"downloader/\" in k or \"scheduler/\" in k),\n        )\n\n        # exclude\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\"]}},\n            lambda k, v: isinstance(v, (int, float)) and \"downloader/\" not in k,\n        )\n\n        # exclude multiple\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: isinstance(v, (int, float))\n            and (\"downloader/\" not in k and \"scheduler/\" not in k),\n        )\n\n        # include exclude combined\n        check(\n            {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n            lambda k, v: isinstance(v, (int, float))\n            and (\"downloader/\" in k and \"bytes\" not in k),\n        )\n\n    def test_log_stats(self):\n        def emulate(settings=None):\n            spider = MetaSpider()\n            ext = extension(settings)\n            ext.spider_opened(spider)\n            ext.set_a()\n            a = ext.log_crawler_stats()\n            ext.set_a()\n            b = ext.log_crawler_stats()\n            ext.spider_closed(spider, reason=\"finished\")\n            return ext, a, b\n\n        def check(settings: dict, condition: typing.Callable):\n            ext, a, b = emulate(settings)\n            assert list(a[\"stats\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n            assert list(b[\"stats\"].keys()) == [\n                k for k, v in ext.stats._stats.items() if condition(k, v)\n            ]\n\n        # Including all\n        check({\"PERIODIC_LOG_STATS\": True}, lambda k, v: True)\n\n        # include:\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"]}},\n            lambda k, v: \"downloader/\" in k,\n        )\n\n        # include multiple\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: \"downloader/\" in k or \"scheduler/\" in k,\n        )\n\n        # exclude\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\"]}},\n            lambda k, v: \"downloader/\" not in k,\n        )\n\n        # exclude multiple\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\", \"scheduler/\"]}},\n            lambda k, v: \"downloader/\" not in k and \"scheduler/\" not in k,\n        )\n\n        # include exclude combined\n        check(\n            {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n            lambda k, v: \"downloader/\" in k and \"bytes\" not in k,\n        )\n        #\n", "tests/test_command_parse.py": "import argparse\nimport os\nfrom pathlib import Path\n\nfrom twisted.internet import defer\n\nfrom scrapy.commands import parse\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.testproc import ProcessTest\nfrom scrapy.utils.testsite import SiteTest\nfrom tests.test_commands import CommandTest\n\n\ndef _textmode(bstr):\n    \"\"\"Normalize input the same as writing to a file\n    and reading from it in text mode\"\"\"\n    return to_unicode(bstr).replace(os.linesep, \"\\n\")\n\n\nclass ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n    command = \"parse\"\n\n    def setUp(self):\n        super().setUp()\n        self.spider_name = \"parse_spider\"\n        (self.proj_mod_path / \"spiders\" / \"myspider.py\").write_text(\n            f\"\"\"\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.utils.test import get_from_asyncio_queue\nimport asyncio\n\n\nclass AsyncDefAsyncioReturnSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_return\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {{status}}\")\n        return [{{'id': 1}}, {{'id': 2}}]\n\nclass AsyncDefAsyncioReturnSingleElementSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_return_single_element\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.1)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {{status}}\")\n        return {{'foo': 42}}\n\nclass AsyncDefAsyncioGenLoopSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_gen_loop\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {{'foo': i}}\n        self.logger.info(f\"Got response {{response.status}}\")\n\nclass AsyncDefAsyncioSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.debug(f\"Got response {{status}}\")\n\nclass AsyncDefAsyncioGenExcSpider(scrapy.Spider):\n    name = \"asyncdef_asyncio_gen_exc\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {{'foo': i}}\n            if i > 5:\n                raise ValueError(\"Stopping the processing\")\n\nclass CallbackSignatureDownloaderMiddleware:\n    def process_request(self, request, spider):\n        from inspect import signature\n        spider.logger.debug(f\"request.callback signature: {{signature(request.callback)}}\")\n\n\nclass MySpider(scrapy.Spider):\n    name = '{self.spider_name}'\n\n    custom_settings = {{\n        \"DOWNLOADER_MIDDLEWARES\": {{\n            CallbackSignatureDownloaderMiddleware: 0,\n        }}\n    }}\n\n    def parse(self, response):\n        if getattr(self, 'test_arg', None):\n            self.logger.debug('It Works!')\n        return [scrapy.Item(), dict(foo='bar')]\n\n    def parse_request_with_meta(self, response):\n        foo = response.meta.get('foo', 'bar')\n\n        if foo == 'bar':\n            self.logger.debug('It Does Not Work :(')\n        else:\n            self.logger.debug('It Works!')\n\n    def parse_request_with_cb_kwargs(self, response, foo=None, key=None):\n        if foo == 'bar' and key == 'value':\n            self.logger.debug('It Works!')\n        else:\n            self.logger.debug('It Does Not Work :(')\n\n    def parse_request_without_meta(self, response):\n        foo = response.meta.get('foo', 'bar')\n\n        if foo == 'bar':\n            self.logger.debug('It Works!')\n        else:\n            self.logger.debug('It Does Not Work :(')\n\nclass MyGoodCrawlSpider(CrawlSpider):\n    name = 'goodcrawl{self.spider_name}'\n\n    rules = (\n        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n        Rule(LinkExtractor(allow=r'/text'), follow=True),\n    )\n\n    def parse_item(self, response):\n        return [scrapy.Item(), dict(foo='bar')]\n\n    def parse(self, response):\n        return [scrapy.Item(), dict(nomatch='default')]\n\n\nclass MyBadCrawlSpider(CrawlSpider):\n    '''Spider which doesn't define a parse_item callback while using it in a rule.'''\n    name = 'badcrawl{self.spider_name}'\n\n    rules = (\n        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n    )\n\n    def parse(self, response):\n        return [scrapy.Item(), dict(foo='bar')]\n\"\"\",\n            encoding=\"utf-8\",\n        )\n\n        (self.proj_mod_path / \"pipelines.py\").write_text(\n            \"\"\"\nimport logging\n\nclass MyPipeline:\n    component_name = 'my_pipeline'\n\n    def process_item(self, item, spider):\n        logging.info('It Works!')\n        return item\n\"\"\",\n            encoding=\"utf-8\",\n        )\n\n        with (self.proj_mod_path / \"settings.py\").open(\"a\", encoding=\"utf-8\") as f:\n            f.write(\n                f\"\"\"\nITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n\"\"\"\n            )\n\n    @defer.inlineCallbacks\n    def test_spider_arguments(self):\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"-a\",\n                \"test_arg=1\",\n                \"-c\",\n                \"parse\",\n                \"--verbose\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_request_with_meta(self):\n        raw_json_string = '{\"foo\" : \"baz\"}'\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"--meta\",\n                raw_json_string,\n                \"-c\",\n                \"parse_request_with_meta\",\n                \"--verbose\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"-m\",\n                raw_json_string,\n                \"-c\",\n                \"parse_request_with_meta\",\n                \"--verbose\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_request_with_cb_kwargs(self):\n        raw_json_string = '{\"foo\" : \"bar\", \"key\": \"value\"}'\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"--cbkwargs\",\n                raw_json_string,\n                \"-c\",\n                \"parse_request_with_cb_kwargs\",\n                \"--verbose\",\n                self.url(\"/html\"),\n            ]\n        )\n        log = _textmode(stderr)\n        self.assertIn(\"DEBUG: It Works!\", log)\n        self.assertIn(\n            \"DEBUG: request.callback signature: (response, foo=None, key=None)\", log\n        )\n\n    @defer.inlineCallbacks\n    def test_request_without_meta(self):\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"-c\",\n                \"parse_request_without_meta\",\n                \"--nolinks\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_pipelines(self):\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"--pipelines\",\n                \"-c\",\n                \"parse\",\n                \"--verbose\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse_items_list(self):\n        status, out, stderr = yield self.execute(\n            [\n                \"--spider\",\n                \"asyncdef_asyncio_return\",\n                \"-c\",\n                \"parse\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n        self.assertIn(\"{'id': 1}\", _textmode(out))\n        self.assertIn(\"{'id': 2}\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse_items_single_element(self):\n        status, out, stderr = yield self.execute(\n            [\n                \"--spider\",\n                \"asyncdef_asyncio_return_single_element\",\n                \"-c\",\n                \"parse\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n        self.assertIn(\"{'foo': 42}\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_async_def_asyncgen_parse_loop(self):\n        status, out, stderr = yield self.execute(\n            [\n                \"--spider\",\n                \"asyncdef_asyncio_gen_loop\",\n                \"-c\",\n                \"parse\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n        for i in range(10):\n            self.assertIn(f\"{{'foo': {i}}}\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_async_def_asyncgen_parse_exc(self):\n        status, out, stderr = yield self.execute(\n            [\n                \"--spider\",\n                \"asyncdef_asyncio_gen_exc\",\n                \"-c\",\n                \"parse\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"ValueError\", _textmode(stderr))\n        for i in range(7):\n            self.assertIn(f\"{{'foo': {i}}}\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_async_def_asyncio_parse(self):\n        _, _, stderr = yield self.execute(\n            [\n                \"--spider\",\n                \"asyncdef_asyncio\",\n                \"-c\",\n                \"parse\",\n                self.url(\"/html\"),\n            ]\n        )\n        self.assertIn(\"DEBUG: Got response 200\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_parse_items(self):\n        status, out, stderr = yield self.execute(\n            [\"--spider\", self.spider_name, \"-c\", \"parse\", self.url(\"/html\")]\n        )\n        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_parse_items_no_callback_passed(self):\n        status, out, stderr = yield self.execute(\n            [\"--spider\", self.spider_name, self.url(\"/html\")]\n        )\n        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_wrong_callback_passed(self):\n        status, out, stderr = yield self.execute(\n            [\"--spider\", self.spider_name, \"-c\", \"dummy\", self.url(\"/html\")]\n        )\n        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n        self.assertIn(\"\"\"Cannot find callback\"\"\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_matching_rule_callback_set(self):\n        \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n        status, out, stderr = yield self.execute(\n            [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n        )\n        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_matching_rule_default_callback(self):\n        \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n        status, out, stderr = yield self.execute(\n            [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/text\")]\n        )\n        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", _textmode(out))\n\n    @defer.inlineCallbacks\n    def test_spider_with_no_rules_attribute(self):\n        \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n        status, out, stderr = yield self.execute(\n            [\"--spider\", self.spider_name, \"-r\", self.url(\"/html\")]\n        )\n        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n        self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_missing_callback(self):\n        status, out, stderr = yield self.execute(\n            [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n        )\n        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n\n    @defer.inlineCallbacks\n    def test_crawlspider_no_matching_rule(self):\n        \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n        status, out, stderr = yield self.execute(\n            [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/enc-gb18030\")]\n        )\n        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n        self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n\n    @defer.inlineCallbacks\n    def test_crawlspider_not_exists_with_not_matched_url(self):\n        status, out, stderr = yield self.execute([self.url(\"/invalid_url\")])\n        self.assertEqual(status, 0)\n\n    @defer.inlineCallbacks\n    def test_output_flag(self):\n        \"\"\"Checks if a file was created successfully having\n        correct format containing correct data in it.\n        \"\"\"\n        file_name = \"data.json\"\n        file_path = Path(self.proj_path, file_name)\n        yield self.execute(\n            [\n                \"--spider\",\n                self.spider_name,\n                \"-c\",\n                \"parse\",\n                \"-o\",\n                file_name,\n                self.url(\"/html\"),\n            ]\n        )\n\n        self.assertTrue(file_path.exists())\n        self.assertTrue(file_path.is_file())\n\n        content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n        self.assertEqual(file_path.read_text(encoding=\"utf-8\"), content)\n\n    def test_parse_add_options(self):\n        command = parse.Command()\n        command.settings = Settings()\n        parser = argparse.ArgumentParser(\n            prog=\"scrapy\",\n            formatter_class=argparse.HelpFormatter,\n            conflict_handler=\"resolve\",\n            prefix_chars=\"-\",\n        )\n        command.add_options(parser)\n        namespace = parser.parse_args(\n            [\"--verbose\", \"--nolinks\", \"-d\", \"2\", \"--spider\", self.spider_name]\n        )\n        self.assertTrue(namespace.nolinks)\n        self.assertEqual(namespace.depth, 2)\n        self.assertEqual(namespace.spider, self.spider_name)\n        self.assertTrue(namespace.verbose)\n", "tests/test_utils_datatypes.py": "import copy\nimport unittest\nimport warnings\nfrom collections.abc import Mapping, MutableMapping\nfrom typing import Iterator\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request\nfrom scrapy.utils.datatypes import (\n    CaseInsensitiveDict,\n    CaselessDict,\n    LocalCache,\n    LocalWeakReferencedCache,\n    SequenceExclude,\n)\nfrom scrapy.utils.python import garbage_collect\n\n__doctests__ = [\"scrapy.utils.datatypes\"]\n\n\nclass CaseInsensitiveDictMixin:\n    def test_init_dict(self):\n        seq = {\"red\": 1, \"black\": 3}\n        d = self.dict_class(seq)\n        self.assertEqual(d[\"red\"], 1)\n        self.assertEqual(d[\"black\"], 3)\n\n    def test_init_pair_sequence(self):\n        seq = ((\"red\", 1), (\"black\", 3))\n        d = self.dict_class(seq)\n        self.assertEqual(d[\"red\"], 1)\n        self.assertEqual(d[\"black\"], 3)\n\n    def test_init_mapping(self):\n        class MyMapping(Mapping):\n            def __init__(self, **kwargs):\n                self._d = kwargs\n\n            def __getitem__(self, key):\n                return self._d[key]\n\n            def __iter__(self):\n                return iter(self._d)\n\n            def __len__(self):\n                return len(self._d)\n\n        seq = MyMapping(red=1, black=3)\n        d = self.dict_class(seq)\n        self.assertEqual(d[\"red\"], 1)\n        self.assertEqual(d[\"black\"], 3)\n\n    def test_init_mutable_mapping(self):\n        class MyMutableMapping(MutableMapping):\n            def __init__(self, **kwargs):\n                self._d = kwargs\n\n            def __getitem__(self, key):\n                return self._d[key]\n\n            def __setitem__(self, key, value):\n                self._d[key] = value\n\n            def __delitem__(self, key):\n                del self._d[key]\n\n            def __iter__(self):\n                return iter(self._d)\n\n            def __len__(self):\n                return len(self._d)\n\n        seq = MyMutableMapping(red=1, black=3)\n        d = self.dict_class(seq)\n        self.assertEqual(d[\"red\"], 1)\n        self.assertEqual(d[\"black\"], 3)\n\n    def test_caseless(self):\n        d = self.dict_class()\n        d[\"key_Lower\"] = 1\n        self.assertEqual(d[\"KEy_loWer\"], 1)\n        self.assertEqual(d.get(\"KEy_loWer\"), 1)\n\n        d[\"KEY_LOWER\"] = 3\n        self.assertEqual(d[\"key_Lower\"], 3)\n        self.assertEqual(d.get(\"key_Lower\"), 3)\n\n    def test_delete(self):\n        d = self.dict_class({\"key_lower\": 1})\n        del d[\"key_LOWER\"]\n        self.assertRaises(KeyError, d.__getitem__, \"key_LOWER\")\n        self.assertRaises(KeyError, d.__getitem__, \"key_lower\")\n\n    def test_getdefault(self):\n        d = CaselessDict()\n        self.assertEqual(d.get(\"c\", 5), 5)\n        d[\"c\"] = 10\n        self.assertEqual(d.get(\"c\", 5), 10)\n\n    def test_setdefault(self):\n        d = CaselessDict({\"a\": 1, \"b\": 2})\n\n        r = d.setdefault(\"A\", 5)\n        self.assertEqual(r, 1)\n        self.assertEqual(d[\"A\"], 1)\n\n        r = d.setdefault(\"c\", 5)\n        self.assertEqual(r, 5)\n        self.assertEqual(d[\"C\"], 5)\n\n    def test_fromkeys(self):\n        keys = (\"a\", \"b\")\n\n        d = self.dict_class.fromkeys(keys)\n        self.assertEqual(d[\"A\"], None)\n        self.assertEqual(d[\"B\"], None)\n\n        d = self.dict_class.fromkeys(keys, 1)\n        self.assertEqual(d[\"A\"], 1)\n        self.assertEqual(d[\"B\"], 1)\n\n        instance = self.dict_class()\n        d = instance.fromkeys(keys)\n        self.assertEqual(d[\"A\"], None)\n        self.assertEqual(d[\"B\"], None)\n\n        d = instance.fromkeys(keys, 1)\n        self.assertEqual(d[\"A\"], 1)\n        self.assertEqual(d[\"B\"], 1)\n\n    def test_contains(self):\n        d = self.dict_class()\n        d[\"a\"] = 1\n        assert \"A\" in d\n\n    def test_pop(self):\n        d = self.dict_class()\n        d[\"a\"] = 1\n        self.assertEqual(d.pop(\"A\"), 1)\n        self.assertRaises(KeyError, d.pop, \"A\")\n\n    def test_normkey(self):\n        class MyDict(self.dict_class):\n            def _normkey(self, key):\n                return key.title()\n\n            normkey = _normkey  # deprecated CaselessDict class\n\n        d = MyDict()\n        d[\"key-one\"] = 2\n        self.assertEqual(list(d.keys()), [\"Key-One\"])\n\n    def test_normvalue(self):\n        class MyDict(self.dict_class):\n            def _normvalue(self, value):\n                if value is not None:\n                    return value + 1\n\n            normvalue = _normvalue  # deprecated CaselessDict class\n\n        d = MyDict({\"key\": 1})\n        self.assertEqual(d[\"key\"], 2)\n        self.assertEqual(d.get(\"key\"), 2)\n\n        d = MyDict()\n        d[\"key\"] = 1\n        self.assertEqual(d[\"key\"], 2)\n        self.assertEqual(d.get(\"key\"), 2)\n\n        d = MyDict()\n        d.setdefault(\"key\", 1)\n        self.assertEqual(d[\"key\"], 2)\n        self.assertEqual(d.get(\"key\"), 2)\n\n        d = MyDict()\n        d.update({\"key\": 1})\n        self.assertEqual(d[\"key\"], 2)\n        self.assertEqual(d.get(\"key\"), 2)\n\n        d = MyDict.fromkeys((\"key\",), 1)\n        self.assertEqual(d[\"key\"], 2)\n        self.assertEqual(d.get(\"key\"), 2)\n\n    def test_copy(self):\n        h1 = self.dict_class({\"header1\": \"value\"})\n        h2 = copy.copy(h1)\n        assert isinstance(h2, self.dict_class)\n        self.assertEqual(h1, h2)\n        self.assertEqual(h1.get(\"header1\"), h2.get(\"header1\"))\n        self.assertEqual(h1.get(\"header1\"), h2.get(\"HEADER1\"))\n        h3 = h1.copy()\n        assert isinstance(h3, self.dict_class)\n        self.assertEqual(h1, h3)\n        self.assertEqual(h1.get(\"header1\"), h3.get(\"header1\"))\n        self.assertEqual(h1.get(\"header1\"), h3.get(\"HEADER1\"))\n\n\nclass CaseInsensitiveDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n    dict_class = CaseInsensitiveDict\n\n    def test_repr(self):\n        d1 = self.dict_class({\"foo\": \"bar\"})\n        self.assertEqual(repr(d1), \"<CaseInsensitiveDict: {'foo': 'bar'}>\")\n        d2 = self.dict_class({\"AsDf\": \"QwErTy\", \"FoO\": \"bAr\"})\n        self.assertEqual(\n            repr(d2), \"<CaseInsensitiveDict: {'AsDf': 'QwErTy', 'FoO': 'bAr'}>\"\n        )\n\n    def test_iter(self):\n        d = self.dict_class({\"AsDf\": \"QwErTy\", \"FoO\": \"bAr\"})\n        iterkeys = iter(d)\n        self.assertIsInstance(iterkeys, Iterator)\n        self.assertEqual(list(iterkeys), [\"AsDf\", \"FoO\"])\n\n\nclass CaselessDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n    dict_class = CaselessDict\n\n    def test_deprecation_message(self):\n        with warnings.catch_warnings(record=True) as caught:\n            self.dict_class({\"foo\": \"bar\"})\n\n            self.assertEqual(len(caught), 1)\n            self.assertTrue(issubclass(caught[0].category, ScrapyDeprecationWarning))\n            self.assertEqual(\n                \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\",\n                str(caught[0].message),\n            )\n\n\nclass SequenceExcludeTest(unittest.TestCase):\n    def test_list(self):\n        seq = [1, 2, 3]\n        d = SequenceExclude(seq)\n        self.assertIn(0, d)\n        self.assertIn(4, d)\n        self.assertNotIn(2, d)\n\n    def test_range(self):\n        seq = range(10, 20)\n        d = SequenceExclude(seq)\n        self.assertIn(5, d)\n        self.assertIn(20, d)\n        self.assertNotIn(15, d)\n\n    def test_range_step(self):\n        seq = range(10, 20, 3)\n        d = SequenceExclude(seq)\n        are_not_in = [v for v in range(10, 20, 3) if v in d]\n        self.assertEqual([], are_not_in)\n\n        are_not_in = [v for v in range(10, 20) if v in d]\n        self.assertEqual([11, 12, 14, 15, 17, 18], are_not_in)\n\n    def test_string_seq(self):\n        seq = \"cde\"\n        d = SequenceExclude(seq)\n        chars = \"\".join(v for v in \"abcdefg\" if v in d)\n        self.assertEqual(\"abfg\", chars)\n\n    def test_stringset_seq(self):\n        seq = set(\"cde\")\n        d = SequenceExclude(seq)\n        chars = \"\".join(v for v in \"abcdefg\" if v in d)\n        self.assertEqual(\"abfg\", chars)\n\n    def test_set(self):\n        \"\"\"Anything that is not in the supplied sequence will evaluate as 'in' the container.\"\"\"\n        seq = {-3, \"test\", 1.1}\n        d = SequenceExclude(seq)\n        self.assertIn(0, d)\n        self.assertIn(\"foo\", d)\n        self.assertIn(3.14, d)\n        self.assertIn(set(\"bar\"), d)\n\n        # supplied sequence is a set, so checking for list (non)inclusion fails\n        self.assertRaises(TypeError, (0, 1, 2) in d)\n        self.assertRaises(TypeError, d.__contains__, [\"a\", \"b\", \"c\"])\n\n        for v in [-3, \"test\", 1.1]:\n            self.assertNotIn(v, d)\n\n\nclass LocalCacheTest(unittest.TestCase):\n    def test_cache_with_limit(self):\n        cache = LocalCache(limit=2)\n        cache[\"a\"] = 1\n        cache[\"b\"] = 2\n        cache[\"c\"] = 3\n        self.assertEqual(len(cache), 2)\n        self.assertNotIn(\"a\", cache)\n        self.assertIn(\"b\", cache)\n        self.assertIn(\"c\", cache)\n        self.assertEqual(cache[\"b\"], 2)\n        self.assertEqual(cache[\"c\"], 3)\n\n    def test_cache_without_limit(self):\n        maximum = 10**4\n        cache = LocalCache()\n        for x in range(maximum):\n            cache[str(x)] = x\n        self.assertEqual(len(cache), maximum)\n        for x in range(maximum):\n            self.assertIn(str(x), cache)\n            self.assertEqual(cache[str(x)], x)\n\n\nclass LocalWeakReferencedCacheTest(unittest.TestCase):\n    def test_cache_with_limit(self):\n        cache = LocalWeakReferencedCache(limit=2)\n        r1 = Request(\"https://example.org\")\n        r2 = Request(\"https://example.com\")\n        r3 = Request(\"https://example.net\")\n        cache[r1] = 1\n        cache[r2] = 2\n        cache[r3] = 3\n        self.assertEqual(len(cache), 2)\n        self.assertNotIn(r1, cache)\n        self.assertIn(r2, cache)\n        self.assertIn(r3, cache)\n        self.assertEqual(cache[r1], None)\n        self.assertEqual(cache[r2], 2)\n        self.assertEqual(cache[r3], 3)\n        del r2\n\n        # PyPy takes longer to collect dead references\n        garbage_collect()\n\n        self.assertEqual(len(cache), 1)\n\n    def test_cache_non_weak_referenceable_objects(self):\n        cache = LocalWeakReferencedCache()\n        k1 = None\n        k2 = 1\n        k3 = [1, 2, 3]\n        cache[k1] = 1\n        cache[k2] = 2\n        cache[k3] = 3\n        self.assertNotIn(k1, cache)\n        self.assertNotIn(k2, cache)\n        self.assertNotIn(k3, cache)\n        self.assertEqual(len(cache), 0)\n\n    def test_cache_without_limit(self):\n        max = 10**4\n        cache = LocalWeakReferencedCache()\n        refs = []\n        for x in range(max):\n            refs.append(Request(f\"https://example.org/{x}\"))\n            cache[refs[-1]] = x\n        self.assertEqual(len(cache), max)\n        for i, r in enumerate(refs):\n            self.assertIn(r, cache)\n            self.assertEqual(cache[r], i)\n        del r  # delete reference to the last object in the list  # pylint: disable=undefined-loop-variable\n\n        # delete half of the objects, make sure that is reflected in the cache\n        for _ in range(max // 2):\n            refs.pop()\n\n        # PyPy takes longer to collect dead references\n        garbage_collect()\n\n        self.assertEqual(len(cache), max // 2)\n        for i, r in enumerate(refs):\n            self.assertIn(r, cache)\n            self.assertEqual(cache[r], i)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_spider.py": "import unittest\n\nfrom scrapy import Spider\nfrom scrapy.http import Request\nfrom scrapy.item import Item\nfrom scrapy.utils.spider import iter_spider_classes, iterate_spider_output\n\n\nclass MySpider1(Spider):\n    name = \"myspider1\"\n\n\nclass MySpider2(Spider):\n    name = \"myspider2\"\n\n\nclass UtilsSpidersTestCase(unittest.TestCase):\n    def test_iterate_spider_output(self):\n        i = Item()\n        r = Request(\"http://scrapytest.org\")\n        o = object()\n\n        self.assertEqual(list(iterate_spider_output(i)), [i])\n        self.assertEqual(list(iterate_spider_output(r)), [r])\n        self.assertEqual(list(iterate_spider_output(o)), [o])\n        self.assertEqual(list(iterate_spider_output([r, i, o])), [r, i, o])\n\n    def test_iter_spider_classes(self):\n        import tests.test_utils_spider  # pylint: disable=import-self\n\n        it = iter_spider_classes(tests.test_utils_spider)\n        self.assertEqual(set(it), {MySpider1, MySpider2})\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_downloadermiddleware_redirect.py": "import unittest\nfrom itertools import chain, product\n\nimport pytest\n\nfrom scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\nfrom scrapy.downloadermiddlewares.redirect import (\n    MetaRefreshMiddleware,\n    RedirectMiddleware,\n)\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.misc import set_environ\nfrom scrapy.utils.test import get_crawler\n\n\nclass Base:\n    class Test(unittest.TestCase):\n        def test_priority_adjust(self):\n            req = Request(\"http://a.com\")\n            rsp = self.get_response(req, \"http://a.com/redirected\")\n            req2 = self.mw.process_response(req, rsp, self.spider)\n            self.assertGreater(req2.priority, req.priority)\n\n        def test_dont_redirect(self):\n            url = \"http://www.example.com/301\"\n            url2 = \"http://www.example.com/redirected\"\n            req = Request(url, meta={\"dont_redirect\": True})\n            rsp = self.get_response(req, url2)\n\n            r = self.mw.process_response(req, rsp, self.spider)\n            assert isinstance(r, Response)\n            assert r is rsp\n\n            # Test that it redirects when dont_redirect is False\n            req = Request(url, meta={\"dont_redirect\": False})\n            rsp = self.get_response(req, url2)\n\n            r = self.mw.process_response(req, rsp, self.spider)\n            assert isinstance(r, Request)\n\n        def test_post(self):\n            url = \"http://www.example.com/302\"\n            url2 = \"http://www.example.com/redirected2\"\n            req = Request(\n                url,\n                method=\"POST\",\n                body=\"test\",\n                headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n            )\n            rsp = self.get_response(req, url2)\n\n            req2 = self.mw.process_response(req, rsp, self.spider)\n            assert isinstance(req2, Request)\n            self.assertEqual(req2.url, url2)\n            self.assertEqual(req2.method, \"GET\")\n            assert (\n                \"Content-Type\" not in req2.headers\n            ), \"Content-Type header must not be present in redirected request\"\n            assert (\n                \"Content-Length\" not in req2.headers\n            ), \"Content-Length header must not be present in redirected request\"\n            assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n\n        def test_max_redirect_times(self):\n            self.mw.max_redirect_times = 1\n            req = Request(\"http://scrapytest.org/302\")\n            rsp = self.get_response(req, \"/redirected\")\n\n            req = self.mw.process_response(req, rsp, self.spider)\n            assert isinstance(req, Request)\n            assert \"redirect_times\" in req.meta\n            self.assertEqual(req.meta[\"redirect_times\"], 1)\n            self.assertRaises(\n                IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n            )\n\n        def test_ttl(self):\n            self.mw.max_redirect_times = 100\n            req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n            rsp = self.get_response(req, \"/a\")\n\n            req = self.mw.process_response(req, rsp, self.spider)\n            assert isinstance(req, Request)\n            self.assertRaises(\n                IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n            )\n\n        def test_redirect_urls(self):\n            req1 = Request(\"http://scrapytest.org/first\")\n            rsp1 = self.get_response(req1, \"/redirected\")\n            req2 = self.mw.process_response(req1, rsp1, self.spider)\n            rsp2 = self.get_response(req1, \"/redirected2\")\n            req3 = self.mw.process_response(req2, rsp2, self.spider)\n\n            self.assertEqual(req2.url, \"http://scrapytest.org/redirected\")\n            self.assertEqual(\n                req2.meta[\"redirect_urls\"], [\"http://scrapytest.org/first\"]\n            )\n            self.assertEqual(req3.url, \"http://scrapytest.org/redirected2\")\n            self.assertEqual(\n                req3.meta[\"redirect_urls\"],\n                [\"http://scrapytest.org/first\", \"http://scrapytest.org/redirected\"],\n            )\n\n        def test_redirect_reasons(self):\n            req1 = Request(\"http://scrapytest.org/first\")\n            rsp1 = self.get_response(req1, \"/redirected1\")\n            req2 = self.mw.process_response(req1, rsp1, self.spider)\n            rsp2 = self.get_response(req2, \"/redirected2\")\n            req3 = self.mw.process_response(req2, rsp2, self.spider)\n            self.assertEqual(req2.meta[\"redirect_reasons\"], [self.reason])\n            self.assertEqual(req3.meta[\"redirect_reasons\"], [self.reason, self.reason])\n\n        def test_cross_origin_header_dropping(self):\n            safe_headers = {\"A\": \"B\"}\n            cookie_header = {\"Cookie\": \"a=b\"}\n            authorization_header = {\"Authorization\": \"Bearer 123456\"}\n\n            original_request = Request(\n                \"https://example.com\",\n                headers={**safe_headers, **cookie_header, **authorization_header},\n            )\n\n            # Redirects to the same origin (same scheme, same domain, same port)\n            # keep all headers.\n            internal_response = self.get_response(\n                original_request, \"https://example.com/a\"\n            )\n            internal_redirect_request = self.mw.process_response(\n                original_request, internal_response, self.spider\n            )\n            self.assertIsInstance(internal_redirect_request, Request)\n            self.assertEqual(\n                original_request.headers, internal_redirect_request.headers\n            )\n\n            # Redirects to the same origin (same scheme, same domain, same port)\n            # keep all headers also when the scheme is http.\n            http_request = Request(\n                \"http://example.com\",\n                headers={**safe_headers, **cookie_header, **authorization_header},\n            )\n            http_response = self.get_response(http_request, \"http://example.com/a\")\n            http_redirect_request = self.mw.process_response(\n                http_request, http_response, self.spider\n            )\n            self.assertIsInstance(http_redirect_request, Request)\n            self.assertEqual(http_request.headers, http_redirect_request.headers)\n\n            # For default ports, whether the port is explicit or implicit does not\n            # affect the outcome, it is still the same origin.\n            to_explicit_port_response = self.get_response(\n                original_request, \"https://example.com:443/a\"\n            )\n            to_explicit_port_redirect_request = self.mw.process_response(\n                original_request, to_explicit_port_response, self.spider\n            )\n            self.assertIsInstance(to_explicit_port_redirect_request, Request)\n            self.assertEqual(\n                original_request.headers, to_explicit_port_redirect_request.headers\n            )\n\n            # For default ports, whether the port is explicit or implicit does not\n            # affect the outcome, it is still the same origin.\n            to_implicit_port_response = self.get_response(\n                original_request, \"https://example.com/a\"\n            )\n            to_implicit_port_redirect_request = self.mw.process_response(\n                original_request, to_implicit_port_response, self.spider\n            )\n            self.assertIsInstance(to_implicit_port_redirect_request, Request)\n            self.assertEqual(\n                original_request.headers, to_implicit_port_redirect_request.headers\n            )\n\n            # A port change drops the Authorization header because the origin\n            # changes, but keeps the Cookie header because the domain remains the\n            # same.\n            different_port_response = self.get_response(\n                original_request, \"https://example.com:8080/a\"\n            )\n            different_port_redirect_request = self.mw.process_response(\n                original_request, different_port_response, self.spider\n            )\n            self.assertIsInstance(different_port_redirect_request, Request)\n            self.assertEqual(\n                {**safe_headers, **cookie_header},\n                different_port_redirect_request.headers.to_unicode_dict(),\n            )\n\n            # A domain change drops both the Authorization and the Cookie header.\n            external_response = self.get_response(\n                original_request, \"https://example.org/a\"\n            )\n            external_redirect_request = self.mw.process_response(\n                original_request, external_response, self.spider\n            )\n            self.assertIsInstance(external_redirect_request, Request)\n            self.assertEqual(\n                safe_headers, external_redirect_request.headers.to_unicode_dict()\n            )\n\n            # A scheme upgrade (http \u2192 https) drops the Authorization header\n            # because the origin changes, but keeps the Cookie header because the\n            # domain remains the same.\n            upgrade_response = self.get_response(http_request, \"https://example.com/a\")\n            upgrade_redirect_request = self.mw.process_response(\n                http_request, upgrade_response, self.spider\n            )\n            self.assertIsInstance(upgrade_redirect_request, Request)\n            self.assertEqual(\n                {**safe_headers, **cookie_header},\n                upgrade_redirect_request.headers.to_unicode_dict(),\n            )\n\n            # A scheme downgrade (https \u2192 http) drops the Authorization header\n            # because the origin changes, and the Cookie header because its value\n            # cannot indicate whether the cookies were secure (HTTPS-only) or not.\n            #\n            # Note: If the Cookie header is set by the cookie management\n            # middleware, as recommended in the docs, the dropping of Cookie on\n            # scheme downgrade is not an issue, because the cookie management\n            # middleware will add again the Cookie header to the new request if\n            # appropriate.\n            downgrade_response = self.get_response(\n                original_request, \"http://example.com/a\"\n            )\n            downgrade_redirect_request = self.mw.process_response(\n                original_request, downgrade_response, self.spider\n            )\n            self.assertIsInstance(downgrade_redirect_request, Request)\n            self.assertEqual(\n                safe_headers,\n                downgrade_redirect_request.headers.to_unicode_dict(),\n            )\n\n        def test_meta_proxy_http_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"http://example.com\", meta=meta)\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_meta_proxy_http_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"http://example.com\", meta=meta)\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_meta_proxy_https_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"https://example.com\", meta=meta)\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_meta_proxy_https_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"https://example.com\", meta=meta)\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_meta_proxy_http_to_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"http://example.com\", meta=meta)\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_meta_proxy_https_to_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            meta = {\"proxy\": \"https://a:@a.example\"}\n            request1 = Request(\"https://example.com\", meta=meta)\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_http_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_http_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_https_absolute(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_https_relative(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"/a\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"/a\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_proxied_http_to_proxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://b.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://b.example\")\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_proxied_http_to_unproxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n\n        def test_system_proxy_unproxied_http_to_proxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n            self.assertNotIn(\"_auth_proxy\", request1.meta)\n            self.assertNotIn(\"proxy\", request1.meta)\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://b.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://b.example\")\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n        def test_system_proxy_unproxied_http_to_unproxied_https(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"http://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n            self.assertNotIn(\"_auth_proxy\", request1.meta)\n            self.assertNotIn(\"proxy\", request1.meta)\n\n            response1 = self.get_response(request1, \"https://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            response2 = self.get_response(request2, \"http://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n        def test_system_proxy_proxied_https_to_proxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://b.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://b.example\")\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://b.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://b.example\")\n\n        def test_system_proxy_proxied_https_to_unproxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"https_proxy\": \"https://b:@b.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://b.example\")\n            self.assertEqual(request1.meta[\"proxy\"], \"https://b.example\")\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://b.example\")\n            self.assertEqual(request3.meta[\"proxy\"], \"https://b.example\")\n\n        def test_system_proxy_unproxied_https_to_proxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            env = {\n                \"http_proxy\": \"https://a:@a.example\",\n            }\n            with set_environ(**env):\n                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n            self.assertNotIn(\"_auth_proxy\", request1.meta)\n            self.assertNotIn(\"proxy\", request1.meta)\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n        def test_system_proxy_unproxied_https_to_unproxied_http(self):\n            crawler = get_crawler()\n            redirect_mw = self.mwcls.from_crawler(crawler)\n            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n\n            request1 = Request(\"https://example.com\")\n            spider = None\n            proxy_mw.process_request(request1, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n            self.assertNotIn(\"_auth_proxy\", request1.meta)\n            self.assertNotIn(\"proxy\", request1.meta)\n\n            response1 = self.get_response(request1, \"http://example.com\")\n            request2 = redirect_mw.process_response(request1, response1, spider)\n\n            self.assertIsInstance(request2, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            proxy_mw.process_request(request2, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n            self.assertNotIn(\"_auth_proxy\", request2.meta)\n            self.assertNotIn(\"proxy\", request2.meta)\n\n            response2 = self.get_response(request2, \"https://example.com\")\n            request3 = redirect_mw.process_response(request2, response2, spider)\n\n            self.assertIsInstance(request3, Request)\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n            proxy_mw.process_request(request3, spider)\n\n            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n            self.assertNotIn(\"_auth_proxy\", request3.meta)\n            self.assertNotIn(\"proxy\", request3.meta)\n\n\nclass RedirectMiddlewareTest(Base.Test):\n    mwcls = RedirectMiddleware\n    reason = 302\n\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mw = self.mwcls.from_crawler(self.crawler)\n\n    def get_response(self, request, location, status=302):\n        headers = {\"Location\": location}\n        return Response(request.url, status=status, headers=headers)\n\n    def test_redirect_3xx_permanent(self):\n        def _test(method, status=301):\n            url = f\"http://www.example.com/{status}\"\n            url2 = \"http://www.example.com/redirected\"\n            req = Request(url, method=method)\n            rsp = Response(url, headers={\"Location\": url2}, status=status)\n\n            req2 = self.mw.process_response(req, rsp, self.spider)\n            assert isinstance(req2, Request)\n            self.assertEqual(req2.url, url2)\n            self.assertEqual(req2.method, method)\n\n            # response without Location header but with status code is 3XX should be ignored\n            del rsp.headers[\"Location\"]\n            assert self.mw.process_response(req, rsp, self.spider) is rsp\n\n        _test(\"GET\")\n        _test(\"POST\")\n        _test(\"HEAD\")\n\n        _test(\"GET\", status=307)\n        _test(\"POST\", status=307)\n        _test(\"HEAD\", status=307)\n\n        _test(\"GET\", status=308)\n        _test(\"POST\", status=308)\n        _test(\"HEAD\", status=308)\n\n    def test_redirect_302_head(self):\n        url = \"http://www.example.com/302\"\n        url2 = \"http://www.example.com/redirected2\"\n        req = Request(url, method=\"HEAD\")\n        rsp = Response(url, headers={\"Location\": url2}, status=302)\n\n        req2 = self.mw.process_response(req, rsp, self.spider)\n        assert isinstance(req2, Request)\n        self.assertEqual(req2.url, url2)\n        self.assertEqual(req2.method, \"HEAD\")\n\n    def test_redirect_302_relative(self):\n        url = \"http://www.example.com/302\"\n        url2 = \"///i8n.example2.com/302\"\n        url3 = \"http://i8n.example2.com/302\"\n        req = Request(url, method=\"HEAD\")\n        rsp = Response(url, headers={\"Location\": url2}, status=302)\n\n        req2 = self.mw.process_response(req, rsp, self.spider)\n        assert isinstance(req2, Request)\n        self.assertEqual(req2.url, url3)\n        self.assertEqual(req2.method, \"HEAD\")\n\n    def test_spider_handling(self):\n        smartspider = self.crawler._create_spider(\"smarty\")\n        smartspider.handle_httpstatus_list = [404, 301, 302]\n        url = \"http://www.example.com/301\"\n        url2 = \"http://www.example.com/redirected\"\n        req = Request(url)\n        rsp = Response(url, headers={\"Location\": url2}, status=301)\n        r = self.mw.process_response(req, rsp, smartspider)\n        self.assertIs(r, rsp)\n\n    def test_request_meta_handling(self):\n        url = \"http://www.example.com/301\"\n        url2 = \"http://www.example.com/redirected\"\n\n        def _test_passthrough(req):\n            rsp = Response(url, headers={\"Location\": url2}, status=301, request=req)\n            r = self.mw.process_response(req, rsp, self.spider)\n            self.assertIs(r, rsp)\n\n        _test_passthrough(\n            Request(url, meta={\"handle_httpstatus_list\": [404, 301, 302]})\n        )\n        _test_passthrough(Request(url, meta={\"handle_httpstatus_all\": True}))\n\n    def test_latin1_location(self):\n        req = Request(\"http://scrapytest.org/first\")\n        latin1_location = \"/a\u00e7\u00e3o\".encode(\"latin1\")  # HTTP historically supports latin1\n        resp = Response(\n            \"http://scrapytest.org/first\",\n            headers={\"Location\": latin1_location},\n            status=302,\n        )\n        req_result = self.mw.process_response(req, resp, self.spider)\n        perc_encoded_utf8_url = \"http://scrapytest.org/a%E7%E3o\"\n        self.assertEqual(perc_encoded_utf8_url, req_result.url)\n\n    def test_utf8_location(self):\n        req = Request(\"http://scrapytest.org/first\")\n        utf8_location = \"/a\u00e7\u00e3o\".encode()  # header using UTF-8 encoding\n        resp = Response(\n            \"http://scrapytest.org/first\",\n            headers={\"Location\": utf8_location},\n            status=302,\n        )\n        req_result = self.mw.process_response(req, resp, self.spider)\n        perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"\n        self.assertEqual(perc_encoded_utf8_url, req_result.url)\n\n    def test_no_location(self):\n        request = Request(\"https://example.com\")\n        response = Response(request.url, status=302)\n        assert self.mw.process_response(request, response, self.spider) is response\n\n\nSCHEME_PARAMS = (\"url\", \"location\", \"target\")\nHTTP_SCHEMES = (\"http\", \"https\")\nNON_HTTP_SCHEMES = (\"data\", \"file\", \"ftp\", \"s3\", \"foo\")\nREDIRECT_SCHEME_CASES = (\n    # http/https \u2192 http/https redirects\n    *(\n        (\n            f\"{input_scheme}://example.com/a\",\n            f\"{output_scheme}://example.com/b\",\n            f\"{output_scheme}://example.com/b\",\n        )\n        for input_scheme, output_scheme in product(HTTP_SCHEMES, repeat=2)\n    ),\n    # http/https \u2192 data/file/ftp/s3/foo does not redirect\n    *(\n        (\n            f\"{input_scheme}://example.com/a\",\n            f\"{output_scheme}://example.com/b\",\n            None,\n        )\n        for input_scheme in HTTP_SCHEMES\n        for output_scheme in NON_HTTP_SCHEMES\n    ),\n    # http/https \u2192 relative redirects\n    *(\n        (\n            f\"{scheme}://example.com/a\",\n            location,\n            f\"{scheme}://example.com/b\",\n        )\n        for scheme in HTTP_SCHEMES\n        for location in (\"//example.com/b\", \"/b\")\n    ),\n    # Note: We do not test data/file/ftp/s3 schemes for the initial URL\n    # because their download handlers cannot return a status code of 3xx.\n)\n\n\n@pytest.mark.parametrize(SCHEME_PARAMS, REDIRECT_SCHEME_CASES)\ndef test_redirect_schemes(url, location, target):\n    crawler = get_crawler(Spider)\n    spider = crawler._create_spider(\"foo\")\n    mw = RedirectMiddleware.from_crawler(crawler)\n    request = Request(url)\n    response = Response(url, headers={\"Location\": location}, status=301)\n    redirect = mw.process_response(request, response, spider)\n    if target is None:\n        assert redirect == response\n    else:\n        assert isinstance(redirect, Request)\n        assert redirect.url == target\n\n\ndef meta_refresh_body(url, interval=5):\n    html = f\"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{interval};url={url}\"/></head></html>\"\"\"\n    return html.encode(\"utf-8\")\n\n\nclass MetaRefreshMiddlewareTest(Base.Test):\n    mwcls = MetaRefreshMiddleware\n    reason = \"meta refresh\"\n\n    def setUp(self):\n        crawler = get_crawler(Spider)\n        self.spider = crawler._create_spider(\"foo\")\n        self.mw = self.mwcls.from_crawler(crawler)\n\n    def _body(self, interval=5, url=\"http://example.org/newpage\"):\n        return meta_refresh_body(url, interval)\n\n    def get_response(self, request, location):\n        return HtmlResponse(request.url, body=self._body(url=location))\n\n    def test_meta_refresh(self):\n        req = Request(url=\"http://example.org\")\n        rsp = HtmlResponse(req.url, body=self._body())\n        req2 = self.mw.process_response(req, rsp, self.spider)\n        assert isinstance(req2, Request)\n        self.assertEqual(req2.url, \"http://example.org/newpage\")\n\n    def test_meta_refresh_with_high_interval(self):\n        # meta-refresh with high intervals don't trigger redirects\n        req = Request(url=\"http://example.org\")\n        rsp = HtmlResponse(\n            url=\"http://example.org\", body=self._body(interval=1000), encoding=\"utf-8\"\n        )\n        rsp2 = self.mw.process_response(req, rsp, self.spider)\n        assert rsp is rsp2\n\n    def test_meta_refresh_trough_posted_request(self):\n        req = Request(\n            url=\"http://example.org\",\n            method=\"POST\",\n            body=\"test\",\n            headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n        )\n        rsp = HtmlResponse(req.url, body=self._body())\n        req2 = self.mw.process_response(req, rsp, self.spider)\n\n        assert isinstance(req2, Request)\n        self.assertEqual(req2.url, \"http://example.org/newpage\")\n        self.assertEqual(req2.method, \"GET\")\n        assert (\n            \"Content-Type\" not in req2.headers\n        ), \"Content-Type header must not be present in redirected request\"\n        assert (\n            \"Content-Length\" not in req2.headers\n        ), \"Content-Length header must not be present in redirected request\"\n        assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n\n    def test_ignore_tags_default(self):\n        req = Request(url=\"http://example.org\")\n        body = (\n            \"\"\"<noscript><meta http-equiv=\"refresh\" \"\"\"\n            \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n        )\n        rsp = HtmlResponse(req.url, body=body.encode())\n        response = self.mw.process_response(req, rsp, self.spider)\n        assert isinstance(response, Response)\n\n    def test_ignore_tags_1_x_list(self):\n        \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n        settings = {\"METAREFRESH_IGNORE_TAGS\": [\"script\", \"noscript\"]}\n        crawler = get_crawler(Spider, settings)\n        mw = MetaRefreshMiddleware.from_crawler(crawler)\n        req = Request(url=\"http://example.org\")\n        body = (\n            \"\"\"<noscript><meta http-equiv=\"refresh\" \"\"\"\n            \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n        )\n        rsp = HtmlResponse(req.url, body=body.encode())\n        response = mw.process_response(req, rsp, self.spider)\n        assert isinstance(response, Response)\n\n\n@pytest.mark.parametrize(\n    SCHEME_PARAMS,\n    (\n        *REDIRECT_SCHEME_CASES,\n        # data/file/ftp/s3/foo \u2192 * does not redirect\n        *(\n            (\n                f\"{input_scheme}://example.com/a\",\n                f\"{output_scheme}://example.com/b\",\n                None,\n            )\n            for input_scheme in NON_HTTP_SCHEMES\n            for output_scheme in chain(HTTP_SCHEMES, NON_HTTP_SCHEMES)\n        ),\n        # data/file/ftp/s3/foo \u2192 relative does not redirect\n        *(\n            (\n                f\"{scheme}://example.com/a\",\n                location,\n                None,\n            )\n            for scheme in NON_HTTP_SCHEMES\n            for location in (\"//example.com/b\", \"/b\")\n        ),\n    ),\n)\ndef test_meta_refresh_schemes(url, location, target):\n    crawler = get_crawler(Spider)\n    spider = crawler._create_spider(\"foo\")\n    mw = MetaRefreshMiddleware.from_crawler(crawler)\n    request = Request(url)\n    response = HtmlResponse(url, body=meta_refresh_body(location))\n    redirect = mw.process_response(request, response, spider)\n    if target is None:\n        assert redirect == response\n    else:\n        assert isinstance(redirect, Request)\n        assert redirect.url == target\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_downloadermiddleware_offsite.py": "import pytest\n\nfrom scrapy import Request, Spider\nfrom scrapy.downloadermiddlewares.offsite import OffsiteMiddleware\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.utils.test import get_crawler\n\nUNSET = object()\n\n\n@pytest.mark.parametrize(\n    (\"allowed_domain\", \"url\", \"allowed\"),\n    (\n        (\"example.com\", \"http://example.com/1\", True),\n        (\"example.com\", \"http://example.org/1\", False),\n        (\"example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://example.com/1\", False),\n        (\"example.com\", \"http://example.com:8000/1\", True),\n        (\"example.com\", \"http://example.org/example.com\", False),\n        (\"example.com\", \"http://example.org/foo.example.com\", False),\n        (\"example.com\", \"http://example.com.example\", False),\n        (\"a.example\", \"http://nota.example\", False),\n        (\"b.a.example\", \"http://notb.a.example\", False),\n    ),\n)\ndef test_process_request_domain_filtering(allowed_domain, url, allowed):\n    crawler = get_crawler(Spider)\n    spider = crawler._create_spider(name=\"a\", allowed_domains=[allowed_domain])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    request = Request(url)\n    if allowed:\n        assert mw.process_request(request, spider) is None\n    else:\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request, spider)\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"filtered\"),\n    (\n        (UNSET, True),\n        (None, True),\n        (False, True),\n        (True, False),\n    ),\n)\ndef test_process_request_dont_filter(value, filtered):\n    crawler = get_crawler(Spider)\n    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"dont_filter\"] = value\n    request = Request(\"https://b.example\", **kwargs)\n    if filtered:\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request, spider)\n    else:\n        assert mw.process_request(request, spider) is None\n\n\n@pytest.mark.parametrize(\n    \"value\",\n    (\n        UNSET,\n        None,\n        [],\n    ),\n)\ndef test_process_request_no_allowed_domains(value):\n    crawler = get_crawler(Spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"allowed_domains\"] = value\n    spider = crawler._create_spider(name=\"a\", **kwargs)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    request = Request(\"https://example.com\")\n    assert mw.process_request(request, spider) is None\n\n\ndef test_process_request_invalid_domains():\n    crawler = get_crawler(Spider)\n    allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n    spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    request = Request(\"https://a.example\")\n    assert mw.process_request(request, spider) is None\n    for letter in (\"b\", \"c\"):\n        request = Request(f\"https://{letter}.example\")\n        with pytest.raises(IgnoreRequest):\n            mw.process_request(request, spider)\n\n\n@pytest.mark.parametrize(\n    (\"allowed_domain\", \"url\", \"allowed\"),\n    (\n        (\"example.com\", \"http://example.com/1\", True),\n        (\"example.com\", \"http://example.org/1\", False),\n        (\"example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://sub.example.com/1\", True),\n        (\"sub.example.com\", \"http://example.com/1\", False),\n        (\"example.com\", \"http://example.com:8000/1\", True),\n        (\"example.com\", \"http://example.org/example.com\", False),\n        (\"example.com\", \"http://example.org/foo.example.com\", False),\n        (\"example.com\", \"http://example.com.example\", False),\n        (\"a.example\", \"http://nota.example\", False),\n        (\"b.a.example\", \"http://notb.a.example\", False),\n    ),\n)\ndef test_request_scheduled_domain_filtering(allowed_domain, url, allowed):\n    crawler = get_crawler(Spider)\n    spider = crawler._create_spider(name=\"a\", allowed_domains=[allowed_domain])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    request = Request(url)\n    if allowed:\n        assert mw.request_scheduled(request, spider) is None\n    else:\n        with pytest.raises(IgnoreRequest):\n            mw.request_scheduled(request, spider)\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"filtered\"),\n    (\n        (UNSET, True),\n        (None, True),\n        (False, True),\n        (True, False),\n    ),\n)\ndef test_request_scheduled_dont_filter(value, filtered):\n    crawler = get_crawler(Spider)\n    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"dont_filter\"] = value\n    request = Request(\"https://b.example\", **kwargs)\n    if filtered:\n        with pytest.raises(IgnoreRequest):\n            mw.request_scheduled(request, spider)\n    else:\n        assert mw.request_scheduled(request, spider) is None\n\n\n@pytest.mark.parametrize(\n    \"value\",\n    (\n        UNSET,\n        None,\n        [],\n    ),\n)\ndef test_request_scheduled_no_allowed_domains(value):\n    crawler = get_crawler(Spider)\n    kwargs = {}\n    if value is not UNSET:\n        kwargs[\"allowed_domains\"] = value\n    spider = crawler._create_spider(name=\"a\", **kwargs)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    request = Request(\"https://example.com\")\n    assert mw.request_scheduled(request, spider) is None\n\n\ndef test_request_scheduled_invalid_domains():\n    crawler = get_crawler(Spider)\n    allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n    spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n    mw = OffsiteMiddleware.from_crawler(crawler)\n    mw.spider_opened(spider)\n    request = Request(\"https://a.example\")\n    assert mw.request_scheduled(request, spider) is None\n    for letter in (\"b\", \"c\"):\n        request = Request(f\"https://{letter}.example\")\n        with pytest.raises(IgnoreRequest):\n            mw.request_scheduled(request, spider)\n", "tests/test_mail.py": "import unittest\nfrom email.charset import Charset\nfrom io import BytesIO\n\nfrom twisted import version as twisted_version\nfrom twisted.internet import defer\nfrom twisted.internet._sslverify import ClientTLSOptions\nfrom twisted.internet.ssl import ClientContextFactory\nfrom twisted.python.versions import Version\n\nfrom scrapy.mail import MailSender\n\n\nclass MailSenderTest(unittest.TestCase):\n    def test_send(self):\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=\"subject\",\n            body=\"body\",\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n\n        self.assertEqual(self.catched_msg[\"to\"], [\"test@scrapy.org\"])\n        self.assertEqual(self.catched_msg[\"subject\"], \"subject\")\n        self.assertEqual(self.catched_msg[\"body\"], \"body\")\n\n        msg = self.catched_msg[\"msg\"]\n        self.assertEqual(msg[\"to\"], \"test@scrapy.org\")\n        self.assertEqual(msg[\"subject\"], \"subject\")\n        self.assertEqual(msg.get_payload(), \"body\")\n        self.assertEqual(msg.get(\"Content-Type\"), \"text/plain\")\n\n    def test_send_single_values_to_and_cc(self):\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=\"test@scrapy.org\",\n            subject=\"subject\",\n            body=\"body\",\n            cc=\"test@scrapy.org\",\n            _callback=self._catch_mail_sent,\n        )\n\n    def test_send_html(self):\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=\"subject\",\n            body=\"<p>body</p>\",\n            mimetype=\"text/html\",\n            _callback=self._catch_mail_sent,\n        )\n\n        msg = self.catched_msg[\"msg\"]\n        self.assertEqual(msg.get_payload(), \"<p>body</p>\")\n        self.assertEqual(msg.get(\"Content-Type\"), \"text/html\")\n\n    def test_send_attach(self):\n        attach = BytesIO()\n        attach.write(b\"content\")\n        attach.seek(0)\n        attachs = [(\"attachment\", \"text/plain\", attach)]\n\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=\"subject\",\n            body=\"body\",\n            attachs=attachs,\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n        self.assertEqual(self.catched_msg[\"to\"], [\"test@scrapy.org\"])\n        self.assertEqual(self.catched_msg[\"subject\"], \"subject\")\n        self.assertEqual(self.catched_msg[\"body\"], \"body\")\n\n        msg = self.catched_msg[\"msg\"]\n        self.assertEqual(msg[\"to\"], \"test@scrapy.org\")\n        self.assertEqual(msg[\"subject\"], \"subject\")\n\n        payload = msg.get_payload()\n        assert isinstance(payload, list)\n        self.assertEqual(len(payload), 2)\n\n        text, attach = payload\n        self.assertEqual(text.get_payload(decode=True), b\"body\")\n        self.assertEqual(text.get_charset(), Charset(\"us-ascii\"))\n        self.assertEqual(attach.get_payload(decode=True), b\"content\")\n\n    def _catch_mail_sent(self, **kwargs):\n        self.catched_msg = {**kwargs}\n\n    def test_send_utf8(self):\n        subject = \"s\u00fcbj\u00e8\u00e7t\"\n        body = \"b\u00f6d\u00ff-\u00e0\u00e9\u00ef\u00f6\u00f1\u00df\"\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=subject,\n            body=body,\n            charset=\"utf-8\",\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n        self.assertEqual(self.catched_msg[\"subject\"], subject)\n        self.assertEqual(self.catched_msg[\"body\"], body)\n\n        msg = self.catched_msg[\"msg\"]\n        self.assertEqual(msg[\"subject\"], subject)\n        self.assertEqual(msg.get_payload(decode=True).decode(\"utf-8\"), body)\n        self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n        self.assertEqual(msg.get(\"Content-Type\"), 'text/plain; charset=\"utf-8\"')\n\n    def test_send_attach_utf8(self):\n        subject = \"s\u00fcbj\u00e8\u00e7t\"\n        body = \"b\u00f6d\u00ff-\u00e0\u00e9\u00ef\u00f6\u00f1\u00df\"\n        attach = BytesIO()\n        attach.write(body.encode(\"utf-8\"))\n        attach.seek(0)\n        attachs = [(\"attachment\", \"text/plain\", attach)]\n\n        mailsender = MailSender(debug=True)\n        mailsender.send(\n            to=[\"test@scrapy.org\"],\n            subject=subject,\n            body=body,\n            attachs=attachs,\n            charset=\"utf-8\",\n            _callback=self._catch_mail_sent,\n        )\n\n        assert self.catched_msg\n        self.assertEqual(self.catched_msg[\"subject\"], subject)\n        self.assertEqual(self.catched_msg[\"body\"], body)\n\n        msg = self.catched_msg[\"msg\"]\n        self.assertEqual(msg[\"subject\"], subject)\n        self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n        self.assertEqual(msg.get(\"Content-Type\"), 'multipart/mixed; charset=\"utf-8\"')\n\n        payload = msg.get_payload()\n        assert isinstance(payload, list)\n        self.assertEqual(len(payload), 2)\n\n        text, attach = payload\n        self.assertEqual(text.get_payload(decode=True).decode(\"utf-8\"), body)\n        self.assertEqual(text.get_charset(), Charset(\"utf-8\"))\n        self.assertEqual(attach.get_payload(decode=True).decode(\"utf-8\"), body)\n\n    def test_create_sender_factory_with_host(self):\n        mailsender = MailSender(debug=False, smtphost=\"smtp.testhost.com\")\n\n        factory = mailsender._create_sender_factory(\n            to_addrs=[\"test@scrapy.org\"], msg=\"test\", d=defer.Deferred()\n        )\n\n        context = factory.buildProtocol(\"test@scrapy.org\").context\n        if twisted_version >= Version(\"twisted\", 21, 2, 0):\n            self.assertIsInstance(context, ClientTLSOptions)\n        else:\n            self.assertIsInstance(context, ClientContextFactory)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_response.py": "import unittest\nfrom pathlib import Path\nfrom time import process_time\nfrom urllib.parse import urlparse\n\nimport pytest\n\nfrom scrapy.http import HtmlResponse, Response, TextResponse\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.response import (\n    _remove_html_comments,\n    get_base_url,\n    get_meta_refresh,\n    open_in_browser,\n    response_status_message,\n)\n\n__doctests__ = [\"scrapy.utils.response\"]\n\n\nclass ResponseUtilsTest(unittest.TestCase):\n    dummy_response = TextResponse(url=\"http://example.org/\", body=b\"dummy_response\")\n\n    def test_open_in_browser(self):\n        url = \"http:///www.example.com/some/page.html\"\n        body = b\"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n\n        def browser_open(burl):\n            path = urlparse(burl).path\n            if not path or not Path(path).exists():\n                path = burl.replace(\"file://\", \"\")\n            bbody = Path(path).read_bytes()\n            self.assertIn(b'<base href=\"' + to_bytes(url) + b'\">', bbody)\n            return True\n\n        response = HtmlResponse(url, body=body)\n        assert open_in_browser(response, _openfunc=browser_open), \"Browser not called\"\n\n        resp = Response(url, body=body)\n        self.assertRaises(TypeError, open_in_browser, resp, debug=True)\n\n    def test_get_meta_refresh(self):\n        r1 = HtmlResponse(\n            \"http://www.example.com\",\n            body=b\"\"\"\n        <html>\n        <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n        <body>blahablsdfsal&amp;</body>\n        </html>\"\"\",\n        )\n        r2 = HtmlResponse(\n            \"http://www.example.com\",\n            body=b\"\"\"\n        <html>\n        <head><title>Dummy</title><noScript>\n        <meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n        </noSCRIPT>\n        <body>blahablsdfsal&amp;</body>\n        </html>\"\"\",\n        )\n        r3 = HtmlResponse(\n            \"http://www.example.com\",\n            body=b\"\"\"\n    <noscript><meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage</noscript>\n    <script type=\"text/javascript\">\n    if(!checkCookies()){\n        document.write('<meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage\">');\n    }\n    </script>\n        \"\"\",\n        )\n        self.assertEqual(get_meta_refresh(r1), (5.0, \"http://example.org/newpage\"))\n        self.assertEqual(get_meta_refresh(r2), (None, None))\n        self.assertEqual(get_meta_refresh(r3), (None, None))\n\n    def test_get_base_url(self):\n        resp = HtmlResponse(\n            \"http://www.example.com\",\n            body=b\"\"\"\n        <html>\n        <head><base href=\"http://www.example.com/img/\" target=\"_blank\"></head>\n        <body>blahablsdfsal&amp;</body>\n        </html>\"\"\",\n        )\n        self.assertEqual(get_base_url(resp), \"http://www.example.com/img/\")\n\n        resp2 = HtmlResponse(\n            \"http://www.example.com\",\n            body=b\"\"\"\n        <html><body>blahablsdfsal&amp;</body></html>\"\"\",\n        )\n        self.assertEqual(get_base_url(resp2), \"http://www.example.com\")\n\n    def test_response_status_message(self):\n        self.assertEqual(response_status_message(200), \"200 OK\")\n        self.assertEqual(response_status_message(404), \"404 Not Found\")\n        self.assertEqual(response_status_message(573), \"573 Unknown Status\")\n\n    def test_inject_base_url(self):\n        url = \"http://www.example.com\"\n\n        def check_base_url(burl):\n            path = urlparse(burl).path\n            if not path or not Path(path).exists():\n                path = burl.replace(\"file://\", \"\")\n            bbody = Path(path).read_bytes()\n            self.assertEqual(bbody.count(b'<base href=\"' + to_bytes(url) + b'\">'), 1)\n            return True\n\n        r1 = HtmlResponse(\n            url,\n            body=b\"\"\"\n        <html>\n            <head><title>Dummy</title></head>\n            <body><p>Hello world.</p></body>\n        </html>\"\"\",\n        )\n        r2 = HtmlResponse(\n            url,\n            body=b\"\"\"\n        <html>\n            <head id=\"foo\"><title>Dummy</title></head>\n            <body>Hello world.</body>\n        </html>\"\"\",\n        )\n        r3 = HtmlResponse(\n            url,\n            body=b\"\"\"\n        <html>\n            <head><title>Dummy</title></head>\n            <body>\n                <header>Hello header</header>\n                <p>Hello world.</p>\n            </body>\n        </html>\"\"\",\n        )\n        r4 = HtmlResponse(\n            url,\n            body=b\"\"\"\n        <html>\n            <!-- <head>Dummy comment</head> -->\n            <head><title>Dummy</title></head>\n            <body><p>Hello world.</p></body>\n        </html>\"\"\",\n        )\n        r5 = HtmlResponse(\n            url,\n            body=b\"\"\"\n        <html>\n            <!--[if IE]>\n            <head><title>IE head</title></head>\n            <![endif]-->\n            <!--[if !IE]>-->\n            <head><title>Standard head</title></head>\n            <!--<![endif]-->\n            <body><p>Hello world.</p></body>\n        </html>\"\"\",\n        )\n\n        assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n        assert open_in_browser(\n            r2, _openfunc=check_base_url\n        ), \"Inject base url with argumented head\"\n        assert open_in_browser(\n            r3, _openfunc=check_base_url\n        ), \"Inject unique base url with misleading tag\"\n        assert open_in_browser(\n            r4, _openfunc=check_base_url\n        ), \"Inject unique base url with misleading comment\"\n        assert open_in_browser(\n            r5, _openfunc=check_base_url\n        ), \"Inject unique base url with conditional comment\"\n\n    def test_open_in_browser_redos_comment(self):\n        MAX_CPU_TIME = 0.02\n\n        # Exploit input from\n        # https://makenowjust-labs.github.io/recheck/playground/\n        # for /<!--.*?-->/ (old pattern to remove comments).\n        body = b\"-><!--\\x00\" * 25_000 + b\"->\\n<!---->\"\n\n        response = HtmlResponse(\"https://example.com\", body=body)\n\n        start_time = process_time()\n\n        open_in_browser(response, lambda url: True)\n\n        end_time = process_time()\n        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n\n    def test_open_in_browser_redos_head(self):\n        MAX_CPU_TIME = 0.02\n\n        # Exploit input from\n        # https://makenowjust-labs.github.io/recheck/playground/\n        # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).\n        body = b\"<head\\t\" * 8_000\n\n        response = HtmlResponse(\"https://example.com\", body=body)\n\n        start_time = process_time()\n\n        open_in_browser(response, lambda url: True)\n\n        end_time = process_time()\n        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n\n\n@pytest.mark.parametrize(\n    \"input_body,output_body\",\n    (\n        (\n            b\"a<!--\",\n            b\"a\",\n        ),\n        (\n            b\"a<!---->b\",\n            b\"ab\",\n        ),\n        (\n            b\"a<!--b-->c\",\n            b\"ac\",\n        ),\n        (\n            b\"a<!--b-->c<!--\",\n            b\"ac\",\n        ),\n        (\n            b\"a<!--b-->c<!--d\",\n            b\"ac\",\n        ),\n        (\n            b\"a<!--b-->c<!---->d\",\n            b\"acd\",\n        ),\n        (\n            b\"a<!--b--><!--c-->d\",\n            b\"ad\",\n        ),\n    ),\n)\ndef test_remove_html_comments(input_body, output_body):\n    assert (\n        _remove_html_comments(input_body) == output_body\n    ), f\"{_remove_html_comments(input_body)=} == {output_body=}\"\n", "tests/test_downloadermiddleware_retry.py": "import logging\nimport unittest\nimport warnings\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.internet.error import (\n    ConnectError,\n    ConnectionDone,\n    ConnectionLost,\n    DNSLookupError,\n    TCPTimedOutError,\n)\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import Request, Response\nfrom scrapy.settings.default_settings import RETRY_EXCEPTIONS\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass RetryTest(unittest.TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.mw = RetryMiddleware.from_crawler(self.crawler)\n        self.mw.max_retry_times = 2\n\n    def test_priority_adjust(self):\n        req = Request(\"http://www.scrapytest.org/503\")\n        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n        req2 = self.mw.process_response(req, rsp, self.spider)\n        assert req2.priority < req.priority\n\n    def test_404(self):\n        req = Request(\"http://www.scrapytest.org/404\")\n        rsp = Response(\"http://www.scrapytest.org/404\", body=b\"\", status=404)\n\n        # dont retry 404s\n        assert self.mw.process_response(req, rsp, self.spider) is rsp\n\n    def test_dont_retry(self):\n        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": True})\n        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n\n        # first retry\n        r = self.mw.process_response(req, rsp, self.spider)\n        assert r is rsp\n\n        # Test retry when dont_retry set to False\n        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": False})\n        rsp = Response(\"http://www.scrapytest.org/503\")\n\n        # first retry\n        r = self.mw.process_response(req, rsp, self.spider)\n        assert r is rsp\n\n    def test_dont_retry_exc(self):\n        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": True})\n\n        r = self.mw.process_exception(req, DNSLookupError(), self.spider)\n        assert r is None\n\n    def test_503(self):\n        req = Request(\"http://www.scrapytest.org/503\")\n        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n\n        # first retry\n        req = self.mw.process_response(req, rsp, self.spider)\n        assert isinstance(req, Request)\n        self.assertEqual(req.meta[\"retry_times\"], 1)\n\n        # second retry\n        req = self.mw.process_response(req, rsp, self.spider)\n        assert isinstance(req, Request)\n        self.assertEqual(req.meta[\"retry_times\"], 2)\n\n        # discard it\n        assert self.mw.process_response(req, rsp, self.spider) is rsp\n\n        assert self.crawler.stats.get_value(\"retry/max_reached\") == 1\n        assert (\n            self.crawler.stats.get_value(\"retry/reason_count/503 Service Unavailable\")\n            == 2\n        )\n        assert self.crawler.stats.get_value(\"retry/count\") == 2\n\n    def test_twistederrors(self):\n        exceptions = [\n            ConnectError,\n            ConnectionDone,\n            ConnectionLost,\n            ConnectionRefusedError,\n            defer.TimeoutError,\n            DNSLookupError,\n            ResponseFailed,\n            TCPTimedOutError,\n            TimeoutError,\n        ]\n\n        for exc in exceptions:\n            req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n            self._test_retry_exception(req, exc(\"foo\"))\n\n        stats = self.crawler.stats\n        assert stats.get_value(\"retry/max_reached\") == len(exceptions)\n        assert stats.get_value(\"retry/count\") == len(exceptions) * 2\n        assert (\n            stats.get_value(\"retry/reason_count/twisted.internet.defer.TimeoutError\")\n            == 2\n        )\n\n    def test_exception_to_retry_added(self):\n        exc = ValueError\n        settings_dict = {\n            \"RETRY_EXCEPTIONS\": list(RETRY_EXCEPTIONS) + [exc],\n        }\n        crawler = get_crawler(Spider, settings_dict=settings_dict)\n        mw = RetryMiddleware.from_crawler(crawler)\n        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n        self._test_retry_exception(req, exc(\"foo\"), mw)\n\n    def test_exception_to_retry_custom_middleware(self):\n        exc = ValueError\n\n        with warnings.catch_warnings(record=True) as warns:\n\n            class MyRetryMiddleware(RetryMiddleware):\n                EXCEPTIONS_TO_RETRY = RetryMiddleware.EXCEPTIONS_TO_RETRY + (exc,)\n\n            self.assertEqual(len(warns), 1)\n\n        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n        req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n        assert isinstance(req, Request)\n        self.assertEqual(req.meta[\"retry_times\"], 1)\n\n    def test_exception_to_retry_custom_middleware_self(self):\n        class MyRetryMiddleware(RetryMiddleware):\n            def process_exception(self, request, exception, spider):\n                if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n                    return self._retry(request, exception, spider)\n\n        exc = OSError\n        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n        with warnings.catch_warnings(record=True) as warns:\n            req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n        assert isinstance(req, Request)\n        self.assertEqual(req.meta[\"retry_times\"], 1)\n        self.assertEqual(len(warns), 1)\n\n    def _test_retry_exception(self, req, exception, mw=None):\n        if mw is None:\n            mw = self.mw\n\n        # first retry\n        req = mw.process_exception(req, exception, self.spider)\n        assert isinstance(req, Request)\n        self.assertEqual(req.meta[\"retry_times\"], 1)\n\n        # second retry\n        req = mw.process_exception(req, exception, self.spider)\n        assert isinstance(req, Request)\n        self.assertEqual(req.meta[\"retry_times\"], 2)\n\n        # discard it\n        req = mw.process_exception(req, exception, self.spider)\n        self.assertEqual(req, None)\n\n\nclass MaxRetryTimesTest(unittest.TestCase):\n    invalid_url = \"http://www.scrapytest.org/invalid_url\"\n\n    def get_spider_and_middleware(self, settings=None):\n        crawler = get_crawler(Spider, settings or {})\n        spider = crawler._create_spider(\"foo\")\n        middleware = RetryMiddleware.from_crawler(crawler)\n        return spider, middleware\n\n    def test_with_settings_zero(self):\n        max_retry_times = 0\n        settings = {\"RETRY_TIMES\": max_retry_times}\n        spider, middleware = self.get_spider_and_middleware(settings)\n        req = Request(self.invalid_url)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n\n    def test_with_metakey_zero(self):\n        max_retry_times = 0\n        spider, middleware = self.get_spider_and_middleware()\n        meta = {\"max_retry_times\": max_retry_times}\n        req = Request(self.invalid_url, meta=meta)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n\n    def test_without_metakey(self):\n        max_retry_times = 5\n        settings = {\"RETRY_TIMES\": max_retry_times}\n        spider, middleware = self.get_spider_and_middleware(settings)\n        req = Request(self.invalid_url)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n\n    def test_with_metakey_greater(self):\n        meta_max_retry_times = 3\n        middleware_max_retry_times = 2\n\n        req1 = Request(self.invalid_url, meta={\"max_retry_times\": meta_max_retry_times})\n        req2 = Request(self.invalid_url)\n\n        settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n        spider, middleware = self.get_spider_and_middleware(settings)\n\n        self._test_retry(\n            req1,\n            DNSLookupError(\"foo\"),\n            meta_max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n        self._test_retry(\n            req2,\n            DNSLookupError(\"foo\"),\n            middleware_max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n\n    def test_with_metakey_lesser(self):\n        meta_max_retry_times = 4\n        middleware_max_retry_times = 5\n\n        req1 = Request(self.invalid_url, meta={\"max_retry_times\": meta_max_retry_times})\n        req2 = Request(self.invalid_url)\n\n        settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n        spider, middleware = self.get_spider_and_middleware(settings)\n\n        self._test_retry(\n            req1,\n            DNSLookupError(\"foo\"),\n            meta_max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n        self._test_retry(\n            req2,\n            DNSLookupError(\"foo\"),\n            middleware_max_retry_times,\n            spider=spider,\n            middleware=middleware,\n        )\n\n    def test_with_dont_retry(self):\n        max_retry_times = 4\n        spider, middleware = self.get_spider_and_middleware()\n        meta = {\n            \"max_retry_times\": max_retry_times,\n            \"dont_retry\": True,\n        }\n        req = Request(self.invalid_url, meta=meta)\n        self._test_retry(\n            req,\n            DNSLookupError(\"foo\"),\n            0,\n            spider=spider,\n            middleware=middleware,\n        )\n\n    def _test_retry(\n        self,\n        req,\n        exception,\n        max_retry_times,\n        spider=None,\n        middleware=None,\n    ):\n        spider = spider or self.spider\n        middleware = middleware or self.mw\n\n        for i in range(0, max_retry_times):\n            req = middleware.process_exception(req, exception, spider)\n            assert isinstance(req, Request)\n\n        # discard it\n        req = middleware.process_exception(req, exception, spider)\n        self.assertEqual(req, None)\n\n\nclass GetRetryRequestTest(unittest.TestCase):\n    def get_spider(self, settings=None):\n        crawler = get_crawler(Spider, settings or {})\n        return crawler._create_spider(\"foo\")\n\n    def test_basic_usage(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                request,\n                spider=spider,\n            )\n        self.assertIsInstance(new_request, Request)\n        self.assertNotEqual(new_request, request)\n        self.assertEqual(new_request.dont_filter, True)\n        expected_retry_times = 1\n        self.assertEqual(new_request.meta[\"retry_times\"], expected_retry_times)\n        self.assertEqual(new_request.priority, -1)\n        expected_reason = \"unspecified\"\n        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_max_retries_reached(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        max_retry_times = 0\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                request,\n                spider=spider,\n                max_retry_times=max_retry_times,\n            )\n        self.assertEqual(new_request, None)\n        self.assertEqual(spider.crawler.stats.get_value(\"retry/max_reached\"), 1)\n        failure_count = max_retry_times + 1\n        expected_reason = \"unspecified\"\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"ERROR\",\n                f\"Gave up retrying {request} (failed {failure_count} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_one_retry(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                request,\n                spider=spider,\n                max_retry_times=1,\n            )\n        self.assertIsInstance(new_request, Request)\n        self.assertNotEqual(new_request, request)\n        self.assertEqual(new_request.dont_filter, True)\n        expected_retry_times = 1\n        self.assertEqual(new_request.meta[\"retry_times\"], expected_retry_times)\n        self.assertEqual(new_request.priority, -1)\n        expected_reason = \"unspecified\"\n        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_two_retries(self):\n        spider = self.get_spider()\n        request = Request(\"https://example.com\")\n        new_request = request\n        max_retry_times = 2\n        for index in range(max_retry_times):\n            with LogCapture() as log:\n                new_request = get_retry_request(\n                    new_request,\n                    spider=spider,\n                    max_retry_times=max_retry_times,\n                )\n            self.assertIsInstance(new_request, Request)\n            self.assertNotEqual(new_request, request)\n            self.assertEqual(new_request.dont_filter, True)\n            expected_retry_times = index + 1\n            self.assertEqual(new_request.meta[\"retry_times\"], expected_retry_times)\n            self.assertEqual(new_request.priority, -expected_retry_times)\n            expected_reason = \"unspecified\"\n            for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n                value = spider.crawler.stats.get_value(stat)\n                self.assertEqual(value, expected_retry_times)\n            log.check_present(\n                (\n                    \"scrapy.downloadermiddlewares.retry\",\n                    \"DEBUG\",\n                    f\"Retrying {request} (failed {expected_retry_times} times): \"\n                    f\"{expected_reason}\",\n                )\n            )\n\n        with LogCapture() as log:\n            new_request = get_retry_request(\n                new_request,\n                spider=spider,\n                max_retry_times=max_retry_times,\n            )\n        self.assertEqual(new_request, None)\n        self.assertEqual(spider.crawler.stats.get_value(\"retry/max_reached\"), 1)\n        failure_count = max_retry_times + 1\n        expected_reason = \"unspecified\"\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"ERROR\",\n                f\"Gave up retrying {request} (failed {failure_count} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_no_spider(self):\n        request = Request(\"https://example.com\")\n        with self.assertRaises(TypeError):\n            get_retry_request(request)  # pylint: disable=missing-kwoa\n\n    def test_max_retry_times_setting(self):\n        max_retry_times = 0\n        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times})\n        request = Request(\"https://example.com\")\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n        )\n        self.assertEqual(new_request, None)\n\n    def test_max_retry_times_meta(self):\n        max_retry_times = 0\n        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n        meta = {\"max_retry_times\": max_retry_times}\n        request = Request(\"https://example.com\", meta=meta)\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n        )\n        self.assertEqual(new_request, None)\n\n    def test_max_retry_times_argument(self):\n        max_retry_times = 0\n        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n        meta = {\"max_retry_times\": max_retry_times + 1}\n        request = Request(\"https://example.com\", meta=meta)\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n            max_retry_times=max_retry_times,\n        )\n        self.assertEqual(new_request, None)\n\n    def test_priority_adjust_setting(self):\n        priority_adjust = 1\n        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust})\n        request = Request(\"https://example.com\")\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n        )\n        self.assertEqual(new_request.priority, priority_adjust)\n\n    def test_priority_adjust_argument(self):\n        priority_adjust = 1\n        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust + 1})\n        request = Request(\"https://example.com\")\n        new_request = get_retry_request(\n            request,\n            spider=spider,\n            priority_adjust=priority_adjust,\n        )\n        self.assertEqual(new_request.priority, priority_adjust)\n\n    def test_log_extra_retry_success(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture(attributes=(\"spider\",)) as log:\n            get_retry_request(\n                request,\n                spider=spider,\n            )\n        log.check_present(spider)\n\n    def test_log_extra_retries_exceeded(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        with LogCapture(attributes=(\"spider\",)) as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                max_retry_times=0,\n            )\n        log.check_present(spider)\n\n    def test_reason_string(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = \"because\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_builtin_exception(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = NotImplementedError()\n        expected_reason_string = \"builtins.NotImplementedError\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        self.assertEqual(stat, 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_builtin_exception_class(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = NotImplementedError\n        expected_reason_string = \"builtins.NotImplementedError\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        self.assertEqual(stat, 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_custom_exception(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = IgnoreRequest()\n        expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        self.assertEqual(stat, 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_reason_custom_exception_class(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = IgnoreRequest\n        expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n            )\n        expected_retry_times = 1\n        stat = spider.crawler.stats.get_value(\n            f\"retry/reason_count/{expected_reason_string}\"\n        )\n        self.assertEqual(stat, 1)\n        log.check_present(\n            (\n                \"scrapy.downloadermiddlewares.retry\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed {expected_retry_times} times): \"\n                f\"{expected_reason}\",\n            )\n        )\n\n    def test_custom_logger(self):\n        logger = logging.getLogger(\"custom-logger\")\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = \"because\"\n        with LogCapture() as log:\n            get_retry_request(\n                request,\n                spider=spider,\n                reason=expected_reason,\n                logger=logger,\n            )\n        log.check_present(\n            (\n                \"custom-logger\",\n                \"DEBUG\",\n                f\"Retrying {request} (failed 1 times): {expected_reason}\",\n            )\n        )\n\n    def test_custom_stats_key(self):\n        request = Request(\"https://example.com\")\n        spider = self.get_spider()\n        expected_reason = \"because\"\n        stats_key = \"custom_retry\"\n        get_retry_request(\n            request,\n            spider=spider,\n            reason=expected_reason,\n            stats_base_key=stats_key,\n        )\n        for stat in (\n            f\"{stats_key}/count\",\n            f\"{stats_key}/reason_count/{expected_reason}\",\n        ):\n            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_request_dict.py": "import unittest\n\nfrom scrapy import Request, Spider\nfrom scrapy.http import FormRequest, JsonRequest\nfrom scrapy.utils.request import request_from_dict\n\n\nclass CustomRequest(Request):\n    pass\n\n\nclass RequestSerializationTest(unittest.TestCase):\n    def setUp(self):\n        self.spider = TestSpider()\n\n    def test_basic(self):\n        r = Request(\"http://www.example.com\")\n        self._assert_serializes_ok(r)\n\n    def test_all_attributes(self):\n        r = Request(\n            url=\"http://www.example.com\",\n            callback=self.spider.parse_item,\n            errback=self.spider.handle_error,\n            method=\"POST\",\n            body=b\"some body\",\n            headers={\"content-encoding\": \"text/html; charset=latin-1\"},\n            cookies={\"currency\": \"\u0440\u0443\u0431\"},\n            encoding=\"latin-1\",\n            priority=20,\n            meta={\"a\": \"b\"},\n            cb_kwargs={\"k\": \"v\"},\n            flags=[\"testFlag\"],\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_latin1_body(self):\n        r = Request(\"http://www.example.com\", body=b\"\\xa3\")\n        self._assert_serializes_ok(r)\n\n    def test_utf8_body(self):\n        r = Request(\"http://www.example.com\", body=b\"\\xc2\\xa3\")\n        self._assert_serializes_ok(r)\n\n    def _assert_serializes_ok(self, request, spider=None):\n        d = request.to_dict(spider=spider)\n        request2 = request_from_dict(d, spider=spider)\n        self._assert_same_request(request, request2)\n\n    def _assert_same_request(self, r1, r2):\n        self.assertEqual(r1.__class__, r2.__class__)\n        self.assertEqual(r1.url, r2.url)\n        self.assertEqual(r1.callback, r2.callback)\n        self.assertEqual(r1.errback, r2.errback)\n        self.assertEqual(r1.method, r2.method)\n        self.assertEqual(r1.body, r2.body)\n        self.assertEqual(r1.headers, r2.headers)\n        self.assertEqual(r1.cookies, r2.cookies)\n        self.assertEqual(r1.meta, r2.meta)\n        self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n        self.assertEqual(r1.encoding, r2.encoding)\n        self.assertEqual(r1._encoding, r2._encoding)\n        self.assertEqual(r1.priority, r2.priority)\n        self.assertEqual(r1.dont_filter, r2.dont_filter)\n        self.assertEqual(r1.flags, r2.flags)\n        if isinstance(r1, JsonRequest):\n            self.assertEqual(r1.dumps_kwargs, r2.dumps_kwargs)\n\n    def test_request_class(self):\n        r1 = FormRequest(\"http://www.example.com\")\n        self._assert_serializes_ok(r1, spider=self.spider)\n        r2 = CustomRequest(\"http://www.example.com\")\n        self._assert_serializes_ok(r2, spider=self.spider)\n        r3 = JsonRequest(\"http://www.example.com\", dumps_kwargs={\"indent\": 4})\n        self._assert_serializes_ok(r3, spider=self.spider)\n\n    def test_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider.parse_item,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_reference_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider.parse_item_reference,\n            errback=self.spider.handle_error_reference,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n        request_dict = r.to_dict(spider=self.spider)\n        self.assertEqual(request_dict[\"callback\"], \"parse_item_reference\")\n        self.assertEqual(request_dict[\"errback\"], \"handle_error_reference\")\n\n    def test_private_reference_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider._TestSpider__parse_item_reference,\n            errback=self.spider._TestSpider__handle_error_reference,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n        request_dict = r.to_dict(spider=self.spider)\n        self.assertEqual(request_dict[\"callback\"], \"_TestSpider__parse_item_reference\")\n        self.assertEqual(request_dict[\"errback\"], \"_TestSpider__handle_error_reference\")\n\n    def test_private_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider._TestSpider__parse_item_private,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_mixin_private_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider._TestSpiderMixin__mixin_callback,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_delegated_callback_serialization(self):\n        r = Request(\n            \"http://www.example.com\",\n            callback=self.spider.delegated_callback,\n            errback=self.spider.handle_error,\n        )\n        self._assert_serializes_ok(r, spider=self.spider)\n\n    def test_unserializable_callback1(self):\n        r = Request(\"http://www.example.com\", callback=lambda x: x)\n        self.assertRaises(ValueError, r.to_dict, spider=self.spider)\n\n    def test_unserializable_callback2(self):\n        r = Request(\"http://www.example.com\", callback=self.spider.parse_item)\n        self.assertRaises(ValueError, r.to_dict, spider=None)\n\n    def test_unserializable_callback3(self):\n        \"\"\"Parser method is removed or replaced dynamically.\"\"\"\n\n        class MySpider(Spider):\n            name = \"my_spider\"\n\n            def parse(self, response):\n                pass\n\n        spider = MySpider()\n        r = Request(\"http://www.example.com\", callback=spider.parse)\n        spider.parse = None\n        self.assertRaises(ValueError, r.to_dict, spider=spider)\n\n    def test_callback_not_available(self):\n        \"\"\"Callback method is not available in the spider passed to from_dict\"\"\"\n        spider = TestSpiderDelegation()\n        r = Request(\"http://www.example.com\", callback=spider.delegated_callback)\n        d = r.to_dict(spider=spider)\n        self.assertRaises(ValueError, request_from_dict, d, spider=Spider(\"foo\"))\n\n\nclass TestSpiderMixin:\n    def __mixin_callback(self, response):\n        pass\n\n\nclass TestSpiderDelegation:\n    def delegated_callback(self, response):\n        pass\n\n\ndef parse_item(response):\n    pass\n\n\ndef handle_error(failure):\n    pass\n\n\ndef private_parse_item(response):\n    pass\n\n\ndef private_handle_error(failure):\n    pass\n\n\nclass TestSpider(Spider, TestSpiderMixin):\n    name = \"test\"\n    parse_item_reference = parse_item\n    handle_error_reference = handle_error\n    __parse_item_reference = private_parse_item\n    __handle_error_reference = private_handle_error\n\n    def __init__(self):\n        self.delegated_callback = TestSpiderDelegation().delegated_callback\n\n    def parse_item(self, response):\n        pass\n\n    def handle_error(self, failure):\n        pass\n\n    def __parse_item_private(self, response):\n        pass\n", "tests/test_extension_throttle.py": "from logging import INFO\nfrom unittest.mock import Mock\n\nimport pytest\n\nfrom scrapy import Request, Spider\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions.throttle import AutoThrottle\nfrom scrapy.http.response import Response\nfrom scrapy.settings.default_settings import (\n    AUTOTHROTTLE_MAX_DELAY,\n    AUTOTHROTTLE_START_DELAY,\n    DOWNLOAD_DELAY,\n)\nfrom scrapy.utils.misc import build_from_crawler\nfrom scrapy.utils.test import get_crawler as _get_crawler\n\nUNSET = object()\n\n\nclass TestSpider(Spider):\n    name = \"test\"\n\n\ndef get_crawler(settings=None, spidercls=None):\n    settings = settings or {}\n    settings[\"AUTOTHROTTLE_ENABLED\"] = True\n    return _get_crawler(settings_dict=settings, spidercls=spidercls)\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    (\n        (UNSET, False),\n        (False, False),\n        (True, True),\n    ),\n)\ndef test_enabled(value, expected):\n    settings = {}\n    if value is not UNSET:\n        settings[\"AUTOTHROTTLE_ENABLED\"] = value\n    crawler = _get_crawler(settings_dict=settings)\n    if expected:\n        build_from_crawler(AutoThrottle, crawler)\n    else:\n        with pytest.raises(NotConfigured):\n            build_from_crawler(AutoThrottle, crawler)\n\n\n@pytest.mark.parametrize(\n    \"value\",\n    (\n        0.0,\n        -1.0,\n    ),\n)\ndef test_target_concurrency_invalid(value):\n    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": value}\n    crawler = get_crawler(settings)\n    with pytest.raises(NotConfigured):\n        build_from_crawler(AutoThrottle, crawler)\n\n\n@pytest.mark.parametrize(\n    (\"spider\", \"setting\", \"expected\"),\n    (\n        (UNSET, UNSET, DOWNLOAD_DELAY),\n        (1.0, UNSET, 1.0),\n        (UNSET, 1.0, 1.0),\n        (1.0, 2.0, 1.0),\n        (3.0, 2.0, 3.0),\n    ),\n)\ndef test_mindelay_definition(spider, setting, expected):\n    settings = {}\n    if setting is not UNSET:\n        settings[\"DOWNLOAD_DELAY\"] = setting\n\n    class _TestSpider(Spider):\n        name = \"test\"\n\n    if spider is not UNSET:\n        _TestSpider.download_delay = spider\n\n    crawler = get_crawler(settings, _TestSpider)\n    at = build_from_crawler(AutoThrottle, crawler)\n    at._spider_opened(_TestSpider())\n    assert at.mindelay == expected\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    (\n        (UNSET, AUTOTHROTTLE_MAX_DELAY),\n        (1.0, 1.0),\n    ),\n)\ndef test_maxdelay_definition(value, expected):\n    settings = {}\n    if value is not UNSET:\n        settings[\"AUTOTHROTTLE_MAX_DELAY\"] = value\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    at._spider_opened(TestSpider())\n    assert at.maxdelay == expected\n\n\n@pytest.mark.parametrize(\n    (\"min_spider\", \"min_setting\", \"start_setting\", \"expected\"),\n    (\n        (UNSET, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n        (AUTOTHROTTLE_START_DELAY - 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n        (AUTOTHROTTLE_START_DELAY + 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n        (UNSET, AUTOTHROTTLE_START_DELAY - 1.0, UNSET, AUTOTHROTTLE_START_DELAY),\n        (UNSET, AUTOTHROTTLE_START_DELAY + 1.0, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n        (UNSET, UNSET, AUTOTHROTTLE_START_DELAY - 1.0, AUTOTHROTTLE_START_DELAY - 1.0),\n        (UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0, AUTOTHROTTLE_START_DELAY + 1.0),\n        (\n            AUTOTHROTTLE_START_DELAY + 1.0,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n            UNSET,\n            AUTOTHROTTLE_START_DELAY + 1.0,\n        ),\n        (\n            AUTOTHROTTLE_START_DELAY + 2.0,\n            UNSET,\n            AUTOTHROTTLE_START_DELAY + 1.0,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n        ),\n        (\n            AUTOTHROTTLE_START_DELAY + 1.0,\n            UNSET,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n            AUTOTHROTTLE_START_DELAY + 2.0,\n        ),\n    ),\n)\ndef test_startdelay_definition(min_spider, min_setting, start_setting, expected):\n    settings = {}\n    if min_setting is not UNSET:\n        settings[\"DOWNLOAD_DELAY\"] = min_setting\n    if start_setting is not UNSET:\n        settings[\"AUTOTHROTTLE_START_DELAY\"] = start_setting\n\n    class _TestSpider(Spider):\n        name = \"test\"\n\n    if min_spider is not UNSET:\n        _TestSpider.download_delay = min_spider\n\n    crawler = get_crawler(settings, _TestSpider)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = _TestSpider()\n    at._spider_opened(spider)\n    assert spider.download_delay == expected\n\n\n@pytest.mark.parametrize(\n    (\"meta\", \"slot\", \"throttle\"),\n    (\n        ({}, None, None),\n        ({\"download_latency\": 1.0}, None, None),\n        ({\"download_slot\": \"foo\"}, None, None),\n        ({\"download_slot\": \"foo\"}, \"foo\", None),\n        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None, None),\n        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, \"foo\", False),\n    ),\n)\ndef test_skipped(meta, slot, throttle):\n    crawler = get_crawler()\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = TestSpider()\n    at._spider_opened(spider)\n    request = Request(\"https://example.com\", meta=meta)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    if slot is not None:\n        _slot = Mock()\n        _slot.throttle = throttle\n        crawler.engine.downloader.slots[slot] = _slot\n    at._adjust_delay = None  # Raise exception if called.\n\n    at._response_downloaded(None, request, spider)\n\n\n@pytest.mark.parametrize(\n    (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n    (\n        (2.0, 2.0, 1.0, 1.0),\n        (1.0, 2.0, 1.0, 0.75),\n        (4.0, 2.0, 1.0, 2.0),\n        (2.0, 1.0, 1.0, 2.0),\n        (2.0, 4.0, 1.0, 0.75),\n        (2.0, 2.0, 0.5, 1.0),\n        (2.0, 2.0, 2.0, 1.5),\n    ),\n)\ndef test_adjustment(download_latency, target_concurrency, slot_delay, expected):\n    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = TestSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = slot_delay\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    at._response_downloaded(response, request, spider)\n\n    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n\n\n@pytest.mark.parametrize(\n    (\"mindelay\", \"maxdelay\", \"expected\"),\n    (\n        (0.5, 2.0, 1.0),\n        (0.25, 0.5, 0.5),\n        (2.0, 4.0, 2.0),\n    ),\n)\ndef test_adjustment_limits(mindelay, maxdelay, expected):\n    download_latency, target_concurrency, slot_delay = (2.0, 2.0, 1.0)\n    # expected adjustment without limits with these values: 1.0\n    settings = {\n        \"AUTOTHROTTLE_MAX_DELAY\": maxdelay,\n        \"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency,\n        \"DOWNLOAD_DELAY\": mindelay,\n    }\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = TestSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = slot_delay\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    at._response_downloaded(response, request, spider)\n\n    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n\n\n@pytest.mark.parametrize(\n    (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n    (\n        (2.0, 2.0, 1.0, 1.0),\n        (1.0, 2.0, 1.0, 1.0),  # Instead of 0.75\n        (4.0, 2.0, 1.0, 2.0),\n    ),\n)\ndef test_adjustment_bad_response(\n    download_latency, target_concurrency, slot_delay, expected\n):\n    settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = TestSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url, status=400)\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = slot_delay\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    at._response_downloaded(response, request, spider)\n\n    assert slot.delay == expected, f\"{slot.delay} != {expected}\"\n\n\ndef test_debug(caplog):\n    settings = {\"AUTOTHROTTLE_DEBUG\": True}\n    crawler = get_crawler(settings)\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = TestSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url, body=b\"foo\")\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = 2.0\n    slot.transferring = (None, None)\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    caplog.clear()\n    with caplog.at_level(INFO):\n        at._response_downloaded(response, request, spider)\n\n    assert caplog.record_tuples == [\n        (\n            \"scrapy.extensions.throttle\",\n            INFO,\n            \"slot: foo | conc: 2 | delay: 1500 ms (-500) | latency: 1000 ms | size:     3 bytes\",\n        ),\n    ]\n\n\ndef test_debug_disabled(caplog):\n    crawler = get_crawler()\n    at = build_from_crawler(AutoThrottle, crawler)\n    spider = TestSpider()\n    at._spider_opened(spider)\n    meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n    request = Request(\"https://example.com\", meta=meta)\n    response = Response(request.url, body=b\"foo\")\n\n    crawler.engine = Mock()\n    crawler.engine.downloader = Mock()\n    crawler.engine.downloader.slots = {}\n    slot = Mock()\n    slot.delay = 2.0\n    slot.transferring = (None, None)\n    crawler.engine.downloader.slots[\"foo\"] = slot\n\n    caplog.clear()\n    with caplog.at_level(INFO):\n        at._response_downloaded(response, request, spider)\n\n    assert caplog.record_tuples == []\n", "tests/test_utils_curl.py": "import unittest\nimport warnings\n\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy import Request\nfrom scrapy.utils.curl import curl_to_request_kwargs\n\n\nclass CurlToRequestKwargsTest(unittest.TestCase):\n    maxDiff = 5000\n\n    def _test_command(self, curl_command, expected_result):\n        result = curl_to_request_kwargs(curl_command)\n        self.assertEqual(result, expected_result)\n        try:\n            Request(**result)\n        except TypeError as e:\n            self.fail(f\"Request kwargs are not correct {e}\")\n\n    def test_get(self):\n        curl_command = \"curl http://example.org/\"\n        expected_result = {\"method\": \"GET\", \"url\": \"http://example.org/\"}\n        self._test_command(curl_command, expected_result)\n\n    def test_get_without_scheme(self):\n        curl_command = \"curl www.example.org\"\n        expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.org\"}\n        self._test_command(curl_command, expected_result)\n\n    def test_get_basic_auth(self):\n        curl_command = 'curl \"https://api.test.com/\" -u \"some_username:some_password\"'\n        expected_result = {\n            \"method\": \"GET\",\n            \"url\": \"https://api.test.com/\",\n            \"headers\": [\n                (\"Authorization\", basic_auth_header(\"some_username\", \"some_password\"))\n            ],\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_get_complex(self):\n        curl_command = (\n            \"curl 'http://httpbin.org/get' -H 'Accept-Encoding: gzip, deflate'\"\n            \" -H 'Accept-Language: en-US,en;q=0.9,ru;q=0.8,es;q=0.7' -H 'Upgra\"\n            \"de-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0 (X11; Linux \"\n            \"x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/62\"\n            \".0.3202.75 Chrome/62.0.3202.75 Safari/537.36' -H 'Accept: text/ht\"\n            \"ml,application/xhtml+xml,application/xml;q=0.9,image/webp,image/a\"\n            \"png,*/*;q=0.8' -H 'Referer: http://httpbin.org/' -H 'Cookie: _gau\"\n            \"ges_unique_year=1; _gauges_unique=1; _gauges_unique_month=1; _gau\"\n            \"ges_unique_hour=1; _gauges_unique_day=1' -H 'Connection: keep-ali\"\n            \"ve' --compressed\"\n        )\n        expected_result = {\n            \"method\": \"GET\",\n            \"url\": \"http://httpbin.org/get\",\n            \"headers\": [\n                (\"Accept-Encoding\", \"gzip, deflate\"),\n                (\"Accept-Language\", \"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"),\n                (\"Upgrade-Insecure-Requests\", \"1\"),\n                (\n                    \"User-Agent\",\n                    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML\"\n                    \", like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.32\"\n                    \"02.75 Safari/537.36\",\n                ),\n                (\n                    \"Accept\",\n                    \"text/html,application/xhtml+xml,application/xml;q=0.9,ima\"\n                    \"ge/webp,image/apng,*/*;q=0.8\",\n                ),\n                (\"Referer\", \"http://httpbin.org/\"),\n                (\"Connection\", \"keep-alive\"),\n            ],\n            \"cookies\": {\n                \"_gauges_unique_year\": \"1\",\n                \"_gauges_unique_hour\": \"1\",\n                \"_gauges_unique_day\": \"1\",\n                \"_gauges_unique\": \"1\",\n                \"_gauges_unique_month\": \"1\",\n            },\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_post(self):\n        curl_command = (\n            \"curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique\"\n            \"_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique\"\n            \"_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H\"\n            \" 'Accept-Encoding: gzip, deflate' -H 'Accept-Language: en-US,en;q\"\n            \"=0.9,ru;q=0.8,es;q=0.7' -H 'Upgrade-Insecure-Requests: 1' -H 'Use\"\n            \"r-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTM\"\n            \"L, like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.3202.75 S\"\n            \"afari/537.36' -H 'Content-Type: application/x-www-form-urlencoded\"\n            \"' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0\"\n            \".9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0'\"\n            \" -H 'Referer: http://httpbin.org/forms/post' -H 'Connection: keep\"\n            \"-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith%\"\n            \"40example.org&size=small&topping=cheese&topping=onion&delivery=12\"\n            \"%3A15&comments=' --compressed\"\n        )\n        expected_result = {\n            \"method\": \"POST\",\n            \"url\": \"http://httpbin.org/post\",\n            \"body\": \"custname=John+Smith&custtel=500&custemail=jsmith%40exampl\"\n            \"e.org&size=small&topping=cheese&topping=onion&delivery=12\"\n            \"%3A15&comments=\",\n            \"cookies\": {\n                \"_gauges_unique_year\": \"1\",\n                \"_gauges_unique_hour\": \"1\",\n                \"_gauges_unique_day\": \"1\",\n                \"_gauges_unique\": \"1\",\n                \"_gauges_unique_month\": \"1\",\n            },\n            \"headers\": [\n                (\"Origin\", \"http://httpbin.org\"),\n                (\"Accept-Encoding\", \"gzip, deflate\"),\n                (\"Accept-Language\", \"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"),\n                (\"Upgrade-Insecure-Requests\", \"1\"),\n                (\n                    \"User-Agent\",\n                    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML\"\n                    \", like Gecko) Ubuntu Chromium/62.0.3202.75 Chrome/62.0.32\"\n                    \"02.75 Safari/537.36\",\n                ),\n                (\"Content-Type\", \"application/x-www-form-urlencoded\"),\n                (\n                    \"Accept\",\n                    \"text/html,application/xhtml+xml,application/xml;q=0.9,ima\"\n                    \"ge/webp,image/apng,*/*;q=0.8\",\n                ),\n                (\"Cache-Control\", \"max-age=0\"),\n                (\"Referer\", \"http://httpbin.org/forms/post\"),\n                (\"Connection\", \"keep-alive\"),\n            ],\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_post_data_raw(self):\n        curl_command = (\n            \"curl 'https://www.example.org/' --data-raw 'excerptLength=200&ena\"\n            \"bleDidYouMean=true&sortCriteria=ffirstz32xnamez32x201740686%20asc\"\n            \"ending&queryFunctions=%5B%5D&rankingFunctions=%5B%5D'\"\n        )\n        expected_result = {\n            \"method\": \"POST\",\n            \"url\": \"https://www.example.org/\",\n            \"body\": (\n                \"excerptLength=200&enableDidYouMean=true&sortCriteria=ffirstz3\"\n                \"2xnamez32x201740686%20ascending&queryFunctions=%5B%5D&ranking\"\n                \"Functions=%5B%5D\"\n            ),\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_post_data_raw_with_string_prefix(self):\n        curl_command = \"curl 'https://www.example.org/' --data-raw $'{\\\"$filters\\\":\\\"Filter\\u0021\\\"}'\"\n        expected_result = {\n            \"method\": \"POST\",\n            \"url\": \"https://www.example.org/\",\n            \"body\": '{\"$filters\":\"Filter!\"}',\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_explicit_get_with_data(self):\n        curl_command = \"curl httpbin.org/anything -X GET --data asdf\"\n        expected_result = {\n            \"method\": \"GET\",\n            \"url\": \"http://httpbin.org/anything\",\n            \"body\": \"asdf\",\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_patch(self):\n        curl_command = (\n            'curl \"https://example.com/api/fake\" -u \"username:password\" -H \"Ac'\n            'cept: application/vnd.go.cd.v4+json\" -H \"Content-Type: applicatio'\n            'n/json\" -X PATCH -d \\'{\"hostname\": \"agent02.example.com\",  \"agent'\n            '_config_state\": \"Enabled\", \"resources\": [\"Java\",\"Linux\"], \"enviro'\n            'nments\": [\"Dev\"]}\\''\n        )\n        expected_result = {\n            \"method\": \"PATCH\",\n            \"url\": \"https://example.com/api/fake\",\n            \"headers\": [\n                (\"Accept\", \"application/vnd.go.cd.v4+json\"),\n                (\"Content-Type\", \"application/json\"),\n                (\"Authorization\", basic_auth_header(\"username\", \"password\")),\n            ],\n            \"body\": '{\"hostname\": \"agent02.example.com\",  \"agent_config_state\"'\n            ': \"Enabled\", \"resources\": [\"Java\",\"Linux\"], \"environments'\n            '\": [\"Dev\"]}',\n        }\n        self._test_command(curl_command, expected_result)\n\n    def test_delete(self):\n        curl_command = 'curl -X \"DELETE\" https://www.url.com/page'\n        expected_result = {\"method\": \"DELETE\", \"url\": \"https://www.url.com/page\"}\n        self._test_command(curl_command, expected_result)\n\n    def test_get_silent(self):\n        curl_command = 'curl --silent \"www.example.com\"'\n        expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n        self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n\n    def test_too_few_arguments_error(self):\n        self.assertRaisesRegex(\n            ValueError,\n            r\"too few arguments|the following arguments are required:\\s*url\",\n            lambda: curl_to_request_kwargs(\"curl\"),\n        )\n\n    def test_ignore_unknown_options(self):\n        # case 1: ignore_unknown_options=True:\n        with warnings.catch_warnings():  # avoid warning when executing tests\n            warnings.simplefilter(\"ignore\")\n            curl_command = \"curl --bar --baz http://www.example.com\"\n            expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n            self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n\n        # case 2: ignore_unknown_options=False (raise exception):\n        self.assertRaisesRegex(\n            ValueError,\n            \"Unrecognized options:.*--bar.*--baz\",\n            lambda: curl_to_request_kwargs(\n                \"curl --bar --baz http://www.example.com\", ignore_unknown_options=False\n            ),\n        )\n\n    def test_must_start_with_curl_error(self):\n        self.assertRaises(\n            ValueError,\n            lambda: curl_to_request_kwargs(\"carl -X POST http://example.org\"),\n        )\n", "tests/test_dependencies.py": "import os\nimport re\nfrom configparser import ConfigParser\nfrom importlib import import_module\nfrom pathlib import Path\n\nfrom twisted import version as twisted_version\nfrom twisted.trial import unittest\n\n\nclass ScrapyUtilsTest(unittest.TestCase):\n    def test_required_openssl_version(self):\n        try:\n            module = import_module(\"OpenSSL\")\n        except ImportError:\n            raise unittest.SkipTest(\"OpenSSL is not available\")\n\n        if hasattr(module, \"__version__\"):\n            installed_version = [int(x) for x in module.__version__.split(\".\")[:2]]\n            assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n\n    def test_pinned_twisted_version(self):\n        \"\"\"When running tests within a Tox environment with pinned\n        dependencies, make sure that the version of Twisted is the pinned\n        version.\n\n        See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011\n        \"\"\"\n        if not os.environ.get(\"_SCRAPY_PINNED\", None):\n            self.skipTest(\"Not in a pinned environment\")\n\n        tox_config_file_path = Path(__file__).parent / \"..\" / \"tox.ini\"\n        config_parser = ConfigParser()\n        config_parser.read(tox_config_file_path)\n        pattern = r\"Twisted\\[http2\\]==([\\d.]+)\"\n        match = re.search(pattern, config_parser[\"pinned\"][\"deps\"])\n        pinned_twisted_version_string = match[1]\n\n        self.assertEqual(twisted_version.short(), pinned_twisted_version_string)\n", "tests/test_utils_console.py": "import unittest\n\nfrom scrapy.utils.console import get_shell_embed_func\n\ntry:\n    import bpython\n\n    bpy = True\n    del bpython\nexcept ImportError:\n    bpy = False\ntry:\n    import IPython\n\n    ipy = True\n    del IPython\nexcept ImportError:\n    ipy = False\n\n\nclass UtilsConsoleTestCase(unittest.TestCase):\n    def test_get_shell_embed_func(self):\n        shell = get_shell_embed_func([\"invalid\"])\n        self.assertEqual(shell, None)\n\n        shell = get_shell_embed_func([\"invalid\", \"python\"])\n        self.assertTrue(callable(shell))\n        self.assertEqual(shell.__name__, \"_embed_standard_shell\")\n\n    @unittest.skipIf(not bpy, \"bpython not available in testenv\")\n    def test_get_shell_embed_func2(self):\n        shell = get_shell_embed_func([\"bpython\"])\n        self.assertTrue(callable(shell))\n        self.assertEqual(shell.__name__, \"_embed_bpython_shell\")\n\n    @unittest.skipIf(not ipy, \"IPython not available in testenv\")\n    def test_get_shell_embed_func3(self):\n        # default shell should be 'ipython'\n        shell = get_shell_embed_func()\n        self.assertEqual(shell.__name__, \"_embed_ipython_shell\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_gz.py": "import unittest\nfrom pathlib import Path\n\nfrom w3lib.encoding import html_to_unicode\n\nfrom scrapy.http import Response\nfrom scrapy.utils.gz import gunzip, gzip_magic_number\nfrom tests import tests_datadir\n\nSAMPLEDIR = Path(tests_datadir, \"compressed\")\n\n\nclass GunzipTest(unittest.TestCase):\n    def test_gunzip_basic(self):\n        r1 = Response(\n            \"http://www.example.com\",\n            body=(SAMPLEDIR / \"feed-sample1.xml.gz\").read_bytes(),\n        )\n        self.assertTrue(gzip_magic_number(r1))\n\n        r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n        self.assertFalse(gzip_magic_number(r2))\n        self.assertEqual(len(r2.body), 9950)\n\n    def test_gunzip_truncated(self):\n        text = gunzip((SAMPLEDIR / \"truncated-crc-error.gz\").read_bytes())\n        assert text.endswith(b\"</html\")\n\n    def test_gunzip_no_gzip_file_raises(self):\n        self.assertRaises(\n            OSError, gunzip, (SAMPLEDIR / \"feed-sample1.xml\").read_bytes()\n        )\n\n    def test_gunzip_truncated_short(self):\n        r1 = Response(\n            \"http://www.example.com\",\n            body=(SAMPLEDIR / \"truncated-crc-error-short.gz\").read_bytes(),\n        )\n        self.assertTrue(gzip_magic_number(r1))\n\n        r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n        assert r2.body.endswith(b\"</html>\")\n        self.assertFalse(gzip_magic_number(r2))\n\n    def test_is_gzipped_empty(self):\n        r1 = Response(\"http://www.example.com\")\n        self.assertFalse(gzip_magic_number(r1))\n\n    def test_gunzip_illegal_eof(self):\n        text = html_to_unicode(\n            \"charset=cp1252\", gunzip((SAMPLEDIR / \"unexpected-eof.gz\").read_bytes())\n        )[1]\n        expected_text = (SAMPLEDIR / \"unexpected-eof-output.txt\").read_text(\n            encoding=\"utf-8\"\n        )\n        self.assertEqual(len(text), len(expected_text))\n        self.assertEqual(text, expected_text)\n", "tests/test_spidermiddleware_httperror.py": "import logging\nfrom typing import Set\nfrom unittest import TestCase\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase as TrialTestCase\n\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spidermiddlewares.httperror import HttpError, HttpErrorMiddleware\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import MockServerSpider\n\n\nclass _HttpErrorSpider(MockServerSpider):\n    name = \"httperror\"\n    bypass_status_codes: Set[int] = set()\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = [\n            self.mockserver.url(\"/status?n=200\"),\n            self.mockserver.url(\"/status?n=404\"),\n            self.mockserver.url(\"/status?n=402\"),\n            self.mockserver.url(\"/status?n=500\"),\n        ]\n        self.failed = set()\n        self.skipped = set()\n        self.parsed = set()\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield Request(url, self.parse, errback=self.on_error)\n\n    def parse(self, response):\n        self.parsed.add(response.url[-3:])\n\n    def on_error(self, failure):\n        if isinstance(failure.value, HttpError):\n            response = failure.value.response\n            if response.status in self.bypass_status_codes:\n                self.skipped.add(response.url[-3:])\n                return self.parse(response)\n\n        # it assumes there is a response attached to failure\n        self.failed.add(failure.value.response.url[-3:])\n        return failure\n\n\ndef _responses(request, status_codes):\n    responses = []\n    for code in status_codes:\n        response = Response(request.url, status=code)\n        response.request = request\n        responses.append(response)\n    return responses\n\n\nclass TestHttpErrorMiddleware(TestCase):\n    def setUp(self):\n        crawler = get_crawler(Spider)\n        self.spider = Spider.from_crawler(crawler, name=\"foo\")\n        self.mw = HttpErrorMiddleware(Settings({}))\n        self.req = Request(\"http://scrapytest.org\")\n        self.res200, self.res404 = _responses(self.req, [200, 404])\n\n    def test_process_spider_input(self):\n        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n        self.assertRaises(\n            HttpError, self.mw.process_spider_input, self.res404, self.spider\n        )\n\n    def test_process_spider_exception(self):\n        self.assertEqual(\n            [],\n            self.mw.process_spider_exception(\n                self.res404, HttpError(self.res404), self.spider\n            ),\n        )\n        self.assertIsNone(\n            self.mw.process_spider_exception(self.res404, Exception(), self.spider)\n        )\n\n    def test_handle_httpstatus_list(self):\n        res = self.res404.copy()\n        res.request = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n        )\n        self.assertIsNone(self.mw.process_spider_input(res, self.spider))\n\n        self.spider.handle_httpstatus_list = [404]\n        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n\n\nclass TestHttpErrorMiddlewareSettings(TestCase):\n    \"\"\"Similar test, but with settings\"\"\"\n\n    def setUp(self):\n        self.spider = Spider(\"foo\")\n        self.mw = HttpErrorMiddleware(Settings({\"HTTPERROR_ALLOWED_CODES\": (402,)}))\n        self.req = Request(\"http://scrapytest.org\")\n        self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n\n    def test_process_spider_input(self):\n        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n        self.assertRaises(\n            HttpError, self.mw.process_spider_input, self.res404, self.spider\n        )\n        self.assertIsNone(self.mw.process_spider_input(self.res402, self.spider))\n\n    def test_meta_overrides_settings(self):\n        request = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n        )\n        res404 = self.res404.copy()\n        res404.request = request\n        res402 = self.res402.copy()\n        res402.request = request\n\n        self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n        self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)\n\n    def test_spider_override_settings(self):\n        self.spider.handle_httpstatus_list = [404]\n        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n        self.assertRaises(\n            HttpError, self.mw.process_spider_input, self.res402, self.spider\n        )\n\n\nclass TestHttpErrorMiddlewareHandleAll(TestCase):\n    def setUp(self):\n        self.spider = Spider(\"foo\")\n        self.mw = HttpErrorMiddleware(Settings({\"HTTPERROR_ALLOW_ALL\": True}))\n        self.req = Request(\"http://scrapytest.org\")\n        self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n\n    def test_process_spider_input(self):\n        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n\n    def test_meta_overrides_settings(self):\n        request = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n        )\n        res404 = self.res404.copy()\n        res404.request = request\n        res402 = self.res402.copy()\n        res402.request = request\n\n        self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n        self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)\n\n    def test_httperror_allow_all_false(self):\n        crawler = get_crawler(_HttpErrorSpider)\n        mw = HttpErrorMiddleware.from_crawler(crawler)\n        request_httpstatus_false = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_all\": False}\n        )\n        request_httpstatus_true = Request(\n            \"http://scrapytest.org\", meta={\"handle_httpstatus_all\": True}\n        )\n        res404 = self.res404.copy()\n        res404.request = request_httpstatus_false\n        res402 = self.res402.copy()\n        res402.request = request_httpstatus_true\n\n        self.assertRaises(HttpError, mw.process_spider_input, res404, self.spider)\n        self.assertIsNone(mw.process_spider_input(res402, self.spider))\n\n\nclass TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_middleware_works(self):\n        crawler = get_crawler(_HttpErrorSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        assert not crawler.spider.skipped, crawler.spider.skipped\n        self.assertEqual(crawler.spider.parsed, {\"200\"})\n        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n\n        get_value = crawler.stats.get_value\n        self.assertEqual(get_value(\"httperror/response_ignored_count\"), 3)\n        self.assertEqual(get_value(\"httperror/response_ignored_status_count/404\"), 1)\n        self.assertEqual(get_value(\"httperror/response_ignored_status_count/402\"), 1)\n        self.assertEqual(get_value(\"httperror/response_ignored_status_count/500\"), 1)\n\n    @defer.inlineCallbacks\n    def test_logging(self):\n        crawler = get_crawler(_HttpErrorSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(mockserver=self.mockserver, bypass_status_codes={402})\n        self.assertEqual(crawler.spider.parsed, {\"200\", \"402\"})\n        self.assertEqual(crawler.spider.skipped, {\"402\"})\n        self.assertEqual(crawler.spider.failed, {\"404\", \"500\"})\n\n        self.assertIn(\"Ignoring response <404\", str(log))\n        self.assertIn(\"Ignoring response <500\", str(log))\n        self.assertNotIn(\"Ignoring response <200\", str(log))\n        self.assertNotIn(\"Ignoring response <402\", str(log))\n\n    @defer.inlineCallbacks\n    def test_logging_level(self):\n        # HttpError logs ignored responses with level INFO\n        crawler = get_crawler(_HttpErrorSpider)\n        with LogCapture(level=logging.INFO) as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(crawler.spider.parsed, {\"200\"})\n        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n\n        self.assertIn(\"Ignoring response <402\", str(log))\n        self.assertIn(\"Ignoring response <404\", str(log))\n        self.assertIn(\"Ignoring response <500\", str(log))\n        self.assertNotIn(\"Ignoring response <200\", str(log))\n\n        # with level WARNING, we shouldn't capture anything from HttpError\n        crawler = get_crawler(_HttpErrorSpider)\n        with LogCapture(level=logging.WARNING) as log:\n            yield crawler.crawl(mockserver=self.mockserver)\n        self.assertEqual(crawler.spider.parsed, {\"200\"})\n        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n\n        self.assertNotIn(\"Ignoring response <402\", str(log))\n        self.assertNotIn(\"Ignoring response <404\", str(log))\n        self.assertNotIn(\"Ignoring response <500\", str(log))\n        self.assertNotIn(\"Ignoring response <200\", str(log))\n", "tests/test_commands.py": "import argparse\nimport inspect\nimport json\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\nfrom contextlib import contextmanager\nfrom itertools import chain\nfrom pathlib import Path\nfrom shutil import copytree, rmtree\nfrom stat import S_IWRITE as ANYONE_WRITE_PERMISSION\nfrom tempfile import TemporaryFile, mkdtemp\nfrom threading import Timer\nfrom typing import Dict, Iterator, Optional, Union\nfrom unittest import skipIf\n\nfrom pytest import mark\nfrom twisted.trial import unittest\n\nimport scrapy\nfrom scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\nfrom scrapy.commands.startproject import IGNORE\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.test import get_testenv\nfrom tests.test_crawler import ExceptionSpider, NoRequestsSpider\n\n\nclass CommandSettings(unittest.TestCase):\n    def setUp(self):\n        self.command = ScrapyCommand()\n        self.command.settings = Settings()\n        self.parser = argparse.ArgumentParser(\n            formatter_class=ScrapyHelpFormatter, conflict_handler=\"resolve\"\n        )\n        self.command.add_options(self.parser)\n\n    def test_settings_json_string(self):\n        feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n        opts, args = self.parser.parse_known_args(\n            args=[\"-s\", f\"FEEDS={feeds_json}\", \"spider.py\"]\n        )\n        self.command.process_options(args, opts)\n        self.assertIsInstance(\n            self.command.settings[\"FEEDS\"], scrapy.settings.BaseSettings\n        )\n        self.assertEqual(dict(self.command.settings[\"FEEDS\"]), json.loads(feeds_json))\n\n    def test_help_formatter(self):\n        formatter = ScrapyHelpFormatter(prog=\"scrapy\")\n        part_strings = [\n            \"usage: scrapy genspider [options] <name> <domain>\\n\\n\",\n            \"\\n\",\n            \"optional arguments:\\n\",\n            \"\\n\",\n            \"Global Options:\\n\",\n        ]\n        self.assertEqual(\n            formatter._join_parts(part_strings),\n            (\n                \"Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n\"\n                \"Optional Arguments\\n==================\\n\\n\"\n                \"Global Options\\n--------------\\n\"\n            ),\n        )\n\n\nclass ProjectTest(unittest.TestCase):\n    project_name = \"testproject\"\n\n    def setUp(self):\n        self.temp_path = mkdtemp()\n        self.cwd = self.temp_path\n        self.proj_path = Path(self.temp_path, self.project_name)\n        self.proj_mod_path = self.proj_path / self.project_name\n        self.env = get_testenv()\n\n    def tearDown(self):\n        rmtree(self.temp_path)\n\n    def call(self, *new_args, **kwargs):\n        with TemporaryFile() as out:\n            args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n            return subprocess.call(\n                args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **kwargs\n            )\n\n    def proc(self, *new_args, **popen_kwargs):\n        args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n        p = subprocess.Popen(\n            args,\n            cwd=popen_kwargs.pop(\"cwd\", self.cwd),\n            env=self.env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            **popen_kwargs,\n        )\n\n        def kill_proc():\n            p.kill()\n            p.communicate()\n            raise AssertionError(\"Command took too much time to complete\")\n\n        timer = Timer(15, kill_proc)\n        try:\n            timer.start()\n            stdout, stderr = p.communicate()\n        finally:\n            timer.cancel()\n\n        return p, to_unicode(stdout), to_unicode(stderr)\n\n    def find_in_file(\n        self, filename: Union[str, os.PathLike], regex\n    ) -> Optional[re.Match]:\n        \"\"\"Find first pattern occurrence in file\"\"\"\n        pattern = re.compile(regex)\n        with Path(filename).open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                match = pattern.search(line)\n                if match is not None:\n                    return match\n        return None\n\n\nclass StartprojectTest(ProjectTest):\n    def test_startproject(self):\n        p, out, err = self.proc(\"startproject\", self.project_name)\n        print(out)\n        print(err, file=sys.stderr)\n        self.assertEqual(p.returncode, 0)\n\n        assert Path(self.proj_path, \"scrapy.cfg\").exists()\n        assert Path(self.proj_path, \"testproject\").exists()\n        assert Path(self.proj_mod_path, \"__init__.py\").exists()\n        assert Path(self.proj_mod_path, \"items.py\").exists()\n        assert Path(self.proj_mod_path, \"pipelines.py\").exists()\n        assert Path(self.proj_mod_path, \"settings.py\").exists()\n        assert Path(self.proj_mod_path, \"spiders\", \"__init__.py\").exists()\n\n        self.assertEqual(1, self.call(\"startproject\", self.project_name))\n        self.assertEqual(1, self.call(\"startproject\", \"wrong---project---name\"))\n        self.assertEqual(1, self.call(\"startproject\", \"sys\"))\n\n    def test_startproject_with_project_dir(self):\n        project_dir = mkdtemp()\n        self.assertEqual(0, self.call(\"startproject\", self.project_name, project_dir))\n\n        assert Path(project_dir, \"scrapy.cfg\").exists()\n        assert Path(project_dir, \"testproject\").exists()\n        assert Path(project_dir, self.project_name, \"__init__.py\").exists()\n        assert Path(project_dir, self.project_name, \"items.py\").exists()\n        assert Path(project_dir, self.project_name, \"pipelines.py\").exists()\n        assert Path(project_dir, self.project_name, \"settings.py\").exists()\n        assert Path(project_dir, self.project_name, \"spiders\", \"__init__.py\").exists()\n\n        self.assertEqual(\n            0, self.call(\"startproject\", self.project_name, project_dir + \"2\")\n        )\n\n        self.assertEqual(1, self.call(\"startproject\", self.project_name, project_dir))\n        self.assertEqual(\n            1, self.call(\"startproject\", self.project_name + \"2\", project_dir)\n        )\n        self.assertEqual(1, self.call(\"startproject\", \"wrong---project---name\"))\n        self.assertEqual(1, self.call(\"startproject\", \"sys\"))\n        self.assertEqual(2, self.call(\"startproject\"))\n        self.assertEqual(\n            2,\n            self.call(\"startproject\", self.project_name, project_dir, \"another_params\"),\n        )\n\n    def test_existing_project_dir(self):\n        project_dir = mkdtemp()\n        project_name = self.project_name + \"_existing\"\n        project_path = Path(project_dir, project_name)\n        project_path.mkdir()\n\n        p, out, err = self.proc(\"startproject\", project_name, cwd=project_dir)\n        print(out)\n        print(err, file=sys.stderr)\n        self.assertEqual(p.returncode, 0)\n\n        assert Path(project_path, \"scrapy.cfg\").exists()\n        assert Path(project_path, project_name).exists()\n        assert Path(project_path, project_name, \"__init__.py\").exists()\n        assert Path(project_path, project_name, \"items.py\").exists()\n        assert Path(project_path, project_name, \"pipelines.py\").exists()\n        assert Path(project_path, project_name, \"settings.py\").exists()\n        assert Path(project_path, project_name, \"spiders\", \"__init__.py\").exists()\n\n\ndef get_permissions_dict(\n    path: Union[str, os.PathLike], renamings=None, ignore=None\n) -> Dict[str, str]:\n    def get_permissions(path: Path) -> str:\n        return oct(path.stat().st_mode)\n\n    path_obj = Path(path)\n\n    renamings = renamings or ()\n    permissions_dict = {\n        \".\": get_permissions(path_obj),\n    }\n    for root, dirs, files in os.walk(path_obj):\n        nodes = list(chain(dirs, files))\n        if ignore:\n            ignored_names = ignore(root, nodes)\n            nodes = [node for node in nodes if node not in ignored_names]\n        for node in nodes:\n            absolute_path = Path(root, node)\n            relative_path = str(absolute_path.relative_to(path))\n            for search_string, replacement in renamings:\n                relative_path = relative_path.replace(search_string, replacement)\n            permissions = get_permissions(absolute_path)\n            permissions_dict[relative_path] = permissions\n    return permissions_dict\n\n\nclass StartprojectTemplatesTest(ProjectTest):\n    maxDiff = None\n\n    def setUp(self):\n        super().setUp()\n        self.tmpl = str(Path(self.temp_path, \"templates\"))\n        self.tmpl_proj = str(Path(self.tmpl, \"project\"))\n\n    def test_startproject_template_override(self):\n        copytree(Path(scrapy.__path__[0], \"templates\"), self.tmpl)\n        Path(self.tmpl_proj, \"root_template\").write_bytes(b\"\")\n        assert Path(self.tmpl_proj, \"root_template\").exists()\n\n        args = [\"--set\", f\"TEMPLATES_DIR={self.tmpl}\"]\n        p, out, err = self.proc(\"startproject\", self.project_name, *args)\n        self.assertIn(\n            f\"New Scrapy project '{self.project_name}', \" \"using template directory\",\n            out,\n        )\n        self.assertIn(self.tmpl_proj, out)\n        assert Path(self.proj_path, \"root_template\").exists()\n\n    def test_startproject_permissions_from_writable(self):\n        \"\"\"Check that generated files have the right permissions when the\n        template folder has the same permissions as in the project, i.e.\n        everything is writable.\"\"\"\n        scrapy_path = scrapy.__path__[0]\n        project_template = Path(scrapy_path, \"templates\", \"project\")\n        project_name = \"startproject1\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        destination = mkdtemp()\n        process = subprocess.Popen(\n            (\n                sys.executable,\n                \"-m\",\n                \"scrapy.cmdline\",\n                \"startproject\",\n                project_name,\n            ),\n            cwd=destination,\n            env=self.env,\n        )\n        process.wait()\n\n        project_dir = Path(destination, project_name)\n        actual_permissions = get_permissions_dict(project_dir)\n\n        self.assertEqual(actual_permissions, expected_permissions)\n\n    def test_startproject_permissions_from_read_only(self):\n        \"\"\"Check that generated files have the right permissions when the\n        template folder has been made read-only, which is something that some\n        systems do.\n\n        See https://github.com/scrapy/scrapy/pull/4604\n        \"\"\"\n        scrapy_path = scrapy.__path__[0]\n        templates_dir = Path(scrapy_path, \"templates\")\n        project_template = Path(templates_dir, \"project\")\n        project_name = \"startproject2\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        def _make_read_only(path: Path):\n            current_permissions = path.stat().st_mode\n            path.chmod(current_permissions & ~ANYONE_WRITE_PERMISSION)\n\n        read_only_templates_dir = str(Path(mkdtemp()) / \"templates\")\n        copytree(templates_dir, read_only_templates_dir)\n\n        for root, dirs, files in os.walk(read_only_templates_dir):\n            for node in chain(dirs, files):\n                _make_read_only(Path(root, node))\n\n        destination = mkdtemp()\n        process = subprocess.Popen(\n            (\n                sys.executable,\n                \"-m\",\n                \"scrapy.cmdline\",\n                \"startproject\",\n                project_name,\n                \"--set\",\n                f\"TEMPLATES_DIR={read_only_templates_dir}\",\n            ),\n            cwd=destination,\n            env=self.env,\n        )\n        process.wait()\n\n        project_dir = Path(destination, project_name)\n        actual_permissions = get_permissions_dict(project_dir)\n\n        self.assertEqual(actual_permissions, expected_permissions)\n\n    def test_startproject_permissions_unchanged_in_destination(self):\n        \"\"\"Check that preexisting folders and files in the destination folder\n        do not see their permissions modified.\"\"\"\n        scrapy_path = scrapy.__path__[0]\n        project_template = Path(scrapy_path, \"templates\", \"project\")\n        project_name = \"startproject3\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        destination = mkdtemp()\n        project_dir = Path(destination, project_name)\n\n        existing_nodes = {\n            oct(permissions)[2:] + extension: permissions\n            for extension in (\"\", \".d\")\n            for permissions in (\n                0o444,\n                0o555,\n                0o644,\n                0o666,\n                0o755,\n                0o777,\n            )\n        }\n        project_dir.mkdir()\n        for node, permissions in existing_nodes.items():\n            path = project_dir / node\n            if node.endswith(\".d\"):\n                path.mkdir(mode=permissions)\n            else:\n                path.touch(mode=permissions)\n            expected_permissions[node] = oct(path.stat().st_mode)\n\n        process = subprocess.Popen(\n            (\n                sys.executable,\n                \"-m\",\n                \"scrapy.cmdline\",\n                \"startproject\",\n                project_name,\n                \".\",\n            ),\n            cwd=project_dir,\n            env=self.env,\n        )\n        process.wait()\n\n        actual_permissions = get_permissions_dict(project_dir)\n\n        self.assertEqual(actual_permissions, expected_permissions)\n\n    def test_startproject_permissions_umask_022(self):\n        \"\"\"Check that generated files have the right permissions when the\n        system uses a umask value that causes new files to have different\n        permissions than those from the template folder.\"\"\"\n\n        @contextmanager\n        def umask(new_mask):\n            cur_mask = os.umask(new_mask)\n            yield\n            os.umask(cur_mask)\n\n        scrapy_path = scrapy.__path__[0]\n        project_template = Path(scrapy_path, \"templates\", \"project\")\n        project_name = \"umaskproject\"\n        renamings = (\n            (\"module\", project_name),\n            (\".tmpl\", \"\"),\n        )\n        expected_permissions = get_permissions_dict(\n            project_template,\n            renamings,\n            IGNORE,\n        )\n\n        with umask(0o002):\n            destination = mkdtemp()\n            process = subprocess.Popen(\n                (\n                    sys.executable,\n                    \"-m\",\n                    \"scrapy.cmdline\",\n                    \"startproject\",\n                    project_name,\n                ),\n                cwd=destination,\n                env=self.env,\n            )\n            process.wait()\n\n            project_dir = Path(destination, project_name)\n            actual_permissions = get_permissions_dict(project_dir)\n\n            self.assertEqual(actual_permissions, expected_permissions)\n\n\nclass CommandTest(ProjectTest):\n    def setUp(self):\n        super().setUp()\n        self.call(\"startproject\", self.project_name)\n        self.cwd = Path(self.temp_path, self.project_name)\n        self.env[\"SCRAPY_SETTINGS_MODULE\"] = f\"{self.project_name}.settings\"\n\n\nclass GenspiderCommandTest(CommandTest):\n    def test_arguments(self):\n        # only pass one argument. spider script shouldn't be created\n        self.assertEqual(2, self.call(\"genspider\", \"test_name\"))\n        assert not Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n        # pass two arguments <name> <domain>. spider script should be created\n        self.assertEqual(0, self.call(\"genspider\", \"test_name\", \"test.com\"))\n        assert Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n\n    def test_template(self, tplname=\"crawl\"):\n        args = [f\"--template={tplname}\"] if tplname else []\n        spname = \"test_spider\"\n        spmodule = f\"{self.project_name}.spiders.{spname}\"\n        p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n        self.assertIn(\n            f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\",\n            out,\n        )\n        self.assertTrue(Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").exists())\n        modify_time_before = (\n            Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n        )\n        p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n        self.assertIn(f\"Spider {spname!r} already exists in module\", out)\n        modify_time_after = (\n            Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n        )\n        self.assertEqual(modify_time_after, modify_time_before)\n\n    def test_template_basic(self):\n        self.test_template(\"basic\")\n\n    def test_template_csvfeed(self):\n        self.test_template(\"csvfeed\")\n\n    def test_template_xmlfeed(self):\n        self.test_template(\"xmlfeed\")\n\n    def test_list(self):\n        self.assertEqual(0, self.call(\"genspider\", \"--list\"))\n\n    def test_dump(self):\n        self.assertEqual(0, self.call(\"genspider\", \"--dump=basic\"))\n        self.assertEqual(0, self.call(\"genspider\", \"-d\", \"basic\"))\n\n    def test_same_name_as_project(self):\n        self.assertEqual(2, self.call(\"genspider\", self.project_name))\n        assert not Path(\n            self.proj_mod_path, \"spiders\", f\"{self.project_name}.py\"\n        ).exists()\n\n    def test_same_filename_as_existing_spider(self, force=False):\n        file_name = \"example\"\n        file_path = Path(self.proj_mod_path, \"spiders\", f\"{file_name}.py\")\n        self.assertEqual(0, self.call(\"genspider\", file_name, \"example.com\"))\n        assert file_path.exists()\n\n        # change name of spider but not its file name\n        with file_path.open(\"r+\", encoding=\"utf-8\") as spider_file:\n            file_data = spider_file.read()\n            file_data = file_data.replace('name = \"example\"', 'name = \"renamed\"')\n            spider_file.seek(0)\n            spider_file.write(file_data)\n            spider_file.truncate()\n        modify_time_before = file_path.stat().st_mtime\n        file_contents_before = file_data\n\n        if force:\n            p, out, err = self.proc(\"genspider\", \"--force\", file_name, \"example.com\")\n            self.assertIn(\n                f\"Created spider {file_name!r} using template 'basic' in module\", out\n            )\n            modify_time_after = file_path.stat().st_mtime\n            self.assertNotEqual(modify_time_after, modify_time_before)\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            self.assertNotEqual(file_contents_after, file_contents_before)\n        else:\n            p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n            self.assertIn(f\"{file_path.resolve()} already exists\", out)\n            modify_time_after = file_path.stat().st_mtime\n            self.assertEqual(modify_time_after, modify_time_before)\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            self.assertEqual(file_contents_after, file_contents_before)\n\n    def test_same_filename_as_existing_spider_force(self):\n        self.test_same_filename_as_existing_spider(force=True)\n\n    def test_url(self, url=\"test.com\", domain=\"test.com\"):\n        self.assertEqual(0, self.call(\"genspider\", \"--force\", \"test_name\", url))\n        self.assertEqual(\n            domain,\n            self.find_in_file(\n                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                r\"allowed_domains\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n            ).group(1),\n        )\n        self.assertEqual(\n            f\"https://{domain}\",\n            self.find_in_file(\n                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n            ).group(1),\n        )\n\n    def test_url_schema(self):\n        self.test_url(\"https://test.com\", \"test.com\")\n\n    def test_template_start_urls(\n        self, url=\"test.com\", expected=\"https://test.com\", template=\"basic\"\n    ):\n        self.assertEqual(\n            0, self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url)\n        )\n        self.assertEqual(\n            expected,\n            self.find_in_file(\n                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n            ).group(1),\n        )\n\n    def test_genspider_basic_start_urls(self):\n        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"basic\")\n        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"basic\")\n        self.test_template_start_urls(\n            \"http://test.com/other/path\", \"http://test.com/other/path\", \"basic\"\n        )\n        self.test_template_start_urls(\n            \"test.com/other/path\", \"https://test.com/other/path\", \"basic\"\n        )\n\n    def test_genspider_crawl_start_urls(self):\n        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"crawl\")\n        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"crawl\")\n        self.test_template_start_urls(\n            \"http://test.com/other/path\", \"http://test.com/other/path\", \"crawl\"\n        )\n        self.test_template_start_urls(\n            \"test.com/other/path\", \"https://test.com/other/path\", \"crawl\"\n        )\n        self.test_template_start_urls(\"test.com\", \"https://test.com\", \"crawl\")\n\n    def test_genspider_xmlfeed_start_urls(self):\n        self.test_template_start_urls(\n            \"https://test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n        )\n        self.test_template_start_urls(\n            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"xmlfeed\"\n        )\n        self.test_template_start_urls(\n            \"test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n        )\n\n    def test_genspider_csvfeed_start_urls(self):\n        self.test_template_start_urls(\n            \"https://test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n        )\n        self.test_template_start_urls(\n            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"csvfeed\"\n        )\n        self.test_template_start_urls(\n            \"test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n        )\n\n\nclass GenspiderStandaloneCommandTest(ProjectTest):\n    def test_generate_standalone_spider(self):\n        self.call(\"genspider\", \"example\", \"example.com\")\n        assert Path(self.temp_path, \"example.py\").exists()\n\n    def test_same_name_as_existing_file(self, force=False):\n        file_name = \"example\"\n        file_path = Path(self.temp_path, file_name + \".py\")\n        p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n        self.assertIn(f\"Created spider {file_name!r} using template 'basic' \", out)\n        assert file_path.exists()\n        modify_time_before = file_path.stat().st_mtime\n        file_contents_before = file_path.read_text(encoding=\"utf-8\")\n\n        if force:\n            # use different template to ensure contents were changed\n            p, out, err = self.proc(\n                \"genspider\", \"--force\", \"-t\", \"crawl\", file_name, \"example.com\"\n            )\n            self.assertIn(f\"Created spider {file_name!r} using template 'crawl' \", out)\n            modify_time_after = file_path.stat().st_mtime\n            self.assertNotEqual(modify_time_after, modify_time_before)\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            self.assertNotEqual(file_contents_after, file_contents_before)\n        else:\n            p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n            self.assertIn(\n                f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\",\n                out,\n            )\n            modify_time_after = file_path.stat().st_mtime\n            self.assertEqual(modify_time_after, modify_time_before)\n            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n            self.assertEqual(file_contents_after, file_contents_before)\n\n    def test_same_name_as_existing_file_force(self):\n        self.test_same_name_as_existing_file(force=True)\n\n\nclass MiscCommandsTest(CommandTest):\n    def test_list(self):\n        self.assertEqual(0, self.call(\"list\"))\n\n\nclass RunSpiderCommandTest(CommandTest):\n    spider_filename = \"myspider.py\"\n\n    debug_log_spider = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug(\"It Works!\")\n        return []\n\"\"\"\n\n    badspider = \"\"\"\nimport scrapy\n\nclass BadSpider(scrapy.Spider):\n    name = \"bad\"\n    def start_requests(self):\n        raise Exception(\"oops!\")\n        \"\"\"\n\n    @contextmanager\n    def _create_file(self, content, name=None) -> Iterator[str]:\n        tmpdir = Path(self.mktemp())\n        tmpdir.mkdir()\n        if name:\n            fname = (tmpdir / name).resolve()\n        else:\n            fname = (tmpdir / self.spider_filename).resolve()\n        fname.write_text(content, encoding=\"utf-8\")\n        try:\n            yield str(fname)\n        finally:\n            rmtree(tmpdir)\n\n    def runspider(self, code, name=None, args=()):\n        with self._create_file(code, name) as fname:\n            return self.proc(\"runspider\", fname, *args)\n\n    def get_log(self, code, name=None, args=()):\n        p, stdout, stderr = self.runspider(code, name, args=args)\n        return stderr\n\n    def test_runspider(self):\n        log = self.get_log(self.debug_log_spider)\n        self.assertIn(\"DEBUG: It Works!\", log)\n        self.assertIn(\"INFO: Spider opened\", log)\n        self.assertIn(\"INFO: Closing spider (finished)\", log)\n        self.assertIn(\"INFO: Spider closed (finished)\", log)\n\n    def test_run_fail_spider(self):\n        proc, _, _ = self.runspider(\n            \"import scrapy\\n\" + inspect.getsource(ExceptionSpider)\n        )\n        ret = proc.returncode\n        self.assertNotEqual(ret, 0)\n\n    def test_run_good_spider(self):\n        proc, _, _ = self.runspider(\n            \"import scrapy\\n\" + inspect.getsource(NoRequestsSpider)\n        )\n        ret = proc.returncode\n        self.assertEqual(ret, 0)\n\n    def test_runspider_log_level(self):\n        log = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_LEVEL=INFO\"))\n        self.assertNotIn(\"DEBUG: It Works!\", log)\n        self.assertIn(\"INFO: Spider opened\", log)\n\n    def test_runspider_dnscache_disabled(self):\n        # see https://github.com/scrapy/scrapy/issues/2811\n        # The spider below should not be able to connect to localhost:12345,\n        # which is intended,\n        # but this should not be because of DNS lookup error\n        # assumption: localhost will resolve in all cases (true?)\n        dnscache_spider = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n    start_urls = ['http://localhost:12345']\n\n    def parse(self, response):\n        return {'test': 'value'}\n\"\"\"\n        log = self.get_log(dnscache_spider, args=(\"-s\", \"DNSCACHE_ENABLED=False\"))\n        self.assertNotIn(\"DNSLookupError\", log)\n        self.assertIn(\"INFO: Spider opened\", log)\n\n    def test_runspider_log_short_names(self):\n        log1 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=1\"))\n        self.assertIn(\"[myspider] DEBUG: It Works!\", log1)\n        self.assertIn(\"[scrapy]\", log1)\n        self.assertNotIn(\"[scrapy.core.engine]\", log1)\n\n        log2 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=0\"))\n        self.assertIn(\"[myspider] DEBUG: It Works!\", log2)\n        self.assertNotIn(\"[scrapy]\", log2)\n        self.assertIn(\"[scrapy.core.engine]\", log2)\n\n    def test_runspider_no_spider_found(self):\n        log = self.get_log(\"from scrapy.spiders import Spider\\n\")\n        self.assertIn(\"No spider found in file\", log)\n\n    def test_runspider_file_not_found(self):\n        _, _, log = self.proc(\"runspider\", \"some_non_existent_file\")\n        self.assertIn(\"File not found: some_non_existent_file\", log)\n\n    def test_runspider_unable_to_load(self):\n        log = self.get_log(\"\", name=\"myspider.txt\")\n        self.assertIn(\"Unable to load\", log)\n\n    def test_start_requests_errors(self):\n        log = self.get_log(self.badspider, name=\"badspider.py\")\n        self.assertIn(\"start_requests\", log)\n        self.assertIn(\"badspider.py\", log)\n\n    def test_asyncio_enabled_true(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\n                \"-s\",\n                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            ],\n        )\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_asyncio_enabled_default(self):\n        log = self.get_log(self.debug_log_spider, args=[])\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_asyncio_enabled_false(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\"-s\", \"TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor\"],\n        )\n        self.assertIn(\n            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n        )\n        self.assertNotIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    @mark.requires_uvloop\n    def test_custom_asyncio_loop_enabled_true(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\n                \"-s\",\n                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                \"-s\",\n                \"ASYNCIO_EVENT_LOOP=uvloop.Loop\",\n            ],\n        )\n        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n\n    def test_custom_asyncio_loop_enabled_false(self):\n        log = self.get_log(\n            self.debug_log_spider,\n            args=[\n                \"-s\",\n                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            ],\n        )\n        import asyncio\n\n        if sys.platform != \"win32\":\n            loop = asyncio.new_event_loop()\n        else:\n            loop = asyncio.SelectorEventLoop()\n        self.assertIn(\n            f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\",\n            log,\n        )\n\n    def test_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n        return []\n\"\"\"\n        args = [\"-o\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log\n        )\n\n    def test_overwrite_output(self):\n        spider_code = \"\"\"\nimport json\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug(\n            'FEEDS: {}'.format(\n                json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n            )\n        )\n        return []\n\"\"\"\n        Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n        args = [\"-O\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}',\n            log,\n        )\n        with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n            first_line = f2.readline()\n        self.assertNotEqual(first_line, \"not empty\")\n\n    def test_output_and_overwrite_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        return []\n\"\"\"\n        args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            \"error: Please use only one of -o/--output and -O/--overwrite-output\", log\n        )\n\n    def test_output_stdout(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n        return []\n\"\"\"\n        args = [\"-o\", \"-:json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\", log)\n\n    @skipIf(platform.system() == \"Windows\", reason=\"Linux only\")\n    def test_absolute_path_linux(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    start_urls = [\"data:,\"]\n\n    def parse(self, response):\n        yield {\"hello\": \"world\"}\n        \"\"\"\n        temp_dir = mkdtemp()\n\n        args = [\"-o\", f\"{temp_dir}/output1.json:json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output1.json\",\n            log,\n        )\n\n        args = [\"-o\", f\"{temp_dir}/output2.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output2.json\",\n            log,\n        )\n\n    @skipIf(platform.system() != \"Windows\", reason=\"Windows only\")\n    def test_absolute_path_windows(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    start_urls = [\"data:,\"]\n\n    def parse(self, response):\n        yield {\"hello\": \"world\"}\n        \"\"\"\n        temp_dir = mkdtemp()\n\n        args = [\"-o\", f\"{temp_dir}\\\\output1.json:json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output1.json\",\n            log,\n        )\n\n        args = [\"-o\", f\"{temp_dir}\\\\output2.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output2.json\",\n            log,\n        )\n\n    def test_args_change_settings(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n        return spider\n\n    def start_requests(self):\n        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n        return []\n\"\"\"\n        args = [\"-a\", \"foo=42\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\"The value of FOO is 42\", log)\n\n\nclass WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n    spider_filename = \"myspider.pyw\"\n\n    def setUp(self):\n        if platform.system() != \"Windows\":\n            raise unittest.SkipTest(\"Windows required for .pyw files\")\n        return super().setUp()\n\n    def test_start_requests_errors(self):\n        log = self.get_log(self.badspider, name=\"badspider.pyw\")\n        self.assertIn(\"start_requests\", log)\n        self.assertIn(\"badspider.pyw\", log)\n\n    def test_runspider_unable_to_load(self):\n        raise unittest.SkipTest(\"Already Tested in 'RunSpiderCommandTest' \")\n\n\nclass BenchCommandTest(CommandTest):\n    def test_run(self):\n        _, _, log = self.proc(\n            \"bench\", \"-s\", \"LOGSTATS_INTERVAL=0.001\", \"-s\", \"CLOSESPIDER_TIMEOUT=0.01\"\n        )\n        self.assertIn(\"INFO: Crawled\", log)\n        self.assertNotIn(\"Unhandled Error\", log)\n\n\nclass ViewCommandTest(CommandTest):\n    def test_methods(self):\n        command = view.Command()\n        command.settings = Settings()\n        parser = argparse.ArgumentParser(\n            prog=\"scrapy\",\n            prefix_chars=\"-\",\n            formatter_class=ScrapyHelpFormatter,\n            conflict_handler=\"resolve\",\n        )\n        command.add_options(parser)\n        self.assertEqual(command.short_desc(), \"Open URL in browser, as seen by Scrapy\")\n        self.assertIn(\n            \"URL using the Scrapy downloader and show its\", command.long_desc()\n        )\n\n\nclass CrawlCommandTest(CommandTest):\n    def crawl(self, code, args=()):\n        Path(self.proj_mod_path, \"spiders\", \"myspider.py\").write_text(\n            code, encoding=\"utf-8\"\n        )\n        return self.proc(\"crawl\", \"myspider\", *args)\n\n    def get_log(self, code, args=()):\n        _, _, stderr = self.crawl(code, args=args)\n        return stderr\n\n    def test_no_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug('It works!')\n        return []\n\"\"\"\n        log = self.get_log(spider_code)\n        self.assertIn(\"[myspider] DEBUG: It works!\", log)\n\n    def test_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n        return []\n\"\"\"\n        args = [\"-o\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log\n        )\n\n    def test_overwrite_output(self):\n        spider_code = \"\"\"\nimport json\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        self.logger.debug(\n            'FEEDS: {}'.format(\n                json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n            )\n        )\n        return []\n\"\"\"\n        Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n        args = [\"-O\", \"example.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}',\n            log,\n        )\n        with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n            first_line = f2.readline()\n        self.assertNotEqual(first_line, \"not empty\")\n\n    def test_output_and_overwrite_output(self):\n        spider_code = \"\"\"\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        return []\n\"\"\"\n        args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n        log = self.get_log(spider_code, args=args)\n        self.assertIn(\n            \"error: Please use only one of -o/--output and -O/--overwrite-output\", log\n        )\n\n\nclass HelpMessageTest(CommandTest):\n    def setUp(self):\n        super().setUp()\n        self.commands = [\n            \"parse\",\n            \"startproject\",\n            \"view\",\n            \"crawl\",\n            \"edit\",\n            \"list\",\n            \"fetch\",\n            \"settings\",\n            \"shell\",\n            \"runspider\",\n            \"version\",\n            \"genspider\",\n            \"check\",\n            \"bench\",\n        ]\n\n    def test_help_messages(self):\n        for command in self.commands:\n            _, out, _ = self.proc(command, \"-h\")\n            self.assertIn(\"Usage\", out)\n", "tests/test_squeues.py": "import pickle\nimport sys\n\nfrom queuelib.tests import test_queue as t\n\nfrom scrapy.http import Request\nfrom scrapy.item import Field, Item\nfrom scrapy.loader import ItemLoader\nfrom scrapy.selector import Selector\nfrom scrapy.squeues import (\n    _MarshalFifoSerializationDiskQueue,\n    _MarshalLifoSerializationDiskQueue,\n    _PickleFifoSerializationDiskQueue,\n    _PickleLifoSerializationDiskQueue,\n)\n\n\nclass TestItem(Item):\n    name = Field()\n\n\ndef _test_procesor(x):\n    return x + x\n\n\nclass TestLoader(ItemLoader):\n    default_item_class = TestItem\n    name_out = staticmethod(_test_procesor)\n\n\ndef nonserializable_object_test(self):\n    q = self.queue()\n    self.assertRaises(ValueError, q.push, lambda x: x)\n    # Selectors should fail (lxml.html.HtmlElement objects can't be pickled)\n    sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n    self.assertRaises(ValueError, q.push, sel)\n\n\nclass FifoDiskQueueTestMixin:\n    def test_serialize(self):\n        q = self.queue()\n        q.push(\"a\")\n        q.push(123)\n        q.push({\"a\": \"dict\"})\n        self.assertEqual(q.pop(), \"a\")\n        self.assertEqual(q.pop(), 123)\n        self.assertEqual(q.pop(), {\"a\": \"dict\"})\n\n    test_nonserializable_object = nonserializable_object_test\n\n\nclass MarshalFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n    chunksize = 100000\n\n    def queue(self):\n        return _MarshalFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)\n\n\nclass ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 1\n\n\nclass ChunkSize2MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 2\n\n\nclass ChunkSize3MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 3\n\n\nclass ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n    chunksize = 4\n\n\nclass PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n    chunksize = 100000\n\n    def queue(self):\n        return _PickleFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)\n\n    def test_serialize_item(self):\n        q = self.queue()\n        i = TestItem(name=\"foo\")\n        q.push(i)\n        i2 = q.pop()\n        assert isinstance(i2, TestItem)\n        self.assertEqual(i, i2)\n\n    def test_serialize_loader(self):\n        q = self.queue()\n        loader = TestLoader()\n        q.push(loader)\n        loader2 = q.pop()\n        assert isinstance(loader2, TestLoader)\n        assert loader2.default_item_class is TestItem\n        self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n\n    def test_serialize_request_recursive(self):\n        q = self.queue()\n        r = Request(\"http://www.example.com\")\n        r.meta[\"request\"] = r\n        q.push(r)\n        r2 = q.pop()\n        assert isinstance(r2, Request)\n        self.assertEqual(r.url, r2.url)\n        assert r2.meta[\"request\"] is r2\n\n    def test_non_pickable_object(self):\n        q = self.queue()\n        try:\n            q.push(lambda x: x)\n        except ValueError as exc:\n            if hasattr(sys, \"pypy_version_info\"):\n                self.assertIsInstance(exc.__context__, pickle.PicklingError)\n            else:\n                self.assertIsInstance(exc.__context__, AttributeError)\n        sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n        try:\n            q.push(sel)\n        except ValueError as exc:\n            self.assertIsInstance(exc.__context__, TypeError)\n\n\nclass ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 1\n\n\nclass ChunkSize2PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 2\n\n\nclass ChunkSize3PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 3\n\n\nclass ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n    chunksize = 4\n\n\nclass LifoDiskQueueTestMixin:\n    def test_serialize(self):\n        q = self.queue()\n        q.push(\"a\")\n        q.push(123)\n        q.push({\"a\": \"dict\"})\n        self.assertEqual(q.pop(), {\"a\": \"dict\"})\n        self.assertEqual(q.pop(), 123)\n        self.assertEqual(q.pop(), \"a\")\n\n    test_nonserializable_object = nonserializable_object_test\n\n\nclass MarshalLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n    def queue(self):\n        return _MarshalLifoSerializationDiskQueue(self.qpath)\n\n\nclass PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n    def queue(self):\n        return _PickleLifoSerializationDiskQueue(self.qpath)\n\n    def test_serialize_item(self):\n        q = self.queue()\n        i = TestItem(name=\"foo\")\n        q.push(i)\n        i2 = q.pop()\n        assert isinstance(i2, TestItem)\n        self.assertEqual(i, i2)\n\n    def test_serialize_loader(self):\n        q = self.queue()\n        loader = TestLoader()\n        q.push(loader)\n        loader2 = q.pop()\n        assert isinstance(loader2, TestLoader)\n        assert loader2.default_item_class is TestItem\n        self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n\n    def test_serialize_request_recursive(self):\n        q = self.queue()\n        r = Request(\"http://www.example.com\")\n        r.meta[\"request\"] = r\n        q.push(r)\n        r2 = q.pop()\n        assert isinstance(r2, Request)\n        self.assertEqual(r.url, r2.url)\n        assert r2.meta[\"request\"] is r2\n", "tests/test_squeues_request.py": "import shutil\nimport tempfile\nimport unittest\n\nimport queuelib\n\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.squeues import (\n    FifoMemoryQueue,\n    LifoMemoryQueue,\n    MarshalFifoDiskQueue,\n    MarshalLifoDiskQueue,\n    PickleFifoDiskQueue,\n    PickleLifoDiskQueue,\n)\nfrom scrapy.utils.test import get_crawler\n\n\"\"\"\nQueues that handle requests\n\"\"\"\n\n\nclass BaseQueueTestCase(unittest.TestCase):\n    def setUp(self):\n        self.tmpdir = tempfile.mkdtemp(prefix=\"scrapy-queue-tests-\")\n        self.qpath = self.tempfilename()\n        self.qdir = tempfile.mkdtemp()\n        self.crawler = get_crawler(Spider)\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpdir)\n\n    def tempfilename(self):\n        with tempfile.NamedTemporaryFile(dir=self.tmpdir) as nf:\n            return nf.name\n\n    def mkdtemp(self):\n        return tempfile.mkdtemp(dir=self.tmpdir)\n\n\nclass RequestQueueTestMixin:\n    def queue(self):\n        raise NotImplementedError()\n\n    def test_one_element_with_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n        q = self.queue()\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.peek())\n        self.assertIsNone(q.pop())\n        req = Request(\"http://www.example.com\")\n        q.push(req)\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q.peek().url, req.url)\n        self.assertEqual(q.pop().url, req.url)\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.peek())\n        self.assertIsNone(q.pop())\n        q.close()\n\n    def test_one_element_without_peek(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"The queuelib queues define peek\")\n        q = self.queue()\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.pop())\n        req = Request(\"http://www.example.com\")\n        q.push(req)\n        self.assertEqual(len(q), 1)\n        with self.assertRaises(\n            NotImplementedError,\n            msg=\"The underlying queue class does not implement 'peek'\",\n        ):\n            q.peek()\n        self.assertEqual(q.pop().url, req.url)\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.pop())\n        q.close()\n\n\nclass FifoQueueMixin(RequestQueueTestMixin):\n    def test_fifo_with_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n        q = self.queue()\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.peek())\n        self.assertIsNone(q.pop())\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        q.push(req1)\n        q.push(req2)\n        q.push(req3)\n        self.assertEqual(len(q), 3)\n        self.assertEqual(q.peek().url, req1.url)\n        self.assertEqual(q.pop().url, req1.url)\n        self.assertEqual(len(q), 2)\n        self.assertEqual(q.peek().url, req2.url)\n        self.assertEqual(q.pop().url, req2.url)\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q.peek().url, req3.url)\n        self.assertEqual(q.pop().url, req3.url)\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.peek())\n        self.assertIsNone(q.pop())\n        q.close()\n\n    def test_fifo_without_peek(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n        q = self.queue()\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.pop())\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        q.push(req1)\n        q.push(req2)\n        q.push(req3)\n        with self.assertRaises(\n            NotImplementedError,\n            msg=\"The underlying queue class does not implement 'peek'\",\n        ):\n            q.peek()\n        self.assertEqual(len(q), 3)\n        self.assertEqual(q.pop().url, req1.url)\n        self.assertEqual(len(q), 2)\n        self.assertEqual(q.pop().url, req2.url)\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q.pop().url, req3.url)\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.pop())\n        q.close()\n\n\nclass LifoQueueMixin(RequestQueueTestMixin):\n    def test_lifo_with_peek(self):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n        q = self.queue()\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.peek())\n        self.assertIsNone(q.pop())\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        q.push(req1)\n        q.push(req2)\n        q.push(req3)\n        self.assertEqual(len(q), 3)\n        self.assertEqual(q.peek().url, req3.url)\n        self.assertEqual(q.pop().url, req3.url)\n        self.assertEqual(len(q), 2)\n        self.assertEqual(q.peek().url, req2.url)\n        self.assertEqual(q.pop().url, req2.url)\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q.peek().url, req1.url)\n        self.assertEqual(q.pop().url, req1.url)\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.peek())\n        self.assertIsNone(q.pop())\n        q.close()\n\n    def test_lifo_without_peek(self):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n        q = self.queue()\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.pop())\n        req1 = Request(\"http://www.example.com/1\")\n        req2 = Request(\"http://www.example.com/2\")\n        req3 = Request(\"http://www.example.com/3\")\n        q.push(req1)\n        q.push(req2)\n        q.push(req3)\n        with self.assertRaises(\n            NotImplementedError,\n            msg=\"The underlying queue class does not implement 'peek'\",\n        ):\n            q.peek()\n        self.assertEqual(len(q), 3)\n        self.assertEqual(q.pop().url, req3.url)\n        self.assertEqual(len(q), 2)\n        self.assertEqual(q.pop().url, req2.url)\n        self.assertEqual(len(q), 1)\n        self.assertEqual(q.pop().url, req1.url)\n        self.assertEqual(len(q), 0)\n        self.assertIsNone(q.pop())\n        q.close()\n\n\nclass PickleFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n    def queue(self):\n        return PickleFifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/fifo\")\n\n\nclass PickleLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n    def queue(self):\n        return PickleLifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/lifo\")\n\n\nclass MarshalFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n    def queue(self):\n        return MarshalFifoDiskQueue.from_crawler(\n            crawler=self.crawler, key=\"marshal/fifo\"\n        )\n\n\nclass MarshalLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n    def queue(self):\n        return MarshalLifoDiskQueue.from_crawler(\n            crawler=self.crawler, key=\"marshal/lifo\"\n        )\n\n\nclass FifoMemoryQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n    def queue(self):\n        return FifoMemoryQueue.from_crawler(crawler=self.crawler)\n\n\nclass LifoMemoryQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n    def queue(self):\n        return LifoMemoryQueue.from_crawler(crawler=self.crawler)\n", "tests/test_downloadermiddleware_httpcompression.py": "from gzip import GzipFile\nfrom io import BytesIO\nfrom logging import WARNING\nfrom pathlib import Path\nfrom unittest import SkipTest, TestCase\nfrom warnings import catch_warnings\n\nfrom testfixtures import LogCapture\nfrom w3lib.encoding import resolve_encoding\n\nfrom scrapy.downloadermiddlewares.httpcompression import (\n    ACCEPTED_ENCODINGS,\n    HttpCompressionMiddleware,\n)\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.gz import gunzip\nfrom scrapy.utils.test import get_crawler\nfrom tests import tests_datadir\n\nSAMPLEDIR = Path(tests_datadir, \"compressed\")\n\nFORMAT = {\n    \"gzip\": (\"html-gzip.bin\", \"gzip\"),\n    \"x-gzip\": (\"html-gzip.bin\", \"gzip\"),\n    \"rawdeflate\": (\"html-rawdeflate.bin\", \"deflate\"),\n    \"zlibdeflate\": (\"html-zlibdeflate.bin\", \"deflate\"),\n    \"gzip-deflate\": (\"html-gzip-deflate.bin\", \"gzip, deflate\"),\n    \"gzip-deflate-gzip\": (\"html-gzip-deflate-gzip.bin\", \"gzip, deflate, gzip\"),\n    \"br\": (\"html-br.bin\", \"br\"),\n    # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin\n    \"zstd-static-content-size\": (\"html-zstd-static-content-size.bin\", \"zstd\"),\n    # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin\n    \"zstd-static-no-content-size\": (\"html-zstd-static-no-content-size.bin\", \"zstd\"),\n    # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin\n    \"zstd-streaming-no-content-size\": (\n        \"html-zstd-streaming-no-content-size.bin\",\n        \"zstd\",\n    ),\n    **{\n        f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)\n        for format_id in (\n            \"br\",  # 34 \u2192 11 511 612\n            \"deflate\",  # 27 968 \u2192 11 511 612\n            \"gzip\",  # 27 988 \u2192 11 511 612\n            \"zstd\",  # 1 096 \u2192 11 511 612\n        )\n    },\n}\n\n\nclass HttpCompressionTest(TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"scrapytest.org\")\n        self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n        self.crawler.stats.open_spider(self.spider)\n\n    def _getresponse(self, coding):\n        if coding not in FORMAT:\n            raise ValueError()\n\n        samplefile, contentencoding = FORMAT[coding]\n\n        body = (SAMPLEDIR / samplefile).read_bytes()\n\n        headers = {\n            \"Server\": \"Yaws/1.49 Yet Another Web Server\",\n            \"Date\": \"Sun, 08 Mar 2009 00:41:03 GMT\",\n            \"Content-Length\": len(body),\n            \"Content-Type\": \"text/html\",\n            \"Content-Encoding\": contentencoding,\n        }\n\n        response = Response(\"http://scrapytest.org/\", body=body, headers=headers)\n        response.request = Request(\n            \"http://scrapytest.org\", headers={\"Accept-Encoding\": \"gzip, deflate\"}\n        )\n        return response\n\n    def assertStatsEqual(self, key, value):\n        self.assertEqual(\n            self.crawler.stats.get_value(key, spider=self.spider),\n            value,\n            str(self.crawler.stats.get_stats(self.spider)),\n        )\n\n    def test_setting_false_compression_enabled(self):\n        self.assertRaises(\n            NotConfigured,\n            HttpCompressionMiddleware.from_crawler,\n            get_crawler(settings_dict={\"COMPRESSION_ENABLED\": False}),\n        )\n\n    def test_setting_default_compression_enabled(self):\n        self.assertIsInstance(\n            HttpCompressionMiddleware.from_crawler(get_crawler()),\n            HttpCompressionMiddleware,\n        )\n\n    def test_setting_true_compression_enabled(self):\n        self.assertIsInstance(\n            HttpCompressionMiddleware.from_crawler(\n                get_crawler(settings_dict={\"COMPRESSION_ENABLED\": True})\n            ),\n            HttpCompressionMiddleware,\n        )\n\n    def test_process_request(self):\n        request = Request(\"http://scrapytest.org\")\n        assert \"Accept-Encoding\" not in request.headers\n        self.mw.process_request(request, self.spider)\n        self.assertEqual(\n            request.headers.get(\"Accept-Encoding\"), b\", \".join(ACCEPTED_ENCODINGS)\n        )\n\n    def test_process_response_gzip(self):\n        response = self._getresponse(\"gzip\")\n        request = response.request\n\n        self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        response = self._getresponse(\"br\")\n        request = response.request\n        self.assertEqual(response.headers[\"Content-Encoding\"], b\"br\")\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        raw_content = None\n        for check_key in FORMAT:\n            if not check_key.startswith(\"zstd-\"):\n                continue\n            response = self._getresponse(check_key)\n            request = response.request\n            self.assertEqual(response.headers[\"Content-Encoding\"], b\"zstd\")\n            newresponse = self.mw.process_response(request, response, self.spider)\n            if raw_content is None:\n                raw_content = newresponse.body\n            else:\n                assert raw_content == newresponse.body\n            assert newresponse is not response\n            assert newresponse.body.startswith(b\"<!DOCTYPE\")\n            assert \"Content-Encoding\" not in newresponse.headers\n\n    def test_process_response_rawdeflate(self):\n        response = self._getresponse(\"rawdeflate\")\n        request = response.request\n\n        self.assertEqual(response.headers[\"Content-Encoding\"], b\"deflate\")\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74840)\n\n    def test_process_response_zlibdelate(self):\n        response = self._getresponse(\"zlibdeflate\")\n        request = response.request\n\n        self.assertEqual(response.headers[\"Content-Encoding\"], b\"deflate\")\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        assert \"Content-Encoding\" not in newresponse.headers\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74840)\n\n    def test_process_response_plain(self):\n        response = Response(\"http://scrapytest.org\", body=b\"<!DOCTYPE...\")\n        request = Request(\"http://scrapytest.org\")\n\n        assert not response.headers.get(\"Content-Encoding\")\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is response\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n        self.assertStatsEqual(\"httpcompression/response_count\", None)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n\n    def test_multipleencodings(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Encoding\"] = [\"uuencode\", \"gzip\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        self.assertEqual(newresponse.headers.getlist(\"Content-Encoding\"), [b\"uuencode\"])\n\n    def test_multi_compression_single_header(self):\n        response = self._getresponse(\"gzip-deflate\")\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert \"Content-Encoding\" not in newresponse.headers\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n    def test_multi_compression_single_header_invalid_compression(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [b\"gzip, foo, deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        self.assertEqual(\n            newresponse.headers.getlist(\"Content-Encoding\"), [b\"gzip\", b\"foo\"]\n        )\n\n    def test_multi_compression_multiple_header(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert \"Content-Encoding\" not in newresponse.headers\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n    def test_multi_compression_multiple_header_invalid_compression(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"foo\", \"deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        self.assertEqual(\n            newresponse.headers.getlist(\"Content-Encoding\"), [b\"gzip\", b\"foo\"]\n        )\n\n    def test_multi_compression_single_and_multiple_header(self):\n        response = self._getresponse(\"gzip-deflate-gzip\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"deflate, gzip\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        assert \"Content-Encoding\" not in newresponse.headers\n        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n    def test_multi_compression_single_and_multiple_header_invalid_compression(self):\n        response = self._getresponse(\"gzip-deflate\")\n        response.headers[\"Content-Encoding\"] = [\"gzip\", \"foo,deflate\"]\n        request = response.request\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert newresponse is not response\n        self.assertEqual(\n            newresponse.headers.getlist(\"Content-Encoding\"), [b\"gzip\", b\"foo\"]\n        )\n\n    def test_process_response_encoding_inside_body(self):\n        headers = {\n            \"Content-Type\": \"text/html\",\n            \"Content-Encoding\": \"gzip\",\n        }\n        f = BytesIO()\n        plainbody = (\n            b\"<html><head><title>Some page</title>\"\n            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n        )\n        zf = GzipFile(fileobj=f, mode=\"wb\")\n        zf.write(plainbody)\n        zf.close()\n        response = Response(\n            \"http;//www.example.com/\", headers=headers, body=f.getvalue()\n        )\n        request = Request(\"http://www.example.com/\")\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert isinstance(newresponse, HtmlResponse)\n        self.assertEqual(newresponse.body, plainbody)\n        self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n\n    def test_process_response_force_recalculate_encoding(self):\n        headers = {\n            \"Content-Type\": \"text/html\",\n            \"Content-Encoding\": \"gzip\",\n        }\n        f = BytesIO()\n        plainbody = (\n            b\"<html><head><title>Some page</title>\"\n            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n        )\n        zf = GzipFile(fileobj=f, mode=\"wb\")\n        zf.write(plainbody)\n        zf.close()\n        response = HtmlResponse(\n            \"http;//www.example.com/page.html\", headers=headers, body=f.getvalue()\n        )\n        request = Request(\"http://www.example.com/\")\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert isinstance(newresponse, HtmlResponse)\n        self.assertEqual(newresponse.body, plainbody)\n        self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n\n    def test_process_response_no_content_type_header(self):\n        headers = {\n            \"Content-Encoding\": \"identity\",\n        }\n        plainbody = (\n            b\"<html><head><title>Some page</title>\"\n            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n        )\n        respcls = responsetypes.from_args(\n            url=\"http://www.example.com/index\", headers=headers, body=plainbody\n        )\n        response = respcls(\n            \"http://www.example.com/index\", headers=headers, body=plainbody\n        )\n        request = Request(\"http://www.example.com/index\")\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        assert isinstance(newresponse, respcls)\n        self.assertEqual(newresponse.body, plainbody)\n        self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n\n    def test_process_response_gzipped_contenttype(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Type\"] = \"application/gzip\"\n        request = response.request\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        self.assertIsNot(newresponse, response)\n        self.assertTrue(newresponse.body.startswith(b\"<!DOCTYPE\"))\n        self.assertNotIn(\"Content-Encoding\", newresponse.headers)\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_gzip_app_octetstream_contenttype(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Type\"] = \"application/octet-stream\"\n        request = response.request\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        self.assertIsNot(newresponse, response)\n        self.assertTrue(newresponse.body.startswith(b\"<!DOCTYPE\"))\n        self.assertNotIn(\"Content-Encoding\", newresponse.headers)\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_gzip_binary_octetstream_contenttype(self):\n        response = self._getresponse(\"x-gzip\")\n        response.headers[\"Content-Type\"] = \"binary/octet-stream\"\n        request = response.request\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        self.assertIsNot(newresponse, response)\n        self.assertTrue(newresponse.body.startswith(b\"<!DOCTYPE\"))\n        self.assertNotIn(\"Content-Encoding\", newresponse.headers)\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n\n    def test_process_response_gzipped_gzip_file(self):\n        \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n        only once by the middleware, leaving gunzipping of the file\n        to upper layers.\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/gzip\",\n            \"Content-Encoding\": \"gzip\",\n        }\n        # build a gzipped file (here, a sitemap)\n        f = BytesIO()\n        plainbody = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n  <url>\n    <loc>http://www.example.com/</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>daily</changefreq>\n    <priority>1</priority>\n  </url>\n  <url>\n    <loc>http://www.example.com/Special-Offers.html</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>weekly</changefreq>\n    <priority>0.8</priority>\n  </url>\n</urlset>\"\"\"\n        gz_file = GzipFile(fileobj=f, mode=\"wb\")\n        gz_file.write(plainbody)\n        gz_file.close()\n\n        # build a gzipped response body containing this gzipped file\n        r = BytesIO()\n        gz_resp = GzipFile(fileobj=r, mode=\"wb\")\n        gz_resp.write(f.getvalue())\n        gz_resp.close()\n\n        response = Response(\n            \"http;//www.example.com/\", headers=headers, body=r.getvalue()\n        )\n        request = Request(\"http://www.example.com/\")\n\n        newresponse = self.mw.process_response(request, response, self.spider)\n        self.assertEqual(gunzip(newresponse.body), plainbody)\n        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", 230)\n\n    def test_process_response_head_request_no_decode_required(self):\n        response = self._getresponse(\"gzip\")\n        response.headers[\"Content-Type\"] = \"application/gzip\"\n        request = response.request\n        request.method = \"HEAD\"\n        response = response.replace(body=None)\n        newresponse = self.mw.process_response(request, response, self.spider)\n        self.assertIs(newresponse, response)\n        self.assertEqual(response.body, b\"\")\n        self.assertStatsEqual(\"httpcompression/response_count\", None)\n        self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n\n    def _test_compression_bomb_setting(self, compression_id):\n        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n        crawler = get_crawler(Spider, settings_dict=settings)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        self.assertRaises(\n            IgnoreRequest,\n            mw.process_response,\n            response.request,\n            response,\n            spider,\n        )\n\n    def test_compression_bomb_setting_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        self._test_compression_bomb_setting(\"br\")\n\n    def test_compression_bomb_setting_deflate(self):\n        self._test_compression_bomb_setting(\"deflate\")\n\n    def test_compression_bomb_setting_gzip(self):\n        self._test_compression_bomb_setting(\"gzip\")\n\n    def test_compression_bomb_setting_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        self._test_compression_bomb_setting(\"zstd\")\n\n    def _test_compression_bomb_spider_attr(self, compression_id):\n        class DownloadMaxSizeSpider(Spider):\n            download_maxsize = 10_000_000\n\n        crawler = get_crawler(DownloadMaxSizeSpider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        self.assertRaises(\n            IgnoreRequest,\n            mw.process_response,\n            response.request,\n            response,\n            spider,\n        )\n\n    def test_compression_bomb_spider_attr_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        self._test_compression_bomb_spider_attr(\"br\")\n\n    def test_compression_bomb_spider_attr_deflate(self):\n        self._test_compression_bomb_spider_attr(\"deflate\")\n\n    def test_compression_bomb_spider_attr_gzip(self):\n        self._test_compression_bomb_spider_attr(\"gzip\")\n\n    def test_compression_bomb_spider_attr_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        self._test_compression_bomb_spider_attr(\"zstd\")\n\n    def _test_compression_bomb_request_meta(self, compression_id):\n        crawler = get_crawler(Spider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        response.meta[\"download_maxsize\"] = 10_000_000\n        self.assertRaises(\n            IgnoreRequest,\n            mw.process_response,\n            response.request,\n            response,\n            spider,\n        )\n\n    def test_compression_bomb_request_meta_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        self._test_compression_bomb_request_meta(\"br\")\n\n    def test_compression_bomb_request_meta_deflate(self):\n        self._test_compression_bomb_request_meta(\"deflate\")\n\n    def test_compression_bomb_request_meta_gzip(self):\n        self._test_compression_bomb_request_meta(\"gzip\")\n\n    def test_compression_bomb_request_meta_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        self._test_compression_bomb_request_meta(\"zstd\")\n\n    def _test_download_warnsize_setting(self, compression_id):\n        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n        crawler = get_crawler(Spider, settings_dict=settings)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n        response = self._getresponse(f\"bomb-{compression_id}\")\n\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            mw.process_response(response.request, response, spider)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"<200 http://scrapytest.org/> body size after \"\n                    \"decompression (11511612 B) is larger than the download \"\n                    \"warning size (10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_setting_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        self._test_download_warnsize_setting(\"br\")\n\n    def test_download_warnsize_setting_deflate(self):\n        self._test_download_warnsize_setting(\"deflate\")\n\n    def test_download_warnsize_setting_gzip(self):\n        self._test_download_warnsize_setting(\"gzip\")\n\n    def test_download_warnsize_setting_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        self._test_download_warnsize_setting(\"zstd\")\n\n    def _test_download_warnsize_spider_attr(self, compression_id):\n        class DownloadWarnSizeSpider(Spider):\n            download_warnsize = 10_000_000\n\n        crawler = get_crawler(DownloadWarnSizeSpider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n        response = self._getresponse(f\"bomb-{compression_id}\")\n\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            mw.process_response(response.request, response, spider)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"<200 http://scrapytest.org/> body size after \"\n                    \"decompression (11511612 B) is larger than the download \"\n                    \"warning size (10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_spider_attr_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        self._test_download_warnsize_spider_attr(\"br\")\n\n    def test_download_warnsize_spider_attr_deflate(self):\n        self._test_download_warnsize_spider_attr(\"deflate\")\n\n    def test_download_warnsize_spider_attr_gzip(self):\n        self._test_download_warnsize_spider_attr(\"gzip\")\n\n    def test_download_warnsize_spider_attr_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        self._test_download_warnsize_spider_attr(\"zstd\")\n\n    def _test_download_warnsize_request_meta(self, compression_id):\n        crawler = get_crawler(Spider)\n        spider = crawler._create_spider(\"scrapytest.org\")\n        mw = HttpCompressionMiddleware.from_crawler(crawler)\n        mw.open_spider(spider)\n        response = self._getresponse(f\"bomb-{compression_id}\")\n        response.meta[\"download_warnsize\"] = 10_000_000\n\n        with LogCapture(\n            \"scrapy.downloadermiddlewares.httpcompression\",\n            propagate=False,\n            level=WARNING,\n        ) as log:\n            mw.process_response(response.request, response, spider)\n        log.check(\n            (\n                \"scrapy.downloadermiddlewares.httpcompression\",\n                \"WARNING\",\n                (\n                    \"<200 http://scrapytest.org/> body size after \"\n                    \"decompression (11511612 B) is larger than the download \"\n                    \"warning size (10000000 B).\"\n                ),\n            ),\n        )\n\n    def test_download_warnsize_request_meta_br(self):\n        try:\n            try:\n                import brotli  # noqa: F401\n            except ImportError:\n                import brotlicffi  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no brotli\")\n        self._test_download_warnsize_request_meta(\"br\")\n\n    def test_download_warnsize_request_meta_deflate(self):\n        self._test_download_warnsize_request_meta(\"deflate\")\n\n    def test_download_warnsize_request_meta_gzip(self):\n        self._test_download_warnsize_request_meta(\"gzip\")\n\n    def test_download_warnsize_request_meta_zstd(self):\n        try:\n            import zstandard  # noqa: F401\n        except ImportError:\n            raise SkipTest(\"no zstd support (zstandard)\")\n        self._test_download_warnsize_request_meta(\"zstd\")\n\n\nclass HttpCompressionSubclassTest(TestCase):\n    def test_init_missing_stats(self):\n        class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):\n            def __init__(self):\n                super().__init__()\n\n        crawler = get_crawler(Spider)\n        with catch_warnings(record=True) as caught_warnings:\n            HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n        messages = tuple(\n            str(warning.message)\n            for warning in caught_warnings\n            if warning.category is ScrapyDeprecationWarning\n        )\n        self.assertEqual(\n            messages,\n            (\n                (\n                    \"HttpCompressionMiddleware subclasses must either modify \"\n                    \"their '__init__' method to support a 'crawler' parameter \"\n                    \"or reimplement their 'from_crawler' method.\"\n                ),\n            ),\n        )\n", "tests/test_responsetypes.py": "import unittest\n\nfrom scrapy.http import (\n    Headers,\n    HtmlResponse,\n    JsonResponse,\n    Response,\n    TextResponse,\n    XmlResponse,\n)\nfrom scrapy.responsetypes import responsetypes\n\n\nclass ResponseTypesTest(unittest.TestCase):\n    def test_from_filename(self):\n        mappings = [\n            (\"data.bin\", Response),\n            (\"file.txt\", TextResponse),\n            (\"file.xml.gz\", Response),\n            (\"file.xml\", XmlResponse),\n            (\"file.html\", HtmlResponse),\n            (\"file.unknownext\", Response),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_filename(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_content_disposition(self):\n        mappings = [\n            (b'attachment; filename=\"data.xml\"', XmlResponse),\n            (b\"attachment; filename=data.xml\", XmlResponse),\n            (\"attachment;filename=data\u00a3.tar.gz\".encode(), Response),\n            (\"attachment;filename=data\u00b5.tar.gz\".encode(\"latin-1\"), Response),\n            (\"attachment;filename=data\u9ad8.doc\".encode(\"gbk\"), Response),\n            (\"attachment;filename=\u062f\u0648\u0631\u0647data.html\".encode(\"cp720\"), HtmlResponse),\n            (\n                \"attachment;filename=\u65e5\u672c\u8a9e\u7248Wikipedia.xml\".encode(\"iso2022_jp\"),\n                XmlResponse,\n            ),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_content_disposition(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_content_type(self):\n        mappings = [\n            (\"text/html; charset=UTF-8\", HtmlResponse),\n            (\"text/xml; charset=UTF-8\", XmlResponse),\n            (\"application/xhtml+xml; charset=UTF-8\", HtmlResponse),\n            (\"application/vnd.wap.xhtml+xml; charset=utf-8\", HtmlResponse),\n            (\"application/xml; charset=UTF-8\", XmlResponse),\n            (\"application/octet-stream\", Response),\n            (\"application/json; encoding=UTF8;charset=UTF-8\", JsonResponse),\n            (\"application/x-json; encoding=UTF8;charset=UTF-8\", JsonResponse),\n            (\"application/json-amazonui-streaming;charset=UTF-8\", JsonResponse),\n            (b\"application/x-download; filename=\\x80dummy.txt\", Response),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_content_type(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_body(self):\n        mappings = [\n            (b\"\\x03\\x02\\xdf\\xdd\\x23\", Response),\n            (b\"Some plain text\\ndata with tabs\\t and null bytes\\0\", TextResponse),\n            (b\"<html><head><title>Hello</title></head>\", HtmlResponse),\n            # https://codersblock.com/blog/the-smallest-valid-html5-page/\n            (b\"<!DOCTYPE html>\\n<title>.</title>\", HtmlResponse),\n            (b'<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_body(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_headers(self):\n        mappings = [\n            ({\"Content-Type\": [\"text/html; charset=utf-8\"]}, HtmlResponse),\n            (\n                {\n                    \"Content-Type\": [\"text/html; charset=utf-8\"],\n                    \"Content-Encoding\": [\"gzip\"],\n                },\n                Response,\n            ),\n            (\n                {\n                    \"Content-Type\": [\"application/octet-stream\"],\n                    \"Content-Disposition\": [\"attachment; filename=data.txt\"],\n                },\n                TextResponse,\n            ),\n        ]\n        for source, cls in mappings:\n            source = Headers(source)\n            retcls = responsetypes.from_headers(source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_from_args(self):\n        # TODO: add more tests that check precedence between the different arguments\n        mappings = [\n            ({\"url\": \"http://www.example.com/data.csv\"}, TextResponse),\n            # headers takes precedence over url\n            (\n                {\n                    \"headers\": Headers({\"Content-Type\": [\"text/html; charset=utf-8\"]}),\n                    \"url\": \"http://www.example.com/item/\",\n                },\n                HtmlResponse,\n            ),\n            (\n                {\n                    \"headers\": Headers(\n                        {\"Content-Disposition\": ['attachment; filename=\"data.xml.gz\"']}\n                    ),\n                    \"url\": \"http://www.example.com/page/\",\n                },\n                Response,\n            ),\n        ]\n        for source, cls in mappings:\n            retcls = responsetypes.from_args(**source)\n            assert retcls is cls, f\"{source} ==> {retcls} != {cls}\"\n\n    def test_custom_mime_types_loaded(self):\n        # check that mime.types files shipped with scrapy are loaded\n        self.assertEqual(\n            responsetypes.mimetypes.guess_type(\"x.scrapytest\")[0], \"x-scrapy/test\"\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_scheduler.py": "import collections\nimport shutil\nimport tempfile\nimport unittest\nfrom typing import Optional\n\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.core.downloader import Downloader\nfrom scrapy.core.scheduler import Scheduler\nfrom scrapy.crawler import Crawler\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.misc import load_object\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\n\nMockEngine = collections.namedtuple(\"MockEngine\", [\"downloader\"])\nMockSlot = collections.namedtuple(\"MockSlot\", [\"active\"])\n\n\nclass MockDownloader:\n    def __init__(self):\n        self.slots = {}\n\n    def get_slot_key(self, request):\n        if Downloader.DOWNLOAD_SLOT in request.meta:\n            return request.meta[Downloader.DOWNLOAD_SLOT]\n\n        return urlparse_cached(request).hostname or \"\"\n\n    def increment(self, slot_key):\n        slot = self.slots.setdefault(slot_key, MockSlot(active=[]))\n        slot.active.append(1)\n\n    def decrement(self, slot_key):\n        slot = self.slots.get(slot_key)\n        slot.active.pop()\n\n    def close(self):\n        pass\n\n\nclass MockCrawler(Crawler):\n    def __init__(self, priority_queue_cls, jobdir):\n        settings = {\n            \"SCHEDULER_DEBUG\": False,\n            \"SCHEDULER_DISK_QUEUE\": \"scrapy.squeues.PickleLifoDiskQueue\",\n            \"SCHEDULER_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n            \"SCHEDULER_PRIORITY_QUEUE\": priority_queue_cls,\n            \"JOBDIR\": jobdir,\n            \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n        }\n        super().__init__(Spider, settings)\n        self.engine = MockEngine(downloader=MockDownloader())\n        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n\n\nclass SchedulerHandler:\n    priority_queue_cls: Optional[str] = None\n    jobdir = None\n\n    def create_scheduler(self):\n        self.mock_crawler = MockCrawler(self.priority_queue_cls, self.jobdir)\n        self.scheduler = Scheduler.from_crawler(self.mock_crawler)\n        self.spider = Spider(name=\"spider\")\n        self.scheduler.open(self.spider)\n\n    def close_scheduler(self):\n        self.scheduler.close(\"finished\")\n        self.mock_crawler.stop()\n        self.mock_crawler.engine.downloader.close()\n\n    def setUp(self):\n        self.create_scheduler()\n\n    def tearDown(self):\n        self.close_scheduler()\n\n\n_PRIORITIES = [\n    (\"http://foo.com/a\", -2),\n    (\"http://foo.com/d\", 1),\n    (\"http://foo.com/b\", -1),\n    (\"http://foo.com/c\", 0),\n    (\"http://foo.com/e\", 2),\n]\n\n\n_URLS = {\"http://foo.com/a\", \"http://foo.com/b\", \"http://foo.com/c\"}\n\n\nclass BaseSchedulerInMemoryTester(SchedulerHandler):\n    def test_length(self):\n        self.assertFalse(self.scheduler.has_pending_requests())\n        self.assertEqual(len(self.scheduler), 0)\n\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        self.assertTrue(self.scheduler.has_pending_requests())\n        self.assertEqual(len(self.scheduler), len(_URLS))\n\n    def test_dequeue(self):\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        urls = set()\n        while self.scheduler.has_pending_requests():\n            urls.add(self.scheduler.next_request().url)\n\n        self.assertEqual(urls, _URLS)\n\n    def test_dequeue_priorities(self):\n        for url, priority in _PRIORITIES:\n            self.scheduler.enqueue_request(Request(url, priority=priority))\n\n        priorities = []\n        while self.scheduler.has_pending_requests():\n            priorities.append(self.scheduler.next_request().priority)\n\n        self.assertEqual(\n            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n        )\n\n\nclass BaseSchedulerOnDiskTester(SchedulerHandler):\n    def setUp(self):\n        self.jobdir = tempfile.mkdtemp()\n        self.create_scheduler()\n\n    def tearDown(self):\n        self.close_scheduler()\n\n        shutil.rmtree(self.jobdir)\n        self.jobdir = None\n\n    def test_length(self):\n        self.assertFalse(self.scheduler.has_pending_requests())\n        self.assertEqual(len(self.scheduler), 0)\n\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        self.close_scheduler()\n        self.create_scheduler()\n\n        self.assertTrue(self.scheduler.has_pending_requests())\n        self.assertEqual(len(self.scheduler), len(_URLS))\n\n    def test_dequeue(self):\n        for url in _URLS:\n            self.scheduler.enqueue_request(Request(url))\n\n        self.close_scheduler()\n        self.create_scheduler()\n\n        urls = set()\n        while self.scheduler.has_pending_requests():\n            urls.add(self.scheduler.next_request().url)\n\n        self.assertEqual(urls, _URLS)\n\n    def test_dequeue_priorities(self):\n        for url, priority in _PRIORITIES:\n            self.scheduler.enqueue_request(Request(url, priority=priority))\n\n        self.close_scheduler()\n        self.create_scheduler()\n\n        priorities = []\n        while self.scheduler.has_pending_requests():\n            priorities.append(self.scheduler.next_request().priority)\n\n        self.assertEqual(\n            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n        )\n\n\nclass TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n    priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n\n\nclass TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n    priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n\n\n_URLS_WITH_SLOTS = [\n    (\"http://foo.com/a\", \"a\"),\n    (\"http://foo.com/b\", \"a\"),\n    (\"http://foo.com/c\", \"b\"),\n    (\"http://foo.com/d\", \"b\"),\n    (\"http://foo.com/e\", \"c\"),\n    (\"http://foo.com/f\", \"c\"),\n]\n\n\nclass TestMigration(unittest.TestCase):\n    def setUp(self):\n        self.tmpdir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpdir)\n\n    def _migration(self, tmp_dir):\n        prev_scheduler_handler = SchedulerHandler()\n        prev_scheduler_handler.priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n        prev_scheduler_handler.jobdir = tmp_dir\n\n        prev_scheduler_handler.create_scheduler()\n        for url in _URLS:\n            prev_scheduler_handler.scheduler.enqueue_request(Request(url))\n        prev_scheduler_handler.close_scheduler()\n\n        next_scheduler_handler = SchedulerHandler()\n        next_scheduler_handler.priority_queue_cls = (\n            \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n        )\n        next_scheduler_handler.jobdir = tmp_dir\n\n        next_scheduler_handler.create_scheduler()\n\n    def test_migration(self):\n        with self.assertRaises(ValueError):\n            self._migration(self.tmpdir)\n\n\ndef _is_scheduling_fair(enqueued_slots, dequeued_slots):\n    \"\"\"\n    We enqueued same number of requests for every slot.\n    Assert correct order, e.g.\n\n    >>> enqueued = ['a', 'b', 'c'] * 2\n    >>> correct = ['a', 'c', 'b', 'b', 'a', 'c']\n    >>> incorrect = ['a', 'a', 'b', 'c', 'c', 'b']\n    >>> _is_scheduling_fair(enqueued, correct)\n    True\n    >>> _is_scheduling_fair(enqueued, incorrect)\n    False\n    \"\"\"\n    if len(dequeued_slots) != len(enqueued_slots):\n        return False\n\n    slots_number = len(set(enqueued_slots))\n    for i in range(0, len(dequeued_slots), slots_number):\n        part = dequeued_slots[i : i + slots_number]\n        if len(part) != len(set(part)):\n            return False\n\n    return True\n\n\nclass DownloaderAwareSchedulerTestMixin:\n    priority_queue_cls: Optional[str] = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n    reopen = False\n\n    def test_logic(self):\n        for url, slot in _URLS_WITH_SLOTS:\n            request = Request(url)\n            request.meta[Downloader.DOWNLOAD_SLOT] = slot\n            self.scheduler.enqueue_request(request)\n\n        if self.reopen:\n            self.close_scheduler()\n            self.create_scheduler()\n\n        dequeued_slots = []\n        requests = []\n        downloader = self.mock_crawler.engine.downloader\n        while self.scheduler.has_pending_requests():\n            request = self.scheduler.next_request()\n            # pylint: disable=protected-access\n            slot = downloader.get_slot_key(request)\n            dequeued_slots.append(slot)\n            downloader.increment(slot)\n            requests.append(request)\n\n        for request in requests:\n            # pylint: disable=protected-access\n            slot = downloader.get_slot_key(request)\n            downloader.decrement(slot)\n\n        self.assertTrue(\n            _is_scheduling_fair([s for u, s in _URLS_WITH_SLOTS], dequeued_slots)\n        )\n        self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n\n\nclass TestSchedulerWithDownloaderAwareInMemory(\n    DownloaderAwareSchedulerTestMixin, BaseSchedulerInMemoryTester, unittest.TestCase\n):\n    pass\n\n\nclass TestSchedulerWithDownloaderAwareOnDisk(\n    DownloaderAwareSchedulerTestMixin, BaseSchedulerOnDiskTester, unittest.TestCase\n):\n    reopen = True\n\n\nclass StartUrlsSpider(Spider):\n    def __init__(self, start_urls):\n        self.start_urls = start_urls\n        super().__init__(name=\"StartUrlsSpider\")\n\n    def parse(self, response):\n        pass\n\n\nclass TestIntegrationWithDownloaderAwareInMemory(TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(\n            spidercls=StartUrlsSpider,\n            settings_dict={\n                \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n                \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n            },\n        )\n\n    @defer.inlineCallbacks\n    def tearDown(self):\n        yield self.crawler.stop()\n\n    @defer.inlineCallbacks\n    def test_integration_downloader_aware_priority_queue(self):\n        with MockServer() as mockserver:\n            url = mockserver.url(\"/status?n=200\", is_secure=False)\n            start_urls = [url] * 6\n            yield self.crawler.crawl(start_urls)\n            self.assertEqual(\n                self.crawler.stats.get_value(\"downloader/response_count\"),\n                len(start_urls),\n            )\n\n\nclass TestIncompatibility(unittest.TestCase):\n    def _incompatible(self):\n        settings = {\n            \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n            \"CONCURRENT_REQUESTS_PER_IP\": 1,\n        }\n        crawler = get_crawler(Spider, settings)\n        scheduler = Scheduler.from_crawler(crawler)\n        spider = Spider(name=\"spider\")\n        scheduler.open(spider)\n\n    def test_incompatibility(self):\n        with self.assertRaises(ValueError):\n            self._incompatible()\n", "tests/test_spidermiddleware_urllength.py": "from unittest import TestCase\n\nfrom testfixtures import LogCapture\n\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestUrlLengthMiddleware(TestCase):\n    def setUp(self):\n        self.maxlength = 25\n        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n\n        crawler = get_crawler(Spider)\n        self.spider = crawler._create_spider(\"foo\")\n        self.stats = crawler.stats\n        self.mw = UrlLengthMiddleware.from_settings(settings)\n\n        self.response = Response(\"http://scrapytest.org\")\n        self.short_url_req = Request(\"http://scrapytest.org/\")\n        self.long_url_req = Request(\"http://scrapytest.org/this_is_a_long_url\")\n        self.reqs = [self.short_url_req, self.long_url_req]\n\n    def process_spider_output(self):\n        return list(\n            self.mw.process_spider_output(self.response, self.reqs, self.spider)\n        )\n\n    def test_middleware_works(self):\n        self.assertEqual(self.process_spider_output(), [self.short_url_req])\n\n    def test_logging(self):\n        with LogCapture() as log:\n            self.process_spider_output()\n\n        ric = self.stats.get_value(\n            \"urllength/request_ignored_count\", spider=self.spider\n        )\n        self.assertEqual(ric, 1)\n\n        self.assertIn(f\"Ignoring link (url length > {self.maxlength})\", str(log))\n", "tests/pipelines.py": "\"\"\"\nSome pipelines used for testing\n\"\"\"\n\n\nclass ZeroDivisionErrorPipeline:\n    def open_spider(self, spider):\n        1 / 0\n\n    def process_item(self, item, spider):\n        return item\n\n\nclass ProcessWithZeroDivisionErrorPipeline:\n    def process_item(self, item, spider):\n        1 / 0\n", "tests/test_addons.py": "import itertools\nfrom typing import Any, Dict\nfrom unittest.mock import patch\n\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.trial import unittest\n\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler, CrawlerRunner\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.test import get_crawler\n\n\nclass SimpleAddon:\n    def update_settings(self, settings):\n        pass\n\n\ndef get_addon_cls(config: Dict[str, Any]) -> type:\n    class AddonWithConfig:\n        def update_settings(self, settings: BaseSettings):\n            settings.update(config, priority=\"addon\")\n\n    return AddonWithConfig\n\n\nclass CreateInstanceAddon:\n    def __init__(self, crawler: Crawler) -> None:\n        super().__init__()\n        self.crawler = crawler\n        self.config = crawler.settings.getdict(\"MYADDON\")\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler):\n        return cls(crawler)\n\n    def update_settings(self, settings):\n        settings.update(self.config, \"addon\")\n\n\nclass AddonTest(unittest.TestCase):\n    def test_update_settings(self):\n        settings = BaseSettings()\n        settings.set(\"KEY1\", \"default\", priority=\"default\")\n        settings.set(\"KEY2\", \"project\", priority=\"project\")\n        addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n        testaddon = get_addon_cls(addon_config)()\n        testaddon.update_settings(settings)\n        self.assertEqual(settings[\"KEY1\"], \"addon\")\n        self.assertEqual(settings[\"KEY2\"], \"project\")\n        self.assertEqual(settings[\"KEY3\"], \"addon\")\n\n\nclass AddonManagerTest(unittest.TestCase):\n    def test_load_settings(self):\n        settings_dict = {\n            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        manager = crawler.addons\n        self.assertIsInstance(manager.addons[0], SimpleAddon)\n\n    def test_notconfigured(self):\n        class NotConfiguredAddon:\n            def update_settings(self, settings):\n                raise NotConfigured()\n\n        settings_dict = {\n            \"ADDONS\": {NotConfiguredAddon: 0},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        manager = crawler.addons\n        self.assertFalse(manager.addons)\n\n    def test_load_settings_order(self):\n        # Get three addons with different settings\n        addonlist = []\n        for i in range(3):\n            addon = get_addon_cls({\"KEY1\": i})\n            addon.number = i\n            addonlist.append(addon)\n        # Test for every possible ordering\n        for ordered_addons in itertools.permutations(addonlist):\n            expected_order = [a.number for a in ordered_addons]\n            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n            crawler = get_crawler(settings_dict=settings)\n            manager = crawler.addons\n            self.assertEqual([a.number for a in manager.addons], expected_order)\n            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n\n    def test_build_from_crawler(self):\n        settings_dict = {\n            \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n            \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        manager = crawler.addons\n        self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n        self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n\n    def test_settings_priority(self):\n        config = {\n            \"KEY\": 15,  # priority=addon\n        }\n        settings_dict = {\n            \"ADDONS\": {get_addon_cls(config): 1},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n\n        settings = Settings(settings_dict)\n        settings.set(\"KEY\", 0, priority=\"default\")\n        runner = CrawlerRunner(settings)\n        crawler = runner.create_crawler(Spider)\n        crawler._apply_settings()\n        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n\n        settings_dict = {\n            \"KEY\": 20,  # priority=project\n            \"ADDONS\": {get_addon_cls(config): 1},\n        }\n        settings = Settings(settings_dict)\n        settings.set(\"KEY\", 0, priority=\"default\")\n        runner = CrawlerRunner(settings)\n        crawler = runner.create_crawler(Spider)\n        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n\n    def test_fallback_workflow(self):\n        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n\n        class AddonWithFallback:\n            def update_settings(self, settings):\n                if not settings.get(FALLBACK_SETTING):\n                    settings.set(\n                        FALLBACK_SETTING,\n                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n                        \"addon\",\n                    )\n                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n\n        settings_dict = {\n            \"ADDONS\": {AddonWithFallback: 1},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        self.assertEqual(\n            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n        )\n        self.assertEqual(\n            crawler.settings.get(FALLBACK_SETTING),\n            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n        )\n\n        settings_dict = {\n            \"ADDONS\": {AddonWithFallback: 1},\n            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        self.assertEqual(\n            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n        )\n        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\n\n    def test_logging_message(self):\n        class LoggedAddon:\n            def update_settings(self, settings):\n                pass\n\n        with patch(\"scrapy.addons.logger\") as logger_mock:\n            with patch(\"scrapy.addons.build_from_crawler\") as build_from_crawler_mock:\n                settings_dict = {\n                    \"ADDONS\": {LoggedAddon: 1},\n                }\n                addon = LoggedAddon()\n                build_from_crawler_mock.return_value = addon\n                crawler = get_crawler(settings_dict=settings_dict)\n                logger_mock.info.assert_called_once_with(\n                    \"Enabled addons:\\n%(addons)s\",\n                    {\"addons\": [addon]},\n                    extra={\"crawler\": crawler},\n                )\n\n    @inlineCallbacks\n    def test_enable_addon_in_spider(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler, *args, **kwargs):\n                spider = super().from_crawler(crawler, *args, **kwargs)\n                addon_config = {\"KEY\": \"addon\"}\n                addon_cls = get_addon_cls(addon_config)\n                spider.settings.set(\"ADDONS\", {addon_cls: 1}, priority=\"spider\")\n                return spider\n\n        settings = Settings()\n        settings.set(\"KEY\", \"default\", priority=\"default\")\n        runner = CrawlerRunner(settings)\n        crawler = runner.create_crawler(MySpider)\n        self.assertEqual(crawler.settings.get(\"KEY\"), \"default\")\n        yield crawler.crawl()\n        self.assertEqual(crawler.settings.get(\"KEY\"), \"addon\")\n", "tests/test_utils_template.py": "import unittest\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\n\nfrom scrapy.utils.template import render_templatefile\n\n__doctests__ = [\"scrapy.utils.template\"]\n\n\nclass UtilsRenderTemplateFileTestCase(unittest.TestCase):\n    def setUp(self):\n        self.tmp_path = mkdtemp()\n\n    def tearDown(self):\n        rmtree(self.tmp_path)\n\n    def test_simple_render(self):\n        context = {\"project_name\": \"proj\", \"name\": \"spi\", \"classname\": \"TheSpider\"}\n        template = \"from ${project_name}.spiders.${name} import ${classname}\"\n        rendered = \"from proj.spiders.spi import TheSpider\"\n\n        template_path = Path(self.tmp_path, \"templ.py.tmpl\")\n        render_path = Path(self.tmp_path, \"templ.py\")\n\n        template_path.write_text(template, encoding=\"utf8\")\n        assert template_path.is_file()  # Failure of test itself\n\n        render_templatefile(template_path, **context)\n\n        self.assertFalse(template_path.exists())\n        self.assertEqual(render_path.read_text(encoding=\"utf8\"), rendered)\n\n        render_path.unlink()\n        assert not render_path.exists()  # Failure of test itself\n\n\nif \"__main__\" == __name__:\n    unittest.main()\n", "tests/test_utils_display.py": "from io import StringIO\nfrom unittest import TestCase, mock\n\nfrom scrapy.utils.display import pformat, pprint\n\n\nclass TestDisplay(TestCase):\n    object = {\"a\": 1}\n    colorized_strings = {\n        (\n            (\n                \"{\\x1b[33m'\\x1b[39;49;00m\\x1b[33ma\\x1b[39;49;00m\\x1b[33m'\"\n                \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\"\n            )\n            + suffix\n        )\n        for suffix in (\n            # https://github.com/pygments/pygments/issues/2313\n            \"\\n\",  # pygments \u2264 2.13\n            \"\\x1b[37m\\x1b[39;49;00m\\n\",  # pygments \u2265 2.14\n        )\n    }\n    plain_string = \"{'a': 1}\"\n\n    @mock.patch(\"sys.platform\", \"linux\")\n    @mock.patch(\"sys.stdout.isatty\")\n    def test_pformat(self, isatty):\n        isatty.return_value = True\n        self.assertIn(pformat(self.object), self.colorized_strings)\n\n    @mock.patch(\"sys.stdout.isatty\")\n    def test_pformat_dont_colorize(self, isatty):\n        isatty.return_value = True\n        self.assertEqual(pformat(self.object, colorize=False), self.plain_string)\n\n    def test_pformat_not_tty(self):\n        self.assertEqual(pformat(self.object), self.plain_string)\n\n    @mock.patch(\"sys.platform\", \"win32\")\n    @mock.patch(\"platform.version\")\n    @mock.patch(\"sys.stdout.isatty\")\n    def test_pformat_old_windows(self, isatty, version):\n        isatty.return_value = True\n        version.return_value = \"10.0.14392\"\n        self.assertIn(pformat(self.object), self.colorized_strings)\n\n    @mock.patch(\"sys.platform\", \"win32\")\n    @mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n    @mock.patch(\"platform.version\")\n    @mock.patch(\"sys.stdout.isatty\")\n    def test_pformat_windows_no_terminal_processing(\n        self, isatty, version, terminal_processing\n    ):\n        isatty.return_value = True\n        version.return_value = \"10.0.14393\"\n        terminal_processing.return_value = False\n        self.assertEqual(pformat(self.object), self.plain_string)\n\n    @mock.patch(\"sys.platform\", \"win32\")\n    @mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n    @mock.patch(\"platform.version\")\n    @mock.patch(\"sys.stdout.isatty\")\n    def test_pformat_windows(self, isatty, version, terminal_processing):\n        isatty.return_value = True\n        version.return_value = \"10.0.14393\"\n        terminal_processing.return_value = True\n        self.assertIn(pformat(self.object), self.colorized_strings)\n\n    @mock.patch(\"sys.platform\", \"linux\")\n    @mock.patch(\"sys.stdout.isatty\")\n    def test_pformat_no_pygments(self, isatty):\n        isatty.return_value = True\n\n        import builtins\n\n        real_import = builtins.__import__\n\n        def mock_import(name, globals, locals, fromlist, level):\n            if \"pygments\" in name:\n                raise ImportError\n            return real_import(name, globals, locals, fromlist, level)\n\n        builtins.__import__ = mock_import\n        self.assertEqual(pformat(self.object), self.plain_string)\n        builtins.__import__ = real_import\n\n    def test_pprint(self):\n        with mock.patch(\"sys.stdout\", new=StringIO()) as mock_out:\n            pprint(self.object)\n            self.assertEqual(mock_out.getvalue(), \"{'a': 1}\\n\")\n", "tests/test_loader_deprecated.py": "\"\"\"\nThese tests are kept as references from the ones that were ported to a itemloaders library.\nOnce we remove the references from scrapy, we can remove these tests.\n\"\"\"\n\nimport unittest\nfrom functools import partial\n\nfrom itemloaders.processors import (\n    Compose,\n    Identity,\n    Join,\n    MapCompose,\n    SelectJmes,\n    TakeFirst,\n)\n\nfrom scrapy.item import Field, Item\nfrom scrapy.loader import ItemLoader\n\n\n# test items\nclass NameItem(Item):\n    name = Field()\n\n\nclass TestItem(NameItem):\n    url = Field()\n    summary = Field()\n\n\n# test item loaders\nclass NameItemLoader(ItemLoader):\n    default_item_class = TestItem\n\n\nclass TestItemLoader(NameItemLoader):\n    name_in = MapCompose(lambda v: v.title())\n\n\nclass DefaultedItemLoader(NameItemLoader):\n    default_input_processor = MapCompose(lambda v: v[:-1])\n\n\n# test processors\ndef processor_with_args(value, other=None, loader_context=None):\n    if \"key\" in loader_context:\n        return loader_context[\"key\"]\n    return value\n\n\nclass BasicItemLoaderTest(unittest.TestCase):\n    def test_load_item_using_default_loader(self):\n        i = TestItem()\n        i[\"summary\"] = \"lala\"\n        il = ItemLoader(item=i)\n        il.add_value(\"name\", \"marta\")\n        item = il.load_item()\n        assert item is i\n        self.assertEqual(item[\"summary\"], [\"lala\"])\n        self.assertEqual(item[\"name\"], [\"marta\"])\n\n    def test_load_item_using_custom_loader(self):\n        il = TestItemLoader()\n        il.add_value(\"name\", \"marta\")\n        item = il.load_item()\n        self.assertEqual(item[\"name\"], [\"Marta\"])\n\n    def test_load_item_ignore_none_field_values(self):\n        def validate_sku(value):\n            # Let's assume a SKU is only digits.\n            if value.isdigit():\n                return value\n\n        class MyLoader(ItemLoader):\n            name_out = Compose(lambda vs: vs[0])  # take first which allows empty values\n            price_out = Compose(TakeFirst(), float)\n            sku_out = Compose(TakeFirst(), validate_sku)\n\n        valid_fragment = \"SKU: 1234\"\n        invalid_fragment = \"SKU: not available\"\n        sku_re = \"SKU: (.+)\"\n\n        il = MyLoader(item={})\n        # Should not return \"sku: None\".\n        il.add_value(\"sku\", [invalid_fragment], re=sku_re)\n        # Should not ignore empty values.\n        il.add_value(\"name\", \"\")\n        il.add_value(\"price\", [\"0\"])\n        self.assertEqual(\n            il.load_item(),\n            {\n                \"name\": \"\",\n                \"price\": 0.0,\n            },\n        )\n\n        il.replace_value(\"sku\", [valid_fragment], re=sku_re)\n        self.assertEqual(il.load_item()[\"sku\"], \"1234\")\n\n    def test_self_referencing_loader(self):\n        class MyLoader(ItemLoader):\n            url_out = TakeFirst()\n\n            def img_url_out(self, values):\n                return (self.get_output_value(\"url\") or \"\") + values[0]\n\n        il = MyLoader(item={})\n        il.add_value(\"url\", \"http://example.com/\")\n        il.add_value(\"img_url\", \"1234.png\")\n        self.assertEqual(\n            il.load_item(),\n            {\n                \"url\": \"http://example.com/\",\n                \"img_url\": \"http://example.com/1234.png\",\n            },\n        )\n\n        il = MyLoader(item={})\n        il.add_value(\"img_url\", \"1234.png\")\n        self.assertEqual(\n            il.load_item(),\n            {\n                \"img_url\": \"1234.png\",\n            },\n        )\n\n    def test_add_value(self):\n        il = TestItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n        il.add_value(\"name\", \"pepe\")\n        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\", \"Pepe\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\", \"Pepe\"])\n\n        # test add object value\n        il.add_value(\"summary\", {\"key\": 1})\n        self.assertEqual(il.get_collected_values(\"summary\"), [{\"key\": 1}])\n\n        il.add_value(None, \"Jim\", lambda x: {\"name\": x})\n        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\", \"Pepe\", \"Jim\"])\n\n    def test_add_zero(self):\n        il = NameItemLoader()\n        il.add_value(\"name\", 0)\n        self.assertEqual(il.get_collected_values(\"name\"), [0])\n\n    def test_replace_value(self):\n        il = TestItemLoader()\n        il.replace_value(\"name\", \"marta\")\n        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n        il.replace_value(\"name\", \"pepe\")\n        self.assertEqual(il.get_collected_values(\"name\"), [\"Pepe\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Pepe\"])\n\n        il.replace_value(None, \"Jim\", lambda x: {\"name\": x})\n        self.assertEqual(il.get_collected_values(\"name\"), [\"Jim\"])\n\n    def test_get_value(self):\n        il = NameItemLoader()\n        self.assertEqual(\"FOO\", il.get_value([\"foo\", \"bar\"], TakeFirst(), str.upper))\n        self.assertEqual(\n            [\"foo\", \"bar\"], il.get_value([\"name:foo\", \"name:bar\"], re=\"name:(.*)$\")\n        )\n        self.assertEqual(\n            \"foo\", il.get_value([\"name:foo\", \"name:bar\"], TakeFirst(), re=\"name:(.*)$\")\n        )\n\n        il.add_value(\"name\", [\"name:foo\", \"name:bar\"], TakeFirst(), re=\"name:(.*)$\")\n        self.assertEqual([\"foo\"], il.get_collected_values(\"name\"))\n        il.replace_value(\"name\", \"name:bar\", re=\"name:(.*)$\")\n        self.assertEqual([\"bar\"], il.get_collected_values(\"name\"))\n\n    def test_iter_on_input_processor_input(self):\n        class NameFirstItemLoader(NameItemLoader):\n            name_in = TakeFirst()\n\n        il = NameFirstItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n        il = NameFirstItemLoader()\n        il.add_value(\"name\", [\"marta\", \"jose\"])\n        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n\n        il = NameFirstItemLoader()\n        il.replace_value(\"name\", \"marta\")\n        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n        il = NameFirstItemLoader()\n        il.replace_value(\"name\", [\"marta\", \"jose\"])\n        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n\n        il = NameFirstItemLoader()\n        il.add_value(\"name\", \"marta\")\n        il.add_value(\"name\", [\"jose\", \"pedro\"])\n        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\", \"jose\"])\n\n    def test_map_compose_filter(self):\n        def filter_world(x):\n            return None if x == \"world\" else x\n\n        proc = MapCompose(filter_world, str.upper)\n        self.assertEqual(\n            proc([\"hello\", \"world\", \"this\", \"is\", \"scrapy\"]),\n            [\"HELLO\", \"THIS\", \"IS\", \"SCRAPY\"],\n        )\n\n    def test_map_compose_filter_multil(self):\n        class TestItemLoader(NameItemLoader):\n            name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])\n\n        il = TestItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"Mart\"])\n        item = il.load_item()\n        self.assertEqual(item[\"name\"], [\"Mart\"])\n\n    def test_default_input_processor(self):\n        il = DefaultedItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"mart\"])\n\n    def test_inherited_default_input_processor(self):\n        class InheritDefaultedItemLoader(DefaultedItemLoader):\n            pass\n\n        il = InheritDefaultedItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"mart\"])\n\n    def test_input_processor_inheritance(self):\n        class ChildItemLoader(TestItemLoader):\n            url_in = MapCompose(lambda v: v.lower())\n\n        il = ChildItemLoader()\n        il.add_value(\"url\", \"HTTP://scrapy.ORG\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"http://scrapy.org\"])\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n\n        class ChildChildItemLoader(ChildItemLoader):\n            url_in = MapCompose(lambda v: v.upper())\n            summary_in = MapCompose(lambda v: v)\n\n        il = ChildChildItemLoader()\n        il.add_value(\"url\", \"http://scrapy.org\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"HTTP://SCRAPY.ORG\"])\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n\n    def test_empty_map_compose(self):\n        class IdentityDefaultedItemLoader(DefaultedItemLoader):\n            name_in = MapCompose()\n\n        il = IdentityDefaultedItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"marta\"])\n\n    def test_identity_input_processor(self):\n        class IdentityDefaultedItemLoader(DefaultedItemLoader):\n            name_in = Identity()\n\n        il = IdentityDefaultedItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"marta\"])\n\n    def test_extend_custom_input_processors(self):\n        class ChildItemLoader(TestItemLoader):\n            name_in = MapCompose(TestItemLoader.name_in, str.swapcase)\n\n        il = ChildItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"mARTA\"])\n\n    def test_extend_default_input_processors(self):\n        class ChildDefaultedItemLoader(DefaultedItemLoader):\n            name_in = MapCompose(\n                DefaultedItemLoader.default_input_processor, str.swapcase\n            )\n\n        il = ChildDefaultedItemLoader()\n        il.add_value(\"name\", \"marta\")\n        self.assertEqual(il.get_output_value(\"name\"), [\"MART\"])\n\n    def test_output_processor_using_function(self):\n        il = TestItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n\n        class TakeFirstItemLoader(TestItemLoader):\n            name_out = \" \".join\n\n        il = TakeFirstItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), \"Mar Ta\")\n\n    def test_output_processor_error(self):\n        class TestItemLoader(ItemLoader):\n            default_item_class = TestItem\n            name_out = MapCompose(float)\n\n        il = TestItemLoader()\n        il.add_value(\"name\", [\"$10\"])\n        try:\n            float(\"$10\")\n        except Exception as e:\n            expected_exc_str = str(e)\n\n        exc = None\n        try:\n            il.load_item()\n        except Exception as e:\n            exc = e\n        assert isinstance(exc, ValueError)\n        s = str(exc)\n        assert \"name\" in s, s\n        assert \"$10\" in s, s\n        assert \"ValueError\" in s, s\n        assert expected_exc_str in s, s\n\n    def test_output_processor_using_classes(self):\n        il = TestItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n\n        class TakeFirstItemLoader(TestItemLoader):\n            name_out = Join()\n\n        il = TakeFirstItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), \"Mar Ta\")\n\n        class TakeFirstItemLoader(TestItemLoader):\n            name_out = Join(\"<br>\")\n\n        il = TakeFirstItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), \"Mar<br>Ta\")\n\n    def test_default_output_processor(self):\n        il = TestItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n\n        class LalaItemLoader(TestItemLoader):\n            default_output_processor = Identity()\n\n        il = LalaItemLoader()\n        il.add_value(\"name\", [\"mar\", \"ta\"])\n        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n\n    def test_loader_context_on_declaration(self):\n        class ChildItemLoader(TestItemLoader):\n            url_in = MapCompose(processor_with_args, key=\"val\")\n\n        il = ChildItemLoader()\n        il.add_value(\"url\", \"text\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n        il.replace_value(\"url\", \"text2\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n\n    def test_loader_context_on_instantiation(self):\n        class ChildItemLoader(TestItemLoader):\n            url_in = MapCompose(processor_with_args)\n\n        il = ChildItemLoader(key=\"val\")\n        il.add_value(\"url\", \"text\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n        il.replace_value(\"url\", \"text2\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n\n    def test_loader_context_on_assign(self):\n        class ChildItemLoader(TestItemLoader):\n            url_in = MapCompose(processor_with_args)\n\n        il = ChildItemLoader()\n        il.context[\"key\"] = \"val\"\n        il.add_value(\"url\", \"text\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n        il.replace_value(\"url\", \"text2\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n\n    def test_item_passed_to_input_processor_functions(self):\n        def processor(value, loader_context):\n            return loader_context[\"item\"][\"name\"]\n\n        class ChildItemLoader(TestItemLoader):\n            url_in = MapCompose(processor)\n\n        it = TestItem(name=\"marta\")\n        il = ChildItemLoader(item=it)\n        il.add_value(\"url\", \"text\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"marta\"])\n        il.replace_value(\"url\", \"text2\")\n        self.assertEqual(il.get_output_value(\"url\"), [\"marta\"])\n\n    def test_compose_processor(self):\n        class TestItemLoader(NameItemLoader):\n            name_out = Compose(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])\n\n        il = TestItemLoader()\n        il.add_value(\"name\", [\"marta\", \"other\"])\n        self.assertEqual(il.get_output_value(\"name\"), \"Mart\")\n        item = il.load_item()\n        self.assertEqual(item[\"name\"], \"Mart\")\n\n    def test_partial_processor(self):\n        def join(values, sep=None, loader_context=None, ignored=None):\n            if sep is not None:\n                return sep.join(values)\n            if loader_context and \"sep\" in loader_context:\n                return loader_context[\"sep\"].join(values)\n            return \"\".join(values)\n\n        class TestItemLoader(NameItemLoader):\n            name_out = Compose(partial(join, sep=\"+\"))\n            url_out = Compose(partial(join, loader_context={\"sep\": \".\"}))\n            summary_out = Compose(partial(join, ignored=\"foo\"))\n\n        il = TestItemLoader()\n        il.add_value(\"name\", [\"rabbit\", \"hole\"])\n        il.add_value(\"url\", [\"rabbit\", \"hole\"])\n        il.add_value(\"summary\", [\"rabbit\", \"hole\"])\n        item = il.load_item()\n        self.assertEqual(item[\"name\"], \"rabbit+hole\")\n        self.assertEqual(item[\"url\"], \"rabbit.hole\")\n        self.assertEqual(item[\"summary\"], \"rabbithole\")\n\n    def test_error_input_processor(self):\n        class TestItem(Item):\n            name = Field()\n\n        class TestItemLoader(ItemLoader):\n            default_item_class = TestItem\n            name_in = MapCompose(float)\n\n        il = TestItemLoader()\n        self.assertRaises(ValueError, il.add_value, \"name\", [\"marta\", \"other\"])\n\n    def test_error_output_processor(self):\n        class TestItem(Item):\n            name = Field()\n\n        class TestItemLoader(ItemLoader):\n            default_item_class = TestItem\n            name_out = Compose(Join(), float)\n\n        il = TestItemLoader()\n        il.add_value(\"name\", \"marta\")\n        with self.assertRaises(ValueError):\n            il.load_item()\n\n    def test_error_processor_as_argument(self):\n        class TestItem(Item):\n            name = Field()\n\n        class TestItemLoader(ItemLoader):\n            default_item_class = TestItem\n\n        il = TestItemLoader()\n        self.assertRaises(\n            ValueError, il.add_value, \"name\", [\"marta\", \"other\"], Compose(float)\n        )\n\n\nclass InitializationFromDictTest(unittest.TestCase):\n    item_class = dict\n\n    def test_keep_single_value(self):\n        \"\"\"Loaded item should contain values from the initial item\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\"]})\n\n    def test_keep_list(self):\n        \"\"\"Loaded item should contain values from the initial item\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\"]})\n\n    def test_add_value_singlevalue_singlevalue(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", \"bar\")\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\"]})\n\n    def test_add_value_singlevalue_list(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", [\"item\", \"loader\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"item\", \"loader\"]})\n\n    def test_add_value_list_singlevalue(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", \"qwerty\")\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\", \"qwerty\"]})\n\n    def test_add_value_list_list(self):\n        \"\"\"Values added after initialization should be appended\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        il.add_value(\"name\", [\"item\", \"loader\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\", \"item\", \"loader\"]})\n\n    def test_get_output_value_singlevalue(self):\n        \"\"\"Getting output value must not remove value from item\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il.get_output_value(\"name\"), [\"foo\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(loaded_item, {\"name\": [\"foo\"]})\n\n    def test_get_output_value_list(self):\n        \"\"\"Getting output value must not remove value from item\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il.get_output_value(\"name\"), [\"foo\", \"bar\"])\n        loaded_item = il.load_item()\n        self.assertIsInstance(loaded_item, self.item_class)\n        self.assertEqual(loaded_item, {\"name\": [\"foo\", \"bar\"]})\n\n    def test_values_single(self):\n        \"\"\"Values from initial item must be added to loader._values\"\"\"\n        input_item = self.item_class(name=\"foo\")\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il._values.get(\"name\"), [\"foo\"])\n\n    def test_values_list(self):\n        \"\"\"Values from initial item must be added to loader._values\"\"\"\n        input_item = self.item_class(name=[\"foo\", \"bar\"])\n        il = ItemLoader(item=input_item)\n        self.assertEqual(il._values.get(\"name\"), [\"foo\", \"bar\"])\n\n\nclass BaseNoInputReprocessingLoader(ItemLoader):\n    title_in = MapCompose(str.upper)\n    title_out = TakeFirst()\n\n\nclass NoInputReprocessingDictLoader(BaseNoInputReprocessingLoader):\n    default_item_class = dict\n\n\nclass NoInputReprocessingFromDictTest(unittest.TestCase):\n    \"\"\"\n    Loaders initialized from loaded items must not reprocess fields (dict instances)\n    \"\"\"\n\n    def test_avoid_reprocessing_with_initial_values_single(self):\n        il = NoInputReprocessingDictLoader(item={\"title\": \"foo\"})\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n        self.assertEqual(\n            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n        )\n\n    def test_avoid_reprocessing_with_initial_values_list(self):\n        il = NoInputReprocessingDictLoader(item={\"title\": [\"foo\", \"bar\"]})\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n        self.assertEqual(\n            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n        )\n\n    def test_avoid_reprocessing_without_initial_values_single(self):\n        il = NoInputReprocessingDictLoader()\n        il.add_value(\"title\", \"foo\")\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n        self.assertEqual(\n            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n        )\n\n    def test_avoid_reprocessing_without_initial_values_list(self):\n        il = NoInputReprocessingDictLoader()\n        il.add_value(\"title\", [\"foo\", \"bar\"])\n        il_loaded = il.load_item()\n        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n        self.assertEqual(\n            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n        )\n\n\nclass TestOutputProcessorDict(unittest.TestCase):\n    def test_output_processor(self):\n        class TempDict(dict):\n            def __init__(self, *args, **kwargs):\n                super().__init__(self, *args, **kwargs)\n                self.setdefault(\"temp\", 0.3)\n\n        class TempLoader(ItemLoader):\n            default_item_class = TempDict\n            default_input_processor = Identity()\n            default_output_processor = Compose(TakeFirst())\n\n        loader = TempLoader()\n        item = loader.load_item()\n        self.assertIsInstance(item, TempDict)\n        self.assertEqual(dict(item), {\"temp\": 0.3})\n\n\nclass ProcessorsTest(unittest.TestCase):\n    def test_take_first(self):\n        proc = TakeFirst()\n        self.assertEqual(proc([None, \"\", \"hello\", \"world\"]), \"hello\")\n        self.assertEqual(proc([None, \"\", 0, \"hello\", \"world\"]), 0)\n\n    def test_identity(self):\n        proc = Identity()\n        self.assertEqual(\n            proc([None, \"\", \"hello\", \"world\"]), [None, \"\", \"hello\", \"world\"]\n        )\n\n    def test_join(self):\n        proc = Join()\n        self.assertRaises(TypeError, proc, [None, \"\", \"hello\", \"world\"])\n        self.assertEqual(proc([\"\", \"hello\", \"world\"]), \" hello world\")\n        self.assertEqual(proc([\"hello\", \"world\"]), \"hello world\")\n        self.assertIsInstance(proc([\"hello\", \"world\"]), str)\n\n    def test_compose(self):\n        proc = Compose(lambda v: v[0], str.upper)\n        self.assertEqual(proc([\"hello\", \"world\"]), \"HELLO\")\n        proc = Compose(str.upper)\n        self.assertEqual(proc(None), None)\n        proc = Compose(str.upper, stop_on_none=False)\n        self.assertRaises(ValueError, proc, None)\n        proc = Compose(str.upper, lambda x: x + 1)\n        self.assertRaises(ValueError, proc, \"hello\")\n\n    def test_mapcompose(self):\n        def filter_world(x):\n            return None if x == \"world\" else x\n\n        proc = MapCompose(filter_world, str.upper)\n        self.assertEqual(\n            proc([\"hello\", \"world\", \"this\", \"is\", \"scrapy\"]),\n            [\"HELLO\", \"THIS\", \"IS\", \"SCRAPY\"],\n        )\n        proc = MapCompose(filter_world, str.upper)\n        self.assertEqual(proc(None), [])\n        proc = MapCompose(filter_world, str.upper)\n        self.assertRaises(ValueError, proc, [1])\n        proc = MapCompose(filter_world, lambda x: x + 1)\n        self.assertRaises(ValueError, proc, \"hello\")\n\n\nclass SelectJmesTestCase(unittest.TestCase):\n    test_list_equals = {\n        \"simple\": (\"foo.bar\", {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n        \"invalid\": (\"foo.bar.baz\", {\"foo\": {\"bar\": \"baz\"}}, None),\n        \"top_level\": (\"foo\", {\"foo\": {\"bar\": \"baz\"}}, {\"bar\": \"baz\"}),\n        \"double_vs_single_quote_string\": (\"foo.bar\", {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n        \"dict\": (\n            \"foo.bar[*].name\",\n            {\"foo\": {\"bar\": [{\"name\": \"one\"}, {\"name\": \"two\"}]}},\n            [\"one\", \"two\"],\n        ),\n        \"list\": (\"[1]\", [1, 2], 2),\n    }\n\n    def test_output(self):\n        for k, v in self.test_list_equals.items():\n            expr, test_list, expected = v\n            test = SelectJmes(expr)(test_list)\n            self.assertEqual(\n                test, expected, msg=f'test \"{k}\" got {test} expected {expected}'\n            )\n\n\n# Functions as processors\n\n\ndef function_processor_strip(iterable):\n    return [x.strip() for x in iterable]\n\n\ndef function_processor_upper(iterable):\n    return [x.upper() for x in iterable]\n\n\nclass FunctionProcessorItem(Item):\n    foo = Field(\n        input_processor=function_processor_strip,\n        output_processor=function_processor_upper,\n    )\n\n\nclass FunctionProcessorDictLoader(ItemLoader):\n    default_item_class = dict\n    foo_in = function_processor_strip\n    foo_out = function_processor_upper\n\n\nclass FunctionProcessorTestCase(unittest.TestCase):\n    def test_processor_defined_in_item_loader(self):\n        lo = FunctionProcessorDictLoader()\n        lo.add_value(\"foo\", \"  bar  \")\n        lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n        self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_downloadermiddleware_httpproxy.py": "import os\n\nimport pytest\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\nspider = Spider(\"foo\")\n\n\nclass TestHttpProxyMiddleware(TestCase):\n    failureException = AssertionError  # type: ignore[assignment]\n\n    def setUp(self):\n        self._oldenv = os.environ.copy()\n\n    def tearDown(self):\n        os.environ = self._oldenv\n\n    def test_not_enabled(self):\n        crawler = get_crawler(Spider, {\"HTTPPROXY_ENABLED\": False})\n        with pytest.raises(NotConfigured):\n            HttpProxyMiddleware.from_crawler(crawler)\n\n    def test_no_environment_proxies(self):\n        os.environ = {\"dummy_proxy\": \"reset_env_and_do_not_raise\"}\n        mw = HttpProxyMiddleware()\n\n        for url in (\"http://e.com\", \"https://e.com\", \"file:///tmp/a\"):\n            req = Request(url)\n            assert mw.process_request(req, spider) is None\n            self.assertEqual(req.url, url)\n            self.assertEqual(req.meta, {})\n\n    def test_environment_proxies(self):\n        os.environ[\"http_proxy\"] = http_proxy = \"https://proxy.for.http:3128\"\n        os.environ[\"https_proxy\"] = https_proxy = \"http://proxy.for.https:8080\"\n        os.environ.pop(\"file_proxy\", None)\n        mw = HttpProxyMiddleware()\n\n        for url, proxy in [\n            (\"http://e.com\", http_proxy),\n            (\"https://e.com\", https_proxy),\n            (\"file://tmp/a\", None),\n        ]:\n            req = Request(url)\n            assert mw.process_request(req, spider) is None\n            self.assertEqual(req.url, url)\n            self.assertEqual(req.meta.get(\"proxy\"), proxy)\n\n    def test_proxy_precedence_meta(self):\n        os.environ[\"http_proxy\"] = \"https://proxy.com\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://scrapytest.org\", meta={\"proxy\": \"https://new.proxy:3128\"})\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta, {\"proxy\": \"https://new.proxy:3128\"})\n\n    def test_proxy_auth(self):\n        os.environ[\"http_proxy\"] = \"https://user:pass@proxy:3128\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcjpwYXNz\")\n        # proxy from request.meta\n        req = Request(\n            \"http://scrapytest.org\",\n            meta={\"proxy\": \"https://username:password@proxy:3128\"},\n        )\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(\n            req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcm5hbWU6cGFzc3dvcmQ=\"\n        )\n\n    def test_proxy_auth_empty_passwd(self):\n        os.environ[\"http_proxy\"] = \"https://user:@proxy:3128\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcjo=\")\n        # proxy from request.meta\n        req = Request(\n            \"http://scrapytest.org\", meta={\"proxy\": \"https://username:@proxy:3128\"}\n        )\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcm5hbWU6\")\n\n    def test_proxy_auth_encoding(self):\n        # utf-8 encoding\n        os.environ[\"http_proxy\"] = \"https://m\\u00E1n:pass@proxy:3128\"\n        mw = HttpProxyMiddleware(auth_encoding=\"utf-8\")\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic bcOhbjpwYXNz\")\n\n        # proxy from request.meta\n        req = Request(\n            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00FCser:pass@proxy:3128\"}\n        )\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(\n            req.headers.get(\"Proxy-Authorization\"), b\"Basic w7xzZXI6cGFzcw==\"\n        )\n\n        # default latin-1 encoding\n        mw = HttpProxyMiddleware(auth_encoding=\"latin-1\")\n        req = Request(\"http://scrapytest.org\")\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic beFuOnBhc3M=\")\n\n        # proxy from request.meta, latin-1 encoding\n        req = Request(\n            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00FCser:pass@proxy:3128\"}\n        )\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic /HNlcjpwYXNz\")\n\n    def test_proxy_already_seted(self):\n        os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n        mw = HttpProxyMiddleware()\n        req = Request(\"http://noproxy.com\", meta={\"proxy\": None})\n        assert mw.process_request(req, spider) is None\n        assert \"proxy\" in req.meta and req.meta[\"proxy\"] is None\n\n    def test_no_proxy(self):\n        os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n        mw = HttpProxyMiddleware()\n\n        os.environ[\"no_proxy\"] = \"*\"\n        req = Request(\"http://noproxy.com\")\n        assert mw.process_request(req, spider) is None\n        assert \"proxy\" not in req.meta\n\n        os.environ[\"no_proxy\"] = \"other.com\"\n        req = Request(\"http://noproxy.com\")\n        assert mw.process_request(req, spider) is None\n        assert \"proxy\" in req.meta\n\n        os.environ[\"no_proxy\"] = \"other.com,noproxy.com\"\n        req = Request(\"http://noproxy.com\")\n        assert mw.process_request(req, spider) is None\n        assert \"proxy\" not in req.meta\n\n        # proxy from meta['proxy'] takes precedence\n        os.environ[\"no_proxy\"] = \"*\"\n        req = Request(\"http://noproxy.com\", meta={\"proxy\": \"http://proxy.com\"})\n        assert mw.process_request(req, spider) is None\n        self.assertEqual(req.meta, {\"proxy\": \"http://proxy.com\"})\n\n    def test_no_proxy_invalid_values(self):\n        os.environ[\"no_proxy\"] = \"/var/run/docker.sock\"\n        mw = HttpProxyMiddleware()\n        # '/var/run/docker.sock' may be used by the user for\n        # no_proxy value but is not parseable and should be skipped\n        assert \"no\" not in mw.proxies\n\n    def test_add_proxy_without_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\"https://example.com\")\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = \"https://example.com\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_add_proxy_with_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\"https://example.com\")\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = \"https://user1:password1@example.com\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n    def test_remove_proxy_without_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = None\n        assert middleware.process_request(request, spider) is None\n        self.assertIsNone(request.meta[\"proxy\"])\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_remove_proxy_with_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = None\n        assert middleware.process_request(request, spider) is None\n        self.assertIsNone(request.meta[\"proxy\"])\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_add_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with the same\n        proxy and adds credentials (there were no credentials before), the new\n        credentials must be used.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n\n        request.meta[\"proxy\"] = \"https://user1:password1@example.com\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n    def test_change_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with different\n        credentials, those new credentials must be used.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = \"https://user2:password2@example.com\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user2\",\n            \"password2\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n    def test_remove_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with the same\n        proxy but no credentials, the original credentials must be still\n        used.\n\n        To remove credentials while keeping the same proxy URL, users must\n        delete the Proxy-Authorization header.\n        \"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n\n        request.meta[\"proxy\"] = \"https://example.com\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n        request.meta[\"proxy\"] = \"https://example.com\"\n        del request.headers[b\"Proxy-Authorization\"]\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_change_proxy_add_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n\n        request.meta[\"proxy\"] = \"https://user1:password1@example.org\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.org\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n    def test_change_proxy_keep_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n\n        request.meta[\"proxy\"] = \"https://user1:password1@example.org\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.org\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n        # Make sure, indirectly, that _auth_proxy is updated.\n        request.meta[\"proxy\"] = \"https://example.com\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_change_proxy_change_credentials(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n\n        request.meta[\"proxy\"] = \"https://user2:password2@example.org\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.org\")\n        encoded_credentials = middleware._basic_auth_header(\n            \"user2\",\n            \"password2\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n    def test_change_proxy_remove_credentials(self):\n        \"\"\"If the proxy request meta switches to a proxy URL with a different\n        proxy and no credentials, no credentials must be used.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = \"https://example.org\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta, {\"proxy\": \"https://example.org\"})\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_change_proxy_remove_credentials_preremoved_header(self):\n        \"\"\"Corner case of proxy switch with credentials removal where the\n        credentials have been removed beforehand.\n\n        It ensures that our implementation does not assume that the credentials\n        header exists when trying to remove it.\n        \"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        request.meta[\"proxy\"] = \"https://example.org\"\n        del request.headers[b\"Proxy-Authorization\"]\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta, {\"proxy\": \"https://example.org\"})\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_proxy_authentication_header_undefined_proxy(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": \"Basic foo\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        self.assertNotIn(\"proxy\", request.meta)\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_proxy_authentication_header_disabled_proxy(self):\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": \"Basic foo\"},\n            meta={\"proxy\": None},\n        )\n        assert middleware.process_request(request, spider) is None\n        self.assertIsNone(request.meta[\"proxy\"])\n        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n\n    def test_proxy_authentication_header_proxy_without_credentials(self):\n        \"\"\"As long as the proxy URL in request metadata remains the same, the\n        Proxy-Authorization header is used and kept, and may even be\n        changed.\"\"\"\n        middleware = HttpProxyMiddleware()\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": \"Basic foo\"},\n            meta={\"proxy\": \"https://example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertEqual(request.headers[\"Proxy-Authorization\"], b\"Basic foo\")\n\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertEqual(request.headers[\"Proxy-Authorization\"], b\"Basic foo\")\n\n        request.headers[\"Proxy-Authorization\"] = b\"Basic bar\"\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertEqual(request.headers[\"Proxy-Authorization\"], b\"Basic bar\")\n\n    def test_proxy_authentication_header_proxy_with_same_credentials(self):\n        middleware = HttpProxyMiddleware()\n        encoded_credentials = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": b\"Basic \" + encoded_credentials},\n            meta={\"proxy\": \"https://user1:password1@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials,\n        )\n\n    def test_proxy_authentication_header_proxy_with_different_credentials(self):\n        middleware = HttpProxyMiddleware()\n        encoded_credentials1 = middleware._basic_auth_header(\n            \"user1\",\n            \"password1\",\n        )\n        request = Request(\n            \"https://example.com\",\n            headers={\"Proxy-Authorization\": b\"Basic \" + encoded_credentials1},\n            meta={\"proxy\": \"https://user2:password2@example.com\"},\n        )\n        assert middleware.process_request(request, spider) is None\n        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n        encoded_credentials2 = middleware._basic_auth_header(\n            \"user2\",\n            \"password2\",\n        )\n        self.assertEqual(\n            request.headers[\"Proxy-Authorization\"],\n            b\"Basic \" + encoded_credentials2,\n        )\n", "tests/test_spiderstate.py": "import shutil\nfrom datetime import datetime, timezone\nfrom tempfile import mkdtemp\n\nfrom twisted.trial import unittest\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions.spiderstate import SpiderState\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n\nclass SpiderStateTest(unittest.TestCase):\n    def test_store_load(self):\n        jobdir = mkdtemp()\n        try:\n            spider = Spider(name=\"default\")\n            dt = datetime.now(tz=timezone.utc)\n\n            ss = SpiderState(jobdir)\n            ss.spider_opened(spider)\n            spider.state[\"one\"] = 1\n            spider.state[\"dt\"] = dt\n            ss.spider_closed(spider)\n\n            spider2 = Spider(name=\"default\")\n            ss2 = SpiderState(jobdir)\n            ss2.spider_opened(spider2)\n            self.assertEqual(spider.state, {\"one\": 1, \"dt\": dt})\n            ss2.spider_closed(spider2)\n        finally:\n            shutil.rmtree(jobdir)\n\n    def test_state_attribute(self):\n        # state attribute must be present if jobdir is not set, to provide a\n        # consistent interface\n        spider = Spider(name=\"default\")\n        ss = SpiderState()\n        ss.spider_opened(spider)\n        self.assertEqual(spider.state, {})\n        ss.spider_closed(spider)\n\n    def test_not_configured(self):\n        crawler = get_crawler(Spider)\n        self.assertRaises(NotConfigured, SpiderState.from_crawler, crawler)\n", "tests/test_closespider.py": "from twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import ErrorSpider, FollowAllSpider, ItemSpider, SlowSpider\n\n\nclass TestCloseSpider(TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_closespider_itemcount(self):\n        close_on = 5\n        crawler = get_crawler(ItemSpider, {\"CLOSESPIDER_ITEMCOUNT\": close_on})\n        yield crawler.crawl(mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        self.assertEqual(reason, \"closespider_itemcount\")\n        itemcount = crawler.stats.get_value(\"item_scraped_count\")\n        self.assertTrue(itemcount >= close_on)\n\n    @defer.inlineCallbacks\n    def test_closespider_pagecount(self):\n        close_on = 5\n        crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_PAGECOUNT\": close_on})\n        yield crawler.crawl(mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        self.assertEqual(reason, \"closespider_pagecount\")\n        pagecount = crawler.stats.get_value(\"response_received_count\")\n        self.assertTrue(pagecount >= close_on)\n\n    @defer.inlineCallbacks\n    def test_closespider_errorcount(self):\n        close_on = 5\n        crawler = get_crawler(ErrorSpider, {\"CLOSESPIDER_ERRORCOUNT\": close_on})\n        yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        self.assertEqual(reason, \"closespider_errorcount\")\n        key = f\"spider_exceptions/{crawler.spider.exception_cls.__name__}\"\n        errorcount = crawler.stats.get_value(key)\n        self.assertTrue(errorcount >= close_on)\n\n    @defer.inlineCallbacks\n    def test_closespider_timeout(self):\n        close_on = 0.1\n        crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_TIMEOUT\": close_on})\n        yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        self.assertEqual(reason, \"closespider_timeout\")\n        total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n        self.assertTrue(total_seconds >= close_on)\n\n    @defer.inlineCallbacks\n    def test_closespider_timeout_no_item(self):\n        timeout = 1\n        crawler = get_crawler(SlowSpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n        yield crawler.crawl(n=3, mockserver=self.mockserver)\n        reason = crawler.spider.meta[\"close_reason\"]\n        self.assertEqual(reason, \"closespider_timeout_no_item\")\n        total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n        self.assertTrue(total_seconds >= timeout)\n", "tests/test_toplevel.py": "from unittest import TestCase\n\nimport scrapy\n\n\nclass ToplevelTestCase(TestCase):\n    def test_version(self):\n        self.assertIs(type(scrapy.__version__), str)\n\n    def test_version_info(self):\n        self.assertIs(type(scrapy.version_info), tuple)\n\n    def test_request_shortcut(self):\n        from scrapy.http import FormRequest, Request\n\n        self.assertIs(scrapy.Request, Request)\n        self.assertIs(scrapy.FormRequest, FormRequest)\n\n    def test_spider_shortcut(self):\n        from scrapy.spiders import Spider\n\n        self.assertIs(scrapy.Spider, Spider)\n\n    def test_selector_shortcut(self):\n        from scrapy.selector import Selector\n\n        self.assertIs(scrapy.Selector, Selector)\n\n    def test_item_shortcut(self):\n        from scrapy.item import Field, Item\n\n        self.assertIs(scrapy.Item, Item)\n        self.assertIs(scrapy.Field, Field)\n", "tests/test_http2_client_protocol.py": "import json\nimport random\nimport re\nimport shutil\nimport string\nfrom ipaddress import IPv4Address\nfrom pathlib import Path\nfrom tempfile import mkdtemp\nfrom typing import Dict\nfrom unittest import mock, skipIf\nfrom urllib.parse import urlencode\n\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import (\n    CancelledError,\n    Deferred,\n    DeferredList,\n    inlineCallbacks,\n)\nfrom twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\nfrom twisted.internet.error import TimeoutError\nfrom twisted.internet.ssl import Certificate, PrivateCertificate, optionsForClientTLS\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase\nfrom twisted.web.client import URI, ResponseFailed\nfrom twisted.web.http import H2_ENABLED\nfrom twisted.web.http import Request as TxRequest\nfrom twisted.web.server import NOT_DONE_YET, Site\nfrom twisted.web.static import File\n\nfrom scrapy.http import JsonRequest, Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom tests.mockserver import LeafResource, Status, ssl_context_factory\n\n\ndef generate_random_string(size):\n    return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=size))\n\n\ndef make_html_body(val):\n    response = f\"\"\"<html>\n<h1>Hello from HTTP2<h1>\n<p>{val}</p>\n</html>\"\"\"\n    return bytes(response, \"utf-8\")\n\n\nclass DummySpider(Spider):\n    name = \"dummy\"\n    start_urls: list = []\n\n    def parse(self, response):\n        print(response)\n\n\nclass Data:\n    SMALL_SIZE = 1024  # 1 KB\n    LARGE_SIZE = 1024**2  # 1 MB\n\n    STR_SMALL = generate_random_string(SMALL_SIZE)\n    STR_LARGE = generate_random_string(LARGE_SIZE)\n\n    EXTRA_SMALL = generate_random_string(1024 * 15)\n    EXTRA_LARGE = generate_random_string((1024**2) * 15)\n\n    HTML_SMALL = make_html_body(STR_SMALL)\n    HTML_LARGE = make_html_body(STR_LARGE)\n\n    JSON_SMALL = {\"data\": STR_SMALL}\n    JSON_LARGE = {\"data\": STR_LARGE}\n\n    DATALOSS = b\"Dataloss Content\"\n    NO_CONTENT_LENGTH = b\"This response do not have any content-length header\"\n\n\nclass GetDataHtmlSmall(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"text/html; charset=UTF-8\")\n        return Data.HTML_SMALL\n\n\nclass GetDataHtmlLarge(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"text/html; charset=UTF-8\")\n        return Data.HTML_LARGE\n\n\nclass PostDataJsonMixin:\n    @staticmethod\n    def make_response(request: TxRequest, extra_data: str):\n        assert request.content is not None\n        response = {\n            \"request-headers\": {},\n            \"request-body\": json.loads(request.content.read()),\n            \"extra-data\": extra_data,\n        }\n        for k, v in request.requestHeaders.getAllRawHeaders():\n            response[\"request-headers\"][str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n        response_bytes = bytes(json.dumps(response), \"utf-8\")\n        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n        return response_bytes\n\n\nclass PostDataJsonSmall(LeafResource, PostDataJsonMixin):\n    def render_POST(self, request: TxRequest):\n        return self.make_response(request, Data.EXTRA_SMALL)\n\n\nclass PostDataJsonLarge(LeafResource, PostDataJsonMixin):\n    def render_POST(self, request: TxRequest):\n        return self.make_response(request, Data.EXTRA_LARGE)\n\n\nclass Dataloss(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(b\"Content-Length\", b\"1024\")\n        self.deferRequest(request, 0, self._delayed_render, request)\n        return NOT_DONE_YET\n\n    @staticmethod\n    def _delayed_render(request: TxRequest):\n        request.write(Data.DATALOSS)\n        request.finish()\n\n\nclass NoContentLengthHeader(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.requestHeaders.removeHeader(\"Content-Length\")\n        self.deferRequest(request, 0, self._delayed_render, request)\n        return NOT_DONE_YET\n\n    @staticmethod\n    def _delayed_render(request: TxRequest):\n        request.write(Data.NO_CONTENT_LENGTH)\n        request.finish()\n\n\nclass TimeoutResponse(LeafResource):\n    def render_GET(self, request: TxRequest):\n        return NOT_DONE_YET\n\n\nclass QueryParams(LeafResource):\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n\n        query_params: Dict[str, str] = {}\n        assert request.args is not None\n        for k, v in request.args.items():\n            query_params[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n        return bytes(json.dumps(query_params), \"utf-8\")\n\n\nclass RequestHeaders(LeafResource):\n    \"\"\"Sends all the headers received as a response\"\"\"\n\n    def render_GET(self, request: TxRequest):\n        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n        headers = {}\n        for k, v in request.requestHeaders.getAllRawHeaders():\n            headers[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n        return bytes(json.dumps(headers), \"utf-8\")\n\n\ndef get_client_certificate(\n    key_file: Path, certificate_file: Path\n) -> PrivateCertificate:\n    pem = key_file.read_text(encoding=\"utf-8\") + certificate_file.read_text(\n        encoding=\"utf-8\"\n    )\n\n    return PrivateCertificate.loadPEM(pem)\n\n\n@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\nclass Https2ClientProtocolTestCase(TestCase):\n    scheme = \"https\"\n    key_file = Path(__file__).parent / \"keys\" / \"localhost.key\"\n    certificate_file = Path(__file__).parent / \"keys\" / \"localhost.crt\"\n\n    def _init_resource(self):\n        self.temp_directory = mkdtemp()\n        r = File(self.temp_directory)\n        r.putChild(b\"get-data-html-small\", GetDataHtmlSmall())\n        r.putChild(b\"get-data-html-large\", GetDataHtmlLarge())\n\n        r.putChild(b\"post-data-json-small\", PostDataJsonSmall())\n        r.putChild(b\"post-data-json-large\", PostDataJsonLarge())\n\n        r.putChild(b\"dataloss\", Dataloss())\n        r.putChild(b\"no-content-length-header\", NoContentLengthHeader())\n        r.putChild(b\"status\", Status())\n        r.putChild(b\"query-params\", QueryParams())\n        r.putChild(b\"timeout\", TimeoutResponse())\n        r.putChild(b\"request-headers\", RequestHeaders())\n        return r\n\n    @inlineCallbacks\n    def setUp(self):\n        # Initialize resource tree\n        root = self._init_resource()\n        self.site = Site(root, timeout=None)\n\n        # Start server for testing\n        self.hostname = \"localhost\"\n        context_factory = ssl_context_factory(\n            str(self.key_file), str(self.certificate_file)\n        )\n\n        server_endpoint = SSL4ServerEndpoint(\n            reactor, 0, context_factory, interface=self.hostname\n        )\n        self.server = yield server_endpoint.listen(self.site)\n        self.port_number = self.server.getHost().port\n\n        # Connect H2 client with server\n        self.client_certificate = get_client_certificate(\n            self.key_file, self.certificate_file\n        )\n        client_options = optionsForClientTLS(\n            hostname=self.hostname,\n            trustRoot=self.client_certificate,\n            acceptableProtocols=[b\"h2\"],\n        )\n        uri = URI.fromBytes(bytes(self.get_url(\"/\"), \"utf-8\"))\n\n        self.conn_closed_deferred = Deferred()\n        from scrapy.core.http2.protocol import H2ClientFactory\n\n        h2_client_factory = H2ClientFactory(uri, Settings(), self.conn_closed_deferred)\n        client_endpoint = SSL4ClientEndpoint(\n            reactor, self.hostname, self.port_number, client_options\n        )\n        self.client = yield client_endpoint.connect(h2_client_factory)\n\n    @inlineCallbacks\n    def tearDown(self):\n        if self.client.connected:\n            yield self.client.transport.loseConnection()\n            yield self.client.transport.abortConnection()\n        yield self.server.stopListening()\n        shutil.rmtree(self.temp_directory)\n        self.conn_closed_deferred = None\n\n    def get_url(self, path):\n        \"\"\"\n        :param path: Should have / at the starting compulsorily if not empty\n        :return: Complete url\n        \"\"\"\n        assert len(path) > 0 and (path[0] == \"/\" or path[0] == \"&\")\n        return f\"{self.scheme}://{self.hostname}:{self.port_number}{path}\"\n\n    def make_request(self, request: Request) -> Deferred:\n        return self.client.request(request, DummySpider())\n\n    @staticmethod\n    def _check_repeat(get_deferred, count):\n        d_list = []\n        for _ in range(count):\n            d = get_deferred()\n            d_list.append(d)\n\n        return DeferredList(d_list, fireOnOneErrback=True)\n\n    def _check_GET(self, request: Request, expected_body, expected_status):\n        def check_response(response: Response):\n            self.assertEqual(response.status, expected_status)\n            self.assertEqual(response.body, expected_body)\n            self.assertEqual(response.request, request)\n\n            content_length_header = response.headers.get(\"Content-Length\")\n            assert content_length_header is not None\n            content_length = int(content_length_header)\n            self.assertEqual(len(response.body), content_length)\n\n        d = self.make_request(request)\n        d.addCallback(check_response)\n        d.addErrback(self.fail)\n        return d\n\n    def test_GET_small_body(self):\n        request = Request(self.get_url(\"/get-data-html-small\"))\n        return self._check_GET(request, Data.HTML_SMALL, 200)\n\n    def test_GET_large_body(self):\n        request = Request(self.get_url(\"/get-data-html-large\"))\n        return self._check_GET(request, Data.HTML_LARGE, 200)\n\n    def _check_GET_x10(self, *args, **kwargs):\n        def get_deferred():\n            return self._check_GET(*args, **kwargs)\n\n        return self._check_repeat(get_deferred, 10)\n\n    def test_GET_small_body_x10(self):\n        return self._check_GET_x10(\n            Request(self.get_url(\"/get-data-html-small\")), Data.HTML_SMALL, 200\n        )\n\n    def test_GET_large_body_x10(self):\n        return self._check_GET_x10(\n            Request(self.get_url(\"/get-data-html-large\")), Data.HTML_LARGE, 200\n        )\n\n    def _check_POST_json(\n        self,\n        request: Request,\n        expected_request_body,\n        expected_extra_data,\n        expected_status: int,\n    ):\n        d = self.make_request(request)\n\n        def assert_response(response: Response):\n            self.assertEqual(response.status, expected_status)\n            self.assertEqual(response.request, request)\n\n            content_length_header = response.headers.get(\"Content-Length\")\n            assert content_length_header is not None\n            content_length = int(content_length_header)\n            self.assertEqual(len(response.body), content_length)\n\n            # Parse the body\n            content_encoding_header = response.headers[b\"Content-Encoding\"]\n            assert content_encoding_header is not None\n            content_encoding = str(content_encoding_header, \"utf-8\")\n            body = json.loads(str(response.body, content_encoding))\n            self.assertIn(\"request-body\", body)\n            self.assertIn(\"extra-data\", body)\n            self.assertIn(\"request-headers\", body)\n\n            request_body = body[\"request-body\"]\n            self.assertEqual(request_body, expected_request_body)\n\n            extra_data = body[\"extra-data\"]\n            self.assertEqual(extra_data, expected_extra_data)\n\n            # Check if headers were sent successfully\n            request_headers = body[\"request-headers\"]\n            for k, v in request.headers.items():\n                k_str = str(k, \"utf-8\")\n                self.assertIn(k_str, request_headers)\n                self.assertEqual(request_headers[k_str], str(v[0], \"utf-8\"))\n\n        d.addCallback(assert_response)\n        d.addErrback(self.fail)\n        return d\n\n    def test_POST_small_json(self):\n        request = JsonRequest(\n            url=self.get_url(\"/post-data-json-small\"),\n            method=\"POST\",\n            data=Data.JSON_SMALL,\n        )\n        return self._check_POST_json(request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200)\n\n    def test_POST_large_json(self):\n        request = JsonRequest(\n            url=self.get_url(\"/post-data-json-large\"),\n            method=\"POST\",\n            data=Data.JSON_LARGE,\n        )\n        return self._check_POST_json(request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200)\n\n    def _check_POST_json_x10(self, *args, **kwargs):\n        def get_deferred():\n            return self._check_POST_json(*args, **kwargs)\n\n        return self._check_repeat(get_deferred, 10)\n\n    def test_POST_small_json_x10(self):\n        request = JsonRequest(\n            url=self.get_url(\"/post-data-json-small\"),\n            method=\"POST\",\n            data=Data.JSON_SMALL,\n        )\n        return self._check_POST_json_x10(\n            request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200\n        )\n\n    def test_POST_large_json_x10(self):\n        request = JsonRequest(\n            url=self.get_url(\"/post-data-json-large\"),\n            method=\"POST\",\n            data=Data.JSON_LARGE,\n        )\n        return self._check_POST_json_x10(\n            request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200\n        )\n\n    @inlineCallbacks\n    def test_invalid_negotiated_protocol(self):\n        with mock.patch(\n            \"scrapy.core.http2.protocol.PROTOCOL_NAME\", return_value=b\"not-h2\"\n        ):\n            request = Request(url=self.get_url(\"/status?n=200\"))\n            with self.assertRaises(ResponseFailed):\n                yield self.make_request(request)\n\n    def test_cancel_request(self):\n        request = Request(url=self.get_url(\"/get-data-html-large\"))\n\n        def assert_response(response: Response):\n            self.assertEqual(response.status, 499)\n            self.assertEqual(response.request, request)\n\n        d = self.make_request(request)\n        d.addCallback(assert_response)\n        d.addErrback(self.fail)\n        d.cancel()\n\n        return d\n\n    def test_download_maxsize_exceeded(self):\n        request = Request(\n            url=self.get_url(\"/get-data-html-large\"), meta={\"download_maxsize\": 1000}\n        )\n\n        def assert_cancelled_error(failure):\n            self.assertIsInstance(failure.value, CancelledError)\n            error_pattern = re.compile(\n                rf\"Cancelling download of {request.url}: received response \"\n                rf\"size \\(\\d*\\) larger than download max size \\(1000\\)\"\n            )\n            self.assertEqual(len(re.findall(error_pattern, str(failure.value))), 1)\n\n        d = self.make_request(request)\n        d.addCallback(self.fail)\n        d.addErrback(assert_cancelled_error)\n        return d\n\n    def test_received_dataloss_response(self):\n        \"\"\"In case when value of Header Content-Length != len(Received Data)\n        ProtocolError is raised\"\"\"\n        request = Request(url=self.get_url(\"/dataloss\"))\n\n        def assert_failure(failure: Failure):\n            self.assertTrue(len(failure.value.reasons) > 0)\n            from h2.exceptions import InvalidBodyLengthError\n\n            self.assertTrue(\n                any(\n                    isinstance(error, InvalidBodyLengthError)\n                    for error in failure.value.reasons\n                )\n            )\n\n        d = self.make_request(request)\n        d.addCallback(self.fail)\n        d.addErrback(assert_failure)\n        return d\n\n    def test_missing_content_length_header(self):\n        request = Request(url=self.get_url(\"/no-content-length-header\"))\n\n        def assert_content_length(response: Response):\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.body, Data.NO_CONTENT_LENGTH)\n            self.assertEqual(response.request, request)\n            self.assertNotIn(\"Content-Length\", response.headers)\n\n        d = self.make_request(request)\n        d.addCallback(assert_content_length)\n        d.addErrback(self.fail)\n        return d\n\n    @inlineCallbacks\n    def _check_log_warnsize(self, request, warn_pattern, expected_body):\n        with self.assertLogs(\"scrapy.core.http2.stream\", level=\"WARNING\") as cm:\n            response = yield self.make_request(request)\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.request, request)\n            self.assertEqual(response.body, expected_body)\n\n            # Check the warning is raised only once for this request\n            self.assertEqual(\n                sum(len(re.findall(warn_pattern, log)) for log in cm.output), 1\n            )\n\n    @inlineCallbacks\n    def test_log_expected_warnsize(self):\n        request = Request(\n            url=self.get_url(\"/get-data-html-large\"), meta={\"download_warnsize\": 1000}\n        )\n        warn_pattern = re.compile(\n            rf\"Expected response size \\(\\d*\\) larger than \"\n            rf\"download warn size \\(1000\\) in request {request}\"\n        )\n\n        yield self._check_log_warnsize(request, warn_pattern, Data.HTML_LARGE)\n\n    @inlineCallbacks\n    def test_log_received_warnsize(self):\n        request = Request(\n            url=self.get_url(\"/no-content-length-header\"),\n            meta={\"download_warnsize\": 10},\n        )\n        warn_pattern = re.compile(\n            rf\"Received more \\(\\d*\\) bytes than download \"\n            rf\"warn size \\(10\\) in request {request}\"\n        )\n\n        yield self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)\n\n    def test_max_concurrent_streams(self):\n        \"\"\"Send 500 requests at one to check if we can handle\n        very large number of request.\n        \"\"\"\n\n        def get_deferred():\n            return self._check_GET(\n                Request(self.get_url(\"/get-data-html-small\")), Data.HTML_SMALL, 200\n            )\n\n        return self._check_repeat(get_deferred, 500)\n\n    def test_inactive_stream(self):\n        \"\"\"Here we send 110 requests considering the MAX_CONCURRENT_STREAMS\n        by default is 100. After sending the first 100 requests we close the\n        connection.\"\"\"\n        d_list = []\n\n        def assert_inactive_stream(failure):\n            self.assertIsNotNone(failure.check(ResponseFailed))\n            from scrapy.core.http2.stream import InactiveStreamClosed\n\n            self.assertTrue(\n                any(isinstance(e, InactiveStreamClosed) for e in failure.value.reasons)\n            )\n\n        # Send 100 request (we do not check the result)\n        for _ in range(100):\n            d = self.make_request(Request(self.get_url(\"/get-data-html-small\")))\n            d.addBoth(lambda _: None)\n            d_list.append(d)\n\n        # Now send 10 extra request and save the response deferred in a list\n        for _ in range(10):\n            d = self.make_request(Request(self.get_url(\"/get-data-html-small\")))\n            d.addCallback(self.fail)\n            d.addErrback(assert_inactive_stream)\n            d_list.append(d)\n\n        # Close the connection now to fire all the extra 10 requests errback\n        # with InactiveStreamClosed\n        self.client.transport.loseConnection()\n\n        return DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)\n\n    def test_invalid_request_type(self):\n        with self.assertRaises(TypeError):\n            self.make_request(\"https://InvalidDataTypePassed.com\")\n\n    def test_query_parameters(self):\n        params = {\n            \"a\": generate_random_string(20),\n            \"b\": generate_random_string(20),\n            \"c\": generate_random_string(20),\n            \"d\": generate_random_string(20),\n        }\n        request = Request(self.get_url(f\"/query-params?{urlencode(params)}\"))\n\n        def assert_query_params(response: Response):\n            content_encoding_header = response.headers[b\"Content-Encoding\"]\n            assert content_encoding_header is not None\n            content_encoding = str(content_encoding_header, \"utf-8\")\n            data = json.loads(str(response.body, content_encoding))\n            self.assertEqual(data, params)\n\n        d = self.make_request(request)\n        d.addCallback(assert_query_params)\n        d.addErrback(self.fail)\n\n        return d\n\n    def test_status_codes(self):\n        def assert_response_status(response: Response, expected_status: int):\n            self.assertEqual(response.status, expected_status)\n\n        d_list = []\n        for status in [200, 404]:\n            request = Request(self.get_url(f\"/status?n={status}\"))\n            d = self.make_request(request)\n            d.addCallback(assert_response_status, status)\n            d.addErrback(self.fail)\n            d_list.append(d)\n\n        return DeferredList(d_list, fireOnOneErrback=True)\n\n    def test_response_has_correct_certificate_ip_address(self):\n        request = Request(self.get_url(\"/status?n=200\"))\n\n        def assert_metadata(response: Response):\n            self.assertEqual(response.request, request)\n            self.assertIsInstance(response.certificate, Certificate)\n            assert response.certificate  # typing\n            self.assertIsNotNone(response.certificate.original)\n            self.assertEqual(\n                response.certificate.getIssuer(), self.client_certificate.getIssuer()\n            )\n            self.assertTrue(\n                response.certificate.getPublicKey().matches(\n                    self.client_certificate.getPublicKey()\n                )\n            )\n\n            self.assertIsInstance(response.ip_address, IPv4Address)\n            self.assertEqual(str(response.ip_address), \"127.0.0.1\")\n\n        d = self.make_request(request)\n        d.addCallback(assert_metadata)\n        d.addErrback(self.fail)\n\n        return d\n\n    def _check_invalid_netloc(self, url):\n        request = Request(url)\n\n        def assert_invalid_hostname(failure: Failure):\n            from scrapy.core.http2.stream import InvalidHostname\n\n            self.assertIsNotNone(failure.check(InvalidHostname))\n            error_msg = str(failure.value)\n            self.assertIn(\"localhost\", error_msg)\n            self.assertIn(\"127.0.0.1\", error_msg)\n            self.assertIn(str(request), error_msg)\n\n        d = self.make_request(request)\n        d.addCallback(self.fail)\n        d.addErrback(assert_invalid_hostname)\n        return d\n\n    def test_invalid_hostname(self):\n        return self._check_invalid_netloc(\"https://notlocalhost.notlocalhostdomain\")\n\n    def test_invalid_host_port(self):\n        port = self.port_number + 1\n        return self._check_invalid_netloc(f\"https://127.0.0.1:{port}\")\n\n    def test_connection_stays_with_invalid_requests(self):\n        d_list = [\n            self.test_invalid_hostname(),\n            self.test_invalid_host_port(),\n            self.test_GET_small_body(),\n            self.test_POST_small_json(),\n        ]\n\n        return DeferredList(d_list, fireOnOneErrback=True)\n\n    def test_connection_timeout(self):\n        request = Request(self.get_url(\"/timeout\"))\n        d = self.make_request(request)\n\n        # Update the timer to 1s to test connection timeout\n        self.client.setTimeout(1)\n\n        def assert_timeout_error(failure: Failure):\n            for err in failure.value.reasons:\n                from scrapy.core.http2.protocol import H2ClientProtocol\n\n                if isinstance(err, TimeoutError):\n                    self.assertIn(\n                        f\"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s\",\n                        str(err),\n                    )\n                    break\n            else:\n                self.fail()\n\n        d.addCallback(self.fail)\n        d.addErrback(assert_timeout_error)\n        return d\n\n    def test_request_headers_received(self):\n        request = Request(\n            self.get_url(\"/request-headers\"),\n            headers={\"header-1\": \"header value 1\", \"header-2\": \"header value 2\"},\n        )\n        d = self.make_request(request)\n\n        def assert_request_headers(response: Response):\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.request, request)\n\n            response_headers = json.loads(str(response.body, \"utf-8\"))\n            self.assertIsInstance(response_headers, dict)\n            for k, v in request.headers.items():\n                k, v = str(k, \"utf-8\"), str(v[0], \"utf-8\")\n                self.assertIn(k, response_headers)\n                self.assertEqual(v, response_headers[k])\n\n        d.addErrback(self.fail)\n        d.addCallback(assert_request_headers)\n        return d\n", "tests/test_stats.py": "import unittest\nfrom datetime import datetime\nfrom unittest import mock\n\nfrom scrapy.extensions.corestats import CoreStats\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import DummyStatsCollector, StatsCollector\nfrom scrapy.utils.test import get_crawler\n\n\nclass CoreStatsExtensionTest(unittest.TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"foo\")\n\n    @mock.patch(\"scrapy.extensions.corestats.datetime\")\n    def test_core_stats_default_stats_collector(self, mock_datetime):\n        fixed_datetime = datetime(2019, 12, 1, 11, 38)\n        mock_datetime.now = mock.Mock(return_value=fixed_datetime)\n        self.crawler.stats = StatsCollector(self.crawler)\n        ext = CoreStats.from_crawler(self.crawler)\n        ext.spider_opened(self.spider)\n        ext.item_scraped({}, self.spider)\n        ext.response_received(self.spider)\n        ext.item_dropped({}, self.spider, ZeroDivisionError())\n        ext.spider_closed(self.spider, \"finished\")\n        self.assertEqual(\n            ext.stats._stats,\n            {\n                \"start_time\": fixed_datetime,\n                \"finish_time\": fixed_datetime,\n                \"item_scraped_count\": 1,\n                \"response_received_count\": 1,\n                \"item_dropped_count\": 1,\n                \"item_dropped_reasons_count/ZeroDivisionError\": 1,\n                \"finish_reason\": \"finished\",\n                \"elapsed_time_seconds\": 0.0,\n            },\n        )\n\n    def test_core_stats_dummy_stats_collector(self):\n        self.crawler.stats = DummyStatsCollector(self.crawler)\n        ext = CoreStats.from_crawler(self.crawler)\n        ext.spider_opened(self.spider)\n        ext.item_scraped({}, self.spider)\n        ext.response_received(self.spider)\n        ext.item_dropped({}, self.spider, ZeroDivisionError())\n        ext.spider_closed(self.spider, \"finished\")\n        self.assertEqual(ext.stats._stats, {})\n\n\nclass StatsCollectorTest(unittest.TestCase):\n    def setUp(self):\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider(\"foo\")\n\n    def test_collector(self):\n        stats = StatsCollector(self.crawler)\n        self.assertEqual(stats.get_stats(), {})\n        self.assertEqual(stats.get_value(\"anything\"), None)\n        self.assertEqual(stats.get_value(\"anything\", \"default\"), \"default\")\n        stats.set_value(\"test\", \"value\")\n        self.assertEqual(stats.get_stats(), {\"test\": \"value\"})\n        stats.set_value(\"test2\", 23)\n        self.assertEqual(stats.get_stats(), {\"test\": \"value\", \"test2\": 23})\n        self.assertEqual(stats.get_value(\"test2\"), 23)\n        stats.inc_value(\"test2\")\n        self.assertEqual(stats.get_value(\"test2\"), 24)\n        stats.inc_value(\"test2\", 6)\n        self.assertEqual(stats.get_value(\"test2\"), 30)\n        stats.max_value(\"test2\", 6)\n        self.assertEqual(stats.get_value(\"test2\"), 30)\n        stats.max_value(\"test2\", 40)\n        self.assertEqual(stats.get_value(\"test2\"), 40)\n        stats.max_value(\"test3\", 1)\n        self.assertEqual(stats.get_value(\"test3\"), 1)\n        stats.min_value(\"test2\", 60)\n        self.assertEqual(stats.get_value(\"test2\"), 40)\n        stats.min_value(\"test2\", 35)\n        self.assertEqual(stats.get_value(\"test2\"), 35)\n        stats.min_value(\"test4\", 7)\n        self.assertEqual(stats.get_value(\"test4\"), 7)\n\n    def test_dummy_collector(self):\n        stats = DummyStatsCollector(self.crawler)\n        self.assertEqual(stats.get_stats(), {})\n        self.assertEqual(stats.get_value(\"anything\"), None)\n        self.assertEqual(stats.get_value(\"anything\", \"default\"), \"default\")\n        stats.set_value(\"test\", \"value\")\n        stats.inc_value(\"v1\")\n        stats.max_value(\"v2\", 100)\n        stats.min_value(\"v3\", 100)\n        stats.open_spider(\"a\")\n        stats.set_value(\"test\", \"value\", spider=self.spider)\n        self.assertEqual(stats.get_stats(), {})\n        self.assertEqual(stats.get_stats(\"a\"), {})\n", "tests/test_http_response.py": "import codecs\nimport unittest\nfrom unittest import mock\n\nfrom packaging.version import Version as parse_version\nfrom pytest import mark\nfrom w3lib import __version__ as w3lib_version\nfrom w3lib.encoding import resolve_encoding\n\nfrom scrapy.exceptions import NotSupported\nfrom scrapy.http import (\n    Headers,\n    HtmlResponse,\n    Request,\n    Response,\n    TextResponse,\n    XmlResponse,\n)\nfrom scrapy.link import Link\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import to_unicode\nfrom tests import get_testdata\n\n\nclass BaseResponseTest(unittest.TestCase):\n    response_class = Response\n\n    def test_init(self):\n        # Response requires url in the constructor\n        self.assertRaises(Exception, self.response_class)\n        self.assertTrue(\n            isinstance(self.response_class(\"http://example.com/\"), self.response_class)\n        )\n        self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n        self.assertRaises(\n            TypeError, self.response_class, url=\"http://example.com\", body={}\n        )\n        # body can be str or None\n        self.assertTrue(\n            isinstance(\n                self.response_class(\"http://example.com/\", body=b\"\"),\n                self.response_class,\n            )\n        )\n        self.assertTrue(\n            isinstance(\n                self.response_class(\"http://example.com/\", body=b\"body\"),\n                self.response_class,\n            )\n        )\n        # test presence of all optional parameters\n        self.assertTrue(\n            isinstance(\n                self.response_class(\n                    \"http://example.com/\", body=b\"\", headers={}, status=200\n                ),\n                self.response_class,\n            )\n        )\n\n        r = self.response_class(\"http://www.example.com\")\n        assert isinstance(r.url, str)\n        self.assertEqual(r.url, \"http://www.example.com\")\n        self.assertEqual(r.status, 200)\n\n        assert isinstance(r.headers, Headers)\n        self.assertEqual(r.headers, {})\n\n        headers = {\"foo\": \"bar\"}\n        body = b\"a body\"\n        r = self.response_class(\"http://www.example.com\", headers=headers, body=body)\n\n        assert r.headers is not headers\n        self.assertEqual(r.headers[b\"foo\"], b\"bar\")\n\n        r = self.response_class(\"http://www.example.com\", status=301)\n        self.assertEqual(r.status, 301)\n        r = self.response_class(\"http://www.example.com\", status=\"301\")\n        self.assertEqual(r.status, 301)\n        self.assertRaises(\n            ValueError,\n            self.response_class,\n            \"http://example.com\",\n            status=\"lala200\",\n        )\n\n    def test_copy(self):\n        \"\"\"Test Response copy\"\"\"\n\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        r1.flags.append(\"cached\")\n        r2 = r1.copy()\n\n        self.assertEqual(r1.status, r2.status)\n        self.assertEqual(r1.body, r2.body)\n\n        # make sure flags list is shallow copied\n        assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n        self.assertEqual(r1.flags, r2.flags)\n\n        # make sure headers attribute is shallow copied\n        assert (\n            r1.headers is not r2.headers\n        ), \"headers must be a shallow copy, not identical\"\n        self.assertEqual(r1.headers, r2.headers)\n\n    def test_copy_meta(self):\n        req = Request(\"http://www.example.com\")\n        req.meta[\"foo\"] = \"bar\"\n        r1 = self.response_class(\n            \"http://www.example.com\", body=b\"Some body\", request=req\n        )\n        assert r1.meta is req.meta\n\n    def test_copy_cb_kwargs(self):\n        req = Request(\"http://www.example.com\")\n        req.cb_kwargs[\"foo\"] = \"bar\"\n        r1 = self.response_class(\n            \"http://www.example.com\", body=b\"Some body\", request=req\n        )\n        assert r1.cb_kwargs is req.cb_kwargs\n\n    def test_unavailable_meta(self):\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        with self.assertRaisesRegex(AttributeError, r\"Response\\.meta not available\"):\n            r1.meta\n\n    def test_unavailable_cb_kwargs(self):\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        with self.assertRaisesRegex(\n            AttributeError, r\"Response\\.cb_kwargs not available\"\n        ):\n            r1.cb_kwargs\n\n    def test_copy_inherited_classes(self):\n        \"\"\"Test Response children copies preserve their class\"\"\"\n\n        class CustomResponse(self.response_class):\n            pass\n\n        r1 = CustomResponse(\"http://www.example.com\")\n        r2 = r1.copy()\n\n        assert isinstance(r2, CustomResponse)\n\n    def test_replace(self):\n        \"\"\"Test Response.replace() method\"\"\"\n        hdrs = Headers({\"key\": \"value\"})\n        r1 = self.response_class(\"http://www.example.com\")\n        r2 = r1.replace(status=301, body=b\"New body\", headers=hdrs)\n        assert r1.body == b\"\"\n        self.assertEqual(r1.url, r2.url)\n        self.assertEqual((r1.status, r2.status), (200, 301))\n        self.assertEqual((r1.body, r2.body), (b\"\", b\"New body\"))\n        self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n\n        # Empty attributes (which may fail if not compared properly)\n        r3 = self.response_class(\"http://www.example.com\", flags=[\"cached\"])\n        r4 = r3.replace(body=b\"\", flags=[])\n        self.assertEqual(r4.body, b\"\")\n        self.assertEqual(r4.flags, [])\n\n    def _assert_response_values(self, response, encoding, body):\n        if isinstance(body, str):\n            body_unicode = body\n            body_bytes = body.encode(encoding)\n        else:\n            body_unicode = body.decode(encoding)\n            body_bytes = body\n\n        assert isinstance(response.body, bytes)\n        assert isinstance(response.text, str)\n        self._assert_response_encoding(response, encoding)\n        self.assertEqual(response.body, body_bytes)\n        self.assertEqual(response.text, body_unicode)\n\n    def _assert_response_encoding(self, response, encoding):\n        self.assertEqual(response.encoding, resolve_encoding(encoding))\n\n    def test_immutable_attributes(self):\n        r = self.response_class(\"http://example.com\")\n        self.assertRaises(AttributeError, setattr, r, \"url\", \"http://example2.com\")\n        self.assertRaises(AttributeError, setattr, r, \"body\", \"xxx\")\n\n    def test_urljoin(self):\n        \"\"\"Test urljoin shortcut (only for existence, since behavior equals urljoin)\"\"\"\n        joined = self.response_class(\"http://www.example.com\").urljoin(\"/test\")\n        absolute = \"http://www.example.com/test\"\n        self.assertEqual(joined, absolute)\n\n    def test_shortcut_attributes(self):\n        r = self.response_class(\"http://example.com\", body=b\"hello\")\n        if self.response_class == Response:\n            msg = \"Response content isn't text\"\n            self.assertRaisesRegex(AttributeError, msg, getattr, r, \"text\")\n            self.assertRaisesRegex(NotSupported, msg, r.css, \"body\")\n            self.assertRaisesRegex(NotSupported, msg, r.xpath, \"//body\")\n            self.assertRaisesRegex(NotSupported, msg, r.jmespath, \"body\")\n        else:\n            r.text\n            r.css(\"body\")\n            r.xpath(\"//body\")\n\n    # Response.follow\n\n    def test_follow_url_absolute(self):\n        self._assert_followed_url(\"http://foo.example.com\", \"http://foo.example.com\")\n\n    def test_follow_url_relative(self):\n        self._assert_followed_url(\"foo\", \"http://example.com/foo\")\n\n    def test_follow_link(self):\n        self._assert_followed_url(\n            Link(\"http://example.com/foo\"), \"http://example.com/foo\"\n        )\n\n    def test_follow_None_url(self):\n        r = self.response_class(\"http://example.com\")\n        self.assertRaises(ValueError, r.follow, None)\n\n    @mark.xfail(\n        parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n        reason=\"https://github.com/scrapy/w3lib/pull/207\",\n        strict=True,\n    )\n    def test_follow_whitespace_url(self):\n        self._assert_followed_url(\"foo \", \"http://example.com/foo\")\n\n    @mark.xfail(\n        parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n        reason=\"https://github.com/scrapy/w3lib/pull/207\",\n        strict=True,\n    )\n    def test_follow_whitespace_link(self):\n        self._assert_followed_url(\n            Link(\"http://example.com/foo \"), \"http://example.com/foo\"\n        )\n\n    def test_follow_flags(self):\n        res = self.response_class(\"http://example.com/\")\n        fol = res.follow(\"http://example.com/\", flags=[\"cached\", \"allowed\"])\n        self.assertEqual(fol.flags, [\"cached\", \"allowed\"])\n\n    # Response.follow_all\n\n    def test_follow_all_absolute(self):\n        url_list = [\n            \"http://example.org\",\n            \"http://www.example.org\",\n            \"http://example.com\",\n            \"http://www.example.com\",\n        ]\n        self._assert_followed_all_urls(url_list, url_list)\n\n    def test_follow_all_relative(self):\n        relative = [\"foo\", \"bar\", \"foo/bar\", \"bar/foo\"]\n        absolute = [\n            \"http://example.com/foo\",\n            \"http://example.com/bar\",\n            \"http://example.com/foo/bar\",\n            \"http://example.com/bar/foo\",\n        ]\n        self._assert_followed_all_urls(relative, absolute)\n\n    def test_follow_all_links(self):\n        absolute = [\n            \"http://example.com/foo\",\n            \"http://example.com/bar\",\n            \"http://example.com/foo/bar\",\n            \"http://example.com/bar/foo\",\n        ]\n        links = map(Link, absolute)\n        self._assert_followed_all_urls(links, absolute)\n\n    def test_follow_all_empty(self):\n        r = self.response_class(\"http://example.com\")\n        self.assertEqual([], list(r.follow_all([])))\n\n    def test_follow_all_invalid(self):\n        r = self.response_class(\"http://example.com\")\n        if self.response_class == Response:\n            with self.assertRaises(TypeError):\n                list(r.follow_all(urls=None))\n            with self.assertRaises(TypeError):\n                list(r.follow_all(urls=12345))\n            with self.assertRaises(ValueError):\n                list(r.follow_all(urls=[None]))\n        else:\n            with self.assertRaises(ValueError):\n                list(r.follow_all(urls=None))\n            with self.assertRaises(TypeError):\n                list(r.follow_all(urls=12345))\n            with self.assertRaises(ValueError):\n                list(r.follow_all(urls=[None]))\n\n    def test_follow_all_whitespace(self):\n        relative = [\"foo \", \"bar \", \"foo/bar \", \"bar/foo \"]\n        absolute = [\n            \"http://example.com/foo%20\",\n            \"http://example.com/bar%20\",\n            \"http://example.com/foo/bar%20\",\n            \"http://example.com/bar/foo%20\",\n        ]\n        self._assert_followed_all_urls(relative, absolute)\n\n    def test_follow_all_whitespace_links(self):\n        absolute = [\n            \"http://example.com/foo \",\n            \"http://example.com/bar \",\n            \"http://example.com/foo/bar \",\n            \"http://example.com/bar/foo \",\n        ]\n        links = map(Link, absolute)\n        expected = [u.replace(\" \", \"%20\") for u in absolute]\n        self._assert_followed_all_urls(links, expected)\n\n    def test_follow_all_flags(self):\n        re = self.response_class(\"http://www.example.com/\")\n        urls = [\n            \"http://www.example.com/\",\n            \"http://www.example.com/2\",\n            \"http://www.example.com/foo\",\n        ]\n        fol = re.follow_all(urls, flags=[\"cached\", \"allowed\"])\n        for req in fol:\n            self.assertEqual(req.flags, [\"cached\", \"allowed\"])\n\n    def _assert_followed_url(self, follow_obj, target_url, response=None):\n        if response is None:\n            response = self._links_response()\n        req = response.follow(follow_obj)\n        self.assertEqual(req.url, target_url)\n        return req\n\n    def _assert_followed_all_urls(self, follow_obj, target_urls, response=None):\n        if response is None:\n            response = self._links_response()\n        followed = response.follow_all(follow_obj)\n        for req, target in zip(followed, target_urls):\n            self.assertEqual(req.url, target)\n            yield req\n\n    def _links_response(self):\n        body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n        resp = self.response_class(\"http://example.com/index\", body=body)\n        return resp\n\n    def _links_response_no_href(self):\n        body = get_testdata(\"link_extractor\", \"linkextractor_no_href.html\")\n        resp = self.response_class(\"http://example.com/index\", body=body)\n        return resp\n\n\nclass TextResponseTest(BaseResponseTest):\n    response_class = TextResponse\n\n    def test_replace(self):\n        super().test_replace()\n        r1 = self.response_class(\n            \"http://www.example.com\", body=\"hello\", encoding=\"cp852\"\n        )\n        r2 = r1.replace(url=\"http://www.example.com/other\")\n        r3 = r1.replace(url=\"http://www.example.com/other\", encoding=\"latin1\")\n\n        assert isinstance(r2, self.response_class)\n        self.assertEqual(r2.url, \"http://www.example.com/other\")\n        self._assert_response_encoding(r2, \"cp852\")\n        self.assertEqual(r3.url, \"http://www.example.com/other\")\n        self.assertEqual(r3._declared_encoding(), \"latin1\")\n\n    def test_unicode_url(self):\n        # instantiate with unicode url without encoding (should set default encoding)\n        resp = self.response_class(\"http://www.example.com/\")\n        self._assert_response_encoding(resp, self.response_class._DEFAULT_ENCODING)\n\n        # make sure urls are converted to str\n        resp = self.response_class(url=\"http://www.example.com/\", encoding=\"utf-8\")\n        assert isinstance(resp.url, str)\n\n        resp = self.response_class(\n            url=\"http://www.example.com/price/\\xa3\", encoding=\"utf-8\"\n        )\n        self.assertEqual(resp.url, to_unicode(b\"http://www.example.com/price/\\xc2\\xa3\"))\n        resp = self.response_class(\n            url=\"http://www.example.com/price/\\xa3\", encoding=\"latin-1\"\n        )\n        self.assertEqual(resp.url, \"http://www.example.com/price/\\xa3\")\n        resp = self.response_class(\n            \"http://www.example.com/price/\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n        )\n        self.assertEqual(resp.url, to_unicode(b\"http://www.example.com/price/\\xc2\\xa3\"))\n        resp = self.response_class(\n            \"http://www.example.com/price/\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n        )\n        self.assertEqual(resp.url, \"http://www.example.com/price/\\xa3\")\n\n    def test_unicode_body(self):\n        unicode_string = (\n            \"\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \"\n            \"\\u0442\\u0435\\u043a\\u0441\\u0442\"\n        )\n        self.assertRaises(\n            TypeError,\n            self.response_class,\n            \"http://www.example.com\",\n            body=\"unicode body\",\n        )\n\n        original_string = unicode_string.encode(\"cp1251\")\n        r1 = self.response_class(\n            \"http://www.example.com\", body=original_string, encoding=\"cp1251\"\n        )\n\n        # check response.text\n        self.assertTrue(isinstance(r1.text, str))\n        self.assertEqual(r1.text, unicode_string)\n\n    def test_encoding(self):\n        r1 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xc2\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n        )\n        r2 = self.response_class(\n            \"http://www.example.com\", encoding=\"utf-8\", body=\"\\xa3\"\n        )\n        r3 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n        )\n        r4 = self.response_class(\"http://www.example.com\", body=b\"\\xa2\\xa3\")\n        r5 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xc2\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=None\"]},\n        )\n        r6 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xa8D\",\n            headers={\"Content-type\": [\"text/html; charset=gb2312\"]},\n        )\n        r7 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xa8D\",\n            headers={\"Content-type\": [\"text/html; charset=gbk\"]},\n        )\n        r8 = self.response_class(\n            \"http://www.example.com\",\n            body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n            headers={\"Content-type\": [\"text/html; charset=cp1251\"]},\n        )\n        r9 = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\x80\",\n            headers={\n                \"Content-type\": [b\"application/x-download; filename=\\x80dummy.txt\"]\n            },\n        )\n\n        self.assertEqual(r1._headers_encoding(), \"utf-8\")\n        self.assertEqual(r2._headers_encoding(), None)\n        self.assertEqual(r2._declared_encoding(), \"utf-8\")\n        self._assert_response_encoding(r2, \"utf-8\")\n        self.assertEqual(r3._headers_encoding(), \"cp1252\")\n        self.assertEqual(r3._declared_encoding(), \"cp1252\")\n        self.assertEqual(r4._headers_encoding(), None)\n        self.assertEqual(r5._headers_encoding(), None)\n        self.assertEqual(r8._headers_encoding(), \"cp1251\")\n        self.assertEqual(r9._headers_encoding(), None)\n        self.assertEqual(r8._declared_encoding(), \"utf-8\")\n        self.assertEqual(r9._declared_encoding(), None)\n        self._assert_response_encoding(r5, \"utf-8\")\n        self._assert_response_encoding(r8, \"utf-8\")\n        self._assert_response_encoding(r9, \"cp1252\")\n        assert (\n            r4._body_inferred_encoding() is not None\n            and r4._body_inferred_encoding() != \"ascii\"\n        )\n        self._assert_response_values(r1, \"utf-8\", \"\\xa3\")\n        self._assert_response_values(r2, \"utf-8\", \"\\xa3\")\n        self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n        self._assert_response_values(r6, \"gb18030\", \"\\u2015\")\n        self._assert_response_values(r7, \"gb18030\", \"\\u2015\")\n        self._assert_response_values(r9, \"cp1252\", \"\u20ac\")\n\n        # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n        self.assertRaises(\n            TypeError,\n            self.response_class,\n            \"http://www.example.com\",\n            body=\"\\xa3\",\n        )\n\n    def test_declared_encoding_invalid(self):\n        \"\"\"Check that unknown declared encodings are ignored\"\"\"\n        r = self.response_class(\n            \"http://www.example.com\",\n            headers={\"Content-type\": [\"text/html; charset=UNKNOWN\"]},\n            body=b\"\\xc2\\xa3\",\n        )\n        self.assertEqual(r._declared_encoding(), None)\n        self._assert_response_values(r, \"utf-8\", \"\\xa3\")\n\n    def test_utf16(self):\n        \"\"\"Test utf-16 because UnicodeDammit is known to have problems with\"\"\"\n        r = self.response_class(\n            \"http://www.example.com\",\n            body=b\"\\xff\\xfeh\\x00i\\x00\",\n            encoding=\"utf-16\",\n        )\n        self._assert_response_values(r, \"utf-16\", \"hi\")\n\n    def test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):\n        r6 = self.response_class(\n            \"http://www.example.com\",\n            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n            body=b\"\\xef\\xbb\\xbfWORD\\xe3\\xab\",\n        )\n        self.assertEqual(r6.encoding, \"utf-8\")\n        self.assertIn(\n            r6.text,\n            {\n                \"WORD\\ufffd\\ufffd\",  # w3lib < 1.19.0\n                \"WORD\\ufffd\",  # w3lib >= 1.19.0\n            },\n        )\n\n    def test_bom_is_removed_from_body(self):\n        # Inferring encoding from body also cache decoded body as sideeffect,\n        # this test tries to ensure that calling response.encoding and\n        # response.text in indistinct order doesn't affect final\n        # response.text in indistinct order doesn't affect final\n        # values for encoding and decoded body.\n        url = \"http://example.com\"\n        body = b\"\\xef\\xbb\\xbfWORD\"\n        headers = {\"Content-type\": [\"text/html; charset=utf-8\"]}\n\n        # Test response without content-type and BOM encoding\n        response = self.response_class(url, body=body)\n        self.assertEqual(response.encoding, \"utf-8\")\n        self.assertEqual(response.text, \"WORD\")\n        response = self.response_class(url, body=body)\n        self.assertEqual(response.text, \"WORD\")\n        self.assertEqual(response.encoding, \"utf-8\")\n\n        # Body caching sideeffect isn't triggered when encoding is declared in\n        # content-type header but BOM still need to be removed from decoded\n        # body\n        response = self.response_class(url, headers=headers, body=body)\n        self.assertEqual(response.encoding, \"utf-8\")\n        self.assertEqual(response.text, \"WORD\")\n        response = self.response_class(url, headers=headers, body=body)\n        self.assertEqual(response.text, \"WORD\")\n        self.assertEqual(response.encoding, \"utf-8\")\n\n    def test_replace_wrong_encoding(self):\n        \"\"\"Test invalid chars are replaced properly\"\"\"\n        r = self.response_class(\n            \"http://www.example.com\",\n            encoding=\"utf-8\",\n            body=b\"PREFIX\\xe3\\xabSUFFIX\",\n        )\n        # XXX: Policy for replacing invalid chars may suffer minor variations\n        # but it should always contain the unicode replacement char ('\\ufffd')\n        assert \"\\ufffd\" in r.text, repr(r.text)\n        assert \"PREFIX\" in r.text, repr(r.text)\n        assert \"SUFFIX\" in r.text, repr(r.text)\n\n        # Do not destroy html tags due to encoding bugs\n        r = self.response_class(\n            \"http://example.com\",\n            encoding=\"utf-8\",\n            body=b\"\\xf0<span>value</span>\",\n        )\n        assert \"<span>value</span>\" in r.text, repr(r.text)\n\n        # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n        # r = self.response_class(\"http://www.example.com\", body=b'PREFIX\\xe3\\xabSUFFIX')\n        # assert '\\ufffd' in r.text, repr(r.text)\n\n    def test_selector(self):\n        body = b\"<html><head><title>Some page</title><body></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertIsInstance(response.selector, Selector)\n        self.assertEqual(response.selector.type, \"html\")\n        self.assertIs(response.selector, response.selector)  # property is cached\n        self.assertIs(response.selector.response, response)\n\n        self.assertEqual(\n            response.selector.xpath(\"//title/text()\").getall(), [\"Some page\"]\n        )\n        self.assertEqual(response.selector.css(\"title::text\").getall(), [\"Some page\"])\n        self.assertEqual(response.selector.re(\"Some (.*)</title>\"), [\"page\"])\n\n    def test_selector_shortcuts(self):\n        body = b\"<html><head><title>Some page</title><body></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\"//title/text()\").getall(),\n            response.selector.xpath(\"//title/text()\").getall(),\n        )\n        self.assertEqual(\n            response.css(\"title::text\").getall(),\n            response.selector.css(\"title::text\").getall(),\n        )\n\n    def test_selector_shortcuts_kwargs(self):\n        body = b'<html><head><title>Some page</title><body><p class=\"content\">A nice paragraph.</p></body></html>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\n                \"normalize-space(//p[@class=$pclass])\", pclass=\"content\"\n            ).getall(),\n            response.xpath('normalize-space(//p[@class=\"content\"])').getall(),\n        )\n        self.assertEqual(\n            response.xpath(\n                \"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n                pclass=\"content\",\n                pcount=1,\n            ).getall(),\n            response.xpath(\n                '//title[count(following::p[@class=\"content\"])=1]/text()'\n            ).getall(),\n        )\n\n    def test_urljoin_with_base_url(self):\n        \"\"\"Test urljoin shortcut which also evaluates base-url through get_base_url().\"\"\"\n        body = b'<html><body><base href=\"https://example.net\"></body></html>'\n        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n            \"/test\"\n        )\n        absolute = \"https://example.net/test\"\n        self.assertEqual(joined, absolute)\n\n        body = b'<html><body><base href=\"/elsewhere\"></body></html>'\n        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n            \"test\"\n        )\n        absolute = \"http://www.example.com/test\"\n        self.assertEqual(joined, absolute)\n\n        body = b'<html><body><base href=\"/elsewhere/\"></body></html>'\n        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n            \"test\"\n        )\n        absolute = \"http://www.example.com/elsewhere/test\"\n        self.assertEqual(joined, absolute)\n\n    def test_follow_selector(self):\n        resp = self._links_response()\n        urls = [\n            \"http://example.com/sample2.html\",\n            \"http://example.com/sample3.html\",\n            \"http://example.com/sample3.html\",\n            \"http://example.com/sample3.html\",\n            \"http://example.com/sample3.html#foo\",\n            \"http://www.google.com/something\",\n            \"http://example.com/innertag.html\",\n        ]\n\n        # select <a> elements\n        for sellist in [resp.css(\"a\"), resp.xpath(\"//a\")]:\n            for sel, url in zip(sellist, urls):\n                self._assert_followed_url(sel, url, response=resp)\n\n        # select <link> elements\n        self._assert_followed_url(\n            Selector(text='<link href=\"foo\"></link>').css(\"link\")[0],\n            \"http://example.com/foo\",\n            response=resp,\n        )\n\n        # href attributes should work\n        for sellist in [resp.css(\"a::attr(href)\"), resp.xpath(\"//a/@href\")]:\n            for sel, url in zip(sellist, urls):\n                self._assert_followed_url(sel, url, response=resp)\n\n        # non-a elements are not supported\n        self.assertRaises(ValueError, resp.follow, resp.css(\"div\")[0])\n\n    def test_follow_selector_list(self):\n        resp = self._links_response()\n        self.assertRaisesRegex(ValueError, \"SelectorList\", resp.follow, resp.css(\"a\"))\n\n    def test_follow_selector_invalid(self):\n        resp = self._links_response()\n        self.assertRaisesRegex(\n            ValueError,\n            \"Unsupported\",\n            resp.follow,\n            resp.xpath(\"count(//div)\")[0],\n        )\n\n    def test_follow_selector_attribute(self):\n        resp = self._links_response()\n        for src in resp.css(\"img::attr(src)\"):\n            self._assert_followed_url(src, \"http://example.com/sample2.jpg\")\n\n    def test_follow_selector_no_href(self):\n        resp = self.response_class(\n            url=\"http://example.com\",\n            body=b\"<html><body><a name=123>click me</a></body></html>\",\n        )\n        self.assertRaisesRegex(ValueError, \"no href\", resp.follow, resp.css(\"a\")[0])\n\n    def test_follow_whitespace_selector(self):\n        resp = self.response_class(\n            \"http://example.com\",\n            body=b\"\"\"<html><body><a href=\" foo\\n\">click me</a></body></html>\"\"\",\n        )\n        self._assert_followed_url(\n            resp.css(\"a\")[0], \"http://example.com/foo\", response=resp\n        )\n        self._assert_followed_url(\n            resp.css(\"a::attr(href)\")[0],\n            \"http://example.com/foo\",\n            response=resp,\n        )\n\n    def test_follow_encoding(self):\n        resp1 = self.response_class(\n            \"http://example.com\",\n            encoding=\"utf8\",\n            body='<html><body><a href=\"foo?\u043f\u0440\u0438\u0432\u0435\u0442\">click me</a></body></html>'.encode(),\n        )\n        req = self._assert_followed_url(\n            resp1.css(\"a\")[0],\n            \"http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82\",\n            response=resp1,\n        )\n        self.assertEqual(req.encoding, \"utf8\")\n\n        resp2 = self.response_class(\n            \"http://example.com\",\n            encoding=\"cp1251\",\n            body='<html><body><a href=\"foo?\u043f\u0440\u0438\u0432\u0435\u0442\">click me</a></body></html>'.encode(\n                \"cp1251\"\n            ),\n        )\n        req = self._assert_followed_url(\n            resp2.css(\"a\")[0],\n            \"http://example.com/foo?%EF%F0%E8%E2%E5%F2\",\n            response=resp2,\n        )\n        self.assertEqual(req.encoding, \"cp1251\")\n\n    def test_follow_flags(self):\n        res = self.response_class(\"http://example.com/\")\n        fol = res.follow(\"http://example.com/\", flags=[\"cached\", \"allowed\"])\n        self.assertEqual(fol.flags, [\"cached\", \"allowed\"])\n\n    def test_follow_all_flags(self):\n        re = self.response_class(\"http://www.example.com/\")\n        urls = [\n            \"http://www.example.com/\",\n            \"http://www.example.com/2\",\n            \"http://www.example.com/foo\",\n        ]\n        fol = re.follow_all(urls, flags=[\"cached\", \"allowed\"])\n        for req in fol:\n            self.assertEqual(req.flags, [\"cached\", \"allowed\"])\n\n    def test_follow_all_css(self):\n        expected = [\n            \"http://example.com/sample3.html\",\n            \"http://example.com/innertag.html\",\n        ]\n        response = self._links_response()\n        extracted = [r.url for r in response.follow_all(css='a[href*=\"example.com\"]')]\n        self.assertEqual(expected, extracted)\n\n    def test_follow_all_css_skip_invalid(self):\n        expected = [\n            \"http://example.com/page/1/\",\n            \"http://example.com/page/3/\",\n            \"http://example.com/page/4/\",\n        ]\n        response = self._links_response_no_href()\n        extracted1 = [r.url for r in response.follow_all(css=\".pagination a\")]\n        self.assertEqual(expected, extracted1)\n        extracted2 = [r.url for r in response.follow_all(response.css(\".pagination a\"))]\n        self.assertEqual(expected, extracted2)\n\n    def test_follow_all_xpath(self):\n        expected = [\n            \"http://example.com/sample3.html\",\n            \"http://example.com/innertag.html\",\n        ]\n        response = self._links_response()\n        extracted = response.follow_all(xpath='//a[contains(@href, \"example.com\")]')\n        self.assertEqual(expected, [r.url for r in extracted])\n\n    def test_follow_all_xpath_skip_invalid(self):\n        expected = [\n            \"http://example.com/page/1/\",\n            \"http://example.com/page/3/\",\n            \"http://example.com/page/4/\",\n        ]\n        response = self._links_response_no_href()\n        extracted1 = [\n            r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')\n        ]\n        self.assertEqual(expected, extracted1)\n        extracted2 = [\n            r.url\n            for r in response.follow_all(response.xpath('//div[@id=\"pagination\"]/a'))\n        ]\n        self.assertEqual(expected, extracted2)\n\n    def test_follow_all_too_many_arguments(self):\n        response = self._links_response()\n        with self.assertRaises(ValueError):\n            response.follow_all(\n                css='a[href*=\"example.com\"]',\n                xpath='//a[contains(@href, \"example.com\")]',\n            )\n\n    def test_json_response(self):\n        json_body = b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\"\n        json_response = self.response_class(\"http://www.example.com\", body=json_body)\n        self.assertEqual(json_response.json(), {\"ip\": \"109.187.217.200\"})\n\n        text_body = b\"\"\"<html><body>text</body></html>\"\"\"\n        text_response = self.response_class(\"http://www.example.com\", body=text_body)\n        with self.assertRaises(ValueError):\n            text_response.json()\n\n    def test_cache_json_response(self):\n        json_valid_bodies = [b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\", b\"\"\"null\"\"\"]\n        for json_body in json_valid_bodies:\n            json_response = self.response_class(\n                \"http://www.example.com\", body=json_body\n            )\n\n            with mock.patch(\"json.loads\") as mock_json:\n                for _ in range(2):\n                    json_response.json()\n                mock_json.assert_called_once_with(json_body)\n\n\nclass HtmlResponseTest(TextResponseTest):\n    response_class = HtmlResponse\n\n    def test_html_encoding(self):\n        body = b\"\"\"<html><head><title>Some page</title>\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n        </head><body>Price: \\xa3100</body></html>'\n        \"\"\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, \"iso-8859-1\", body)\n\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n        <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n        Price: \\xa3100\n        \"\"\"\n        r2 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r2, \"iso-8859-1\", body)\n\n        # for conflicting declarations headers must take precedence\n        body = b\"\"\"<html><head><title>Some page</title>\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n        </head><body>Price: \\xa3100</body></html>'\n        \"\"\"\n        r3 = self.response_class(\n            \"http://www.example.com\",\n            body=body,\n            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n        )\n        self._assert_response_values(r3, \"iso-8859-1\", body)\n\n        # make sure replace() preserves the encoding of the original response\n        body = b\"New body \\xa3\"\n        r4 = r3.replace(body=body)\n        self._assert_response_values(r4, \"iso-8859-1\", body)\n\n    def test_html5_meta_charset(self):\n        body = b\"\"\"<html><head><meta charset=\"gb2312\" /><title>Some page</title><body>bla bla</body>\"\"\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, \"gb2312\", body)\n\n\nclass XmlResponseTest(TextResponseTest):\n    response_class = XmlResponse\n\n    def test_xml_encoding(self):\n        body = b\"<xml></xml>\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, self.response_class._DEFAULT_ENCODING, body)\n\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        r2 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r2, \"iso-8859-1\", body)\n\n        # make sure replace() preserves the explicit encoding passed in the __init__ method\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        r3 = self.response_class(\"http://www.example.com\", body=body, encoding=\"utf-8\")\n        body2 = b\"New body\"\n        r4 = r3.replace(body=body2)\n        self._assert_response_values(r4, \"utf-8\", body2)\n\n    def test_replace_encoding(self):\n        # make sure replace() keeps the previous encoding unless overridden explicitly\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        body2 = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><xml></xml>\"\"\"\n        r5 = self.response_class(\"http://www.example.com\", body=body)\n        r6 = r5.replace(body=body2)\n        r7 = r5.replace(body=body2, encoding=\"utf-8\")\n        self._assert_response_values(r5, \"iso-8859-1\", body)\n        self._assert_response_values(r6, \"iso-8859-1\", body2)\n        self._assert_response_values(r7, \"utf-8\", body2)\n\n    def test_selector(self):\n        body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertIsInstance(response.selector, Selector)\n        self.assertEqual(response.selector.type, \"xml\")\n        self.assertIs(response.selector, response.selector)  # property is cached\n        self.assertIs(response.selector.response, response)\n\n        self.assertEqual(response.selector.xpath(\"//elem/text()\").getall(), [\"value\"])\n\n    def test_selector_shortcuts(self):\n        body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\"//elem/text()\").getall(),\n            response.selector.xpath(\"//elem/text()\").getall(),\n        )\n\n    def test_selector_shortcuts_kwargs(self):\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n        <xml xmlns:somens=\"http://scrapy.org\">\n        <somens:elem>value</somens:elem>\n        </xml>\"\"\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\n                \"//s:elem/text()\", namespaces={\"s\": \"http://scrapy.org\"}\n            ).getall(),\n            response.selector.xpath(\n                \"//s:elem/text()\", namespaces={\"s\": \"http://scrapy.org\"}\n            ).getall(),\n        )\n\n        response.selector.register_namespace(\"s2\", \"http://scrapy.org\")\n        self.assertEqual(\n            response.xpath(\n                \"//s1:elem/text()\", namespaces={\"s1\": \"http://scrapy.org\"}\n            ).getall(),\n            response.selector.xpath(\"//s2:elem/text()\").getall(),\n        )\n\n\nclass CustomResponse(TextResponse):\n    attributes = TextResponse.attributes + (\"foo\", \"bar\")\n\n    def __init__(self, *args, **kwargs) -> None:\n        self.foo = kwargs.pop(\"foo\", None)\n        self.bar = kwargs.pop(\"bar\", None)\n        self.lost = kwargs.pop(\"lost\", None)\n        super().__init__(*args, **kwargs)\n\n\nclass CustomResponseTest(TextResponseTest):\n    response_class = CustomResponse\n\n    def test_copy(self):\n        super().test_copy()\n        r1 = self.response_class(\n            url=\"https://example.org\",\n            status=200,\n            foo=\"foo\",\n            bar=\"bar\",\n            lost=\"lost\",\n        )\n        r2 = r1.copy()\n        self.assertIsInstance(r2, self.response_class)\n        self.assertEqual(r1.foo, r2.foo)\n        self.assertEqual(r1.bar, r2.bar)\n        self.assertEqual(r1.lost, \"lost\")\n        self.assertIsNone(r2.lost)\n\n    def test_replace(self):\n        super().test_replace()\n        r1 = self.response_class(\n            url=\"https://example.org\",\n            status=200,\n            foo=\"foo\",\n            bar=\"bar\",\n            lost=\"lost\",\n        )\n\n        r2 = r1.replace(foo=\"new-foo\", bar=\"new-bar\", lost=\"new-lost\")\n        self.assertIsInstance(r2, self.response_class)\n        self.assertEqual(r1.foo, \"foo\")\n        self.assertEqual(r1.bar, \"bar\")\n        self.assertEqual(r1.lost, \"lost\")\n        self.assertEqual(r2.foo, \"new-foo\")\n        self.assertEqual(r2.bar, \"new-bar\")\n        self.assertEqual(r2.lost, \"new-lost\")\n\n        r3 = r1.replace(foo=\"new-foo\", bar=\"new-bar\")\n        self.assertIsInstance(r3, self.response_class)\n        self.assertEqual(r1.foo, \"foo\")\n        self.assertEqual(r1.bar, \"bar\")\n        self.assertEqual(r1.lost, \"lost\")\n        self.assertEqual(r3.foo, \"new-foo\")\n        self.assertEqual(r3.bar, \"new-bar\")\n        self.assertIsNone(r3.lost)\n\n        r4 = r1.replace(foo=\"new-foo\")\n        self.assertIsInstance(r4, self.response_class)\n        self.assertEqual(r1.foo, \"foo\")\n        self.assertEqual(r1.bar, \"bar\")\n        self.assertEqual(r1.lost, \"lost\")\n        self.assertEqual(r4.foo, \"new-foo\")\n        self.assertEqual(r4.bar, \"bar\")\n        self.assertIsNone(r4.lost)\n\n        with self.assertRaises(TypeError) as ctx:\n            r1.replace(unknown=\"unknown\")\n        self.assertTrue(\n            str(ctx.exception).endswith(\n                \"__init__() got an unexpected keyword argument 'unknown'\"\n            )\n        )\n", "tests/test_command_fetch.py": "from twisted.internet import defer\nfrom twisted.trial import unittest\n\nfrom scrapy.utils.testproc import ProcessTest\nfrom scrapy.utils.testsite import SiteTest\n\n\nclass FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n    command = \"fetch\"\n\n    @defer.inlineCallbacks\n    def test_output(self):\n        _, out, _ = yield self.execute([self.url(\"/text\")])\n        self.assertEqual(out.strip(), b\"Works\")\n\n    @defer.inlineCallbacks\n    def test_redirect_default(self):\n        _, out, _ = yield self.execute([self.url(\"/redirect\")])\n        self.assertEqual(out.strip(), b\"Redirected here\")\n\n    @defer.inlineCallbacks\n    def test_redirect_disabled(self):\n        _, out, err = yield self.execute(\n            [\"--no-redirect\", self.url(\"/redirect-no-meta-refresh\")]\n        )\n        err = err.strip()\n        self.assertIn(b\"downloader/response_status_count/302\", err, err)\n        self.assertNotIn(b\"downloader/response_status_count/200\", err, err)\n\n    @defer.inlineCallbacks\n    def test_headers(self):\n        _, out, _ = yield self.execute([self.url(\"/text\"), \"--headers\"])\n        out = out.replace(b\"\\r\", b\"\")  # required on win32\n        assert b\"Server: TwistedWeb\" in out, out\n        assert b\"Content-Type: text/plain\" in out\n", "tests/test_pipeline_images.py": "import dataclasses\nimport hashlib\nimport io\nimport random\nimport warnings\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\nfrom typing import Dict, List, Optional\nfrom unittest.mock import patch\n\nimport attr\nfrom itemadapter import ItemAdapter\nfrom twisted.trial import unittest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.pipelines.images import ImageException, ImagesPipeline, NoimagesDrop\nfrom scrapy.settings import Settings\nfrom scrapy.utils.python import to_bytes\n\nskip_pillow: Optional[str]\ntry:\n    from PIL import Image\nexcept ImportError:\n    skip_pillow = (\n        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n    )\nelse:\n    encoders = {\"jpeg_encoder\", \"jpeg_decoder\"}\n    if not encoders.issubset(set(Image.core.__dict__)):  # type: ignore[attr-defined]\n        skip_pillow = \"Missing JPEG encoders\"\n    else:\n        skip_pillow = None\n\n\nclass ImagesPipelineTestCase(unittest.TestCase):\n    skip = skip_pillow\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n        self.pipeline = ImagesPipeline(self.tempdir)\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def test_file_path(self):\n        file_path = self.pipeline.file_path\n        self.assertEqual(\n            file_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n            \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"\n                )\n            ),\n            \"full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")\n            ),\n            \"full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n                )\n            ),\n            \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\",\n        )\n        self.assertEqual(\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n            \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg\",\n        )\n        self.assertEqual(\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                info=object(),\n            ),\n            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg\",\n        )\n\n    def test_thumbnail_name(self):\n        thumb_path = self.pipeline.thumb_path\n        name = \"50\"\n        self.assertEqual(\n            thumb_path(Request(\"file:///tmp/foo.jpg\"), name),\n            \"thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg\",\n        )\n        self.assertEqual(\n            thumb_path(Request(\"file://foo.png\"), name),\n            \"thumbs/50/e55b765eba0ec7348e50a1df496040449071b96a.jpg\",\n        )\n        self.assertEqual(\n            thumb_path(Request(\"file:///tmp/foo\"), name),\n            \"thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg\",\n        )\n        self.assertEqual(\n            thumb_path(Request(\"file:///tmp/some.name/foo\"), name),\n            \"thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg\",\n        )\n        self.assertEqual(\n            thumb_path(\n                Request(\"file:///tmp/some.name/foo\"),\n                name,\n                response=Response(\"file:///tmp/some.name/foo\"),\n                info=object(),\n            ),\n            \"thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg\",\n        )\n\n    def test_thumbnail_name_from_item(self):\n        \"\"\"\n        Custom thumbnail name based on item data, overriding default implementation\n        \"\"\"\n\n        class CustomImagesPipeline(ImagesPipeline):\n            def thumb_path(\n                self, request, thumb_id, response=None, info=None, item=None\n            ):\n                return f\"thumb/{thumb_id}/{item.get('path')}\"\n\n        thumb_path = CustomImagesPipeline.from_settings(\n            Settings({\"IMAGES_STORE\": self.tempdir})\n        ).thumb_path\n        item = {\"path\": \"path-to-store-file\"}\n        request = Request(\"http://example.com\")\n        self.assertEqual(\n            thumb_path(request, \"small\", item=item), \"thumb/small/path-to-store-file\"\n        )\n\n    def test_get_images_exception(self):\n        self.pipeline.min_width = 100\n        self.pipeline.min_height = 100\n\n        _, buf1 = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n        _, buf2 = _create_image(\"JPEG\", \"RGB\", (150, 50), (0, 0, 0))\n        _, buf3 = _create_image(\"JPEG\", \"RGB\", (50, 150), (0, 0, 0))\n\n        resp1 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf1.getvalue())\n        resp2 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf2.getvalue())\n        resp3 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf3.getvalue())\n        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n\n        with self.assertRaises(ImageException):\n            next(self.pipeline.get_images(response=resp1, request=req, info=object()))\n        with self.assertRaises(ImageException):\n            next(self.pipeline.get_images(response=resp2, request=req, info=object()))\n        with self.assertRaises(ImageException):\n            next(self.pipeline.get_images(response=resp3, request=req, info=object()))\n\n    def test_get_images_new(self):\n        self.pipeline.min_width = 0\n        self.pipeline.min_height = 0\n        self.pipeline.thumbs = {\"small\": (20, 20)}\n\n        orig_im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n        orig_thumb, orig_thumb_buf = _create_image(\"JPEG\", \"RGB\", (20, 20), (0, 0, 0))\n        resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n\n        get_images_gen = self.pipeline.get_images(\n            response=resp, request=req, info=object()\n        )\n\n        path, new_im, new_buf = next(get_images_gen)\n        self.assertEqual(path, \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\")\n        self.assertEqual(orig_im, new_im)\n        self.assertEqual(buf.getvalue(), new_buf.getvalue())\n\n        thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n        self.assertEqual(\n            thumb_path, \"thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n        )\n        self.assertEqual(thumb_img, thumb_img)\n        self.assertEqual(orig_thumb_buf.getvalue(), thumb_buf.getvalue())\n\n    def test_get_images_old(self):\n        self.pipeline.thumbs = {\"small\": (20, 20)}\n        orig_im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n        resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n\n        def overridden_convert_image(image, size=None):\n            im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n            return im, buf\n\n        with patch.object(self.pipeline, \"convert_image\", overridden_convert_image):\n            with warnings.catch_warnings(record=True) as w:\n                warnings.simplefilter(\"always\")\n                get_images_gen = self.pipeline.get_images(\n                    response=resp, request=req, info=object()\n                )\n                path, new_im, new_buf = next(get_images_gen)\n                self.assertEqual(\n                    path, \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n                )\n                self.assertEqual(orig_im.mode, new_im.mode)\n                self.assertEqual(orig_im.getcolors(), new_im.getcolors())\n                self.assertEqual(buf.getvalue(), new_buf.getvalue())\n\n                thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n                self.assertEqual(\n                    thumb_path,\n                    \"thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\",\n                )\n                self.assertEqual(orig_im.mode, thumb_img.mode)\n                self.assertEqual(orig_im.getcolors(), thumb_img.getcolors())\n                self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n\n                expected_warning_msg = (\n                    \".convert_image() method overridden in a deprecated way, \"\n                    \"overridden method does not accept response_body argument.\"\n                )\n                self.assertEqual(\n                    len(\n                        [\n                            warning\n                            for warning in w\n                            if expected_warning_msg in str(warning.message)\n                        ]\n                    ),\n                    1,\n                )\n\n    def test_convert_image_old(self):\n        # tests for old API\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            SIZE = (100, 100)\n            # straight forward case: RGB and JPEG\n            COLOUR = (0, 127, 255)\n            im, _ = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n            converted, _ = self.pipeline.convert_image(im)\n            self.assertEqual(converted.mode, \"RGB\")\n            self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n\n            # check that thumbnail keep image ratio\n            thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n            self.assertEqual(thumbnail.mode, \"RGB\")\n            self.assertEqual(thumbnail.size, (10, 10))\n\n            # transparency case: RGBA and PNG\n            COLOUR = (0, 127, 255, 50)\n            im, _ = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n            converted, _ = self.pipeline.convert_image(im)\n            self.assertEqual(converted.mode, \"RGB\")\n            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n\n            # transparency case with palette: P and PNG\n            COLOUR = (0, 127, 255, 50)\n            im, _ = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n            im = im.convert(\"P\")\n            converted, _ = self.pipeline.convert_image(im)\n            self.assertEqual(converted.mode, \"RGB\")\n            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n\n            # ensure that we received deprecation warnings\n            expected_warning_msg = \".convert_image() method called in a deprecated way\"\n            self.assertTrue(\n                len(\n                    [\n                        warning\n                        for warning in w\n                        if expected_warning_msg in str(warning.message)\n                    ]\n                )\n                == 4\n            )\n\n    def test_convert_image_new(self):\n        # tests for new API\n        SIZE = (100, 100)\n        # straight forward case: RGB and JPEG\n        COLOUR = (0, 127, 255)\n        im, buf = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n        converted, converted_buf = self.pipeline.convert_image(im, response_body=buf)\n        self.assertEqual(converted.mode, \"RGB\")\n        self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n        # check that we don't convert JPEGs again\n        self.assertEqual(converted_buf, buf)\n\n        # check that thumbnail keep image ratio\n        thumbnail, _ = self.pipeline.convert_image(\n            converted, size=(10, 25), response_body=converted_buf\n        )\n        self.assertEqual(thumbnail.mode, \"RGB\")\n        self.assertEqual(thumbnail.size, (10, 10))\n\n        # transparency case: RGBA and PNG\n        COLOUR = (0, 127, 255, 50)\n        im, buf = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n        converted, _ = self.pipeline.convert_image(im, response_body=buf)\n        self.assertEqual(converted.mode, \"RGB\")\n        self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n\n        # transparency case with palette: P and PNG\n        COLOUR = (0, 127, 255, 50)\n        im, buf = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n        im = im.convert(\"P\")\n        converted, _ = self.pipeline.convert_image(im, response_body=buf)\n        self.assertEqual(converted.mode, \"RGB\")\n        self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n\n\nclass DeprecatedImagesPipeline(ImagesPipeline):\n    def file_key(self, url):\n        return self.image_key(url)\n\n    def image_key(self, url):\n        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n        return f\"empty/{image_guid}.jpg\"\n\n    def thumb_key(self, url, thumb_id):\n        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n        return f\"thumbsup/{thumb_id}/{thumb_guid}.jpg\"\n\n\nclass ImagesPipelineTestCaseFieldsMixin:\n    skip = skip_pillow\n\n    def test_item_fields_default(self):\n        url = \"http://www.example.com/images/1.jpg\"\n        item = self.item_class(name=\"item1\", image_urls=[url])\n        pipeline = ImagesPipeline.from_settings(\n            Settings({\"IMAGES_STORE\": \"s3://example/images/\"})\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        self.assertEqual(requests[0].url, url)\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        images = ItemAdapter(item).get(\"images\")\n        self.assertEqual(images, [results[0][1]])\n        self.assertIsInstance(item, self.item_class)\n\n    def test_item_fields_override_settings(self):\n        url = \"http://www.example.com/images/1.jpg\"\n        item = self.item_class(name=\"item1\", custom_image_urls=[url])\n        pipeline = ImagesPipeline.from_settings(\n            Settings(\n                {\n                    \"IMAGES_STORE\": \"s3://example/images/\",\n                    \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n                    \"IMAGES_RESULT_FIELD\": \"custom_images\",\n                }\n            )\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        self.assertEqual(requests[0].url, url)\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        custom_images = ItemAdapter(item).get(\"custom_images\")\n        self.assertEqual(custom_images, [results[0][1]])\n        self.assertIsInstance(item, self.item_class)\n\n\nclass ImagesPipelineTestCaseFieldsDict(\n    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = dict\n\n\nclass ImagesPipelineTestItem(Item):\n    name = Field()\n    # default fields\n    image_urls = Field()\n    images = Field()\n    # overridden fields\n    custom_image_urls = Field()\n    custom_images = Field()\n\n\nclass ImagesPipelineTestCaseFieldsItem(\n    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = ImagesPipelineTestItem\n\n\n@dataclasses.dataclass\nclass ImagesPipelineTestDataClass:\n    name: str\n    # default fields\n    image_urls: list = dataclasses.field(default_factory=list)\n    images: list = dataclasses.field(default_factory=list)\n    # overridden fields\n    custom_image_urls: list = dataclasses.field(default_factory=list)\n    custom_images: list = dataclasses.field(default_factory=list)\n\n\nclass ImagesPipelineTestCaseFieldsDataClass(\n    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = ImagesPipelineTestDataClass\n\n\n@attr.s\nclass ImagesPipelineTestAttrsItem:\n    name = attr.ib(default=\"\")\n    # default fields\n    image_urls: List[str] = attr.ib(default=lambda: [])\n    images: List[Dict[str, str]] = attr.ib(default=lambda: [])\n    # overridden fields\n    custom_image_urls: List[str] = attr.ib(default=lambda: [])\n    custom_images: List[Dict[str, str]] = attr.ib(default=lambda: [])\n\n\nclass ImagesPipelineTestCaseFieldsAttrsItem(\n    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = ImagesPipelineTestAttrsItem\n\n\nclass ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n    skip = skip_pillow\n\n    img_cls_attribute_names = [\n        # Pipeline attribute names with corresponding setting names.\n        (\"EXPIRES\", \"IMAGES_EXPIRES\"),\n        (\"MIN_WIDTH\", \"IMAGES_MIN_WIDTH\"),\n        (\"MIN_HEIGHT\", \"IMAGES_MIN_HEIGHT\"),\n        (\"IMAGES_URLS_FIELD\", \"IMAGES_URLS_FIELD\"),\n        (\"IMAGES_RESULT_FIELD\", \"IMAGES_RESULT_FIELD\"),\n        (\"THUMBS\", \"IMAGES_THUMBS\"),\n    ]\n\n    # This should match what is defined in ImagesPipeline.\n    default_pipeline_settings = {\n        \"MIN_WIDTH\": 0,\n        \"MIN_HEIGHT\": 0,\n        \"EXPIRES\": 90,\n        \"THUMBS\": {},\n        \"IMAGES_URLS_FIELD\": \"image_urls\",\n        \"IMAGES_RESULT_FIELD\": \"images\",\n    }\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def _generate_fake_settings(self, prefix=None):\n        \"\"\"\n        :param prefix: string for setting keys\n        :return: dictionary of image pipeline settings\n        \"\"\"\n\n        def random_string():\n            return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n\n        settings = {\n            \"IMAGES_EXPIRES\": random.randint(100, 1000),\n            \"IMAGES_STORE\": self.tempdir,\n            \"IMAGES_RESULT_FIELD\": random_string(),\n            \"IMAGES_URLS_FIELD\": random_string(),\n            \"IMAGES_MIN_WIDTH\": random.randint(1, 1000),\n            \"IMAGES_MIN_HEIGHT\": random.randint(1, 1000),\n            \"IMAGES_THUMBS\": {\n                \"small\": (random.randint(1, 1000), random.randint(1, 1000)),\n                \"big\": (random.randint(1, 1000), random.randint(1, 1000)),\n            },\n        }\n        if not prefix:\n            return settings\n\n        return {\n            prefix.upper() + \"_\" + k if k != \"IMAGES_STORE\" else k: v\n            for k, v in settings.items()\n        }\n\n    def _generate_fake_pipeline_subclass(self):\n        \"\"\"\n        :return: ImagePipeline class will all uppercase attributes set.\n        \"\"\"\n\n        class UserDefinedImagePipeline(ImagesPipeline):\n            # Values should be in different range than fake_settings.\n            MIN_WIDTH = random.randint(1000, 2000)\n            MIN_HEIGHT = random.randint(1000, 2000)\n            THUMBS = {\n                \"small\": (random.randint(1000, 2000), random.randint(1000, 2000)),\n                \"big\": (random.randint(1000, 2000), random.randint(1000, 2000)),\n            }\n            EXPIRES = random.randint(1000, 2000)\n            IMAGES_URLS_FIELD = \"field_one\"\n            IMAGES_RESULT_FIELD = \"field_two\"\n\n        return UserDefinedImagePipeline\n\n    def test_different_settings_for_different_instances(self):\n        \"\"\"\n        If there are two instances of ImagesPipeline class with different settings, they should\n        have different settings.\n        \"\"\"\n        custom_settings = self._generate_fake_settings()\n        default_settings = Settings()\n        default_sts_pipe = ImagesPipeline(self.tempdir, settings=default_settings)\n        user_sts_pipe = ImagesPipeline.from_settings(Settings(custom_settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n            custom_value = custom_settings.get(settings_attr)\n            self.assertNotEqual(expected_default_value, custom_value)\n            self.assertEqual(\n                getattr(default_sts_pipe, pipe_attr.lower()), expected_default_value\n            )\n            self.assertEqual(getattr(user_sts_pipe, pipe_attr.lower()), custom_value)\n\n    def test_subclass_attrs_preserved_default_settings(self):\n        \"\"\"\n        If image settings are not defined at all subclass of ImagePipeline takes values\n        from class attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline_subclass()\n        pipeline = pipeline_cls.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n            attr_value = getattr(pipeline, pipe_attr.lower())\n            self.assertNotEqual(attr_value, self.default_pipeline_settings[pipe_attr])\n            self.assertEqual(attr_value, getattr(pipeline, pipe_attr))\n\n    def test_subclass_attrs_preserved_custom_settings(self):\n        \"\"\"\n        If image settings are defined but they are not defined for subclass default\n        values taken from settings should be preserved.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline_subclass()\n        settings = self._generate_fake_settings()\n        pipeline = pipeline_cls.from_settings(Settings(settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Instance attribute (lowercase) must be equal to\n            # value defined in settings.\n            value = getattr(pipeline, pipe_attr.lower())\n            self.assertNotEqual(value, self.default_pipeline_settings[pipe_attr])\n            setings_value = settings.get(settings_attr)\n            self.assertEqual(value, setings_value)\n\n    def test_no_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are no settings for subclass and no subclass attributes, pipeline should use\n        attributes of base class.\n        \"\"\"\n\n        class UserDefinedImagePipeline(ImagesPipeline):\n            pass\n\n        user_pipeline = UserDefinedImagePipeline.from_settings(\n            Settings({\"IMAGES_STORE\": self.tempdir})\n        )\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = self.default_pipeline_settings.get(pipe_attr.upper())\n            self.assertEqual(getattr(user_pipeline, pipe_attr.lower()), custom_value)\n\n    def test_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass and NO class attributes, pipeline should use custom\n        settings.\n        \"\"\"\n\n        class UserDefinedImagePipeline(ImagesPipeline):\n            pass\n\n        prefix = UserDefinedImagePipeline.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_attr.lower()), custom_value)\n\n    def test_custom_settings_and_class_attrs_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass AND class attributes\n        setting keys are preferred and override attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline_subclass()\n        prefix = pipeline_cls.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_attr.lower()), custom_value)\n\n    def test_cls_attrs_with_DEFAULT_prefix(self):\n        class UserDefinedImagePipeline(ImagesPipeline):\n            DEFAULT_IMAGES_URLS_FIELD = \"something\"\n            DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n\n        pipeline = UserDefinedImagePipeline.from_settings(\n            Settings({\"IMAGES_STORE\": self.tempdir})\n        )\n        self.assertEqual(\n            pipeline.images_result_field,\n            UserDefinedImagePipeline.DEFAULT_IMAGES_RESULT_FIELD,\n        )\n        self.assertEqual(\n            pipeline.images_urls_field,\n            UserDefinedImagePipeline.DEFAULT_IMAGES_URLS_FIELD,\n        )\n\n    def test_user_defined_subclass_default_key_names(self):\n        \"\"\"Test situation when user defines subclass of ImagePipeline,\n        but uses attribute names for default pipeline (without prefixing\n        them with pipeline class name).\n        \"\"\"\n        settings = self._generate_fake_settings()\n\n        class UserPipe(ImagesPipeline):\n            pass\n\n        pipeline_cls = UserPipe.from_settings(Settings(settings))\n\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            expected_value = settings.get(settings_attr)\n            self.assertEqual(getattr(pipeline_cls, pipe_attr.lower()), expected_value)\n\n\nclass NoimagesDropTestCase(unittest.TestCase):\n    def test_deprecation_warning(self):\n        arg = \"\"\n        with warnings.catch_warnings(record=True) as w:\n            NoimagesDrop(arg)\n            self.assertEqual(len(w), 1)\n            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n        with warnings.catch_warnings(record=True) as w:\n\n            class SubclassedNoimagesDrop(NoimagesDrop):\n                pass\n\n            SubclassedNoimagesDrop(arg)\n            self.assertEqual(len(w), 1)\n            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n\n\ndef _create_image(format, *a, **kw):\n    buf = io.BytesIO()\n    Image.new(*a, **kw).save(buf, format)\n    buf.seek(0)\n    return Image.open(buf), buf\n", "tests/__init__.py": "\"\"\"\ntests: this package contains all Scrapy unittests\n\nsee https://docs.scrapy.org/en/latest/contributing.html#running-tests\n\"\"\"\n\nimport os\nimport socket\nfrom pathlib import Path\n\n# ignore system-wide proxies for tests\n# which would send requests to a totally unsuspecting server\n# (e.g. because urllib does not fully understand the proxy spec)\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\nos.environ[\"ftp_proxy\"] = \"\"\n\n# Absolutize paths to coverage config and output file because tests that\n# spawn subprocesses also changes current working directory.\n_sourceroot = Path(__file__).resolve().parent.parent\nif \"COV_CORE_CONFIG\" in os.environ:\n    os.environ[\"COVERAGE_FILE\"] = str(_sourceroot / \".coverage\")\n    os.environ[\"COV_CORE_CONFIG\"] = str(_sourceroot / os.environ[\"COV_CORE_CONFIG\"])\n\ntests_datadir = str(Path(__file__).parent.resolve() / \"sample_data\")\n\n\n# In some environments accessing a non-existing host doesn't raise an\n# error. In such cases we're going to skip tests which rely on it.\ntry:\n    socket.getaddrinfo(\"non-existing-host\", 80)\n    NON_EXISTING_RESOLVABLE = True\nexcept socket.gaierror:\n    NON_EXISTING_RESOLVABLE = False\n\n\ndef get_testdata(*paths: str) -> bytes:\n    \"\"\"Return test data\"\"\"\n    return Path(tests_datadir, *paths).read_bytes()\n", "tests/test_proxy_connect.py": "import json\nimport os\nimport re\nimport sys\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\nfrom urllib.parse import urlsplit, urlunsplit\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.http import Request\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import SimpleSpider, SingleRequestSpider\n\n\nclass MitmProxy:\n    auth_user = \"scrapy\"\n    auth_pass = \"scrapy\"\n\n    def start(self):\n        script = \"\"\"\nimport sys\nfrom mitmproxy.tools.main import mitmdump\nsys.argv[0] = \"mitmdump\"\nsys.exit(mitmdump())\n        \"\"\"\n        cert_path = Path(__file__).parent.resolve() / \"keys\"\n        self.proc = Popen(\n            [\n                sys.executable,\n                \"-u\",\n                \"-c\",\n                script,\n                \"--listen-host\",\n                \"127.0.0.1\",\n                \"--listen-port\",\n                \"0\",\n                \"--proxyauth\",\n                f\"{self.auth_user}:{self.auth_pass}\",\n                \"--set\",\n                f\"confdir={cert_path}\",\n                \"--ssl-insecure\",\n            ],\n            stdout=PIPE,\n        )\n        line = self.proc.stdout.readline().decode(\"utf-8\")\n        host_port = re.search(r\"listening at (?:http://)?([^:]+:\\d+)\", line).group(1)\n        address = f\"http://{self.auth_user}:{self.auth_pass}@{host_port}\"\n        return address\n\n    def stop(self):\n        self.proc.kill()\n        self.proc.communicate()\n\n\ndef _wrong_credentials(proxy_url):\n    bad_auth_proxy = list(urlsplit(proxy_url))\n    bad_auth_proxy[1] = bad_auth_proxy[1].replace(\"scrapy:scrapy@\", \"wrong:wronger@\")\n    return urlunsplit(bad_auth_proxy)\n\n\nclass ProxyConnectTestCase(TestCase):\n    def setUp(self):\n        try:\n            import mitmproxy  # noqa: F401\n        except ImportError:\n            self.skipTest(\"mitmproxy is not installed\")\n\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self._oldenv = os.environ.copy()\n\n        self._proxy = MitmProxy()\n        proxy_url = self._proxy.start()\n        os.environ[\"https_proxy\"] = proxy_url\n        os.environ[\"http_proxy\"] = proxy_url\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n        self._proxy.stop()\n        os.environ = self._oldenv\n\n    @defer.inlineCallbacks\n    def test_https_connect_tunnel(self):\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n        self._assert_got_response_code(200, log)\n\n    @defer.inlineCallbacks\n    def test_https_tunnel_auth_error(self):\n        os.environ[\"https_proxy\"] = _wrong_credentials(os.environ[\"https_proxy\"])\n        crawler = get_crawler(SimpleSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n        # The proxy returns a 407 error code but it does not reach the client;\n        # he just sees a TunnelError.\n        self._assert_got_tunnel_error(log)\n\n    @defer.inlineCallbacks\n    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n        crawler = get_crawler(SingleRequestSpider)\n        with LogCapture() as log:\n            yield crawler.crawl(seed=request)\n        self._assert_got_response_code(200, log)\n        echo = json.loads(crawler.spider.meta[\"responses\"][0].text)\n        self.assertTrue(\"Proxy-Authorization\" not in echo[\"headers\"])\n\n    def _assert_got_response_code(self, code, log):\n        print(log)\n        self.assertEqual(str(log).count(f\"Crawled ({code})\"), 1)\n\n    def _assert_got_tunnel_error(self, log):\n        print(log)\n        self.assertIn(\"TunnelError\", str(log))\n", "tests/test_utils_iterators.py": "import pytest\nfrom twisted.trial import unittest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Response, TextResponse, XmlResponse\nfrom scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml\nfrom tests import get_testdata\n\n\nclass XmliterBaseTestCase:\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                      xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n              <product id=\"001\">\n                <type>Type 1</type>\n                <name>Name 1</name>\n              </product>\n              <product id=\"002\">\n                <type>Type 2</type>\n                <name>Name 2</name>\n              </product>\n            </products>\n        \"\"\"\n\n        response = XmlResponse(url=\"http://example.com\", body=body)\n        attrs = []\n        for x in self.xmliter(response, \"product\"):\n            attrs.append(\n                (\n                    x.attrib[\"id\"],\n                    x.xpath(\"name/text()\").getall(),\n                    x.xpath(\"./type/text()\").getall(),\n                )\n            )\n\n        self.assertEqual(\n            attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]\n        )\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_unusual_node(self):\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <root>\n                <matchme...></matchme...>\n                <matchmenot></matchmenot>\n            </root>\n        \"\"\"\n        response = XmlResponse(url=\"http://example.com\", body=body)\n        nodenames = [\n            e.xpath(\"name()\").getall() for e in self.xmliter(response, \"matchme...\")\n        ]\n        self.assertEqual(nodenames, [[\"matchme...\"]])\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_unicode(self):\n        # example taken from https://github.com/scrapy/scrapy/issues/1665\n        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <\u00feingflokkar>\n               <\u00feingflokkur id=\"26\">\n                  <heiti />\n                  <skammstafanir>\n                     <stuttskammst\u00f6fun>-</stuttskammst\u00f6fun>\n                     <l\u00f6ngskammst\u00f6fun />\n                  </skammstafanir>\n                  <t\u00edmabil>\n                     <fyrsta\u00feing>80</fyrsta\u00feing>\n                  </t\u00edmabil>\n               </\u00feingflokkur>\n               <\u00feingflokkur id=\"21\">\n                  <heiti>Al\u00fe\u00fd\u00f0ubandalag</heiti>\n                  <skammstafanir>\n                     <stuttskammst\u00f6fun>Ab</stuttskammst\u00f6fun>\n                     <l\u00f6ngskammst\u00f6fun>Al\u00feb.</l\u00f6ngskammst\u00f6fun>\n                  </skammstafanir>\n                  <t\u00edmabil>\n                     <fyrsta\u00feing>76</fyrsta\u00feing>\n                     <s\u00ed\u00f0asta\u00feing>123</s\u00ed\u00f0asta\u00feing>\n                  </t\u00edmabil>\n               </\u00feingflokkur>\n               <\u00feingflokkur id=\"27\">\n                  <heiti>Al\u00fe\u00fd\u00f0uflokkur</heiti>\n                  <skammstafanir>\n                     <stuttskammst\u00f6fun>A</stuttskammst\u00f6fun>\n                     <l\u00f6ngskammst\u00f6fun>Al\u00fefl.</l\u00f6ngskammst\u00f6fun>\n                  </skammstafanir>\n                  <t\u00edmabil>\n                     <fyrsta\u00feing>27</fyrsta\u00feing>\n                     <s\u00ed\u00f0asta\u00feing>120</s\u00ed\u00f0asta\u00feing>\n                  </t\u00edmabil>\n               </\u00feingflokkur>\n            </\u00feingflokkar>\"\"\"\n\n        for r in (\n            # with bytes\n            XmlResponse(url=\"http://example.com\", body=body.encode(\"utf-8\")),\n            # Unicode body needs encoding information\n            XmlResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\"),\n        ):\n            attrs = []\n            for x in self.xmliter(r, \"\u00feingflokkur\"):\n                attrs.append(\n                    (\n                        x.attrib[\"id\"],\n                        x.xpath(\"./skammstafanir/stuttskammst\u00f6fun/text()\").getall(),\n                        x.xpath(\"./t\u00edmabil/fyrsta\u00feing/text()\").getall(),\n                    )\n                )\n\n            self.assertEqual(\n                attrs,\n                [(\"26\", [\"-\"], [\"80\"]), (\"21\", [\"Ab\"], [\"76\"]), (\"27\", [\"A\"], [\"27\"])],\n            )\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_text(self):\n        body = (\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n            \"<products><product>one</product><product>two</product></products>\"\n        )\n\n        self.assertEqual(\n            [x.xpath(\"text()\").getall() for x in self.xmliter(body, \"product\")],\n            [[\"one\"], [\"two\"]],\n        )\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_namespaces(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n                    <g:id>ITEM_1</g:id>\n                    <g:price>400</g:price>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"item\")\n        node = next(my_iter)\n        node.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\n        self.assertEqual(node.xpath(\"title/text()\").getall(), [\"Item 1\"])\n        self.assertEqual(node.xpath(\"description/text()\").getall(), [\"This is item 1\"])\n        self.assertEqual(\n            node.xpath(\"link/text()\").getall(),\n            [\"http://www.mydummycompany.com/items/1\"],\n        )\n        self.assertEqual(\n            node.xpath(\"g:image_link/text()\").getall(),\n            [\"http://www.mydummycompany.com/images/item1.jpg\"],\n        )\n        self.assertEqual(node.xpath(\"g:id/text()\").getall(), [\"ITEM_1\"])\n        self.assertEqual(node.xpath(\"g:price/text()\").getall(), [\"400\"])\n        self.assertEqual(node.xpath(\"image_link/text()\").getall(), [])\n        self.assertEqual(node.xpath(\"id/text()\").getall(), [])\n        self.assertEqual(node.xpath(\"price/text()\").getall(), [])\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_namespaced_nodename(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n                    <g:id>ITEM_1</g:id>\n                    <g:price>400</g:price>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"g:image_link\")\n        node = next(my_iter)\n        node.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\n        self.assertEqual(\n            node.xpath(\"text()\").extract(),\n            [\"http://www.mydummycompany.com/images/item1.jpg\"],\n        )\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_namespaced_nodename_missing(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n                    <g:id>ITEM_1</g:id>\n                    <g:price>400</g:price>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"g:link_image\")\n        with self.assertRaises(StopIteration):\n            next(my_iter)\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_exception(self):\n        body = (\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n            \"<products><product>one</product><product>two</product></products>\"\n        )\n\n        iter = self.xmliter(body, \"product\")\n        next(iter)\n        next(iter)\n\n        self.assertRaises(StopIteration, next, iter)\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_objtype_exception(self):\n        i = self.xmliter(42, \"product\")\n        self.assertRaises(TypeError, next, i)\n\n    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n    def test_xmliter_encoding(self):\n        body = (\n            b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'\n            b\"<xml>\\n\"\n            b\"    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n\"\n            b\"</xml>\\n\\n\"\n        )\n        response = XmlResponse(\"http://www.example.com\", body=body)\n        self.assertEqual(\n            next(self.xmliter(response, \"item\")).get(),\n            \"<item>Some Turkish Characters \\xd6\\xc7\\u015e\\u0130\\u011e\\xdc \\xfc\\u011f\\u0131\\u015f\\xe7\\xf6</item>\",\n        )\n\n\nclass XmliterTestCase(XmliterBaseTestCase, unittest.TestCase):\n    xmliter = staticmethod(xmliter)\n\n    def test_deprecation(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <products>\n              <product></product>\n            </products>\n        \"\"\"\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=\"xmliter\",\n        ):\n            next(self.xmliter(body, \"product\"))\n\n\nclass LxmlXmliterTestCase(XmliterBaseTestCase, unittest.TestCase):\n    xmliter = staticmethod(xmliter_lxml)\n\n    def test_xmliter_iterate_namespace(self):\n        body = b\"\"\"\n            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <rss version=\"2.0\" xmlns=\"http://base.google.com/ns/1.0\">\n                <channel>\n                <title>My Dummy Company</title>\n                <link>http://www.mydummycompany.com</link>\n                <description>This is a dummy company. We do nothing.</description>\n                <item>\n                    <title>Item 1</title>\n                    <description>This is item 1</description>\n                    <link>http://www.mydummycompany.com/items/1</link>\n                    <image_link>http://www.mydummycompany.com/images/item1.jpg</image_link>\n                    <image_link>http://www.mydummycompany.com/images/item2.jpg</image_link>\n                </item>\n                </channel>\n            </rss>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n\n        no_namespace_iter = self.xmliter(response, \"image_link\")\n        self.assertEqual(len(list(no_namespace_iter)), 0)\n\n        namespace_iter = self.xmliter(\n            response, \"image_link\", \"http://base.google.com/ns/1.0\"\n        )\n        node = next(namespace_iter)\n        self.assertEqual(\n            node.xpath(\"text()\").getall(),\n            [\"http://www.mydummycompany.com/images/item1.jpg\"],\n        )\n        node = next(namespace_iter)\n        self.assertEqual(\n            node.xpath(\"text()\").getall(),\n            [\"http://www.mydummycompany.com/images/item2.jpg\"],\n        )\n\n    def test_xmliter_namespaces_prefix(self):\n        body = b\"\"\"\n        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n        <root>\n            <h:table xmlns:h=\"http://www.w3.org/TR/html4/\">\n              <h:tr>\n                <h:td>Apples</h:td>\n                <h:td>Bananas</h:td>\n              </h:tr>\n            </h:table>\n\n            <f:table xmlns:f=\"http://www.w3schools.com/furniture\">\n              <f:name>African Coffee Table</f:name>\n              <f:width>80</f:width>\n              <f:length>120</f:length>\n            </f:table>\n\n        </root>\n        \"\"\"\n        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n        my_iter = self.xmliter(response, \"table\", \"http://www.w3.org/TR/html4/\", \"h\")\n\n        node = next(my_iter)\n        self.assertEqual(len(node.xpath(\"h:tr/h:td\").getall()), 2)\n        self.assertEqual(node.xpath(\"h:tr/h:td[1]/text()\").getall(), [\"Apples\"])\n        self.assertEqual(node.xpath(\"h:tr/h:td[2]/text()\").getall(), [\"Bananas\"])\n\n        my_iter = self.xmliter(\n            response, \"table\", \"http://www.w3schools.com/furniture\", \"f\"\n        )\n\n        node = next(my_iter)\n        self.assertEqual(node.xpath(\"f:name/text()\").getall(), [\"African Coffee Table\"])\n\n    def test_xmliter_objtype_exception(self):\n        i = self.xmliter(42, \"product\")\n        self.assertRaises(TypeError, next, i)\n\n\nclass UtilsCsvTestCase(unittest.TestCase):\n    def test_csviter_defaults(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response)\n\n        result = list(csv)\n        self.assertEqual(\n            result,\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n        # explicit type check cuz' we no like stinkin' autocasting! yarrr\n        for result_row in result:\n            self.assertTrue(all(isinstance(k, str) for k in result_row.keys()))\n            self.assertTrue(all(isinstance(v, str) for v in result_row.values()))\n\n    def test_csviter_delimiter(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\").replace(b\",\", b\"\\t\")\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response, delimiter=\"\\t\")\n\n        self.assertEqual(\n            list(csv),\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n    def test_csviter_quotechar(self):\n        body1 = get_testdata(\"feeds\", \"feed-sample6.csv\")\n        body2 = get_testdata(\"feeds\", \"feed-sample6.csv\").replace(b\",\", b\"|\")\n\n        response1 = TextResponse(url=\"http://example.com/\", body=body1)\n        csv1 = csviter(response1, quotechar=\"'\")\n\n        self.assertEqual(\n            list(csv1),\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n        response2 = TextResponse(url=\"http://example.com/\", body=body2)\n        csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n\n        self.assertEqual(\n            list(csv2),\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n    def test_csviter_wrong_quotechar(self):\n        body = get_testdata(\"feeds\", \"feed-sample6.csv\")\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response)\n\n        self.assertEqual(\n            list(csv),\n            [\n                {\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n                {\n                    \"'id'\": \"2\",\n                    \"'name'\": \"'unicode'\",\n                    \"'value'\": \"'\\xfan\\xedc\\xf3d\\xe9\\u203d'\",\n                },\n                {\"'id'\": \"'3'\", \"'name'\": \"'multi'\", \"'value'\": \"'foo\"},\n                {\"'id'\": \"4\", \"'name'\": \"'empty'\", \"'value'\": \"\"},\n            ],\n        )\n\n    def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\").replace(b\",\", b\"\\t\")\n        response = Response(url=\"http://example.com/\", body=body)\n        csv = csviter(response, delimiter=\"\\t\")\n\n        self.assertEqual(\n            list(csv),\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n    def test_csviter_headers(self):\n        sample = get_testdata(\"feeds\", \"feed-sample3.csv\").splitlines()\n        headers, body = sample[0].split(b\",\"), b\"\\n\".join(sample[1:])\n\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response, headers=[h.decode(\"utf-8\") for h in headers])\n\n        self.assertEqual(\n            list(csv),\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n    def test_csviter_falserow(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n        body = b\"\\n\".join((body, b\"a,b\", b\"a,b,c,d\"))\n\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        csv = csviter(response)\n\n        self.assertEqual(\n            list(csv),\n            [\n                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n            ],\n        )\n\n    def test_csviter_exception(self):\n        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n\n        response = TextResponse(url=\"http://example.com/\", body=body)\n        iter = csviter(response)\n        next(iter)\n        next(iter)\n        next(iter)\n        next(iter)\n\n        self.assertRaises(StopIteration, next, iter)\n\n    def test_csviter_encoding(self):\n        body1 = get_testdata(\"feeds\", \"feed-sample4.csv\")\n        body2 = get_testdata(\"feeds\", \"feed-sample5.csv\")\n\n        response = TextResponse(\n            url=\"http://example.com/\", body=body1, encoding=\"latin1\"\n        )\n        csv = csviter(response)\n        self.assertEqual(\n            list(csv),\n            [\n                {\"id\": \"1\", \"name\": \"latin1\", \"value\": \"test\"},\n                {\"id\": \"2\", \"name\": \"something\", \"value\": \"\\xf1\\xe1\\xe9\\xf3\"},\n            ],\n        )\n\n        response = TextResponse(url=\"http://example.com/\", body=body2, encoding=\"cp852\")\n        csv = csviter(response)\n        self.assertEqual(\n            list(csv),\n            [\n                {\"id\": \"1\", \"name\": \"cp852\", \"value\": \"test\"},\n                {\n                    \"id\": \"2\",\n                    \"name\": \"something\",\n                    \"value\": \"\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557\",\n                },\n            ],\n        )\n\n\nclass TestHelper(unittest.TestCase):\n    bbody = b\"utf8-body\"\n    ubody = bbody.decode(\"utf8\")\n    txtresponse = TextResponse(url=\"http://example.org/\", body=bbody, encoding=\"utf-8\")\n    response = Response(url=\"http://example.org/\", body=bbody)\n\n    def test_body_or_str(self):\n        for obj in (self.bbody, self.ubody, self.txtresponse, self.response):\n            r1 = _body_or_str(obj)\n            self._assert_type_and_value(r1, self.ubody, obj)\n            r2 = _body_or_str(obj, unicode=True)\n            self._assert_type_and_value(r2, self.ubody, obj)\n            r3 = _body_or_str(obj, unicode=False)\n            self._assert_type_and_value(r3, self.bbody, obj)\n            self.assertTrue(type(r1) is type(r2))\n            self.assertTrue(type(r1) is not type(r3))\n\n    def _assert_type_and_value(self, a, b, obj):\n        self.assertTrue(\n            type(a) is type(b), f\"Got {type(a)}, expected {type(b)} for {obj!r}\"\n        )\n        self.assertEqual(a, b)\n", "tests/test_utils_sitemap.py": "import unittest\n\nfrom scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n\n\nclass SitemapTest(unittest.TestCase):\n    def test_sitemap(self):\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n  <url>\n    <loc>http://www.example.com/</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>daily</changefreq>\n    <priority>1</priority>\n  </url>\n  <url>\n    <loc>http://www.example.com/Special-Offers.html</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>weekly</changefreq>\n    <priority>0.8</priority>\n  </url>\n</urlset>\"\"\"\n        )\n        assert s.type == \"urlset\"\n        self.assertEqual(\n            list(s),\n            [\n                {\n                    \"priority\": \"1\",\n                    \"loc\": \"http://www.example.com/\",\n                    \"lastmod\": \"2009-08-16\",\n                    \"changefreq\": \"daily\",\n                },\n                {\n                    \"priority\": \"0.8\",\n                    \"loc\": \"http://www.example.com/Special-Offers.html\",\n                    \"lastmod\": \"2009-08-16\",\n                    \"changefreq\": \"weekly\",\n                },\n            ],\n        )\n\n    def test_sitemap_index(self):\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n   <sitemap>\n      <loc>http://www.example.com/sitemap1.xml.gz</loc>\n      <lastmod>2004-10-01T18:23:17+00:00</lastmod>\n   </sitemap>\n   <sitemap>\n      <loc>http://www.example.com/sitemap2.xml.gz</loc>\n      <lastmod>2005-01-01</lastmod>\n   </sitemap>\n</sitemapindex>\"\"\"\n        )\n        assert s.type == \"sitemapindex\"\n        self.assertEqual(\n            list(s),\n            [\n                {\n                    \"loc\": \"http://www.example.com/sitemap1.xml.gz\",\n                    \"lastmod\": \"2004-10-01T18:23:17+00:00\",\n                },\n                {\n                    \"loc\": \"http://www.example.com/sitemap2.xml.gz\",\n                    \"lastmod\": \"2005-01-01\",\n                },\n            ],\n        )\n\n    def test_sitemap_strip(self):\n        \"\"\"Assert we can deal with trailing spaces inside <loc> tags - we've\n        seen those\n        \"\"\"\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n  <url>\n    <loc> http://www.example.com/</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>daily</changefreq>\n    <priority>1</priority>\n  </url>\n  <url>\n    <loc> http://www.example.com/2</loc>\n    <lastmod />\n  </url>\n</urlset>\n\"\"\"\n        )\n        self.assertEqual(\n            list(s),\n            [\n                {\n                    \"priority\": \"1\",\n                    \"loc\": \"http://www.example.com/\",\n                    \"lastmod\": \"2009-08-16\",\n                    \"changefreq\": \"daily\",\n                },\n                {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n            ],\n        )\n\n    def test_sitemap_wrong_ns(self):\n        \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n        with these, though is not 100% confirmed\"\"\"\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n  <url xmlns=\"\">\n    <loc> http://www.example.com/</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>daily</changefreq>\n    <priority>1</priority>\n  </url>\n  <url xmlns=\"\">\n    <loc> http://www.example.com/2</loc>\n    <lastmod />\n  </url>\n</urlset>\n\"\"\"\n        )\n        self.assertEqual(\n            list(s),\n            [\n                {\n                    \"priority\": \"1\",\n                    \"loc\": \"http://www.example.com/\",\n                    \"lastmod\": \"2009-08-16\",\n                    \"changefreq\": \"daily\",\n                },\n                {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n            ],\n        )\n\n    def test_sitemap_wrong_ns2(self):\n        \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n        with these, though is not 100% confirmed\"\"\"\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset>\n  <url xmlns=\"\">\n    <loc> http://www.example.com/</loc>\n    <lastmod>2009-08-16</lastmod>\n    <changefreq>daily</changefreq>\n    <priority>1</priority>\n  </url>\n  <url xmlns=\"\">\n    <loc> http://www.example.com/2</loc>\n    <lastmod />\n  </url>\n</urlset>\n\"\"\"\n        )\n        assert s.type == \"urlset\"\n        self.assertEqual(\n            list(s),\n            [\n                {\n                    \"priority\": \"1\",\n                    \"loc\": \"http://www.example.com/\",\n                    \"lastmod\": \"2009-08-16\",\n                    \"changefreq\": \"daily\",\n                },\n                {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n            ],\n        )\n\n    def test_sitemap_urls_from_robots(self):\n        robots = \"\"\"User-agent: *\nDisallow: /aff/\nDisallow: /wl/\n\n# Search and shopping refining\nDisallow: /s*/*facet\nDisallow: /s*/*tags\n\n# Sitemap files\nSitemap: http://example.com/sitemap.xml\nSitemap: http://example.com/sitemap-product-index.xml\nSitemap: HTTP://example.com/sitemap-uppercase.xml\nSitemap: /sitemap-relative-url.xml\n\n# Forums\nDisallow: /forum/search/\nDisallow: /forum/active/\n\"\"\"\n        self.assertEqual(\n            list(sitemap_urls_from_robots(robots, base_url=\"http://example.com\")),\n            [\n                \"http://example.com/sitemap.xml\",\n                \"http://example.com/sitemap-product-index.xml\",\n                \"http://example.com/sitemap-uppercase.xml\",\n                \"http://example.com/sitemap-relative-url.xml\",\n            ],\n        )\n\n    def test_sitemap_blanklines(self):\n        \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n        s = Sitemap(\n            b\"\"\"\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n\n<!-- cache: cached = yes name = sitemap_jspCache key = sitemap -->\n<sitemap>\n<loc>http://www.example.com/sitemap1.xml</loc>\n<lastmod>2013-07-15</lastmod>\n</sitemap>\n\n<sitemap>\n<loc>http://www.example.com/sitemap2.xml</loc>\n<lastmod>2013-07-15</lastmod>\n</sitemap>\n\n<sitemap>\n<loc>http://www.example.com/sitemap3.xml</loc>\n<lastmod>2013-07-15</lastmod>\n</sitemap>\n\n<!-- end cache -->\n</sitemapindex>\n\"\"\"\n        )\n        self.assertEqual(\n            list(s),\n            [\n                {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap1.xml\"},\n                {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap2.xml\"},\n                {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap3.xml\"},\n            ],\n        )\n\n    def test_comment(self):\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/</loc>\n            <!-- this is a comment on which the parser might raise an exception if implemented incorrectly -->\n        </url>\n    </urlset>\"\"\"\n        )\n\n        self.assertEqual(list(s), [{\"loc\": \"http://www.example.com/\"}])\n\n    def test_alternate(self):\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n        <url>\n            <loc>http://www.example.com/english/</loc>\n            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n                href=\"http://www.example.com/deutsch/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"de-ch\"\n                href=\"http://www.example.com/schweiz-deutsch/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"en\"\n                href=\"http://www.example.com/english/\"/>\n            <xhtml:link rel=\"alternate\" hreflang=\"en\"/><!-- wrong tag without href -->\n        </url>\n    </urlset>\"\"\"\n        )\n\n        self.assertEqual(\n            list(s),\n            [\n                {\n                    \"loc\": \"http://www.example.com/english/\",\n                    \"alternate\": [\n                        \"http://www.example.com/deutsch/\",\n                        \"http://www.example.com/schweiz-deutsch/\",\n                        \"http://www.example.com/english/\",\n                    ],\n                }\n            ],\n        )\n\n    def test_xml_entity_expansion(self):\n        s = Sitemap(\n            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n          <!DOCTYPE foo [\n          <!ELEMENT foo ANY >\n          <!ENTITY xxe SYSTEM \"file:///etc/passwd\" >\n          ]>\n          <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n            <url>\n              <loc>http://127.0.0.1:8000/&xxe;</loc>\n            </url>\n          </urlset>\n        \"\"\"\n        )\n\n        self.assertEqual(list(s), [{\"loc\": \"http://127.0.0.1:8000/\"}])\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_conf.py": "import unittest\nimport warnings\n\nimport pytest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning, UsageError\nfrom scrapy.settings import BaseSettings, Settings\nfrom scrapy.utils.conf import (\n    arglist_to_dict,\n    build_component_list,\n    feed_complete_default_values_from_settings,\n    feed_process_params_from_cli,\n)\n\n\nclass BuildComponentListTest(unittest.TestCase):\n    def test_build_dict(self):\n        d = {\"one\": 1, \"two\": None, \"three\": 8, \"four\": 4}\n        self.assertEqual(\n            build_component_list(d, convert=lambda x: x), [\"one\", \"four\", \"three\"]\n        )\n\n    def test_backward_compatible_build_dict(self):\n        base = {\"one\": 1, \"two\": 2, \"three\": 3, \"five\": 5, \"six\": None}\n        custom = {\"two\": None, \"three\": 8, \"four\": 4}\n        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n            self.assertEqual(\n                build_component_list(base, custom, convert=lambda x: x),\n                [\"one\", \"four\", \"five\", \"three\"],\n            )\n\n    def test_return_list(self):\n        custom = [\"a\", \"b\", \"c\"]\n        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n            self.assertEqual(\n                build_component_list(None, custom, convert=lambda x: x), custom\n            )\n\n    def test_map_dict(self):\n        custom = {\"one\": 1, \"two\": 2, \"three\": 3}\n        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n            self.assertEqual(\n                build_component_list({}, custom, convert=lambda x: x.upper()),\n                [\"ONE\", \"TWO\", \"THREE\"],\n            )\n\n    def test_map_list(self):\n        custom = [\"a\", \"b\", \"c\"]\n        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n            self.assertEqual(\n                build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n            )\n\n    def test_duplicate_components_in_dict(self):\n        duplicate_dict = {\"one\": 1, \"two\": 2, \"ONE\": 4}\n        with self.assertRaises(ValueError):\n            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n                build_component_list({}, duplicate_dict, convert=lambda x: x.lower())\n\n    def test_duplicate_components_in_list(self):\n        duplicate_list = [\"a\", \"b\", \"a\"]\n        with self.assertRaises(ValueError) as cm:\n            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n                build_component_list(None, duplicate_list, convert=lambda x: x)\n        self.assertIn(str(duplicate_list), str(cm.exception))\n\n    def test_duplicate_components_in_basesettings(self):\n        # Higher priority takes precedence\n        duplicate_bs = BaseSettings({\"one\": 1, \"two\": 2}, priority=0)\n        duplicate_bs.set(\"ONE\", 4, priority=10)\n        self.assertEqual(\n            build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n            [\"two\", \"one\"],\n        )\n        duplicate_bs.set(\"one\", duplicate_bs[\"one\"], priority=20)\n        self.assertEqual(\n            build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n            [\"one\", \"two\"],\n        )\n        # Same priority raises ValueError\n        duplicate_bs.set(\"ONE\", duplicate_bs[\"ONE\"], priority=20)\n        with self.assertRaises(ValueError):\n            build_component_list(duplicate_bs, convert=lambda x: x.lower())\n\n    def test_valid_numbers(self):\n        # work well with None and numeric values\n        d = {\"a\": 10, \"b\": None, \"c\": 15, \"d\": 5.0}\n        self.assertEqual(build_component_list(d, convert=lambda x: x), [\"d\", \"a\", \"c\"])\n        d = {\n            \"a\": 33333333333333333333,\n            \"b\": 11111111111111111111,\n            \"c\": 22222222222222222222,\n        }\n        self.assertEqual(build_component_list(d, convert=lambda x: x), [\"b\", \"c\", \"a\"])\n        # raise exception for invalid values\n        d = {\"one\": \"5\"}\n        with self.assertRaises(ValueError):\n            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n                build_component_list({}, d, convert=lambda x: x)\n\n\nclass UtilsConfTestCase(unittest.TestCase):\n    def test_arglist_to_dict(self):\n        self.assertEqual(\n            arglist_to_dict([\"arg1=val1\", \"arg2=val2\"]),\n            {\"arg1\": \"val1\", \"arg2\": \"val2\"},\n        )\n\n\nclass FeedExportConfigTestCase(unittest.TestCase):\n    def test_feed_export_config_invalid_format(self):\n        settings = Settings()\n        self.assertRaises(\n            UsageError,\n            feed_process_params_from_cli,\n            settings,\n            [\"items.dat\"],\n            \"noformat\",\n        )\n\n    def test_feed_export_config_mismatch(self):\n        settings = Settings()\n        self.assertRaises(\n            UsageError,\n            feed_process_params_from_cli,\n            settings,\n            [\"items1.dat\", \"items2.dat\"],\n            \"noformat\",\n        )\n\n    def test_feed_export_config_backward_compatible(self):\n        with warnings.catch_warnings(record=True) as cw:\n            settings = Settings()\n            self.assertEqual(\n                {\"items.dat\": {\"format\": \"csv\"}},\n                feed_process_params_from_cli(settings, [\"items.dat\"], \"csv\"),\n            )\n            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n\n    def test_feed_export_config_explicit_formats(self):\n        settings = Settings()\n        self.assertEqual(\n            {\n                \"items_1.dat\": {\"format\": \"json\"},\n                \"items_2.dat\": {\"format\": \"xml\"},\n                \"items_3.dat\": {\"format\": \"csv\"},\n            },\n            feed_process_params_from_cli(\n                settings, [\"items_1.dat:json\", \"items_2.dat:xml\", \"items_3.dat:csv\"]\n            ),\n        )\n\n    def test_feed_export_config_implicit_formats(self):\n        settings = Settings()\n        self.assertEqual(\n            {\n                \"items_1.json\": {\"format\": \"json\"},\n                \"items_2.xml\": {\"format\": \"xml\"},\n                \"items_3.csv\": {\"format\": \"csv\"},\n            },\n            feed_process_params_from_cli(\n                settings, [\"items_1.json\", \"items_2.xml\", \"items_3.csv\"]\n            ),\n        )\n\n    def test_feed_export_config_stdout(self):\n        settings = Settings()\n        self.assertEqual(\n            {\"stdout:\": {\"format\": \"pickle\"}},\n            feed_process_params_from_cli(settings, [\"-:pickle\"]),\n        )\n\n    def test_feed_export_config_overwrite(self):\n        settings = Settings()\n        self.assertEqual(\n            {\"output.json\": {\"format\": \"json\", \"overwrite\": True}},\n            feed_process_params_from_cli(settings, [], None, [\"output.json\"]),\n        )\n\n    def test_output_and_overwrite_output(self):\n        with self.assertRaises(UsageError):\n            feed_process_params_from_cli(\n                Settings(),\n                [\"output1.json\"],\n                None,\n                [\"output2.json\"],\n            )\n\n    def test_feed_complete_default_values_from_settings_empty(self):\n        feed = {}\n        settings = Settings(\n            {\n                \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n                \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n                \"FEED_EXPORT_INDENT\": 42,\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_URI_PARAMS\": (1, 2, 3, 4),\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n            }\n        )\n        new_feed = feed_complete_default_values_from_settings(feed, settings)\n        self.assertEqual(\n            new_feed,\n            {\n                \"encoding\": \"custom encoding\",\n                \"fields\": [\"f1\", \"f2\", \"f3\"],\n                \"indent\": 42,\n                \"store_empty\": True,\n                \"uri_params\": (1, 2, 3, 4),\n                \"batch_item_count\": 2,\n                \"item_export_kwargs\": {},\n            },\n        )\n\n    def test_feed_complete_default_values_from_settings_non_empty(self):\n        feed = {\n            \"encoding\": \"other encoding\",\n            \"fields\": None,\n        }\n        settings = Settings(\n            {\n                \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n                \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n                \"FEED_EXPORT_INDENT\": 42,\n                \"FEED_STORE_EMPTY\": True,\n                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n            }\n        )\n        new_feed = feed_complete_default_values_from_settings(feed, settings)\n        self.assertEqual(\n            new_feed,\n            {\n                \"encoding\": \"other encoding\",\n                \"fields\": None,\n                \"indent\": 42,\n                \"store_empty\": True,\n                \"uri_params\": None,\n                \"batch_item_count\": 2,\n                \"item_export_kwargs\": {},\n            },\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_logformatter.py": "import unittest\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase as TwistedTestCase\n\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.logformatter import LogFormatter\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import ItemSpider\n\n\nclass CustomItem(Item):\n    name = Field()\n\n    def __str__(self):\n        return f\"name: {self['name']}\"\n\n\nclass LogFormatterTestCase(unittest.TestCase):\n    def setUp(self):\n        self.formatter = LogFormatter()\n        self.spider = Spider(\"default\")\n\n    def test_crawled_with_referer(self):\n        req = Request(\"http://www.example.com\")\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline, \"Crawled (200) <GET http://www.example.com> (referer: None)\"\n        )\n\n    def test_crawled_without_referer(self):\n        req = Request(\n            \"http://www.example.com\", headers={\"referer\": \"http://example.com\"}\n        )\n        res = Response(\"http://www.example.com\", flags=[\"cached\"])\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline,\n            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\",\n        )\n\n    def test_flags_in_request(self):\n        req = Request(\"http://www.example.com\", flags=[\"test\", \"flag\"])\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline,\n            \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\",\n        )\n\n    def test_dropped(self):\n        item = {}\n        exception = Exception(\"\\u2018\")\n        response = Response(\"http://www.example.com\")\n        logkws = self.formatter.dropped(item, exception, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        lines = logline.splitlines()\n        assert all(isinstance(x, str) for x in lines)\n        self.assertEqual(lines, [\"Dropped: \\u2018\", \"{}\"])\n\n    def test_item_error(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        item = {\"key\": \"value\"}\n        exception = Exception()\n        response = Response(\"http://www.example.com\")\n        logkws = self.formatter.item_error(item, exception, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(logline, \"Error processing {'key': 'value'}\")\n\n    def test_spider_error(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        failure = Failure(Exception())\n        request = Request(\n            \"http://www.example.com\", headers={\"Referer\": \"http://example.org\"}\n        )\n        response = Response(\"http://www.example.com\", request=request)\n        logkws = self.formatter.spider_error(failure, request, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline,\n            \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\",\n        )\n\n    def test_download_error_short(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        failure = Failure(Exception())\n        request = Request(\"http://www.example.com\")\n        logkws = self.formatter.download_error(failure, request, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(logline, \"Error downloading <GET http://www.example.com>\")\n\n    def test_download_error_long(self):\n        # In practice, the complete traceback is shown by passing the\n        # 'exc_info' argument to the logging function\n        failure = Failure(Exception())\n        request = Request(\"http://www.example.com\")\n        logkws = self.formatter.download_error(\n            failure, request, self.spider, \"Some message\"\n        )\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline, \"Error downloading <GET http://www.example.com>: Some message\"\n        )\n\n    def test_scraped(self):\n        item = CustomItem()\n        item[\"name\"] = \"\\xa3\"\n        response = Response(\"http://www.example.com\")\n        logkws = self.formatter.scraped(item, response, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        lines = logline.splitlines()\n        assert all(isinstance(x, str) for x in lines)\n        self.assertEqual(\n            lines, [\"Scraped from <200 http://www.example.com>\", \"name: \\xa3\"]\n        )\n\n\nclass LogFormatterSubclass(LogFormatter):\n    def crawled(self, request, response, spider):\n        kwargs = super().crawled(request, response, spider)\n        CRAWLEDMSG = \"Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s\"\n        log_args = kwargs[\"args\"]\n        log_args[\"flags\"] = str(request.flags)\n        return {\n            \"level\": kwargs[\"level\"],\n            \"msg\": CRAWLEDMSG,\n            \"args\": log_args,\n        }\n\n\nclass LogformatterSubclassTest(LogFormatterTestCase):\n    def setUp(self):\n        self.formatter = LogFormatterSubclass()\n        self.spider = Spider(\"default\")\n\n    def test_crawled_with_referer(self):\n        req = Request(\"http://www.example.com\")\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline, \"Crawled (200) <GET http://www.example.com> (referer: None) []\"\n        )\n\n    def test_crawled_without_referer(self):\n        req = Request(\n            \"http://www.example.com\",\n            headers={\"referer\": \"http://example.com\"},\n            flags=[\"cached\"],\n        )\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline,\n            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\",\n        )\n\n    def test_flags_in_request(self):\n        req = Request(\"http://www.example.com\", flags=[\"test\", \"flag\"])\n        res = Response(\"http://www.example.com\")\n        logkws = self.formatter.crawled(req, res, self.spider)\n        logline = logkws[\"msg\"] % logkws[\"args\"]\n        self.assertEqual(\n            logline,\n            \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\",\n        )\n\n\nclass SkipMessagesLogFormatter(LogFormatter):\n    def crawled(self, *args, **kwargs):\n        return None\n\n    def scraped(self, *args, **kwargs):\n        return None\n\n    def dropped(self, *args, **kwargs):\n        return None\n\n\nclass DropSomeItemsPipeline:\n    drop = True\n\n    def process_item(self, item, spider):\n        if self.drop:\n            self.drop = False\n            raise DropItem(\"Ignoring item\")\n        else:\n            self.drop = True\n\n\nclass ShowOrSkipMessagesTestCase(TwistedTestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self.base_settings = {\n            \"LOG_LEVEL\": \"DEBUG\",\n            \"ITEM_PIPELINES\": {\n                DropSomeItemsPipeline: 300,\n            },\n        }\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_show_messages(self):\n        crawler = get_crawler(ItemSpider, self.base_settings)\n        with LogCapture() as lc:\n            yield crawler.crawl(mockserver=self.mockserver)\n        self.assertIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n        self.assertIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n        self.assertIn(\"Dropped: Ignoring item\", str(lc))\n\n    @defer.inlineCallbacks\n    def test_skip_messages(self):\n        settings = self.base_settings.copy()\n        settings[\"LOG_FORMATTER\"] = SkipMessagesLogFormatter\n        crawler = get_crawler(ItemSpider, settings)\n        with LogCapture() as lc:\n            yield crawler.crawl(mockserver=self.mockserver)\n        self.assertNotIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n        self.assertNotIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n        self.assertNotIn(\"Dropped: Ignoring item\", str(lc))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_pipeline_media.py": "from typing import Optional\n\nfrom testfixtures import LogCapture\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\nfrom twisted.trial import unittest\n\nfrom scrapy import signals\nfrom scrapy.http import Request, Response\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.files import FileException\nfrom scrapy.pipelines.media import MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.signal import disconnect_all\nfrom scrapy.utils.test import get_crawler\n\ntry:\n    from PIL import Image  # noqa: imported just to check for the import error\nexcept ImportError:\n    skip_pillow: Optional[str] = (\n        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n    )\nelse:\n    skip_pillow = None\n\n\ndef _mocked_download_func(request, info):\n    assert request.callback is NO_CALLBACK\n    response = request.meta.get(\"response\")\n    return response() if callable(response) else response\n\n\nclass UserDefinedPipeline(MediaPipeline):\n\n    def media_to_download(self, request, info, *, item=None):\n        pass\n\n    def get_media_requests(self, item, info):\n        pass\n\n    def media_downloaded(self, response, request, info, *, item=None):\n        return {}\n\n    def media_failed(self, failure, request, info):\n        return failure\n\n    def file_path(self, request, response=None, info=None, *, item=None):\n        return \"\"\n\n\nclass BaseMediaPipelineTestCase(unittest.TestCase):\n    pipeline_class = UserDefinedPipeline\n    settings = None\n\n    def setUp(self):\n        spider_cls = Spider\n        self.spider = spider_cls(\"media.com\")\n        crawler = get_crawler(spider_cls, self.settings)\n        self.pipe = self.pipeline_class.from_crawler(crawler)\n        self.pipe.download_func = _mocked_download_func\n        self.pipe.open_spider(self.spider)\n        self.info = self.pipe.spiderinfo\n        self.fingerprint = crawler.request_fingerprinter.fingerprint\n\n    def tearDown(self):\n        for name, signal in vars(signals).items():\n            if not name.startswith(\"_\"):\n                disconnect_all(signal)\n\n    def test_modify_media_request(self):\n        request = Request(\"http://url\")\n        self.pipe._modify_media_request(request)\n        assert request.meta == {\"handle_httpstatus_all\": True}\n\n    def test_should_remove_req_res_references_before_caching_the_results(self):\n        \"\"\"Regression test case to prevent a memory leak in the Media Pipeline.\n\n        The memory leak is triggered when an exception is raised when a Response\n        scheduled by the Media Pipeline is being returned. For example, when a\n        FileException('download-error') is raised because the Response status\n        code is not 200 OK.\n\n        It happens because we are keeping a reference to the Response object\n        inside the FileException context. This is caused by the way Twisted\n        return values from inline callbacks. It raises a custom exception\n        encapsulating the original return value.\n\n        The solution is to remove the exception context when this context is a\n        _DefGen_Return instance, the BaseException used by Twisted to pass the\n        returned value from those inline callbacks.\n\n        Maybe there's a better and more reliable way to test the case described\n        here, but it would be more complicated and involve running - or at least\n        mocking - some async steps from the Media Pipeline. The current test\n        case is simple and detects the problem very fast. On the other hand, it\n        would not detect another kind of leak happening due to old object\n        references being kept inside the Media Pipeline cache.\n\n        This problem does not occur in Python 2.7 since we don't have Exception\n        Chaining (https://www.python.org/dev/peps/pep-3134/).\n        \"\"\"\n        # Create sample pair of Request and Response objects\n        request = Request(\"http://url\")\n        response = Response(\"http://url\", body=b\"\", request=request)\n\n        # Simulate the Media Pipeline behavior to produce a Twisted Failure\n        try:\n            # Simulate a Twisted inline callback returning a Response\n            raise StopIteration(response)\n        except StopIteration as exc:\n            def_gen_return_exc = exc\n            try:\n                # Simulate the media_downloaded callback raising a FileException\n                # This usually happens when the status code is not 200 OK\n                raise FileException(\"download-error\")\n            except Exception as exc:\n                file_exc = exc\n                # Simulate Twisted capturing the FileException\n                # It encapsulates the exception inside a Twisted Failure\n                failure = Failure(file_exc)\n\n        # The Failure should encapsulate a FileException ...\n        self.assertEqual(failure.value, file_exc)\n        # ... and it should have the StopIteration exception set as its context\n        self.assertEqual(failure.value.__context__, def_gen_return_exc)\n\n        # Let's calculate the request fingerprint and fake some runtime data...\n        fp = self.fingerprint(request)\n        info = self.pipe.spiderinfo\n        info.downloading.add(fp)\n        info.waiting[fp] = []\n\n        # When calling the method that caches the Request's result ...\n        self.pipe._cache_result_and_execute_waiters(failure, fp, info)\n        # ... it should store the Twisted Failure ...\n        self.assertEqual(info.downloaded[fp], failure)\n        # ... encapsulating the original FileException ...\n        self.assertEqual(info.downloaded[fp].value, file_exc)\n        # ... but it should not store the StopIteration exception on its context\n        context = getattr(info.downloaded[fp].value, \"__context__\", None)\n        self.assertIsNone(context)\n\n    def test_default_item_completed(self):\n        item = {\"name\": \"name\"}\n        assert self.pipe.item_completed([], item, self.info) is item\n\n        # Check that failures are logged by default\n        fail = Failure(Exception())\n        results = [(True, 1), (False, fail)]\n\n        with LogCapture() as log:\n            new_item = self.pipe.item_completed(results, item, self.info)\n\n        assert new_item is item\n        assert len(log.records) == 1\n        record = log.records[0]\n        assert record.levelname == \"ERROR\"\n        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n\n        # disable failure logging and check again\n        self.pipe.LOG_FAILED_RESULTS = False\n        with LogCapture() as log:\n            new_item = self.pipe.item_completed(results, item, self.info)\n        assert new_item is item\n        assert len(log.records) == 0\n\n    @inlineCallbacks\n    def test_default_process_item(self):\n        item = {\"name\": \"name\"}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        assert new_item is item\n\n\nclass MockedMediaPipeline(UserDefinedPipeline):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._mockcalled = []\n\n    def download(self, request, info):\n        self._mockcalled.append(\"download\")\n        return super().download(request, info)\n\n    def media_to_download(self, request, info, *, item=None):\n        self._mockcalled.append(\"media_to_download\")\n        if \"result\" in request.meta:\n            return request.meta.get(\"result\")\n        return super().media_to_download(request, info)\n\n    def get_media_requests(self, item, info):\n        self._mockcalled.append(\"get_media_requests\")\n        return item.get(\"requests\")\n\n    def media_downloaded(self, response, request, info, *, item=None):\n        self._mockcalled.append(\"media_downloaded\")\n        return super().media_downloaded(response, request, info)\n\n    def media_failed(self, failure, request, info):\n        self._mockcalled.append(\"media_failed\")\n        return super().media_failed(failure, request, info)\n\n    def item_completed(self, results, item, info):\n        self._mockcalled.append(\"item_completed\")\n        item = super().item_completed(results, item, info)\n        item[\"results\"] = results\n        return item\n\n\nclass MediaPipelineTestCase(BaseMediaPipelineTestCase):\n    pipeline_class = MockedMediaPipeline\n\n    def _errback(self, result):\n        self.pipe._mockcalled.append(\"request_errback\")\n        return result\n\n    @inlineCallbacks\n    def test_result_succeed(self):\n        rsp = Response(\"http://url1\")\n        req = Request(\n            \"http://url1\",\n            meta={\"response\": rsp},\n            errback=self._errback,\n        )\n        item = {\"requests\": req}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertEqual(new_item[\"results\"], [(True, {})])\n        self.assertEqual(\n            self.pipe._mockcalled,\n            [\n                \"get_media_requests\",\n                \"media_to_download\",\n                \"media_downloaded\",\n                \"item_completed\",\n            ],\n        )\n\n    @inlineCallbacks\n    def test_result_failure(self):\n        self.pipe.LOG_FAILED_RESULTS = False\n        fail = Failure(Exception())\n        req = Request(\n            \"http://url1\",\n            meta={\"response\": fail},\n            errback=self._errback,\n        )\n        item = {\"requests\": req}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertEqual(new_item[\"results\"], [(False, fail)])\n        self.assertEqual(\n            self.pipe._mockcalled,\n            [\n                \"get_media_requests\",\n                \"media_to_download\",\n                \"media_failed\",\n                \"request_errback\",\n                \"item_completed\",\n            ],\n        )\n\n    @inlineCallbacks\n    def test_mix_of_success_and_failure(self):\n        self.pipe.LOG_FAILED_RESULTS = False\n        rsp1 = Response(\"http://url1\")\n        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n        fail = Failure(Exception())\n        req2 = Request(\"http://url2\", meta={\"response\": fail})\n        item = {\"requests\": [req1, req2]}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertEqual(new_item[\"results\"], [(True, {}), (False, fail)])\n        m = self.pipe._mockcalled\n        # only once\n        self.assertEqual(m[0], \"get_media_requests\")  # first hook called\n        self.assertEqual(m.count(\"get_media_requests\"), 1)\n        self.assertEqual(m.count(\"item_completed\"), 1)\n        self.assertEqual(m[-1], \"item_completed\")  # last hook called\n        # twice, one per request\n        self.assertEqual(m.count(\"media_to_download\"), 2)\n        # one to handle success and other for failure\n        self.assertEqual(m.count(\"media_downloaded\"), 1)\n        self.assertEqual(m.count(\"media_failed\"), 1)\n\n    @inlineCallbacks\n    def test_get_media_requests(self):\n        # returns single Request (without callback)\n        req = Request(\"http://url\")\n        item = {\"requests\": req}  # pass a single item\n        new_item = yield self.pipe.process_item(item, self.spider)\n        assert new_item is item\n        self.assertIn(self.fingerprint(req), self.info.downloaded)\n\n        # returns iterable of Requests\n        req1 = Request(\"http://url1\")\n        req2 = Request(\"http://url2\")\n        item = {\"requests\": iter([req1, req2])}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        assert new_item is item\n        assert self.fingerprint(req1) in self.info.downloaded\n        assert self.fingerprint(req2) in self.info.downloaded\n\n    @inlineCallbacks\n    def test_results_are_cached_across_multiple_items(self):\n        rsp1 = Response(\"http://url1\")\n        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n        item = {\"requests\": req1}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertTrue(new_item is item)\n        self.assertEqual(new_item[\"results\"], [(True, {})])\n\n        # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n        req2 = Request(\n            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n        )\n        item = {\"requests\": req2}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertTrue(new_item is item)\n        self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n        self.assertEqual(new_item[\"results\"], [(True, {})])\n\n    @inlineCallbacks\n    def test_results_are_cached_for_requests_of_single_item(self):\n        rsp1 = Response(\"http://url1\")\n        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n        req2 = Request(\n            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n        )\n        item = {\"requests\": [req1, req2]}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertTrue(new_item is item)\n        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n\n    @inlineCallbacks\n    def test_wait_if_request_is_downloading(self):\n        def _check_downloading(response):\n            fp = self.fingerprint(req1)\n            self.assertTrue(fp in self.info.downloading)\n            self.assertTrue(fp in self.info.waiting)\n            self.assertTrue(fp not in self.info.downloaded)\n            self.assertEqual(len(self.info.waiting[fp]), 2)\n            return response\n\n        rsp1 = Response(\"http://url\")\n\n        def rsp1_func():\n            dfd = Deferred().addCallback(_check_downloading)\n            reactor.callLater(0.1, dfd.callback, rsp1)\n            return dfd\n\n        def rsp2_func():\n            self.fail(\"it must cache rsp1 result and must not try to redownload\")\n\n        req1 = Request(\"http://url\", meta={\"response\": rsp1_func})\n        req2 = Request(req1.url, meta={\"response\": rsp2_func})\n        item = {\"requests\": [req1, req2]}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n\n    @inlineCallbacks\n    def test_use_media_to_download_result(self):\n        req = Request(\"http://url\", meta={\"result\": \"ITSME\", \"response\": self.fail})\n        item = {\"requests\": req}\n        new_item = yield self.pipe.process_item(item, self.spider)\n        self.assertEqual(new_item[\"results\"], [(True, \"ITSME\")])\n        self.assertEqual(\n            self.pipe._mockcalled,\n            [\"get_media_requests\", \"media_to_download\", \"item_completed\"],\n        )\n\n    def test_key_for_pipe(self):\n        self.assertEqual(\n            self.pipe._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\"),\n            \"MOCKEDMEDIAPIPELINE_IMAGES\",\n        )\n\n\nclass MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n\n    def _assert_request_no3xx(self, pipeline_class, settings):\n        pipe = pipeline_class(settings=Settings(settings))\n        request = Request(\"http://url\")\n        pipe._modify_media_request(request)\n\n        self.assertIn(\"handle_httpstatus_list\", request.meta)\n        for status, check in [\n            (200, True),\n            # These are the status codes we want\n            # the downloader to handle itself\n            (301, False),\n            (302, False),\n            (302, False),\n            (307, False),\n            (308, False),\n            # we still want to get 4xx and 5xx\n            (400, True),\n            (404, True),\n            (500, True),\n        ]:\n            if check:\n                self.assertIn(status, request.meta[\"handle_httpstatus_list\"])\n            else:\n                self.assertNotIn(status, request.meta[\"handle_httpstatus_list\"])\n\n    def test_subclass_standard_setting(self):\n\n        self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n\n    def test_subclass_specific_setting(self):\n\n        self._assert_request_no3xx(\n            UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n        )\n", "tests/test_spidermiddleware_depth.py": "from unittest import TestCase\n\nfrom scrapy.http import Request, Response\nfrom scrapy.spidermiddlewares.depth import DepthMiddleware\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.test import get_crawler\n\n\nclass TestDepthMiddleware(TestCase):\n    def setUp(self):\n        crawler = get_crawler(Spider)\n        self.spider = crawler._create_spider(\"scrapytest.org\")\n\n        self.stats = StatsCollector(crawler)\n        self.stats.open_spider(self.spider)\n\n        self.mw = DepthMiddleware(1, self.stats, True)\n\n    def test_process_spider_output(self):\n        req = Request(\"http://scrapytest.org\")\n        resp = Response(\"http://scrapytest.org\")\n        resp.request = req\n        result = [Request(\"http://scrapytest.org\")]\n\n        out = list(self.mw.process_spider_output(resp, result, self.spider))\n        self.assertEqual(out, result)\n\n        rdc = self.stats.get_value(\"request_depth_count/1\", spider=self.spider)\n        self.assertEqual(rdc, 1)\n\n        req.meta[\"depth\"] = 1\n\n        out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n        self.assertEqual(out2, [])\n\n        rdm = self.stats.get_value(\"request_depth_max\", spider=self.spider)\n        self.assertEqual(rdm, 1)\n\n    def tearDown(self):\n        self.stats.close_spider(self.spider, \"\")\n", "tests/test_downloadermiddleware_ajaxcrawlable.py": "import unittest\n\nfrom scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware\nfrom scrapy.http import HtmlResponse, Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\n\n__doctests__ = [\"scrapy.downloadermiddlewares.ajaxcrawl\"]\n\n\nclass AjaxCrawlMiddlewareTest(unittest.TestCase):\n    def setUp(self):\n        crawler = get_crawler(Spider, {\"AJAXCRAWL_ENABLED\": True})\n        self.spider = crawler._create_spider(\"foo\")\n        self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n\n    def _ajaxcrawlable_body(self):\n        return b'<html><head><meta name=\"fragment\" content=\"!\"/></head><body></body></html>'\n\n    def _req_resp(self, url, req_kwargs=None, resp_kwargs=None):\n        req = Request(url, **(req_kwargs or {}))\n        resp = HtmlResponse(url, request=req, **(resp_kwargs or {}))\n        return req, resp\n\n    def test_non_get(self):\n        req, resp = self._req_resp(\"http://example.com/\", {\"method\": \"HEAD\"})\n        resp2 = self.mw.process_response(req, resp, self.spider)\n        self.assertEqual(resp, resp2)\n\n    def test_binary_response(self):\n        req = Request(\"http://example.com/\")\n        resp = Response(\"http://example.com/\", body=b\"foobar\\x00\\x01\\x02\", request=req)\n        resp2 = self.mw.process_response(req, resp, self.spider)\n        self.assertIs(resp, resp2)\n\n    def test_ajaxcrawl(self):\n        req, resp = self._req_resp(\n            \"http://example.com/\",\n            {\"meta\": {\"foo\": \"bar\"}},\n            {\"body\": self._ajaxcrawlable_body()},\n        )\n        req2 = self.mw.process_response(req, resp, self.spider)\n        self.assertEqual(req2.url, \"http://example.com/?_escaped_fragment_=\")\n        self.assertEqual(req2.meta[\"foo\"], \"bar\")\n\n    def test_ajaxcrawl_loop(self):\n        req, resp = self._req_resp(\n            \"http://example.com/\", {}, {\"body\": self._ajaxcrawlable_body()}\n        )\n        req2 = self.mw.process_response(req, resp, self.spider)\n        resp2 = HtmlResponse(req2.url, body=resp.body, request=req2)\n        resp3 = self.mw.process_response(req2, resp2, self.spider)\n\n        assert isinstance(resp3, HtmlResponse), (resp3.__class__, resp3)\n        self.assertEqual(resp3.request.url, \"http://example.com/?_escaped_fragment_=\")\n        assert resp3 is resp2\n\n    def test_noncrawlable_body(self):\n        req, resp = self._req_resp(\n            \"http://example.com/\", {}, {\"body\": b\"<html></html>\"}\n        )\n        resp2 = self.mw.process_response(req, resp, self.spider)\n        self.assertIs(resp, resp2)\n", "tests/test_request_attribute_binding.py": "from testfixtures import LogCapture\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy import Request, signals\nfrom scrapy.http.response import Response\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer\nfrom tests.spiders import SingleRequestSpider\n\nOVERRIDDEN_URL = \"https://example.org\"\n\n\nclass ProcessResponseMiddleware:\n    def process_response(self, request, response, spider):\n        return response.replace(request=Request(OVERRIDDEN_URL))\n\n\nclass RaiseExceptionRequestMiddleware:\n    def process_request(self, request, spider):\n        1 / 0\n        return request\n\n\nclass CatchExceptionOverrideRequestMiddleware:\n    def process_exception(self, request, exception, spider):\n        return Response(\n            url=\"http://localhost/\",\n            body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n            request=Request(OVERRIDDEN_URL),\n        )\n\n\nclass CatchExceptionDoNotOverrideRequestMiddleware:\n    def process_exception(self, request, exception, spider):\n        return Response(\n            url=\"http://localhost/\",\n            body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n        )\n\n\nclass AlternativeCallbacksSpider(SingleRequestSpider):\n    name = \"alternative_callbacks_spider\"\n\n    def alt_callback(self, response, foo=None):\n        self.logger.info(\"alt_callback was invoked with foo=%s\", foo)\n\n\nclass AlternativeCallbacksMiddleware:\n    def process_response(self, request, response, spider):\n        new_request = request.replace(\n            url=OVERRIDDEN_URL,\n            callback=spider.alt_callback,\n            cb_kwargs={\"foo\": \"bar\"},\n        )\n        return response.replace(request=new_request)\n\n\nclass CrawlTestCase(TestCase):\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_response_200(self):\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(SingleRequestSpider)\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        response = crawler.spider.meta[\"responses\"][0]\n        self.assertEqual(response.request.url, url)\n\n    @defer.inlineCallbacks\n    def test_response_error(self):\n        for status in (\"404\", \"500\"):\n            url = self.mockserver.url(f\"/status?n={status}\")\n            crawler = get_crawler(SingleRequestSpider)\n            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n            failure = crawler.spider.meta[\"failure\"]\n            response = failure.value.response\n            self.assertEqual(failure.request.url, url)\n            self.assertEqual(response.request.url, url)\n\n    @defer.inlineCallbacks\n    def test_downloader_middleware_raise_exception(self):\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    RaiseExceptionRequestMiddleware: 590,\n                },\n            },\n        )\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        failure = crawler.spider.meta[\"failure\"]\n        self.assertEqual(failure.request.url, url)\n        self.assertIsInstance(failure.value, ZeroDivisionError)\n\n    @defer.inlineCallbacks\n    def test_downloader_middleware_override_request_in_process_response(self):\n        \"\"\"\n        Downloader middleware which returns a response with an specific 'request' attribute.\n\n        * The spider callback should receive the overridden response.request\n        * Handlers listening to the response_received signal should receive the overridden response.request\n        * The \"crawled\" log message should show the overridden response.request\n        \"\"\"\n        signal_params = {}\n\n        def signal_handler(response, request, spider):\n            signal_params[\"response\"] = response\n            signal_params[\"request\"] = request\n\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    ProcessResponseMiddleware: 595,\n                }\n            },\n        )\n        crawler.signals.connect(signal_handler, signal=signals.response_received)\n\n        with LogCapture() as log:\n            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n\n        response = crawler.spider.meta[\"responses\"][0]\n        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n\n        self.assertEqual(signal_params[\"response\"].url, url)\n        self.assertEqual(signal_params[\"request\"].url, OVERRIDDEN_URL)\n\n        log.check_present(\n            (\n                \"scrapy.core.engine\",\n                \"DEBUG\",\n                f\"Crawled (200) <GET {OVERRIDDEN_URL}> (referer: None)\",\n            ),\n        )\n\n    @defer.inlineCallbacks\n    def test_downloader_middleware_override_in_process_exception(self):\n        \"\"\"\n        An exception is raised but caught by the next middleware, which\n        returns a Response with a specific 'request' attribute.\n\n        The spider callback should receive the overridden response.request\n        \"\"\"\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    RaiseExceptionRequestMiddleware: 590,\n                    CatchExceptionOverrideRequestMiddleware: 595,\n                },\n            },\n        )\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        response = crawler.spider.meta[\"responses\"][0]\n        self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n\n    @defer.inlineCallbacks\n    def test_downloader_middleware_do_not_override_in_process_exception(self):\n        \"\"\"\n        An exception is raised but caught by the next middleware, which\n        returns a Response without a specific 'request' attribute.\n\n        The spider callback should receive the original response.request\n        \"\"\"\n        url = self.mockserver.url(\"/status?n=200\")\n        crawler = get_crawler(\n            SingleRequestSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    RaiseExceptionRequestMiddleware: 590,\n                    CatchExceptionDoNotOverrideRequestMiddleware: 595,\n                },\n            },\n        )\n        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n        response = crawler.spider.meta[\"responses\"][0]\n        self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n        self.assertEqual(response.request.url, url)\n\n    @defer.inlineCallbacks\n    def test_downloader_middleware_alternative_callback(self):\n        \"\"\"\n        Downloader middleware which returns a response with a\n        specific 'request' attribute, with an alternative callback\n        \"\"\"\n        crawler = get_crawler(\n            AlternativeCallbacksSpider,\n            {\n                \"DOWNLOADER_MIDDLEWARES\": {\n                    AlternativeCallbacksMiddleware: 595,\n                }\n            },\n        )\n\n        with LogCapture() as log:\n            url = self.mockserver.url(\"/status?n=200\")\n            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n\n        log.check_present(\n            (\n                \"alternative_callbacks_spider\",\n                \"INFO\",\n                \"alt_callback was invoked with foo=bar\",\n            ),\n        )\n", "tests/test_command_version.py": "import sys\n\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\n\nimport scrapy\nfrom scrapy.utils.testproc import ProcessTest\n\n\nclass VersionTest(ProcessTest, unittest.TestCase):\n    command = \"version\"\n\n    @defer.inlineCallbacks\n    def test_output(self):\n        encoding = sys.stdout.encoding or \"utf-8\"\n        _, out, _ = yield self.execute([])\n        self.assertEqual(\n            out.strip().decode(encoding),\n            f\"Scrapy {scrapy.__version__}\",\n        )\n\n    @defer.inlineCallbacks\n    def test_verbose_output(self):\n        encoding = sys.stdout.encoding or \"utf-8\"\n        _, out, _ = yield self.execute([\"-v\"])\n        headers = [\n            line.partition(\":\")[0].strip()\n            for line in out.strip().decode(encoding).splitlines()\n        ]\n        self.assertEqual(\n            headers,\n            [\n                \"Scrapy\",\n                \"lxml\",\n                \"libxml2\",\n                \"cssselect\",\n                \"parsel\",\n                \"w3lib\",\n                \"Twisted\",\n                \"Python\",\n                \"pyOpenSSL\",\n                \"cryptography\",\n                \"Platform\",\n            ],\n        )\n", "tests/test_extension_telnet.py": "from twisted.conch.telnet import ITelnetProtocol\nfrom twisted.cred import credentials\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\n\nfrom scrapy.extensions.telnet import TelnetConsole\nfrom scrapy.utils.test import get_crawler\n\n\nclass TelnetExtensionTest(unittest.TestCase):\n    def _get_console_and_portal(self, settings=None):\n        crawler = get_crawler(settings_dict=settings)\n        console = TelnetConsole(crawler)\n\n        # This function has some side effects we don't need for this test\n        console._get_telnet_vars = lambda: {}\n\n        console.start_listening()\n        protocol = console.protocol()\n        portal = protocol.protocolArgs[0]\n\n        return console, portal\n\n    @defer.inlineCallbacks\n    def test_bad_credentials(self):\n        console, portal = self._get_console_and_portal()\n        creds = credentials.UsernamePassword(b\"username\", b\"password\")\n        d = portal.login(creds, None, ITelnetProtocol)\n        yield self.assertFailure(d, ValueError)\n        console.stop_listening()\n\n    @defer.inlineCallbacks\n    def test_good_credentials(self):\n        console, portal = self._get_console_and_portal()\n        creds = credentials.UsernamePassword(\n            console.username.encode(\"utf8\"), console.password.encode(\"utf8\")\n        )\n        d = portal.login(creds, None, ITelnetProtocol)\n        yield d\n        console.stop_listening()\n\n    @defer.inlineCallbacks\n    def test_custom_credentials(self):\n        settings = {\n            \"TELNETCONSOLE_USERNAME\": \"user\",\n            \"TELNETCONSOLE_PASSWORD\": \"pass\",\n        }\n        console, portal = self._get_console_and_portal(settings=settings)\n        creds = credentials.UsernamePassword(b\"user\", b\"pass\")\n        d = portal.login(creds, None, ITelnetProtocol)\n        yield d\n        console.stop_listening()\n", "tests/test_downloadermiddleware_httpauth.py": "import unittest\n\nfrom w3lib.http import basic_auth_header\n\nfrom scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\nfrom scrapy.http import Request\nfrom scrapy.spiders import Spider\n\n\nclass TestSpiderLegacy(Spider):\n    http_user = \"foo\"\n    http_pass = \"bar\"\n\n\nclass TestSpider(Spider):\n    http_user = \"foo\"\n    http_pass = \"bar\"\n    http_auth_domain = \"example.com\"\n\n\nclass TestSpiderAny(Spider):\n    http_user = \"foo\"\n    http_pass = \"bar\"\n    http_auth_domain = None\n\n\nclass HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n    def setUp(self):\n        self.spider = TestSpiderLegacy(\"foo\")\n\n    def test_auth(self):\n        with self.assertRaises(AttributeError):\n            mw = HttpAuthMiddleware()\n            mw.spider_opened(self.spider)\n\n\nclass HttpAuthMiddlewareTest(unittest.TestCase):\n    def setUp(self):\n        self.mw = HttpAuthMiddleware()\n        self.spider = TestSpider(\"foo\")\n        self.mw.spider_opened(self.spider)\n\n    def tearDown(self):\n        del self.mw\n\n    def test_no_auth(self):\n        req = Request(\"http://example-noauth.com/\")\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertNotIn(\"Authorization\", req.headers)\n\n    def test_auth_domain(self):\n        req = Request(\"http://example.com/\")\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n\n    def test_auth_subdomain(self):\n        req = Request(\"http://foo.example.com/\")\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n\n    def test_auth_already_set(self):\n        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n\n\nclass HttpAuthAnyMiddlewareTest(unittest.TestCase):\n    def setUp(self):\n        self.mw = HttpAuthMiddleware()\n        self.spider = TestSpiderAny(\"foo\")\n        self.mw.spider_opened(self.spider)\n\n    def tearDown(self):\n        del self.mw\n\n    def test_auth(self):\n        req = Request(\"http://example.com/\")\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n\n    def test_auth_already_set(self):\n        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n        assert self.mw.process_request(req, self.spider) is None\n        self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n", "tests/spiders.py": "\"\"\"\nSome spiders used for testing and benchmarking\n\"\"\"\n\nimport asyncio\nimport time\nfrom typing import Optional\nfrom urllib.parse import urlencode\n\nfrom twisted.internet import defer\n\nfrom scrapy import signals\nfrom scrapy.exceptions import StopDownload\nfrom scrapy.http import Request\nfrom scrapy.item import Item\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import Spider\nfrom scrapy.spiders.crawl import CrawlSpider, Rule\nfrom scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future\nfrom scrapy.utils.test import get_from_asyncio_queue, get_web_client_agent_req\n\n\nclass MockServerSpider(Spider):\n    def __init__(self, mockserver=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mockserver = mockserver\n\n\nclass MetaSpider(MockServerSpider):\n    name = \"meta\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.meta = {}\n\n    def closed(self, reason):\n        self.meta[\"close_reason\"] = reason\n\n\nclass FollowAllSpider(MetaSpider):\n    name = \"follow\"\n    link_extractor = LinkExtractor()\n\n    def __init__(\n        self, total=10, show=20, order=\"rand\", maxlatency=0.0, *args, **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.urls_visited = []\n        self.times = []\n        qargs = {\"total\": total, \"show\": show, \"order\": order, \"maxlatency\": maxlatency}\n        url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=True)}\")\n        self.start_urls = [url]\n\n    def parse(self, response):\n        self.urls_visited.append(response.url)\n        self.times.append(time.time())\n        for link in self.link_extractor.extract_links(response):\n            yield Request(link.url, callback=self.parse)\n\n\nclass DelaySpider(MetaSpider):\n    name = \"delay\"\n\n    def __init__(self, n=1, b=0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n = n\n        self.b = b\n        self.t1 = self.t2 = self.t2_err = 0\n\n    def start_requests(self):\n        self.t1 = time.time()\n        url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n        yield Request(url, callback=self.parse, errback=self.errback)\n\n    def parse(self, response):\n        self.t2 = time.time()\n\n    def errback(self, failure):\n        self.t2_err = time.time()\n\n\nclass LogSpider(MetaSpider):\n    name = \"log_spider\"\n\n    def log_debug(self, message: str, extra: Optional[dict] = None):\n        self.logger.debug(message, extra=extra)\n\n    def log_info(self, message: str, extra: Optional[dict] = None):\n        self.logger.info(message, extra=extra)\n\n    def log_warning(self, message: str, extra: Optional[dict] = None):\n        self.logger.warning(message, extra=extra)\n\n    def log_error(self, message: str, extra: Optional[dict] = None):\n        self.logger.error(message, extra=extra)\n\n    def log_critical(self, message: str, extra: Optional[dict] = None):\n        self.logger.critical(message, extra=extra)\n\n    def parse(self, response):\n        pass\n\n\nclass SlowSpider(DelaySpider):\n    name = \"slow\"\n\n    def start_requests(self):\n        # 1st response is fast\n        url = self.mockserver.url(\"/delay?n=0&b=0\")\n        yield Request(url, callback=self.parse, errback=self.errback)\n\n        # 2nd response is slow\n        url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n        yield Request(url, callback=self.parse, errback=self.errback)\n\n    def parse(self, response):\n        yield Item()\n\n\nclass SimpleSpider(MetaSpider):\n    name = \"simple\"\n\n    def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = [url]\n\n    def parse(self, response):\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefSpider(SimpleSpider):\n    name = \"asyncdef\"\n\n    async def parse(self, response):\n        await defer.succeed(42)\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefAsyncioSpider(SimpleSpider):\n    name = \"asyncdef_asyncio\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}\")\n\n\nclass AsyncDefAsyncioReturnSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_return\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}\")\n        return [{\"id\": 1}, {\"id\": 2}]\n\n\nclass AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_return_single_element\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.1)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}\")\n        return {\"foo\": 42}\n\n\nclass AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_reqs_return\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        req_id = response.meta.get(\"req_id\", 0)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(f\"Got response {status}, req_id {req_id}\")\n        if req_id > 0:\n            return\n        reqs = []\n        for i in range(1, 3):\n            req = Request(self.start_urls[0], dont_filter=True, meta={\"req_id\": i})\n            reqs.append(req)\n        return reqs\n\n\nclass AsyncDefAsyncioGenExcSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen_exc\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {\"foo\": i}\n            if i > 5:\n                raise ValueError(\"Stopping the processing\")\n\n\nclass AsyncDefDeferredDirectSpider(SimpleSpider):\n    name = \"asyncdef_deferred_direct\"\n\n    async def parse(self, response):\n        resp = await get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n        yield {\"code\": resp.code}\n\n\nclass AsyncDefDeferredWrappedSpider(SimpleSpider):\n    name = \"asyncdef_deferred_wrapped\"\n\n    async def parse(self, response):\n        resp = await deferred_to_future(\n            get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n        )\n        yield {\"code\": resp.code}\n\n\nclass AsyncDefDeferredMaybeWrappedSpider(SimpleSpider):\n    name = \"asyncdef_deferred_wrapped\"\n\n    async def parse(self, response):\n        resp = await maybe_deferred_to_future(\n            get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n        )\n        yield {\"code\": resp.code}\n\n\nclass AsyncDefAsyncioGenSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen\"\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        yield {\"foo\": 42}\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen_loop\"\n\n    async def parse(self, response):\n        for i in range(10):\n            await asyncio.sleep(0.1)\n            yield {\"foo\": i}\n        self.logger.info(f\"Got response {response.status}\")\n\n\nclass AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n    name = \"asyncdef_asyncio_gen_complex\"\n    initial_reqs = 4\n    following_reqs = 3\n    depth = 2\n\n    def _get_req(self, index, cb=None):\n        return Request(\n            self.mockserver.url(f\"/status?n=200&request={index}\"),\n            meta={\"index\": index},\n            dont_filter=True,\n            callback=cb,\n        )\n\n    def start_requests(self):\n        for i in range(1, self.initial_reqs + 1):\n            yield self._get_req(i)\n\n    async def parse(self, response):\n        index = response.meta[\"index\"]\n        yield {\"index\": index}\n        if index < 10**self.depth:\n            for new_index in range(10 * index, 10 * index + self.following_reqs):\n                yield self._get_req(new_index)\n        yield self._get_req(index, cb=self.parse2)\n        await asyncio.sleep(0.1)\n        yield {\"index\": index + 5}\n\n    async def parse2(self, response):\n        await asyncio.sleep(0.1)\n        yield {\"index2\": response.meta[\"index\"]}\n\n\nclass ItemSpider(FollowAllSpider):\n    name = \"item\"\n\n    def parse(self, response):\n        for request in super().parse(response):\n            yield request\n            yield Item()\n            yield {}\n\n\nclass DefaultError(Exception):\n    pass\n\n\nclass ErrorSpider(FollowAllSpider):\n    name = \"error\"\n    exception_cls = DefaultError\n\n    def raise_exception(self):\n        raise self.exception_cls(\"Expected exception\")\n\n    def parse(self, response):\n        for request in super().parse(response):\n            yield request\n            self.raise_exception()\n\n\nclass BrokenStartRequestsSpider(FollowAllSpider):\n    fail_before_yield = False\n    fail_yielding = False\n\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        self.seedsseen = []\n\n    def start_requests(self):\n        if self.fail_before_yield:\n            1 / 0\n\n        for s in range(100):\n            qargs = {\"total\": 10, \"seed\": s}\n            url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=True)}\")\n            yield Request(url, meta={\"seed\": s})\n            if self.fail_yielding:\n                2 / 0\n\n        assert (\n            self.seedsseen\n        ), \"All start requests consumed before any download happened\"\n\n    def parse(self, response):\n        self.seedsseen.append(response.meta.get(\"seed\"))\n        yield from super().parse(response)\n\n\nclass SingleRequestSpider(MetaSpider):\n    seed = None\n    callback_func = None\n    errback_func = None\n\n    def start_requests(self):\n        if isinstance(self.seed, Request):\n            yield self.seed.replace(callback=self.parse, errback=self.on_error)\n        else:\n            yield Request(self.seed, callback=self.parse, errback=self.on_error)\n\n    def parse(self, response):\n        self.meta.setdefault(\"responses\", []).append(response)\n        if callable(self.callback_func):\n            return self.callback_func(response)\n        if \"next\" in response.meta:\n            return response.meta[\"next\"]\n\n    def on_error(self, failure):\n        self.meta[\"failure\"] = failure\n        if callable(self.errback_func):\n            return self.errback_func(failure)\n\n\nclass DuplicateStartRequestsSpider(MockServerSpider):\n    dont_filter = True\n    name = \"duplicatestartrequests\"\n    distinct_urls = 2\n    dupe_factor = 3\n\n    def start_requests(self):\n        for i in range(0, self.distinct_urls):\n            for j in range(0, self.dupe_factor):\n                url = self.mockserver.url(f\"/echo?headers=1&body=test{i}\")\n                yield Request(url, dont_filter=self.dont_filter)\n\n    def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.visited = 0\n\n    def parse(self, response):\n        self.visited += 1\n\n\nclass CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n    \"\"\"\n    A CrawlSpider which overrides the 'parse' method\n    \"\"\"\n\n    name = \"crawl_spider_with_parse_method\"\n    custom_settings: dict = {\n        \"RETRY_HTTP_CODES\": [],  # no need to retry\n    }\n    rules = (Rule(LinkExtractor(), callback=\"parse\", follow=True),)\n\n    def start_requests(self):\n        test_body = b\"\"\"\n        <html>\n            <head><title>Page title<title></head>\n            <body>\n                <p><a href=\"/status?n=200\">Item 200</a></p>  <!-- callback -->\n                <p><a href=\"/status?n=201\">Item 201</a></p>  <!-- callback -->\n            </body>\n        </html>\n        \"\"\"\n        url = self.mockserver.url(\"/alpayload\")\n        yield Request(url, method=\"POST\", body=test_body)\n\n    def parse(self, response, foo=None):\n        self.logger.info(\"[parse] status %i (foo: %s)\", response.status, foo)\n        yield Request(\n            self.mockserver.url(\"/status?n=202\"), self.parse, cb_kwargs={\"foo\": \"bar\"}\n        )\n\n\nclass CrawlSpiderWithAsyncCallback(CrawlSpiderWithParseMethod):\n    \"\"\"A CrawlSpider with an async def callback\"\"\"\n\n    name = \"crawl_spider_with_async_callback\"\n    rules = (Rule(LinkExtractor(), callback=\"parse_async\", follow=True),)\n\n    async def parse_async(self, response, foo=None):\n        self.logger.info(\"[parse_async] status %i (foo: %s)\", response.status, foo)\n        return Request(\n            self.mockserver.url(\"/status?n=202\"),\n            self.parse_async,\n            cb_kwargs={\"foo\": \"bar\"},\n        )\n\n\nclass CrawlSpiderWithAsyncGeneratorCallback(CrawlSpiderWithParseMethod):\n    \"\"\"A CrawlSpider with an async generator callback\"\"\"\n\n    name = \"crawl_spider_with_async_generator_callback\"\n    rules = (Rule(LinkExtractor(), callback=\"parse_async_gen\", follow=True),)\n\n    async def parse_async_gen(self, response, foo=None):\n        self.logger.info(\"[parse_async_gen] status %i (foo: %s)\", response.status, foo)\n        yield Request(\n            self.mockserver.url(\"/status?n=202\"),\n            self.parse_async_gen,\n            cb_kwargs={\"foo\": \"bar\"},\n        )\n\n\nclass CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n    name = \"crawl_spider_with_errback\"\n    rules = (Rule(LinkExtractor(), callback=\"parse\", errback=\"errback\", follow=True),)\n\n    def start_requests(self):\n        test_body = b\"\"\"\n        <html>\n            <head><title>Page title<title></head>\n            <body>\n                <p><a href=\"/status?n=200\">Item 200</a></p>  <!-- callback -->\n                <p><a href=\"/status?n=201\">Item 201</a></p>  <!-- callback -->\n                <p><a href=\"/status?n=404\">Item 404</a></p>  <!-- errback -->\n                <p><a href=\"/status?n=500\">Item 500</a></p>  <!-- errback -->\n                <p><a href=\"/status?n=501\">Item 501</a></p>  <!-- errback -->\n            </body>\n        </html>\n        \"\"\"\n        url = self.mockserver.url(\"/alpayload\")\n        yield Request(url, method=\"POST\", body=test_body)\n\n    def errback(self, failure):\n        self.logger.info(\"[errback] status %i\", failure.value.response.status)\n\n\nclass CrawlSpiderWithProcessRequestCallbackKeywordArguments(CrawlSpiderWithParseMethod):\n    name = \"crawl_spider_with_process_request_cb_kwargs\"\n    rules = (\n        Rule(\n            LinkExtractor(),\n            callback=\"parse\",\n            follow=True,\n            process_request=\"process_request\",\n        ),\n    )\n\n    def process_request(self, request, response):\n        request.cb_kwargs[\"foo\"] = \"process_request\"\n        return request\n\n\nclass BytesReceivedCallbackSpider(MetaSpider):\n    full_response_length = 2**18\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.bytes_received, signals.bytes_received)\n        return spider\n\n    def start_requests(self):\n        body = b\"a\" * self.full_response_length\n        url = self.mockserver.url(\"/alpayload\")\n        yield Request(url, method=\"POST\", body=body, errback=self.errback)\n\n    def parse(self, response):\n        self.meta[\"response\"] = response\n\n    def errback(self, failure):\n        self.meta[\"failure\"] = failure\n\n    def bytes_received(self, data, request, spider):\n        self.meta[\"bytes_received\"] = data\n        raise StopDownload(fail=False)\n\n\nclass BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):\n    def bytes_received(self, data, request, spider):\n        self.meta[\"bytes_received\"] = data\n        raise StopDownload(fail=True)\n\n\nclass HeadersReceivedCallbackSpider(MetaSpider):\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.headers_received, signals.headers_received)\n        return spider\n\n    def start_requests(self):\n        yield Request(self.mockserver.url(\"/status\"), errback=self.errback)\n\n    def parse(self, response):\n        self.meta[\"response\"] = response\n\n    def errback(self, failure):\n        self.meta[\"failure\"] = failure\n\n    def headers_received(self, headers, body_length, request, spider):\n        self.meta[\"headers_received\"] = headers\n        raise StopDownload(fail=False)\n\n\nclass HeadersReceivedErrbackSpider(HeadersReceivedCallbackSpider):\n    def headers_received(self, headers, body_length, request, spider):\n        self.meta[\"headers_received\"] = headers\n        raise StopDownload(fail=True)\n", "tests/test_selector.py": "import weakref\n\nimport parsel\nimport pytest\nfrom packaging import version\nfrom twisted.trial import unittest\n\nfrom scrapy.http import HtmlResponse, TextResponse, XmlResponse\nfrom scrapy.selector import Selector\n\nPARSEL_VERSION = version.parse(getattr(parsel, \"__version__\", \"0.0\"))\nPARSEL_18_PLUS = PARSEL_VERSION >= version.parse(\"1.8.0\")\n\n\nclass SelectorTestCase(unittest.TestCase):\n    def test_simple_selection(self):\n        \"\"\"Simple selector tests\"\"\"\n        body = b\"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n        response = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        sel = Selector(response)\n\n        xl = sel.xpath(\"//input\")\n        self.assertEqual(2, len(xl))\n        for x in xl:\n            assert isinstance(x, Selector)\n\n        self.assertEqual(\n            sel.xpath(\"//input\").getall(), [x.get() for x in sel.xpath(\"//input\")]\n        )\n        self.assertEqual(\n            [x.get() for x in sel.xpath(\"//input[@name='a']/@name\")], [\"a\"]\n        )\n        self.assertEqual(\n            [\n                x.get()\n                for x in sel.xpath(\n                    \"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\"\n                )\n            ],\n            [\"12.0\"],\n        )\n        self.assertEqual(sel.xpath(\"concat('xpath', 'rules')\").getall(), [\"xpathrules\"])\n        self.assertEqual(\n            [\n                x.get()\n                for x in sel.xpath(\n                    \"concat(//input[@name='a']/@value, //input[@name='b']/@value)\"\n                )\n            ],\n            [\"12\"],\n        )\n\n    def test_root_base_url(self):\n        body = b'<html><form action=\"/path\"><input name=\"a\" /></form></html>'\n        url = \"http://example.com\"\n        response = TextResponse(url=url, body=body, encoding=\"utf-8\")\n        sel = Selector(response)\n        self.assertEqual(url, sel.root.base)\n\n    def test_flavor_detection(self):\n        text = b'<div><img src=\"a.jpg\"><p>Hello</div>'\n        sel = Selector(XmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n        self.assertEqual(sel.type, \"xml\")\n        self.assertEqual(\n            sel.xpath(\"//div\").getall(),\n            ['<div><img src=\"a.jpg\"><p>Hello</p></img></div>'],\n        )\n\n        sel = Selector(HtmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n        self.assertEqual(sel.type, \"html\")\n        self.assertEqual(\n            sel.xpath(\"//div\").getall(), ['<div><img src=\"a.jpg\"><p>Hello</p></div>']\n        )\n\n    def test_http_header_encoding_precedence(self):\n        # '\\xa3'     = pound symbol in unicode\n        # '\\xc2\\xa3' = pound symbol in utf-8\n        # '\\xa3'     = pound symbol in latin-1 (iso-8859-1)\n\n        meta = (\n            '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">'\n        )\n        head = f\"<head>{meta}</head>\"\n        body_content = '<span id=\"blank\">\\xa3</span>'\n        body = f\"<body>{body_content}</body>\"\n        html = f\"<html>{head}{body}</html>\"\n        encoding = \"utf-8\"\n        html_utf8 = html.encode(encoding)\n\n        headers = {\"Content-Type\": [\"text/html; charset=utf-8\"]}\n        response = HtmlResponse(\n            url=\"http://example.com\", headers=headers, body=html_utf8\n        )\n        x = Selector(response)\n        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").getall(), [\"\\xa3\"])\n\n    def test_badly_encoded_body(self):\n        # \\xe9 alone isn't valid utf8 sequence\n        r1 = TextResponse(\n            \"http://www.example.com\",\n            body=b\"<html><p>an Jos\\xe9 de</p><html>\",\n            encoding=\"utf-8\",\n        )\n        Selector(r1).xpath(\"//text()\").getall()\n\n    def test_weakref_slots(self):\n        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n        x = Selector(text=\"\")\n        weakref.ref(x)\n        assert not hasattr(\n            x, \"__dict__\"\n        ), f\"{x.__class__.__name__} does not use __slots__\"\n\n    def test_selector_bad_args(self):\n        with self.assertRaisesRegex(ValueError, \"received both response and text\"):\n            Selector(TextResponse(url=\"http://example.com\", body=b\"\"), text=\"\")\n\n\nclass JMESPathTestCase(unittest.TestCase):\n    @pytest.mark.skipif(\n        not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n    )\n    def test_json_has_html(self) -> None:\n        \"\"\"Sometimes the information is returned in a json wrapper\"\"\"\n\n        body = \"\"\"\n        {\n            \"content\": [\n                {\n                    \"name\": \"A\",\n                    \"value\": \"a\"\n                },\n                {\n                    \"name\": {\n                        \"age\": 18\n                    },\n                    \"value\": \"b\"\n                },\n                {\n                    \"name\": \"C\",\n                    \"value\": \"c\"\n                },\n                {\n                    \"name\": \"<a>D</a>\",\n                    \"value\": \"<div>d</div>\"\n                }\n            ],\n            \"html\": \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\"\n        }\n        \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        self.assertEqual(\n            resp.jmespath(\"html\").get(),\n            \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\",\n        )\n        self.assertEqual(\n            resp.jmespath(\"html\").xpath(\"//div/a/text()\").getall(),\n            [\"a\", \"b\", \"d\"],\n        )\n        self.assertEqual(resp.jmespath(\"html\").css(\"div > b\").getall(), [\"<b>f</b>\"])\n        self.assertEqual(resp.jmespath(\"content\").jmespath(\"name.age\").get(), \"18\")\n\n    @pytest.mark.skipif(\n        not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n    )\n    def test_html_has_json(self) -> None:\n        body = \"\"\"\n        <div>\n            <h1>Information</h1>\n            <content>\n            {\n              \"user\": [\n                        {\n                                  \"name\": \"A\",\n                                  \"age\": 18\n                        },\n                        {\n                                  \"name\": \"B\",\n                                  \"age\": 32\n                        },\n                        {\n                                  \"name\": \"C\",\n                                  \"age\": 22\n                        },\n                        {\n                                  \"name\": \"D\",\n                                  \"age\": 25\n                        }\n              ],\n              \"total\": 4,\n              \"status\": \"ok\"\n            }\n            </content>\n        </div>\n        \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        self.assertEqual(\n            resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").getall(),\n            [\"A\", \"B\", \"C\", \"D\"],\n        )\n        self.assertEqual(\n            resp.xpath(\"//div/content\").jmespath(\"user[*].name\").getall(),\n            [\"A\", \"B\", \"C\", \"D\"],\n        )\n        self.assertEqual(resp.xpath(\"//div/content\").jmespath(\"total\").get(), \"4\")\n\n    @pytest.mark.skipif(\n        not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n    )\n    def test_jmestpath_with_re(self) -> None:\n        body = \"\"\"\n            <div>\n                <h1>Information</h1>\n                <content>\n                {\n                  \"user\": [\n                            {\n                                      \"name\": \"A\",\n                                      \"age\": 18\n                            },\n                            {\n                                      \"name\": \"B\",\n                                      \"age\": 32\n                            },\n                            {\n                                      \"name\": \"C\",\n                                      \"age\": 22\n                            },\n                            {\n                                      \"name\": \"D\",\n                                      \"age\": 25\n                            }\n                  ],\n                  \"total\": 4,\n                  \"status\": \"ok\"\n                }\n                </content>\n            </div>\n            \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        self.assertEqual(\n            resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").re(r\"(\\w+)\"),\n            [\"A\", \"B\", \"C\", \"D\"],\n        )\n        self.assertEqual(\n            resp.xpath(\"//div/content\").jmespath(\"user[*].name\").re(r\"(\\w+)\"),\n            [\"A\", \"B\", \"C\", \"D\"],\n        )\n\n        self.assertEqual(\n            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re(r\"(\\d+)\"), []\n        )\n\n        self.assertEqual(\n            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re_first(r\"(\\d+)\"),\n            None,\n        )\n\n        self.assertEqual(\n            resp.xpath(\"//div/content\")\n            .jmespath(\"user[*].age.to_string(@)\")\n            .re(r\"(\\d+)\"),\n            [\"18\", \"32\", \"22\", \"25\"],\n        )\n\n    @pytest.mark.skipif(PARSEL_18_PLUS, reason=\"parsel >= 1.8 supports jmespath\")\n    def test_jmespath_not_available(my_json_page) -> None:\n        body = \"\"\"\n        {\n            \"website\": {\"name\": \"Example\"}\n        }\n        \"\"\"\n        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n        with pytest.raises(AttributeError):\n            resp.jmespath(\"website.name\").get()\n", "tests/test_crawler.py": "import logging\nimport os\nimport platform\nimport signal\nimport subprocess\nimport sys\nimport warnings\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\nfrom packaging.version import parse as parse_version\nfrom pexpect.popen_spawn import PopenSpawn\nfrom pytest import mark, raises\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.trial import unittest\nfrom w3lib import __version__ as w3lib_version\nfrom zope.interface.exceptions import MultipleInvalid\n\nimport scrapy\nfrom scrapy import Spider\nfrom scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.extensions import telnet\nfrom scrapy.extensions.throttle import AutoThrottle\nfrom scrapy.settings import Settings, default_settings\nfrom scrapy.spiderloader import SpiderLoader\nfrom scrapy.utils.log import configure_logging, get_scrapy_root_handler\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.test import get_crawler\nfrom tests.mockserver import MockServer, get_mockserver_env\n\n# To prevent warnings.\nBASE_SETTINGS = {\n    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n}\n\n\ndef get_raw_crawler(spidercls=None, settings_dict=None):\n    \"\"\"get_crawler alternative that only calls the __init__ method of the\n    crawler.\"\"\"\n    settings = Settings()\n    settings.setdict(settings_dict or {})\n    return Crawler(spidercls or DefaultSpider, settings)\n\n\nclass BaseCrawlerTest(unittest.TestCase):\n    def assertOptionIsDefault(self, settings, key):\n        self.assertIsInstance(settings, Settings)\n        self.assertEqual(settings[key], getattr(default_settings, key))\n\n\nclass CrawlerTestCase(BaseCrawlerTest):\n    def test_populate_spidercls_settings(self):\n        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n        project_settings = {**BASE_SETTINGS, \"TEST1\": \"project\", \"TEST3\": \"project\"}\n\n        class CustomSettingsSpider(DefaultSpider):\n            custom_settings = spider_settings\n\n        settings = Settings()\n        settings.setdict(project_settings, priority=\"project\")\n        crawler = Crawler(CustomSettingsSpider, settings)\n        crawler._apply_settings()\n\n        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n\n        self.assertFalse(settings.frozen)\n        self.assertTrue(crawler.settings.frozen)\n\n    def test_crawler_accepts_dict(self):\n        crawler = get_crawler(DefaultSpider, {\"foo\": \"bar\"})\n        self.assertEqual(crawler.settings[\"foo\"], \"bar\")\n        self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_accepts_None(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n            crawler = Crawler(DefaultSpider)\n        self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_rejects_spider_objects(self):\n        with raises(ValueError):\n            Crawler(DefaultSpider())\n\n    @inlineCallbacks\n    def test_crawler_crawl_twice_deprecated(self):\n        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n        yield crawler.crawl()\n        with pytest.warns(\n            ScrapyDeprecationWarning,\n            match=r\"Running Crawler.crawl\\(\\) more than once is deprecated\",\n        ):\n            yield crawler.crawl()\n\n    def test_get_addon(self):\n        class ParentAddon:\n            pass\n\n        class TrackingAddon(ParentAddon):\n            instances = []\n\n            def __init__(self):\n                TrackingAddon.instances.append(self)\n\n            def update_settings(self, settings):\n                pass\n\n        settings = {\n            **BASE_SETTINGS,\n            \"ADDONS\": {\n                TrackingAddon: 0,\n            },\n        }\n        crawler = get_crawler(settings_dict=settings)\n        self.assertEqual(len(TrackingAddon.instances), 1)\n        expected = TrackingAddon.instances[-1]\n\n        addon = crawler.get_addon(TrackingAddon)\n        self.assertEqual(addon, expected)\n\n        addon = crawler.get_addon(DefaultSpider)\n        self.assertIsNone(addon)\n\n        addon = crawler.get_addon(ParentAddon)\n        self.assertEqual(addon, expected)\n\n        class ChildAddon(TrackingAddon):\n            pass\n\n        addon = crawler.get_addon(ChildAddon)\n        self.assertIsNone(addon)\n\n    @inlineCallbacks\n    def test_get_downloader_middleware(self):\n        class ParentDownloaderMiddleware:\n            pass\n\n        class TrackingDownloaderMiddleware(ParentDownloaderMiddleware):\n            instances = []\n\n            def __init__(self):\n                TrackingDownloaderMiddleware.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler):\n                self.crawler = crawler\n\n            def start_requests(self):\n                MySpider.result = crawler.get_downloader_middleware(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"DOWNLOADER_MIDDLEWARES\": {\n                TrackingDownloaderMiddleware: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingDownloaderMiddleware\n        yield crawler.crawl()\n        self.assertEqual(len(TrackingDownloaderMiddleware.instances), 1)\n        self.assertEqual(MySpider.result, TrackingDownloaderMiddleware.instances[-1])\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentDownloaderMiddleware\n        yield crawler.crawl()\n        self.assertEqual(MySpider.result, TrackingDownloaderMiddleware.instances[-1])\n\n        class ChildDownloaderMiddleware(TrackingDownloaderMiddleware):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildDownloaderMiddleware\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n    def test_get_downloader_middleware_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        self.assertRaises(\n            RuntimeError, crawler.get_downloader_middleware, DefaultSpider\n        )\n\n    @inlineCallbacks\n    def test_get_downloader_middleware_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_downloader_middleware(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with raises(RuntimeError):\n            yield crawler.crawl()\n\n    @inlineCallbacks\n    def test_get_extension(self):\n        class ParentExtension:\n            pass\n\n        class TrackingExtension(ParentExtension):\n            instances = []\n\n            def __init__(self):\n                TrackingExtension.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler):\n                self.crawler = crawler\n\n            def start_requests(self):\n                MySpider.result = crawler.get_extension(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"EXTENSIONS\": {\n                TrackingExtension: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingExtension\n        yield crawler.crawl()\n        self.assertEqual(len(TrackingExtension.instances), 1)\n        self.assertEqual(MySpider.result, TrackingExtension.instances[-1])\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentExtension\n        yield crawler.crawl()\n        self.assertEqual(MySpider.result, TrackingExtension.instances[-1])\n\n        class ChildExtension(TrackingExtension):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildExtension\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n    def test_get_extension_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        self.assertRaises(RuntimeError, crawler.get_extension, DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_extension_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_extension(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with raises(RuntimeError):\n            yield crawler.crawl()\n\n    @inlineCallbacks\n    def test_get_item_pipeline(self):\n        class ParentItemPipeline:\n            pass\n\n        class TrackingItemPipeline(ParentItemPipeline):\n            instances = []\n\n            def __init__(self):\n                TrackingItemPipeline.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler):\n                self.crawler = crawler\n\n            def start_requests(self):\n                MySpider.result = crawler.get_item_pipeline(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"ITEM_PIPELINES\": {\n                TrackingItemPipeline: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingItemPipeline\n        yield crawler.crawl()\n        self.assertEqual(len(TrackingItemPipeline.instances), 1)\n        self.assertEqual(MySpider.result, TrackingItemPipeline.instances[-1])\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentItemPipeline\n        yield crawler.crawl()\n        self.assertEqual(MySpider.result, TrackingItemPipeline.instances[-1])\n\n        class ChildItemPipeline(TrackingItemPipeline):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildItemPipeline\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n    def test_get_item_pipeline_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        self.assertRaises(RuntimeError, crawler.get_item_pipeline, DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_item_pipeline_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_item_pipeline(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with raises(RuntimeError):\n            yield crawler.crawl()\n\n    @inlineCallbacks\n    def test_get_spider_middleware(self):\n        class ParentSpiderMiddleware:\n            pass\n\n        class TrackingSpiderMiddleware(ParentSpiderMiddleware):\n            instances = []\n\n            def __init__(self):\n                TrackingSpiderMiddleware.instances.append(self)\n\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                return cls(crawler=crawler)\n\n            def __init__(self, crawler):\n                self.crawler = crawler\n\n            def start_requests(self):\n                MySpider.result = crawler.get_spider_middleware(MySpider.cls)\n                return\n                yield\n\n        settings = {\n            **BASE_SETTINGS,\n            \"SPIDER_MIDDLEWARES\": {\n                TrackingSpiderMiddleware: 0,\n            },\n        }\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = TrackingSpiderMiddleware\n        yield crawler.crawl()\n        self.assertEqual(len(TrackingSpiderMiddleware.instances), 1)\n        self.assertEqual(MySpider.result, TrackingSpiderMiddleware.instances[-1])\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = DefaultSpider\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ParentSpiderMiddleware\n        yield crawler.crawl()\n        self.assertEqual(MySpider.result, TrackingSpiderMiddleware.instances[-1])\n\n        class ChildSpiderMiddleware(TrackingSpiderMiddleware):\n            pass\n\n        crawler = get_raw_crawler(MySpider, settings)\n        MySpider.cls = ChildSpiderMiddleware\n        yield crawler.crawl()\n        self.assertIsNone(MySpider.result)\n\n    def test_get_spider_middleware_not_crawling(self):\n        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n        self.assertRaises(RuntimeError, crawler.get_spider_middleware, DefaultSpider)\n\n    @inlineCallbacks\n    def test_get_spider_middleware_no_engine(self):\n        class MySpider(Spider):\n            name = \"myspider\"\n\n            @classmethod\n            def from_crawler(cls, crawler):\n                try:\n                    crawler.get_spider_middleware(DefaultSpider)\n                except Exception as e:\n                    MySpider.result = e\n                    raise\n\n        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n        with raises(RuntimeError):\n            yield crawler.crawl()\n\n\nclass SpiderSettingsTestCase(unittest.TestCase):\n    def test_spider_custom_settings(self):\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n            custom_settings = {\"AUTOTHROTTLE_ENABLED\": True}\n\n        crawler = get_crawler(MySpider)\n        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n        self.assertIn(AutoThrottle, enabled_exts)\n\n\nclass CrawlerLoggingTestCase(unittest.TestCase):\n    def test_no_root_handler_installed(self):\n        handler = get_scrapy_root_handler()\n        if handler is not None:\n            logging.root.removeHandler(handler)\n\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n\n        get_crawler(MySpider)\n        assert get_scrapy_root_handler() is None\n\n    def test_spider_custom_settings_log_level(self):\n        log_file = Path(self.mktemp())\n        log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n            custom_settings = {\n                \"LOG_LEVEL\": \"INFO\",\n                \"LOG_FILE\": str(log_file),\n                # settings to avoid extra warnings\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n            }\n\n        configure_logging()\n        self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n        crawler = get_crawler(MySpider)\n        self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n        info_count = crawler.stats.get_value(\"log_count/INFO\")\n        logging.debug(\"debug message\")\n        logging.info(\"info message\")\n        logging.warning(\"warning message\")\n        logging.error(\"error message\")\n\n        logged = log_file.read_text(encoding=\"utf-8\")\n\n        self.assertIn(\"previous message\", logged)\n        self.assertNotIn(\"debug message\", logged)\n        self.assertIn(\"info message\", logged)\n        self.assertIn(\"warning message\", logged)\n        self.assertIn(\"error message\", logged)\n        self.assertEqual(crawler.stats.get_value(\"log_count/ERROR\"), 1)\n        self.assertEqual(crawler.stats.get_value(\"log_count/WARNING\"), 1)\n        self.assertEqual(crawler.stats.get_value(\"log_count/INFO\") - info_count, 1)\n        self.assertEqual(crawler.stats.get_value(\"log_count/DEBUG\", 0), 0)\n\n    def test_spider_custom_settings_log_append(self):\n        log_file = Path(self.mktemp())\n        log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n\n        class MySpider(scrapy.Spider):\n            name = \"spider\"\n            custom_settings = {\n                \"LOG_FILE\": str(log_file),\n                \"LOG_FILE_APPEND\": False,\n                # disable telnet if not available to avoid an extra warning\n                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n            }\n\n        configure_logging()\n        get_crawler(MySpider)\n        logging.debug(\"debug message\")\n\n        logged = log_file.read_text(encoding=\"utf-8\")\n\n        self.assertNotIn(\"previous message\", logged)\n        self.assertIn(\"debug message\", logged)\n\n\nclass SpiderLoaderWithWrongInterface:\n    def unneeded_method(self):\n        pass\n\n\nclass CustomSpiderLoader(SpiderLoader):\n    pass\n\n\nclass CrawlerRunnerTestCase(BaseCrawlerTest):\n    def test_spider_manager_verify_interface(self):\n        settings = Settings(\n            {\n                \"SPIDER_LOADER_CLASS\": SpiderLoaderWithWrongInterface,\n            }\n        )\n        self.assertRaises(MultipleInvalid, CrawlerRunner, settings)\n\n    def test_crawler_runner_accepts_dict(self):\n        runner = CrawlerRunner({\"foo\": \"bar\"})\n        self.assertEqual(runner.settings[\"foo\"], \"bar\")\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_runner_accepts_None(self):\n        runner = CrawlerRunner()\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n\nclass CrawlerProcessTest(BaseCrawlerTest):\n    def test_crawler_process_accepts_dict(self):\n        runner = CrawlerProcess({\"foo\": \"bar\"})\n        self.assertEqual(runner.settings[\"foo\"], \"bar\")\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n    def test_crawler_process_accepts_None(self):\n        runner = CrawlerProcess()\n        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n\n\nclass ExceptionSpider(scrapy.Spider):\n    name = \"exception\"\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        raise ValueError(\"Exception in from_crawler method\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\n@mark.usefixtures(\"reactor_pytest\")\nclass CrawlerRunnerHasSpider(unittest.TestCase):\n    def _runner(self):\n        return CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_successful(self):\n        runner = self._runner()\n        yield runner.crawl(NoRequestsSpider)\n        self.assertFalse(runner.bootstrap_failed)\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_successful_for_several(self):\n        runner = self._runner()\n        yield runner.crawl(NoRequestsSpider)\n        yield runner.crawl(NoRequestsSpider)\n        self.assertFalse(runner.bootstrap_failed)\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_failed(self):\n        runner = self._runner()\n\n        try:\n            yield runner.crawl(ExceptionSpider)\n        except ValueError:\n            pass\n        else:\n            self.fail(\"Exception should be raised from spider\")\n\n        self.assertTrue(runner.bootstrap_failed)\n\n    @inlineCallbacks\n    def test_crawler_runner_bootstrap_failed_for_several(self):\n        runner = self._runner()\n\n        try:\n            yield runner.crawl(ExceptionSpider)\n        except ValueError:\n            pass\n        else:\n            self.fail(\"Exception should be raised from spider\")\n\n        yield runner.crawl(NoRequestsSpider)\n\n        self.assertTrue(runner.bootstrap_failed)\n\n    @inlineCallbacks\n    def test_crawler_runner_asyncio_enabled_true(self):\n        if self.reactor_pytest == \"asyncio\":\n            CrawlerRunner(\n                settings={\n                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n                }\n            )\n        else:\n            msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n            with self.assertRaisesRegex(Exception, msg):\n                runner = CrawlerRunner(\n                    settings={\n                        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                        \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n                    }\n                )\n                yield runner.crawl(NoRequestsSpider)\n\n\nclass ScriptRunnerMixin:\n    script_dir: Path\n    cwd = os.getcwd()\n\n    def get_script_args(self, script_name: str, *script_args: str) -> List[str]:\n        script_path = self.script_dir / script_name\n        return [sys.executable, str(script_path)] + list(script_args)\n\n    def run_script(self, script_name: str, *script_args: str) -> str:\n        args = self.get_script_args(script_name, *script_args)\n        p = subprocess.Popen(\n            args,\n            env=get_mockserver_env(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = p.communicate()\n        return stderr.decode(\"utf-8\")\n\n\nclass CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n    script_dir = Path(__file__).parent.resolve() / \"CrawlerProcess\"\n\n    def test_simple(self):\n        log = self.run_script(\"simple.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertNotIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_multi(self):\n        log = self.run_script(\"multi.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertNotIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n\n    def test_reactor_default(self):\n        log = self.run_script(\"reactor_default.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertNotIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n\n    def test_reactor_default_twisted_reactor_select(self):\n        log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n        if platform.system() in [\"Windows\", \"Darwin\"]:\n            # The goal of this test function is to test that, when a reactor is\n            # installed (the default one here) and a different reactor is\n            # configured (select here), an error raises.\n            #\n            # In Windows the default reactor is the select reactor, so that\n            # error does not raise.\n            #\n            # If that ever becomes the case on more platforms (i.e. if Linux\n            # also starts using the select reactor by default in a future\n            # version of Twisted), then we will need to rethink this test.\n            self.assertIn(\"Spider closed (finished)\", log)\n        else:\n            self.assertNotIn(\"Spider closed (finished)\", log)\n            self.assertIn(\n                (\n                    \"does not match the requested one \"\n                    \"(twisted.internet.selectreactor.SelectReactor)\"\n                ),\n                log,\n            )\n\n    def test_reactor_select(self):\n        log = self.run_script(\"reactor_select.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n\n    def test_reactor_select_twisted_reactor_select(self):\n        log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n\n    def test_reactor_select_subclass_twisted_reactor_select(self):\n        log = self.run_script(\"reactor_select_subclass_twisted_reactor_select.py\")\n        self.assertNotIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            (\n                \"does not match the requested one \"\n                \"(twisted.internet.selectreactor.SelectReactor)\"\n            ),\n            log,\n        )\n\n    def test_asyncio_enabled_no_reactor(self):\n        log = self.run_script(\"asyncio_enabled_no_reactor.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_asyncio_enabled_reactor(self):\n        log = self.run_script(\"asyncio_enabled_reactor.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    @mark.skipif(\n        parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n        reason=\"w3lib 2.0.0 and later do not allow invalid domains.\",\n    )\n    def test_ipv6_default_name_resolver(self):\n        log = self.run_script(\"default_name_resolver.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\",\n            log,\n        )\n        self.assertIn(\n            \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\",\n            log,\n        )\n\n    def test_caching_hostname_resolver_ipv6(self):\n        log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n\n    def test_caching_hostname_resolver_finite_execution(self):\n        with MockServer() as mock_server:\n            http_address = mock_server.http_address.replace(\"0.0.0.0\", \"127.0.0.1\")\n            log = self.run_script(\"caching_hostname_resolver.py\", http_address)\n            self.assertIn(\"Spider closed (finished)\", log)\n            self.assertNotIn(\"ERROR: Error downloading\", log)\n            self.assertNotIn(\"TimeoutError\", log)\n            self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n\n    def test_twisted_reactor_select(self):\n        log = self.run_script(\"twisted_reactor_select.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n        )\n\n    @mark.skipif(\n        platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n    )\n    def test_twisted_reactor_poll(self):\n        log = self.run_script(\"twisted_reactor_poll.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n\n    def test_twisted_reactor_asyncio(self):\n        log = self.run_script(\"twisted_reactor_asyncio.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_twisted_reactor_asyncio_custom_settings(self):\n        log = self.run_script(\"twisted_reactor_custom_settings.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_twisted_reactor_asyncio_custom_settings_same(self):\n        log = self.run_script(\"twisted_reactor_custom_settings_same.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n\n    def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n        log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n        self.assertIn(\n            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n        )\n        self.assertIn(\n            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\",\n            log,\n        )\n\n    @mark.requires_uvloop\n    def test_custom_loop_asyncio(self):\n        log = self.run_script(\"asyncio_custom_loop.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n\n    @mark.requires_uvloop\n    def test_custom_loop_asyncio_deferred_signal(self):\n        log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n        self.assertIn(\"async pipeline opened!\", log)\n\n    @mark.requires_uvloop\n    def test_asyncio_enabled_reactor_same_loop(self):\n        log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n\n    @mark.requires_uvloop\n    def test_asyncio_enabled_reactor_different_loop(self):\n        log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n        self.assertNotIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            (\n                \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n                \"setting (uvloop.Loop)\"\n            ),\n            log,\n        )\n\n    def test_default_loop_asyncio_deferred_signal(self):\n        log = self.run_script(\"asyncio_deferred_signal.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\n            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n        )\n        self.assertNotIn(\"Using asyncio event loop: uvloop.Loop\", log)\n        self.assertIn(\"async pipeline opened!\", log)\n\n    def test_args_change_settings(self):\n        log = self.run_script(\"args_settings.py\")\n        self.assertIn(\"Spider closed (finished)\", log)\n        self.assertIn(\"The value of FOO is 42\", log)\n\n    def test_shutdown_graceful(self):\n        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=3\")\n        p = PopenSpawn(args, timeout=5)\n        p.expect_exact(\"Spider opened\")\n        p.expect_exact(\"Crawled (200)\")\n        p.kill(sig)\n        p.expect_exact(\"shutting down gracefully\")\n        p.expect_exact(\"Spider closed (shutdown)\")\n        p.wait()\n\n    @inlineCallbacks\n    def test_shutdown_forced(self):\n        from twisted.internet import reactor\n\n        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=10\")\n        p = PopenSpawn(args, timeout=5)\n        p.expect_exact(\"Spider opened\")\n        p.expect_exact(\"Crawled (200)\")\n        p.kill(sig)\n        p.expect_exact(\"shutting down gracefully\")\n        # sending the second signal too fast often causes problems\n        d = Deferred()\n        reactor.callLater(0.1, d.callback, None)\n        yield d\n        p.kill(sig)\n        p.expect_exact(\"forcing unclean shutdown\")\n        p.wait()\n\n\nclass CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n    script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n\n    def test_response_ip_address(self):\n        log = self.run_script(\"ip_address.py\")\n        self.assertIn(\"INFO: Spider closed (finished)\", log)\n        self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n        self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n        self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n\n    def test_change_default_reactor(self):\n        log = self.run_script(\"change_reactor.py\")\n        self.assertIn(\n            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            log,\n        )\n        self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n", "tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py": "from twisted.internet.main import installReactor\nfrom twisted.internet.selectreactor import SelectReactor\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass SelectReactorSubclass(SelectReactor):\n    pass\n\n\nreactor = SelectReactorSubclass()\ninstallReactor(reactor)\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\n\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "tests/CrawlerProcess/caching_hostname_resolver_ipv6.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass CachingHostnameResolverSpider(scrapy.Spider):\n    \"\"\"\n    Finishes without a twisted.internet.error.DNSLookupError exception\n    \"\"\"\n\n    name = \"caching_hostname_resolver_spider\"\n    start_urls = [\"http://[::1]\"]\n\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess(\n        settings={\n            \"RETRY_ENABLED\": False,\n            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n        }\n    )\n    process.crawl(CachingHostnameResolverSpider)\n    process.start()\n", "tests/CrawlerProcess/multi.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/asyncio_custom_loop.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/twisted_reactor_poll.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass PollReactorSpider(scrapy.Spider):\n    name = \"poll_reactor\"\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.pollreactor.PollReactor\",\n    }\n)\nprocess.crawl(PollReactorSpider)\nprocess.start()\n", "tests/CrawlerProcess/twisted_reactor_custom_settings_same.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass AsyncioReactorSpider1(scrapy.Spider):\n    name = \"asyncio_reactor1\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nclass AsyncioReactorSpider2(scrapy.Spider):\n    name = \"asyncio_reactor2\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = CrawlerProcess()\nprocess.crawl(AsyncioReactorSpider1)\nprocess.crawl(AsyncioReactorSpider2)\nprocess.start()\n", "tests/CrawlerProcess/twisted_reactor_custom_settings.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = CrawlerProcess()\nprocess.crawl(AsyncioReactorSpider)\nprocess.start()\n", "tests/CrawlerProcess/simple.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/args_settings.py": "from typing import Any\n\nimport scrapy\nfrom scrapy.crawler import Crawler, CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n        return spider\n\n    def start_requests(self):\n        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n        return []\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider, foo=42)\nprocess.start()\n", "tests/CrawlerProcess/default_name_resolver.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass IPv6Spider(scrapy.Spider):\n    \"\"\"\n    Raises a twisted.internet.error.DNSLookupError:\n    the default name resolver does not handle IPv6 addresses.\n    \"\"\"\n\n    name = \"ipv6_spider\"\n    start_urls = [\"http://[::1]\"]\n\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess(settings={\"RETRY_ENABLED\": False})\n    process.crawl(IPv6Spider)\n    process.start()\n", "tests/CrawlerProcess/twisted_reactor_asyncio.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n)\nprocess.crawl(AsyncioReactorSpider)\nprocess.start()\n", "tests/CrawlerProcess/asyncio_enabled_reactor.py": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\n\nif sys.version_info >= (3, 8) and sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncioreactor.install(asyncio.get_event_loop())\n\nimport scrapy  # noqa: E402\nfrom scrapy.crawler import CrawlerProcess  # noqa: E402\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\nfrom twisted.python import log\n\nif sys.version_info >= (3, 8) and sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncioreactor.install(asyncio.get_event_loop())\n\nimport scrapy  # noqa: E402\nfrom scrapy.crawler import CrawlerProcess  # noqa: E402\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "tests/CrawlerProcess/asyncio_deferred_signal.py": "import asyncio\nimport sys\nfrom typing import Optional\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.defer import deferred_from_coro\n\n\nclass UppercasePipeline:\n    async def _open_spider(self, spider):\n        spider.logger.info(\"async pipeline opened!\")\n        await asyncio.sleep(0.1)\n\n    def open_spider(self, spider):\n        return deferred_from_coro(self._open_spider(spider))\n\n    def process_item(self, item, spider):\n        return {\"url\": item[\"url\"].upper()}\n\n\nclass UrlSpider(Spider):\n    name = \"url_spider\"\n    start_urls = [\"data:,\"]\n    custom_settings = {\n        \"ITEM_PIPELINES\": {UppercasePipeline: 100},\n    }\n\n    def parse(self, response):\n        yield {\"url\": response.url}\n\n\nif __name__ == \"__main__\":\n    ASYNCIO_EVENT_LOOP: Optional[str]\n    try:\n        ASYNCIO_EVENT_LOOP = sys.argv[1]\n    except IndexError:\n        ASYNCIO_EVENT_LOOP = None\n\n    process = CrawlerProcess(\n        settings={\n            \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n        }\n    )\n    process.crawl(UrlSpider)\n    process.start()\n", "tests/CrawlerProcess/reactor_default.py": "from twisted.internet import reactor  # noqa: F401\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py": "import asyncio\nimport sys\n\nfrom twisted.internet import asyncioreactor\nfrom uvloop import Loop\n\nif sys.version_info >= (3, 8) and sys.platform == \"win32\":\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.set_event_loop(Loop())\nasyncioreactor.install(asyncio.get_event_loop())\n\nimport scrapy  # noqa: E402\nfrom scrapy.crawler import CrawlerProcess  # noqa: E402\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/reactor_select.py": "from twisted.internet import selectreactor\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nselectreactor.install()\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/caching_hostname_resolver.py": "import sys\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass CachingHostnameResolverSpider(scrapy.Spider):\n    \"\"\"\n    Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)\n    \"\"\"\n\n    name = \"caching_hostname_resolver_spider\"\n\n    def start_requests(self):\n        yield scrapy.Request(self.url)\n\n    def parse(self, response):\n        for _ in range(10):\n            yield scrapy.Request(\n                response.url, dont_filter=True, callback=self.ignore_response\n            )\n\n    def ignore_response(self, response):\n        self.logger.info(repr(response.ip_address))\n\n\nif __name__ == \"__main__\":\n    process = CrawlerProcess(\n        settings={\n            \"RETRY_ENABLED\": False,\n            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n        }\n    )\n    process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])\n    process.start()\n", "tests/CrawlerProcess/reactor_default_twisted_reactor_select.py": "from twisted.internet import reactor  # noqa: F401\nfrom twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\n\nd = process.crawl(NoRequestsSpider)\nd.addErrback(log.err)\nprocess.start()\n", "tests/CrawlerProcess/asyncio_enabled_no_reactor.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/CrawlerProcess/sleeping.py": "from twisted.internet.defer import Deferred\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass SleepingSpider(scrapy.Spider):\n    name = \"sleeping\"\n\n    start_urls = [\"data:,;\"]\n\n    async def parse(self, response):\n        from twisted.internet import reactor\n\n        d = Deferred()\n        reactor.callLater(int(self.sleep), d.callback, None)\n        await maybe_deferred_to_future(d)\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(SleepingSpider)\nprocess.start()\n", "tests/CrawlerProcess/twisted_reactor_select.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass SelectReactorSpider(scrapy.Spider):\n    name = \"epoll_reactor\"\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\nprocess.crawl(SelectReactorSpider)\nprocess.start()\n", "tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py": "from twisted.python import log\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass SelectReactorSpider(scrapy.Spider):\n    name = \"select_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n\n\nclass AsyncioReactorSpider(scrapy.Spider):\n    name = \"asyncio_reactor\"\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n\nprocess = CrawlerProcess()\nd1 = process.crawl(SelectReactorSpider)\nd1.addErrback(log.err)\nd2 = process.crawl(AsyncioReactorSpider)\nd2.addErrback(log.err)\nprocess.start()\n", "tests/CrawlerProcess/reactor_select_twisted_reactor_select.py": "from twisted.internet import selectreactor\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nselectreactor.install()\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n    }\n)\n\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n", "tests/test_utils_misc/test_return_with_argument_inside_generator.py": "import unittest\nimport warnings\nfrom functools import partial\nfrom unittest import mock\n\nfrom scrapy.utils.misc import (\n    is_generator_with_return_value,\n    warn_on_generator_with_return_value,\n)\n\n\ndef _indentation_error(*args, **kwargs):\n    raise IndentationError()\n\n\ndef top_level_return_something():\n    \"\"\"\n    docstring\n    \"\"\"\n    url = \"\"\"\nhttps://example.org\n\"\"\"\n    yield url\n    return 1\n\n\ndef top_level_return_none():\n    \"\"\"\n    docstring\n    \"\"\"\n    url = \"\"\"\nhttps://example.org\n\"\"\"\n    yield url\n    return\n\n\ndef generator_that_returns_stuff():\n    yield 1\n    yield 2\n    return 3\n\n\nclass UtilsMiscPy3TestCase(unittest.TestCase):\n    def test_generators_return_something(self):\n        def f1():\n            yield 1\n            return 2\n\n        def g1():\n            yield 1\n            return \"asdf\"\n\n        def h1():\n            yield 1\n\n            def helper():\n                return 0\n\n            yield helper()\n            return 2\n\n        def i1():\n            \"\"\"\n            docstring\n            \"\"\"\n            url = \"\"\"\nhttps://example.org\n        \"\"\"\n            yield url\n            return 1\n\n        assert is_generator_with_return_value(top_level_return_something)\n        assert is_generator_with_return_value(f1)\n        assert is_generator_with_return_value(g1)\n        assert is_generator_with_return_value(h1)\n        assert is_generator_with_return_value(i1)\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, top_level_return_something)\n            self.assertEqual(len(w), 1)\n            self.assertIn(\n                'The \"NoneType.top_level_return_something\" method is a generator',\n                str(w[0].message),\n            )\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, f1)\n            self.assertEqual(len(w), 1)\n            self.assertIn('The \"NoneType.f1\" method is a generator', str(w[0].message))\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, g1)\n            self.assertEqual(len(w), 1)\n            self.assertIn('The \"NoneType.g1\" method is a generator', str(w[0].message))\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, h1)\n            self.assertEqual(len(w), 1)\n            self.assertIn('The \"NoneType.h1\" method is a generator', str(w[0].message))\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, i1)\n            self.assertEqual(len(w), 1)\n            self.assertIn('The \"NoneType.i1\" method is a generator', str(w[0].message))\n\n    def test_generators_return_none(self):\n        def f2():\n            yield 1\n            return None\n\n        def g2():\n            yield 1\n            return\n\n        def h2():\n            yield 1\n\n        def i2():\n            yield 1\n            yield from generator_that_returns_stuff()\n\n        def j2():\n            yield 1\n\n            def helper():\n                return 0\n\n            yield helper()\n\n        def k2():\n            \"\"\"\n            docstring\n            \"\"\"\n            url = \"\"\"\nhttps://example.org\n        \"\"\"\n            yield url\n            return\n\n        def l2():\n            return\n\n        assert not is_generator_with_return_value(top_level_return_none)\n        assert not is_generator_with_return_value(f2)\n        assert not is_generator_with_return_value(g2)\n        assert not is_generator_with_return_value(h2)\n        assert not is_generator_with_return_value(i2)\n        assert not is_generator_with_return_value(j2)  # not recursive\n        assert not is_generator_with_return_value(k2)  # not recursive\n        assert not is_generator_with_return_value(l2)\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, top_level_return_none)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, f2)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, g2)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, h2)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, i2)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, j2)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, k2)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, l2)\n            self.assertEqual(len(w), 0)\n\n    def test_generators_return_none_with_decorator(self):\n        def decorator(func):\n            def inner_func():\n                func()\n\n            return inner_func\n\n        @decorator\n        def f3():\n            yield 1\n            return None\n\n        @decorator\n        def g3():\n            yield 1\n            return\n\n        @decorator\n        def h3():\n            yield 1\n\n        @decorator\n        def i3():\n            yield 1\n            yield from generator_that_returns_stuff()\n\n        @decorator\n        def j3():\n            yield 1\n\n            def helper():\n                return 0\n\n            yield helper()\n\n        @decorator\n        def k3():\n            \"\"\"\n            docstring\n            \"\"\"\n            url = \"\"\"\nhttps://example.org\n        \"\"\"\n            yield url\n            return\n\n        @decorator\n        def l3():\n            return\n\n        assert not is_generator_with_return_value(top_level_return_none)\n        assert not is_generator_with_return_value(f3)\n        assert not is_generator_with_return_value(g3)\n        assert not is_generator_with_return_value(h3)\n        assert not is_generator_with_return_value(i3)\n        assert not is_generator_with_return_value(j3)  # not recursive\n        assert not is_generator_with_return_value(k3)  # not recursive\n        assert not is_generator_with_return_value(l3)\n\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, top_level_return_none)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, f3)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, g3)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, h3)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, i3)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, j3)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, k3)\n            self.assertEqual(len(w), 0)\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, l3)\n            self.assertEqual(len(w), 0)\n\n    @mock.patch(\n        \"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error\n    )\n    def test_indentation_error(self):\n        with warnings.catch_warnings(record=True) as w:\n            warn_on_generator_with_return_value(None, top_level_return_none)\n            self.assertEqual(len(w), 1)\n            self.assertIn(\"Unable to determine\", str(w[0].message))\n\n    def test_partial(self):\n        def cb(arg1, arg2):\n            yield {}\n\n        partial_cb = partial(cb, arg1=42)\n        assert not is_generator_with_return_value(partial_cb)\n", "tests/test_utils_misc/__init__.py": "import os\nimport sys\nimport unittest\nfrom pathlib import Path\nfrom unittest import mock\n\nfrom scrapy.item import Field, Item\nfrom scrapy.utils.misc import (\n    arg_to_iter,\n    build_from_crawler,\n    build_from_settings,\n    create_instance,\n    load_object,\n    rel_has_nofollow,\n    set_environ,\n    walk_modules,\n)\n\n__doctests__ = [\"scrapy.utils.misc\"]\n\n\nclass UtilsMiscTestCase(unittest.TestCase):\n    def test_load_object_class(self):\n        obj = load_object(Field)\n        self.assertIs(obj, Field)\n        obj = load_object(\"scrapy.item.Field\")\n        self.assertIs(obj, Field)\n\n    def test_load_object_function(self):\n        obj = load_object(load_object)\n        self.assertIs(obj, load_object)\n        obj = load_object(\"scrapy.utils.misc.load_object\")\n        self.assertIs(obj, load_object)\n\n    def test_load_object_exceptions(self):\n        self.assertRaises(ImportError, load_object, \"nomodule999.mod.function\")\n        self.assertRaises(NameError, load_object, \"scrapy.utils.misc.load_object999\")\n        self.assertRaises(TypeError, load_object, {})\n\n    def test_walk_modules(self):\n        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules\")\n        expected = [\n            \"tests.test_utils_misc.test_walk_modules\",\n            \"tests.test_utils_misc.test_walk_modules.mod\",\n            \"tests.test_utils_misc.test_walk_modules.mod.mod0\",\n            \"tests.test_utils_misc.test_walk_modules.mod1\",\n        ]\n        self.assertEqual({m.__name__ for m in mods}, set(expected))\n\n        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules.mod\")\n        expected = [\n            \"tests.test_utils_misc.test_walk_modules.mod\",\n            \"tests.test_utils_misc.test_walk_modules.mod.mod0\",\n        ]\n        self.assertEqual({m.__name__ for m in mods}, set(expected))\n\n        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules.mod1\")\n        expected = [\n            \"tests.test_utils_misc.test_walk_modules.mod1\",\n        ]\n        self.assertEqual({m.__name__ for m in mods}, set(expected))\n\n        self.assertRaises(ImportError, walk_modules, \"nomodule999\")\n\n    def test_walk_modules_egg(self):\n        egg = str(Path(__file__).parent / \"test.egg\")\n        sys.path.append(egg)\n        try:\n            mods = walk_modules(\"testegg\")\n            expected = [\n                \"testegg.spiders\",\n                \"testegg.spiders.a\",\n                \"testegg.spiders.b\",\n                \"testegg\",\n            ]\n            self.assertEqual({m.__name__ for m in mods}, set(expected))\n        finally:\n            sys.path.remove(egg)\n\n    def test_arg_to_iter(self):\n        class TestItem(Item):\n            name = Field()\n\n        assert hasattr(arg_to_iter(None), \"__iter__\")\n        assert hasattr(arg_to_iter(100), \"__iter__\")\n        assert hasattr(arg_to_iter(\"lala\"), \"__iter__\")\n        assert hasattr(arg_to_iter([1, 2, 3]), \"__iter__\")\n        assert hasattr(arg_to_iter(c for c in \"abcd\"), \"__iter__\")\n\n        self.assertEqual(list(arg_to_iter(None)), [])\n        self.assertEqual(list(arg_to_iter(\"lala\")), [\"lala\"])\n        self.assertEqual(list(arg_to_iter(100)), [100])\n        self.assertEqual(list(arg_to_iter(c for c in \"abc\")), [\"a\", \"b\", \"c\"])\n        self.assertEqual(list(arg_to_iter([1, 2, 3])), [1, 2, 3])\n        self.assertEqual(list(arg_to_iter({\"a\": 1})), [{\"a\": 1}])\n        self.assertEqual(\n            list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")]\n        )\n\n    def test_create_instance(self):\n        settings = mock.MagicMock()\n        crawler = mock.MagicMock(spec_set=[\"settings\"])\n        args = (True, 100.0)\n        kwargs = {\"key\": \"val\"}\n\n        def _test_with_settings(mock, settings):\n            create_instance(mock, settings, None, *args, **kwargs)\n            if hasattr(mock, \"from_crawler\"):\n                self.assertEqual(mock.from_crawler.call_count, 0)\n            if hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                self.assertEqual(mock.call_count, 0)\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n\n        def _test_with_crawler(mock, settings, crawler):\n            create_instance(mock, settings, crawler, *args, **kwargs)\n            if hasattr(mock, \"from_crawler\"):\n                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)\n                if hasattr(mock, \"from_settings\"):\n                    self.assertEqual(mock.from_settings.call_count, 0)\n                self.assertEqual(mock.call_count, 0)\n            elif hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                self.assertEqual(mock.call_count, 0)\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n\n        # Check usage of correct constructor using four mocks:\n        #   1. with no alternative constructors\n        #   2. with from_settings() constructor\n        #   3. with from_crawler() constructor\n        #   4. with from_settings() and from_crawler() constructor\n        spec_sets = (\n            [\"__qualname__\"],\n            [\"__qualname__\", \"from_settings\"],\n            [\"__qualname__\", \"from_crawler\"],\n            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n        )\n        for specs in spec_sets:\n            m = mock.MagicMock(spec_set=specs)\n            _test_with_settings(m, settings)\n            m.reset_mock()\n            _test_with_crawler(m, settings, crawler)\n\n        # Check adoption of crawler settings\n        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n        create_instance(m, None, crawler, *args, **kwargs)\n        m.from_settings.assert_called_once_with(crawler.settings, *args, **kwargs)\n\n        with self.assertRaises(ValueError):\n            create_instance(m, None, None)\n\n        m.from_settings.return_value = None\n        with self.assertRaises(TypeError):\n            create_instance(m, settings, None)\n\n    def test_build_from_crawler(self):\n        settings = mock.MagicMock()\n        crawler = mock.MagicMock(spec_set=[\"settings\"])\n        args = (True, 100.0)\n        kwargs = {\"key\": \"val\"}\n\n        def _test_with_crawler(mock, settings, crawler):\n            build_from_crawler(mock, crawler, *args, **kwargs)\n            if hasattr(mock, \"from_crawler\"):\n                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)\n                if hasattr(mock, \"from_settings\"):\n                    self.assertEqual(mock.from_settings.call_count, 0)\n                self.assertEqual(mock.call_count, 0)\n            elif hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                self.assertEqual(mock.call_count, 0)\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n\n        # Check usage of correct constructor using three mocks:\n        #   1. with no alternative constructors\n        #   2. with from_crawler() constructor\n        #   3. with from_settings() and from_crawler() constructor\n        spec_sets = (\n            [\"__qualname__\"],\n            [\"__qualname__\", \"from_crawler\"],\n            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n        )\n        for specs in spec_sets:\n            m = mock.MagicMock(spec_set=specs)\n            _test_with_crawler(m, settings, crawler)\n            m.reset_mock()\n\n        # Check adoption of crawler\n        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_crawler\"])\n        m.from_crawler.return_value = None\n        with self.assertRaises(TypeError):\n            build_from_crawler(m, crawler, *args, **kwargs)\n\n    def test_build_from_settings(self):\n        settings = mock.MagicMock()\n        args = (True, 100.0)\n        kwargs = {\"key\": \"val\"}\n\n        def _test_with_settings(mock, settings):\n            build_from_settings(mock, settings, *args, **kwargs)\n            if hasattr(mock, \"from_settings\"):\n                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                self.assertEqual(mock.call_count, 0)\n            else:\n                mock.assert_called_once_with(*args, **kwargs)\n\n        # Check usage of correct constructor using three mocks:\n        #   1. with no alternative constructors\n        #   2. with from_settings() constructor\n        #   3. with from_settings() and from_crawler() constructor\n        spec_sets = (\n            [\"__qualname__\"],\n            [\"__qualname__\", \"from_settings\"],\n            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n        )\n        for specs in spec_sets:\n            m = mock.MagicMock(spec_set=specs)\n            _test_with_settings(m, settings)\n            m.reset_mock()\n\n        # Check adoption of crawler settings\n        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n        m.from_settings.return_value = None\n        with self.assertRaises(TypeError):\n            build_from_settings(m, settings, *args, **kwargs)\n\n    def test_set_environ(self):\n        assert os.environ.get(\"some_test_environ\") is None\n        with set_environ(some_test_environ=\"test_value\"):\n            assert os.environ.get(\"some_test_environ\") == \"test_value\"\n        assert os.environ.get(\"some_test_environ\") is None\n\n        os.environ[\"some_test_environ\"] = \"test\"\n        assert os.environ.get(\"some_test_environ\") == \"test\"\n        with set_environ(some_test_environ=\"test_value\"):\n            assert os.environ.get(\"some_test_environ\") == \"test_value\"\n        assert os.environ.get(\"some_test_environ\") == \"test\"\n\n    def test_rel_has_nofollow(self):\n        assert rel_has_nofollow(\"ugc nofollow\") is True\n        assert rel_has_nofollow(\"ugc,nofollow\") is True\n        assert rel_has_nofollow(\"ugc\") is False\n        assert rel_has_nofollow(\"nofollow\") is True\n        assert rel_has_nofollow(\"nofollowfoo\") is False\n        assert rel_has_nofollow(\"foonofollow\") is False\n        assert rel_has_nofollow(\"ugc,  ,  nofollow\") is True\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_utils_misc/test_walk_modules/mod1.py": "", "tests/test_utils_misc/test_walk_modules/__init__.py": "", "tests/test_utils_misc/test_walk_modules/mod/mod0.py": "", "tests/test_utils_misc/test_walk_modules/mod/__init__.py": "", "tests/test_cmdline/settings.py": "from pathlib import Path\n\nEXTENSIONS = {\n    \"tests.test_cmdline.extensions.TestExtension\": 0,\n}\n\nTEST1 = \"default\"\n\nFEEDS = {\n    Path(\"items.csv\"): {\n        \"format\": \"csv\",\n        \"fields\": [\"price\", \"name\"],\n    },\n}\n", "tests/test_cmdline/extensions.py": "\"\"\"A test extension used to check the settings loading order\"\"\"\n\n\nclass TestExtension:\n    def __init__(self, settings):\n        settings.set(\"TEST1\", f\"{settings['TEST1']} + started\")\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.settings)\n\n\nclass DummyExtension:\n    pass\n", "tests/test_cmdline/__init__.py": "import json\nimport os\nimport pstats\nimport shutil\nimport sys\nimport tempfile\nimport unittest\nfrom io import StringIO\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\n\nfrom scrapy.utils.test import get_testenv\n\n\nclass CmdlineTest(unittest.TestCase):\n    def setUp(self):\n        self.env = get_testenv()\n        tests_path = Path(__file__).parent.parent\n        self.env[\"PYTHONPATH\"] += os.pathsep + str(tests_path.parent)\n        self.env[\"SCRAPY_SETTINGS_MODULE\"] = \"tests.test_cmdline.settings\"\n\n    def _execute(self, *new_args, **kwargs):\n        encoding = sys.stdout.encoding or \"utf-8\"\n        args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n        proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n\n    def test_default_settings(self):\n        self.assertEqual(self._execute(\"settings\", \"--get\", \"TEST1\"), \"default\")\n\n    def test_override_settings_using_set_arg(self):\n        self.assertEqual(\n            self._execute(\"settings\", \"--get\", \"TEST1\", \"-s\", \"TEST1=override\"),\n            \"override\",\n        )\n\n    def test_profiling(self):\n        path = Path(tempfile.mkdtemp())\n        filename = path / \"res.prof\"\n        try:\n            self._execute(\"version\", \"--profile\", str(filename))\n            self.assertTrue(filename.exists())\n            out = StringIO()\n            stats = pstats.Stats(str(filename), stream=out)\n            stats.print_stats()\n            out.seek(0)\n            stats = out.read()\n            self.assertIn(str(Path(\"scrapy\", \"commands\", \"version.py\")), stats)\n            self.assertIn(\"tottime\", stats)\n        finally:\n            shutil.rmtree(path)\n\n    def test_override_dict_settings(self):\n        EXT_PATH = \"tests.test_cmdline.extensions.DummyExtension\"\n        EXTENSIONS = {EXT_PATH: 200}\n        settingsstr = self._execute(\n            \"settings\",\n            \"--get\",\n            \"EXTENSIONS\",\n            \"-s\",\n            \"EXTENSIONS=\" + json.dumps(EXTENSIONS),\n        )\n        # XXX: There's gotta be a smarter way to do this...\n        self.assertNotIn(\"...\", settingsstr)\n        for char in (\"'\", \"<\", \">\"):\n            settingsstr = settingsstr.replace(char, '\"')\n        settingsdict = json.loads(settingsstr)\n        self.assertCountEqual(settingsdict.keys(), EXTENSIONS.keys())\n        self.assertEqual(200, settingsdict[EXT_PATH])\n\n    def test_pathlib_path_as_feeds_key(self):\n        self.assertEqual(\n            self._execute(\"settings\", \"--get\", \"FEEDS\"),\n            json.dumps({\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}),\n        )\n", "tests/test_settings/__init__.py": "import unittest\nfrom unittest import mock\n\nimport pytest\n\nfrom scrapy.settings import (\n    SETTINGS_PRIORITIES,\n    BaseSettings,\n    Settings,\n    SettingsAttribute,\n    get_settings_priority,\n)\n\nfrom . import default_settings\n\n\nclass SettingsGlobalFuncsTest(unittest.TestCase):\n    def test_get_settings_priority(self):\n        for prio_str, prio_num in SETTINGS_PRIORITIES.items():\n            self.assertEqual(get_settings_priority(prio_str), prio_num)\n        self.assertEqual(get_settings_priority(99), 99)\n\n\nclass SettingsAttributeTest(unittest.TestCase):\n    def setUp(self):\n        self.attribute = SettingsAttribute(\"value\", 10)\n\n    def test_set_greater_priority(self):\n        self.attribute.set(\"value2\", 20)\n        self.assertEqual(self.attribute.value, \"value2\")\n        self.assertEqual(self.attribute.priority, 20)\n\n    def test_set_equal_priority(self):\n        self.attribute.set(\"value2\", 10)\n        self.assertEqual(self.attribute.value, \"value2\")\n        self.assertEqual(self.attribute.priority, 10)\n\n    def test_set_less_priority(self):\n        self.attribute.set(\"value2\", 0)\n        self.assertEqual(self.attribute.value, \"value\")\n        self.assertEqual(self.attribute.priority, 10)\n\n    def test_overwrite_basesettings(self):\n        original_dict = {\"one\": 10, \"two\": 20}\n        original_settings = BaseSettings(original_dict, 0)\n        attribute = SettingsAttribute(original_settings, 0)\n\n        new_dict = {\"three\": 11, \"four\": 21}\n        attribute.set(new_dict, 10)\n        self.assertIsInstance(attribute.value, BaseSettings)\n        self.assertCountEqual(attribute.value, new_dict)\n        self.assertCountEqual(original_settings, original_dict)\n\n        new_settings = BaseSettings({\"five\": 12}, 0)\n        attribute.set(new_settings, 0)  # Insufficient priority\n        self.assertCountEqual(attribute.value, new_dict)\n        attribute.set(new_settings, 10)\n        self.assertCountEqual(attribute.value, new_settings)\n\n    def test_repr(self):\n        self.assertEqual(\n            repr(self.attribute), \"<SettingsAttribute value='value' priority=10>\"\n        )\n\n\nclass BaseSettingsTest(unittest.TestCase):\n    def setUp(self):\n        self.settings = BaseSettings()\n\n    def test_setdefault_not_existing_value(self):\n        settings = BaseSettings()\n        value = settings.setdefault(\"TEST_OPTION\", \"value\")\n        self.assertEqual(settings[\"TEST_OPTION\"], \"value\")\n        self.assertEqual(value, \"value\")\n        self.assertIsNotNone(value)\n\n    def test_setdefault_existing_value(self):\n        settings = BaseSettings({\"TEST_OPTION\": \"value\"})\n        value = settings.setdefault(\"TEST_OPTION\", None)\n        self.assertEqual(settings[\"TEST_OPTION\"], \"value\")\n        self.assertEqual(value, \"value\")\n\n    def test_set_new_attribute(self):\n        self.settings.set(\"TEST_OPTION\", \"value\", 0)\n        self.assertIn(\"TEST_OPTION\", self.settings.attributes)\n\n        attr = self.settings.attributes[\"TEST_OPTION\"]\n        self.assertIsInstance(attr, SettingsAttribute)\n        self.assertEqual(attr.value, \"value\")\n        self.assertEqual(attr.priority, 0)\n\n    def test_set_settingsattribute(self):\n        myattr = SettingsAttribute(0, 30)  # Note priority 30\n        self.settings.set(\"TEST_ATTR\", myattr, 10)\n        self.assertEqual(self.settings.get(\"TEST_ATTR\"), 0)\n        self.assertEqual(self.settings.getpriority(\"TEST_ATTR\"), 30)\n\n    def test_set_instance_identity_on_update(self):\n        attr = SettingsAttribute(\"value\", 0)\n        self.settings.attributes = {\"TEST_OPTION\": attr}\n        self.settings.set(\"TEST_OPTION\", \"othervalue\", 10)\n\n        self.assertIn(\"TEST_OPTION\", self.settings.attributes)\n        self.assertIs(attr, self.settings.attributes[\"TEST_OPTION\"])\n\n    def test_set_calls_settings_attributes_methods_on_update(self):\n        attr = SettingsAttribute(\"value\", 10)\n        with mock.patch.object(attr, \"__setattr__\") as mock_setattr, mock.patch.object(\n            attr, \"set\"\n        ) as mock_set:\n            self.settings.attributes = {\"TEST_OPTION\": attr}\n\n            for priority in (0, 10, 20):\n                self.settings.set(\"TEST_OPTION\", \"othervalue\", priority)\n                mock_set.assert_called_once_with(\"othervalue\", priority)\n                self.assertFalse(mock_setattr.called)\n                mock_set.reset_mock()\n                mock_setattr.reset_mock()\n\n    def test_setitem(self):\n        settings = BaseSettings()\n        settings.set(\"key\", \"a\", \"default\")\n        settings[\"key\"] = \"b\"\n        self.assertEqual(settings[\"key\"], \"b\")\n        self.assertEqual(settings.getpriority(\"key\"), 20)\n        settings[\"key\"] = \"c\"\n        self.assertEqual(settings[\"key\"], \"c\")\n        settings[\"key2\"] = \"x\"\n        self.assertIn(\"key2\", settings)\n        self.assertEqual(settings[\"key2\"], \"x\")\n        self.assertEqual(settings.getpriority(\"key2\"), 20)\n\n    def test_setdict_alias(self):\n        with mock.patch.object(self.settings, \"set\") as mock_set:\n            self.settings.setdict({\"TEST_1\": \"value1\", \"TEST_2\": \"value2\"}, 10)\n            self.assertEqual(mock_set.call_count, 2)\n            calls = [\n                mock.call(\"TEST_1\", \"value1\", 10),\n                mock.call(\"TEST_2\", \"value2\", 10),\n            ]\n            mock_set.assert_has_calls(calls, any_order=True)\n\n    def test_setmodule_only_load_uppercase_vars(self):\n        class ModuleMock:\n            UPPERCASE_VAR = \"value\"\n            MIXEDcase_VAR = \"othervalue\"\n            lowercase_var = \"anothervalue\"\n\n        self.settings.attributes = {}\n        self.settings.setmodule(ModuleMock(), 10)\n        self.assertIn(\"UPPERCASE_VAR\", self.settings.attributes)\n        self.assertNotIn(\"MIXEDcase_VAR\", self.settings.attributes)\n        self.assertNotIn(\"lowercase_var\", self.settings.attributes)\n        self.assertEqual(len(self.settings.attributes), 1)\n\n    def test_setmodule_alias(self):\n        with mock.patch.object(self.settings, \"set\") as mock_set:\n            self.settings.setmodule(default_settings, 10)\n            mock_set.assert_any_call(\"TEST_DEFAULT\", \"defvalue\", 10)\n            mock_set.assert_any_call(\"TEST_DICT\", {\"key\": \"val\"}, 10)\n\n    def test_setmodule_by_path(self):\n        self.settings.attributes = {}\n        self.settings.setmodule(default_settings, 10)\n        ctrl_attributes = self.settings.attributes.copy()\n\n        self.settings.attributes = {}\n        self.settings.setmodule(\"tests.test_settings.default_settings\", 10)\n\n        self.assertCountEqual(self.settings.attributes.keys(), ctrl_attributes.keys())\n\n        for key in ctrl_attributes.keys():\n            attr = self.settings.attributes[key]\n            ctrl_attr = ctrl_attributes[key]\n            self.assertEqual(attr.value, ctrl_attr.value)\n            self.assertEqual(attr.priority, ctrl_attr.priority)\n\n    def test_update(self):\n        settings = BaseSettings({\"key_lowprio\": 0}, priority=0)\n        settings.set(\"key_highprio\", 10, priority=50)\n        custom_settings = BaseSettings(\n            {\"key_lowprio\": 1, \"key_highprio\": 11}, priority=30\n        )\n        custom_settings.set(\"newkey_one\", None, priority=50)\n        custom_dict = {\"key_lowprio\": 2, \"key_highprio\": 12, \"newkey_two\": None}\n\n        settings.update(custom_dict, priority=20)\n        self.assertEqual(settings[\"key_lowprio\"], 2)\n        self.assertEqual(settings.getpriority(\"key_lowprio\"), 20)\n        self.assertEqual(settings[\"key_highprio\"], 10)\n        self.assertIn(\"newkey_two\", settings)\n        self.assertEqual(settings.getpriority(\"newkey_two\"), 20)\n\n        settings.update(custom_settings)\n        self.assertEqual(settings[\"key_lowprio\"], 1)\n        self.assertEqual(settings.getpriority(\"key_lowprio\"), 30)\n        self.assertEqual(settings[\"key_highprio\"], 10)\n        self.assertIn(\"newkey_one\", settings)\n        self.assertEqual(settings.getpriority(\"newkey_one\"), 50)\n\n        settings.update({\"key_lowprio\": 3}, priority=20)\n        self.assertEqual(settings[\"key_lowprio\"], 1)\n\n    @pytest.mark.xfail(\n        raises=TypeError, reason=\"BaseSettings.update doesn't support kwargs input\"\n    )\n    def test_update_kwargs(self):\n        settings = BaseSettings({\"key\": 0})\n        settings.update(key=1)  # pylint: disable=unexpected-keyword-arg\n\n    @pytest.mark.xfail(\n        raises=AttributeError,\n        reason=\"BaseSettings.update doesn't support iterable input\",\n    )\n    def test_update_iterable(self):\n        settings = BaseSettings({\"key\": 0})\n        settings.update([(\"key\", 1)])\n\n    def test_update_jsonstring(self):\n        settings = BaseSettings({\"number\": 0, \"dict\": BaseSettings({\"key\": \"val\"})})\n        settings.update('{\"number\": 1, \"newnumber\": 2}')\n        self.assertEqual(settings[\"number\"], 1)\n        self.assertEqual(settings[\"newnumber\"], 2)\n        settings.set(\"dict\", '{\"key\": \"newval\", \"newkey\": \"newval2\"}')\n        self.assertEqual(settings[\"dict\"][\"key\"], \"newval\")\n        self.assertEqual(settings[\"dict\"][\"newkey\"], \"newval2\")\n\n    def test_delete(self):\n        settings = BaseSettings({\"key\": None})\n        settings.set(\"key_highprio\", None, priority=50)\n        settings.delete(\"key\")\n        settings.delete(\"key_highprio\")\n        self.assertNotIn(\"key\", settings)\n        self.assertIn(\"key_highprio\", settings)\n        del settings[\"key_highprio\"]\n        self.assertNotIn(\"key_highprio\", settings)\n        with self.assertRaises(KeyError):\n            settings.delete(\"notkey\")\n        with self.assertRaises(KeyError):\n            del settings[\"notkey\"]\n\n    def test_get(self):\n        test_configuration = {\n            \"TEST_ENABLED1\": \"1\",\n            \"TEST_ENABLED2\": True,\n            \"TEST_ENABLED3\": 1,\n            \"TEST_ENABLED4\": \"True\",\n            \"TEST_ENABLED5\": \"true\",\n            \"TEST_ENABLED_WRONG\": \"on\",\n            \"TEST_DISABLED1\": \"0\",\n            \"TEST_DISABLED2\": False,\n            \"TEST_DISABLED3\": 0,\n            \"TEST_DISABLED4\": \"False\",\n            \"TEST_DISABLED5\": \"false\",\n            \"TEST_DISABLED_WRONG\": \"off\",\n            \"TEST_INT1\": 123,\n            \"TEST_INT2\": \"123\",\n            \"TEST_FLOAT1\": 123.45,\n            \"TEST_FLOAT2\": \"123.45\",\n            \"TEST_LIST1\": [\"one\", \"two\"],\n            \"TEST_LIST2\": \"one,two\",\n            \"TEST_STR\": \"value\",\n            \"TEST_DICT1\": {\"key1\": \"val1\", \"ke2\": 3},\n            \"TEST_DICT2\": '{\"key1\": \"val1\", \"ke2\": 3}',\n        }\n        settings = self.settings\n        settings.attributes = {\n            key: SettingsAttribute(value, 0)\n            for key, value in test_configuration.items()\n        }\n\n        self.assertTrue(settings.getbool(\"TEST_ENABLED1\"))\n        self.assertTrue(settings.getbool(\"TEST_ENABLED2\"))\n        self.assertTrue(settings.getbool(\"TEST_ENABLED3\"))\n        self.assertTrue(settings.getbool(\"TEST_ENABLED4\"))\n        self.assertTrue(settings.getbool(\"TEST_ENABLED5\"))\n        self.assertFalse(settings.getbool(\"TEST_ENABLEDx\"))\n        self.assertTrue(settings.getbool(\"TEST_ENABLEDx\", True))\n        self.assertFalse(settings.getbool(\"TEST_DISABLED1\"))\n        self.assertFalse(settings.getbool(\"TEST_DISABLED2\"))\n        self.assertFalse(settings.getbool(\"TEST_DISABLED3\"))\n        self.assertFalse(settings.getbool(\"TEST_DISABLED4\"))\n        self.assertFalse(settings.getbool(\"TEST_DISABLED5\"))\n        self.assertEqual(settings.getint(\"TEST_INT1\"), 123)\n        self.assertEqual(settings.getint(\"TEST_INT2\"), 123)\n        self.assertEqual(settings.getint(\"TEST_INTx\"), 0)\n        self.assertEqual(settings.getint(\"TEST_INTx\", 45), 45)\n        self.assertEqual(settings.getfloat(\"TEST_FLOAT1\"), 123.45)\n        self.assertEqual(settings.getfloat(\"TEST_FLOAT2\"), 123.45)\n        self.assertEqual(settings.getfloat(\"TEST_FLOATx\"), 0.0)\n        self.assertEqual(settings.getfloat(\"TEST_FLOATx\", 55.0), 55.0)\n        self.assertEqual(settings.getlist(\"TEST_LIST1\"), [\"one\", \"two\"])\n        self.assertEqual(settings.getlist(\"TEST_LIST2\"), [\"one\", \"two\"])\n        self.assertEqual(settings.getlist(\"TEST_LISTx\"), [])\n        self.assertEqual(settings.getlist(\"TEST_LISTx\", [\"default\"]), [\"default\"])\n        self.assertEqual(settings[\"TEST_STR\"], \"value\")\n        self.assertEqual(settings.get(\"TEST_STR\"), \"value\")\n        self.assertEqual(settings[\"TEST_STRx\"], None)\n        self.assertEqual(settings.get(\"TEST_STRx\"), None)\n        self.assertEqual(settings.get(\"TEST_STRx\", \"default\"), \"default\")\n        self.assertEqual(settings.getdict(\"TEST_DICT1\"), {\"key1\": \"val1\", \"ke2\": 3})\n        self.assertEqual(settings.getdict(\"TEST_DICT2\"), {\"key1\": \"val1\", \"ke2\": 3})\n        self.assertEqual(settings.getdict(\"TEST_DICT3\"), {})\n        self.assertEqual(settings.getdict(\"TEST_DICT3\", {\"key1\": 5}), {\"key1\": 5})\n        self.assertRaises(ValueError, settings.getdict, \"TEST_LIST1\")\n        self.assertRaises(ValueError, settings.getbool, \"TEST_ENABLED_WRONG\")\n        self.assertRaises(ValueError, settings.getbool, \"TEST_DISABLED_WRONG\")\n\n    def test_getpriority(self):\n        settings = BaseSettings({\"key\": \"value\"}, priority=99)\n        self.assertEqual(settings.getpriority(\"key\"), 99)\n        self.assertEqual(settings.getpriority(\"nonexistentkey\"), None)\n\n    def test_getwithbase(self):\n        s = BaseSettings(\n            {\n                \"TEST_BASE\": BaseSettings({1: 1, 2: 2}, \"project\"),\n                \"TEST\": BaseSettings({1: 10, 3: 30}, \"default\"),\n                \"HASNOBASE\": BaseSettings({3: 3000}, \"default\"),\n            }\n        )\n        s[\"TEST\"].set(2, 200, \"cmdline\")\n        self.assertCountEqual(s.getwithbase(\"TEST\"), {1: 1, 2: 200, 3: 30})\n        self.assertCountEqual(s.getwithbase(\"HASNOBASE\"), s[\"HASNOBASE\"])\n        self.assertEqual(s.getwithbase(\"NONEXISTENT\"), {})\n\n    def test_maxpriority(self):\n        # Empty settings should return 'default'\n        self.assertEqual(self.settings.maxpriority(), 0)\n        self.settings.set(\"A\", 0, 10)\n        self.settings.set(\"B\", 0, 30)\n        self.assertEqual(self.settings.maxpriority(), 30)\n\n    def test_copy(self):\n        values = {\n            \"TEST_BOOL\": True,\n            \"TEST_LIST\": [\"one\", \"two\"],\n            \"TEST_LIST_OF_LISTS\": [\n                [\"first_one\", \"first_two\"],\n                [\"second_one\", \"second_two\"],\n            ],\n        }\n        self.settings.setdict(values)\n        copy = self.settings.copy()\n        self.settings.set(\"TEST_BOOL\", False)\n        self.assertTrue(copy.get(\"TEST_BOOL\"))\n\n        test_list = self.settings.get(\"TEST_LIST\")\n        test_list.append(\"three\")\n        self.assertListEqual(copy.get(\"TEST_LIST\"), [\"one\", \"two\"])\n\n        test_list_of_lists = self.settings.get(\"TEST_LIST_OF_LISTS\")\n        test_list_of_lists[0].append(\"first_three\")\n        self.assertListEqual(\n            copy.get(\"TEST_LIST_OF_LISTS\")[0], [\"first_one\", \"first_two\"]\n        )\n\n    def test_copy_to_dict(self):\n        s = BaseSettings(\n            {\n                \"TEST_STRING\": \"a string\",\n                \"TEST_LIST\": [1, 2],\n                \"TEST_BOOLEAN\": False,\n                \"TEST_BASE\": BaseSettings({1: 1, 2: 2}, \"project\"),\n                \"TEST\": BaseSettings({1: 10, 3: 30}, \"default\"),\n                \"HASNOBASE\": BaseSettings({3: 3000}, \"default\"),\n            }\n        )\n        self.assertDictEqual(\n            s.copy_to_dict(),\n            {\n                \"HASNOBASE\": {3: 3000},\n                \"TEST\": {1: 10, 3: 30},\n                \"TEST_BASE\": {1: 1, 2: 2},\n                \"TEST_LIST\": [1, 2],\n                \"TEST_BOOLEAN\": False,\n                \"TEST_STRING\": \"a string\",\n            },\n        )\n\n    def test_freeze(self):\n        self.settings.freeze()\n        with self.assertRaises(TypeError) as cm:\n            self.settings.set(\"TEST_BOOL\", False)\n            self.assertEqual(\n                str(cm.exception), \"Trying to modify an immutable Settings object\"\n            )\n\n    def test_frozencopy(self):\n        frozencopy = self.settings.frozencopy()\n        self.assertTrue(frozencopy.frozen)\n        self.assertIsNot(frozencopy, self.settings)\n\n\nclass SettingsTest(unittest.TestCase):\n    def setUp(self):\n        self.settings = Settings()\n\n    @mock.patch.dict(\"scrapy.settings.SETTINGS_PRIORITIES\", {\"default\": 10})\n    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n    def test_initial_defaults(self):\n        settings = Settings()\n        self.assertEqual(len(settings.attributes), 2)\n        self.assertIn(\"TEST_DEFAULT\", settings.attributes)\n\n        attr = settings.attributes[\"TEST_DEFAULT\"]\n        self.assertIsInstance(attr, SettingsAttribute)\n        self.assertEqual(attr.value, \"defvalue\")\n        self.assertEqual(attr.priority, 10)\n\n    @mock.patch.dict(\"scrapy.settings.SETTINGS_PRIORITIES\", {})\n    @mock.patch(\"scrapy.settings.default_settings\", {})\n    def test_initial_values(self):\n        settings = Settings({\"TEST_OPTION\": \"value\"}, 10)\n        self.assertEqual(len(settings.attributes), 1)\n        self.assertIn(\"TEST_OPTION\", settings.attributes)\n\n        attr = settings.attributes[\"TEST_OPTION\"]\n        self.assertIsInstance(attr, SettingsAttribute)\n        self.assertEqual(attr.value, \"value\")\n        self.assertEqual(attr.priority, 10)\n\n    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n    def test_autopromote_dicts(self):\n        settings = Settings()\n        mydict = settings.get(\"TEST_DICT\")\n        self.assertIsInstance(mydict, BaseSettings)\n        self.assertIn(\"key\", mydict)\n        self.assertEqual(mydict[\"key\"], \"val\")  # pylint: disable=unsubscriptable-object\n        self.assertEqual(mydict.getpriority(\"key\"), 0)\n\n    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n    def test_getdict_autodegrade_basesettings(self):\n        settings = Settings()\n        mydict = settings.getdict(\"TEST_DICT\")\n        self.assertIsInstance(mydict, dict)\n        self.assertEqual(len(mydict), 1)\n        self.assertIn(\"key\", mydict)\n        self.assertEqual(mydict[\"key\"], \"val\")\n\n    def test_passing_objects_as_values(self):\n        from scrapy.core.downloader.handlers.file import FileDownloadHandler\n        from scrapy.utils.misc import build_from_crawler\n        from scrapy.utils.test import get_crawler\n\n        class TestPipeline:\n            def process_item(self, i, s):\n                return i\n\n        settings = Settings(\n            {\n                \"ITEM_PIPELINES\": {\n                    TestPipeline: 800,\n                },\n                \"DOWNLOAD_HANDLERS\": {\n                    \"ftp\": FileDownloadHandler,\n                },\n            }\n        )\n\n        self.assertIn(\"ITEM_PIPELINES\", settings.attributes)\n\n        mypipeline, priority = settings.getdict(\"ITEM_PIPELINES\").popitem()\n        self.assertEqual(priority, 800)\n        self.assertEqual(mypipeline, TestPipeline)\n        self.assertIsInstance(mypipeline(), TestPipeline)\n        self.assertEqual(mypipeline().process_item(\"item\", None), \"item\")\n\n        myhandler = settings.getdict(\"DOWNLOAD_HANDLERS\").pop(\"ftp\")\n        self.assertEqual(myhandler, FileDownloadHandler)\n        myhandler_instance = build_from_crawler(myhandler, get_crawler())\n        self.assertIsInstance(myhandler_instance, FileDownloadHandler)\n        self.assertTrue(hasattr(myhandler_instance, \"download_request\"))\n\n    def test_pop_item_with_default_value(self):\n        settings = Settings()\n\n        with self.assertRaises(KeyError):\n            settings.pop(\"DUMMY_CONFIG\")\n\n        dummy_config_value = settings.pop(\"DUMMY_CONFIG\", \"dummy_value\")\n        self.assertEqual(dummy_config_value, \"dummy_value\")\n\n    def test_pop_item_with_immutable_settings(self):\n        settings = Settings(\n            {\"DUMMY_CONFIG\": \"dummy_value\", \"OTHER_DUMMY_CONFIG\": \"other_dummy_value\"}\n        )\n\n        self.assertEqual(settings.pop(\"DUMMY_CONFIG\"), \"dummy_value\")\n\n        settings.freeze()\n\n        with self.assertRaises(TypeError) as error:\n            settings.pop(\"OTHER_DUMMY_CONFIG\")\n\n        self.assertEqual(\n            str(error.exception), \"Trying to modify an immutable Settings object\"\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/test_settings/default_settings.py": "TEST_DEFAULT = \"defvalue\"\n\nTEST_DICT = {\"key\": \"val\"}\n", "tests/CrawlerRunner/ip_address.py": "from urllib.parse import urlparse\n\nfrom twisted.internet import reactor\nfrom twisted.names import cache\nfrom twisted.names import hosts as hostsModule\nfrom twisted.names import resolve\nfrom twisted.names.client import Resolver\nfrom twisted.python.runtime import platform\n\nfrom scrapy import Request, Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.log import configure_logging\nfrom tests.mockserver import MockDNSServer, MockServer\n\n\n# https://stackoverflow.com/a/32784190\ndef createResolver(servers=None, resolvconf=None, hosts=None):\n    if hosts is None:\n        hosts = b\"/etc/hosts\" if platform.getType() == \"posix\" else r\"c:\\windows\\hosts\"\n    theResolver = Resolver(resolvconf, servers)\n    hostResolver = hostsModule.Resolver(hosts)\n    chain = [hostResolver, cache.CacheResolver(), theResolver]\n    return resolve.ResolverChain(chain)\n\n\nclass LocalhostSpider(Spider):\n    name = \"localhost_spider\"\n\n    def start_requests(self):\n        yield Request(self.url)\n\n    def parse(self, response):\n        netloc = urlparse_cached(response).netloc\n        host = netloc.split(\":\")[0]\n        self.logger.info(f\"Host: {host}\")\n        self.logger.info(f\"Type: {type(response.ip_address)}\")\n        self.logger.info(f\"IP address: {response.ip_address}\")\n\n\nif __name__ == \"__main__\":\n    with MockServer() as mock_http_server, MockDNSServer() as mock_dns_server:\n        port = urlparse(mock_http_server.http_address).port\n        url = f\"http://not.a.real.domain:{port}/echo\"\n\n        servers = [(mock_dns_server.host, mock_dns_server.port)]\n        reactor.installResolver(createResolver(servers=servers))\n\n        configure_logging()\n        runner = CrawlerRunner()\n        d = runner.crawl(LocalhostSpider, url=url)\n        d.addBoth(lambda _: reactor.stop())\n        reactor.run()\n", "tests/CrawlerRunner/change_reactor.py": "from scrapy import Spider\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\n\nclass NoRequestsSpider(Spider):\n    name = \"no_request\"\n\n    custom_settings = {\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n    }\n\n    def start_requests(self):\n        return []\n\n\nconfigure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n\n\nfrom scrapy.utils.reactor import install_reactor\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\nrunner = CrawlerRunner()\n\nd = runner.crawl(NoRequestsSpider)\n\nfrom twisted.internet import reactor\n\nd.addBoth(callback=lambda _: reactor.stop())\nreactor.run()\n", "tests/mocks/dummydbm.py": "\"\"\"DBM-like dummy module\"\"\"\n\nimport collections\nfrom typing import Any, DefaultDict\n\n\nclass DummyDB(dict):\n    \"\"\"Provide dummy DBM-like interface.\"\"\"\n\n    def close(self):\n        pass\n\n\nerror = KeyError\n\n\n_DATABASES: DefaultDict[Any, DummyDB] = collections.defaultdict(DummyDB)\n\n\ndef open(file, flag=\"r\", mode=0o666):\n    \"\"\"Open or create a dummy database compatible.\n\n    Arguments ``flag`` and ``mode`` are ignored.\n    \"\"\"\n    # return same instance for same file argument\n    return _DATABASES[file]\n", "tests/mocks/__init__.py": "", "tests/test_spiderloader/__init__.py": "import shutil\nimport sys\nimport tempfile\nimport warnings\nfrom pathlib import Path\nfrom tempfile import mkdtemp\n\nfrom twisted.trial import unittest\nfrom zope.interface.verify import verifyObject\n\n# ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n# alone\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.http import Request\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.settings import Settings\nfrom scrapy.spiderloader import SpiderLoader\n\nmodule_dir = Path(__file__).resolve().parent\n\n\ndef _copytree(source: Path, target: Path):\n    try:\n        shutil.copytree(source, target)\n    except shutil.Error:\n        pass\n\n\nclass SpiderLoaderTest(unittest.TestCase):\n    def setUp(self):\n        orig_spiders_dir = module_dir / \"test_spiders\"\n        self.tmpdir = Path(tempfile.mkdtemp())\n        self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n        _copytree(orig_spiders_dir, self.spiders_dir)\n        sys.path.append(str(self.tmpdir))\n        settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n        self.spider_loader = SpiderLoader.from_settings(settings)\n\n    def tearDown(self):\n        del self.spider_loader\n        del sys.modules[\"test_spiders_xxx\"]\n        sys.path.remove(str(self.tmpdir))\n\n    def test_interface(self):\n        verifyObject(ISpiderLoader, self.spider_loader)\n\n    def test_list(self):\n        self.assertEqual(\n            set(self.spider_loader.list()), {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n        )\n\n    def test_load(self):\n        spider1 = self.spider_loader.load(\"spider1\")\n        self.assertEqual(spider1.__name__, \"Spider1\")\n\n    def test_find_by_request(self):\n        self.assertEqual(\n            self.spider_loader.find_by_request(Request(\"http://scrapy1.org/test\")),\n            [\"spider1\"],\n        )\n        self.assertEqual(\n            self.spider_loader.find_by_request(Request(\"http://scrapy2.org/test\")),\n            [\"spider2\"],\n        )\n        self.assertEqual(\n            set(self.spider_loader.find_by_request(Request(\"http://scrapy3.org/test\"))),\n            {\"spider1\", \"spider2\"},\n        )\n        self.assertEqual(\n            self.spider_loader.find_by_request(Request(\"http://scrapy999.org/test\")), []\n        )\n        self.assertEqual(\n            self.spider_loader.find_by_request(Request(\"http://spider3.com\")), []\n        )\n        self.assertEqual(\n            self.spider_loader.find_by_request(Request(\"http://spider3.com/onlythis\")),\n            [\"spider3\"],\n        )\n\n    def test_load_spider_module(self):\n        module = \"tests.test_spiderloader.test_spiders.spider1\"\n        settings = Settings({\"SPIDER_MODULES\": [module]})\n        self.spider_loader = SpiderLoader.from_settings(settings)\n        assert len(self.spider_loader._spiders) == 1\n\n    def test_load_spider_module_multiple(self):\n        prefix = \"tests.test_spiderloader.test_spiders.\"\n        module = \",\".join(prefix + s for s in (\"spider1\", \"spider2\"))\n        settings = Settings({\"SPIDER_MODULES\": module})\n        self.spider_loader = SpiderLoader.from_settings(settings)\n        assert len(self.spider_loader._spiders) == 2\n\n    def test_load_base_spider(self):\n        module = \"tests.test_spiderloader.test_spiders.spider0\"\n        settings = Settings({\"SPIDER_MODULES\": [module]})\n        self.spider_loader = SpiderLoader.from_settings(settings)\n        assert len(self.spider_loader._spiders) == 0\n\n    def test_crawler_runner_loading(self):\n        module = \"tests.test_spiderloader.test_spiders.spider1\"\n        runner = CrawlerRunner(\n            {\n                \"SPIDER_MODULES\": [module],\n                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n            }\n        )\n\n        self.assertRaisesRegex(\n            KeyError, \"Spider not found\", runner.create_crawler, \"spider2\"\n        )\n\n        crawler = runner.create_crawler(\"spider1\")\n        self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n        self.assertEqual(crawler.spidercls.name, \"spider1\")\n\n    def test_bad_spider_modules_exception(self):\n        module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n        settings = Settings({\"SPIDER_MODULES\": [module]})\n        self.assertRaises(ImportError, SpiderLoader.from_settings, settings)\n\n    def test_bad_spider_modules_warning(self):\n        with warnings.catch_warnings(record=True) as w:\n            module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n            settings = Settings(\n                {\"SPIDER_MODULES\": [module], \"SPIDER_LOADER_WARN_ONLY\": True}\n            )\n            spider_loader = SpiderLoader.from_settings(settings)\n            if str(w[0].message).startswith(\"_SixMetaPathImporter\"):\n                # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,\n                # at least until all six versions we can import (including botocore.vendored.six)\n                # are updated to 1.16.0+\n                w.pop(0)\n            self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n\n            spiders = spider_loader.list()\n            self.assertEqual(spiders, [])\n\n\nclass DuplicateSpiderNameLoaderTest(unittest.TestCase):\n    def setUp(self):\n        orig_spiders_dir = module_dir / \"test_spiders\"\n        self.tmpdir = Path(mkdtemp())\n        self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n        _copytree(orig_spiders_dir, self.spiders_dir)\n        sys.path.append(str(self.tmpdir))\n        self.settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n\n    def tearDown(self):\n        del sys.modules[\"test_spiders_xxx\"]\n        sys.path.remove(str(self.tmpdir))\n\n    def test_dupename_warning(self):\n        # copy 1 spider module so as to have duplicate spider name\n        shutil.copyfile(\n            self.tmpdir / \"test_spiders_xxx\" / \"spider3.py\",\n            self.tmpdir / \"test_spiders_xxx\" / \"spider3dupe.py\",\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n            spider_loader = SpiderLoader.from_settings(self.settings)\n\n            self.assertEqual(len(w), 1)\n            msg = str(w[0].message)\n            self.assertIn(\"several spiders with the same name\", msg)\n            self.assertIn(\"'spider3'\", msg)\n            self.assertTrue(msg.count(\"'spider3'\") == 2)\n\n            self.assertNotIn(\"'spider1'\", msg)\n            self.assertNotIn(\"'spider2'\", msg)\n            self.assertNotIn(\"'spider4'\", msg)\n\n            spiders = set(spider_loader.list())\n            self.assertEqual(spiders, {\"spider1\", \"spider2\", \"spider3\", \"spider4\"})\n\n    def test_multiple_dupename_warning(self):\n        # copy 2 spider modules so as to have duplicate spider name\n        # This should issue 2 warning, 1 for each duplicate spider name\n        shutil.copyfile(\n            self.tmpdir / \"test_spiders_xxx\" / \"spider1.py\",\n            self.tmpdir / \"test_spiders_xxx\" / \"spider1dupe.py\",\n        )\n        shutil.copyfile(\n            self.tmpdir / \"test_spiders_xxx\" / \"spider2.py\",\n            self.tmpdir / \"test_spiders_xxx\" / \"spider2dupe.py\",\n        )\n\n        with warnings.catch_warnings(record=True) as w:\n            spider_loader = SpiderLoader.from_settings(self.settings)\n\n            self.assertEqual(len(w), 1)\n            msg = str(w[0].message)\n            self.assertIn(\"several spiders with the same name\", msg)\n            self.assertIn(\"'spider1'\", msg)\n            self.assertTrue(msg.count(\"'spider1'\") == 2)\n\n            self.assertIn(\"'spider2'\", msg)\n            self.assertTrue(msg.count(\"'spider2'\") == 2)\n\n            self.assertNotIn(\"'spider3'\", msg)\n            self.assertNotIn(\"'spider4'\", msg)\n\n            spiders = set(spider_loader.list())\n            self.assertEqual(spiders, {\"spider1\", \"spider2\", \"spider3\", \"spider4\"})\n", "tests/test_spiderloader/test_spiders/spider2.py": "from scrapy.spiders import Spider\n\n\nclass Spider2(Spider):\n    name = \"spider2\"\n    allowed_domains = [\"scrapy2.org\", \"scrapy3.org\"]\n", "tests/test_spiderloader/test_spiders/__init__.py": "", "tests/test_spiderloader/test_spiders/spider0.py": "from scrapy.spiders import Spider\n\n\nclass Spider0(Spider):\n    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n", "tests/test_spiderloader/test_spiders/spider1.py": "from scrapy.spiders import Spider\n\n\nclass Spider1(Spider):\n    name = \"spider1\"\n    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n", "tests/test_spiderloader/test_spiders/spider3.py": "from scrapy.spiders import Spider\n\n\nclass Spider3(Spider):\n    name = \"spider3\"\n    allowed_domains = [\"spider3.com\"]\n\n    @classmethod\n    def handles_request(cls, request):\n        return request.url == \"http://spider3.com/onlythis\"\n", "tests/test_spiderloader/test_spiders/nested/spider4.py": "from scrapy.spiders import Spider\n\n\nclass Spider4(Spider):\n    name = \"spider4\"\n    allowed_domains = [\"spider4.com\"]\n\n    @classmethod\n    def handles_request(cls, request):\n        return request.url == \"http://spider4.com/onlythis\"\n", "tests/test_spiderloader/test_spiders/nested/__init__.py": "", "tests/test_cmdline_crawl_with_pipeline/__init__.py": "import sys\nimport unittest\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\n\n\nclass CmdlineCrawlPipelineTest(unittest.TestCase):\n    def _execute(self, spname):\n        args = (sys.executable, \"-m\", \"scrapy.cmdline\", \"crawl\", spname)\n        cwd = Path(__file__).resolve().parent\n        proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)\n        proc.communicate()\n        return proc.returncode\n\n    def test_open_spider_normally_in_pipeline(self):\n        self.assertEqual(self._execute(\"normal\"), 0)\n\n    def test_exception_at_open_spider_in_pipeline(self):\n        self.assertEqual(self._execute(\"exception\"), 1)\n", "tests/test_cmdline_crawl_with_pipeline/test_spider/settings.py": "BOT_NAME = \"test_spider\"\nSPIDER_MODULES = [\"test_spider.spiders\"]\n", "tests/test_cmdline_crawl_with_pipeline/test_spider/pipelines.py": "class TestSpiderPipeline:\n    def open_spider(self, spider):\n        pass\n\n    def process_item(self, item, spider):\n        return item\n\n\nclass TestSpiderExceptionPipeline:\n    def open_spider(self, spider):\n        raise Exception(\"exception\")\n\n    def process_item(self, item, spider):\n        return item\n", "tests/test_cmdline_crawl_with_pipeline/test_spider/__init__.py": "", "tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/normal.py": "import scrapy\n\n\nclass NormalSpider(scrapy.Spider):\n    name = \"normal\"\n\n    custom_settings = {\n        \"ITEM_PIPELINES\": {\"test_spider.pipelines.TestSpiderPipeline\": 300}\n    }\n\n    def parse(self, response):\n        pass\n", "tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/exception.py": "import scrapy\n\n\nclass ExceptionSpider(scrapy.Spider):\n    name = \"exception\"\n\n    custom_settings = {\n        \"ITEM_PIPELINES\": {\"test_spider.pipelines.TestSpiderExceptionPipeline\": 300}\n    }\n\n    def parse(self, response):\n        pass\n", "tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/__init__.py": "", "tests/keys/__init__.py": "from datetime import datetime, timedelta, timezone\nfrom pathlib import Path\n\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives.hashes import SHA256\nfrom cryptography.hazmat.primitives.serialization import (\n    Encoding,\n    NoEncryption,\n    PrivateFormat,\n)\nfrom cryptography.x509 import (\n    CertificateBuilder,\n    DNSName,\n    Name,\n    NameAttribute,\n    SubjectAlternativeName,\n    random_serial_number,\n)\nfrom cryptography.x509.oid import NameOID\n\n\n# https://cryptography.io/en/latest/x509/tutorial/#creating-a-self-signed-certificate\ndef generate_keys():\n    folder = Path(__file__).parent\n\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n        backend=default_backend(),\n    )\n    (folder / \"localhost.key\").write_bytes(\n        key.private_bytes(\n            encoding=Encoding.PEM,\n            format=PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=NoEncryption(),\n        ),\n    )\n\n    subject = issuer = Name(\n        [\n            NameAttribute(NameOID.COUNTRY_NAME, \"IE\"),\n            NameAttribute(NameOID.ORGANIZATION_NAME, \"Scrapy\"),\n            NameAttribute(NameOID.COMMON_NAME, \"localhost\"),\n        ]\n    )\n    cert = (\n        CertificateBuilder()\n        .subject_name(subject)\n        .issuer_name(issuer)\n        .public_key(key.public_key())\n        .serial_number(random_serial_number())\n        .not_valid_before(datetime.now(tz=timezone.utc))\n        .not_valid_after(datetime.now(tz=timezone.utc) + timedelta(days=10))\n        .add_extension(\n            SubjectAlternativeName([DNSName(\"localhost\")]),\n            critical=False,\n        )\n        .sign(key, SHA256(), default_backend())\n    )\n    (folder / \"localhost.crt\").write_bytes(cert.public_bytes(Encoding.PEM))\n"}