{"noxfile.py": "# -*- coding: utf-8 -*-\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated by synthtool. DO NOT EDIT!\n\nfrom __future__ import absolute_import\nimport os\nimport pathlib\nimport shutil\n\nimport nox\n\n\nBLACK_VERSION = \"black==23.7.0\"\nBLACK_PATHS = [\"docs\", \"google\", \"tests\", \"noxfile.py\", \"setup.py\"]\n\nDEFAULT_PYTHON_VERSION = \"3.8\"\nSYSTEM_TEST_PYTHON_VERSIONS = [\"3.8\"]\nUNIT_TEST_PYTHON_VERSIONS = [\"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\nCONFORMANCE_TEST_PYTHON_VERSIONS = [\"3.8\"]\n\n_DEFAULT_STORAGE_HOST = \"https://storage.googleapis.com\"\n\nCURRENT_DIRECTORY = pathlib.Path(__file__).parent.absolute()\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef lint(session):\n    \"\"\"Run linters.\n\n    Returns a failure if the linters find linting errors or sufficiently\n    serious code quality issues.\n    \"\"\"\n    # Pin flake8 to 6.0.0\n    # See https://github.com/googleapis/python-storage/issues/1102\n    session.install(\"flake8==6.0.0\", BLACK_VERSION)\n    session.run(\n        \"black\",\n        \"--check\",\n        *BLACK_PATHS,\n    )\n    session.run(\"flake8\", \"google\", \"tests\")\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef blacken(session):\n    \"\"\"Run black.\n\n    Format code to uniform standard.\n    \"\"\"\n    session.install(BLACK_VERSION)\n    session.run(\n        \"black\",\n        *BLACK_PATHS,\n    )\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef lint_setup_py(session):\n    \"\"\"Verify that setup.py is valid (including RST check).\"\"\"\n    session.install(\"docutils\", \"pygments\")\n    session.run(\"python\", \"setup.py\", \"check\", \"--restructuredtext\", \"--strict\")\n\n\ndef default(session):\n    constraints_path = str(\n        CURRENT_DIRECTORY / \"testing\" / f\"constraints-{session.python}.txt\"\n    )\n    # Install all test dependencies, then install this package in-place.\n    session.install(\"mock\", \"pytest\", \"pytest-cov\", \"-c\", constraints_path)\n    session.install(\"-e\", \".\", \"-c\", constraints_path)\n\n    # Run py.test against the unit tests.\n    session.run(\n        \"py.test\",\n        \"--quiet\",\n        f\"--junitxml=unit_{session.python}_sponge_log.xml\",\n        \"--cov=google.cloud.storage\",\n        \"--cov=google.cloud\",\n        \"--cov=tests.unit\",\n        \"--cov-append\",\n        \"--cov-config=.coveragerc\",\n        \"--cov-report=\",\n        \"--cov-fail-under=0\",\n        os.path.join(\"tests\", \"unit\"),\n        *session.posargs,\n    )\n\n\n@nox.session(python=UNIT_TEST_PYTHON_VERSIONS)\ndef unit(session):\n    \"\"\"Run the unit test suite.\"\"\"\n    default(session)\n\n\n@nox.session(python=SYSTEM_TEST_PYTHON_VERSIONS)\ndef system(session):\n    constraints_path = str(\n        CURRENT_DIRECTORY / \"testing\" / f\"constraints-{session.python}.txt\"\n    )\n    \"\"\"Run the system test suite.\"\"\"\n    system_test_path = os.path.join(\"tests\", \"system.py\")\n    system_test_folder_path = os.path.join(\"tests\", \"system\")\n    rerun_count = 0\n\n    # Check the value of `RUN_SYSTEM_TESTS` env var. It defaults to true.\n    if os.environ.get(\"RUN_SYSTEM_TESTS\", \"true\") == \"false\":\n        session.skip(\"RUN_SYSTEM_TESTS is set to false, skipping\")\n    # Environment check: Only run tests if the environment variable is set.\n    if not os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\"):\n        session.skip(\n            \"Credentials must be set via environment variable GOOGLE_APPLICATION_CREDENTIALS\"\n        )\n    # mTLS tests requires pyopenssl.\n    if os.environ.get(\"GOOGLE_API_USE_CLIENT_CERTIFICATE\", \"\") == \"true\":\n        session.install(\"pyopenssl\")\n    # Check if endpoint is being overriden for rerun_count\n    if (\n        os.getenv(\"API_ENDPOINT_OVERRIDE\", \"https://storage.googleapis.com\")\n        != \"https://storage.googleapis.com\"\n    ):\n        rerun_count = 3\n\n    system_test_exists = os.path.exists(system_test_path)\n    system_test_folder_exists = os.path.exists(system_test_folder_path)\n    # Environment check: only run tests if found.\n    if not system_test_exists and not system_test_folder_exists:\n        session.skip(\"System tests were not found\")\n\n    # Use pre-release gRPC for system tests.\n    # TODO: Remove ban of 1.52.0rc1 once grpc/grpc#31885 is resolved.\n    session.install(\"--pre\", \"grpcio!=1.52.0rc1\")\n\n    # Install all test dependencies, then install this package into the\n    # virtualenv's dist-packages.\n    # 2021-05-06: defer installing 'google-cloud-*' to after this package,\n    #             in order to work around Python 2.7 googolapis-common-protos\n    #             issue.\n    session.install(\"mock\", \"pytest\", \"pytest-rerunfailures\", \"-c\", constraints_path)\n    session.install(\"-e\", \".\", \"-c\", constraints_path)\n    session.install(\n        \"google-cloud-testutils\",\n        \"google-cloud-iam\",\n        \"google-cloud-pubsub < 2.0.0\",\n        \"google-cloud-kms < 2.0dev\",\n        \"-c\",\n        constraints_path,\n    )\n\n    # Run py.test against the system tests.\n    if system_test_exists:\n        session.run(\n            \"py.test\",\n            \"--quiet\",\n            f\"--junitxml=system_{session.python}_sponge_log.xml\",\n            \"--reruns={}\".format(rerun_count),\n            system_test_path,\n            *session.posargs,\n        )\n    if system_test_folder_exists:\n        session.run(\n            \"py.test\",\n            \"--quiet\",\n            f\"--junitxml=system_{session.python}_sponge_log.xml\",\n            \"--reruns={}\".format(rerun_count),\n            system_test_folder_path,\n            *session.posargs,\n        )\n\n\n@nox.session(python=CONFORMANCE_TEST_PYTHON_VERSIONS)\ndef conftest_retry(session):\n    \"\"\"Run the retry conformance test suite.\"\"\"\n    conformance_test_folder_path = os.path.join(\"tests\", \"conformance\")\n    conformance_test_folder_exists = os.path.exists(conformance_test_folder_path)\n    # Environment check: only run tests if found.\n    if not conformance_test_folder_exists:\n        session.skip(\"Conformance tests were not found\")\n\n    # Install all test dependencies and pytest plugin to run tests in parallel.\n    # Then install this package in-place.\n    session.install(\"pytest\", \"pytest-xdist\")\n    session.install(\"-e\", \".\")\n\n    # Run #CPU processes in parallel if no test session arguments are passed in.\n    if session.posargs:\n        test_cmd = [\n            \"py.test\",\n            \"--quiet\",\n            conformance_test_folder_path,\n            *session.posargs,\n        ]\n    else:\n        test_cmd = [\"py.test\", \"-n\", \"auto\", \"--quiet\", conformance_test_folder_path]\n\n    # Run py.test against the conformance tests.\n    session.run(*test_cmd)\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef cover(session):\n    \"\"\"Run the final coverage report.\n\n    This outputs the coverage report aggregating coverage from the unit\n    test runs (not system test runs), and then erases coverage data.\n    \"\"\"\n    session.install(\"coverage\", \"pytest-cov\")\n    session.run(\"coverage\", \"report\", \"--show-missing\", \"--fail-under=100\")\n\n    session.run(\"coverage\", \"erase\")\n\n\n@nox.session(python=\"3.9\")\ndef docs(session):\n    \"\"\"Build the docs for this library.\"\"\"\n\n    session.install(\"-e\", \".\")\n    session.install(\n        # We need to pin to specific versions of the `sphinxcontrib-*` packages\n        # which still support sphinx 4.x.\n        # See https://github.com/googleapis/sphinx-docfx-yaml/issues/344\n        # and https://github.com/googleapis/sphinx-docfx-yaml/issues/345.\n        \"sphinxcontrib-applehelp==1.0.4\",\n        \"sphinxcontrib-devhelp==1.0.2\",\n        \"sphinxcontrib-htmlhelp==2.0.1\",\n        \"sphinxcontrib-qthelp==1.0.3\",\n        \"sphinxcontrib-serializinghtml==1.1.5\",\n        \"sphinx==4.5.0\",\n        \"alabaster\",\n        \"recommonmark\",\n    )\n\n    shutil.rmtree(os.path.join(\"docs\", \"_build\"), ignore_errors=True)\n    session.run(\n        \"sphinx-build\",\n        \"-W\",  # warnings as errors\n        \"-T\",  # show full traceback on exception\n        \"-N\",  # no colors\n        \"-b\",\n        \"html\",\n        \"-d\",\n        os.path.join(\"docs\", \"_build\", \"doctrees\", \"\"),\n        os.path.join(\"docs\", \"\"),\n        os.path.join(\"docs\", \"_build\", \"html\", \"\"),\n    )\n\n\n@nox.session(python=\"3.10\")\ndef docfx(session):\n    \"\"\"Build the docfx yaml files for this library.\"\"\"\n\n    session.install(\"-e\", \".\")\n    session.install(\"grpcio\")\n    session.install(\n        # We need to pin to specific versions of the `sphinxcontrib-*` packages\n        # which still support sphinx 4.x.\n        # See https://github.com/googleapis/sphinx-docfx-yaml/issues/344\n        # and https://github.com/googleapis/sphinx-docfx-yaml/issues/345.\n        \"sphinxcontrib-applehelp==1.0.4\",\n        \"sphinxcontrib-devhelp==1.0.2\",\n        \"sphinxcontrib-htmlhelp==2.0.1\",\n        \"sphinxcontrib-qthelp==1.0.3\",\n        \"sphinxcontrib-serializinghtml==1.1.5\",\n        \"gcp-sphinx-docfx-yaml\",\n        \"alabaster\",\n        \"recommonmark\",\n    )\n\n    shutil.rmtree(os.path.join(\"docs\", \"_build\"), ignore_errors=True)\n    session.run(\n        \"sphinx-build\",\n        \"-T\",  # show full traceback on exception\n        \"-N\",  # no colors\n        \"-D\",\n        (\n            \"extensions=sphinx.ext.autodoc,\"\n            \"sphinx.ext.autosummary,\"\n            \"docfx_yaml.extension,\"\n            \"sphinx.ext.intersphinx,\"\n            \"sphinx.ext.coverage,\"\n            \"sphinx.ext.napoleon,\"\n            \"sphinx.ext.todo,\"\n            \"sphinx.ext.viewcode,\"\n            \"recommonmark\"\n        ),\n        \"-b\",\n        \"html\",\n        \"-d\",\n        os.path.join(\"docs\", \"_build\", \"doctrees\", \"\"),\n        os.path.join(\"docs\", \"\"),\n        os.path.join(\"docs\", \"_build\", \"html\", \"\"),\n    )\n", "setup.py": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\n\nimport setuptools\n\n\n# Package metadata.\n\nname = \"google-cloud-storage\"\ndescription = \"Google Cloud Storage API client library\"\n# Should be one of:\n# 'Development Status :: 3 - Alpha'\n# 'Development Status :: 4 - Beta'\n# 'Development Status :: 5 - Production/Stable'\nrelease_status = \"Development Status :: 5 - Production/Stable\"\ndependencies = [\n    \"google-auth >= 2.26.1, < 3.0dev\",\n    \"google-api-core >= 2.15.0, <3.0.0dev\",\n    \"google-cloud-core >= 2.3.0, < 3.0dev\",\n    \"google-resumable-media >= 2.6.0\",\n    \"requests >= 2.18.0, < 3.0.0dev\",\n    \"google-crc32c >= 1.0, < 2.0dev\",\n]\nextras = {\"protobuf\": [\"protobuf<5.0.0dev\"]}\n\n\n# Setup boilerplate below this line.\n\npackage_root = os.path.abspath(os.path.dirname(__file__))\n\nversion = {}\nwith open(os.path.join(package_root, \"google/cloud/storage/version.py\")) as fp:\n    exec(fp.read(), version)\nversion = version[\"__version__\"]\n\nreadme_filename = os.path.join(package_root, \"README.rst\")\nwith io.open(readme_filename, encoding=\"utf-8\") as readme_file:\n    readme = readme_file.read()\n\n# Only include packages under the 'google' namespace. Do not include tests,\n# benchmarks, etc.\npackages = [\n    package\n    for package in setuptools.find_namespace_packages()\n    if package.startswith(\"google\")\n]\n\n\nsetuptools.setup(\n    name=name,\n    version=version,\n    description=description,\n    long_description=readme,\n    author=\"Google LLC\",\n    author_email=\"googleapis-packages@google.com\",\n    license=\"Apache 2.0\",\n    url=\"https://github.com/googleapis/python-storage\",\n    classifiers=[\n        release_status,\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Operating System :: OS Independent\",\n        \"Topic :: Internet\",\n    ],\n    platforms=\"Posix; MacOS X; Windows\",\n    packages=packages,\n    install_requires=dependencies,\n    extras_require=extras,\n    python_requires=\">=3.7\",\n    include_package_data=True,\n    zip_safe=False,\n)\n", "owlbot.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This script is used to synthesize generated parts of this library.\"\"\"\n\nimport synthtool as s\nfrom synthtool import gcp\nfrom synthtool.languages import python\n\ncommon = gcp.CommonTemplates()\n\n# ----------------------------------------------------------------------------\n# Add templated files\n# ----------------------------------------------------------------------------\ntemplated_files = common.py_library(\n    cov_level=100,\n    split_system_tests=True,\n    system_test_external_dependencies=[\n        \"google-cloud-iam\",\n        \"google-cloud-pubsub < 2.0.0\",\n        # See: https://github.com/googleapis/python-storage/issues/226\n        \"google-cloud-kms < 2.0dev\",\n    ],\n    intersphinx_dependencies={\n        # python-requests url temporary change related to\n        # https://github.com/psf/requests/issues/6140#issuecomment-1135071992\n        \"requests\": \"https://requests.readthedocs.io/en/stable/\"\n    },\n)\n\ns.move(\n    templated_files,\n    excludes=[\n        \"docs/multiprocessing.rst\",\n        \"noxfile.py\",\n        \"CONTRIBUTING.rst\",\n        \"README.rst\",\n        \".kokoro/samples/python3.6\", # remove python 3.6 support\n        \".github/workflows\", # exclude gh actions as credentials are needed for tests\n        \".github/release-please.yml\", # special support for a python2 branch in this repo\n    ],\n)\n\ns.replace(\n    \".kokoro/build.sh\",\n    \"export PYTHONUNBUFFERED=1\",\n    \"\"\"export PYTHONUNBUFFERED=1\n\n# Export variable to override api endpoint\nexport API_ENDPOINT_OVERRIDE\n\n# Export variable to override api endpoint version\nexport API_VERSION_OVERRIDE\n\n# Export dual region locations\nexport DUAL_REGION_LOC_1\nexport DUAL_REGION_LOC_2\"\"\")\n\ns.replace(\n    \".coveragerc\",\n    \"omit =\",\n    \"\"\"omit =\n  .nox/*\"\"\")\n\npython.py_samples(skip_readmes=True)\n\ns.shell.run([\"nox\", \"-s\", \"blacken\"], hide_output=False)\n", "pylint.config.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This module is used to configure gcp-devrel-py-tools run-pylint.\"\"\"\n\n# Library configuration\n\n# library_additions = {}\n# library_replacements = {}\n\n# Test configuration\n\n# test_additions = copy.deepcopy(library_additions)\n# test_replacements = copy.deepcopy(library_replacements)\n", "samples/snippets/storage_create_bucket_hierarchical_namespace.py": "#!/usr/bin/env python\n\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket_hierarchical_namespace]\nfrom google.cloud import storage\n\n\ndef create_bucket_hierarchical_namespace(bucket_name):\n    \"\"\"Creates a bucket with hierarchical namespace enabled.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.hierarchical_namespace_enabled = True\n    bucket.create()\n\n    print(f\"Created bucket {bucket_name} with hierarchical namespace enabled.\")\n\n\n# [END storage_create_bucket_hierarchical_namespace]\n\n\nif __name__ == \"__main__\":\n    create_bucket_hierarchical_namespace(bucket_name=sys.argv[1])\n", "samples/snippets/storage_make_public.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_make_public]\nfrom google.cloud import storage\n\n\ndef make_blob_public(bucket_name, blob_name):\n    \"\"\"Makes a blob publicly accessible.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    blob.make_public()\n\n    print(\n        f\"Blob {blob.name} is publicly accessible at {blob.public_url}\"\n    )\n\n\n# [END storage_make_public]\n\nif __name__ == \"__main__\":\n    make_blob_public(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_remove_bucket_iam_member.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_iam_member]\nfrom google.cloud import storage\n\n\ndef remove_bucket_iam_member(bucket_name, role, member):\n    \"\"\"Remove member from bucket IAM Policy\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g. roles/storage.objectViewer\"\n    # member = \"IAM identity, e.g. user: name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    for binding in policy.bindings:\n        print(binding)\n        if binding[\"role\"] == role and binding.get(\"condition\") is None:\n            binding[\"members\"].discard(member)\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Removed {member} with role {role} from {bucket_name}.\")\n\n\n# [END storage_remove_bucket_iam_member]\n\nif __name__ == \"__main__\":\n    remove_bucket_iam_member(\n        bucket_name=sys.argv[1], role=sys.argv[2], member=sys.argv[3]\n    )\n", "samples/snippets/storage_remove_bucket_default_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_default_owner]\nfrom google.cloud import storage\n\n\ndef remove_bucket_default_owner(bucket_name, user_email):\n    \"\"\"Removes a user from the access control list of the given bucket's\n    default object access control list.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # remove access for different types of entities.\n    bucket.default_object_acl.user(user_email).revoke_read()\n    bucket.default_object_acl.user(user_email).revoke_write()\n    bucket.default_object_acl.user(user_email).revoke_owner()\n    bucket.default_object_acl.save()\n\n    print(\n        f\"Removed user {user_email} from the default acl of bucket {bucket_name}.\"\n    )\n\n\n# [END storage_remove_bucket_default_owner]\n\nif __name__ == \"__main__\":\n    remove_bucket_default_owner(\n        bucket_name=sys.argv[1], user_email=sys.argv[2]\n    )\n", "samples/snippets/storage_disable_bucket_lifecycle_management.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_bucket_lifecycle_management]\nfrom google.cloud import storage\n\n\ndef disable_bucket_lifecycle_management(bucket_name):\n    \"\"\"Disable lifecycle management for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.clear_lifecyle_rules()\n    bucket.patch()\n    rules = bucket.lifecycle_rules\n\n    print(f\"Lifecycle management is disable for bucket {bucket_name} and the rules are {list(rules)}\")\n    return bucket\n\n\n# [END storage_disable_bucket_lifecycle_management]\n\nif __name__ == \"__main__\":\n    disable_bucket_lifecycle_management(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_service_account.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_get_service_account]\nfrom google.cloud import storage\n\n\ndef get_service_account():\n    \"\"\"Get the service account email\"\"\"\n    storage_client = storage.Client()\n\n    email = storage_client.get_service_account_email()\n    print(\n        f\"The GCS service account for project {storage_client.project} is: {email} \"\n    )\n\n\n# [END storage_get_service_account]\n\nif __name__ == \"__main__\":\n    get_service_account()\n", "samples/snippets/storage_list_files.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_files]\nfrom google.cloud import storage\n\n\ndef list_blobs(bucket_name):\n    \"\"\"Lists all the blobs in the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    # Note: Client.list_blobs requires at least package version 1.17.0.\n    blobs = storage_client.list_blobs(bucket_name)\n\n    # Note: The call returns a response only when the iterator is consumed.\n    for blob in blobs:\n        print(blob.name)\n\n\n# [END storage_list_files]\n\n\nif __name__ == \"__main__\":\n    list_blobs(bucket_name=sys.argv[1])\n", "samples/snippets/storage_transfer_manager_upload_many.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_upload_many]\ndef upload_many_blobs_with_transfer_manager(\n    bucket_name, filenames, source_directory=\"\", workers=8\n):\n    \"\"\"Upload every file in a list to a bucket, concurrently in a process pool.\n\n    Each blob name is derived from the filename, not including the\n    `source_directory` parameter. For complete control of the blob name for each\n    file (and other aspects of individual blob metadata), use\n    transfer_manager.upload_many() instead.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # A list (or other iterable) of filenames to upload.\n    # filenames = [\"file_1.txt\", \"file_2.txt\"]\n\n    # The directory on your computer that is the root of all of the files in the\n    # list of filenames. This string is prepended (with os.path.join()) to each\n    # filename to get the full path to the file. Relative paths and absolute\n    # paths are both accepted. This string is not included in the name of the\n    # uploaded blob; it is only used to find the source files. An empty string\n    # means \"the current working directory\". Note that this parameter allows\n    # directory traversal (e.g. \"/\", \"../\") and is not intended for unsanitized\n    # end user input.\n    # source_directory=\"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    results = transfer_manager.upload_many_from_filenames(\n        bucket, filenames, source_directory=source_directory, max_workers=workers\n    )\n\n    for name, result in zip(filenames, results):\n        # The results list is either `None` or an exception for each filename in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Uploaded {} to {}.\".format(name, bucket.name))\n# [END storage_transfer_manager_upload_many]\n", "samples/snippets/storage_delete_bucket_notification.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that deletes a notification configuration for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_delete_bucket_notification]\nfrom google.cloud import storage\n\n\ndef delete_bucket_notification(bucket_name, notification_id):\n    \"\"\"Deletes a notification configuration for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of the notification\n    # notification_id = \"your-notification-id\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notification = bucket.notification(notification_id=notification_id)\n    notification.delete()\n\n    print(f\"Successfully deleted notification with ID {notification_id} for bucket {bucket_name}\")\n\n# [END storage_delete_bucket_notification]\n\n\nif __name__ == \"__main__\":\n    delete_bucket_notification(bucket_name=sys.argv[1], notification_id=sys.argv[2])\n", "samples/snippets/storage_generate_signed_url_v2.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_generate_signed_url_v2]\nimport datetime\n# [END storage_generate_signed_url_v2]\nimport sys\n# [START storage_generate_signed_url_v2]\n\nfrom google.cloud import storage\n\n\ndef generate_signed_url(bucket_name, blob_name):\n    \"\"\"Generates a v2 signed URL for downloading a blob.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        # This URL is valid for 1 hour\n        expiration=datetime.timedelta(hours=1),\n        # Allow GET requests using this URL.\n        method=\"GET\",\n    )\n\n    print(f\"The signed url for {blob.name} is {url}\")\n    return url\n\n\n# [END storage_generate_signed_url_v2]\n\nif __name__ == \"__main__\":\n    generate_signed_url(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_add_bucket_default_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_default_owner]\nfrom google.cloud import storage\n\n\ndef add_bucket_default_owner(bucket_name, user_email):\n    \"\"\"Adds a user as an owner in the given bucket's default object access\n    control list.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # grant access to different types of entities. You can also use\n    # `grant_read` or `grant_write` to grant different roles.\n    bucket.default_object_acl.user(user_email).grant_owner()\n    bucket.default_object_acl.save()\n\n    print(\n        \"Added user {} as an owner in the default acl on bucket {}.\".format(\n            user_email, bucket_name\n        )\n    )\n\n\n# [END storage_add_bucket_default_owner]\n\nif __name__ == \"__main__\":\n    add_bucket_default_owner(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_get_public_access_prevention.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_public_access_prevention]\nfrom google.cloud import storage\n\n\ndef get_public_access_prevention(bucket_name):\n    \"\"\"Gets the public access prevention setting (either 'inherited' or 'enforced') for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    iam_configuration = bucket.iam_configuration\n\n    print(\n        f\"Public access prevention is {iam_configuration.public_access_prevention} for {bucket.name}.\"\n    )\n\n\n# [END storage_get_public_access_prevention]\n\nif __name__ == \"__main__\":\n    get_public_access_prevention(bucket_name=sys.argv[1])\n", "samples/snippets/storage_generate_signed_url_v4.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_generate_signed_url_v4]\nimport datetime\n# [END storage_generate_signed_url_v4]\nimport sys\n# [START storage_generate_signed_url_v4]\n\nfrom google.cloud import storage\n\n\ndef generate_download_signed_url_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 signed URL for downloading a blob.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        version=\"v4\",\n        # This URL is valid for 15 minutes\n        expiration=datetime.timedelta(minutes=15),\n        # Allow GET requests using this URL.\n        method=\"GET\",\n    )\n\n    print(\"Generated GET signed URL:\")\n    print(url)\n    print(\"You can use this URL with any user agent, for example:\")\n    print(f\"curl '{url}'\")\n    return url\n\n\n# [END storage_generate_signed_url_v4]\n\nif __name__ == \"__main__\":\n    generate_download_signed_url_v4(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_print_file_acl_for_user.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_file_acl_for_user]\nfrom google.cloud import storage\n\n\ndef print_blob_acl_for_user(bucket_name, blob_name, user_email):\n    \"\"\"Prints out a blob's access control list for a given user.\"\"\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    blob.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # get the roles for different types of entities.\n    roles = blob.acl.user(user_email).get_roles()\n\n    print(roles)\n\n\n# [END storage_print_file_acl_for_user]\n\nif __name__ == \"__main__\":\n    print_blob_acl_for_user(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2], user_email=sys.argv[3],\n    )\n", "samples/snippets/storage_release_temporary_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_release_temporary_hold]\nfrom google.cloud import storage\n\n\ndef release_temporary_hold(bucket_name, blob_name):\n    \"\"\"Releases the temporary hold on a given blob\"\"\"\n\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.temporary_hold = False\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(\"Temporary hold was release for #{blob_name}\")\n\n\n# [END storage_release_temporary_hold]\n\n\nif __name__ == \"__main__\":\n    release_temporary_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_print_bucket_acl.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_bucket_acl]\nfrom google.cloud import storage\n\n\ndef print_bucket_acl(bucket_name):\n    \"\"\"Prints out a bucket's access control list.\"\"\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    for entry in bucket.acl:\n        print(f\"{entry['role']}: {entry['entity']}\")\n\n\n# [END storage_print_bucket_acl]\n\nif __name__ == \"__main__\":\n    print_bucket_acl(bucket_name=sys.argv[1])\n", "samples/snippets/storage_bucket_delete_default_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_bucket_delete_default_kms_key]\nfrom google.cloud import storage\n\n\ndef bucket_delete_default_kms_key(bucket_name):\n    \"\"\"Delete a default KMS key of bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.default_kms_key_name = None\n    bucket.patch()\n\n    print(f\"Default KMS key was removed from {bucket.name}\")\n    return bucket\n\n\n# [END storage_bucket_delete_default_kms_key]\n\nif __name__ == \"__main__\":\n    bucket_delete_default_kms_key(bucket_name=sys.argv[1])\n", "samples/snippets/storage_compose_file.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_compose_file]\nfrom google.cloud import storage\n\n\ndef compose_file(bucket_name, first_blob_name, second_blob_name, destination_blob_name):\n    \"\"\"Concatenate source blobs into destination blob.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # first_blob_name = \"first-object-name\"\n    # second_blob_name = \"second-blob-name\"\n    # destination_blob_name = \"destination-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    destination = bucket.blob(destination_blob_name)\n    destination.content_type = \"text/plain\"\n\n    # Note sources is a list of Blob instances, up to the max of 32 instances per request\n    sources = [bucket.blob(first_blob_name), bucket.blob(second_blob_name)]\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to compose is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n    destination_generation_match_precondition = 0\n\n    destination.compose(sources, if_generation_match=destination_generation_match_precondition)\n\n    print(\n        \"New composite object {} in the bucket {} was created by combining {} and {}\".format(\n            destination_blob_name, bucket_name, first_blob_name, second_blob_name\n        )\n    )\n    return destination\n\n\n# [END storage_compose_file]\n\nif __name__ == \"__main__\":\n    compose_file(\n        bucket_name=sys.argv[1],\n        first_blob_name=sys.argv[2],\n        second_blob_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n    )\n", "samples/snippets/storage_remove_cors_configuration.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_cors_configuration]\nfrom google.cloud import storage\n\n\ndef remove_cors_configuration(bucket_name):\n    \"\"\"Remove a bucket's CORS policies configuration.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.cors = []\n    bucket.patch()\n\n    print(f\"Remove CORS policies for bucket {bucket.name}.\")\n    return bucket\n\n\n# [END storage_remove_cors_configuration]\n\nif __name__ == \"__main__\":\n    remove_cors_configuration(bucket_name=sys.argv[1])\n", "samples/snippets/storage_add_bucket_iam_member.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_iam_member]\nfrom google.cloud import storage\n\n\ndef add_bucket_iam_member(bucket_name, role, member):\n    \"\"\"Add a new member to an IAM Policy\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g., roles/storage.objectViewer\"\n    # member = \"IAM identity, e.g., user: name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    policy.bindings.append({\"role\": role, \"members\": {member}})\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Added {member} with role {role} to {bucket_name}.\")\n\n\n# [END storage_add_bucket_iam_member]\n\n\nif __name__ == \"__main__\":\n    add_bucket_iam_member(bucket_name=sys.argv[1], role=sys.argv[2], member=sys.argv[3])\n", "samples/snippets/storage_transfer_manager_download_chunks_concurrently.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_download_chunks_concurrently]\ndef download_chunks_concurrently(\n    bucket_name, blob_name, filename, chunk_size=32 * 1024 * 1024, workers=8\n):\n    \"\"\"Download a single file in chunks, concurrently in a process pool.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The file to be downloaded\n    # blob_name = \"target-file\"\n\n    # The destination filename or path\n    # filename = \"\"\n\n    # The size of each chunk. The performance impact of this value depends on\n    # the use case. The remote service has a minimum of 5 MiB and a maximum of\n    # 5 GiB.\n    # chunk_size = 32 * 1024 * 1024 (32 MiB)\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    transfer_manager.download_chunks_concurrently(\n        blob, filename, chunk_size=chunk_size, max_workers=workers\n    )\n\n    print(\"Downloaded {} to {}.\".format(blob_name, filename))\n\n\n# [END storage_transfer_manager_download_chunks_concurrently]\n", "samples/snippets/storage_set_bucket_public_iam.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_bucket_public_iam]\nfrom typing import List\n\nfrom google.cloud import storage\n\n\ndef set_bucket_public_iam(\n    bucket_name: str = \"your-bucket-name\",\n    members: List[str] = [\"allUsers\"],\n):\n    \"\"\"Set a public IAM Policy to bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    policy.bindings.append(\n        {\"role\": \"roles/storage.objectViewer\", \"members\": members}\n    )\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Bucket {bucket.name} is now publicly readable\")\n\n\n# [END storage_set_bucket_public_iam]\n\nif __name__ == \"__main__\":\n    set_bucket_public_iam(\n        bucket_name=sys.argv[1],\n    )\n", "samples/snippets/storage_list_buckets.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_list_buckets]\nfrom google.cloud import storage\n\n\ndef list_buckets():\n    \"\"\"Lists all buckets.\"\"\"\n\n    storage_client = storage.Client()\n    buckets = storage_client.list_buckets()\n\n    for bucket in buckets:\n        print(bucket.name)\n\n\n# [END storage_list_buckets]\n\n\nif __name__ == \"__main__\":\n    list_buckets()\n", "samples/snippets/storage_upload_from_stream.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_stream_file_upload]\nfrom google.cloud import storage\n\n\ndef upload_blob_from_stream(bucket_name, file_obj, destination_blob_name):\n    \"\"\"Uploads bytes from a stream or other file-like object to a blob.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The stream or file (file-like object) from which to read\n    # import io\n    # file_obj = io.BytesIO()\n    # file_obj.write(b\"This is test data.\")\n\n    # The desired name of the uploaded GCS object (blob)\n    # destination_blob_name = \"storage-object-name\"\n\n    # Construct a client-side representation of the blob.\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    # Rewind the stream to the beginning. This step can be omitted if the input\n    # stream will always be at a correct position.\n    file_obj.seek(0)\n\n    # Upload data from the stream to your bucket.\n    blob.upload_from_file(file_obj)\n\n    print(\n        f\"Stream data uploaded to {destination_blob_name} in bucket {bucket_name}.\"\n    )\n\n# [END storage_stream_file_upload]\n", "samples/snippets/storage_download_into_memory.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_file_download_into_memory]\nfrom google.cloud import storage\n\n\ndef download_blob_into_memory(bucket_name, blob_name):\n    \"\"\"Downloads a blob into memory.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object\n    # blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n    # any content from Google Cloud Storage. As we don't need additional data,\n    # using `Bucket.blob` is preferred here.\n    blob = bucket.blob(blob_name)\n    contents = blob.download_as_bytes()\n\n    print(\n        \"Downloaded storage object {} from bucket {} as the following bytes object: {}.\".format(\n            blob_name, bucket_name, contents.decode(\"utf-8\")\n        )\n    )\n\n\n# [END storage_file_download_into_memory]\n\nif __name__ == \"__main__\":\n    download_blob_into_memory(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n    )\n", "samples/snippets/storage_get_requester_pays_status.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_requester_pays_status]\nfrom google.cloud import storage\n\n\ndef get_requester_pays_status(bucket_name):\n    \"\"\"Get a bucket's requester pays metadata\"\"\"\n    # bucket_name = \"my-bucket\"\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    requester_pays_status = bucket.requester_pays\n\n    if requester_pays_status:\n        print(f\"Requester Pays is enabled for {bucket_name}\")\n    else:\n        print(f\"Requester Pays is disabled for {bucket_name}\")\n\n\n# [END storage_get_requester_pays_status]\n\nif __name__ == \"__main__\":\n    get_requester_pays_status(bucket_name=sys.argv[1])\n", "samples/snippets/storage_delete_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_file]\nfrom google.cloud import storage\n\n\ndef delete_blob(bucket_name, blob_name):\n    \"\"\"Deletes a blob from the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    generation_match_precondition = None\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to delete is aborted if the object's\n    # generation number does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = blob.generation\n\n    blob.delete(if_generation_match=generation_match_precondition)\n\n    print(f\"Blob {blob_name} deleted.\")\n\n\n# [END storage_delete_file]\n\nif __name__ == \"__main__\":\n    delete_blob(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_create_bucket_dual_region.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"\nSample that creates a dual region bucket.\n\"\"\"\n\n# [START storage_create_bucket_dual_region]\nfrom google.cloud import storage\n\n\ndef create_bucket_dual_region(bucket_name, location, region_1, region_2):\n    \"\"\"Creates a Dual-Region Bucket with provided location and regions..\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The bucket's pair of regions. Case-insensitive.\n    # See this documentation for other valid locations:\n    # https://cloud.google.com/storage/docs/locations\n    # region_1 = \"US-EAST1\"\n    # region_2 = \"US-WEST1\"\n    # location = \"US\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.create_bucket(bucket_name, location=location, data_locations=[region_1, region_2])\n\n    print(f\"Created bucket {bucket_name}\")\n    print(f\" - location: {bucket.location}\")\n    print(f\" - location_type: {bucket.location_type}\")\n    print(f\" - customPlacementConfig data_locations: {bucket.data_locations}\")\n\n\n# [END storage_create_bucket_dual_region]\n\n\nif __name__ == \"__main__\":\n    create_bucket_dual_region(\n        bucket_name=sys.argv[1], location=sys.argv[2], region_1=sys.argv[3], region_2=sys.argv[4]\n    )\n", "samples/snippets/storage_define_bucket_website_configuration.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_define_bucket_website_configuration]\nfrom google.cloud import storage\n\n\ndef define_bucket_website_configuration(bucket_name, main_page_suffix, not_found_page):\n    \"\"\"Configure website-related properties of bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # main_page_suffix = \"index.html\"\n    # not_found_page = \"404.html\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.configure_website(main_page_suffix, not_found_page)\n    bucket.patch()\n\n    print(\n        \"Static website bucket {} is set up to use {} as the index page and {} as the 404 page\".format(\n            bucket.name, main_page_suffix, not_found_page\n        )\n    )\n    return bucket\n\n\n# [END storage_define_bucket_website_configuration]\n\nif __name__ == \"__main__\":\n    define_bucket_website_configuration(\n        bucket_name=sys.argv[1],\n        main_page_suffix=sys.argv[2],\n        not_found_page=sys.argv[3],\n    )\n", "samples/snippets/quickstart.py": "#!/usr/bin/env python\n\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef run_quickstart():\n    # [START storage_quickstart]\n    # Imports the Google Cloud client library\n    from google.cloud import storage\n\n    # Instantiates a client\n    storage_client = storage.Client()\n\n    # The name for the new bucket\n    bucket_name = \"my-new-bucket\"\n\n    # Creates the new bucket\n    bucket = storage_client.create_bucket(bucket_name)\n\n    print(f\"Bucket {bucket.name} created.\")\n    # [END storage_quickstart]\n\n\nif __name__ == \"__main__\":\n    run_quickstart()\n", "samples/snippets/storage_get_rpo.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that gets RPO (Recovery Point Objective) of a bucket\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_get_rpo]\n\nfrom google.cloud import storage\n\n\ndef get_rpo(bucket_name):\n    \"\"\"Gets the RPO of the bucket\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    rpo = bucket.rpo\n\n    print(f\"RPO for {bucket.name} is {rpo}.\")\n\n\n# [END storage_get_rpo]\n\nif __name__ == \"__main__\":\n    get_rpo(bucket_name=sys.argv[1])\n", "samples/snippets/storage_list_file_archived_generations.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_file_archived_generations]\nfrom google.cloud import storage\n\n\ndef list_file_archived_generations(bucket_name):\n    \"\"\"Lists all the blobs in the bucket with generation.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    blobs = storage_client.list_blobs(bucket_name, versions=True)\n\n    for blob in blobs:\n        print(f\"{blob.name},{blob.generation}\")\n\n\n# [END storage_list_file_archived_generations]\n\n\nif __name__ == \"__main__\":\n    list_file_archived_generations(bucket_name=sys.argv[1])\n", "samples/snippets/storage_add_bucket_conditional_iam_binding.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_conditional_iam_binding]\nfrom google.cloud import storage\n\n\ndef add_bucket_conditional_iam_binding(\n    bucket_name, role, title, description, expression, members\n):\n    \"\"\"Add a conditional IAM binding to a bucket's IAM policy.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g. roles/storage.objectViewer\"\n    # members = {\"IAM identity, e.g. user: name@example.com}\"\n    # title = \"Condition title.\"\n    # description = \"Condition description.\"\n    # expression = \"Condition expression.\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    # Set the policy's version to 3 to use condition in bindings.\n    policy.version = 3\n\n    policy.bindings.append(\n        {\n            \"role\": role,\n            \"members\": members,\n            \"condition\": {\n                \"title\": title,\n                \"description\": description,\n                \"expression\": expression,\n            },\n        }\n    )\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Added the following member(s) with role {role} to {bucket_name}:\")\n\n    for member in members:\n        print(f\"    {member}\")\n\n    print(\"with condition:\")\n    print(f\"    Title: {title}\")\n    print(f\"    Description: {description}\")\n    print(f\"    Expression: {expression}\")\n\n\n# [END storage_add_bucket_conditional_iam_binding]\n\n\nif __name__ == \"__main__\":\n    add_bucket_conditional_iam_binding(\n        bucket_name=sys.argv[1],\n        role=sys.argv[2],\n        title=sys.argv[3],\n        description=sys.argv[4],\n        expression=sys.argv[5],\n        members=set(sys.argv[6::]),\n    )\n", "samples/snippets/storage_get_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_retention_policy]\nfrom google.cloud import storage\n\n\ndef get_retention_policy(bucket_name):\n    \"\"\"Gets the retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket.reload()\n\n    print(f\"Retention Policy for {bucket_name}\")\n    print(f\"Retention Period: {bucket.retention_period}\")\n    if bucket.retention_policy_locked:\n        print(\"Retention Policy is locked\")\n\n    if bucket.retention_policy_effective_time:\n        print(\n            f\"Effective Time: {bucket.retention_policy_effective_time}\"\n        )\n\n\n# [END storage_get_retention_policy]\n\n\nif __name__ == \"__main__\":\n    get_retention_policy(bucket_name=sys.argv[1])\n", "samples/snippets/storage_remove_file_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_file_owner]\nfrom google.cloud import storage\n\n\ndef remove_blob_owner(bucket_name, blob_name, user_email):\n    \"\"\"Removes a user from the access control list of the given blob in the\n    given bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # remove access for different types of entities.\n    blob.acl.user(user_email).revoke_read()\n    blob.acl.user(user_email).revoke_write()\n    blob.acl.user(user_email).revoke_owner()\n    blob.acl.save()\n\n    print(\n        f\"Removed user {user_email} from blob {blob_name} in bucket {bucket_name}.\"\n    )\n\n\n# [END storage_remove_file_owner]\n\nif __name__ == \"__main__\":\n    remove_blob_owner(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2], user_email=sys.argv[3],\n    )\n", "samples/snippets/storage_disable_default_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_default_event_based_hold]\nfrom google.cloud import storage\n\n\ndef disable_default_event_based_hold(bucket_name):\n    \"\"\"Disables the default event based hold on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.default_event_based_hold = False\n    bucket.patch()\n\n    print(f\"Default event based hold was disabled for {bucket_name}\")\n\n\n# [END storage_disable_default_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    disable_default_event_based_hold(bucket_name=sys.argv[1])\n", "samples/snippets/storage_change_default_storage_class.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_change_default_storage_class]\nfrom google.cloud import storage\nfrom google.cloud.storage import constants\n\n\ndef change_default_storage_class(bucket_name):\n    \"\"\"Change the default storage class of the bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.storage_class = constants.COLDLINE_STORAGE_CLASS\n    bucket.patch()\n\n    print(f\"Default storage class for bucket {bucket_name} has been set to {bucket.storage_class}\")\n    return bucket\n\n\n# [END storage_change_default_storage_class]\n\nif __name__ == \"__main__\":\n    change_default_storage_class(bucket_name=sys.argv[1])\n", "samples/snippets/storage_cors_configuration.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_cors_configuration]\nfrom google.cloud import storage\n\n\ndef cors_configuration(bucket_name):\n    \"\"\"Set a bucket's CORS policies configuration.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.cors = [\n        {\n            \"origin\": [\"*\"],\n            \"responseHeader\": [\n                \"Content-Type\",\n                \"x-goog-resumable\"],\n            \"method\": ['PUT', 'POST'],\n            \"maxAgeSeconds\": 3600\n        }\n    ]\n    bucket.patch()\n\n    print(f\"Set CORS policies for bucket {bucket.name} is {bucket.cors}\")\n    return bucket\n\n\n# [END storage_cors_configuration]\n\nif __name__ == \"__main__\":\n    cors_configuration(bucket_name=sys.argv[1])\n", "samples/snippets/storage_copy_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_copy_file]\nfrom google.cloud import storage\n\n\ndef copy_blob(\n    bucket_name, blob_name, destination_bucket_name, destination_blob_name,\n):\n    \"\"\"Copies a blob from one bucket to another with a new name.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # destination_bucket_name = \"destination-bucket-name\"\n    # destination_blob_name = \"destination-object-name\"\n\n    storage_client = storage.Client()\n\n    source_bucket = storage_client.bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.bucket(destination_bucket_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to copy is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n    destination_generation_match_precondition = 0\n\n    blob_copy = source_bucket.copy_blob(\n        source_blob, destination_bucket, destination_blob_name, if_generation_match=destination_generation_match_precondition,\n    )\n\n    print(\n        \"Blob {} in bucket {} copied to blob {} in bucket {}.\".format(\n            source_blob.name,\n            source_bucket.name,\n            blob_copy.name,\n            destination_bucket.name,\n        )\n    )\n\n\n# [END storage_copy_file]\n\nif __name__ == \"__main__\":\n    copy_blob(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        destination_bucket_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n    )\n", "samples/snippets/storage_async_upload.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport sys\n\n\n\"\"\"Sample that asynchronously uploads a file to GCS\n\"\"\"\n\n\n# [START storage_async_upload]\n# This sample can be run by calling `async.run(async_upload_blob('bucket_name'))`\nasync def async_upload_blob(bucket_name):\n    \"\"\"Uploads a number of files in parallel to the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    import asyncio\n    from functools import partial\n    from google.cloud import storage\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    loop = asyncio.get_running_loop()\n\n    tasks = []\n    count = 3\n    for x in range(count):\n        blob_name = f\"async_sample_blob_{x}\"\n        content = f\"Hello world #{x}\"\n        blob = bucket.blob(blob_name)\n        # The first arg, None, tells it to use the default loops executor\n        tasks.append(loop.run_in_executor(None, partial(blob.upload_from_string, content)))\n\n    # If the method returns a value (such as download_as_string), gather will return the values\n    await asyncio.gather(*tasks)\n\n    print(f\"Uploaded {count} files to bucket {bucket_name}\")\n\n\n# [END storage_async_upload]\n\n\nif __name__ == \"__main__\":\n    asyncio.run(async_upload_blob(\n        bucket_name=sys.argv[1]\n    ))\n", "samples/snippets/storage_print_bucket_acl_for_user.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_bucket_acl_for_user]\nfrom google.cloud import storage\n\n\ndef print_bucket_acl_for_user(bucket_name, user_email):\n    \"\"\"Prints out a bucket's access control list for a given user.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # get the roles for different types of entities.\n    roles = bucket.acl.user(user_email).get_roles()\n\n    print(roles)\n\n\n# [END storage_print_bucket_acl_for_user]\n\nif __name__ == \"__main__\":\n    print_bucket_acl_for_user(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_print_file_acl.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_file_acl]\nfrom google.cloud import storage\n\n\ndef print_blob_acl(bucket_name, blob_name):\n    \"\"\"Prints out a blob's access control list.\"\"\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    for entry in blob.acl:\n        print(f\"{entry['role']}: {entry['entity']}\")\n\n\n# [END storage_print_file_acl]\n\nif __name__ == \"__main__\":\n    print_blob_acl(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_batch_request.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that uses a batch request.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/batch\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_batch_request]\n\nfrom google.cloud import storage\n\n\ndef batch_request(bucket_name, prefix=None):\n    \"\"\"\n    Use a batch request to patch a list of objects with the given prefix in a bucket.\n\n    Note that Cloud Storage does not support batch operations for uploading or downloading.\n    Additionally, the current batch design does not support library methods whose return values\n    depend on the response payload.\n    See https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.batch\n    \"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n    # The prefix of the object paths\n    # prefix = \"directory-prefix/\"\n\n    client = storage.Client()\n    bucket = client.bucket(bucket_name)\n\n    # Accumulate in a list the objects with a given prefix.\n    blobs_to_patch = [blob for blob in bucket.list_blobs(prefix=prefix)]\n\n    # Use a batch context manager to edit metadata in the list of blobs.\n    # The batch request is sent out when the context manager closes.\n    # No more than 100 calls should be included in a single batch request.\n    with client.batch():\n        for blob in blobs_to_patch:\n            metadata = {\"your-metadata-key\": \"your-metadata-value\"}\n            blob.metadata = metadata\n            blob.patch()\n\n    print(\n        f\"Batch request edited metadata for all objects with the given prefix in {bucket.name}.\"\n    )\n\n\n# [END storage_batch_request]\n\nif __name__ == \"__main__\":\n    batch_request(bucket_name=sys.argv[1], prefix=sys.argv[2])\n", "samples/snippets/storage_set_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_event_based_hold]\nfrom google.cloud import storage\n\n\ndef set_event_based_hold(bucket_name, blob_name):\n    \"\"\"Sets a event based hold on a given blob\"\"\"\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.event_based_hold = True\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(f\"Event based hold was set for {blob_name}\")\n\n\n# [END storage_set_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    set_event_based_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_enable_default_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_default_event_based_hold]\nfrom google.cloud import storage\n\n\ndef enable_default_event_based_hold(bucket_name):\n    \"\"\"Enables the default event based hold on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    bucket.default_event_based_hold = True\n    bucket.patch()\n\n    print(f\"Default event based hold was enabled for {bucket_name}\")\n\n\n# [END storage_enable_default_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    enable_default_event_based_hold(bucket_name=sys.argv[1])\n", "samples/snippets/storage_list_bucket_notifications.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that lists notification configurations for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_list_bucket_notifications]\nfrom google.cloud import storage\n\n\ndef list_bucket_notifications(bucket_name):\n    \"\"\"Lists notification configurations for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notifications = bucket.list_notifications()\n\n    for notification in notifications:\n        print(f\"Notification ID: {notification.notification_id}\")\n\n# [END storage_list_bucket_notifications]\n\n\nif __name__ == \"__main__\":\n    list_bucket_notifications(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_uniform_bucket_level_access.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_uniform_bucket_level_access]\nfrom google.cloud import storage\n\n\ndef get_uniform_bucket_level_access(bucket_name):\n    \"\"\"Get uniform bucket-level access for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    iam_configuration = bucket.iam_configuration\n\n    if iam_configuration.uniform_bucket_level_access_enabled:\n        print(\n            f\"Uniform bucket-level access is enabled for {bucket.name}.\"\n        )\n        print(\n            \"Bucket will be locked on {}.\".format(\n                iam_configuration.uniform_bucket_level_locked_time\n            )\n        )\n    else:\n        print(\n            f\"Uniform bucket-level access is disabled for {bucket.name}.\"\n        )\n\n\n# [END storage_get_uniform_bucket_level_access]\n\nif __name__ == \"__main__\":\n    get_uniform_bucket_level_access(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_with_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_upload_with_kms_key]\nfrom google.cloud import storage\n\n\ndef upload_blob_with_kms(\n    bucket_name, source_file_name, destination_blob_name, kms_key_name,\n):\n    \"\"\"Uploads a file to the bucket, encrypting it with the given KMS key.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_file_name = \"local/path/to/file\"\n    # destination_blob_name = \"storage-object-name\"\n    # kms_key_name = \"projects/PROJ/locations/LOC/keyRings/RING/cryptoKey/KEY\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name, kms_key_name=kms_key_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to upload is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    generation_match_precondition = 0\n\n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\n    print(\n        \"File {} uploaded to {} with encryption key {}.\".format(\n            source_file_name, destination_blob_name, kms_key_name\n        )\n    )\n\n\n# [END storage_upload_with_kms_key]\n\nif __name__ == \"__main__\":\n    upload_blob_with_kms(\n        bucket_name=sys.argv[1],\n        source_file_name=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n        kms_key_name=sys.argv[4],\n    )\n", "samples/snippets/storage_rename_file.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_rename_file]\nfrom google.cloud import storage\n\n\ndef rename_blob(bucket_name, blob_name, new_name):\n    \"\"\"Renames a blob.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of the GCS object to rename\n    # blob_name = \"your-object-name\"\n    # The new ID of the GCS object\n    # new_name = \"new-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    new_blob = bucket.rename_blob(blob, new_name)\n\n    print(f\"Blob {blob.name} has been renamed to {new_blob.name}\")\n\n\n# [END storage_rename_file]\n\nif __name__ == \"__main__\":\n    rename_blob(bucket_name=sys.argv[1], blob_name=sys.argv[2], new_name=sys.argv[3])\n", "samples/snippets/storage_get_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_hmac_key]\nfrom google.cloud import storage\n\n\ndef get_key(access_id, project_id):\n    \"\"\"\n    Retrieve the HMACKeyMetadata with the given access id.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an HMAC key\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_get_hmac_key]\n\nif __name__ == \"__main__\":\n    get_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_download_encrypted_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_download_encrypted_file]\nimport base64\n# [END storage_download_encrypted_file]\nimport sys\n# [START storage_download_encrypted_file]\n\nfrom google.cloud import storage\n\n\ndef download_encrypted_blob(\n    bucket_name,\n    source_blob_name,\n    destination_file_name,\n    base64_encryption_key,\n):\n    \"\"\"Downloads a previously-encrypted blob from Google Cloud Storage.\n\n    The encryption key provided must be the same key provided when uploading\n    the blob.\n    \"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_blob_name = \"storage-object-name\"\n    # destination_file_name = \"local/path/to/file\"\n    # base64_encryption_key = \"base64-encoded-encryption-key\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Encryption key must be an AES256 key represented as a bytestring with\n    # 32 bytes. Since it's passed in as a base64 encoded string, it needs\n    # to be decoded.\n    encryption_key = base64.b64decode(base64_encryption_key)\n    blob = bucket.blob(source_blob_name, encryption_key=encryption_key)\n\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        f\"Blob {source_blob_name} downloaded to {destination_file_name}.\"\n    )\n\n\n# [END storage_download_encrypted_file]\n\nif __name__ == \"__main__\":\n    download_encrypted_blob(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        destination_file_name=sys.argv[3],\n        base64_encryption_key=sys.argv[4],\n    )\n", "samples/snippets/storage_disable_versioning.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_versioning]\nfrom google.cloud import storage\n\n\ndef disable_versioning(bucket_name):\n    \"\"\"Disable versioning for this bucket.\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.versioning_enabled = False\n    bucket.patch()\n\n    print(f\"Versioning was disabled for bucket {bucket}\")\n    return bucket\n\n\n# [END storage_disable_versioning]\n\nif __name__ == \"__main__\":\n    disable_versioning(bucket_name=sys.argv[1])\n", "samples/snippets/storage_delete_file_archived_generation.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_file_archived_generation]\nfrom google.cloud import storage\n\n\ndef delete_file_archived_generation(bucket_name, blob_name, generation):\n    \"\"\"Delete a blob in the bucket with the given generation.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # generation = 1579287380533984\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.delete_blob(blob_name, generation=generation)\n    print(\n        f\"Generation {generation} of blob {blob_name} was deleted from {bucket_name}\"\n    )\n\n\n# [END storage_delete_file_archived_generation]\n\n\nif __name__ == \"__main__\":\n    delete_file_archived_generation(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        generation=sys.argv[3]\n    )\n", "samples/snippets/storage_change_file_storage_class.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_change_file_storage_class]\nfrom google.cloud import storage\n\n\ndef change_file_storage_class(bucket_name, blob_name):\n    \"\"\"Change the default storage class of the blob\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    generation_match_precondition = None\n\n    # Optional: set a generation-match precondition to avoid potential race\n    # conditions and data corruptions. The request is aborted if the\n    # object's generation number does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = blob.generation\n\n    blob.update_storage_class(\"NEARLINE\", if_generation_match=generation_match_precondition)\n\n    print(\n        \"Blob {} in bucket {} had its storage class set to {}\".format(\n            blob_name,\n            bucket_name,\n            blob.storage_class\n        )\n    )\n    return blob\n# [END storage_change_file_storage_class]\n\n\nif __name__ == \"__main__\":\n    change_file_storage_class(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_download_byte_range.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_byte_range]\nfrom google.cloud import storage\n\n\ndef download_byte_range(\n    bucket_name, source_blob_name, start_byte, end_byte, destination_file_name\n):\n    \"\"\"Downloads a blob from the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object\n    # source_blob_name = \"storage-object-name\"\n\n    # The starting byte at which to begin the download\n    # start_byte = 0\n\n    # The ending byte at which to end the download\n    # end_byte = 20\n\n    # The path to which the file should be downloaded\n    # destination_file_name = \"local/path/to/file\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n    # any content from Google Cloud Storage. As we don't need additional data,\n    # using `Bucket.blob` is preferred here.\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name, start=start_byte, end=end_byte)\n\n    print(\n        \"Downloaded bytes {} to {} of object {} from bucket {} to local file {}.\".format(\n            start_byte, end_byte, source_blob_name, bucket_name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_byte_range]\n\nif __name__ == \"__main__\":\n    download_byte_range(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        start_byte=sys.argv[3],\n        end_byte=sys.argv[4],\n        destination_file_name=sys.argv[5],\n    )\n", "samples/snippets/storage_download_public_file.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_public_file]\nfrom google.cloud import storage\n\n\ndef download_public_file(bucket_name, source_blob_name, destination_file_name):\n    \"\"\"Downloads a public blob from the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_blob_name = \"storage-object-name\"\n    # destination_file_name = \"local/path/to/file\"\n\n    storage_client = storage.Client.create_anonymous_client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        \"Downloaded public blob {} from bucket {} to {}.\".format(\n            source_blob_name, bucket.name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_public_file]\n\nif __name__ == \"__main__\":\n    download_public_file(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        destination_file_name=sys.argv[3],\n    )\n", "samples/snippets/storage_remove_bucket_label.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_remove_bucket_label]\nimport pprint\n# [END storage_remove_bucket_label]\nimport sys\n# [START storage_remove_bucket_label]\n\nfrom google.cloud import storage\n\n\ndef remove_bucket_label(bucket_name):\n    \"\"\"Remove a label from a bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    labels = bucket.labels\n\n    if \"example\" in labels:\n        del labels[\"example\"]\n\n    bucket.labels = labels\n    bucket.patch()\n\n    print(f\"Removed labels on {bucket.name}.\")\n    pprint.pprint(bucket.labels)\n\n\n# [END storage_remove_bucket_label]\n\nif __name__ == \"__main__\":\n    remove_bucket_label(bucket_name=sys.argv[1])\n", "samples/snippets/storage_transfer_manager_download_many.py": "# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_download_many]\ndef download_many_blobs_with_transfer_manager(\n    bucket_name, blob_names, destination_directory=\"\", workers=8\n):\n    \"\"\"Download blobs in a list by name, concurrently in a process pool.\n\n    The filename of each blob once downloaded is derived from the blob name and\n    the `destination_directory `parameter. For complete control of the filename\n    of each blob, use transfer_manager.download_many() instead.\n\n    Directories will be created automatically as needed to accommodate blob\n    names that include slashes.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The list of blob names to download. The names of each blobs will also\n    # be the name of each destination file (use transfer_manager.download_many()\n    # instead to control each destination file name). If there is a \"/\" in the\n    # blob name, then corresponding directories will be created on download.\n    # blob_names = [\"myblob\", \"myblob2\"]\n\n    # The directory on your computer to which to download all of the files. This\n    # string is prepended (with os.path.join()) to the name of each blob to form\n    # the full path. Relative paths and absolute paths are both accepted. An\n    # empty string means \"the current working directory\". Note that this\n    # parameter allows accepts directory traversal (\"../\" etc.) and is not\n    # intended for unsanitized end user input.\n    # destination_directory = \"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    results = transfer_manager.download_many_to_path(\n        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n    )\n\n    for name, result in zip(blob_names, results):\n        # The results list is either `None` or an exception for each blob in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to download {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n# [END storage_transfer_manager_download_many]\n", "samples/snippets/storage_fileio_write_read.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that writes and read a blob in GCS using file-like IO\n\"\"\"\n\n# [START storage_fileio_write_read]\nfrom google.cloud import storage\n\n\ndef write_read(bucket_name, blob_name):\n    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your new GCS object\n    # blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Mode can be specified as wb/rb for bytes mode.\n    # See: https://docs.python.org/3/library/io.html\n    with blob.open(\"w\") as f:\n        f.write(\"Hello world\")\n\n    with blob.open(\"r\") as f:\n        print(f.read())\n\n\n# [END storage_fileio_write_read]\n\nif __name__ == \"__main__\":\n    write_read(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2]\n    )\n", "samples/snippets/noxfile.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport glob\nimport os\nfrom pathlib import Path\nimport sys\nfrom typing import Callable, Dict, Optional\n\nimport nox\n\n\n# WARNING - WARNING - WARNING - WARNING - WARNING\n# WARNING - WARNING - WARNING - WARNING - WARNING\n#           DO NOT EDIT THIS FILE EVER!\n# WARNING - WARNING - WARNING - WARNING - WARNING\n# WARNING - WARNING - WARNING - WARNING - WARNING\n\nBLACK_VERSION = \"black==22.3.0\"\nISORT_VERSION = \"isort==5.10.1\"\n\n# Copy `noxfile_config.py` to your directory and modify it instead.\n\n# `TEST_CONFIG` dict is a configuration hook that allows users to\n# modify the test configurations. The values here should be in sync\n# with `noxfile_config.py`. Users will copy `noxfile_config.py` into\n# their directory and modify it.\n\nTEST_CONFIG = {\n    # You can opt out from the test for specific Python versions.\n    \"ignored_versions\": [],\n    # Old samples are opted out of enforcing Python type hints\n    # All new samples should feature them\n    \"enforce_type_hints\": False,\n    # An envvar key for determining the project id to use. Change it\n    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a\n    # build specific Cloud project. You can also use your own string\n    # to use your own Cloud project.\n    \"gcloud_project_env\": \"GOOGLE_CLOUD_PROJECT\",\n    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',\n    # If you need to use a specific version of pip,\n    # change pip_version_override to the string representation\n    # of the version number, for example, \"20.2.4\"\n    \"pip_version_override\": None,\n    # A dictionary you want to inject into your test. Don't put any\n    # secrets here. These values will override predefined values.\n    \"envs\": {},\n}\n\n\ntry:\n    # Ensure we can import noxfile_config in the project's directory.\n    sys.path.append(\".\")\n    from noxfile_config import TEST_CONFIG_OVERRIDE\nexcept ImportError as e:\n    print(\"No user noxfile_config found: detail: {}\".format(e))\n    TEST_CONFIG_OVERRIDE = {}\n\n# Update the TEST_CONFIG with the user supplied values.\nTEST_CONFIG.update(TEST_CONFIG_OVERRIDE)\n\n\ndef get_pytest_env_vars() -> Dict[str, str]:\n    \"\"\"Returns a dict for pytest invocation.\"\"\"\n    ret = {}\n\n    # Override the GCLOUD_PROJECT and the alias.\n    env_key = TEST_CONFIG[\"gcloud_project_env\"]\n    # This should error out if not set.\n    ret[\"GOOGLE_CLOUD_PROJECT\"] = os.environ[env_key]\n\n    # Apply user supplied envs.\n    ret.update(TEST_CONFIG[\"envs\"])\n    return ret\n\n\n# DO NOT EDIT - automatically generated.\n# All versions used to test samples.\nALL_VERSIONS = [\"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n# Any default versions that should be ignored.\nIGNORED_VERSIONS = TEST_CONFIG[\"ignored_versions\"]\n\nTESTED_VERSIONS = sorted([v for v in ALL_VERSIONS if v not in IGNORED_VERSIONS])\n\nINSTALL_LIBRARY_FROM_SOURCE = os.environ.get(\"INSTALL_LIBRARY_FROM_SOURCE\", False) in (\n    \"True\",\n    \"true\",\n)\n\n# Error if a python version is missing\nnox.options.error_on_missing_interpreters = True\n\n#\n# Style Checks\n#\n\n\n# Linting with flake8.\n#\n# We ignore the following rules:\n#   E203: whitespace before \u2018:\u2019\n#   E266: too many leading \u2018#\u2019 for block comment\n#   E501: line too long\n#   I202: Additional newline in a section of imports\n#\n# We also need to specify the rules which are ignored by default:\n# ['E226', 'W504', 'E126', 'E123', 'W503', 'E24', 'E704', 'E121']\nFLAKE8_COMMON_ARGS = [\n    \"--show-source\",\n    \"--builtin=gettext\",\n    \"--max-complexity=20\",\n    \"--exclude=.nox,.cache,env,lib,generated_pb2,*_pb2.py,*_pb2_grpc.py\",\n    \"--ignore=E121,E123,E126,E203,E226,E24,E266,E501,E704,W503,W504,I202\",\n    \"--max-line-length=88\",\n]\n\n\n@nox.session\ndef lint(session: nox.sessions.Session) -> None:\n    if not TEST_CONFIG[\"enforce_type_hints\"]:\n        session.install(\"flake8\")\n    else:\n        session.install(\"flake8\", \"flake8-annotations\")\n\n    args = FLAKE8_COMMON_ARGS + [\n        \".\",\n    ]\n    session.run(\"flake8\", *args)\n\n\n#\n# Black\n#\n\n\n@nox.session\ndef blacken(session: nox.sessions.Session) -> None:\n    \"\"\"Run black. Format code to uniform standard.\"\"\"\n    session.install(BLACK_VERSION)\n    python_files = [path for path in os.listdir(\".\") if path.endswith(\".py\")]\n\n    session.run(\"black\", *python_files)\n\n\n#\n# format = isort + black\n#\n\n@nox.session\ndef format(session: nox.sessions.Session) -> None:\n    \"\"\"\n    Run isort to sort imports. Then run black\n    to format code to uniform standard.\n    \"\"\"\n    session.install(BLACK_VERSION, ISORT_VERSION)\n    python_files = [path for path in os.listdir(\".\") if path.endswith(\".py\")]\n\n    # Use the --fss option to sort imports using strict alphabetical order.\n    # See https://pycqa.github.io/isort/docs/configuration/options.html#force-sort-within-sections\n    session.run(\"isort\", \"--fss\", *python_files)\n    session.run(\"black\", *python_files)\n\n\n#\n# Sample Tests\n#\n\n\nPYTEST_COMMON_ARGS = [\"--junitxml=sponge_log.xml\"]\n\n\ndef _session_tests(\n    session: nox.sessions.Session, post_install: Callable = None\n) -> None:\n    # check for presence of tests\n    test_list = glob.glob(\"**/*_test.py\", recursive=True) + glob.glob(\"**/test_*.py\", recursive=True)\n    test_list.extend(glob.glob(\"**/tests\", recursive=True))\n\n    if len(test_list) == 0:\n        print(\"No tests found, skipping directory.\")\n        return\n\n    if TEST_CONFIG[\"pip_version_override\"]:\n        pip_version = TEST_CONFIG[\"pip_version_override\"]\n        session.install(f\"pip=={pip_version}\")\n    \"\"\"Runs py.test for a particular project.\"\"\"\n    concurrent_args = []\n    if os.path.exists(\"requirements.txt\"):\n        if os.path.exists(\"constraints.txt\"):\n            session.install(\"-r\", \"requirements.txt\", \"-c\", \"constraints.txt\")\n        else:\n            session.install(\"-r\", \"requirements.txt\")\n        with open(\"requirements.txt\") as rfile:\n            packages = rfile.read()\n\n    if os.path.exists(\"requirements-test.txt\"):\n        if os.path.exists(\"constraints-test.txt\"):\n            session.install(\n                \"-r\", \"requirements-test.txt\", \"-c\", \"constraints-test.txt\"\n            )\n        else:\n            session.install(\"-r\", \"requirements-test.txt\")\n        with open(\"requirements-test.txt\") as rtfile:\n            packages += rtfile.read()\n\n    if INSTALL_LIBRARY_FROM_SOURCE:\n        session.install(\"-e\", _get_repo_root())\n\n    if post_install:\n        post_install(session)\n\n    if \"pytest-parallel\" in packages:\n        concurrent_args.extend(['--workers', 'auto', '--tests-per-worker', 'auto'])\n    elif \"pytest-xdist\" in packages:\n        concurrent_args.extend(['-n', 'auto'])\n\n    session.run(\n        \"pytest\",\n        *(PYTEST_COMMON_ARGS + session.posargs + concurrent_args),\n        # Pytest will return 5 when no tests are collected. This can happen\n        # on travis where slow and flaky tests are excluded.\n        # See http://doc.pytest.org/en/latest/_modules/_pytest/main.html\n        success_codes=[0, 5],\n        env=get_pytest_env_vars(),\n    )\n\n\n@nox.session(python=ALL_VERSIONS)\ndef py(session: nox.sessions.Session) -> None:\n    \"\"\"Runs py.test for a sample using the specified version of Python.\"\"\"\n    if session.python in TESTED_VERSIONS:\n        _session_tests(session)\n    else:\n        session.skip(\n            \"SKIPPED: {} tests are disabled for this sample.\".format(session.python)\n        )\n\n\n#\n# Readmegen\n#\n\n\ndef _get_repo_root() -> Optional[str]:\n    \"\"\" Returns the root folder of the project. \"\"\"\n    # Get root of this repository. Assume we don't have directories nested deeper than 10 items.\n    p = Path(os.getcwd())\n    for i in range(10):\n        if p is None:\n            break\n        if Path(p / \".git\").exists():\n            return str(p)\n        # .git is not available in repos cloned via Cloud Build\n        # setup.py is always in the library's root, so use that instead\n        # https://github.com/googleapis/synthtool/issues/792\n        if Path(p / \"setup.py\").exists():\n            return str(p)\n        p = p.parent\n    raise Exception(\"Unable to detect repository root.\")\n\n\nGENERATED_READMES = sorted([x for x in Path(\".\").rglob(\"*.rst.in\")])\n\n\n@nox.session\n@nox.parametrize(\"path\", GENERATED_READMES)\ndef readmegen(session: nox.sessions.Session, path: str) -> None:\n    \"\"\"(Re-)generates the readme for a sample.\"\"\"\n    session.install(\"jinja2\", \"pyyaml\")\n    dir_ = os.path.dirname(path)\n\n    if os.path.exists(os.path.join(dir_, \"requirements.txt\")):\n        session.install(\"-r\", os.path.join(dir_, \"requirements.txt\"))\n\n    in_file = os.path.join(dir_, \"README.rst.in\")\n    session.run(\n        \"python\", _get_repo_root() + \"/scripts/readme-gen/readme_gen.py\", in_file\n    )\n", "samples/snippets/storage_object_csek_to_cmek.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport sys\n\n# [START storage_object_csek_to_cmek]\nfrom google.cloud import storage\n\n\ndef object_csek_to_cmek(bucket_name, blob_name, encryption_key, kms_key_name):\n    \"\"\"Change a blob's customer-supplied encryption key to KMS key\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # encryption_key = \"TIbv/fjexq+VmtXzAlc63J4z5kFmWJ6NdAPQulQBT7g=\"\n    # kms_key_name = \"projects/PROJ/locations/LOC/keyRings/RING/cryptoKey/KEY\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    current_encryption_key = base64.b64decode(encryption_key)\n    source_blob = bucket.blob(blob_name, encryption_key=current_encryption_key)\n    destination_blob = bucket.blob(blob_name, kms_key_name=kms_key_name)\n    generation_match_precondition = None\n    token = None\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to rewrite is aborted if the object's\n    # generation number does not match your precondition.\n    source_blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = source_blob.generation\n\n    while True:\n        token, bytes_rewritten, total_bytes = destination_blob.rewrite(\n            source_blob, token=token, if_generation_match=generation_match_precondition\n        )\n        if token is None:\n            break\n\n    print(\n        \"Blob {} in bucket {} is now managed by the KMS key {} instead of a customer-supplied encryption key\".format(\n            blob_name, bucket_name, kms_key_name\n        )\n    )\n    return destination_blob\n\n\n# [END storage_object_csek_to_cmek]\n\nif __name__ == \"__main__\":\n    object_csek_to_cmek(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        encryption_key=sys.argv[3],\n        kms_key_name=sys.argv[4],\n    )\n", "samples/snippets/storage_remove_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_retention_policy]\nfrom google.cloud import storage\n\n\ndef remove_retention_policy(bucket_name):\n    \"\"\"Removes the retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket.reload()\n\n    if bucket.retention_policy_locked:\n        print(\n            \"Unable to remove retention period as retention policy is locked.\"\n        )\n        return\n\n    bucket.retention_period = None\n    bucket.patch()\n\n    print(f\"Removed bucket {bucket.name} retention policy\")\n\n\n# [END storage_remove_retention_policy]\n\n\nif __name__ == \"__main__\":\n    remove_retention_policy(bucket_name=sys.argv[1])\n", "samples/snippets/storage_download_file_requester_pays.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_file_requester_pays]\nfrom google.cloud import storage\n\n\ndef download_file_requester_pays(\n    bucket_name, project_id, source_blob_name, destination_file_name\n):\n    \"\"\"Download file using specified project as the requester\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # project_id = \"your-project-id\"\n    # source_blob_name = \"source-blob-name\"\n    # destination_file_name = \"local-destination-file-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name, user_project=project_id)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        \"Blob {} downloaded to {} using a requester-pays request.\".format(\n            source_blob_name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_file_requester_pays]\n\nif __name__ == \"__main__\":\n    download_file_requester_pays(\n        bucket_name=sys.argv[1],\n        project_id=sys.argv[2],\n        source_blob_name=sys.argv[3],\n        destination_file_name=sys.argv[4],\n    )\n", "samples/snippets/storage_enable_versioning.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_versioning]\nfrom google.cloud import storage\n\n\ndef enable_versioning(bucket_name):\n    \"\"\"Enable versioning for this bucket.\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.versioning_enabled = True\n    bucket.patch()\n\n    print(f\"Versioning was enabled for bucket {bucket.name}\")\n    return bucket\n\n\n# [END storage_enable_versioning]\n\nif __name__ == \"__main__\":\n    enable_versioning(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_default_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_default_event_based_hold]\nfrom google.cloud import storage\n\n\ndef get_default_event_based_hold(bucket_name):\n    \"\"\"Gets the default event based hold on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n\n    if bucket.default_event_based_hold:\n        print(f\"Default event-based hold is enabled for {bucket_name}\")\n    else:\n        print(\n            f\"Default event-based hold is not enabled for {bucket_name}\"\n        )\n\n\n# [END storage_get_default_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    get_default_event_based_hold(bucket_name=sys.argv[1])\n", "samples/snippets/storage_disable_requester_pays.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_requester_pays]\nfrom google.cloud import storage\n\n\ndef disable_requester_pays(bucket_name):\n    \"\"\"Disable a bucket's requesterpays metadata\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.requester_pays = False\n    bucket.patch()\n\n    print(f\"Requester Pays has been disabled for {bucket_name}\")\n\n\n# [END storage_disable_requester_pays]\n\n\nif __name__ == \"__main__\":\n    disable_requester_pays(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket_object_retention.py": "#!/usr/bin/env python\n\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket_with_object_retention]\nfrom google.cloud import storage\n\n\ndef create_bucket_object_retention(bucket_name):\n    \"\"\"Creates a bucket with object retention enabled.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.create_bucket(bucket_name, enable_object_retention=True)\n\n    print(f\"Created bucket {bucket_name} with object retention enabled setting: {bucket.object_retention_mode}\")\n\n\n# [END storage_create_bucket_with_object_retention]\n\n\nif __name__ == \"__main__\":\n    create_bucket_object_retention(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket_class_location.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket_class_location]\nfrom google.cloud import storage\n\n\ndef create_bucket_class_location(bucket_name):\n    \"\"\"\n    Create a new bucket in the US region with the coldline storage\n    class\n    \"\"\"\n    # bucket_name = \"your-new-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    bucket.storage_class = \"COLDLINE\"\n    new_bucket = storage_client.create_bucket(bucket, location=\"us\")\n\n    print(\n        \"Created bucket {} in {} with storage class {}\".format(\n            new_bucket.name, new_bucket.location, new_bucket.storage_class\n        )\n    )\n    return new_bucket\n\n\n# [END storage_create_bucket_class_location]\n\nif __name__ == \"__main__\":\n    create_bucket_class_location(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_from_memory.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_file_upload_from_memory]\nfrom google.cloud import storage\n\n\ndef upload_blob_from_memory(bucket_name, contents, destination_blob_name):\n    \"\"\"Uploads a file to the bucket.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The contents to upload to the file\n    # contents = \"these are my contents\"\n\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    blob.upload_from_string(contents)\n\n    print(\n        f\"{destination_blob_name} with contents {contents} uploaded to {bucket_name}.\"\n    )\n\n# [END storage_file_upload_from_memory]\n\n\nif __name__ == \"__main__\":\n    upload_blob_from_memory(\n        bucket_name=sys.argv[1],\n        contents=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n    )\n", "samples/snippets/storage_remove_bucket_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_owner]\nfrom google.cloud import storage\n\n\ndef remove_bucket_owner(bucket_name, user_email):\n    \"\"\"Removes a user from the access control list of the given bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # remove access for different types of entities.\n    bucket.acl.user(user_email).revoke_read()\n    bucket.acl.user(user_email).revoke_write()\n    bucket.acl.user(user_email).revoke_owner()\n    bucket.acl.save()\n\n    print(f\"Removed user {user_email} from bucket {bucket_name}.\")\n\n\n# [END storage_remove_bucket_owner]\n\nif __name__ == \"__main__\":\n    remove_bucket_owner(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_set_public_access_prevention_inherited.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that sets public access prevention to inherited.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/using-public-access-prevention\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_set_public_access_prevention_inherited]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_INHERITED\n\n\ndef set_public_access_prevention_inherited(bucket_name):\n    \"\"\"Sets the public access prevention status to inherited, so that the bucket inherits its setting from its parent project.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.public_access_prevention = (\n        PUBLIC_ACCESS_PREVENTION_INHERITED\n    )\n    bucket.patch()\n\n    print(f\"Public access prevention is 'inherited' for {bucket.name}.\")\n\n\n# [END storage_set_public_access_prevention_inherited]\n\nif __name__ == \"__main__\":\n    set_public_access_prevention_inherited(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket_turbo_replication.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates a new bucket with dual-region and turbo replication.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_create_bucket_turbo_replication]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import RPO_ASYNC_TURBO\n\n\ndef create_bucket_turbo_replication(bucket_name):\n    \"\"\"Creates dual-region bucket with turbo replication enabled.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket_location = \"NAM4\"\n    bucket.rpo = RPO_ASYNC_TURBO\n    bucket.create(location=bucket_location)\n\n    print(f\"{bucket.name} created with the recovery point objective (RPO) set to {bucket.rpo} in {bucket.location}.\")\n\n\n# [END storage_create_bucket_turbo_replication]\n\nif __name__ == \"__main__\":\n    create_bucket_turbo_replication(bucket_name=sys.argv[1])\n", "samples/snippets/storage_activate_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_activate_hmac_key]\nfrom google.cloud import storage\n\n\ndef activate_key(access_id, project_id):\n    \"\"\"\n    Activate the HMAC key with the given access ID.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an inactive HMAC key\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n    hmac_key.state = \"ACTIVE\"\n    hmac_key.update()\n\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_activate_hmac_key]\n\nif __name__ == \"__main__\":\n    activate_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_list_hmac_keys.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_hmac_keys]\nfrom google.cloud import storage\n\n\ndef list_keys(project_id):\n    \"\"\"\n    List all HMAC keys associated with the project.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n\n    storage_client = storage.Client(project=project_id)\n    hmac_keys = storage_client.list_hmac_keys(project_id=project_id)\n    print(\"HMAC Keys:\")\n    for hmac_key in hmac_keys:\n        print(\n            f\"Service Account Email: {hmac_key.service_account_email}\"\n        )\n        print(f\"Access ID: {hmac_key.access_id}\")\n    return hmac_keys\n\n\n# [END storage_list_hmac_keys]\n\nif __name__ == \"__main__\":\n    list_keys(project_id=sys.argv[1])\n", "samples/snippets/storage_fileio_pandas.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates and consumes a GCS blob using pandas with file-like IO\n\"\"\"\n\n# [START storage_fileio_pandas_write]\n\n\ndef pandas_write(bucket_name, blob_name):\n    \"\"\"Use pandas to interact with GCS using file-like IO\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your new GCS object\n    # blob_name = \"storage-object-name\"\n\n    from google.cloud import storage\n    import pandas as pd\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    with blob.open(\"w\") as f:\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n        f.write(df.to_csv(index=False))\n\n    print(f\"Wrote csv with pandas with name {blob_name} from bucket {bucket.name}.\")\n\n\n# [END storage_fileio_pandas_write]\n\n\n# [START storage_fileio_pandas_read]\n\n\ndef pandas_read(bucket_name, blob_name):\n    \"\"\"Use pandas to interact with GCS using file-like IO\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your new GCS object\n    # blob_name = \"storage-object-name\"\n\n    from google.cloud import storage\n    import pandas as pd\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    with blob.open(\"r\") as f:\n        pd.read_csv(f)\n\n    print(f\"Read csv with pandas with name {blob_name} from bucket {bucket.name}.\")\n\n\n# [END storage_fileio_pandas_read]\n\n\nif __name__ == \"__main__\":\n    pandas_write(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2]\n    )\n\n    pandas_read(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_add_bucket_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_owner]\nfrom google.cloud import storage\n\n\ndef add_bucket_owner(bucket_name, user_email):\n    \"\"\"Adds a user as an owner on the given bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group()`, `domain()`, `all_authenticated()` and `all()`\n    # to grant access to different types of entities.\n    # You can also use `grant_read()` or `grant_write()` to grant different\n    # roles.\n    bucket.acl.user(user_email).grant_owner()\n    bucket.acl.save()\n\n    print(\n        f\"Added user {user_email} as an owner on bucket {bucket_name}.\"\n    )\n\n\n# [END storage_add_bucket_owner]\n\nif __name__ == \"__main__\":\n    add_bucket_owner(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_remove_bucket_conditional_iam_binding.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_conditional_iam_binding]\nfrom google.cloud import storage\n\n\ndef remove_bucket_conditional_iam_binding(\n    bucket_name, role, title, description, expression\n):\n    \"\"\"Remove a conditional IAM binding from a bucket's IAM policy.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g. roles/storage.objectViewer\"\n    # title = \"Condition title.\"\n    # description = \"Condition description.\"\n    # expression = \"Condition expression.\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    # Set the policy's version to 3 to use condition in bindings.\n    policy.version = 3\n\n    condition = {\n        \"title\": title,\n        \"description\": description,\n        \"expression\": expression,\n    }\n    policy.bindings = [\n        binding\n        for binding in policy.bindings\n        if not (binding[\"role\"] == role and binding.get(\"condition\") == condition)\n    ]\n\n    bucket.set_iam_policy(policy)\n\n    print(\"Conditional Binding was removed.\")\n\n\n# [END storage_remove_bucket_conditional_iam_binding]\n\n\nif __name__ == \"__main__\":\n    remove_bucket_conditional_iam_binding(\n        bucket_name=sys.argv[1],\n        role=sys.argv[2],\n        title=sys.argv[3],\n        description=sys.argv[4],\n        expression=sys.argv[5],\n    )\n", "samples/snippets/storage_copy_file_archived_generation.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_copy_file_archived_generation]\nfrom google.cloud import storage\n\n\ndef copy_file_archived_generation(\n        bucket_name, blob_name, destination_bucket_name, destination_blob_name, generation\n):\n    \"\"\"Copies a blob from one bucket to another with a new name with the same generation.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # destination_bucket_name = \"destination-bucket-name\"\n    # destination_blob_name = \"destination-object-name\"\n    # generation = 1579287380533984\n\n    storage_client = storage.Client()\n\n    source_bucket = storage_client.bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.bucket(destination_bucket_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to copy is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    destination_generation_match_precondition = 0\n\n    # source_generation selects a specific revision of the source object, as opposed to the latest version.\n    blob_copy = source_bucket.copy_blob(\n        source_blob, destination_bucket, destination_blob_name, source_generation=generation, if_generation_match=destination_generation_match_precondition\n    )\n\n    print(\n        \"Generation {} of the blob {} in bucket {} copied to blob {} in bucket {}.\".format(\n            generation,\n            source_blob.name,\n            source_bucket.name,\n            blob_copy.name,\n            destination_bucket.name,\n        )\n    )\n\n\n# [END storage_copy_file_archived_generation]\n\nif __name__ == \"__main__\":\n    copy_file_archived_generation(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        destination_bucket_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n        generation=sys.argv[5]\n    )\n", "samples/snippets/storage_generate_upload_signed_url_v4.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_generate_upload_signed_url_v4]\nimport datetime\n# [END storage_generate_upload_signed_url_v4]\nimport sys\n# [START storage_generate_upload_signed_url_v4]\n\nfrom google.cloud import storage\n\n\ndef generate_upload_signed_url_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 signed URL for uploading a blob using HTTP PUT.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        version=\"v4\",\n        # This URL is valid for 15 minutes\n        expiration=datetime.timedelta(minutes=15),\n        # Allow PUT requests using this URL.\n        method=\"PUT\",\n        content_type=\"application/octet-stream\",\n    )\n\n    print(\"Generated PUT signed URL:\")\n    print(url)\n    print(\"You can use this URL with any user agent, for example:\")\n    print(\n        \"curl -X PUT -H 'Content-Type: application/octet-stream' \"\n        \"--upload-file my-file '{}'\".format(url)\n    )\n    return url\n\n\n# [END storage_generate_upload_signed_url_v4]\n\n\nif __name__ == \"__main__\":\n    generate_upload_signed_url_v4(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_rotate_encryption_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_rotate_encryption_key]\nimport base64\n# [END storage_rotate_encryption_key]\nimport sys\n# [START storage_rotate_encryption_key]\n\nfrom google.cloud import storage\n\n\ndef rotate_encryption_key(\n    bucket_name, blob_name, base64_encryption_key, base64_new_encryption_key\n):\n    \"\"\"Performs a key rotation by re-writing an encrypted blob with a new\n    encryption key.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    current_encryption_key = base64.b64decode(base64_encryption_key)\n    new_encryption_key = base64.b64decode(base64_new_encryption_key)\n\n    # Both source_blob and destination_blob refer to the same storage object,\n    # but destination_blob has the new encryption key.\n    source_blob = bucket.blob(\n        blob_name, encryption_key=current_encryption_key\n    )\n    destination_blob = bucket.blob(\n        blob_name, encryption_key=new_encryption_key\n    )\n    generation_match_precondition = None\n    token = None\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to rewrite is aborted if the object's\n    # generation number does not match your precondition.\n    source_blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = source_blob.generation\n\n    while True:\n        token, bytes_rewritten, total_bytes = destination_blob.rewrite(\n            source_blob, token=token, if_generation_match=generation_match_precondition\n        )\n        if token is None:\n            break\n\n    print(f\"Key rotation complete for Blob {blob_name}\")\n\n\n# [END storage_rotate_encryption_key]\n\nif __name__ == \"__main__\":\n    rotate_encryption_key(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        base64_encryption_key=sys.argv[3],\n        base64_new_encryption_key=sys.argv[4],\n    )\n", "samples/snippets/storage_set_rpo_async_turbo.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that sets RPO (Recovery Point Objective) to ASYNC_TURBO\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_set_rpo_async_turbo]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import RPO_ASYNC_TURBO\n\n\ndef set_rpo_async_turbo(bucket_name):\n    \"\"\"Sets the RPO to ASYNC_TURBO, enabling the turbo replication feature\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.rpo = RPO_ASYNC_TURBO\n    bucket.patch()\n\n    print(f\"RPO is set to ASYNC_TURBO for {bucket.name}.\")\n\n\n# [END storage_set_rpo_async_turbo]\n\nif __name__ == \"__main__\":\n    set_rpo_async_turbo(bucket_name=sys.argv[1])\n", "samples/snippets/noxfile_config.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Default TEST_CONFIG_OVERRIDE for python repos.\n\n# You can copy this file into your directory, then it will be imported from\n# the noxfile.py.\n\n# The source of truth:\n# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/noxfile_config.py\n\nimport os\n\n\n# We are reaching maximum number of HMAC keys on the service account.\n# We change the service account based on the value of\n# RUN_TESTS_SESSION. The reason we can not use multiple project is\n# that our new projects are enforced to have\n# 'constraints/iam.disableServiceAccountKeyCreation' policy.\ndef get_service_account_email():\n    session = os.environ.get('RUN_TESTS_SESSION')\n    if session == 'py-3.6':\n        return ('py36-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.7':\n        return ('py37-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.8':\n        return ('py38-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.9':\n        return ('py39-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.10':\n        return ('py310-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    return os.environ['HMAC_KEY_TEST_SERVICE_ACCOUNT']\n\n\n# We change the value of CLOUD_KMS_KEY based on the value of\n# RUN_TESTS_SESSION.\ndef get_cloud_kms_key():\n    session = os.environ.get('RUN_TESTS_SESSION')\n    if session == 'py-3.6':\n        return ('projects/python-docs-samples-tests-py36/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.7':\n        return ('projects/python-docs-samples-tests-py37/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.8':\n        return ('projects/python-docs-samples-tests-py38/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.9':\n        return ('projects/python-docs-samples-tests-py39/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.10':\n        return ('projects/python-docs-samples-tests-310/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.11':\n        return ('projects/python-docs-samples-tests-311/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.12':\n        return ('projects/python-docs-samples-tests-312/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    return os.environ['CLOUD_KMS_KEY']\n\n\nTEST_CONFIG_OVERRIDE = {\n    # You can opt out from the test for specific Python versions.\n    'ignored_versions': [\"2.7\", \"3.6\", \"3.7\", \"3.11\", \"3.12\"],\n\n    # An envvar key for determining the project id to use. Change it\n    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a\n    # build specific Cloud project. You can also use your own string\n    # to use your own Cloud project.\n    # 'gcloud_project_env': 'GOOGLE_CLOUD_PROJECT',\n    'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',\n\n    # A dictionary you want to inject into your test. Don't put any\n    # secrets here. These values will override predefined values.\n    'envs': {\n        'HMAC_KEY_TEST_SERVICE_ACCOUNT': get_service_account_email(),\n        'CLOUD_KMS_KEY': get_cloud_kms_key(),\n        # Some tests can not use multiple projects because of several reasons:\n        # 1. The new projects is enforced to have the\n        # 'constraints/iam.disableServiceAccountKeyCreation' policy.\n        # 2. The new projects buckets need to have universal permission model.\n        # For those tests, we'll use the original project.\n        'MAIN_GOOGLE_CLOUD_PROJECT': 'python-docs-samples-tests',\n        'MAIN_CLOUD_KMS_KEY': ('projects/python-docs-samples-tests/locations/us/'\n                               'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    },\n}\n", "samples/snippets/storage_get_autoclass.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_autoclass]\nfrom google.cloud import storage\n\n\ndef get_autoclass(bucket_name):\n    \"\"\"Get the Autoclass setting for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    autoclass_enabled = bucket.autoclass_enabled\n    autoclass_toggle_time = bucket.autoclass_toggle_time\n    terminal_storage_class = bucket.autoclass_terminal_storage_class\n    tsc_update_time = bucket.autoclass_terminal_storage_class_update_time\n\n    print(f\"Autoclass enabled is set to {autoclass_enabled} for {bucket.name} at {autoclass_toggle_time}.\")\n    print(f\"Autoclass terminal storage class is set to {terminal_storage_class} for {bucket.name} at {tsc_update_time}.\")\n\n    return bucket\n\n\n# [END storage_get_autoclass]\n\nif __name__ == \"__main__\":\n    get_autoclass(bucket_name=sys.argv[1])\n", "samples/snippets/storage_release_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_release_event_based_hold]\nfrom google.cloud import storage\n\n\ndef release_event_based_hold(bucket_name, blob_name):\n    \"\"\"Releases the event based hold on a given blob\"\"\"\n\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.event_based_hold = False\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(f\"Event based hold was released for {blob_name}\")\n\n\n# [END storage_release_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    release_event_based_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_object_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport sys\n\n# [START storage_set_object_retention_policy]\nfrom google.cloud import storage\n\n\ndef set_object_retention_policy(bucket_name, contents, destination_blob_name):\n    \"\"\"Set the object retention policy of a file.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The contents to upload to the file\n    # contents = \"these are my contents\"\n\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_string(contents)\n\n    # Set the retention policy for the file.\n    blob.retention.mode = \"Unlocked\"\n    retention_date = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=10)\n    blob.retention.retain_until_time = retention_date\n    blob.patch()\n    print(\n        f\"Retention policy for file {destination_blob_name} was set to: {blob.retention.mode}.\"\n    )\n\n    # To modify an existing policy on an unlocked file object, pass in the override parameter.\n    new_retention_date = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=9)\n    blob.retention.retain_until_time = new_retention_date\n    blob.patch(override_unlocked_retention=True)\n    print(\n        f\"Retention policy for file {destination_blob_name} was updated to: {blob.retention.retain_until_time}.\"\n    )\n\n\n# [END storage_set_object_retention_policy]\n\n\nif __name__ == \"__main__\":\n    set_object_retention_policy(\n        bucket_name=sys.argv[1],\n        contents=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n    )\n", "samples/snippets/storage_add_file_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_file_owner]\nfrom google.cloud import storage\n\n\ndef add_blob_owner(bucket_name, blob_name, user_email):\n    \"\"\"Adds a user as an owner on the given blob.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    blob.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # grant access to different types of entities. You can also use\n    # `grant_read` or `grant_write` to grant different roles.\n    blob.acl.user(user_email).grant_owner()\n    blob.acl.save()\n\n    print(\n        \"Added user {} as an owner on blob {} in bucket {}.\".format(\n            user_email, blob_name, bucket_name\n        )\n    )\n\n\n# [END storage_add_file_owner]\n\nif __name__ == \"__main__\":\n    add_blob_owner(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2], user_email=sys.argv[3],\n    )\n", "samples/snippets/storage_generate_encryption_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_generate_encryption_key]\nimport base64\nimport os\n\n\ndef generate_encryption_key():\n    \"\"\"Generates a 256 bit (32 byte) AES encryption key and prints the\n    base64 representation.\n\n    This is included for demonstration purposes. You should generate your own\n    key. Please remember that encryption keys should be handled with a\n    comprehensive security policy.\n    \"\"\"\n    key = os.urandom(32)\n    encoded_key = base64.b64encode(key).decode(\"utf-8\")\n\n    print(f\"Base 64 encoded encryption key: {encoded_key}\")\n\n\n# [END storage_generate_encryption_key]\n\nif __name__ == \"__main__\":\n    generate_encryption_key()\n", "samples/snippets/storage_deactivate_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_deactivate_hmac_key]\nfrom google.cloud import storage\n\n\ndef deactivate_key(access_id, project_id):\n    \"\"\"\n    Deactivate the HMAC key with the given access ID.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an active HMAC key\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n    hmac_key.state = \"INACTIVE\"\n    hmac_key.update()\n\n    print(\"The HMAC key is now inactive.\")\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_deactivate_hmac_key]\n\nif __name__ == \"__main__\":\n    deactivate_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_list_files_with_prefix.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_files_with_prefix]\nfrom google.cloud import storage\n\n\ndef list_blobs_with_prefix(bucket_name, prefix, delimiter=None):\n    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n\n    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n\n    The delimiter argument can be used to restrict the results to only the\n    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n    the prefix is returned. For example, given these blobs:\n\n        a/1.txt\n        a/b/2.txt\n\n    If you specify prefix ='a/', without a delimiter, you'll get back:\n\n        a/1.txt\n        a/b/2.txt\n\n    However, if you specify prefix='a/' and delimiter='/', you'll get back\n    only the file directly under 'a/':\n\n        a/1.txt\n\n    As part of the response, you'll also get back a blobs.prefixes entity\n    that lists the \"subfolders\" under `a/`:\n\n        a/b/\n    \"\"\"\n\n    storage_client = storage.Client()\n\n    # Note: Client.list_blobs requires at least package version 1.17.0.\n    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n\n    # Note: The call returns a response only when the iterator is consumed.\n    print(\"Blobs:\")\n    for blob in blobs:\n        print(blob.name)\n\n    if delimiter:\n        print(\"Prefixes:\")\n        for prefix in blobs.prefixes:\n            print(prefix)\n\n\n# [END storage_list_files_with_prefix]\n\nif __name__ == \"__main__\":\n    list_blobs_with_prefix(\n        bucket_name=sys.argv[1], prefix=sys.argv[2], delimiter=sys.argv[3]\n    )\n", "samples/snippets/storage_transfer_manager_upload_directory.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_upload_directory]\ndef upload_directory_with_transfer_manager(bucket_name, source_directory, workers=8):\n    \"\"\"Upload every file in a directory, including all files in subdirectories.\n\n    Each blob name is derived from the filename, not including the `directory`\n    parameter itself. For complete control of the blob name for each file (and\n    other aspects of individual blob metadata), use\n    transfer_manager.upload_many() instead.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The directory on your computer to upload. Files in the directory and its\n    # subdirectories will be uploaded. An empty string means \"the current\n    # working directory\".\n    # source_directory=\"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from pathlib import Path\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Generate a list of paths (in string form) relative to the `directory`.\n    # This can be done in a single list comprehension, but is expanded into\n    # multiple lines here for clarity.\n\n    # First, recursively get all files in `directory` as Path objects.\n    directory_as_path_obj = Path(source_directory)\n    paths = directory_as_path_obj.rglob(\"*\")\n\n    # Filter so the list only includes files, not directories themselves.\n    file_paths = [path for path in paths if path.is_file()]\n\n    # These paths are relative to the current working directory. Next, make them\n    # relative to `directory`\n    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n\n    # Finally, convert them all to strings.\n    string_paths = [str(path) for path in relative_paths]\n\n    print(\"Found {} files.\".format(len(string_paths)))\n\n    # Start the upload.\n    results = transfer_manager.upload_many_from_filenames(\n        bucket, string_paths, source_directory=source_directory, max_workers=workers\n    )\n\n    for name, result in zip(string_paths, results):\n        # The results list is either `None` or an exception for each filename in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Uploaded {} to {}.\".format(name, bucket.name))\n# [END storage_transfer_manager_upload_directory]\n", "samples/snippets/notification_polling.py": "#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This application demonstrates how to poll for GCS notifications from a\nCloud Pub/Sub subscription, parse the incoming message, and acknowledge the\nsuccessful processing of the message.\n\nThis application will work with any subscription configured for pull rather\nthan push notifications. If you do not already have notifications configured,\nyou may consult the docs at\nhttps://cloud.google.com/storage/docs/reporting-changes or follow the steps\nbelow:\n\n1. First, follow the common setup steps for these snippets, specically\n   configuring auth and installing dependencies. See the README's \"Setup\"\n   section.\n\n2. Activate the Google Cloud Pub/Sub API, if you have not already done so.\n   https://console.cloud.google.com/flows/enableapi?apiid=pubsub\n\n3. Create a Google Cloud Storage bucket:\n   $ gsutil mb gs://testbucket\n\n4. Create a Cloud Pub/Sub topic and publish bucket notifications there:\n   $ gsutil notification create -f json -t testtopic gs://testbucket\n\n5. Create a subscription for your new topic:\n   $ gcloud pubsub subscriptions create testsubscription --topic=testtopic\n\n6. Run this program:\n   $ python notification_polling.py my-project-id testsubscription\n\n7. While the program is running, upload and delete some files in the testbucket\n   bucket (you could use the console or gsutil) and watch as changes scroll by\n   in the app.\n\"\"\"\n\nimport argparse\nimport json\nimport time\n\nfrom google.cloud import pubsub_v1\n\n\ndef summarize(message):\n    data = message.data.decode(\"utf-8\")\n    attributes = message.attributes\n\n    event_type = attributes[\"eventType\"]\n    bucket_id = attributes[\"bucketId\"]\n    object_id = attributes[\"objectId\"]\n    generation = attributes[\"objectGeneration\"]\n    description = (\n        \"\\tEvent type: {event_type}\\n\"\n        \"\\tBucket ID: {bucket_id}\\n\"\n        \"\\tObject ID: {object_id}\\n\"\n        \"\\tGeneration: {generation}\\n\"\n    ).format(\n        event_type=event_type,\n        bucket_id=bucket_id,\n        object_id=object_id,\n        generation=generation,\n    )\n\n    if \"overwroteGeneration\" in attributes:\n        description += f\"\\tOverwrote generation: {attributes['overwroteGeneration']}\\n\"\n    if \"overwrittenByGeneration\" in attributes:\n        description += f\"\\tOverwritten by generation: {attributes['overwrittenByGeneration']}\\n\"\n\n    payload_format = attributes[\"payloadFormat\"]\n    if payload_format == \"JSON_API_V1\":\n        object_metadata = json.loads(data)\n        size = object_metadata[\"size\"]\n        content_type = object_metadata[\"contentType\"]\n        metageneration = object_metadata[\"metageneration\"]\n        description += (\n            \"\\tContent type: {content_type}\\n\"\n            \"\\tSize: {object_size}\\n\"\n            \"\\tMetageneration: {metageneration}\\n\"\n        ).format(\n            content_type=content_type,\n            object_size=size,\n            metageneration=metageneration,\n        )\n    return description\n\n\ndef poll_notifications(project, subscription_name):\n    \"\"\"Polls a Cloud Pub/Sub subscription for new GCS events for display.\"\"\"\n    subscriber = pubsub_v1.SubscriberClient()\n    subscription_path = subscriber.subscription_path(\n        project, subscription_name\n    )\n\n    def callback(message):\n        print(f\"Received message:\\n{summarize(message)}\")\n        message.ack()\n\n    subscriber.subscribe(subscription_path, callback=callback)\n\n    # The subscriber is non-blocking, so we must keep the main thread from\n    # exiting to allow it to process messages in the background.\n    print(f\"Listening for messages on {subscription_path}\")\n    while True:\n        time.sleep(60)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\n        \"project\", help=\"The ID of the project that owns the subscription\"\n    )\n    parser.add_argument(\n        \"subscription\", help=\"The ID of the Pub/Sub subscription\"\n    )\n    args = parser.parse_args()\n    poll_notifications(args.project, args.subscription)\n", "samples/snippets/storage_enable_requester_pays.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_requester_pays]\nfrom google.cloud import storage\n\n\ndef enable_requester_pays(bucket_name):\n    \"\"\"Enable a bucket's requesterpays metadata\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.requester_pays = True\n    bucket.patch()\n\n    print(f\"Requester Pays has been enabled for {bucket_name}\")\n\n\n# [END storage_enable_requester_pays]\n\nif __name__ == \"__main__\":\n    enable_requester_pays(bucket_name=sys.argv[1])\n", "samples/snippets/storage_set_autoclass.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_autoclass]\nfrom google.cloud import storage\n\n\ndef set_autoclass(bucket_name):\n    \"\"\"Configure the Autoclass setting for a bucket.\n\n    terminal_storage_class field is optional and defaults to NEARLINE if not otherwise specified.\n    Valid terminal_storage_class values are NEARLINE and ARCHIVE.\n    \"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n    # Enable Autoclass for a bucket. Set enabled to false to disable Autoclass.\n    # Set Autoclass.TerminalStorageClass, valid values are NEARLINE and ARCHIVE.\n    enabled = True\n    terminal_storage_class = \"ARCHIVE\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.autoclass_enabled = enabled\n    bucket.autoclass_terminal_storage_class = terminal_storage_class\n    bucket.patch()\n    print(f\"Autoclass enabled is set to {bucket.autoclass_enabled} for {bucket.name} at {bucket.autoclass_toggle_time}.\")\n    print(f\"Autoclass terminal storage class is {bucket.autoclass_terminal_storage_class}.\")\n\n    return bucket\n\n\n# [END storage_set_autoclass]\n\nif __name__ == \"__main__\":\n    set_autoclass(bucket_name=sys.argv[1])\n", "samples/snippets/storage_configure_retries.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that configures retries on an operation call.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/retry-strategy\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_configure_retries]\nfrom google.cloud import storage\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\ndef configure_retries(bucket_name, blob_name):\n    \"\"\"Configures retries with customizations.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of your GCS object\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Customize retry with a deadline of 500 seconds (default=120 seconds).\n    modified_retry = DEFAULT_RETRY.with_deadline(500.0)\n    # Customize retry with an initial wait time of 1.5 (default=1.0).\n    # Customize retry with a wait time multiplier per iteration of 1.2 (default=2.0).\n    # Customize retry with a maximum wait time of 45.0 (default=60.0).\n    modified_retry = modified_retry.with_delay(initial=1.5, multiplier=1.2, maximum=45.0)\n\n    # blob.delete() uses DEFAULT_RETRY_IF_GENERATION_SPECIFIED by default.\n    # Override with modified_retry so the function retries even if the generation\n    # number is not specified.\n    print(\n        f\"The following library method is customized to be retried according to the following configurations: {modified_retry}\"\n    )\n\n    blob.delete(retry=modified_retry)\n    print(f\"Blob {blob_name} deleted with a customized retry strategy.\")\n\n\n# [END storage_configure_retries]\n\n\nif __name__ == \"__main__\":\n    configure_retries(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_temporary_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_temporary_hold]\nfrom google.cloud import storage\n\n\ndef set_temporary_hold(bucket_name, blob_name):\n    \"\"\"Sets a temporary hold on a given blob\"\"\"\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.temporary_hold = True\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(\"Temporary hold was set for #{blob_name}\")\n\n\n# [END storage_set_temporary_hold]\n\n\nif __name__ == \"__main__\":\n    set_temporary_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_generate_signed_post_policy_v4.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_generate_signed_post_policy_v4]\nimport datetime\n# [END storage_generate_signed_post_policy_v4]\nimport sys\n# [START storage_generate_signed_post_policy_v4]\n\nfrom google.cloud import storage\n\n\ndef generate_signed_post_policy_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 POST Policy and prints an HTML form.\"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n\n    policy = storage_client.generate_signed_post_policy_v4(\n        bucket_name,\n        blob_name,\n        expiration=datetime.timedelta(minutes=10),\n        fields={\n          'x-goog-meta-test': 'data'\n        }\n    )\n\n    # Create an HTML form with the provided policy\n    header = \"<form action='{}' method='POST' enctype='multipart/form-data'>\\n\"\n    form = header.format(policy[\"url\"])\n\n    # Include all fields returned in the HTML form as they're required\n    for key, value in policy[\"fields\"].items():\n        form += f\"  <input name='{key}' value='{value}' type='hidden'/>\\n\"\n\n    form += \"  <input type='file' name='file'/><br />\\n\"\n    form += \"  <input type='submit' value='Upload File' /><br />\\n\"\n    form += \"</form>\"\n\n    print(form)\n\n    return form\n\n\n# [END storage_generate_signed_post_policy_v4]\n\nif __name__ == \"__main__\":\n    generate_signed_post_policy_v4(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_create_bucket_notifications.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates a notification configuration for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_create_bucket_notifications]\nfrom google.cloud import storage\n\n\ndef create_bucket_notifications(bucket_name, topic_name):\n    \"\"\"Creates a notification configuration for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The name of a topic\n    # topic_name = \"your-topic-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notification = bucket.notification(topic_name=topic_name)\n    notification.create()\n\n    print(f\"Successfully created notification with ID {notification.notification_id} for bucket {bucket_name}\")\n\n# [END storage_create_bucket_notifications]\n\n\nif __name__ == \"__main__\":\n    create_bucket_notifications(bucket_name=sys.argv[1], topic_name=sys.argv[2])\n", "samples/snippets/storage_download_to_stream.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_stream_file_download]\nfrom google.cloud import storage\n\n\ndef download_blob_to_stream(bucket_name, source_blob_name, file_obj):\n    \"\"\"Downloads a blob to a stream or other file-like object.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object (blob)\n    # source_blob_name = \"storage-object-name\"\n\n    # The stream or file (file-like object) to which the blob will be written\n    # import io\n    # file_obj = io.BytesIO()\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client-side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` in that it doesn't\n    # retrieve metadata from Google Cloud Storage. As we don't use metadata in\n    # this example, using `Bucket.blob` is preferred here.\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_file(file_obj)\n\n    print(f\"Downloaded blob {source_blob_name} to file-like object.\")\n\n    return file_obj\n    # Before reading from file_obj, remember to rewind with file_obj.seek(0).\n\n# [END storage_stream_file_download]\n", "samples/snippets/storage_enable_bucket_lifecycle_management.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_bucket_lifecycle_management]\nfrom google.cloud import storage\n\n\ndef enable_bucket_lifecycle_management(bucket_name):\n    \"\"\"Enable lifecycle management for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    rules = bucket.lifecycle_rules\n\n    print(f\"Lifecycle management rules for bucket {bucket_name} are {list(rules)}\")\n    bucket.add_lifecycle_delete_rule(age=2)\n    bucket.patch()\n\n    rules = bucket.lifecycle_rules\n    print(f\"Lifecycle management is enable for bucket {bucket_name} and the rules are {list(rules)}\")\n\n    return bucket\n\n\n# [END storage_enable_bucket_lifecycle_management]\n\nif __name__ == \"__main__\":\n    enable_bucket_lifecycle_management(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket]\nfrom google.cloud import storage\n\n\ndef create_bucket(bucket_name):\n    \"\"\"Creates a new bucket.\"\"\"\n    # bucket_name = \"your-new-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.create_bucket(bucket_name)\n\n    print(f\"Bucket {bucket.name} created\")\n\n\n# [END storage_create_bucket]\n\nif __name__ == \"__main__\":\n    create_bucket(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_hmac_key]\nfrom google.cloud import storage\n\n\ndef create_key(project_id, service_account_email):\n    \"\"\"\n    Create a new HMAC key using the given project and service account.\n    \"\"\"\n    # project_id = 'Your Google Cloud project ID'\n    # service_account_email = 'Service account used to generate the HMAC key'\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key, secret = storage_client.create_hmac_key(\n        service_account_email=service_account_email, project_id=project_id\n    )\n\n    print(f\"The base64 encoded secret is {secret}\")\n    print(\"Do not miss that secret, there is no API to recover it.\")\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_create_hmac_key]\n\nif __name__ == \"__main__\":\n    create_key(project_id=sys.argv[1], service_account_email=sys.argv[2])\n", "samples/snippets/storage_delete_bucket.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_bucket]\nfrom google.cloud import storage\n\n\ndef delete_bucket(bucket_name):\n    \"\"\"Deletes a bucket. The bucket must be empty.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.delete()\n\n    print(f\"Bucket {bucket.name} deleted\")\n\n\n# [END storage_delete_bucket]\n\nif __name__ == \"__main__\":\n    delete_bucket(bucket_name=sys.argv[1])\n", "samples/snippets/storage_download_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_file]\nfrom google.cloud import storage\n\n\ndef download_blob(bucket_name, source_blob_name, destination_file_name):\n    \"\"\"Downloads a blob from the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object\n    # source_blob_name = \"storage-object-name\"\n\n    # The path to which the file should be downloaded\n    # destination_file_name = \"local/path/to/file\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n    # any content from Google Cloud Storage. As we don't need additional data,\n    # using `Bucket.blob` is preferred here.\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n            source_blob_name, bucket_name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_file]\n\nif __name__ == \"__main__\":\n    download_blob(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        destination_file_name=sys.argv[3],\n    )\n", "samples/snippets/storage_lock_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_lock_retention_policy]\nfrom google.cloud import storage\n\n\ndef lock_retention_policy(bucket_name):\n    \"\"\"Locks the retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    # get_bucket gets the current metageneration value for the bucket,\n    # required by lock_retention_policy.\n    bucket = storage_client.get_bucket(bucket_name)\n\n    # Warning: Once a retention policy is locked it cannot be unlocked\n    # and retention period can only be increased.\n    bucket.lock_retention_policy()\n\n    print(f\"Retention policy for {bucket_name} is now locked\")\n    print(\n        f\"Retention policy effective as of {bucket.retention_policy_effective_time}\"\n    )\n\n\n# [END storage_lock_retention_policy]\n\n\nif __name__ == \"__main__\":\n    lock_retention_policy(bucket_name=sys.argv[1])\n", "samples/snippets/storage_enable_uniform_bucket_level_access.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_uniform_bucket_level_access]\nfrom google.cloud import storage\n\n\ndef enable_uniform_bucket_level_access(bucket_name):\n    \"\"\"Enable uniform bucket-level access for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.patch()\n\n    print(\n        f\"Uniform bucket-level access was enabled for {bucket.name}.\"\n    )\n\n\n# [END storage_enable_uniform_bucket_level_access]\n\nif __name__ == \"__main__\":\n    enable_uniform_bucket_level_access(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_upload_file]\nfrom google.cloud import storage\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The path to your file to upload\n    # source_file_name = \"local/path/to/file\"\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to upload is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    generation_match_precondition = 0\n\n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\n    print(\n        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n    )\n\n\n# [END storage_upload_file]\n\nif __name__ == \"__main__\":\n    upload_blob(\n        bucket_name=sys.argv[1],\n        source_file_name=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n    )\n", "samples/snippets/storage_print_pubsub_bucket_notification.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that gets a notification configuration for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_print_pubsub_bucket_notification]\nfrom google.cloud import storage\n\n\ndef print_pubsub_bucket_notification(bucket_name, notification_id):\n    \"\"\"Gets a notification configuration for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of the notification\n    # notification_id = \"your-notification-id\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notification = bucket.get_notification(notification_id)\n\n    print(f\"Notification ID: {notification.notification_id}\")\n    print(f\"Topic Name: {notification.topic_name}\")\n    print(f\"Event Types: {notification.event_types}\")\n    print(f\"Custom Attributes: {notification.custom_attributes}\")\n    print(f\"Payload Format: {notification.payload_format}\")\n    print(f\"Blob Name Prefix: {notification.blob_name_prefix}\")\n    print(f\"Etag: {notification.etag}\")\n    print(f\"Self Link: {notification.self_link}\")\n\n# [END storage_print_pubsub_bucket_notification]\n\n\nif __name__ == \"__main__\":\n    print_pubsub_bucket_notification(bucket_name=sys.argv[1], notification_id=sys.argv[2])\n", "samples/snippets/storage_set_rpo_default.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that sets the replication behavior or recovery point objective (RPO) to default.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_set_rpo_default]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import RPO_DEFAULT\n\n\ndef set_rpo_default(bucket_name):\n    \"\"\"Sets the RPO to DEFAULT, disabling the turbo replication feature\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.rpo = RPO_DEFAULT\n    bucket.patch()\n\n    print(f\"RPO is set to DEFAULT for {bucket.name}.\")\n\n\n# [END storage_set_rpo_default]\n\nif __name__ == \"__main__\":\n    set_rpo_default(bucket_name=sys.argv[1])\n", "samples/snippets/storage_transfer_manager_download_bucket.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_download_bucket]\ndef download_bucket_with_transfer_manager(\n    bucket_name, destination_directory=\"\", workers=8, max_results=1000\n):\n    \"\"\"Download all of the blobs in a bucket, concurrently in a process pool.\n\n    The filename of each blob once downloaded is derived from the blob name and\n    the `destination_directory `parameter. For complete control of the filename\n    of each blob, use transfer_manager.download_many() instead.\n\n    Directories will be created automatically as needed, for instance to\n    accommodate blob names that include slashes.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The directory on your computer to which to download all of the files. This\n    # string is prepended (with os.path.join()) to the name of each blob to form\n    # the full path. Relative paths and absolute paths are both accepted. An\n    # empty string means \"the current working directory\". Note that this\n    # parameter allows accepts directory traversal (\"../\" etc.) and is not\n    # intended for unsanitized end user input.\n    # destination_directory = \"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    # The maximum number of results to fetch from bucket.list_blobs(). This\n    # sample code fetches all of the blobs up to max_results and queues them all\n    # for download at once. Though they will still be executed in batches up to\n    # the processes limit, queueing them all at once can be taxing on system\n    # memory if buckets are very large. Adjust max_results as needed for your\n    # system environment, or set it to None if you are sure the bucket is not\n    # too large to hold in memory easily.\n    # max_results=1000\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n\n    results = transfer_manager.download_many_to_path(\n        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n    )\n\n    for name, result in zip(blob_names, results):\n        # The results list is either `None` or an exception for each blob in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to download {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n# [END storage_transfer_manager_download_bucket]\n", "samples/snippets/storage_get_bucket_labels.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_get_bucket_labels]\nimport pprint\n# [END storage_get_bucket_labels]\nimport sys\n# [START storage_get_bucket_labels]\n\nfrom google.cloud import storage\n\n\ndef get_bucket_labels(bucket_name):\n    \"\"\"Prints out a bucket's labels.\"\"\"\n    # bucket_name = 'your-bucket-name'\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n\n    labels = bucket.labels\n    pprint.pprint(labels)\n\n\n# [END storage_get_bucket_labels]\n\nif __name__ == \"__main__\":\n    get_bucket_labels(bucket_name=sys.argv[1])\n", "samples/snippets/storage_set_public_access_prevention_enforced.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_public_access_prevention_enforced]\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_ENFORCED\n\n\ndef set_public_access_prevention_enforced(bucket_name):\n    \"\"\"Enforce public access prevention for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.public_access_prevention = (\n        PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n    bucket.patch()\n\n    print(f\"Public access prevention is set to enforced for {bucket.name}.\")\n\n\n# [END storage_set_public_access_prevention_enforced]\n\nif __name__ == \"__main__\":\n    set_public_access_prevention_enforced(bucket_name=sys.argv[1])\n", "samples/snippets/storage_set_bucket_default_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_bucket_default_kms_key]\nfrom google.cloud import storage\n\n\ndef enable_default_kms_key(bucket_name, kms_key_name):\n    \"\"\"Sets a bucket's default KMS key.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # kms_key_name = \"projects/PROJ/locations/LOC/keyRings/RING/cryptoKey/KEY\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.default_kms_key_name = kms_key_name\n    bucket.patch()\n\n    print(\n        \"Set default KMS key for bucket {} to {}.\".format(\n            bucket.name, bucket.default_kms_key_name\n        )\n    )\n\n\n# [END storage_set_bucket_default_kms_key]\n\nif __name__ == \"__main__\":\n    enable_default_kms_key(bucket_name=sys.argv[1], kms_key_name=sys.argv[2])\n", "samples/snippets/storage_object_get_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_object_get_kms_key]\nfrom google.cloud import storage\n\n\ndef object_get_kms_key(bucket_name, blob_name):\n    \"\"\"Retrieve the KMS key of a blob\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.get_blob(blob_name)\n\n    kms_key = blob.kms_key_name\n\n    print(f\"The KMS key of a blob is {blob.kms_key_name}\")\n    return kms_key\n\n\n# [END storage_object_get_kms_key]\n\nif __name__ == \"__main__\":\n    object_get_kms_key(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_client_endpoint.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates a new bucket in a specified region\n\"\"\"\n\n# [START storage_set_client_endpoint]\n\nfrom google.cloud import storage\n\n\ndef set_client_endpoint(api_endpoint):\n    \"\"\"Initiates client with specified endpoint.\"\"\"\n    # api_endpoint = 'https://storage.googleapis.com'\n\n    storage_client = storage.Client(client_options={'api_endpoint': api_endpoint})\n\n    print(f\"client initiated with endpoint: {storage_client._connection.API_BASE_URL}\")\n\n    return storage_client\n\n\n# [END storage_set_client_endpoint]\n\nif __name__ == \"__main__\":\n    set_client_endpoint(api_endpoint=sys.argv[1])\n", "samples/snippets/storage_disable_uniform_bucket_level_access.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_uniform_bucket_level_access]\nfrom google.cloud import storage\n\n\ndef disable_uniform_bucket_level_access(bucket_name):\n    \"\"\"Disable uniform bucket-level access for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = False\n    bucket.patch()\n\n    print(\n        f\"Uniform bucket-level access was disabled for {bucket.name}.\"\n    )\n\n\n# [END storage_disable_uniform_bucket_level_access]\n\nif __name__ == \"__main__\":\n    disable_uniform_bucket_level_access(bucket_name=sys.argv[1])\n", "samples/snippets/storage_view_bucket_iam_members.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_view_bucket_iam_members]\nfrom google.cloud import storage\n\n\ndef view_bucket_iam_members(bucket_name):\n    \"\"\"View IAM Policy for a bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    for binding in policy.bindings:\n        print(f\"Role: {binding['role']}, Members: {binding['members']}\")\n\n\n# [END storage_view_bucket_iam_members]\n\n\nif __name__ == \"__main__\":\n    view_bucket_iam_members(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_encrypted_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_upload_encrypted_file]\nimport base64\n# [END storage_upload_encrypted_file]\nimport sys\n# [START storage_upload_encrypted_file]\n\nfrom google.cloud import storage\n\n\ndef upload_encrypted_blob(\n    bucket_name,\n    source_file_name,\n    destination_blob_name,\n    base64_encryption_key,\n):\n    \"\"\"Uploads a file to a Google Cloud Storage bucket using a custom\n    encryption key.\n\n    The file will be encrypted by Google Cloud Storage and only\n    retrievable using the provided encryption key.\n    \"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_file_name = \"local/path/to/file\"\n    # destination_blob_name = \"storage-object-name\"\n    # base64_encryption_key = \"TIbv/fjexq+VmtXzAlc63J4z5kFmWJ6NdAPQulQBT7g=\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Encryption key must be an AES256 key represented as a bytestring with\n    # 32 bytes. Since it's passed in as a base64 encoded string, it needs\n    # to be decoded.\n    encryption_key = base64.b64decode(base64_encryption_key)\n    blob = bucket.blob(\n        destination_blob_name, encryption_key=encryption_key\n    )\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to upload is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    generation_match_precondition = 0\n\n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\n    print(\n        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n    )\n\n\n# [END storage_upload_encrypted_file]\n\nif __name__ == \"__main__\":\n    upload_encrypted_blob(\n        bucket_name=sys.argv[1],\n        source_file_name=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n        base64_encryption_key=sys.argv[4],\n    )\n", "samples/snippets/storage_move_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_move_file]\nfrom google.cloud import storage\n\n\ndef move_blob(bucket_name, blob_name, destination_bucket_name, destination_blob_name,):\n    \"\"\"Moves a blob from one bucket to another with a new name.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of your GCS object\n    # blob_name = \"your-object-name\"\n    # The ID of the bucket to move the object to\n    # destination_bucket_name = \"destination-bucket-name\"\n    # The ID of your new GCS object (optional)\n    # destination_blob_name = \"destination-object-name\"\n\n    storage_client = storage.Client()\n\n    source_bucket = storage_client.bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.bucket(destination_bucket_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n    destination_generation_match_precondition = 0\n\n    blob_copy = source_bucket.copy_blob(\n        source_blob, destination_bucket, destination_blob_name, if_generation_match=destination_generation_match_precondition,\n    )\n    source_bucket.delete_blob(blob_name)\n\n    print(\n        \"Blob {} in bucket {} moved to blob {} in bucket {}.\".format(\n            source_blob.name,\n            source_bucket.name,\n            blob_copy.name,\n            destination_bucket.name,\n        )\n    )\n\n\n# [END storage_move_file]\n\nif __name__ == \"__main__\":\n    move_blob(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        destination_bucket_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n    )\n", "samples/snippets/storage_delete_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_hmac_key]\nfrom google.cloud import storage\n\n\ndef delete_key(access_id, project_id):\n    \"\"\"\n    Delete the HMAC key with the given access ID. Key must have state INACTIVE\n    in order to succeed.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an HMAC key (must be in INACTIVE state)\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n    hmac_key.delete()\n\n    print(\n        \"The key is deleted, though it may still appear in list_hmac_keys()\"\n        \" results.\"\n    )\n\n\n# [END storage_delete_hmac_key]\n\nif __name__ == \"__main__\":\n    delete_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_transfer_manager_upload_chunks_concurrently.py": "# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_upload_chunks_concurrently]\ndef upload_chunks_concurrently(\n    bucket_name,\n    source_filename,\n    destination_blob_name,\n    chunk_size=32 * 1024 * 1024,\n    workers=8,\n):\n    \"\"\"Upload a single file, in chunks, concurrently in a process pool.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The path to your file to upload\n    # source_filename = \"local/path/to/file\"\n\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    # The size of each chunk. The performance impact of this value depends on\n    # the use case. The remote service has a minimum of 5 MiB and a maximum of\n    # 5 GiB.\n    # chunk_size = 32 * 1024 * 1024 (32 MiB)\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case. Each additional process\n    # occupies some CPU and memory resources until finished. Threads can be used\n    # instead of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    transfer_manager.upload_chunks_concurrently(\n        source_filename, blob, chunk_size=chunk_size, max_workers=workers\n    )\n\n    print(f\"File {source_filename} uploaded to {destination_blob_name}.\")\n\n\n# [END storage_transfer_manager_upload_chunks_concurrently]\n", "samples/snippets/storage_add_bucket_label.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_add_bucket_label]\nimport pprint\n# [END storage_add_bucket_label]\nimport sys\n# [START storage_add_bucket_label]\n\nfrom google.cloud import storage\n\n\ndef add_bucket_label(bucket_name):\n    \"\"\"Add a label to a bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    labels = bucket.labels\n    labels[\"example\"] = \"label\"\n    bucket.labels = labels\n    bucket.patch()\n\n    print(f\"Updated labels on {bucket.name}.\")\n    pprint.pprint(bucket.labels)\n\n\n# [END storage_add_bucket_label]\n\nif __name__ == \"__main__\":\n    add_bucket_label(bucket_name=sys.argv[1])\n", "samples/snippets/storage_set_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_retention_policy]\nfrom google.cloud import storage\n\n\ndef set_retention_policy(bucket_name, retention_period):\n    \"\"\"Defines a retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n    # retention_period = 10\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.retention_period = retention_period\n    bucket.patch()\n\n    print(\n        \"Bucket {} retention period set for {} seconds\".format(\n            bucket.name, bucket.retention_period\n        )\n    )\n\n\n# [END storage_set_retention_policy]\n\n\nif __name__ == \"__main__\":\n    set_retention_policy(bucket_name=sys.argv[1], retention_period=sys.argv[2])\n", "scripts/readme-gen/readme_gen.py": "#!/usr/bin/env python\n\n# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Generates READMEs using configuration defined in yaml.\"\"\"\n\nimport argparse\nimport io\nimport os\nimport subprocess\n\nimport jinja2\nimport yaml\n\n\njinja_env = jinja2.Environment(\n    trim_blocks=True,\n    loader=jinja2.FileSystemLoader(\n        os.path.abspath(os.path.join(os.path.dirname(__file__), \"templates\"))\n    ),\n    autoescape=True,\n)\n\nREADME_TMPL = jinja_env.get_template(\"README.tmpl.rst\")\n\n\ndef get_help(file):\n    return subprocess.check_output([\"python\", file, \"--help\"]).decode()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"source\")\n    parser.add_argument(\"--destination\", default=\"README.rst\")\n\n    args = parser.parse_args()\n\n    source = os.path.abspath(args.source)\n    root = os.path.dirname(source)\n    destination = os.path.join(root, args.destination)\n\n    jinja_env.globals[\"get_help\"] = get_help\n\n    with io.open(source, \"r\") as f:\n        config = yaml.load(f)\n\n    # This allows get_help to execute in the right directory.\n    os.chdir(root)\n\n    output = README_TMPL.render(config)\n\n    with io.open(destination, \"w\") as f:\n        f.write(output)\n\n\nif __name__ == \"__main__\":\n    main()\n", "google/cloud/storage/_signing.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport base64\nimport binascii\nimport collections\nimport datetime\nimport hashlib\nimport json\n\nimport http\nimport urllib\n\nimport google.auth.credentials\n\nfrom google.auth import exceptions\nfrom google.auth.transport import requests\nfrom google.cloud import _helpers\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\n\n\n# `google.cloud.storage._signing.NOW` is deprecated.\n# Use `_NOW(_UTC)` instead.\nNOW = datetime.datetime.utcnow\n\nSERVICE_ACCOUNT_URL = (\n    \"https://googleapis.dev/python/google-api-core/latest/\"\n    \"auth.html#setting-up-a-service-account\"\n)\n\n\ndef ensure_signed_credentials(credentials):\n    \"\"\"Raise AttributeError if the credentials are unsigned.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: The credentials used to create a private key\n                        for signing text.\n\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n    \"\"\"\n    if not isinstance(credentials, google.auth.credentials.Signing):\n        raise AttributeError(\n            \"you need a private key to sign credentials.\"\n            \"the credentials you are currently using {} \"\n            \"just contains a token. see {} for more \"\n            \"details.\".format(type(credentials), SERVICE_ACCOUNT_URL)\n        )\n\n\ndef get_signed_query_params_v2(credentials, expiration, string_to_sign):\n    \"\"\"Gets query parameters for creating a signed URL.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: The credentials used to create a private key\n                        for signing text.\n\n    :type expiration: int or long\n    :param expiration: When the signed URL should expire.\n\n    :type string_to_sign: str\n    :param string_to_sign: The string to be signed by the credentials.\n\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: dict\n    :returns: Query parameters matching the signing credentials with a\n              signed payload.\n    \"\"\"\n    ensure_signed_credentials(credentials)\n    signature_bytes = credentials.sign_bytes(string_to_sign.encode(\"ascii\"))\n    signature = base64.b64encode(signature_bytes)\n    service_account_name = credentials.signer_email\n    return {\n        \"GoogleAccessId\": service_account_name,\n        \"Expires\": expiration,\n        \"Signature\": signature,\n    }\n\n\ndef get_expiration_seconds_v2(expiration):\n    \"\"\"Convert 'expiration' to a number of seconds in the future.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n\n    :rtype: int\n    :returns: a timestamp as an absolute number of seconds since epoch.\n    \"\"\"\n    # If it's a timedelta, add it to `now` in UTC.\n    if isinstance(expiration, datetime.timedelta):\n        now = _NOW(_UTC)\n        expiration = now + expiration\n\n    # If it's a datetime, convert to a timestamp.\n    if isinstance(expiration, datetime.datetime):\n        micros = _helpers._microseconds_from_datetime(expiration)\n        expiration = micros // 10**6\n\n    if not isinstance(expiration, int):\n        raise TypeError(\n            \"Expected an integer timestamp, datetime, or \"\n            \"timedelta. Got %s\" % type(expiration)\n        )\n    return expiration\n\n\n_EXPIRATION_TYPES = (int, datetime.datetime, datetime.timedelta)\n\n\ndef get_expiration_seconds_v4(expiration):\n    \"\"\"Convert 'expiration' to a number of seconds offset from the current time.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`ValueError` when expiration is too large.\n    :rtype: Integer\n    :returns: seconds in the future when the signed URL will expire\n    \"\"\"\n    if not isinstance(expiration, _EXPIRATION_TYPES):\n        raise TypeError(\n            \"Expected an integer timestamp, datetime, or \"\n            \"timedelta. Got %s\" % type(expiration)\n        )\n\n    now = _NOW(_UTC)\n\n    if isinstance(expiration, int):\n        seconds = expiration\n\n    if isinstance(expiration, datetime.datetime):\n        if expiration.tzinfo is None:\n            expiration = expiration.replace(tzinfo=_helpers.UTC)\n        expiration = expiration - now\n\n    if isinstance(expiration, datetime.timedelta):\n        seconds = int(expiration.total_seconds())\n\n    if seconds > SEVEN_DAYS:\n        raise ValueError(f\"Max allowed expiration interval is seven days {SEVEN_DAYS}\")\n\n    return seconds\n\n\ndef get_canonical_headers(headers):\n    \"\"\"Canonicalize headers for signing.\n\n    See:\n    https://cloud.google.com/storage/docs/access-control/signed-urls#about-canonical-extension-headers\n\n    :type headers: Union[dict|List(Tuple(str,str))]\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :rtype: str\n    :returns: List of headers, normalized / sortted per the URL refernced above.\n    \"\"\"\n    if headers is None:\n        headers = []\n    elif isinstance(headers, dict):\n        headers = list(headers.items())\n\n    if not headers:\n        return [], []\n\n    normalized = collections.defaultdict(list)\n    for key, val in headers:\n        key = key.lower().strip()\n        val = \" \".join(val.split())\n        normalized[key].append(val)\n\n    ordered_headers = sorted((key, \",\".join(val)) for key, val in normalized.items())\n\n    canonical_headers = [\"{}:{}\".format(*item) for item in ordered_headers]\n    return canonical_headers, ordered_headers\n\n\n_Canonical = collections.namedtuple(\n    \"_Canonical\", [\"method\", \"resource\", \"query_parameters\", \"headers\"]\n)\n\n\ndef canonicalize_v2(method, resource, query_parameters, headers):\n    \"\"\"Canonicalize method, resource per the V2 spec.\n\n    :type method: str\n    :param method: The HTTP verb that will be used when requesting the URL.\n                   Defaults to ``'GET'``. If method is ``'RESUMABLE'`` then the\n                   signature will additionally contain the `x-goog-resumable`\n                   header, and the method changed to POST. See the signed URL\n                   docs regarding this flow:\n                   https://cloud.google.com/storage/docs/access-control/signed-urls\n\n    :type resource: str\n    :param resource: A pointer to a specific resource\n                     (typically, ``/bucket-name/path/to/blob.txt``).\n\n    :type query_parameters: dict\n    :param query_parameters:\n        (Optional) Additional query parameters to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n    :type headers: Union[dict|List(Tuple(str,str))]\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :rtype: :class:_Canonical\n    :returns: Canonical method, resource, query_parameters, and headers.\n    \"\"\"\n    headers, _ = get_canonical_headers(headers)\n\n    if method == \"RESUMABLE\":\n        method = \"POST\"\n        headers.append(\"x-goog-resumable:start\")\n\n    if query_parameters is None:\n        return _Canonical(method, resource, [], headers)\n\n    normalized_qp = sorted(\n        (key.lower(), value and value.strip() or \"\")\n        for key, value in query_parameters.items()\n    )\n    encoded_qp = urllib.parse.urlencode(normalized_qp)\n    canonical_resource = f\"{resource}?{encoded_qp}\"\n    return _Canonical(method, canonical_resource, normalized_qp, headers)\n\n\ndef generate_signed_url_v2(\n    credentials,\n    resource,\n    expiration,\n    api_access_endpoint=\"\",\n    method=\"GET\",\n    content_md5=None,\n    content_type=None,\n    response_type=None,\n    response_disposition=None,\n    generation=None,\n    headers=None,\n    query_parameters=None,\n    service_account_email=None,\n    access_token=None,\n):\n    \"\"\"Generate a V2 signed URL to provide query-string auth'n to a resource.\n\n    .. note::\n\n        Assumes ``credentials`` implements the\n        :class:`google.auth.credentials.Signing` interface. Also assumes\n        ``credentials`` has a ``signer_email`` property which\n        identifies the credentials.\n\n    .. note::\n\n        If you are on Google Compute Engine, you can't generate a signed URL.\n        If you'd like to be able to generate a signed URL from GCE, you can use a\n        standard service account from a JSON file rather than a GCE service account.\n\n    See headers [reference](https://cloud.google.com/storage/docs/reference-headers)\n    for more details on optional arguments.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: Credentials object with an associated private key to\n                        sign text.\n\n    :type resource: str\n    :param resource: A pointer to a specific resource\n                     (typically, ``/bucket-name/path/to/blob.txt``).\n                     Caller should have already URL-encoded the value.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :type api_access_endpoint: str\n    :param api_access_endpoint: (Optional) URI base. Defaults to empty string.\n\n    :type method: str\n    :param method: The HTTP verb that will be used when requesting the URL.\n                   Defaults to ``'GET'``. If method is ``'RESUMABLE'`` then the\n                   signature will additionally contain the `x-goog-resumable`\n                   header, and the method changed to POST. See the signed URL\n                   docs regarding this flow:\n                   https://cloud.google.com/storage/docs/access-control/signed-urls\n\n\n    :type content_md5: str\n    :param content_md5: (Optional) The MD5 hash of the object referenced by\n                        ``resource``.\n\n    :type content_type: str\n    :param content_type: (Optional) The content type of the object referenced\n                         by ``resource``.\n\n    :type response_type: str\n    :param response_type: (Optional) Content type of responses to requests for\n                          the signed URL. Ignored if content_type is set on\n                          object/blob metadata.\n\n    :type response_disposition: str\n    :param response_disposition: (Optional) Content disposition of responses to\n                                 requests for the signed URL.\n\n    :type generation: str\n    :param generation: (Optional) A value that indicates which generation of\n                       the resource to fetch.\n\n    :type headers: Union[dict|List(Tuple(str,str))]\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :type service_account_email: str\n    :param service_account_email: (Optional) E-mail address of the service account.\n\n    :type access_token: str\n    :param access_token: (Optional) Access token for a service account.\n\n    :type query_parameters: dict\n    :param query_parameters:\n        (Optional) Additional query parameters to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: str\n    :returns: A signed URL you can use to access the resource\n              until expiration.\n    \"\"\"\n    expiration_stamp = get_expiration_seconds_v2(expiration)\n\n    canonical = canonicalize_v2(method, resource, query_parameters, headers)\n\n    # Generate the string to sign.\n    elements_to_sign = [\n        canonical.method,\n        content_md5 or \"\",\n        content_type or \"\",\n        str(expiration_stamp),\n    ]\n    elements_to_sign.extend(canonical.headers)\n    elements_to_sign.append(canonical.resource)\n    string_to_sign = \"\\n\".join(elements_to_sign)\n\n    # If you are on Google Compute Engine, you can't generate a signed URL.\n    # See https://github.com/googleapis/google-cloud-python/issues/922\n    # Set the right query parameters.\n    if access_token and service_account_email:\n        signature = _sign_message(string_to_sign, access_token, service_account_email)\n        signed_query_params = {\n            \"GoogleAccessId\": service_account_email,\n            \"Expires\": expiration_stamp,\n            \"Signature\": signature,\n        }\n    else:\n        signed_query_params = get_signed_query_params_v2(\n            credentials, expiration_stamp, string_to_sign\n        )\n\n    if response_type is not None:\n        signed_query_params[\"response-content-type\"] = response_type\n    if response_disposition is not None:\n        signed_query_params[\"response-content-disposition\"] = response_disposition\n    if generation is not None:\n        signed_query_params[\"generation\"] = generation\n\n    signed_query_params.update(canonical.query_parameters)\n    sorted_signed_query_params = sorted(signed_query_params.items())\n\n    # Return the built URL.\n    return \"{endpoint}{resource}?{querystring}\".format(\n        endpoint=api_access_endpoint,\n        resource=resource,\n        querystring=urllib.parse.urlencode(sorted_signed_query_params),\n    )\n\n\nSEVEN_DAYS = 7 * 24 * 60 * 60  # max age for V4 signed URLs.\nDEFAULT_ENDPOINT = \"https://storage.googleapis.com\"\n\n\ndef generate_signed_url_v4(\n    credentials,\n    resource,\n    expiration,\n    api_access_endpoint=DEFAULT_ENDPOINT,\n    method=\"GET\",\n    content_md5=None,\n    content_type=None,\n    response_type=None,\n    response_disposition=None,\n    generation=None,\n    headers=None,\n    query_parameters=None,\n    service_account_email=None,\n    access_token=None,\n    _request_timestamp=None,  # for testing only\n):\n    \"\"\"Generate a V4 signed URL to provide query-string auth'n to a resource.\n\n    .. note::\n\n        Assumes ``credentials`` implements the\n        :class:`google.auth.credentials.Signing` interface. Also assumes\n        ``credentials`` has a ``signer_email`` property which\n        identifies the credentials.\n\n    .. note::\n\n        If you are on Google Compute Engine, you can't generate a signed URL.\n        If you'd like to be able to generate a signed URL from GCE,you can use a\n        standard service account from a JSON file rather than a GCE service account.\n\n    See headers [reference](https://cloud.google.com/storage/docs/reference-headers)\n    for more details on optional arguments.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: Credentials object with an associated private key to\n                        sign text. That credentials must provide signer_email\n                        only if service_account_email and access_token are not\n                        passed.\n\n    :type resource: str\n    :param resource: A pointer to a specific resource\n                     (typically, ``/bucket-name/path/to/blob.txt``).\n                     Caller should have already URL-encoded the value.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :type api_access_endpoint: str\n    :param api_access_endpoint: URI base. Defaults to\n                                \"https://storage.googleapis.com/\"\n\n    :type method: str\n    :param method: The HTTP verb that will be used when requesting the URL.\n                   Defaults to ``'GET'``. If method is ``'RESUMABLE'`` then the\n                   signature will additionally contain the `x-goog-resumable`\n                   header, and the method changed to POST. See the signed URL\n                   docs regarding this flow:\n                   https://cloud.google.com/storage/docs/access-control/signed-urls\n\n\n    :type content_md5: str\n    :param content_md5: (Optional) The MD5 hash of the object referenced by\n                        ``resource``.\n\n    :type content_type: str\n    :param content_type: (Optional) The content type of the object referenced\n                         by ``resource``.\n\n    :type response_type: str\n    :param response_type: (Optional) Content type of responses to requests for\n                          the signed URL. Ignored if content_type is set on\n                          object/blob metadata.\n\n    :type response_disposition: str\n    :param response_disposition: (Optional) Content disposition of responses to\n                                 requests for the signed URL.\n\n    :type generation: str\n    :param generation: (Optional) A value that indicates which generation of\n                       the resource to fetch.\n\n    :type headers: dict\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :type query_parameters: dict\n    :param query_parameters:\n        (Optional) Additional query parameters to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n    :type service_account_email: str\n    :param service_account_email: (Optional) E-mail address of the service account.\n\n    :type access_token: str\n    :param access_token: (Optional) Access token for a service account.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: str\n    :returns: A signed URL you can use to access the resource\n              until expiration.\n    \"\"\"\n    expiration_seconds = get_expiration_seconds_v4(expiration)\n\n    if _request_timestamp is None:\n        request_timestamp, datestamp = get_v4_now_dtstamps()\n    else:\n        request_timestamp = _request_timestamp\n        datestamp = _request_timestamp[:8]\n\n    # If you are on Google Compute Engine, you can't generate a signed URL.\n    # See https://github.com/googleapis/google-cloud-python/issues/922\n    client_email = service_account_email\n    if not access_token or not service_account_email:\n        ensure_signed_credentials(credentials)\n        client_email = credentials.signer_email\n\n    credential_scope = f\"{datestamp}/auto/storage/goog4_request\"\n    credential = f\"{client_email}/{credential_scope}\"\n\n    if headers is None:\n        headers = {}\n\n    if content_type is not None:\n        headers[\"Content-Type\"] = content_type\n\n    if content_md5 is not None:\n        headers[\"Content-MD5\"] = content_md5\n\n    header_names = [key.lower() for key in headers]\n    if \"host\" not in header_names:\n        headers[\"Host\"] = urllib.parse.urlparse(api_access_endpoint).netloc\n\n    if method.upper() == \"RESUMABLE\":\n        method = \"POST\"\n        headers[\"x-goog-resumable\"] = \"start\"\n\n    canonical_headers, ordered_headers = get_canonical_headers(headers)\n    canonical_header_string = (\n        \"\\n\".join(canonical_headers) + \"\\n\"\n    )  # Yes, Virginia, the extra newline is part of the spec.\n    signed_headers = \";\".join([key for key, _ in ordered_headers])\n\n    if query_parameters is None:\n        query_parameters = {}\n    else:\n        query_parameters = {key: value or \"\" for key, value in query_parameters.items()}\n\n    query_parameters[\"X-Goog-Algorithm\"] = \"GOOG4-RSA-SHA256\"\n    query_parameters[\"X-Goog-Credential\"] = credential\n    query_parameters[\"X-Goog-Date\"] = request_timestamp\n    query_parameters[\"X-Goog-Expires\"] = expiration_seconds\n    query_parameters[\"X-Goog-SignedHeaders\"] = signed_headers\n\n    if response_type is not None:\n        query_parameters[\"response-content-type\"] = response_type\n\n    if response_disposition is not None:\n        query_parameters[\"response-content-disposition\"] = response_disposition\n\n    if generation is not None:\n        query_parameters[\"generation\"] = generation\n\n    canonical_query_string = _url_encode(query_parameters)\n\n    lowercased_headers = dict(ordered_headers)\n\n    if \"x-goog-content-sha256\" in lowercased_headers:\n        payload = lowercased_headers[\"x-goog-content-sha256\"]\n    else:\n        payload = \"UNSIGNED-PAYLOAD\"\n\n    canonical_elements = [\n        method,\n        resource,\n        canonical_query_string,\n        canonical_header_string,\n        signed_headers,\n        payload,\n    ]\n    canonical_request = \"\\n\".join(canonical_elements)\n\n    canonical_request_hash = hashlib.sha256(\n        canonical_request.encode(\"ascii\")\n    ).hexdigest()\n\n    string_elements = [\n        \"GOOG4-RSA-SHA256\",\n        request_timestamp,\n        credential_scope,\n        canonical_request_hash,\n    ]\n    string_to_sign = \"\\n\".join(string_elements)\n\n    if access_token and service_account_email:\n        signature = _sign_message(string_to_sign, access_token, service_account_email)\n        signature_bytes = base64.b64decode(signature)\n        signature = binascii.hexlify(signature_bytes).decode(\"ascii\")\n    else:\n        signature_bytes = credentials.sign_bytes(string_to_sign.encode(\"ascii\"))\n        signature = binascii.hexlify(signature_bytes).decode(\"ascii\")\n\n    return \"{}{}?{}&X-Goog-Signature={}\".format(\n        api_access_endpoint, resource, canonical_query_string, signature\n    )\n\n\ndef get_v4_now_dtstamps():\n    \"\"\"Get current timestamp and datestamp in V4 valid format.\n\n    :rtype: str, str\n    :returns: Current timestamp, datestamp.\n    \"\"\"\n    now = _NOW(_UTC).replace(tzinfo=None)\n    timestamp = now.strftime(\"%Y%m%dT%H%M%SZ\")\n    datestamp = now.date().strftime(\"%Y%m%d\")\n    return timestamp, datestamp\n\n\ndef _sign_message(message, access_token, service_account_email):\n    \"\"\"Signs a message.\n\n    :type message: str\n    :param message: The message to be signed.\n\n    :type access_token: str\n    :param access_token: Access token for a service account.\n\n\n    :type service_account_email: str\n    :param service_account_email: E-mail address of the service account.\n\n    :raises: :exc:`TransportError` if an `access_token` is unauthorized.\n\n    :rtype: str\n    :returns: The signature of the message.\n\n    \"\"\"\n    message = _helpers._to_bytes(message)\n\n    method = \"POST\"\n    url = \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/{}:signBlob?alt=json\".format(\n        service_account_email\n    )\n    headers = {\n        \"Authorization\": \"Bearer \" + access_token,\n        \"Content-type\": \"application/json\",\n    }\n    body = json.dumps({\"payload\": base64.b64encode(message).decode(\"utf-8\")})\n\n    request = requests.Request()\n    response = request(url=url, method=method, body=body, headers=headers)\n\n    if response.status != http.client.OK:\n        raise exceptions.TransportError(\n            f\"Error calling the IAM signBytes API: {response.data}\"\n        )\n\n    data = json.loads(response.data.decode(\"utf-8\"))\n    return data[\"signedBlob\"]\n\n\ndef _url_encode(query_params):\n    \"\"\"Encode query params into URL.\n\n    :type query_params: dict\n    :param query_params: Query params to be encoded.\n\n    :rtype: str\n    :returns: URL encoded query params.\n    \"\"\"\n    params = [\n        f\"{_quote_param(name)}={_quote_param(value)}\"\n        for name, value in query_params.items()\n    ]\n\n    return \"&\".join(sorted(params))\n\n\ndef _quote_param(param):\n    \"\"\"Quote query param.\n\n    :type param: Any\n    :param param: Query param to be encoded.\n\n    :rtype: str\n    :returns: URL encoded query param.\n    \"\"\"\n    if not isinstance(param, bytes):\n        param = str(param)\n    return urllib.parse.quote(param, safe=\"~\")\n", "google/cloud/storage/_http.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Create / interact with Google Cloud Storage connections.\"\"\"\n\nimport functools\nfrom google.cloud import _http\nfrom google.cloud.storage import __version__\nfrom google.cloud.storage import _helpers\n\n\nclass Connection(_http.JSONConnection):\n    \"\"\"A connection to Google Cloud Storage via the JSON REST API.\n\n    Mutual TLS will be enabled if the \"GOOGLE_API_USE_CLIENT_CERTIFICATE\"\n    environment variable is set to the exact string \"true\" (case-sensitive).\n\n    Mutual TLS is not compatible with any API endpoint or universe domain\n    override at this time. If such settings are enabled along with\n    \"GOOGLE_API_USE_CLIENT_CERTIFICATE\", a ValueError will be raised.\n\n    :type client: :class:`~google.cloud.storage.client.Client`\n    :param client: The client that owns the current connection.\n\n    :type client_info: :class:`~google.api_core.client_info.ClientInfo`\n    :param client_info: (Optional) instance used to generate user agent.\n\n    :type api_endpoint: str\n    :param api_endpoint: (Optional) api endpoint to use.\n    \"\"\"\n\n    DEFAULT_API_ENDPOINT = _helpers._get_default_storage_base_url()\n    DEFAULT_API_MTLS_ENDPOINT = \"https://storage.mtls.googleapis.com\"\n\n    def __init__(self, client, client_info=None, api_endpoint=None):\n        super(Connection, self).__init__(client, client_info)\n        self.API_BASE_URL = api_endpoint or self.DEFAULT_API_ENDPOINT\n        self.API_BASE_MTLS_URL = self.DEFAULT_API_MTLS_ENDPOINT\n        self.ALLOW_AUTO_SWITCH_TO_MTLS_URL = api_endpoint is None\n        self._client_info.client_library_version = __version__\n\n        # TODO: When metrics all use gccl, this should be removed #9552\n        if self._client_info.user_agent is None:  # pragma: no branch\n            self._client_info.user_agent = \"\"\n        agent_version = f\"gcloud-python/{__version__}\"\n        if agent_version not in self._client_info.user_agent:\n            self._client_info.user_agent += f\" {agent_version} \"\n\n    API_VERSION = _helpers._API_VERSION\n    \"\"\"The version of the API, used in building the API call's URL.\"\"\"\n\n    API_URL_TEMPLATE = \"{api_base_url}/storage/{api_version}{path}\"\n    \"\"\"A template for the URL of a particular API call.\"\"\"\n\n    def api_request(self, *args, **kwargs):\n        retry = kwargs.pop(\"retry\", None)\n        kwargs[\"extra_api_info\"] = _helpers._get_invocation_id()\n        call = functools.partial(super(Connection, self).api_request, *args, **kwargs)\n        if retry:\n            # If this is a ConditionalRetryPolicy, check conditions.\n            try:\n                retry = retry.get_retry_policy_if_conditions_met(**kwargs)\n            except AttributeError:  # This is not a ConditionalRetryPolicy.\n                pass\n            if retry:\n                call = retry(call)\n        return call()\n", "google/cloud/storage/hmac_key.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configure HMAC keys that can be used to authenticate requests to Google Cloud Storage.\n\nSee [HMAC keys documentation](https://cloud.google.com/storage/docs/authentication/hmackeys)\n\"\"\"\n\nfrom google.cloud.exceptions import NotFound\nfrom google.cloud._helpers import _rfc3339_nanos_to_datetime\n\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\n\n\nclass HMACKeyMetadata(object):\n    \"\"\"Metadata about an HMAC service account key withn Cloud Storage.\n\n    :type client: :class:`~google.cloud.stoage.client.Client`\n    :param client: client associated with the key metadata.\n\n    :type access_id: str\n    :param access_id: (Optional) Unique ID of an existing key.\n\n    :type project_id: str\n    :param project_id: (Optional) Project ID of an existing key.\n        Defaults to client's project.\n\n    :type user_project: str\n    :param user_project: (Optional) This parameter is currently ignored.\n    \"\"\"\n\n    ACTIVE_STATE = \"ACTIVE\"\n    \"\"\"Key is active, and may be used to sign requests.\"\"\"\n    INACTIVE_STATE = \"INACTIVE\"\n    \"\"\"Key is inactive, and may not be used to sign requests.\n\n    It can be re-activated via :meth:`update`.\n    \"\"\"\n    DELETED_STATE = \"DELETED\"\n    \"\"\"Key is deleted.  It cannot be re-activated.\"\"\"\n\n    _SETTABLE_STATES = (ACTIVE_STATE, INACTIVE_STATE)\n\n    def __init__(self, client, access_id=None, project_id=None, user_project=None):\n        self._client = client\n        self._properties = {}\n\n        if access_id is not None:\n            self._properties[\"accessId\"] = access_id\n\n        if project_id is not None:\n            self._properties[\"projectId\"] = project_id\n\n        self._user_project = user_project\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._client == other._client and self.access_id == other.access_id\n\n    def __hash__(self):\n        return hash(self._client) + hash(self.access_id)\n\n    @property\n    def access_id(self):\n        \"\"\"Access ID of the key.\n\n        :rtype: str or None\n        :returns: unique identifier of the key within a project.\n        \"\"\"\n        return self._properties.get(\"accessId\")\n\n    @property\n    def etag(self):\n        \"\"\"ETag identifying the version of the key metadata.\n\n        :rtype: str or None\n        :returns: ETag for the version of the key's metadata.\n        \"\"\"\n        return self._properties.get(\"etag\")\n\n    @property\n    def id(self):\n        \"\"\"ID of the key, including the Project ID and the Access ID.\n\n        :rtype: str or None\n        :returns: ID of the key.\n        \"\"\"\n        return self._properties.get(\"id\")\n\n    @property\n    def project(self):\n        \"\"\"Project ID associated with the key.\n\n        :rtype: str or None\n        :returns: project identfier for the key.\n        \"\"\"\n        return self._properties.get(\"projectId\")\n\n    @property\n    def service_account_email(self):\n        \"\"\"Service account e-mail address associated with the key.\n\n        :rtype: str or None\n        :returns: e-mail address for the service account which created the key.\n        \"\"\"\n        return self._properties.get(\"serviceAccountEmail\")\n\n    @property\n    def state(self):\n        \"\"\"Get / set key's state.\n\n        One of:\n            - ``ACTIVE``\n            - ``INACTIVE``\n            - ``DELETED``\n\n        :rtype: str or None\n        :returns: key's current state.\n        \"\"\"\n        return self._properties.get(\"state\")\n\n    @state.setter\n    def state(self, value):\n        self._properties[\"state\"] = value\n\n    @property\n    def time_created(self):\n        \"\"\"Retrieve the timestamp at which the HMAC key was created.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"timeCreated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def updated(self):\n        \"\"\"Retrieve the timestamp at which the HMAC key was created.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"updated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def path(self):\n        \"\"\"Resource path for the metadata's key.\"\"\"\n\n        if self.access_id is None:\n            raise ValueError(\"No 'access_id' set.\")\n\n        project = self.project\n        if project is None:\n            project = self._client.project\n\n        return f\"/projects/{project}/hmacKeys/{self.access_id}\"\n\n    @property\n    def user_project(self):\n        \"\"\"Project ID to be billed for API requests made via this bucket.\n\n        This property is currently ignored by the server.\n\n        :rtype: str\n        \"\"\"\n        return self._user_project\n\n    def exists(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Determine whether or not the key for this metadata exists.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: bool\n        :returns: True if the key exists in Cloud Storage.\n        \"\"\"\n        try:\n            qs_params = {}\n\n            if self.user_project is not None:\n                qs_params[\"userProject\"] = self.user_project\n\n            self._client._get_resource(\n                self.path,\n                query_params=qs_params,\n                timeout=timeout,\n                retry=retry,\n            )\n        except NotFound:\n            return False\n        else:\n            return True\n\n    def reload(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Reload properties from Cloud Storage.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises :class:`~google.api_core.exceptions.NotFound`:\n            if the key does not exist on the back-end.\n        \"\"\"\n        qs_params = {}\n\n        if self.user_project is not None:\n            qs_params[\"userProject\"] = self.user_project\n\n        self._properties = self._client._get_resource(\n            self.path,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def update(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY_IF_ETAG_IN_JSON):\n        \"\"\"Save writable properties to Cloud Storage.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises :class:`~google.api_core.exceptions.NotFound`:\n            if the key does not exist on the back-end.\n        \"\"\"\n        qs_params = {}\n        if self.user_project is not None:\n            qs_params[\"userProject\"] = self.user_project\n\n        payload = {\"state\": self.state}\n        self._properties = self._client._put_resource(\n            self.path,\n            payload,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def delete(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Delete the key from Cloud Storage.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises :class:`~google.api_core.exceptions.NotFound`:\n            if the key does not exist on the back-end.\n        \"\"\"\n        qs_params = {}\n        if self.user_project is not None:\n            qs_params[\"userProject\"] = self.user_project\n\n        self._client._delete_resource(\n            self.path,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n", "google/cloud/storage/retry.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helpers for configuring retries with exponential back-off.\n\nSee [Retry Strategy for Google Cloud Storage](https://cloud.google.com/storage/docs/retry-strategy#client-libraries)\n\"\"\"\n\nimport requests\nimport requests.exceptions as requests_exceptions\n\nfrom google.api_core import exceptions as api_exceptions\nfrom google.api_core import retry\nfrom google.auth import exceptions as auth_exceptions\n\n\n_RETRYABLE_TYPES = (\n    api_exceptions.TooManyRequests,  # 429\n    api_exceptions.InternalServerError,  # 500\n    api_exceptions.BadGateway,  # 502\n    api_exceptions.ServiceUnavailable,  # 503\n    api_exceptions.GatewayTimeout,  # 504\n    ConnectionError,\n    requests.ConnectionError,\n    requests_exceptions.ChunkedEncodingError,\n    requests_exceptions.Timeout,\n)\n\n\n# Some retriable errors don't have their own custom exception in api_core.\n_ADDITIONAL_RETRYABLE_STATUS_CODES = (408,)\n\n\ndef _should_retry(exc):\n    \"\"\"Predicate for determining when to retry.\"\"\"\n    if isinstance(exc, _RETRYABLE_TYPES):\n        return True\n    elif isinstance(exc, api_exceptions.GoogleAPICallError):\n        return exc.code in _ADDITIONAL_RETRYABLE_STATUS_CODES\n    elif isinstance(exc, auth_exceptions.TransportError):\n        return _should_retry(exc.args[0])\n    else:\n        return False\n\n\nDEFAULT_RETRY = retry.Retry(predicate=_should_retry)\n\"\"\"The default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES.\n\nTo modify the default retry behavior, create a new retry object modeled after\nthis one by calling it a ``with_XXX`` method. For example, to create a copy of\nDEFAULT_RETRY with a deadline of 30 seconds, pass\n``retry=DEFAULT_RETRY.with_deadline(30)``. See google-api-core reference\n(https://googleapis.dev/python/google-api-core/latest/retry.html) for details.\n\"\"\"\n\n\nclass ConditionalRetryPolicy(object):\n    \"\"\"A class for use when an API call is only conditionally safe to retry.\n\n    This class is intended for use in inspecting the API call parameters of an\n    API call to verify that any flags necessary to make the API call idempotent\n    (such as specifying an ``if_generation_match`` or related flag) are present.\n\n    It can be used in place of a ``retry.Retry`` object, in which case\n    ``_http.Connection.api_request`` will pass the requested api call keyword\n    arguments into the ``conditional_predicate`` and return the ``retry_policy``\n    if the conditions are met.\n\n    :type retry_policy: class:`google.api_core.retry.Retry`\n    :param retry_policy: A retry object defining timeouts, persistence and which\n        exceptions to retry.\n\n    :type conditional_predicate: callable\n    :param conditional_predicate: A callable that accepts exactly the number of\n        arguments in ``required_kwargs``, in order, and returns True if the\n        arguments have sufficient data to determine that the call is safe to\n        retry (idempotent).\n\n    :type required_kwargs: list(str)\n    :param required_kwargs:\n        A list of keyword argument keys that will be extracted from the API call\n        and passed into the ``conditional predicate`` in order. For example,\n        ``[\"query_params\"]`` is commmonly used for preconditions in query_params.\n    \"\"\"\n\n    def __init__(self, retry_policy, conditional_predicate, required_kwargs):\n        self.retry_policy = retry_policy\n        self.conditional_predicate = conditional_predicate\n        self.required_kwargs = required_kwargs\n\n    def get_retry_policy_if_conditions_met(self, **kwargs):\n        if self.conditional_predicate(*[kwargs[key] for key in self.required_kwargs]):\n            return self.retry_policy\n        return None\n\n\ndef is_generation_specified(query_params):\n    \"\"\"Return True if generation or if_generation_match is specified.\"\"\"\n    generation = query_params.get(\"generation\") is not None\n    if_generation_match = query_params.get(\"ifGenerationMatch\") is not None\n    return generation or if_generation_match\n\n\ndef is_metageneration_specified(query_params):\n    \"\"\"Return True if if_metageneration_match is specified.\"\"\"\n    if_metageneration_match = query_params.get(\"ifMetagenerationMatch\") is not None\n    return if_metageneration_match\n\n\ndef is_etag_in_data(data):\n    \"\"\"Return True if an etag is contained in the request body.\n\n    :type data: dict or None\n    :param data: A dict representing the request JSON body. If not passed, returns False.\n    \"\"\"\n    return data is not None and \"etag\" in data\n\n\ndef is_etag_in_json(data):\n    \"\"\"\n    ``is_etag_in_json`` is supported for backwards-compatibility reasons only;\n    please use ``is_etag_in_data`` instead.\n    \"\"\"\n    return is_etag_in_data(data)\n\n\nDEFAULT_RETRY_IF_GENERATION_SPECIFIED = ConditionalRetryPolicy(\n    DEFAULT_RETRY, is_generation_specified, [\"query_params\"]\n)\n\"\"\"Conditional wrapper for the default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES, but only if the request included an\n``ifGenerationMatch`` header.\n\"\"\"\n\nDEFAULT_RETRY_IF_METAGENERATION_SPECIFIED = ConditionalRetryPolicy(\n    DEFAULT_RETRY, is_metageneration_specified, [\"query_params\"]\n)\n\"\"\"Conditional wrapper for the default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES, but only if the request included an\n``ifMetagenerationMatch`` header.\n\"\"\"\n\nDEFAULT_RETRY_IF_ETAG_IN_JSON = ConditionalRetryPolicy(\n    DEFAULT_RETRY, is_etag_in_json, [\"data\"]\n)\n\"\"\"Conditional wrapper for the default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES, but only if the request included an\n``ETAG`` entry in its payload.\n\"\"\"\n", "google/cloud/storage/iam.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Storage API IAM policy definitions\n\nFor allowed roles / permissions, see:\nhttps://cloud.google.com/storage/docs/access-control/iam\n\"\"\"\n\n# Storage-specific IAM roles\n\nSTORAGE_OBJECT_CREATOR_ROLE = \"roles/storage.objectCreator\"\n\"\"\"Role implying rights to create objects, but not delete or overwrite them.\"\"\"\n\nSTORAGE_OBJECT_VIEWER_ROLE = \"roles/storage.objectViewer\"\n\"\"\"Role implying rights to view object properties, excluding ACLs.\"\"\"\n\nSTORAGE_OBJECT_ADMIN_ROLE = \"roles/storage.objectAdmin\"\n\"\"\"Role implying full control of objects.\"\"\"\n\nSTORAGE_ADMIN_ROLE = \"roles/storage.admin\"\n\"\"\"Role implying full control of objects and buckets.\"\"\"\n\nSTORAGE_VIEWER_ROLE = \"Viewer\"\n\"\"\"Can list buckets.\"\"\"\n\nSTORAGE_EDITOR_ROLE = \"Editor\"\n\"\"\"Can create, list, and delete buckets.\"\"\"\n\nSTORAGE_OWNER_ROLE = \"Owners\"\n\"\"\"Can create, list, and delete buckets.\"\"\"\n\n\n# Storage-specific permissions\n\nSTORAGE_BUCKETS_CREATE = \"storage.buckets.create\"\n\"\"\"Permission: create buckets.\"\"\"\n\nSTORAGE_BUCKETS_DELETE = \"storage.buckets.delete\"\n\"\"\"Permission: delete buckets.\"\"\"\n\nSTORAGE_BUCKETS_GET = \"storage.buckets.get\"\n\"\"\"Permission: read bucket metadata, excluding ACLs.\"\"\"\n\nSTORAGE_BUCKETS_GET_IAM_POLICY = \"storage.buckets.getIamPolicy\"\n\"\"\"Permission: read bucket ACLs.\"\"\"\n\nSTORAGE_BUCKETS_LIST = \"storage.buckets.list\"\n\"\"\"Permission: list buckets.\"\"\"\n\nSTORAGE_BUCKETS_SET_IAM_POLICY = \"storage.buckets.setIamPolicy\"\n\"\"\"Permission: update bucket ACLs.\"\"\"\n\nSTORAGE_BUCKETS_UPDATE = \"storage.buckets.list\"\n\"\"\"Permission: update buckets, excluding ACLS.\"\"\"\n\nSTORAGE_OBJECTS_CREATE = \"storage.objects.create\"\n\"\"\"Permission: add new objects to a bucket.\"\"\"\n\nSTORAGE_OBJECTS_DELETE = \"storage.objects.delete\"\n\"\"\"Permission: delete objects.\"\"\"\n\nSTORAGE_OBJECTS_GET = \"storage.objects.get\"\n\"\"\"Permission: read object data / metadata, excluding ACLs.\"\"\"\n\nSTORAGE_OBJECTS_GET_IAM_POLICY = \"storage.objects.getIamPolicy\"\n\"\"\"Permission: read object ACLs.\"\"\"\n\nSTORAGE_OBJECTS_LIST = \"storage.objects.list\"\n\"\"\"Permission: list objects in a bucket.\"\"\"\n\nSTORAGE_OBJECTS_SET_IAM_POLICY = \"storage.objects.setIamPolicy\"\n\"\"\"Permission: update object ACLs.\"\"\"\n\nSTORAGE_OBJECTS_UPDATE = \"storage.objects.update\"\n\"\"\"Permission: update object metadat, excluding ACLs.\"\"\"\n", "google/cloud/storage/version.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__version__ = \"2.17.0\"\n", "google/cloud/storage/transfer_manager.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Concurrent media operations.\"\"\"\n\nimport concurrent.futures\n\nimport io\nimport inspect\nimport os\nimport warnings\nimport pickle\nimport copyreg\nimport struct\nimport base64\nimport functools\n\nfrom google.api_core import exceptions\nfrom google.cloud.storage import Client\nfrom google.cloud.storage import Blob\nfrom google.cloud.storage.blob import _get_host_name\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage._helpers import _api_core_retry_to_resumable_media_retry\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\nimport google_crc32c\n\nfrom google.resumable_media.requests.upload import XMLMPUContainer\nfrom google.resumable_media.requests.upload import XMLMPUPart\nfrom google.resumable_media.common import DataCorruption\n\nTM_DEFAULT_CHUNK_SIZE = 32 * 1024 * 1024\nDEFAULT_MAX_WORKERS = 8\nMAX_CRC32C_ZERO_ARRAY_SIZE = 4 * 1024 * 1024\nMETADATA_HEADER_TRANSLATION = {\n    \"cacheControl\": \"Cache-Control\",\n    \"contentDisposition\": \"Content-Disposition\",\n    \"contentEncoding\": \"Content-Encoding\",\n    \"contentLanguage\": \"Content-Language\",\n    \"customTime\": \"x-goog-custom-time\",\n    \"storageClass\": \"x-goog-storage-class\",\n}\n\n# Constants to be passed in as `worker_type`.\nPROCESS = \"process\"\nTHREAD = \"thread\"\n\nDOWNLOAD_CRC32C_MISMATCH_TEMPLATE = \"\"\"\\\nChecksum mismatch while downloading:\n\n  {}\n\nThe object metadata indicated a crc32c checksum of:\n\n  {}\n\nbut the actual crc32c checksum of the downloaded contents was:\n\n  {}\n\"\"\"\n\n\n_cached_clients = {}\n\n\ndef _deprecate_threads_param(func):\n    @functools.wraps(func)\n    def convert_threads_or_raise(*args, **kwargs):\n        binding = inspect.signature(func).bind(*args, **kwargs)\n        threads = binding.arguments.get(\"threads\")\n        if threads:\n            worker_type = binding.arguments.get(\"worker_type\")\n            max_workers = binding.arguments.get(\"max_workers\")\n            if worker_type or max_workers:  # Parameter conflict\n                raise ValueError(\n                    \"The `threads` parameter is deprecated and conflicts with its replacement parameters, `worker_type` and `max_workers`.\"\n                )\n            # No conflict, so issue a warning and set worker_type and max_workers.\n            warnings.warn(\n                \"The `threads` parameter is deprecated. Please use `worker_type` and `max_workers` parameters instead.\"\n            )\n            args = binding.args\n            kwargs = binding.kwargs\n            kwargs[\"worker_type\"] = THREAD\n            kwargs[\"max_workers\"] = threads\n            return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n\n    return convert_threads_or_raise\n\n\n@_deprecate_threads_param\ndef upload_many(\n    file_blob_pairs,\n    skip_if_exists=False,\n    upload_kwargs=None,\n    threads=None,\n    deadline=None,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n):\n    \"\"\"Upload many files concurrently via a worker pool.\n\n    :type file_blob_pairs: List(Tuple(IOBase or str, 'google.cloud.storage.blob.Blob'))\n    :param file_blob_pairs:\n        A list of tuples of a file or filename and a blob. Each file will be\n        uploaded to the corresponding blob by using APIs identical to\n        `blob.upload_from_file()` or `blob.upload_from_filename()` as\n        appropriate.\n\n        File handlers are only supported if worker_type is set to THREAD.\n        If worker_type is set to PROCESS, please use filenames only.\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        If True, blobs that already have a live version will not be overwritten.\n        This is accomplished by setting `if_generation_match = 0` on uploads.\n        Uploads so skipped will result in a 412 Precondition Failed response\n        code, which will be included in the return value but not raised\n        as an exception regardless of the value of raise_exception.\n\n    :type upload_kwargs: dict\n    :param upload_kwargs:\n        A dictionary of keyword arguments to pass to the upload method. Refer\n        to the documentation for `blob.upload_from_file()` or\n        `blob.upload_from_filename()` for more information. The dict is directly\n        passed into the upload methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure.\n\n        If skip_if_exists is True, 412 Precondition Failed responses are\n        considered part of normal operation and are not raised as an exception.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n        PROCESS workers do not support writing to file handlers. Please refer\n        to files by filename only when using PROCESS workers.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        upload method is used (which will be None).\n    \"\"\"\n    if upload_kwargs is None:\n        upload_kwargs = {}\n\n    if skip_if_exists:\n        upload_kwargs = upload_kwargs.copy()\n        upload_kwargs[\"if_generation_match\"] = 0\n\n    upload_kwargs[\"command\"] = \"tm.upload_many\"\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n\n    with pool_class(max_workers=max_workers) as executor:\n        futures = []\n        for path_or_file, blob in file_blob_pairs:\n            # File objects are only supported by the THREAD worker because they can't\n            # be pickled.\n            if needs_pickling and not isinstance(path_or_file, str):\n                raise ValueError(\n                    \"Passing in a file object is only supported by the THREAD worker type. Please either select THREAD workers, or pass in filenames only.\"\n                )\n\n            futures.append(\n                executor.submit(\n                    _call_method_on_maybe_pickled_blob,\n                    _pickle_client(blob) if needs_pickling else blob,\n                    \"_handle_filename_and_upload\"\n                    if isinstance(path_or_file, str)\n                    else \"_prep_and_do_upload\",\n                    path_or_file,\n                    **upload_kwargs,\n                )\n            )\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    results = []\n    for future in futures:\n        exp = future.exception()\n\n        # If raise_exception is False, don't call future.result()\n        if exp and not raise_exception:\n            results.append(exp)\n        # If skip_if_exists and the exception is PreconditionFailed, do same.\n        elif exp and skip_if_exists and isinstance(exp, exceptions.PreconditionFailed):\n            results.append(exp)\n        # Get the real result. If there was an exception not handled above,\n        # this will raise it.\n        else:\n            results.append(future.result())\n    return results\n\n\n@_deprecate_threads_param\ndef download_many(\n    blob_file_pairs,\n    download_kwargs=None,\n    threads=None,\n    deadline=None,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    skip_if_exists=False,\n):\n    \"\"\"Download many blobs concurrently via a worker pool.\n\n    :type blob_file_pairs: List(Tuple('google.cloud.storage.blob.Blob', IOBase or str))\n    :param blob_file_pairs:\n        A list of tuples of blob and a file or filename. Each blob will be downloaded to the corresponding blob by using APIs identical to blob.download_to_file() or blob.download_to_filename() as appropriate.\n\n        Note that blob.download_to_filename() does not delete the destination file if the download fails.\n\n        File handlers are only supported if worker_type is set to THREAD.\n        If worker_type is set to PROCESS, please use filenames only.\n\n    :type download_kwargs: dict\n    :param download_kwargs:\n        A dictionary of keyword arguments to pass to the download method. Refer\n        to the documentation for `blob.download_to_file()` or\n        `blob.download_to_filename()` for more information. The dict is directly\n        passed into the download methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n        PROCESS workers do not support writing to file handlers. Please refer\n        to files by filename only when using PROCESS workers.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        Before downloading each blob, check if the file for the filename exists;\n        if it does, skip that blob.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        download method is used (which will be None).\n    \"\"\"\n\n    if download_kwargs is None:\n        download_kwargs = {}\n\n    download_kwargs[\"command\"] = \"tm.download_many\"\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n\n    with pool_class(max_workers=max_workers) as executor:\n        futures = []\n        for blob, path_or_file in blob_file_pairs:\n            # File objects are only supported by the THREAD worker because they can't\n            # be pickled.\n            if needs_pickling and not isinstance(path_or_file, str):\n                raise ValueError(\n                    \"Passing in a file object is only supported by the THREAD worker type. Please either select THREAD workers, or pass in filenames only.\"\n                )\n\n            if skip_if_exists and isinstance(path_or_file, str):\n                if os.path.isfile(path_or_file):\n                    continue\n\n            futures.append(\n                executor.submit(\n                    _call_method_on_maybe_pickled_blob,\n                    _pickle_client(blob) if needs_pickling else blob,\n                    \"_handle_filename_and_download\"\n                    if isinstance(path_or_file, str)\n                    else \"_prep_and_do_download\",\n                    path_or_file,\n                    **download_kwargs,\n                )\n            )\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    results = []\n    for future in futures:\n        # If raise_exception is False, don't call future.result()\n        if not raise_exception:\n            exp = future.exception()\n            if exp:\n                results.append(exp)\n                continue\n        # Get the real result. If there was an exception, this will raise it.\n        results.append(future.result())\n    return results\n\n\n@_deprecate_threads_param\ndef upload_many_from_filenames(\n    bucket,\n    filenames,\n    source_directory=\"\",\n    blob_name_prefix=\"\",\n    skip_if_exists=False,\n    blob_constructor_kwargs=None,\n    upload_kwargs=None,\n    threads=None,\n    deadline=None,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    additional_blob_attributes=None,\n):\n    \"\"\"Upload many files concurrently by their filenames.\n\n    The destination blobs are automatically created, with blob names based on\n    the source filenames and the blob_name_prefix.\n\n    For example, if the `filenames` include \"images/icon.jpg\",\n    `source_directory` is \"/home/myuser/\", and `blob_name_prefix` is \"myfiles/\",\n    then the file at \"/home/myuser/images/icon.jpg\" will be uploaded to a blob\n    named \"myfiles/images/icon.jpg\".\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket:\n        The bucket which will contain the uploaded blobs.\n\n    :type filenames: list(str)\n    :param filenames:\n        A list of filenames to be uploaded. This may include part of the path.\n        The file will be accessed at the full path of `source_directory` +\n        `filename`.\n\n    :type source_directory: str\n    :param source_directory:\n        A string that will be prepended (with `os.path.join()`) to each filename\n        in the input list, in order to find the source file for each blob.\n        Unlike the filename itself, the source_directory does not affect the\n        name of the uploaded blob.\n\n        For instance, if the source_directory is \"/tmp/img/\" and a filename is\n        \"0001.jpg\", with an empty blob_name_prefix, then the file uploaded will\n        be \"/tmp/img/0001.jpg\" and the destination blob will be \"0001.jpg\".\n\n        This parameter can be an empty string.\n\n        Note that this parameter allows directory traversal (e.g. \"/\", \"../\")\n        and is not intended for unsanitized end user input.\n\n    :type blob_name_prefix: str\n    :param blob_name_prefix:\n        A string that will be prepended to each filename in the input list, in\n        order to determine the name of the destination blob. Unlike the filename\n        itself, the prefix string does not affect the location the library will\n        look for the source data on the local filesystem.\n\n        For instance, if the source_directory is \"/tmp/img/\", the\n        blob_name_prefix is \"myuser/mystuff-\" and a filename is \"0001.jpg\" then\n        the file uploaded will be \"/tmp/img/0001.jpg\" and the destination blob\n        will be \"myuser/mystuff-0001.jpg\".\n\n        The blob_name_prefix can be blank (an empty string).\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        If True, blobs that already have a live version will not be overwritten.\n        This is accomplished by setting `if_generation_match = 0` on uploads.\n        Uploads so skipped will result in a 412 Precondition Failed response\n        code, which will be included in the return value, but not raised\n        as an exception regardless of the value of raise_exception.\n\n    :type blob_constructor_kwargs: dict\n    :param blob_constructor_kwargs:\n        A dictionary of keyword arguments to pass to the blob constructor. Refer\n        to the documentation for `blob.Blob()` for more information. The dict is\n        directly passed into the constructor and is not validated by this\n        function. `name` and `bucket` keyword arguments are reserved by this\n        function and will result in an error if passed in here.\n\n    :type upload_kwargs: dict\n    :param upload_kwargs:\n        A dictionary of keyword arguments to pass to the upload method. Refer\n        to the documentation for `blob.upload_from_file()` or\n        `blob.upload_from_filename()` for more information. The dict is directly\n        passed into the upload methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure.\n\n        If skip_if_exists is True, 412 Precondition Failed responses are\n        considered part of normal operation and are not raised as an exception.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type additional_blob_attributes: dict\n    :param additional_blob_attributes:\n        A dictionary of blob attribute names and values. This allows the\n        configuration of blobs beyond what is possible with\n        blob_constructor_kwargs. For instance, {\"cache_control\": \"no-cache\"}\n        would set the cache_control attribute of each blob to \"no-cache\".\n\n        As with blob_constructor_kwargs, this affects the creation of every\n        blob identically. To fine-tune each blob individually, use `upload_many`\n        and create the blobs as desired before passing them in.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        upload method is used (which will be None).\n    \"\"\"\n    if blob_constructor_kwargs is None:\n        blob_constructor_kwargs = {}\n    if additional_blob_attributes is None:\n        additional_blob_attributes = {}\n\n    file_blob_pairs = []\n\n    for filename in filenames:\n        path = os.path.join(source_directory, filename)\n        blob_name = blob_name_prefix + filename\n        blob = bucket.blob(blob_name, **blob_constructor_kwargs)\n        for prop, value in additional_blob_attributes.items():\n            setattr(blob, prop, value)\n        file_blob_pairs.append((path, blob))\n\n    return upload_many(\n        file_blob_pairs,\n        skip_if_exists=skip_if_exists,\n        upload_kwargs=upload_kwargs,\n        deadline=deadline,\n        raise_exception=raise_exception,\n        worker_type=worker_type,\n        max_workers=max_workers,\n    )\n\n\n@_deprecate_threads_param\ndef download_many_to_path(\n    bucket,\n    blob_names,\n    destination_directory=\"\",\n    blob_name_prefix=\"\",\n    download_kwargs=None,\n    threads=None,\n    deadline=None,\n    create_directories=True,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    skip_if_exists=False,\n):\n    \"\"\"Download many files concurrently by their blob names.\n\n    The destination files are automatically created, with paths based on the\n    source blob_names and the destination_directory.\n\n    The destination files are not automatically deleted if their downloads fail,\n    so please check the return value of this function for any exceptions, or\n    enable `raise_exception=True`, and process the files accordingly.\n\n    For example, if the `blob_names` include \"icon.jpg\", `destination_directory`\n    is \"/home/myuser/\", and `blob_name_prefix` is \"images/\", then the blob named\n    \"images/icon.jpg\" will be downloaded to a file named\n    \"/home/myuser/icon.jpg\".\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket:\n        The bucket which contains the blobs to be downloaded\n\n    :type blob_names: list(str)\n    :param blob_names:\n        A list of blobs to be downloaded. The blob name in this string will be\n        used to determine the destination file path as well.\n\n        The full name to the blob must be blob_name_prefix + blob_name. The\n        blob_name is separate from the blob_name_prefix because the blob_name\n        will also determine the name of the destination blob. Any shared part of\n        the blob names that need not be part of the destination path should be\n        included in the blob_name_prefix.\n\n    :type destination_directory: str\n    :param destination_directory:\n        A string that will be prepended (with os.path.join()) to each blob_name\n        in the input list, in order to determine the destination path for that\n        blob.\n\n        For instance, if the destination_directory string is \"/tmp/img\" and a\n        blob_name is \"0001.jpg\", with an empty blob_name_prefix, then the source\n        blob \"0001.jpg\" will be downloaded to destination \"/tmp/img/0001.jpg\" .\n\n        This parameter can be an empty string.\n\n        Note that this parameter allows directory traversal (e.g. \"/\", \"../\")\n        and is not intended for unsanitized end user input.\n\n    :type blob_name_prefix: str\n    :param blob_name_prefix:\n        A string that will be prepended to each blob_name in the input list, in\n        order to determine the name of the source blob. Unlike the blob_name\n        itself, the prefix string does not affect the destination path on the\n        local filesystem. For instance, if the destination_directory is\n        \"/tmp/img/\", the blob_name_prefix is \"myuser/mystuff-\" and a blob_name\n        is \"0001.jpg\" then the source blob \"myuser/mystuff-0001.jpg\" will be\n        downloaded to \"/tmp/img/0001.jpg\". The blob_name_prefix can be blank\n        (an empty string).\n\n    :type download_kwargs: dict\n    :param download_kwargs:\n        A dictionary of keyword arguments to pass to the download method. Refer\n        to the documentation for `blob.download_to_file()` or\n        `blob.download_to_filename()` for more information. The dict is directly\n        passed into the download methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type create_directories: bool\n    :param create_directories:\n        If True, recursively create any directories that do not exist. For\n        instance, if downloading object \"images/img001.png\", create the\n        directory \"images\" before downloading.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure. If skip_if_exists is True, 412\n        Precondition Failed responses are considered part of normal operation\n        and are not raised as an exception.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        Before downloading each blob, check if the file for the filename exists;\n        if it does, skip that blob. This only works for filenames.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        download method is used (which will be None).\n    \"\"\"\n    blob_file_pairs = []\n\n    for blob_name in blob_names:\n        full_blob_name = blob_name_prefix + blob_name\n        path = os.path.join(destination_directory, blob_name)\n        if create_directories:\n            directory, _ = os.path.split(path)\n            os.makedirs(directory, exist_ok=True)\n        blob_file_pairs.append((bucket.blob(full_blob_name), path))\n\n    return download_many(\n        blob_file_pairs,\n        download_kwargs=download_kwargs,\n        deadline=deadline,\n        raise_exception=raise_exception,\n        worker_type=worker_type,\n        max_workers=max_workers,\n        skip_if_exists=skip_if_exists,\n    )\n\n\ndef download_chunks_concurrently(\n    blob,\n    filename,\n    chunk_size=TM_DEFAULT_CHUNK_SIZE,\n    download_kwargs=None,\n    deadline=None,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    crc32c_checksum=True,\n):\n    \"\"\"Download a single file in chunks, concurrently.\n\n    In some environments, using this feature with mutiple processes will result\n    in faster downloads of large files.\n\n    Using this feature with multiple threads is unlikely to improve download\n    performance under normal circumstances due to Python interpreter threading\n    behavior. The default is therefore to use processes instead of threads.\n\n    :type blob: :class:`google.cloud.storage.blob.Blob`\n    :param blob:\n        The blob to be downloaded.\n\n    :type filename: str\n    :param filename:\n        The destination filename or path.\n\n    :type chunk_size: int\n    :param chunk_size:\n        The size in bytes of each chunk to send. The optimal chunk size for\n        maximum throughput may vary depending on the exact network environment\n        and size of the blob.\n\n    :type download_kwargs: dict\n    :param download_kwargs:\n        A dictionary of keyword arguments to pass to the download method. Refer\n        to the documentation for `blob.download_to_file()` or\n        `blob.download_to_filename()` for more information. The dict is directly\n        passed into the download methods and is not validated by this function.\n\n        Keyword arguments \"start\" and \"end\" which are not supported and will\n        cause a ValueError if present. The key \"checksum\" is also not supported\n        in `download_kwargs`, but see the argument `crc32c_checksum` (which does\n        not go in `download_kwargs`) below.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type crc32c_checksum: bool\n    :param crc32c_checksum:\n        Whether to compute a checksum for the resulting object, using the crc32c\n        algorithm. As the checksums for each chunk must be combined using a\n        feature of crc32c that is not available for md5, md5 is not supported.\n\n    :raises:\n        :exc:`concurrent.futures.TimeoutError`\n            if deadline is exceeded.\n        :exc:`google.resumable_media.common.DataCorruption`\n            if the download's checksum doesn't agree with server-computed\n            checksum. The `google.resumable_media` exception is used here for\n            consistency with other download methods despite the exception\n            originating elsewhere.\n    \"\"\"\n    client = blob.client\n\n    if download_kwargs is None:\n        download_kwargs = {}\n    if \"start\" in download_kwargs or \"end\" in download_kwargs:\n        raise ValueError(\n            \"Download arguments 'start' and 'end' are not supported by download_chunks_concurrently.\"\n        )\n    if \"checksum\" in download_kwargs:\n        raise ValueError(\n            \"'checksum' is in download_kwargs, but is not supported because sliced downloads have a different checksum mechanism from regular downloads. Use the 'crc32c_checksum' argument on download_chunks_concurrently instead.\"\n        )\n\n    download_kwargs[\"command\"] = \"tm.download_sharded\"\n\n    # We must know the size and the generation of the blob.\n    if not blob.size or not blob.generation:\n        blob.reload()\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n    # Pickle the blob ahead of time (just once, not once per chunk) if needed.\n    maybe_pickled_blob = _pickle_client(blob) if needs_pickling else blob\n\n    futures = []\n\n    # Create and/or truncate the destination file to prepare for sparse writing.\n    with open(filename, \"wb\") as _:\n        pass\n\n    with pool_class(max_workers=max_workers) as executor:\n        cursor = 0\n        end = blob.size\n        while cursor < end:\n            start = cursor\n            cursor = min(cursor + chunk_size, end)\n            futures.append(\n                executor.submit(\n                    _download_and_write_chunk_in_place,\n                    maybe_pickled_blob,\n                    filename,\n                    start=start,\n                    end=cursor - 1,\n                    download_kwargs=download_kwargs,\n                    crc32c_checksum=crc32c_checksum,\n                )\n            )\n\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    # Raise any exceptions; combine checksums.\n    results = []\n    for future in futures:\n        results.append(future.result())\n\n    if crc32c_checksum and results:\n        crc_digest = _digest_ordered_checksum_and_size_pairs(results)\n        actual_checksum = base64.b64encode(crc_digest).decode(\"utf-8\")\n        expected_checksum = blob.crc32c\n        if actual_checksum != expected_checksum:\n            # For consistency with other download methods we will use\n            # \"google.resumable_media.common.DataCorruption\" despite the error\n            # not originating inside google.resumable_media.\n            download_url = blob._get_download_url(\n                client,\n                if_generation_match=download_kwargs.get(\"if_generation_match\"),\n                if_generation_not_match=download_kwargs.get(\"if_generation_not_match\"),\n                if_metageneration_match=download_kwargs.get(\"if_metageneration_match\"),\n                if_metageneration_not_match=download_kwargs.get(\n                    \"if_metageneration_not_match\"\n                ),\n            )\n            raise DataCorruption(\n                None,\n                DOWNLOAD_CRC32C_MISMATCH_TEMPLATE.format(\n                    download_url, expected_checksum, actual_checksum\n                ),\n            )\n    return None\n\n\ndef upload_chunks_concurrently(\n    filename,\n    blob,\n    content_type=None,\n    chunk_size=TM_DEFAULT_CHUNK_SIZE,\n    deadline=None,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    checksum=\"md5\",\n    timeout=_DEFAULT_TIMEOUT,\n    retry=DEFAULT_RETRY,\n):\n    \"\"\"Upload a single file in chunks, concurrently.\n\n    This function uses the XML MPU API to initialize an upload and upload a\n    file in chunks, concurrently with a worker pool.\n\n    The XML MPU API is significantly different from other uploads; please review\n    the documentation at `https://cloud.google.com/storage/docs/multipart-uploads`\n    before using this feature.\n\n    The library will attempt to cancel uploads that fail due to an exception.\n    If the upload fails in a way that precludes cancellation, such as a\n    hardware failure, process termination, or power outage, then the incomplete\n    upload may persist indefinitely. To mitigate this, set the\n    `AbortIncompleteMultipartUpload` with a nonzero `Age` in bucket lifecycle\n    rules, or refer to the XML API documentation linked above to learn more\n    about how to list and delete individual downloads.\n\n    Using this feature with multiple threads is unlikely to improve upload\n    performance under normal circumstances due to Python interpreter threading\n    behavior. The default is therefore to use processes instead of threads.\n\n    ACL information cannot be sent with this function and should be set\n    separately with :class:`ObjectACL` methods.\n\n    :type filename: str\n    :param filename:\n        The path to the file to upload. File-like objects are not supported.\n\n    :type blob: :class:`google.cloud.storage.blob.Blob`\n    :param blob:\n        The blob to which to upload.\n\n    :type content_type: str\n    :param content_type: (Optional) Type of content being uploaded.\n\n    :type chunk_size: int\n    :param chunk_size:\n        The size in bytes of each chunk to send. The optimal chunk size for\n        maximum throughput may vary depending on the exact network environment\n        and size of the blob. The remote API has restrictions on the minimum\n        and maximum size allowable, see: `https://cloud.google.com/storage/quotas#requests`\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type checksum: str\n    :param checksum:\n        (Optional) The checksum scheme to use: either \"md5\", \"crc32c\" or None.\n        Each individual part is checksummed. At present, the selected checksum\n        rule is only applied to parts and a separate checksum of the entire\n        resulting blob is not computed. Please compute and compare the checksum\n        of the file to the resulting blob separately if needed, using the\n        \"crc32c\" algorithm as per the XML MPU documentation.\n\n    :type timeout: float or tuple\n    :param timeout:\n        (Optional) The amount of time, in seconds, to wait\n        for the server response.  See: :ref:`configuring_timeouts`\n\n    :type retry: google.api_core.retry.Retry\n    :param retry: (Optional) How to retry the RPC. A None value will disable\n        retries. A `google.api_core.retry.Retry` value will enable retries,\n        and the object will configure backoff and timeout options. Custom\n        predicates (customizable error codes) are not supported for media\n        operations such as this one.\n\n        This function does not accept `ConditionalRetryPolicy` values because\n        preconditions are not supported by the underlying API call.\n\n        See the retry.py source code and docstrings in this package\n        (`google.cloud.storage.retry`) for information on retry types and how\n        to configure them.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n    \"\"\"\n\n    bucket = blob.bucket\n    client = blob.client\n    transport = blob._get_transport(client)\n\n    hostname = _get_host_name(client._connection)\n    url = \"{hostname}/{bucket}/{blob}\".format(\n        hostname=hostname, bucket=bucket.name, blob=blob.name\n    )\n\n    base_headers, object_metadata, content_type = blob._get_upload_arguments(\n        client, content_type, filename=filename, command=\"tm.upload_sharded\"\n    )\n    headers = {**base_headers, **_headers_from_metadata(object_metadata)}\n\n    if blob.user_project is not None:\n        headers[\"x-goog-user-project\"] = blob.user_project\n\n    # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n    # at rest, object resource metadata will store the version of the Key Management\n    # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n    # used to upload a new version of the object then the existing kmsKeyName version\n    # value can't be used in the upload request and the client instead ignores it.\n    if blob.kms_key_name is not None and \"cryptoKeyVersions\" not in blob.kms_key_name:\n        headers[\"x-goog-encryption-kms-key-name\"] = blob.kms_key_name\n\n    container = XMLMPUContainer(url, filename, headers=headers)\n    container._retry_strategy = _api_core_retry_to_resumable_media_retry(retry)\n\n    container.initiate(transport=transport, content_type=content_type)\n    upload_id = container.upload_id\n\n    size = os.path.getsize(filename)\n    num_of_parts = -(size // -chunk_size)  # Ceiling division\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n    # Pickle the blob ahead of time (just once, not once per chunk) if needed.\n    maybe_pickled_client = _pickle_client(client) if needs_pickling else client\n\n    futures = []\n\n    with pool_class(max_workers=max_workers) as executor:\n        for part_number in range(1, num_of_parts + 1):\n            start = (part_number - 1) * chunk_size\n            end = min(part_number * chunk_size, size)\n\n            futures.append(\n                executor.submit(\n                    _upload_part,\n                    maybe_pickled_client,\n                    url,\n                    upload_id,\n                    filename,\n                    start=start,\n                    end=end,\n                    part_number=part_number,\n                    checksum=checksum,\n                    headers=headers,\n                    retry=retry,\n                )\n            )\n\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    try:\n        # Harvest results and raise exceptions.\n        for future in futures:\n            part_number, etag = future.result()\n            container.register_part(part_number, etag)\n\n        container.finalize(blob._get_transport(client))\n    except Exception:\n        container.cancel(blob._get_transport(client))\n        raise\n\n\ndef _upload_part(\n    maybe_pickled_client,\n    url,\n    upload_id,\n    filename,\n    start,\n    end,\n    part_number,\n    checksum,\n    headers,\n    retry,\n):\n    \"\"\"Helper function that runs inside a thread or subprocess to upload a part.\n\n    `maybe_pickled_client` is either a Client (for threads) or a specially\n    pickled Client (for processes) because the default pickling mangles Client\n    objects.\"\"\"\n\n    if isinstance(maybe_pickled_client, Client):\n        client = maybe_pickled_client\n    else:\n        client = pickle.loads(maybe_pickled_client)\n    part = XMLMPUPart(\n        url,\n        upload_id,\n        filename,\n        start=start,\n        end=end,\n        part_number=part_number,\n        checksum=checksum,\n        headers=headers,\n    )\n    part._retry_strategy = _api_core_retry_to_resumable_media_retry(retry)\n    part.upload(client._http)\n    return (part_number, part.etag)\n\n\ndef _headers_from_metadata(metadata):\n    \"\"\"Helper function to translate object metadata into a header dictionary.\"\"\"\n\n    headers = {}\n    # Handle standard writable metadata\n    for key, value in metadata.items():\n        if key in METADATA_HEADER_TRANSLATION:\n            headers[METADATA_HEADER_TRANSLATION[key]] = value\n    # Handle custom metadata\n    if \"metadata\" in metadata:\n        for key, value in metadata[\"metadata\"].items():\n            headers[\"x-goog-meta-\" + key] = value\n    return headers\n\n\ndef _download_and_write_chunk_in_place(\n    maybe_pickled_blob, filename, start, end, download_kwargs, crc32c_checksum\n):\n    \"\"\"Helper function that runs inside a thread or subprocess.\n\n    `maybe_pickled_blob` is either a Blob (for threads) or a specially pickled\n    Blob (for processes) because the default pickling mangles Client objects\n    which are attached to Blobs.\n\n    Returns a crc if configured (or None) and the size written.\n    \"\"\"\n\n    if isinstance(maybe_pickled_blob, Blob):\n        blob = maybe_pickled_blob\n    else:\n        blob = pickle.loads(maybe_pickled_blob)\n\n    with _ChecksummingSparseFileWrapper(filename, start, crc32c_checksum) as f:\n        blob._prep_and_do_download(f, start=start, end=end, **download_kwargs)\n        return (f.crc, (end - start) + 1)\n\n\nclass _ChecksummingSparseFileWrapper:\n    \"\"\"A file wrapper that writes to a sparse file and optionally checksums.\n\n    This wrapper only implements write() and does not inherit from `io` module\n    base classes.\n    \"\"\"\n\n    def __init__(self, filename, start_position, crc32c_enabled):\n        # Open in mixed read/write mode to avoid truncating or appending\n        self.f = open(filename, \"rb+\")\n        self.f.seek(start_position)\n        self._crc = None\n        self._crc32c_enabled = crc32c_enabled\n\n    def write(self, chunk):\n        if self._crc32c_enabled:\n            if self._crc is None:\n                self._crc = google_crc32c.value(chunk)\n            else:\n                self._crc = google_crc32c.extend(self._crc, chunk)\n        self.f.write(chunk)\n\n    @property\n    def crc(self):\n        return self._crc\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        self.f.close()\n\n\ndef _call_method_on_maybe_pickled_blob(\n    maybe_pickled_blob, method_name, *args, **kwargs\n):\n    \"\"\"Helper function that runs inside a thread or subprocess.\n\n    `maybe_pickled_blob` is either a Blob (for threads) or a specially pickled\n    Blob (for processes) because the default pickling mangles Client objects\n    which are attached to Blobs.\"\"\"\n\n    if isinstance(maybe_pickled_blob, Blob):\n        blob = maybe_pickled_blob\n    else:\n        blob = pickle.loads(maybe_pickled_blob)\n    return getattr(blob, method_name)(*args, **kwargs)\n\n\ndef _reduce_client(cl):\n    \"\"\"Replicate a Client by constructing a new one with the same params.\n\n    LazyClient performs transparent caching for when the same client is needed\n    on the same process multiple times.\"\"\"\n\n    client_object_id = id(cl)\n    project = cl.project\n    credentials = cl._credentials\n    _http = None  # Can't carry this over\n    client_info = cl._initial_client_info\n    client_options = cl._initial_client_options\n    extra_headers = cl._extra_headers\n\n    return _LazyClient, (\n        client_object_id,\n        project,\n        credentials,\n        _http,\n        client_info,\n        client_options,\n        extra_headers,\n    )\n\n\ndef _pickle_client(obj):\n    \"\"\"Pickle a Client or an object that owns a Client (like a Blob)\"\"\"\n\n    # We need a custom pickler to process Client objects, which are attached to\n    # Buckets (and therefore to Blobs in turn). Unfortunately, the Python\n    # multiprocessing library doesn't seem to have a good way to use a custom\n    # pickler, and using copyreg will mutate global state and affect code\n    # outside of the client library. Instead, we'll pre-pickle the object and\n    # pass the bytestring in.\n    f = io.BytesIO()\n    p = pickle.Pickler(f)\n    p.dispatch_table = copyreg.dispatch_table.copy()\n    p.dispatch_table[Client] = _reduce_client\n    p.dump(obj)\n    return f.getvalue()\n\n\ndef _get_pool_class_and_requirements(worker_type):\n    \"\"\"Returns the pool class, and whether the pool requires pickled Blobs.\"\"\"\n\n    if worker_type == PROCESS:\n        # Use processes. Pickle blobs with custom logic to handle the client.\n        return (concurrent.futures.ProcessPoolExecutor, True)\n    elif worker_type == THREAD:\n        # Use threads. Pass blobs through unpickled.\n        return (concurrent.futures.ThreadPoolExecutor, False)\n    else:\n        raise ValueError(\n            \"The worker_type must be google.cloud.storage.transfer_manager.PROCESS or google.cloud.storage.transfer_manager.THREAD\"\n        )\n\n\ndef _digest_ordered_checksum_and_size_pairs(checksum_and_size_pairs):\n    base_crc = None\n    zeroes = bytes(MAX_CRC32C_ZERO_ARRAY_SIZE)\n    for part_crc, size in checksum_and_size_pairs:\n        if not base_crc:\n            base_crc = part_crc\n        else:\n            base_crc ^= 0xFFFFFFFF  # precondition\n\n            # Zero pad base_crc32c. To conserve memory, do so with only\n            # MAX_CRC32C_ZERO_ARRAY_SIZE at a time. Reuse the zeroes array where\n            # possible.\n            padded = 0\n            while padded < size:\n                desired_zeroes_size = min((size - padded), MAX_CRC32C_ZERO_ARRAY_SIZE)\n                base_crc = google_crc32c.extend(base_crc, zeroes[:desired_zeroes_size])\n                padded += desired_zeroes_size\n\n            base_crc ^= 0xFFFFFFFF  # postcondition\n            base_crc ^= part_crc\n    crc_digest = struct.pack(\n        \">L\", base_crc\n    )  # https://cloud.google.com/storage/docs/json_api/v1/objects#crc32c\n    return crc_digest\n\n\nclass _LazyClient:\n    \"\"\"An object that will transform into either a cached or a new Client\"\"\"\n\n    def __new__(cls, id, *args, **kwargs):\n        cached_client = _cached_clients.get(id)\n        if cached_client:\n            return cached_client\n        else:\n            cached_client = Client(*args, **kwargs)\n            _cached_clients[id] = cached_client\n            return cached_client\n", "google/cloud/storage/acl.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Manage access to objects and buckets.\"\"\"\n\nfrom google.cloud.storage._helpers import _add_generation_match_parameters\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\n\nclass _ACLEntity(object):\n    \"\"\"Class representing a set of roles for an entity.\n\n    This is a helper class that you likely won't ever construct\n    outside of using the factor methods on the :class:`ACL` object.\n\n    :type entity_type: str\n    :param entity_type: The type of entity (ie, 'group' or 'user').\n\n    :type identifier: str\n    :param identifier: (Optional) The ID or e-mail of the entity. For the special\n                       entity types (like 'allUsers').\n    \"\"\"\n\n    READER_ROLE = \"READER\"\n    WRITER_ROLE = \"WRITER\"\n    OWNER_ROLE = \"OWNER\"\n\n    def __init__(self, entity_type, identifier=None):\n        self.identifier = identifier\n        self.roles = set([])\n        self.type = entity_type\n\n    def __str__(self):\n        if not self.identifier:\n            return str(self.type)\n        else:\n            return \"{acl.type}-{acl.identifier}\".format(acl=self)\n\n    def __repr__(self):\n        return f\"<ACL Entity: {self} ({', '.join(self.roles)})>\"\n\n    def get_roles(self):\n        \"\"\"Get the list of roles permitted by this entity.\n\n        :rtype: list of strings\n        :returns: The list of roles associated with this entity.\n        \"\"\"\n        return self.roles\n\n    def grant(self, role):\n        \"\"\"Add a role to the entity.\n\n        :type role: str\n        :param role: The role to add to the entity.\n        \"\"\"\n        self.roles.add(role)\n\n    def revoke(self, role):\n        \"\"\"Remove a role from the entity.\n\n        :type role: str\n        :param role: The role to remove from the entity.\n        \"\"\"\n        if role in self.roles:\n            self.roles.remove(role)\n\n    def grant_read(self):\n        \"\"\"Grant read access to the current entity.\"\"\"\n        self.grant(_ACLEntity.READER_ROLE)\n\n    def grant_write(self):\n        \"\"\"Grant write access to the current entity.\"\"\"\n        self.grant(_ACLEntity.WRITER_ROLE)\n\n    def grant_owner(self):\n        \"\"\"Grant owner access to the current entity.\"\"\"\n        self.grant(_ACLEntity.OWNER_ROLE)\n\n    def revoke_read(self):\n        \"\"\"Revoke read access from the current entity.\"\"\"\n        self.revoke(_ACLEntity.READER_ROLE)\n\n    def revoke_write(self):\n        \"\"\"Revoke write access from the current entity.\"\"\"\n        self.revoke(_ACLEntity.WRITER_ROLE)\n\n    def revoke_owner(self):\n        \"\"\"Revoke owner access from the current entity.\"\"\"\n        self.revoke(_ACLEntity.OWNER_ROLE)\n\n\nclass ACL(object):\n    \"\"\"Container class representing a list of access controls.\"\"\"\n\n    _URL_PATH_ELEM = \"acl\"\n    _PREDEFINED_QUERY_PARAM = \"predefinedAcl\"\n\n    PREDEFINED_XML_ACLS = {\n        # XML API name -> JSON API name\n        \"project-private\": \"projectPrivate\",\n        \"public-read\": \"publicRead\",\n        \"public-read-write\": \"publicReadWrite\",\n        \"authenticated-read\": \"authenticatedRead\",\n        \"bucket-owner-read\": \"bucketOwnerRead\",\n        \"bucket-owner-full-control\": \"bucketOwnerFullControl\",\n    }\n\n    PREDEFINED_JSON_ACLS = frozenset(\n        [\n            \"private\",\n            \"projectPrivate\",\n            \"publicRead\",\n            \"publicReadWrite\",\n            \"authenticatedRead\",\n            \"bucketOwnerRead\",\n            \"bucketOwnerFullControl\",\n        ]\n    )\n    \"\"\"See\n    https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n    \"\"\"\n\n    loaded = False\n\n    # Subclasses must override to provide these attributes (typically,\n    # as properties).\n    reload_path = None\n    save_path = None\n    user_project = None\n\n    def __init__(self):\n        self.entities = {}\n\n    def _ensure_loaded(self, timeout=_DEFAULT_TIMEOUT):\n        \"\"\"Load if not already loaded.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n        \"\"\"\n        if not self.loaded:\n            self.reload(timeout=timeout)\n\n    @classmethod\n    def validate_predefined(cls, predefined):\n        \"\"\"Ensures predefined is in list of predefined json values\n\n        :type predefined: str\n        :param predefined: name of a predefined acl\n\n        :type predefined: str\n        :param predefined: validated JSON name of predefined acl\n\n        :raises: :exc: `ValueError`: If predefined is not a valid acl\n        \"\"\"\n        predefined = cls.PREDEFINED_XML_ACLS.get(predefined, predefined)\n        if predefined and predefined not in cls.PREDEFINED_JSON_ACLS:\n            raise ValueError(f\"Invalid predefined ACL: {predefined}\")\n        return predefined\n\n    def reset(self):\n        \"\"\"Remove all entities from the ACL, and clear the ``loaded`` flag.\"\"\"\n        self.entities.clear()\n        self.loaded = False\n\n    def __iter__(self):\n        self._ensure_loaded()\n\n        for entity in self.entities.values():\n            for role in entity.get_roles():\n                if role:\n                    yield {\"entity\": str(entity), \"role\": role}\n\n    def entity_from_dict(self, entity_dict):\n        \"\"\"Build an _ACLEntity object from a dictionary of data.\n\n        An entity is a mutable object that represents a list of roles\n        belonging to either a user or group or the special types for all\n        users and all authenticated users.\n\n        :type entity_dict: dict\n        :param entity_dict: Dictionary full of data from an ACL lookup.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity constructed from the dictionary.\n        \"\"\"\n        entity = entity_dict[\"entity\"]\n        role = entity_dict[\"role\"]\n\n        if entity == \"allUsers\":\n            entity = self.all()\n\n        elif entity == \"allAuthenticatedUsers\":\n            entity = self.all_authenticated()\n\n        elif \"-\" in entity:\n            entity_type, identifier = entity.split(\"-\", 1)\n            entity = self.entity(entity_type=entity_type, identifier=identifier)\n\n        if not isinstance(entity, _ACLEntity):\n            raise ValueError(f\"Invalid dictionary: {entity_dict}\")\n\n        entity.grant(role)\n        return entity\n\n    def has_entity(self, entity):\n        \"\"\"Returns whether or not this ACL has any entries for an entity.\n\n        :type entity: :class:`_ACLEntity`\n        :param entity: The entity to check for existence in this ACL.\n\n        :rtype: bool\n        :returns: True of the entity exists in the ACL.\n        \"\"\"\n        self._ensure_loaded()\n        return str(entity) in self.entities\n\n    def get_entity(self, entity, default=None):\n        \"\"\"Gets an entity object from the ACL.\n\n        :type entity: :class:`_ACLEntity` or string\n        :param entity: The entity to get lookup in the ACL.\n\n        :type default: anything\n        :param default: This value will be returned if the entity\n                        doesn't exist.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: The corresponding entity or the value provided\n                  to ``default``.\n        \"\"\"\n        self._ensure_loaded()\n        return self.entities.get(str(entity), default)\n\n    def add_entity(self, entity):\n        \"\"\"Add an entity to the ACL.\n\n        :type entity: :class:`_ACLEntity`\n        :param entity: The entity to add to this ACL.\n        \"\"\"\n        self._ensure_loaded()\n        self.entities[str(entity)] = entity\n\n    def entity(self, entity_type, identifier=None):\n        \"\"\"Factory method for creating an Entity.\n\n        If an entity with the same type and identifier already exists,\n        this will return a reference to that entity.  If not, it will\n        create a new one and add it to the list of known entities for\n        this ACL.\n\n        :type entity_type: str\n        :param entity_type: The type of entity to create\n                            (ie, ``user``, ``group``, etc)\n\n        :type identifier: str\n        :param identifier: The ID of the entity (if applicable).\n                           This can be either an ID or an e-mail address.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: A new Entity or a reference to an existing identical entity.\n        \"\"\"\n        entity = _ACLEntity(entity_type=entity_type, identifier=identifier)\n        if self.has_entity(entity):\n            entity = self.get_entity(entity)\n        else:\n            self.add_entity(entity)\n        return entity\n\n    def user(self, identifier):\n        \"\"\"Factory method for a user Entity.\n\n        :type identifier: str\n        :param identifier: An id or e-mail for this particular user.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity corresponding to this user.\n        \"\"\"\n        return self.entity(\"user\", identifier=identifier)\n\n    def group(self, identifier):\n        \"\"\"Factory method for a group Entity.\n\n        :type identifier: str\n        :param identifier: An id or e-mail for this particular group.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity corresponding to this group.\n        \"\"\"\n        return self.entity(\"group\", identifier=identifier)\n\n    def domain(self, domain):\n        \"\"\"Factory method for a domain Entity.\n\n        :type domain: str\n        :param domain: The domain for this entity.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An entity corresponding to this domain.\n        \"\"\"\n        return self.entity(\"domain\", identifier=domain)\n\n    def all(self):\n        \"\"\"Factory method for an Entity representing all users.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An entity representing all users.\n        \"\"\"\n        return self.entity(\"allUsers\")\n\n    def all_authenticated(self):\n        \"\"\"Factory method for an Entity representing all authenticated users.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An entity representing all authenticated users.\n        \"\"\"\n        return self.entity(\"allAuthenticatedUsers\")\n\n    def get_entities(self):\n        \"\"\"Get a list of all Entity objects.\n\n        :rtype: list of :class:`_ACLEntity` objects\n        :returns: A list of all Entity objects.\n        \"\"\"\n        self._ensure_loaded()\n        return list(self.entities.values())\n\n    @property\n    def client(self):\n        \"\"\"Abstract getter for the object client.\"\"\"\n        raise NotImplementedError\n\n    def _require_client(self, client):\n        \"\"\"Check client or verify over-ride.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current ACL.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: The client passed in or the currently bound client.\n        \"\"\"\n        if client is None:\n            client = self.client\n        return client\n\n    def reload(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Reload the ACL data from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: :class:`~google.api_core.retry.Retry`\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        path = self.reload_path\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        self.entities.clear()\n\n        found = client._get_resource(\n            path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        self.loaded = True\n\n        for entry in found.get(\"items\", ()):\n            self.add_entity(self.entity_from_dict(entry))\n\n    def _save(\n        self,\n        acl,\n        predefined,\n        client,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Helper for :meth:`save` and :meth:`save_predefined`.\n\n        :type acl: :class:`google.cloud.storage.acl.ACL`, or a compatible list.\n        :param acl: The ACL object to save.  If left blank, this will save\n                    current entries.\n\n        :type predefined: str\n        :param predefined: An identifier for a predefined ACL.  Must be one of the\n            keys in :attr:`PREDEFINED_JSON_ACLS` If passed, `acl` must be None.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {\"projection\": \"full\"}\n\n        if predefined is not None:\n            acl = []\n            query_params[self._PREDEFINED_QUERY_PARAM] = predefined\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        path = self.save_path\n\n        result = client._patch_resource(\n            path,\n            {self._URL_PATH_ELEM: list(acl)},\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.entities.clear()\n\n        for entry in result.get(self._URL_PATH_ELEM, ()):\n            self.add_entity(self.entity_from_dict(entry))\n\n        self.loaded = True\n\n    def save(\n        self,\n        acl=None,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Save this ACL for the current bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type acl: :class:`google.cloud.storage.acl.ACL`, or a compatible list.\n        :param acl: The ACL object to save.  If left blank, this will save\n                    current entries.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        if acl is None:\n            acl = self\n            save_to_backend = acl.loaded\n        else:\n            save_to_backend = True\n\n        if save_to_backend:\n            self._save(\n                acl,\n                None,\n                client,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                timeout=timeout,\n                retry=retry,\n            )\n\n    def save_predefined(\n        self,\n        predefined,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Save this ACL for the current bucket using a predefined ACL.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type predefined: str\n        :param predefined: An identifier for a predefined ACL.  Must be one\n                           of the keys in :attr:`PREDEFINED_JSON_ACLS`\n                           or :attr:`PREDEFINED_XML_ACLS` (which will be\n                           aliased to the corresponding JSON name).\n                           If passed, `acl` must be None.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        predefined = self.validate_predefined(predefined)\n        self._save(\n            None,\n            predefined,\n            client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def clear(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Remove all ACL entries.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        Note that this won't actually remove *ALL* the rules, but it\n        will remove all the non-default rules.  In short, you'll still\n        have access to a bucket that you created even after you clear\n        ACL rules with this method.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        self.save(\n            [],\n            client=client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n\nclass BucketACL(ACL):\n    \"\"\"An ACL specifically for a bucket.\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket: The bucket to which this ACL relates.\n    \"\"\"\n\n    def __init__(self, bucket):\n        super(BucketACL, self).__init__()\n        self.bucket = bucket\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this ACL's bucket.\"\"\"\n        return self.bucket.client\n\n    @property\n    def reload_path(self):\n        \"\"\"Compute the path for GET API requests for this ACL.\"\"\"\n        return f\"{self.bucket.path}/{self._URL_PATH_ELEM}\"\n\n    @property\n    def save_path(self):\n        \"\"\"Compute the path for PATCH API requests for this ACL.\"\"\"\n        return self.bucket.path\n\n    @property\n    def user_project(self):\n        \"\"\"Compute the user project charged for API requests for this ACL.\"\"\"\n        return self.bucket.user_project\n\n\nclass DefaultObjectACL(BucketACL):\n    \"\"\"A class representing the default object ACL for a bucket.\"\"\"\n\n    _URL_PATH_ELEM = \"defaultObjectAcl\"\n    _PREDEFINED_QUERY_PARAM = \"predefinedDefaultObjectAcl\"\n\n\nclass ObjectACL(ACL):\n    \"\"\"An ACL specifically for a Cloud Storage object / blob.\n\n    :type blob: :class:`google.cloud.storage.blob.Blob`\n    :param blob: The blob that this ACL corresponds to.\n    \"\"\"\n\n    def __init__(self, blob):\n        super(ObjectACL, self).__init__()\n        self.blob = blob\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this ACL's blob.\"\"\"\n        return self.blob.client\n\n    @property\n    def reload_path(self):\n        \"\"\"Compute the path for GET API requests for this ACL.\"\"\"\n        return f\"{self.blob.path}/acl\"\n\n    @property\n    def save_path(self):\n        \"\"\"Compute the path for PATCH API requests for this ACL.\"\"\"\n        return self.blob.path\n\n    @property\n    def user_project(self):\n        \"\"\"Compute the user project charged for API requests for this ACL.\"\"\"\n        return self.blob.user_project\n", "google/cloud/storage/notification.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configure bucket notification resources to interact with Google Cloud Pub/Sub.\n\nSee [Cloud Pub/Sub Notifications for Google Cloud Storage](https://cloud.google.com/storage/docs/pubsub-notifications)\n\"\"\"\n\nimport re\n\nfrom google.api_core.exceptions import NotFound\n\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\nOBJECT_FINALIZE_EVENT_TYPE = \"OBJECT_FINALIZE\"\nOBJECT_METADATA_UPDATE_EVENT_TYPE = \"OBJECT_METADATA_UPDATE\"\nOBJECT_DELETE_EVENT_TYPE = \"OBJECT_DELETE\"\nOBJECT_ARCHIVE_EVENT_TYPE = \"OBJECT_ARCHIVE\"\n\nJSON_API_V1_PAYLOAD_FORMAT = \"JSON_API_V1\"\nNONE_PAYLOAD_FORMAT = \"NONE\"\n\n_TOPIC_REF_FMT = \"//pubsub.googleapis.com/projects/{}/topics/{}\"\n_PROJECT_PATTERN = r\"(?P<project>[a-z][a-z0-9-]{4,28}[a-z0-9])\"\n_TOPIC_NAME_PATTERN = r\"(?P<name>[A-Za-z](\\w|[-_.~+%])+)\"\n_TOPIC_REF_PATTERN = _TOPIC_REF_FMT.format(_PROJECT_PATTERN, _TOPIC_NAME_PATTERN)\n_TOPIC_REF_RE = re.compile(_TOPIC_REF_PATTERN)\n_BAD_TOPIC = (\n    \"Resource has invalid topic: {}; see \"\n    \"https://cloud.google.com/storage/docs/json_api/v1/\"\n    \"notifications/insert#topic\"\n)\n\n\nclass BucketNotification(object):\n    \"\"\"Represent a single notification resource for a bucket.\n\n    See: https://cloud.google.com/storage/docs/json_api/v1/notifications\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket: Bucket to which the notification is bound.\n\n    :type topic_name: str\n    :param topic_name:\n        (Optional) Topic name to which notifications are published.\n\n    :type topic_project: str\n    :param topic_project:\n        (Optional) Project ID of topic to which notifications are published.\n        If not passed, uses the project ID of the bucket's client.\n\n    :type custom_attributes: dict\n    :param custom_attributes:\n        (Optional) Additional attributes passed with notification events.\n\n    :type event_types: list(str)\n    :param event_types:\n        (Optional) Event types for which notification events are published.\n\n    :type blob_name_prefix: str\n    :param blob_name_prefix:\n        (Optional) Prefix of blob names for which notification events are\n        published.\n\n    :type payload_format: str\n    :param payload_format:\n        (Optional) Format of payload for notification events.\n\n    :type notification_id: str\n    :param notification_id:\n        (Optional) The ID of the notification.\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket,\n        topic_name=None,\n        topic_project=None,\n        custom_attributes=None,\n        event_types=None,\n        blob_name_prefix=None,\n        payload_format=NONE_PAYLOAD_FORMAT,\n        notification_id=None,\n    ):\n        self._bucket = bucket\n        self._topic_name = topic_name\n\n        if topic_project is None:\n            topic_project = bucket.client.project\n\n        if topic_project is None:\n            raise ValueError(\"Client project not set:  pass an explicit topic_project.\")\n\n        self._topic_project = topic_project\n\n        self._properties = {}\n\n        if custom_attributes is not None:\n            self._properties[\"custom_attributes\"] = custom_attributes\n\n        if event_types is not None:\n            self._properties[\"event_types\"] = event_types\n\n        if blob_name_prefix is not None:\n            self._properties[\"object_name_prefix\"] = blob_name_prefix\n\n        if notification_id is not None:\n            self._properties[\"id\"] = notification_id\n\n        self._properties[\"payload_format\"] = payload_format\n\n    @classmethod\n    def from_api_repr(cls, resource, bucket):\n        \"\"\"Construct an instance from the JSON repr returned by the server.\n\n        See: https://cloud.google.com/storage/docs/json_api/v1/notifications\n\n        :type resource: dict\n        :param resource: JSON repr of the notification\n\n        :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n        :param bucket: Bucket to which the notification is bound.\n\n        :rtype: :class:`BucketNotification`\n        :returns: the new notification instance\n        \"\"\"\n        topic_path = resource.get(\"topic\")\n        if topic_path is None:\n            raise ValueError(\"Resource has no topic\")\n\n        name, project = _parse_topic_path(topic_path)\n        instance = cls(bucket, name, topic_project=project)\n        instance._properties = resource\n\n        return instance\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket to which the notification is bound.\"\"\"\n        return self._bucket\n\n    @property\n    def topic_name(self):\n        \"\"\"Topic name to which notifications are published.\"\"\"\n        return self._topic_name\n\n    @property\n    def topic_project(self):\n        \"\"\"Project ID of topic to which notifications are published.\"\"\"\n        return self._topic_project\n\n    @property\n    def custom_attributes(self):\n        \"\"\"Custom attributes passed with notification events.\"\"\"\n        return self._properties.get(\"custom_attributes\")\n\n    @property\n    def event_types(self):\n        \"\"\"Event types for which notification events are published.\"\"\"\n        return self._properties.get(\"event_types\")\n\n    @property\n    def blob_name_prefix(self):\n        \"\"\"Prefix of blob names for which notification events are published.\"\"\"\n        return self._properties.get(\"object_name_prefix\")\n\n    @property\n    def payload_format(self):\n        \"\"\"Format of payload of notification events.\"\"\"\n        return self._properties.get(\"payload_format\")\n\n    @property\n    def notification_id(self):\n        \"\"\"Server-set ID of notification resource.\"\"\"\n        return self._properties.get(\"id\")\n\n    @property\n    def etag(self):\n        \"\"\"Server-set ETag of notification resource.\"\"\"\n        return self._properties.get(\"etag\")\n\n    @property\n    def self_link(self):\n        \"\"\"Server-set ETag of notification resource.\"\"\"\n        return self._properties.get(\"selfLink\")\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this notfication.\"\"\"\n        return self.bucket.client\n\n    @property\n    def path(self):\n        \"\"\"The URL path for this notification.\"\"\"\n        return f\"/b/{self.bucket.name}/notificationConfigs/{self.notification_id}\"\n\n    def _require_client(self, client):\n        \"\"\"Check client or verify over-ride.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: The client passed in or the bucket's client.\n        \"\"\"\n        if client is None:\n            client = self.client\n        return client\n\n    def _set_properties(self, response):\n        \"\"\"Helper for :meth:`reload`.\n\n        :type response: dict\n        :param response: resource mapping from server\n        \"\"\"\n        self._properties.clear()\n        self._properties.update(response)\n\n    def create(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=None):\n        \"\"\"API wrapper: create the notification.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/insert\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the notification's bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError: if the notification already exists.\n        \"\"\"\n        if self.notification_id is not None:\n            raise ValueError(\n                f\"notification_id already set to {self.notification_id}; must be None to create a Notification.\"\n            )\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        path = f\"/b/{self.bucket.name}/notificationConfigs\"\n        properties = self._properties.copy()\n\n        if self.topic_name is None:\n            properties[\"topic\"] = _TOPIC_REF_FMT.format(self.topic_project, \"\")\n        else:\n            properties[\"topic\"] = _TOPIC_REF_FMT.format(\n                self.topic_project, self.topic_name\n            )\n\n        self._properties = client._post_resource(\n            path,\n            properties,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def exists(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Test whether this notification exists.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/get\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: bool\n        :returns: True, if the notification exists, else False.\n        :raises ValueError: if the notification has no ID.\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification ID not set: set an explicit notification_id\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        try:\n            client._get_resource(\n                self.path,\n                query_params=query_params,\n                timeout=timeout,\n                retry=retry,\n            )\n        except NotFound:\n            return False\n        else:\n            return True\n\n    def reload(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Update this notification from the server configuration.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/get\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n\n        :raises ValueError: if the notification has no ID.\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification ID not set: set an explicit notification_id\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        response = client._get_resource(\n            self.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        self._set_properties(response)\n\n    def delete(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Delete this notification.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/delete\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises: :class:`google.api_core.exceptions.NotFound`:\n            if the notification does not exist.\n        :raises ValueError: if the notification has no ID.\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification ID not set: set an explicit notification_id\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        client._delete_resource(\n            self.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n\ndef _parse_topic_path(topic_path):\n    \"\"\"Verify that a topic path is in the correct format.\n\n    Expected to be of the form:\n\n        //pubsub.googleapis.com/projects/{project}/topics/{topic}\n\n    where the ``project`` value must be \"6 to 30 lowercase letters, digits,\n    or hyphens. It must start with a letter. Trailing hyphens are prohibited.\"\n    (see [`resource manager docs`](https://cloud.google.com/resource-manager/reference/rest/v1beta1/projects#Project.FIELDS.project_id))\n    and ``topic`` must have length at least two,\n    must start with a letter and may only contain alphanumeric characters or\n    ``-``, ``_``, ``.``, ``~``, ``+`` or ``%`` (i.e characters used for URL\n    encoding, see [`topic spec`](https://cloud.google.com/storage/docs/json_api/v1/notifications/insert#topic)).\n\n    Args:\n        topic_path (str): The topic path to be verified.\n\n    Returns:\n        Tuple[str, str]: The ``project`` and ``topic`` parsed from the\n        ``topic_path``.\n\n    Raises:\n        ValueError: If the topic path is invalid.\n    \"\"\"\n    match = _TOPIC_REF_RE.match(topic_path)\n    if match is None:\n        raise ValueError(_BAD_TOPIC.format(topic_path))\n\n    return match.group(\"name\"), match.group(\"project\")\n", "google/cloud/storage/fileio.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for file-like access of blobs, usually invoked via Blob.open().\"\"\"\n\nimport io\nimport warnings\n\nfrom google.api_core.exceptions import RequestRangeNotSatisfiable\nfrom google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import ConditionalRetryPolicy\n\n\n# Resumable uploads require a chunk size of precisely a multiple of 256 KiB.\nCHUNK_SIZE_MULTIPLE = 256 * 1024  # 256 KiB\nDEFAULT_CHUNK_SIZE = 40 * 1024 * 1024  # 40 MiB\n\n# Valid keyword arguments for download methods, and blob.reload() if needed.\n# Note: Changes here need to be reflected in the blob.open() docstring.\nVALID_DOWNLOAD_KWARGS = {\n    \"if_generation_match\",\n    \"if_generation_not_match\",\n    \"if_metageneration_match\",\n    \"if_metageneration_not_match\",\n    \"timeout\",\n    \"retry\",\n    \"raw_download\",\n}\n\n# Valid keyword arguments for upload methods.\n# Note: Changes here need to be reflected in the blob.open() docstring.\nVALID_UPLOAD_KWARGS = {\n    \"content_type\",\n    \"predefined_acl\",\n    \"num_retries\",\n    \"if_generation_match\",\n    \"if_generation_not_match\",\n    \"if_metageneration_match\",\n    \"if_metageneration_not_match\",\n    \"timeout\",\n    \"checksum\",\n    \"retry\",\n}\n\n\nclass BlobReader(io.BufferedIOBase):\n    \"\"\"A file-like object that reads from a blob.\n\n    :type blob: 'google.cloud.storage.blob.Blob'\n    :param blob:\n        The blob to download.\n\n    :type chunk_size: long\n    :param chunk_size:\n        (Optional) The minimum number of bytes to read at a time. If fewer\n        bytes than the chunk_size are requested, the remainder is buffered.\n        The default is the chunk_size of the blob, or 40MiB.\n\n    :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n    :param retry:\n        (Optional) How to retry the RPC. A None value will disable\n        retries. A google.api_core.retry.Retry value will enable retries,\n        and the object will define retriable response codes and errors and\n        configure backoff and timeout options.\n\n        A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n        Retry object and activates it only if certain conditions are met.\n        This class exists to provide safe defaults for RPC calls that are\n        not technically safe to retry normally (due to potential data\n        duplication or other side-effects) but become safe to retry if a\n        condition such as if_metageneration_match is set.\n\n        See the retry.py source code and docstrings in this package\n        (google.cloud.storage.retry) for information on retry types and how\n        to configure them.\n\n        Media operations (downloads and uploads) do not support non-default\n        predicates in a Retry object. The default will always be used. Other\n        configuration changes for Retry objects such as delays and deadlines\n        are respected.\n\n    :param download_kwargs:\n        Keyword arguments to pass to the underlying API calls.\n        The following arguments are supported:\n\n        - ``if_generation_match``\n        - ``if_generation_not_match``\n        - ``if_metageneration_match``\n        - ``if_metageneration_not_match``\n        - ``timeout``\n\n        Note that download_kwargs are also applied to blob.reload(), if a reload\n        is needed during seek().\n    \"\"\"\n\n    def __init__(self, blob, chunk_size=None, retry=DEFAULT_RETRY, **download_kwargs):\n        for kwarg in download_kwargs:\n            if kwarg not in VALID_DOWNLOAD_KWARGS:\n                raise ValueError(\n                    f\"BlobReader does not support keyword argument {kwarg}.\"\n                )\n\n        self._blob = blob\n        self._pos = 0\n        self._buffer = io.BytesIO()\n        self._chunk_size = chunk_size or blob.chunk_size or DEFAULT_CHUNK_SIZE\n        self._retry = retry\n        self._download_kwargs = download_kwargs\n\n    def read(self, size=-1):\n        self._checkClosed()  # Raises ValueError if closed.\n\n        result = self._buffer.read(size)\n        # If the read request demands more bytes than are buffered, fetch more.\n        remaining_size = size - len(result)\n        if remaining_size > 0 or size < 0:\n            self._pos += self._buffer.tell()\n            read_size = len(result)\n\n            self._buffer.seek(0)\n            self._buffer.truncate(0)  # Clear the buffer to make way for new data.\n            fetch_start = self._pos\n            if size > 0:\n                # Fetch the larger of self._chunk_size or the remaining_size.\n                fetch_end = fetch_start + max(remaining_size, self._chunk_size)\n            else:\n                fetch_end = None\n\n            # Download the blob. Checksumming must be disabled as we are using\n            # chunked downloads, and the server only knows the checksum of the\n            # entire file.\n            try:\n                result += self._blob.download_as_bytes(\n                    start=fetch_start,\n                    end=fetch_end,\n                    checksum=None,\n                    retry=self._retry,\n                    **self._download_kwargs,\n                )\n            except RequestRangeNotSatisfiable:\n                # We've reached the end of the file. Python file objects should\n                # return an empty response in this case, not raise an error.\n                pass\n\n            # If more bytes were read than is immediately needed, buffer the\n            # remainder and then trim the result.\n            if size > 0 and len(result) > size:\n                self._buffer.write(result[size:])\n                self._buffer.seek(0)\n                result = result[:size]\n            # Increment relative offset by true amount read.\n            self._pos += len(result) - read_size\n        return result\n\n    def read1(self, size=-1):\n        return self.read(size)\n\n    def seek(self, pos, whence=0):\n        \"\"\"Seek within the blob.\n\n        This implementation of seek() uses knowledge of the blob size to\n        validate that the reported position does not exceed the blob last byte.\n        If the blob size is not already known it will call blob.reload().\n        \"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        if self._blob.size is None:\n            self._blob.reload(**self._download_kwargs)\n\n        initial_offset = self._pos + self._buffer.tell()\n\n        if whence == 0:\n            target_pos = pos\n        elif whence == 1:\n            target_pos = initial_offset + pos\n        elif whence == 2:\n            target_pos = self._blob.size + pos\n        if whence not in {0, 1, 2}:\n            raise ValueError(\"invalid whence value\")\n\n        if target_pos > self._blob.size:\n            target_pos = self._blob.size\n\n        # Seek or invalidate buffer as needed.\n        if target_pos < self._pos:\n            # Target position < relative offset <= true offset.\n            # As data is not in buffer, invalidate buffer.\n            self._buffer.seek(0)\n            self._buffer.truncate(0)\n            new_pos = target_pos\n            self._pos = target_pos\n        else:\n            # relative offset <= target position <= size of file.\n            difference = target_pos - initial_offset\n            new_pos = self._pos + self._buffer.seek(difference, 1)\n        return new_pos\n\n    def close(self):\n        self._buffer.close()\n\n    @property\n    def closed(self):\n        return self._buffer.closed\n\n    def readable(self):\n        return True\n\n    def writable(self):\n        return False\n\n    def seekable(self):\n        return True\n\n\nclass BlobWriter(io.BufferedIOBase):\n    \"\"\"A file-like object that writes to a blob.\n\n    :type blob: 'google.cloud.storage.blob.Blob'\n    :param blob:\n        The blob to which to write.\n\n    :type chunk_size: long\n    :param chunk_size:\n        (Optional) The maximum number of bytes to buffer before sending data\n        to the server, and the size of each request when data is sent.\n        Writes are implemented as a \"resumable upload\", so chunk_size for\n        writes must be exactly a multiple of 256KiB as with other resumable\n        uploads. The default is the chunk_size of the blob, or 40 MiB.\n\n    :type text_mode: bool\n    :param text_mode:\n        (Deprecated) A synonym for ignore_flush. For backwards-compatibility,\n        if True, sets ignore_flush to True. Use ignore_flush instead. This\n        parameter will be removed in a future release.\n\n    :type ignore_flush: bool\n    :param ignore_flush:\n        Makes flush() do nothing instead of raise an error. flush() without\n        closing is not supported by the remote service and therefore calling it\n        on this class normally results in io.UnsupportedOperation. However, that\n        behavior is incompatible with some consumers and wrappers of file\n        objects in Python, such as zipfile.ZipFile or io.TextIOWrapper. Setting\n        ignore_flush will cause flush() to successfully do nothing, for\n        compatibility with those contexts. The correct way to actually flush\n        data to the remote server is to close() (using this object as a context\n        manager is recommended).\n\n    :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n    :param retry:\n        (Optional) How to retry the RPC. A None value will disable\n        retries. A google.api_core.retry.Retry value will enable retries,\n        and the object will define retriable response codes and errors and\n        configure backoff and timeout options.\n\n        A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n        Retry object and activates it only if certain conditions are met.\n        This class exists to provide safe defaults for RPC calls that are\n        not technically safe to retry normally (due to potential data\n        duplication or other side-effects) but become safe to retry if a\n        condition such as if_metageneration_match is set.\n\n        See the retry.py source code and docstrings in this package\n        (google.cloud.storage.retry) for information on retry types and how\n        to configure them.\n\n        Media operations (downloads and uploads) do not support non-default\n        predicates in a Retry object. The default will always be used. Other\n        configuration changes for Retry objects such as delays and deadlines\n        are respected.\n\n    :param upload_kwargs:\n        Keyword arguments to pass to the underlying API\n        calls. The following arguments are supported:\n\n        - ``if_generation_match``\n        - ``if_generation_not_match``\n        - ``if_metageneration_match``\n        - ``if_metageneration_not_match``\n        - ``timeout``\n        - ``content_type``\n        - ``num_retries``\n        - ``predefined_acl``\n        - ``checksum``\n    \"\"\"\n\n    def __init__(\n        self,\n        blob,\n        chunk_size=None,\n        text_mode=False,\n        ignore_flush=False,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        **upload_kwargs,\n    ):\n        for kwarg in upload_kwargs:\n            if kwarg not in VALID_UPLOAD_KWARGS:\n                raise ValueError(\n                    f\"BlobWriter does not support keyword argument {kwarg}.\"\n                )\n        self._blob = blob\n        self._buffer = SlidingBuffer()\n        self._upload_and_transport = None\n        # Resumable uploads require a chunk size of a multiple of 256KiB.\n        # self._chunk_size must not be changed after the upload is initiated.\n        self._chunk_size = chunk_size or blob.chunk_size or DEFAULT_CHUNK_SIZE\n        # text_mode is a deprecated synonym for ignore_flush\n        self._ignore_flush = ignore_flush or text_mode\n        self._retry = retry\n        self._upload_kwargs = upload_kwargs\n\n    @property\n    def _chunk_size(self):\n        \"\"\"Get the blob's default chunk size.\n\n        :rtype: int or ``NoneType``\n        :returns: The current blob's chunk size, if it is set.\n        \"\"\"\n        return self.__chunk_size\n\n    @_chunk_size.setter\n    def _chunk_size(self, value):\n        \"\"\"Set the blob's default chunk size.\n\n        :type value: int\n        :param value: (Optional) The current blob's chunk size, if it is set.\n\n        :raises: :class:`ValueError` if ``value`` is not ``None`` and is not a\n                 multiple of 256 KiB.\n        \"\"\"\n        if value is not None and value > 0 and value % CHUNK_SIZE_MULTIPLE != 0:\n            raise ValueError(\n                \"Chunk size must be a multiple of %d.\" % CHUNK_SIZE_MULTIPLE\n            )\n        self.__chunk_size = value\n\n    def write(self, b):\n        self._checkClosed()  # Raises ValueError if closed.\n\n        pos = self._buffer.write(b)\n\n        # If there is enough content, upload chunks.\n        num_chunks = len(self._buffer) // self._chunk_size\n        if num_chunks:\n            self._upload_chunks_from_buffer(num_chunks)\n\n        return pos\n\n    def _initiate_upload(self):\n        # num_retries is only supported for backwards-compatibility reasons.\n        num_retries = self._upload_kwargs.pop(\"num_retries\", None)\n        retry = self._retry\n        content_type = self._upload_kwargs.pop(\"content_type\", None)\n\n        if num_retries is not None:\n            warnings.warn(_NUM_RETRIES_MESSAGE, DeprecationWarning, stacklevel=2)\n            # num_retries and retry are mutually exclusive. If num_retries is\n            # set and retry is exactly the default, then nullify retry for\n            # backwards compatibility.\n            if retry is DEFAULT_RETRY_IF_GENERATION_SPECIFIED:\n                retry = None\n\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": self._upload_kwargs.get(\"if_generation_match\"),\n                \"ifMetagenerationMatch\": self._upload_kwargs.get(\n                    \"if_metageneration_match\"\n                ),\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        self._upload_and_transport = self._blob._initiate_resumable_upload(\n            self._blob.bucket.client,\n            self._buffer,\n            content_type,\n            None,\n            num_retries,\n            chunk_size=self._chunk_size,\n            retry=retry,\n            **self._upload_kwargs,\n        )\n\n    def _upload_chunks_from_buffer(self, num_chunks):\n        \"\"\"Upload a specified number of chunks.\"\"\"\n\n        # Initialize the upload if necessary.\n        if not self._upload_and_transport:\n            self._initiate_upload()\n\n        upload, transport = self._upload_and_transport\n\n        # Attach timeout if specified in the keyword arguments.\n        # Otherwise, the default timeout will be used from the media library.\n        kwargs = {}\n        if \"timeout\" in self._upload_kwargs:\n            kwargs = {\"timeout\": self._upload_kwargs.get(\"timeout\")}\n\n        # Upload chunks. The SlidingBuffer class will manage seek position.\n        for _ in range(num_chunks):\n            upload.transmit_next_chunk(transport, **kwargs)\n\n        # Wipe the buffer of chunks uploaded, preserving any remaining data.\n        self._buffer.flush()\n\n    def tell(self):\n        return self._buffer.tell() + len(self._buffer)\n\n    def flush(self):\n        # flush() is not fully supported by the remote service, so raise an\n        # error here, unless self._ignore_flush is set.\n        if not self._ignore_flush:\n            raise io.UnsupportedOperation(\n                \"Cannot flush without finalizing upload. Use close() instead, \"\n                \"or set ignore_flush=True when constructing this class (see \"\n                \"docstring).\"\n            )\n\n    def close(self):\n        if not self._buffer.closed:\n            self._upload_chunks_from_buffer(1)\n        self._buffer.close()\n\n    @property\n    def closed(self):\n        return self._buffer.closed\n\n    def readable(self):\n        return False\n\n    def writable(self):\n        return True\n\n    def seekable(self):\n        return False\n\n\nclass SlidingBuffer(object):\n    \"\"\"A non-rewindable buffer that frees memory of chunks already consumed.\n\n    This class is necessary because `google-resumable-media-python` expects\n    `tell()` to work relative to the start of the file, not relative to a place\n    in an intermediate buffer. Using this class, we present an external\n    interface with consistent seek and tell behavior without having to actually\n    store bytes already sent.\n\n    Behavior of this class differs from an ordinary BytesIO buffer. `write()`\n    will always append to the end of the file only and not change the seek\n    position otherwise. `flush()` will delete all data already read (data to the\n    left of the seek position). `tell()` will report the seek position of the\n    buffer including all deleted data. Additionally the class implements\n    __len__() which will report the size of the actual underlying buffer.\n\n    This class does not attempt to implement the entire Python I/O interface.\n    \"\"\"\n\n    def __init__(self):\n        self._buffer = io.BytesIO()\n        self._cursor = 0\n\n    def write(self, b):\n        \"\"\"Append to the end of the buffer without changing the position.\"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        bookmark = self._buffer.tell()\n        self._buffer.seek(0, io.SEEK_END)\n        pos = self._buffer.write(b)\n        self._buffer.seek(bookmark)\n        return self._cursor + pos\n\n    def read(self, size=-1):\n        \"\"\"Read and move the cursor.\"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        data = self._buffer.read(size)\n        self._cursor += len(data)\n        return data\n\n    def flush(self):\n        \"\"\"Delete already-read data (all data to the left of the position).\"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        # BytesIO can't be deleted from the left, so save any leftover, unread\n        # data and truncate at 0, then readd leftover data.\n        leftover = self._buffer.read()\n        self._buffer.seek(0)\n        self._buffer.truncate(0)\n        self._buffer.write(leftover)\n        self._buffer.seek(0)\n\n    def tell(self):\n        \"\"\"Report how many bytes have been read from the buffer in total.\"\"\"\n        return self._cursor\n\n    def seek(self, pos):\n        \"\"\"Seek to a position (backwards only) within the internal buffer.\n\n        This implementation of seek() verifies that the seek destination is\n        contained in _buffer. It will raise ValueError if the destination byte\n        has already been purged from the buffer.\n\n        The \"whence\" argument is not supported in this implementation.\n        \"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        buffer_initial_pos = self._buffer.tell()\n        difference = pos - self._cursor\n        buffer_seek_result = self._buffer.seek(difference, io.SEEK_CUR)\n        if (\n            not buffer_seek_result - buffer_initial_pos == difference\n            or pos > self._cursor\n        ):\n            # The seek did not arrive at the expected byte because the internal\n            # buffer does not (or no longer) contains the byte. Reset and raise.\n            self._buffer.seek(buffer_initial_pos)\n            raise ValueError(\"Cannot seek() to that value.\")\n\n        self._cursor = pos\n        return self._cursor\n\n    def __len__(self):\n        \"\"\"Determine the size of the buffer by seeking to the end.\"\"\"\n        bookmark = self._buffer.tell()\n        length = self._buffer.seek(0, io.SEEK_END)\n        self._buffer.seek(bookmark)\n        return length\n\n    def close(self):\n        return self._buffer.close()\n\n    def _checkClosed(self):\n        return self._buffer._checkClosed()\n\n    @property\n    def closed(self):\n        return self._buffer.closed\n", "google/cloud/storage/constants.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Constants used across google.cloud.storage modules.\n\nSee [Python Storage Client Constants Page](https://github.com/googleapis/python-storage/blob/main/google/cloud/storage/constants.py)\nfor constants used across storage classes, location types, public access prevention, etc.\n\n\"\"\"\n\n# Storage classes\n\nSTANDARD_STORAGE_CLASS = \"STANDARD\"\n\"\"\"Storage class for objects accessed more than once per month.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nNEARLINE_STORAGE_CLASS = \"NEARLINE\"\n\"\"\"Storage class for objects accessed at most once per month.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nCOLDLINE_STORAGE_CLASS = \"COLDLINE\"\n\"\"\"Storage class for objects accessed at most once per year.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nARCHIVE_STORAGE_CLASS = \"ARCHIVE\"\n\"\"\"Storage class for objects accessed less frequently than once per year.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nMULTI_REGIONAL_LEGACY_STORAGE_CLASS = \"MULTI_REGIONAL\"\n\"\"\"Legacy storage class.\n\nAlias for :attr:`STANDARD_STORAGE_CLASS`.\n\nCan only be used for objects in buckets whose\n:attr:`~google.cloud.storage.bucket.Bucket.location_type` is\n:attr:`~google.cloud.storage.bucket.Bucket.MULTI_REGION_LOCATION_TYPE`.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nREGIONAL_LEGACY_STORAGE_CLASS = \"REGIONAL\"\n\"\"\"Legacy storage class.\n\nAlias for :attr:`STANDARD_STORAGE_CLASS`.\n\nCan only be used for objects in buckets whose\n:attr:`~google.cloud.storage.bucket.Bucket.location_type` is\n:attr:`~google.cloud.storage.bucket.Bucket.REGION_LOCATION_TYPE`.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nDURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS = \"DURABLE_REDUCED_AVAILABILITY\"\n\"\"\"Legacy storage class.\n\nSimilar to :attr:`NEARLINE_STORAGE_CLASS`.\n\"\"\"\n\n\n# Location types\n\nMULTI_REGION_LOCATION_TYPE = \"multi-region\"\n\"\"\"Location type: data will be replicated across regions in a multi-region.\n\nProvides highest availability across largest area.\n\"\"\"\n\nREGION_LOCATION_TYPE = \"region\"\n\"\"\"Location type: data will be stored within a single region.\n\nProvides lowest latency within a single region.\n\"\"\"\n\nDUAL_REGION_LOCATION_TYPE = \"dual-region\"\n\"\"\"Location type: data will be stored within two primary regions.\n\nProvides high availability and low latency across two regions.\n\"\"\"\n\n\n# Internal constants\n\n_DEFAULT_TIMEOUT = 60  # in seconds\n\"\"\"The default request timeout in seconds if a timeout is not explicitly given.\n\"\"\"\n\n# Public Access Prevention\nPUBLIC_ACCESS_PREVENTION_ENFORCED = \"enforced\"\n\"\"\"Enforced public access prevention value.\n\nSee: https://cloud.google.com/storage/docs/public-access-prevention\n\"\"\"\n\nPUBLIC_ACCESS_PREVENTION_UNSPECIFIED = \"unspecified\"\n\"\"\"Unspecified public access prevention value.\n\nDEPRECATED: Use 'PUBLIC_ACCESS_PREVENTION_INHERITED' instead.\n\nSee: https://cloud.google.com/storage/docs/public-access-prevention\n\"\"\"\n\nPUBLIC_ACCESS_PREVENTION_INHERITED = \"inherited\"\n\"\"\"Inherited public access prevention value.\n\nSee: https://cloud.google.com/storage/docs/public-access-prevention\n\"\"\"\n\nRPO_ASYNC_TURBO = \"ASYNC_TURBO\"\n\"\"\"The recovery point objective (RPO) indicates how quickly newly written objects are asynchronously replicated to a separate geographic location.\nWhen the RPO value is set to ASYNC_TURBO, the turbo replication feature is enabled.\n\nSee: https://cloud.google.com/storage/docs/managing-turbo-replication\n\"\"\"\n\nRPO_DEFAULT = \"DEFAULT\"\n\"\"\"The recovery point objective (RPO) indicates how quickly newly written objects are asynchronously replicated to a separate geographic location.\nWhen the RPO value is set to DEFAULT, the default replication behavior is enabled.\n\nSee: https://cloud.google.com/storage/docs/managing-turbo-replication\n\"\"\"\n", "google/cloud/storage/blob.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: disable=too-many-lines\n\n\"\"\"Create / interact with Google Cloud Storage blobs.\n\"\"\"\n\nimport base64\nimport copy\nimport hashlib\nfrom io import BytesIO\nfrom io import TextIOWrapper\nimport logging\nimport mimetypes\nimport os\nimport re\nfrom email.parser import HeaderParser\nfrom urllib.parse import parse_qsl\nfrom urllib.parse import quote\nfrom urllib.parse import urlencode\nfrom urllib.parse import urlsplit\nfrom urllib.parse import urlunsplit\nimport warnings\n\nfrom google import resumable_media\nfrom google.resumable_media.requests import ChunkedDownload\nfrom google.resumable_media.requests import Download\nfrom google.resumable_media.requests import RawDownload\nfrom google.resumable_media.requests import RawChunkedDownload\nfrom google.resumable_media.requests import MultipartUpload\nfrom google.resumable_media.requests import ResumableUpload\n\nfrom google.api_core.iam import Policy\nfrom google.cloud import exceptions\nfrom google.cloud._helpers import _bytes_to_unicode\nfrom google.cloud._helpers import _datetime_to_rfc3339\nfrom google.cloud._helpers import _rfc3339_nanos_to_datetime\nfrom google.cloud._helpers import _to_bytes\nfrom google.cloud.exceptions import NotFound\nfrom google.cloud.storage._helpers import _add_etag_match_headers\nfrom google.cloud.storage._helpers import _add_generation_match_parameters\nfrom google.cloud.storage._helpers import _PropertyMixin\nfrom google.cloud.storage._helpers import _scalar_property\nfrom google.cloud.storage._helpers import _bucket_bound_hostname_url\nfrom google.cloud.storage._helpers import _raise_if_more_than_one_set\nfrom google.cloud.storage._helpers import _api_core_retry_to_resumable_media_retry\nfrom google.cloud.storage._helpers import _get_default_headers\nfrom google.cloud.storage._helpers import _get_default_storage_base_url\nfrom google.cloud.storage._signing import generate_signed_url_v2\nfrom google.cloud.storage._signing import generate_signed_url_v4\nfrom google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\nfrom google.cloud.storage._helpers import _API_VERSION\nfrom google.cloud.storage._helpers import _virtual_hosted_style_base_url\nfrom google.cloud.storage.acl import ACL\nfrom google.cloud.storage.acl import ObjectACL\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.constants import ARCHIVE_STORAGE_CLASS\nfrom google.cloud.storage.constants import COLDLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import MULTI_REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import NEARLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import STANDARD_STORAGE_CLASS\nfrom google.cloud.storage.retry import ConditionalRetryPolicy\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\nfrom google.cloud.storage.fileio import BlobReader\nfrom google.cloud.storage.fileio import BlobWriter\n\n\n_DEFAULT_CONTENT_TYPE = \"application/octet-stream\"\n_DOWNLOAD_URL_TEMPLATE = \"{hostname}/download/storage/{api_version}{path}?alt=media\"\n_BASE_UPLOAD_TEMPLATE = (\n    \"{hostname}/upload/storage/{api_version}{bucket_path}/o?uploadType=\"\n)\n_MULTIPART_URL_TEMPLATE = _BASE_UPLOAD_TEMPLATE + \"multipart\"\n_RESUMABLE_URL_TEMPLATE = _BASE_UPLOAD_TEMPLATE + \"resumable\"\n# NOTE: \"acl\" is also writeable but we defer ACL management to\n#       the classes in the google.cloud.storage.acl module.\n_CONTENT_TYPE_FIELD = \"contentType\"\n_WRITABLE_FIELDS = (\n    \"cacheControl\",\n    \"contentDisposition\",\n    \"contentEncoding\",\n    \"contentLanguage\",\n    _CONTENT_TYPE_FIELD,\n    \"crc32c\",\n    \"customTime\",\n    \"md5Hash\",\n    \"metadata\",\n    \"name\",\n    \"retention\",\n    \"storageClass\",\n)\n_READ_LESS_THAN_SIZE = (\n    \"Size {:d} was specified but the file-like object only had \" \"{:d} bytes remaining.\"\n)\n_CHUNKED_DOWNLOAD_CHECKSUM_MESSAGE = (\n    \"A checksum of type `{}` was requested, but checksumming is not available \"\n    \"for downloads when chunk_size is set.\"\n)\n_COMPOSE_IF_GENERATION_LIST_DEPRECATED = (\n    \"'if_generation_match: type list' is deprecated and supported for \"\n    \"backwards-compatability reasons only.  Use 'if_source_generation_match' \"\n    \"instead' to match source objects' generations.\"\n)\n_COMPOSE_IF_GENERATION_LIST_AND_IF_SOURCE_GENERATION_ERROR = (\n    \"Use 'if_generation_match' to match the generation of the destination \"\n    \"object by passing in a generation number, instead of a list. \"\n    \"Use 'if_source_generation_match' to match source objects generations.\"\n)\n_COMPOSE_IF_METAGENERATION_LIST_DEPRECATED = (\n    \"'if_metageneration_match: type list' is deprecated and supported for \"\n    \"backwards-compatability reasons only. Note that the metageneration to \"\n    \"be matched is that of the destination blob. Please pass in a single \"\n    \"value (type long).\"\n)\n_COMPOSE_IF_SOURCE_GENERATION_MISMATCH_ERROR = (\n    \"'if_source_generation_match' length must be the same as 'sources' length\"\n)\n_DOWNLOAD_AS_STRING_DEPRECATED = (\n    \"Blob.download_as_string() is deprecated and will be removed in future. \"\n    \"Use Blob.download_as_bytes() instead.\"\n)\n_GS_URL_REGEX_PATTERN = re.compile(\n    r\"(?P<scheme>gs)://(?P<bucket_name>[a-z0-9_.-]+)/(?P<object_name>.+)\"\n)\n\n_DEFAULT_CHUNKSIZE = 104857600  # 1024 * 1024 B * 100 = 100 MB\n_MAX_MULTIPART_SIZE = 8388608  # 8 MB\n\n_logger = logging.getLogger(__name__)\n\n\nclass Blob(_PropertyMixin):\n    \"\"\"A wrapper around Cloud Storage's concept of an ``Object``.\n\n    :type name: str\n    :param name: The name of the blob.  This corresponds to the unique path of\n                 the object in the bucket. If bytes, will be converted to a\n                 unicode object. Blob / object names can contain any sequence\n                 of valid unicode characters, of length 1-1024 bytes when\n                 UTF-8 encoded.\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket: The bucket to which this blob belongs.\n\n    :type chunk_size: int\n    :param chunk_size:\n        (Optional) The size of a chunk of data whenever iterating (in bytes).\n        This must be a multiple of 256 KB per the API specification. If not\n        specified, the chunk_size of the blob itself is used. If that is not\n        specified, a default value of 40 MB is used.\n\n    :type encryption_key: bytes\n    :param encryption_key:\n        (Optional) 32 byte encryption key for customer-supplied encryption.\n        See https://cloud.google.com/storage/docs/encryption#customer-supplied.\n\n    :type kms_key_name: str\n    :param kms_key_name:\n        (Optional) Resource name of Cloud KMS key used to encrypt the blob's\n        contents.\n\n    :type generation: long\n    :param generation:\n        (Optional) If present, selects a specific revision of this object.\n    \"\"\"\n\n    _chunk_size = None  # Default value for each instance.\n    _CHUNK_SIZE_MULTIPLE = 256 * 1024\n    \"\"\"Number (256 KB, in bytes) that must divide the chunk size.\"\"\"\n\n    STORAGE_CLASSES = (\n        STANDARD_STORAGE_CLASS,\n        NEARLINE_STORAGE_CLASS,\n        COLDLINE_STORAGE_CLASS,\n        ARCHIVE_STORAGE_CLASS,\n        MULTI_REGIONAL_LEGACY_STORAGE_CLASS,\n        REGIONAL_LEGACY_STORAGE_CLASS,\n    )\n    \"\"\"Allowed values for :attr:`storage_class`.\n\n    See\n    https://cloud.google.com/storage/docs/json_api/v1/objects#storageClass\n    https://cloud.google.com/storage/docs/per-object-storage-class\n\n    .. note::\n       This list does not include 'DURABLE_REDUCED_AVAILABILITY', which\n       is only documented for buckets (and deprecated).\n    \"\"\"\n\n    def __init__(\n        self,\n        name,\n        bucket,\n        chunk_size=None,\n        encryption_key=None,\n        kms_key_name=None,\n        generation=None,\n    ):\n        \"\"\"\n        property :attr:`name`\n            Get the blob's name.\n        \"\"\"\n        name = _bytes_to_unicode(name)\n        super(Blob, self).__init__(name=name)\n\n        self.chunk_size = chunk_size  # Check that setter accepts value.\n        self._bucket = bucket\n        self._acl = ObjectACL(self)\n        _raise_if_more_than_one_set(\n            encryption_key=encryption_key, kms_key_name=kms_key_name\n        )\n\n        self._encryption_key = encryption_key\n\n        if kms_key_name is not None:\n            self._properties[\"kmsKeyName\"] = kms_key_name\n\n        if generation is not None:\n            self._properties[\"generation\"] = generation\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket which contains the object.\n\n        :rtype: :class:`~google.cloud.storage.bucket.Bucket`\n        :returns: The object's bucket.\n        \"\"\"\n        return self._bucket\n\n    @property\n    def chunk_size(self):\n        \"\"\"Get the blob's default chunk size.\n\n        :rtype: int or ``NoneType``\n        :returns: The current blob's chunk size, if it is set.\n        \"\"\"\n        return self._chunk_size\n\n    @chunk_size.setter\n    def chunk_size(self, value):\n        \"\"\"Set the blob's default chunk size.\n\n        :type value: int\n        :param value: (Optional) The current blob's chunk size, if it is set.\n\n        :raises: :class:`ValueError` if ``value`` is not ``None`` and is not a\n                 multiple of 256 KB.\n        \"\"\"\n        if value is not None and value > 0 and value % self._CHUNK_SIZE_MULTIPLE != 0:\n            raise ValueError(\n                \"Chunk size must be a multiple of %d.\" % (self._CHUNK_SIZE_MULTIPLE,)\n            )\n        self._chunk_size = value\n\n    @property\n    def encryption_key(self):\n        \"\"\"Retrieve the customer-supplied encryption key for the object.\n\n        :rtype: bytes or ``NoneType``\n        :returns:\n            The encryption key or ``None`` if no customer-supplied encryption key was used,\n            or the blob's resource has not been loaded from the server.\n        \"\"\"\n        return self._encryption_key\n\n    @encryption_key.setter\n    def encryption_key(self, value):\n        \"\"\"Set the blob's encryption key.\n\n        See https://cloud.google.com/storage/docs/encryption#customer-supplied\n\n        To perform a key rotation for an encrypted blob, use :meth:`rewrite`.\n        See https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys?hl=ca#rotating\n\n        :type value: bytes\n        :param value: 32 byte encryption key for customer-supplied encryption.\n        \"\"\"\n        self._encryption_key = value\n\n    @staticmethod\n    def path_helper(bucket_path, blob_name):\n        \"\"\"Relative URL path for a blob.\n\n        :type bucket_path: str\n        :param bucket_path: The URL path for a bucket.\n\n        :type blob_name: str\n        :param blob_name: The name of the blob.\n\n        :rtype: str\n        :returns: The relative URL path for ``blob_name``.\n        \"\"\"\n        return bucket_path + \"/o/\" + _quote(blob_name)\n\n    @property\n    def acl(self):\n        \"\"\"Create our ACL on demand.\"\"\"\n        return self._acl\n\n    def __repr__(self):\n        if self.bucket:\n            bucket_name = self.bucket.name\n        else:\n            bucket_name = None\n\n        return f\"<Blob: {bucket_name}, {self.name}, {self.generation}>\"\n\n    @property\n    def path(self):\n        \"\"\"Getter property for the URL path to this Blob.\n\n        :rtype: str\n        :returns: The URL path to this Blob.\n        \"\"\"\n        if not self.name:\n            raise ValueError(\"Cannot determine path without a blob name.\")\n\n        return self.path_helper(self.bucket.path, self.name)\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this blob.\"\"\"\n        return self.bucket.client\n\n    @property\n    def user_project(self):\n        \"\"\"Project ID billed for API requests made via this blob.\n\n        Derived from bucket's value.\n\n        :rtype: str\n        \"\"\"\n        return self.bucket.user_project\n\n    def _encryption_headers(self):\n        \"\"\"Return any encryption headers needed to fetch the object.\n\n        :rtype: List(Tuple(str, str))\n        :returns: a list of tuples to be passed as headers.\n        \"\"\"\n        return _get_encryption_headers(self._encryption_key)\n\n    @property\n    def _query_params(self):\n        \"\"\"Default query parameters.\"\"\"\n        params = {}\n        if self.generation is not None:\n            params[\"generation\"] = self.generation\n        if self.user_project is not None:\n            params[\"userProject\"] = self.user_project\n        return params\n\n    @property\n    def public_url(self):\n        \"\"\"The public URL for this blob.\n\n        Use :meth:`make_public` to enable anonymous access via the returned\n        URL.\n\n        :rtype: `string`\n        :returns: The public URL for this blob.\n        \"\"\"\n        if self.client:\n            endpoint = self.client.api_endpoint\n        else:\n            endpoint = _get_default_storage_base_url()\n        return \"{storage_base_url}/{bucket_name}/{quoted_name}\".format(\n            storage_base_url=endpoint,\n            bucket_name=self.bucket.name,\n            quoted_name=_quote(self.name, safe=b\"/~\"),\n        )\n\n    @classmethod\n    def from_string(cls, uri, client=None):\n        \"\"\"Get a constructor for blob object by URI.\n\n        .. code-block:: python\n\n            from google.cloud import storage\n            from google.cloud.storage.blob import Blob\n            client = storage.Client()\n            blob = Blob.from_string(\"gs://bucket/object\", client=client)\n\n        :type uri: str\n        :param uri: The blob uri following a gs://bucket/object pattern.\n          Both a bucket and object name is required to construct a blob object.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  Application code should\n            *always* pass ``client``.\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The blob object created.\n        \"\"\"\n        from google.cloud.storage.bucket import Bucket\n\n        match = _GS_URL_REGEX_PATTERN.match(uri)\n        if not match:\n            raise ValueError(\"URI pattern must be gs://bucket/object\")\n        bucket = Bucket(client, name=match.group(\"bucket_name\"))\n        return cls(match.group(\"object_name\"), bucket)\n\n    def generate_signed_url(\n        self,\n        expiration=None,\n        api_access_endpoint=None,\n        method=\"GET\",\n        content_md5=None,\n        content_type=None,\n        response_disposition=None,\n        response_type=None,\n        generation=None,\n        headers=None,\n        query_parameters=None,\n        client=None,\n        credentials=None,\n        version=None,\n        service_account_email=None,\n        access_token=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n    ):\n        \"\"\"Generates a signed URL for this blob.\n\n        .. note::\n\n            If you are on Google Compute Engine, you can't generate a signed\n            URL using GCE service account.\n            If you'd like to be able to generate a signed URL from GCE,\n            you can use a standard service account from a JSON file rather\n            than a GCE service account.\n\n        If you have a blob that you want to allow access to for a set\n        amount of time, you can use this method to generate a URL that\n        is only valid within a certain time period.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-generate-signed-url-v4#storage_generate_signed_url_v4-python).\n\n        This is particularly useful if you don't want publicly\n        accessible blobs, but don't want to require users to explicitly\n        log in.\n\n        If ``bucket_bound_hostname`` is set as an argument of :attr:`api_access_endpoint`,\n        ``https`` works only if using a ``CDN``.\n\n        :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n        :param expiration:\n            Point in time when the signed URL should expire. If a ``datetime``\n            instance is passed without an explicit ``tzinfo`` set,  it will be\n            assumed to be ``UTC``.\n\n        :type api_access_endpoint: str\n        :param api_access_endpoint: (Optional) URI base, for instance\n            \"https://storage.googleapis.com\". If not specified, the client's\n            api_endpoint will be used. Incompatible with bucket_bound_hostname.\n\n        :type method: str\n        :param method: The HTTP verb that will be used when requesting the URL.\n\n        :type content_md5: str\n        :param content_md5:\n            (Optional) The MD5 hash of the object referenced by ``resource``.\n\n        :type content_type: str\n        :param content_type:\n            (Optional) The content type of the object referenced by\n            ``resource``.\n\n        :type response_disposition: str\n        :param response_disposition:\n            (Optional) Content disposition of responses to requests for the\n            signed URL.  For example, to enable the signed URL to initiate a\n            file of ``blog.png``, use the value ``'attachment;\n            filename=blob.png'``.\n\n        :type response_type: str\n        :param response_type:\n            (Optional) Content type of responses to requests for the signed\n            URL. Ignored if content_type is set on object/blob metadata.\n\n        :type generation: str\n        :param generation:\n            (Optional) A value that indicates which generation of the resource\n            to fetch.\n\n        :type headers: dict\n        :param headers:\n            (Optional) Additional HTTP headers to be included as part of the\n            signed URLs. See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers\n            Requests using the signed URL *must* pass the specified header\n            (name and value) with each request for the URL.\n\n        :type query_parameters: dict\n        :param query_parameters:\n            (Optional) Additional query parameters to be included as part of the\n            signed URLs. See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type credentials: :class:`google.auth.credentials.Credentials`\n        :param credentials:\n            (Optional) The authorization credentials to attach to requests.\n            These credentials identify this application to the service.  If\n            none are specified, the client will attempt to ascertain the\n            credentials from the environment.\n\n        :type version: str\n        :param version:\n            (Optional) The version of signed credential to create.  Must be one\n            of 'v2' | 'v4'.\n\n        :type service_account_email: str\n        :param service_account_email:\n            (Optional) E-mail address of the service account.\n\n        :type access_token: str\n        :param access_token: (Optional) Access token for a service account.\n\n        :type virtual_hosted_style: bool\n        :param virtual_hosted_style:\n            (Optional) If true, then construct the URL relative the bucket's\n            virtual hostname, e.g., '<bucket-name>.storage.googleapis.com'.\n            Incompatible with bucket_bound_hostname.\n\n        :type bucket_bound_hostname: str\n        :param bucket_bound_hostname:\n            (Optional) If passed, then construct the URL relative to the bucket-bound hostname.\n            Value can be a bare or with scheme, e.g., 'example.com' or 'http://example.com'.\n            Incompatible with api_access_endpoint and virtual_hosted_style.\n            See: https://cloud.google.com/storage/docs/request-endpoints#cname\n\n        :type scheme: str\n        :param scheme:\n            (Optional) If ``bucket_bound_hostname`` is passed as a bare\n            hostname, use this value as the scheme.  ``https`` will work only\n            when using a CDN.  Defaults to ``\"http\"``.\n\n        :raises: :exc:`ValueError` when version is invalid or mutually exclusive arguments are used.\n        :raises: :exc:`TypeError` when expiration is not a valid type.\n        :raises: :exc:`AttributeError` if credentials is not an instance\n                of :class:`google.auth.credentials.Signing`.\n\n        :rtype: str\n        :returns: A signed URL you can use to access the resource\n                  until expiration.\n        \"\"\"\n        if version is None:\n            version = \"v2\"\n        elif version not in (\"v2\", \"v4\"):\n            raise ValueError(\"'version' must be either 'v2' or 'v4'\")\n\n        if (\n            api_access_endpoint is not None or virtual_hosted_style\n        ) and bucket_bound_hostname:\n            raise ValueError(\n                \"The bucket_bound_hostname argument is not compatible with \"\n                \"either api_access_endpoint or virtual_hosted_style.\"\n            )\n\n        if api_access_endpoint is None:\n            client = self._require_client(client)\n            api_access_endpoint = client.api_endpoint\n\n        quoted_name = _quote(self.name, safe=b\"/~\")\n\n        # If you are on Google Compute Engine, you can't generate a signed URL\n        # using GCE service account.\n        # See https://github.com/googleapis/google-auth-library-python/issues/50\n        if virtual_hosted_style:\n            api_access_endpoint = _virtual_hosted_style_base_url(\n                api_access_endpoint, self.bucket.name\n            )\n            resource = f\"/{quoted_name}\"\n        elif bucket_bound_hostname:\n            api_access_endpoint = _bucket_bound_hostname_url(\n                bucket_bound_hostname, scheme\n            )\n            resource = f\"/{quoted_name}\"\n        else:\n            resource = f\"/{self.bucket.name}/{quoted_name}\"\n\n        if credentials is None:\n            client = self._require_client(client)  # May be redundant, but that's ok.\n            credentials = client._credentials\n\n        if version == \"v2\":\n            helper = generate_signed_url_v2\n        else:\n            helper = generate_signed_url_v4\n\n        if self._encryption_key is not None:\n            encryption_headers = _get_encryption_headers(self._encryption_key)\n            if headers is None:\n                headers = {}\n            if version == \"v2\":\n                # See: https://cloud.google.com/storage/docs/access-control/signed-urls-v2#about-canonical-extension-headers\n                v2_copy_only = \"X-Goog-Encryption-Algorithm\"\n                headers[v2_copy_only] = encryption_headers[v2_copy_only]\n            else:\n                headers.update(encryption_headers)\n\n        return helper(\n            credentials,\n            resource=resource,\n            expiration=expiration,\n            api_access_endpoint=api_access_endpoint,\n            method=method.upper(),\n            content_md5=content_md5,\n            content_type=content_type,\n            response_type=response_type,\n            response_disposition=response_disposition,\n            generation=generation,\n            headers=headers,\n            query_parameters=query_parameters,\n            service_account_email=service_account_email,\n            access_token=access_token,\n        )\n\n    def exists(\n        self,\n        client=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        soft_deleted=None,\n    ):\n        \"\"\"Determines whether or not this blob exists.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If True, looks for a soft-deleted object. Will only return True\n            if the object exists and is in a soft-deleted state.\n            :attr:`generation` is required to be set on the blob if ``soft_deleted`` is set to True.\n            See: https://cloud.google.com/storage/docs/soft-delete\n\n        :rtype: bool\n        :returns: True if the blob exists in Cloud Storage.\n        \"\"\"\n        client = self._require_client(client)\n        # We only need the status code (200 or not) so we seek to\n        # minimize the returned payload.\n        query_params = self._query_params\n        query_params[\"fields\"] = \"name\"\n        if soft_deleted is not None:\n            query_params[\"softDeleted\"] = soft_deleted\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        headers = {}\n        _add_etag_match_headers(\n            headers, if_etag_match=if_etag_match, if_etag_not_match=if_etag_not_match\n        )\n\n        try:\n            # We intentionally pass `_target_object=None` since fields=name\n            # would limit the local properties.\n            client._get_resource(\n                self.path,\n                query_params=query_params,\n                headers=headers,\n                timeout=timeout,\n                retry=retry,\n                _target_object=None,\n            )\n        except NotFound:\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            return False\n        return True\n\n    def delete(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Deletes a blob from Cloud Storage.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n                 (propagated from\n                 :meth:`google.cloud.storage.bucket.Bucket.delete_blob`).\n        \"\"\"\n        self.bucket.delete_blob(\n            self.name,\n            client=client,\n            generation=self.generation,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def _get_transport(self, client):\n        \"\"\"Return the client's transport.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :rtype transport:\n            :class:`~google.auth.transport.requests.AuthorizedSession`\n        :returns: The transport (with credentials) that will\n                  make authenticated requests.\n        \"\"\"\n        client = self._require_client(client)\n        return client._http\n\n    def _get_download_url(\n        self,\n        client,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n    ):\n        \"\"\"Get the download URL for the current blob.\n\n        If the ``media_link`` has been loaded, it will be used, otherwise\n        the URL will be constructed from the current blob's path (and possibly\n        generation) to avoid a round trip.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: The client to use.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :rtype: str\n        :returns: The download URL for the current blob.\n        \"\"\"\n        name_value_pairs = []\n        if self.media_link is None:\n            hostname = _get_host_name(client._connection)\n            base_url = _DOWNLOAD_URL_TEMPLATE.format(\n                hostname=hostname, path=self.path, api_version=_API_VERSION\n            )\n            if self.generation is not None:\n                name_value_pairs.append((\"generation\", f\"{self.generation:d}\"))\n        else:\n            base_url = self.media_link\n\n        if self.user_project is not None:\n            name_value_pairs.append((\"userProject\", self.user_project))\n\n        _add_generation_match_parameters(\n            name_value_pairs,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        return _add_query_parameters(base_url, name_value_pairs)\n\n    def _extract_headers_from_download(self, response):\n        \"\"\"Extract headers from a non-chunked request's http object.\n\n        This avoids the need to make a second request for commonly used\n        headers.\n\n        :type response:\n            :class requests.models.Response\n        :param response: The server response from downloading a non-chunked file\n        \"\"\"\n        self._properties[\"contentEncoding\"] = response.headers.get(\n            \"Content-Encoding\", None\n        )\n        self._properties[_CONTENT_TYPE_FIELD] = response.headers.get(\n            \"Content-Type\", None\n        )\n        self._properties[\"cacheControl\"] = response.headers.get(\"Cache-Control\", None)\n        self._properties[\"storageClass\"] = response.headers.get(\n            \"X-Goog-Storage-Class\", None\n        )\n        self._properties[\"contentLanguage\"] = response.headers.get(\n            \"Content-Language\", None\n        )\n        self._properties[\"etag\"] = response.headers.get(\"ETag\", None)\n        self._properties[\"generation\"] = response.headers.get(\"X-goog-generation\", None)\n        self._properties[\"metageneration\"] = response.headers.get(\n            \"X-goog-metageneration\", None\n        )\n        #  'X-Goog-Hash': 'crc32c=4gcgLQ==,md5=CS9tHYTtyFntzj7B9nkkJQ==',\n        x_goog_hash = response.headers.get(\"X-Goog-Hash\", \"\")\n\n        if x_goog_hash:\n            digests = {}\n            for encoded_digest in x_goog_hash.split(\",\"):\n                match = re.match(r\"(crc32c|md5)=([\\w\\d/\\+/]+={0,3})\", encoded_digest)\n                if match:\n                    method, digest = match.groups()\n                    digests[method] = digest\n\n            self._properties[\"crc32c\"] = digests.get(\"crc32c\", None)\n            self._properties[\"md5Hash\"] = digests.get(\"md5\", None)\n\n    def _do_download(\n        self,\n        transport,\n        file_obj,\n        download_url,\n        headers,\n        start=None,\n        end=None,\n        raw_download=False,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=None,\n    ):\n        \"\"\"Perform a download without any error handling.\n\n        This is intended to be called by :meth:`_prep_and_do_download` so it can\n        be wrapped with error handling / remapping.\n\n        :type transport:\n            :class:`~google.auth.transport.requests.AuthorizedSession`\n        :param transport:\n            The transport (with credentials) that will make authenticated\n            requests.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type download_url: str\n        :param download_url: The URL where the media can be accessed.\n\n        :type headers: dict\n        :param headers: Headers to be sent with the request(s).\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._prep_and_do_download().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n        \"\"\"\n\n        retry_strategy = _api_core_retry_to_resumable_media_retry(retry)\n\n        if self.chunk_size is None:\n            if raw_download:\n                klass = RawDownload\n            else:\n                klass = Download\n\n            download = klass(\n                download_url,\n                stream=file_obj,\n                headers=headers,\n                start=start,\n                end=end,\n                checksum=checksum,\n            )\n            download._retry_strategy = retry_strategy\n            response = download.consume(transport, timeout=timeout)\n            self._extract_headers_from_download(response)\n        else:\n            if checksum:\n                msg = _CHUNKED_DOWNLOAD_CHECKSUM_MESSAGE.format(checksum)\n                _logger.info(msg)\n\n            if raw_download:\n                klass = RawChunkedDownload\n            else:\n                klass = ChunkedDownload\n\n            download = klass(\n                download_url,\n                self.chunk_size,\n                file_obj,\n                headers=headers,\n                start=start if start else 0,\n                end=end,\n            )\n\n            download._retry_strategy = retry_strategy\n            while not download.finished:\n                download.consume_next_chunk(transport, timeout=timeout)\n\n    def download_to_file(\n        self,\n        file_obj,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob into a file-like object.\n\n        .. note::\n\n           If the server-set property, :attr:`media_link`, is not yet\n           initialized, makes an additional API request to load it.\n\n        If the :attr:`chunk_size` of a current blob is `None`, will download data\n        in single download request otherwise it will download the :attr:`chunk_size`\n        of data in each request.\n\n        For more fine-grained control over the download process, check out\n        [`google-resumable-media`](https://googleapis.dev/python/google-resumable-media/latest/index.html).\n        For example, this library allows downloading **parts** of a blob rather than the whole thing.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n\n        self._prep_and_do_download(\n            file_obj,\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def _handle_filename_and_download(self, filename, *args, **kwargs):\n        \"\"\"Download the contents of this blob into a named file.\n\n        :type filename: str\n        :param filename: A filename to be passed to ``open``.\n\n        For *args and **kwargs, refer to the documentation for download_to_filename() for more information.\n        \"\"\"\n\n        try:\n            with open(filename, \"wb\") as file_obj:\n                self._prep_and_do_download(\n                    file_obj,\n                    *args,\n                    **kwargs,\n                )\n\n        except resumable_media.DataCorruption:\n            # Delete the corrupt downloaded file.\n            os.remove(filename)\n            raise\n\n        updated = self.updated\n        if updated is not None:\n            mtime = updated.timestamp()\n            os.utime(file_obj.name, (mtime, mtime))\n\n    def download_to_filename(\n        self,\n        filename,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob into a named file.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-download-encrypted-file#storage_download_encrypted_file-python)\n        to download a file with a [`customer-supplied encryption key`](https://cloud.google.com/storage/docs/encryption#customer-supplied).\n\n        :type filename: str\n        :param filename: A filename to be passed to ``open``.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n\n        self._handle_filename_and_download(\n            filename,\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def download_as_bytes(\n        self,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob as a bytes object.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :rtype: bytes\n        :returns: The data stored in this blob.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n\n        string_buffer = BytesIO()\n\n        self._prep_and_do_download(\n            string_buffer,\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n        return string_buffer.getvalue()\n\n    def download_as_string(\n        self,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"(Deprecated) Download the contents of this blob as a bytes object.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        .. note::\n           Deprecated alias for :meth:`download_as_bytes`.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :rtype: bytes\n        :returns: The data stored in this blob.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n        warnings.warn(\n            _DOWNLOAD_AS_STRING_DEPRECATED, PendingDeprecationWarning, stacklevel=2\n        )\n        return self.download_as_bytes(\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def download_as_text(\n        self,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        encoding=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob as text (*not* bytes).\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type encoding: str\n        :param encoding: (Optional) encoding to be used to decode the\n            downloaded bytes.  Defaults to the ``charset`` param of\n            attr:`content_type`, or else to \"utf-8\".\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :rtype: text\n        :returns: The data stored in this blob, decoded to text.\n        \"\"\"\n        data = self.download_as_bytes(\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        if encoding is not None:\n            return data.decode(encoding)\n\n        if self.content_type is not None:\n            msg = HeaderParser().parsestr(\"Content-Type: \" + self.content_type)\n            params = dict(msg.get_params()[1:])\n            if \"charset\" in params:\n                return data.decode(params[\"charset\"])\n\n        return data.decode(\"utf-8\")\n\n    def _get_content_type(self, content_type, filename=None):\n        \"\"\"Determine the content type from the current object.\n\n        The return value will be determined in order of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content.\n\n        :type filename: str\n        :param filename:\n            (Optional) The name of the file where the content is stored.\n\n        :rtype: str\n        :returns: Type of content gathered from the object.\n        \"\"\"\n        if content_type is None:\n            content_type = self.content_type\n\n        if content_type is None and filename is not None:\n            content_type, _ = mimetypes.guess_type(filename)\n\n        if content_type is None:\n            content_type = _DEFAULT_CONTENT_TYPE\n\n        return content_type\n\n    def _get_writable_metadata(self):\n        \"\"\"Get the object / blob metadata which is writable.\n\n        This is intended to be used when creating a new object / blob.\n\n        See the [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects)\n        for more information, the fields marked as writable are:\n\n        * ``acl``\n        * ``cacheControl``\n        * ``contentDisposition``\n        * ``contentEncoding``\n        * ``contentLanguage``\n        * ``contentType``\n        * ``crc32c``\n        * ``customTime``\n        * ``md5Hash``\n        * ``metadata``\n        * ``name``\n        * ``retention``\n        * ``storageClass``\n\n        For now, we don't support ``acl``, access control lists should be\n        managed directly through :class:`ObjectACL` methods.\n        \"\"\"\n        # NOTE: This assumes `self.name` is unicode.\n        object_metadata = {\"name\": self.name}\n        for key in self._changes:\n            if key in _WRITABLE_FIELDS:\n                object_metadata[key] = self._properties[key]\n\n        return object_metadata\n\n    def _get_upload_arguments(self, client, content_type, filename=None, command=None):\n        \"\"\"Get required arguments for performing an upload.\n\n        The content type returned will be determined in order of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: tuple\n        :returns: A triple of\n\n                  * A header dictionary\n                  * An object metadata dictionary\n                  * The ``content_type`` as a string (according to precedence)\n        \"\"\"\n        content_type = self._get_content_type(content_type, filename=filename)\n        # Add any client attached custom headers to the upload headers.\n        headers = {\n            **_get_default_headers(\n                client._connection.user_agent, content_type, command=command\n            ),\n            **_get_encryption_headers(self._encryption_key),\n            **client._extra_headers,\n        }\n        object_metadata = self._get_writable_metadata()\n        return headers, object_metadata, content_type\n\n    def _do_multipart_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl,\n        if_generation_match,\n        if_generation_not_match,\n        if_metageneration_match,\n        if_metageneration_not_match,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Perform a multipart upload.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. The request metadata will be amended\n            to include the computed value. Using this option will override a\n            manually-set checksum value. Supported values are \"md5\",\n            \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._do_upload().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: :class:`~requests.Response`\n        :returns: The \"200 OK\" response object returned after the multipart\n                  upload request.\n        :raises: :exc:`ValueError` if ``size`` is not :data:`None` but the\n                 ``stream`` has fewer than ``size`` bytes remaining.\n        \"\"\"\n        if size is None:\n            data = stream.read()\n        else:\n            data = stream.read(size)\n            if len(data) < size:\n                msg = _READ_LESS_THAN_SIZE.format(size, len(data))\n                raise ValueError(msg)\n\n        client = self._require_client(client)\n        transport = self._get_transport(client)\n        if \"metadata\" in self._properties and \"metadata\" not in self._changes:\n            self._changes.add(\"metadata\")\n        info = self._get_upload_arguments(client, content_type, command=command)\n        headers, object_metadata, content_type = info\n\n        hostname = _get_host_name(client._connection)\n        base_url = _MULTIPART_URL_TEMPLATE.format(\n            hostname=hostname, bucket_path=self.bucket.path, api_version=_API_VERSION\n        )\n        name_value_pairs = []\n\n        if self.user_project is not None:\n            name_value_pairs.append((\"userProject\", self.user_project))\n\n        # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n        # at rest, object resource metadata will store the version of the Key Management\n        # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n        # used to upload a new version of the object then the existing kmsKeyName version\n        # value can't be used in the upload request and the client instead ignores it.\n        if (\n            self.kms_key_name is not None\n            and \"cryptoKeyVersions\" not in self.kms_key_name\n        ):\n            name_value_pairs.append((\"kmsKeyName\", self.kms_key_name))\n\n        if predefined_acl is not None:\n            name_value_pairs.append((\"predefinedAcl\", predefined_acl))\n\n        if if_generation_match is not None:\n            name_value_pairs.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            name_value_pairs.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            name_value_pairs.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            name_value_pairs.append(\n                (\"ifMetaGenerationNotMatch\", if_metageneration_not_match)\n            )\n\n        upload_url = _add_query_parameters(base_url, name_value_pairs)\n        upload = MultipartUpload(upload_url, headers=headers, checksum=checksum)\n\n        upload._retry_strategy = _api_core_retry_to_resumable_media_retry(\n            retry, num_retries\n        )\n\n        response = upload.transmit(\n            transport, data, object_metadata, content_type, timeout=timeout\n        )\n\n        return response\n\n    def _initiate_resumable_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl=None,\n        extra_headers=None,\n        chunk_size=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Initiate a resumable upload.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type extra_headers: dict\n        :param extra_headers:\n            (Optional) Extra headers to add to standard headers.\n\n        :type chunk_size: int\n        :param chunk_size:\n            (Optional) Chunk size to use when creating a\n            :class:`~google.resumable_media.requests.ResumableUpload`.\n            If not passed, will fall back to the chunk size on the\n            current blob, if the chunk size of a current blob is also\n            `None`, will set the default value.\n            The default value of ``chunk_size`` is 100 MB.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. After the upload is complete, the\n            server-computed checksum of the resulting object will be checked\n            and google.resumable_media.common.DataCorruption will be raised on\n            a mismatch. On a validation failure, the client will attempt to\n            delete the uploaded object automatically. Supported values\n            are \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._do_upload().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: tuple\n        :returns:\n            Pair of\n\n            * The :class:`~google.resumable_media.requests.ResumableUpload`\n              that was created\n            * The ``transport`` used to initiate the upload.\n        \"\"\"\n        client = self._require_client(client)\n        if chunk_size is None:\n            chunk_size = self.chunk_size\n            if chunk_size is None:\n                chunk_size = _DEFAULT_CHUNKSIZE\n\n        transport = self._get_transport(client)\n        if \"metadata\" in self._properties and \"metadata\" not in self._changes:\n            self._changes.add(\"metadata\")\n        info = self._get_upload_arguments(client, content_type, command=command)\n        headers, object_metadata, content_type = info\n        if extra_headers is not None:\n            headers.update(extra_headers)\n\n        hostname = _get_host_name(client._connection)\n        base_url = _RESUMABLE_URL_TEMPLATE.format(\n            hostname=hostname, bucket_path=self.bucket.path, api_version=_API_VERSION\n        )\n        name_value_pairs = []\n\n        if self.user_project is not None:\n            name_value_pairs.append((\"userProject\", self.user_project))\n\n        # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n        # at rest, object resource metadata will store the version of the Key Management\n        # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n        # used to upload a new version of the object then the existing kmsKeyName version\n        # value can't be used in the upload request and the client instead ignores it.\n        if (\n            self.kms_key_name is not None\n            and \"cryptoKeyVersions\" not in self.kms_key_name\n        ):\n            name_value_pairs.append((\"kmsKeyName\", self.kms_key_name))\n\n        if predefined_acl is not None:\n            name_value_pairs.append((\"predefinedAcl\", predefined_acl))\n\n        if if_generation_match is not None:\n            name_value_pairs.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            name_value_pairs.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            name_value_pairs.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            name_value_pairs.append(\n                (\"ifMetaGenerationNotMatch\", if_metageneration_not_match)\n            )\n\n        upload_url = _add_query_parameters(base_url, name_value_pairs)\n        upload = ResumableUpload(\n            upload_url, chunk_size, headers=headers, checksum=checksum\n        )\n\n        upload._retry_strategy = _api_core_retry_to_resumable_media_retry(\n            retry, num_retries\n        )\n\n        upload.initiate(\n            transport,\n            stream,\n            object_metadata,\n            content_type,\n            total_bytes=size,\n            stream_final=False,\n            timeout=timeout,\n        )\n\n        return upload, transport\n\n    def _do_resumable_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl,\n        if_generation_match,\n        if_generation_not_match,\n        if_metageneration_match,\n        if_metageneration_not_match,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Perform a resumable upload.\n\n        Assumes ``chunk_size`` is not :data:`None` on the current blob.\n        The default value of ``chunk_size`` is 100 MB.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. After the upload is complete, the\n            server-computed checksum of the resulting object will be checked\n            and google.resumable_media.common.DataCorruption will be raised on\n            a mismatch. On a validation failure, the client will attempt to\n            delete the uploaded object automatically. Supported values\n            are \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._do_upload().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: :class:`~requests.Response`\n        :returns: The \"200 OK\" response object returned after the final chunk\n                  is uploaded.\n        \"\"\"\n        upload, transport = self._initiate_resumable_upload(\n            client,\n            stream,\n            content_type,\n            size,\n            num_retries,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n            command=command,\n        )\n        while not upload.finished:\n            try:\n                response = upload.transmit_next_chunk(transport, timeout=timeout)\n            except resumable_media.DataCorruption:\n                # Attempt to delete the corrupted object.\n                self.delete()\n                raise\n        return response\n\n    def _do_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl,\n        if_generation_match,\n        if_generation_not_match,\n        if_metageneration_match,\n        if_metageneration_not_match,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Determine an upload strategy and then perform the upload.\n\n        If the size of the data to be uploaded exceeds 8 MB a resumable media\n        request will be used, otherwise the content and the metadata will be\n        uploaded in a single multipart upload request.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_generation_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: dict\n        :returns: The parsed JSON from the \"200 OK\" response. This will be the\n                  **only** response in the multipart case and it will be the\n                  **final** response in the resumable case.\n        \"\"\"\n\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": if_generation_match,\n                \"ifMetagenerationMatch\": if_metageneration_match,\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        if size is not None and size <= _MAX_MULTIPART_SIZE:\n            response = self._do_multipart_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n                command=command,\n            )\n        else:\n            response = self._do_resumable_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n                command=command,\n            )\n\n        return response.json()\n\n    def _prep_and_do_upload(\n        self,\n        file_obj,\n        rewind=False,\n        size=None,\n        content_type=None,\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        command=None,\n    ):\n        \"\"\"Upload the contents of this blob from a file-like object.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If the size of the data to be uploaded exceeds 8 MB a resumable media\n        request will be used, otherwise the content and the metadata will be\n        uploaded in a single multipart upload request.\n\n        For more fine-grained over the upload process, check out\n        [`google-resumable-media`](https://googleapis.dev/python/google-resumable-media/latest/index.html).\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle opened in binary mode for reading.\n\n        :type rewind: bool\n        :param rewind:\n            If True, seek to the beginning of the file handle before writing\n            the file to Cloud Storage.\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``file_obj``). If not provided, the upload will be concluded once\n            ``file_obj`` is exhausted.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_generation_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :raises: :class:`~google.cloud.exceptions.GoogleCloudError`\n                 if the upload response returns an error status.\n        \"\"\"\n        if num_retries is not None:\n            warnings.warn(_NUM_RETRIES_MESSAGE, DeprecationWarning, stacklevel=2)\n            # num_retries and retry are mutually exclusive. If num_retries is\n            # set and retry is exactly the default, then nullify retry for\n            # backwards compatibility.\n            if retry is DEFAULT_RETRY_IF_GENERATION_SPECIFIED:\n                retry = None\n\n        _maybe_rewind(file_obj, rewind=rewind)\n        predefined_acl = ACL.validate_predefined(predefined_acl)\n\n        try:\n            created_json = self._do_upload(\n                client,\n                file_obj,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n                command=command,\n            )\n            self._set_properties(created_json)\n        except resumable_media.InvalidResponse as exc:\n            _raise_from_invalid_response(exc)\n\n    def upload_from_file(\n        self,\n        file_obj,\n        rewind=False,\n        size=None,\n        content_type=None,\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Upload the contents of this blob from a file-like object.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If the size of the data to be uploaded exceeds 8 MB a resumable media\n        request will be used, otherwise the content and the metadata will be\n        uploaded in a single multipart upload request.\n\n        For more fine-grained over the upload process, check out\n        [`google-resumable-media`](https://googleapis.dev/python/google-resumable-media/latest/index.html).\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle opened in binary mode for reading.\n\n        :type rewind: bool\n        :param rewind:\n            If True, seek to the beginning of the file handle before writing\n            the file to Cloud Storage.\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``file_obj``). If not provided, the upload will be concluded once\n            ``file_obj`` is exhausted.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n\n        :raises: :class:`~google.cloud.exceptions.GoogleCloudError`\n                 if the upload response returns an error status.\n        \"\"\"\n        self._prep_and_do_upload(\n            file_obj,\n            rewind=rewind,\n            size=size,\n            content_type=content_type,\n            num_retries=num_retries,\n            client=client,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def _handle_filename_and_upload(self, filename, content_type=None, *args, **kwargs):\n        \"\"\"Upload this blob's contents from the content of a named file.\n\n        :type filename: str\n        :param filename: The path to the file.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        For *args and **kwargs, refer to the documentation for upload_from_filename() for more information.\n        \"\"\"\n\n        content_type = self._get_content_type(content_type, filename=filename)\n\n        with open(filename, \"rb\") as file_obj:\n            total_bytes = os.fstat(file_obj.fileno()).st_size\n            self._prep_and_do_upload(\n                file_obj,\n                content_type=content_type,\n                size=total_bytes,\n                *args,\n                **kwargs,\n            )\n\n    def upload_from_filename(\n        self,\n        filename,\n        content_type=None,\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Upload this blob's contents from the content of a named file.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The value given by ``mimetypes.guess_type``\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-upload-encrypted-file#storage_upload_encrypted_file-python)\n        to upload a file with a\n        [`customer-supplied encryption key`](https://cloud.google.com/storage/docs/encryption#customer-supplied).\n\n        :type filename: str\n        :param filename: The path to the file.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n        \"\"\"\n\n        self._handle_filename_and_upload(\n            filename,\n            content_type=content_type,\n            num_retries=num_retries,\n            client=client,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def upload_from_string(\n        self,\n        data,\n        content_type=\"text/plain\",\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Upload contents of this blob from the provided string.\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type data: bytes or str\n        :param data:\n            The data to store in this blob.  If the value is text, it will be\n            encoded as UTF-8.\n\n        :type content_type: str\n        :param content_type:\n            (Optional) Type of content being uploaded. Defaults to\n            ``'text/plain'``.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n        \"\"\"\n        data = _to_bytes(data, encoding=\"utf-8\")\n        string_buffer = BytesIO(data)\n        self.upload_from_file(\n            file_obj=string_buffer,\n            size=len(data),\n            content_type=content_type,\n            num_retries=num_retries,\n            client=client,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def create_resumable_upload_session(\n        self,\n        content_type=None,\n        size=None,\n        origin=None,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Create a resumable upload session.\n\n        Resumable upload sessions allow you to start an upload session from\n        one client and complete the session in another. This method is called\n        by the initiator to set the metadata and limits. The initiator then\n        passes the session URL to the client that will upload the binary data.\n        The client performs a PUT request on the session URL to complete the\n        upload. This process allows untrusted clients to upload to an\n        access-controlled bucket.\n\n        For more details, see the\n        documentation on [`signed URLs`](https://cloud.google.com/storage/docs/access-control/signed-urls#signing-resumable).\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If :attr:`encryption_key` is set, the blob will be encrypted with\n        a [`customer-supplied`](https://cloud.google.com/storage/docs/encryption#customer-supplied)\n        encryption key.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type size: int\n        :param size:\n            (Optional) The maximum number of bytes that can be uploaded using\n            this session. If the size is not known when creating the session,\n            this should be left blank.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type origin: str\n        :param origin:\n            (Optional) If set, the upload can only be completed by a user-agent\n            that uploads from the given origin. This can be useful when passing\n            the session to a web client.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. After the upload is complete, the\n            server-computed checksum of the resulting object will be checked\n            and google.resumable_media.common.DataCorruption will be raised on\n            a mismatch. On a validation failure, the client will attempt to\n            delete the uploaded object automatically. Supported values\n            are \"md5\", \"crc32c\" and None. The default is None.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n\n        :rtype: str\n        :returns: The resumable upload session URL. The upload can be\n                  completed by making an HTTP PUT request with the\n                  file's contents.\n\n        :raises: :class:`google.cloud.exceptions.GoogleCloudError`\n                 if the session creation response returns an error status.\n        \"\"\"\n\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": if_generation_match,\n                \"ifMetagenerationMatch\": if_metageneration_match,\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        extra_headers = {}\n        if origin is not None:\n            # This header is specifically for client-side uploads, it\n            # determines the origins allowed for CORS.\n            extra_headers[\"Origin\"] = origin\n\n        try:\n            fake_stream = BytesIO(b\"\")\n            # Send a fake the chunk size which we **know** will be acceptable\n            # to the `ResumableUpload` constructor. The chunk size only\n            # matters when **sending** bytes to an upload.\n            upload, _ = self._initiate_resumable_upload(\n                client,\n                fake_stream,\n                content_type,\n                size,\n                None,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                extra_headers=extra_headers,\n                chunk_size=self._CHUNK_SIZE_MULTIPLE,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n            )\n\n            return upload.resumable_url\n        except resumable_media.InvalidResponse as exc:\n            _raise_from_invalid_response(exc)\n\n    def get_iam_policy(\n        self,\n        client=None,\n        requested_policy_version=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Retrieve the IAM policy for the object.\n\n        .. note::\n\n           Blob- / object-level IAM support does not yet exist and methods\n           currently call an internal ACL backend not providing any utility\n           beyond the blob's :attr:`acl` at this time. The API may be enhanced\n           in the future and is currently undocumented. Use :attr:`acl` for\n           managing object access control.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current object's bucket.\n\n        :type requested_policy_version: int or ``NoneType``\n        :param requested_policy_version:\n            (Optional) The version of IAM policies to request.  If a policy\n            with a condition is requested without setting this, the server will\n            return an error.  This must be set to a value of 3 to retrieve IAM\n            policies containing conditions. This is to prevent client code that\n            isn't aware of IAM conditions from interpreting and modifying\n            policies incorrectly.  The service might return a policy with\n            version lower than the one that was requested, based on the feature\n            syntax in the policy fetched.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``getIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        if requested_policy_version is not None:\n            query_params[\"optionsRequestedPolicyVersion\"] = requested_policy_version\n\n        info = client._get_resource(\n            f\"{self.path}/iam\",\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return Policy.from_api_repr(info)\n\n    def set_iam_policy(\n        self,\n        policy,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n    ):\n        \"\"\"Update the IAM policy for the bucket.\n\n        .. note::\n\n           Blob- / object-level IAM support does not yet exist and methods\n           currently call an internal ACL backend not providing any utility\n           beyond the blob's :attr:`acl` at this time. The API may be enhanced\n           in the future and is currently undocumented. Use :attr:`acl` for\n           managing object access control.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type policy: :class:`google.api_core.iam.Policy`\n        :param policy: policy instance used to update bucket's IAM policy.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``setIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam\"\n        resource = policy.to_api_repr()\n        resource[\"resourceId\"] = self.path\n        info = client._put_resource(\n            path,\n            resource,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return Policy.from_api_repr(info)\n\n    def test_iam_permissions(\n        self, permissions, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"API call:  test permissions\n\n        .. note::\n\n           Blob- / object-level IAM support does not yet exist and methods\n           currently call an internal ACL backend not providing any utility\n           beyond the blob's :attr:`acl` at this time. The API may be enhanced\n           in the future and is currently undocumented. Use :attr:`acl` for\n           managing object access control.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type permissions: list of string\n        :param permissions: the permissions to check\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: list of string\n        :returns: the permissions returned by the ``testIamPermissions`` API\n                  request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {\"permissions\": permissions}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam/testPermissions\"\n        resp = client._get_resource(\n            path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n        return resp.get(\"permissions\", [])\n\n    def make_public(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update blob's ACL, granting read access to anonymous users.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        self.acl.all().grant_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def make_private(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update blob's ACL, revoking read access for anonymous users.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        self.acl.all().revoke_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def compose(\n        self,\n        sources,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_metageneration_match=None,\n        if_source_generation_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Concatenate source blobs into this one.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/objects/compose)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-compose-file#storage_compose_file-python).\n\n        :type sources: list of :class:`Blob`\n        :param sources: Blobs whose contents will be composed into this blob.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) Makes the operation conditional on whether the\n            destination object's current generation matches the given value.\n            Setting to 0 makes the operation succeed only if there are no live\n            versions of the object.\n            Note: In a previous version, this argument worked identically to the\n            ``if_source_generation_match`` argument. For\n            backwards-compatibility reasons, if a list is passed in,\n            this argument will behave like ``if_source_generation_match``\n            and also issue a DeprecationWarning.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) Makes the operation conditional on whether the\n            destination object's current metageneration matches the given\n            value.\n\n            If a list of long is passed in, no match operation will be\n            performed.  (Deprecated: type(list of long) is supported for\n            backwards-compatability reasons only.)\n\n        :type if_source_generation_match: list of long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the current\n            generation of each source blob matches the corresponding generation.\n            The list must match ``sources`` item-to-item.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n        \"\"\"\n        sources_len = len(sources)\n        client = self._require_client(client)\n        query_params = {}\n\n        if isinstance(if_generation_match, list):\n            warnings.warn(\n                _COMPOSE_IF_GENERATION_LIST_DEPRECATED,\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n            if if_source_generation_match is not None:\n                raise ValueError(\n                    _COMPOSE_IF_GENERATION_LIST_AND_IF_SOURCE_GENERATION_ERROR\n                )\n\n            if_source_generation_match = if_generation_match\n            if_generation_match = None\n\n        if isinstance(if_metageneration_match, list):\n            warnings.warn(\n                _COMPOSE_IF_METAGENERATION_LIST_DEPRECATED,\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n            if_metageneration_match = None\n\n        if if_source_generation_match is None:\n            if_source_generation_match = [None] * sources_len\n        if len(if_source_generation_match) != sources_len:\n            raise ValueError(_COMPOSE_IF_SOURCE_GENERATION_MISMATCH_ERROR)\n\n        source_objects = []\n        for source, source_generation in zip(sources, if_source_generation_match):\n            source_object = {\"name\": source.name, \"generation\": source.generation}\n\n            preconditions = {}\n            if source_generation is not None:\n                preconditions[\"ifGenerationMatch\"] = source_generation\n\n            if preconditions:\n                source_object[\"objectPreconditions\"] = preconditions\n\n            source_objects.append(source_object)\n\n        request = {\n            \"sourceObjects\": source_objects,\n            \"destination\": self._properties.copy(),\n        }\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_metageneration_match=if_metageneration_match,\n        )\n\n        api_response = client._post_resource(\n            f\"{self.path}/compose\",\n            request,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n    def rewrite(\n        self,\n        source,\n        token=None,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Rewrite source blob into this one.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        .. note::\n\n           ``rewrite`` is not supported in a ``Batch`` context.\n\n        :type source: :class:`Blob`\n        :param source: blob whose contents will be rewritten into this blob.\n\n        :type token: str\n        :param token:\n            (Optional) Token returned from an earlier, not-completed call to\n            rewrite the same source blob.  If passed, result will include\n            updated status, total bytes written.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: tuple\n        :returns: ``(token, bytes_rewritten, total_bytes)``, where ``token``\n                  is a rewrite token (``None`` if the rewrite is complete),\n                  ``bytes_rewritten`` is the number of bytes rewritten so far,\n                  and ``total_bytes`` is the total number of bytes to be\n                  rewritten.\n        \"\"\"\n        client = self._require_client(client)\n        headers = _get_encryption_headers(self._encryption_key)\n        headers.update(_get_encryption_headers(source._encryption_key, source=True))\n\n        query_params = self._query_params\n        if \"generation\" in query_params:\n            del query_params[\"generation\"]\n\n        if token:\n            query_params[\"rewriteToken\"] = token\n\n        if source.generation:\n            query_params[\"sourceGeneration\"] = source.generation\n\n        # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n        # at rest, object resource metadata will store the version of the Key Management\n        # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n        # used to rewrite the object, then the existing kmsKeyName version\n        # value can't be used in the rewrite request and the client instead ignores it.\n        if (\n            self.kms_key_name is not None\n            and \"cryptoKeyVersions\" not in self.kms_key_name\n        ):\n            query_params[\"destinationKmsKeyName\"] = self.kms_key_name\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n        )\n\n        path = f\"{source.path}/rewriteTo{self.path}\"\n        api_response = client._post_resource(\n            path,\n            self._properties,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        rewritten = int(api_response[\"totalBytesRewritten\"])\n        size = int(api_response[\"objectSize\"])\n\n        # The resource key is set if and only if the API response is\n        # completely done. Additionally, there is no rewrite token to return\n        # in this case.\n        if api_response[\"done\"]:\n            self._set_properties(api_response[\"resource\"])\n            return None, rewritten, size\n\n        return api_response[\"rewriteToken\"], rewritten, size\n\n    def update_storage_class(\n        self,\n        new_class,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Update blob's storage class via a rewrite-in-place. This helper will\n        wait for the rewrite to complete before returning, so it may take some\n        time for large files.\n\n        See\n        https://cloud.google.com/storage/docs/per-object-storage-class\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type new_class: str\n        :param new_class:\n            new storage class for the object.   One of:\n            :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n            or\n            :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n        \"\"\"\n        # Update current blob's storage class prior to rewrite\n        self._patch_property(\"storageClass\", new_class)\n\n        # Execute consecutive rewrite operations until operation is done\n        token, _, _ = self.rewrite(\n            self,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n        while token is not None:\n            token, _, _ = self.rewrite(\n                self,\n                token=token,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                if_source_generation_match=if_source_generation_match,\n                if_source_generation_not_match=if_source_generation_not_match,\n                if_source_metageneration_match=if_source_metageneration_match,\n                if_source_metageneration_not_match=if_source_metageneration_not_match,\n                timeout=timeout,\n                retry=retry,\n            )\n\n    def open(\n        self,\n        mode=\"r\",\n        chunk_size=None,\n        ignore_flush=None,\n        encoding=None,\n        errors=None,\n        newline=None,\n        **kwargs,\n    ):\n        r\"\"\"Create a file handler for file-like I/O to or from this blob.\n\n        This method can be used as a context manager, just like Python's\n        built-in 'open()' function.\n\n        While reading, as with other read methods, if blob.generation is not set\n        the most recent blob generation will be used. Because the file-like IO\n        reader downloads progressively in chunks, this could result in data from\n        multiple versions being mixed together. If this is a concern, use\n        either bucket.get_blob(), or blob.reload(), which will download the\n        latest generation number and set it; or, if the generation is known, set\n        it manually, for instance with bucket.blob(generation=123456).\n\n        Checksumming (hashing) to verify data integrity is disabled for reads\n        using this feature because reads are implemented using request ranges,\n        which do not provide checksums to validate. See\n        https://cloud.google.com/storage/docs/hashes-etags for details.\n\n        See a [code sample](https://github.com/googleapis/python-storage/blob/main/samples/snippets/storage_fileio_write_read.py).\n\n        Keyword arguments to pass to the underlying API calls.\n        For both uploads and downloads, the following arguments are\n        supported:\n\n        - ``if_generation_match``\n        - ``if_generation_not_match``\n        - ``if_metageneration_match``\n        - ``if_metageneration_not_match``\n        - ``timeout``\n        - ``retry``\n\n        For downloads only, the following additional arguments are supported:\n\n        - ``raw_download``\n\n        For uploads only, the following additional arguments are supported:\n\n        - ``content_type``\n        - ``num_retries``\n        - ``predefined_acl``\n        - ``checksum``\n\n        .. note::\n\n           ``num_retries`` is supported for backwards-compatibility\n           reasons only; please use ``retry`` with a Retry object or\n           ConditionalRetryPolicy instead.\n\n        :type mode: str\n        :param mode:\n            (Optional) A mode string, as per standard Python `open()` semantics.The first\n            character must be 'r', to open the blob for reading, or 'w' to open\n            it for writing. The second character, if present, must be 't' for\n            (unicode) text mode, or 'b' for bytes mode. If the second character\n            is omitted, text mode is the default.\n\n        :type chunk_size: long\n        :param chunk_size:\n            (Optional) For reads, the minimum number of bytes to read at a time.\n            If fewer bytes than the chunk_size are requested, the remainder is\n            buffered. For writes, the maximum number of bytes to buffer before\n            sending data to the server, and the size of each request when data\n            is sent. Writes are implemented as a \"resumable upload\", so\n            chunk_size for writes must be exactly a multiple of 256KiB as with\n            other resumable uploads. The default is 40 MiB.\n\n        :type ignore_flush: bool\n        :param ignore_flush:\n            (Optional) For non text-mode writes, makes flush() do nothing\n            instead of raising an error. flush() without closing is not\n            supported by the remote service and therefore calling it normally\n            results in io.UnsupportedOperation. However, that behavior is\n            incompatible with some consumers and wrappers of file objects in\n            Python, such as zipfile.ZipFile or io.TextIOWrapper. Setting\n            ignore_flush will cause flush() to successfully do nothing, for\n            compatibility with those contexts. The correct way to actually flush\n            data to the remote server is to close() (using a context manager,\n            such as in the example, will cause this to happen automatically).\n\n        :type encoding: str\n        :param encoding:\n            (Optional) For text mode only, the name of the encoding that the stream will\n            be decoded or encoded with. If omitted, it defaults to\n            locale.getpreferredencoding(False).\n\n        :type errors: str\n        :param errors:\n            (Optional) For text mode only, an optional string that specifies how encoding\n            and decoding errors are to be handled. Pass 'strict' to raise a\n            ValueError exception if there is an encoding error (the default of\n            None has the same effect), or pass 'ignore' to ignore errors. (Note\n            that ignoring encoding errors can lead to data loss.) Other more\n            rarely-used options are also available; see the Python 'io' module\n            documentation for 'io.TextIOWrapper' for a complete list.\n\n        :type newline: str\n        :param newline:\n            (Optional) For text mode only, controls how line endings are handled. It can\n            be None, '', '\\n', '\\r', and '\\r\\n'. If None, reads use \"universal\n            newline mode\" and writes use the system default. See the Python\n            'io' module documentation for 'io.TextIOWrapper' for details.\n\n        :returns: A 'BlobReader' or 'BlobWriter' from\n            'google.cloud.storage.fileio', or an 'io.TextIOWrapper' around one\n            of those classes, depending on the 'mode' argument.\n        \"\"\"\n        if mode == \"rb\":\n            if encoding or errors or newline:\n                raise ValueError(\n                    \"encoding, errors and newline arguments are for text mode only\"\n                )\n            if ignore_flush:\n                raise ValueError(\n                    \"ignore_flush argument is for non-text write mode only\"\n                )\n            return BlobReader(self, chunk_size=chunk_size, **kwargs)\n        elif mode == \"wb\":\n            if encoding or errors or newline:\n                raise ValueError(\n                    \"encoding, errors and newline arguments are for text mode only\"\n                )\n            return BlobWriter(\n                self, chunk_size=chunk_size, ignore_flush=ignore_flush, **kwargs\n            )\n        elif mode in (\"r\", \"rt\"):\n            if ignore_flush:\n                raise ValueError(\n                    \"ignore_flush argument is for non-text write mode only\"\n                )\n            return TextIOWrapper(\n                BlobReader(self, chunk_size=chunk_size, **kwargs),\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n        elif mode in (\"w\", \"wt\"):\n            if ignore_flush is False:\n                raise ValueError(\n                    \"ignore_flush is required for text mode writing and \"\n                    \"cannot be set to False\"\n                )\n            return TextIOWrapper(\n                BlobWriter(self, chunk_size=chunk_size, ignore_flush=True, **kwargs),\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n        else:\n            raise NotImplementedError(\n                \"Supported modes strings are 'r', 'rb', 'rt', 'w', 'wb', and 'wt' only.\"\n            )\n\n    cache_control = _scalar_property(\"cacheControl\")\n    \"\"\"HTTP 'Cache-Control' header for this object.\n\n    See [`RFC 7234`](https://tools.ietf.org/html/rfc7234#section-5.2)\n    and [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n\n    \"\"\"\n\n    content_disposition = _scalar_property(\"contentDisposition\")\n    \"\"\"HTTP 'Content-Disposition' header for this object.\n\n    See [`RFC 6266`](https://tools.ietf.org/html/rfc7234#section-5.2) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    content_encoding = _scalar_property(\"contentEncoding\")\n    \"\"\"HTTP 'Content-Encoding' header for this object.\n\n    See [`RFC 7231`](https://tools.ietf.org/html/rfc7231#section-3.1.2.2) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    content_language = _scalar_property(\"contentLanguage\")\n    \"\"\"HTTP 'Content-Language' header for this object.\n\n    See [`BCP47`](https://tools.ietf.org/html/bcp47) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    content_type = _scalar_property(_CONTENT_TYPE_FIELD)\n    \"\"\"HTTP 'Content-Type' header for this object.\n\n    See [`RFC 2616`](https://tools.ietf.org/html/rfc2616#section-14.17) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    crc32c = _scalar_property(\"crc32c\")\n    \"\"\"CRC32C checksum for this object.\n\n    This returns the blob's CRC32C checksum. To retrieve the value, first use a\n    reload method of the Blob class which loads the blob's properties from the server.\n\n    See [`RFC 4960`](https://tools.ietf.org/html/rfc4960#appendix-B) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If not set before upload, the server will compute the hash.\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    def _prep_and_do_download(\n        self,\n        file_obj,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n        command=None,\n    ):\n        \"\"\"Download the contents of a blob object into a file-like object.\n\n        See https://cloud.google.com/storage/docs/downloading-objects\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for download was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n        \"\"\"\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": if_generation_match,\n                \"ifMetagenerationMatch\": if_metageneration_match,\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        client = self._require_client(client)\n\n        download_url = self._get_download_url(\n            client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        headers = _get_encryption_headers(self._encryption_key)\n        headers[\"accept-encoding\"] = \"gzip\"\n        _add_etag_match_headers(\n            headers,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n        )\n        # Add any client attached custom headers to be sent with the request.\n        headers = {\n            **_get_default_headers(client._connection.user_agent, command=command),\n            **headers,\n            **client._extra_headers,\n        }\n\n        transport = client._http\n\n        try:\n            self._do_download(\n                transport,\n                file_obj,\n                download_url,\n                headers,\n                start,\n                end,\n                raw_download,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n            )\n        except resumable_media.InvalidResponse as exc:\n            _raise_from_invalid_response(exc)\n\n    @property\n    def component_count(self):\n        \"\"\"Number of underlying components that make up this object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The component count (in case of a composed object) or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server.  This property will not be set on objects\n                  not created via ``compose``.\n        \"\"\"\n        component_count = self._properties.get(\"componentCount\")\n        if component_count is not None:\n            return int(component_count)\n\n    @property\n    def etag(self):\n        \"\"\"Retrieve the ETag for the object.\n\n        See [`RFC 2616 (etags)`](https://tools.ietf.org/html/rfc2616#section-3.11) and\n        [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n        :rtype: str or ``NoneType``\n        :returns: The blob etag or ``None`` if the blob's resource has not\n                  been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"etag\")\n\n    event_based_hold = _scalar_property(\"eventBasedHold\")\n    \"\"\"Is an event-based hold active on the object?\n\n    See [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If the property is not set locally, returns :data:`None`.\n\n    :rtype: bool or ``NoneType``\n    \"\"\"\n\n    @property\n    def generation(self):\n        \"\"\"Retrieve the generation for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The generation of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        generation = self._properties.get(\"generation\")\n        if generation is not None:\n            return int(generation)\n\n    @property\n    def id(self):\n        \"\"\"Retrieve the ID for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        The ID consists of the bucket name, object name, and generation number.\n\n        :rtype: str or ``NoneType``\n        :returns: The ID of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"id\")\n\n    md5_hash = _scalar_property(\"md5Hash\")\n    \"\"\"MD5 hash for this object.\n\n    This returns the blob's MD5 hash. To retrieve the value, first use a\n    reload method of the Blob class which loads the blob's properties from the server.\n\n    See [`RFC 1321`](https://tools.ietf.org/html/rfc1321) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If not set before upload, the server will compute the hash.\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    @property\n    def media_link(self):\n        \"\"\"Retrieve the media download URI for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: str or ``NoneType``\n        :returns: The media link for the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"mediaLink\")\n\n    @property\n    def metadata(self):\n        \"\"\"Retrieve arbitrary/application specific metadata for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :setter: Update arbitrary/application specific metadata for the\n                 object.\n        :getter: Retrieve arbitrary/application specific metadata for\n                 the object.\n\n        :rtype: dict or ``NoneType``\n        :returns: The metadata associated with the blob or ``None`` if the\n                  property is not set.\n        \"\"\"\n        return copy.deepcopy(self._properties.get(\"metadata\"))\n\n    @metadata.setter\n    def metadata(self, value):\n        \"\"\"Update arbitrary/application specific metadata for the object.\n\n        Values are stored to GCS as strings. To delete a key, set its value to\n        None and call blob.patch().\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :type value: dict\n        :param value: The blob metadata to set.\n        \"\"\"\n        if value is not None:\n            value = {k: str(v) if v is not None else None for k, v in value.items()}\n        self._patch_property(\"metadata\", value)\n\n    @property\n    def metageneration(self):\n        \"\"\"Retrieve the metageneration for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The metageneration of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        metageneration = self._properties.get(\"metageneration\")\n        if metageneration is not None:\n            return int(metageneration)\n\n    @property\n    def owner(self):\n        \"\"\"Retrieve info about the owner of the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: dict or ``NoneType``\n        :returns: Mapping of owner's role/ID, or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return copy.deepcopy(self._properties.get(\"owner\"))\n\n    @property\n    def retention_expiration_time(self):\n        \"\"\"Retrieve timestamp at which the object's retention period expires.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the property is not set locally.\n        \"\"\"\n        value = self._properties.get(\"retentionExpirationTime\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def self_link(self):\n        \"\"\"Retrieve the URI for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: str or ``NoneType``\n        :returns: The self link for the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"selfLink\")\n\n    @property\n    def size(self):\n        \"\"\"Size of the object, in bytes.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The size of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        size = self._properties.get(\"size\")\n        if size is not None:\n            return int(size)\n\n    @property\n    def kms_key_name(self):\n        \"\"\"Resource name of Cloud KMS key used to encrypt the blob's contents.\n\n        :rtype: str or ``NoneType``\n        :returns:\n            The resource name or ``None`` if no Cloud KMS key was used,\n            or the blob's resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"kmsKeyName\")\n\n    @kms_key_name.setter\n    def kms_key_name(self, value):\n        \"\"\"Set KMS encryption key for object.\n\n        :type value: str or ``NoneType``\n        :param value: new KMS key name (None to clear any existing key).\n        \"\"\"\n        self._patch_property(\"kmsKeyName\", value)\n\n    storage_class = _scalar_property(\"storageClass\")\n    \"\"\"Retrieve the storage class for the object.\n\n    This can only be set at blob / object **creation** time. If you'd\n    like to change the storage class **after** the blob / object already\n    exists in a bucket, call :meth:`update_storage_class` (which uses\n    :meth:`rewrite`).\n\n    See https://cloud.google.com/storage/docs/storage-classes\n\n    :rtype: str or ``NoneType``\n    :returns:\n        If set, one of\n        :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.DURABLE_REDUCED_AVAILABILITY_STORAGE_CLASS`,\n        else ``None``.\n    \"\"\"\n\n    temporary_hold = _scalar_property(\"temporaryHold\")\n    \"\"\"Is a temporary hold active on the object?\n\n    See [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If the property is not set locally, returns :data:`None`.\n\n    :rtype: bool or ``NoneType``\n    \"\"\"\n\n    @property\n    def time_deleted(self):\n        \"\"\"Retrieve the timestamp at which the object was deleted.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`). If the blob has\n                  not been deleted, this will never be set.\n        \"\"\"\n        value = self._properties.get(\"timeDeleted\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def time_created(self):\n        \"\"\"Retrieve the timestamp at which the object was created.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self._properties.get(\"timeCreated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def updated(self):\n        \"\"\"Retrieve the timestamp at which the object was updated.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self._properties.get(\"updated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def custom_time(self):\n        \"\"\"Retrieve the custom time for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self._properties.get(\"customTime\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @custom_time.setter\n    def custom_time(self, value):\n        \"\"\"Set the custom time for the object.\n\n        Once set on the server side object, this value can't be unset, but may\n        only changed to a custom datetime in the future.\n\n        If :attr:`custom_time` must be unset, either perform a rewrite\n        operation or upload the data again.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :type value: :class:`datetime.datetime`\n        :param value: new value\n        \"\"\"\n        if value is not None:\n            value = _datetime_to_rfc3339(value)\n\n        self._patch_property(\"customTime\", value)\n\n    @property\n    def retention(self):\n        \"\"\"Retrieve the retention configuration for this object.\n\n        :rtype: :class:`Retention`\n        :returns: an instance for managing the object's retention configuration.\n        \"\"\"\n        info = self._properties.get(\"retention\", {})\n        return Retention.from_api_repr(info, self)\n\n    @property\n    def soft_delete_time(self):\n        \"\"\"If this object has been soft-deleted, returns the time at which it became soft-deleted.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns:\n            (readonly) The time that the object became soft-deleted.\n             Note this property is only set for soft-deleted objects.\n        \"\"\"\n        soft_delete_time = self._properties.get(\"softDeleteTime\")\n        if soft_delete_time is not None:\n            return _rfc3339_nanos_to_datetime(soft_delete_time)\n\n    @property\n    def hard_delete_time(self):\n        \"\"\"If this object has been soft-deleted, returns the time at which it will be permanently deleted.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns:\n            (readonly) The time that the object will be permanently deleted.\n            Note this property is only set for soft-deleted objects.\n        \"\"\"\n        hard_delete_time = self._properties.get(\"hardDeleteTime\")\n        if hard_delete_time is not None:\n            return _rfc3339_nanos_to_datetime(hard_delete_time)\n\n\ndef _get_host_name(connection):\n    \"\"\"Returns the host name from the given connection.\n\n    :type connection: :class:`~google.cloud.storage._http.Connection`\n    :param connection: The connection object.\n\n    :rtype: str\n    :returns: The host name.\n    \"\"\"\n    # TODO: After google-cloud-core 1.6.0 is stable and we upgrade it\n    # to 1.6.0 in setup.py, we no longer need to check the attribute\n    # existence. We can simply return connection.get_api_base_url_for_mtls().\n    return (\n        connection.API_BASE_URL\n        if not hasattr(connection, \"get_api_base_url_for_mtls\")\n        else connection.get_api_base_url_for_mtls()\n    )\n\n\ndef _get_encryption_headers(key, source=False):\n    \"\"\"Builds customer encryption key headers\n\n    :type key: bytes\n    :param key: 32 byte key to build request key and hash.\n\n    :type source: bool\n    :param source: If true, return headers for the \"source\" blob; otherwise,\n                   return headers for the \"destination\" blob.\n\n    :rtype: dict\n    :returns: dict of HTTP headers being sent in request.\n    \"\"\"\n    if key is None:\n        return {}\n\n    key = _to_bytes(key)\n    key_hash = hashlib.sha256(key).digest()\n    key_hash = base64.b64encode(key_hash)\n    key = base64.b64encode(key)\n\n    if source:\n        prefix = \"X-Goog-Copy-Source-Encryption-\"\n    else:\n        prefix = \"X-Goog-Encryption-\"\n\n    return {\n        prefix + \"Algorithm\": \"AES256\",\n        prefix + \"Key\": _bytes_to_unicode(key),\n        prefix + \"Key-Sha256\": _bytes_to_unicode(key_hash),\n    }\n\n\ndef _quote(value, safe=b\"~\"):\n    \"\"\"URL-quote a string.\n\n    If the value is unicode, this method first UTF-8 encodes it as bytes and\n    then quotes the bytes. (In Python 3, ``urllib.parse.quote`` does this\n    encoding automatically, but in Python 2, non-ASCII characters cannot be\n    quoted.)\n\n    :type value: str or bytes\n    :param value: The value to be URL-quoted.\n\n    :type safe: bytes\n    :param safe: Bytes *not* to be quoted.  By default, includes only ``b'~'``.\n\n    :rtype: str\n    :returns: The encoded value (bytes in Python 2, unicode in Python 3).\n    \"\"\"\n    value = _to_bytes(value, encoding=\"utf-8\")\n    return quote(value, safe=safe)\n\n\ndef _maybe_rewind(stream, rewind=False):\n    \"\"\"Rewind the stream if desired.\n\n    :type stream: IO[bytes]\n    :param stream: A bytes IO object open for reading.\n\n    :type rewind: bool\n    :param rewind: Indicates if we should seek to the beginning of the stream.\n    \"\"\"\n    if rewind:\n        stream.seek(0, os.SEEK_SET)\n\n\ndef _raise_from_invalid_response(error):\n    \"\"\"Re-wrap and raise an ``InvalidResponse`` exception.\n\n    :type error: :exc:`google.resumable_media.InvalidResponse`\n    :param error: A caught exception from the ``google-resumable-media``\n                  library.\n\n    :raises: :class:`~google.cloud.exceptions.GoogleCloudError` corresponding\n             to the failed status code\n    \"\"\"\n    response = error.response\n\n    # The 'response.text' gives the actual reason of error, where 'error' gives\n    # the message of expected status code.\n    if response.text:\n        error_message = response.text + \": \" + str(error)\n    else:\n        error_message = str(error)\n\n    message = f\"{response.request.method} {response.request.url}: {error_message}\"\n\n    raise exceptions.from_http_status(response.status_code, message, response=response)\n\n\ndef _add_query_parameters(base_url, name_value_pairs):\n    \"\"\"Add one query parameter to a base URL.\n\n    :type base_url: string\n    :param base_url: Base URL (may already contain query parameters)\n\n    :type name_value_pairs: list of (string, string) tuples.\n    :param name_value_pairs: Names and values of the query parameters to add\n\n    :rtype: string\n    :returns: URL with additional query strings appended.\n    \"\"\"\n    if len(name_value_pairs) == 0:\n        return base_url\n\n    scheme, netloc, path, query, frag = urlsplit(base_url)\n    query = parse_qsl(query)\n    query.extend(name_value_pairs)\n    return urlunsplit((scheme, netloc, path, urlencode(query), frag))\n\n\nclass Retention(dict):\n    \"\"\"Map an object's retention configuration.\n\n    :type blob: :class:`Blob`\n    :params blob: blob for which this retention configuration applies to.\n\n    :type mode: str or ``NoneType``\n    :params mode:\n        (Optional) The mode of the retention configuration, which can be either Unlocked or Locked.\n        See: https://cloud.google.com/storage/docs/object-lock\n\n    :type retain_until_time: :class:`datetime.datetime` or ``NoneType``\n    :params retain_until_time:\n        (Optional) The earliest time that the object can be deleted or replaced, which is the\n        retention configuration set for this object.\n\n    :type retention_expiration_time: :class:`datetime.datetime` or ``NoneType``\n    :params retention_expiration_time:\n        (Optional) The earliest time that the object can be deleted, which depends on any\n        retention configuration set for the object and any retention policy set for the bucket\n        that contains the object. This value should normally only be set by the back-end API.\n    \"\"\"\n\n    def __init__(\n        self,\n        blob,\n        mode=None,\n        retain_until_time=None,\n        retention_expiration_time=None,\n    ):\n        data = {\"mode\": mode}\n        if retain_until_time is not None:\n            retain_until_time = _datetime_to_rfc3339(retain_until_time)\n        data[\"retainUntilTime\"] = retain_until_time\n\n        if retention_expiration_time is not None:\n            retention_expiration_time = _datetime_to_rfc3339(retention_expiration_time)\n        data[\"retentionExpirationTime\"] = retention_expiration_time\n\n        super(Retention, self).__init__(data)\n        self._blob = blob\n\n    @classmethod\n    def from_api_repr(cls, resource, blob):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type blob: :class:`Blob`\n        :params blob: Blob for which this retention configuration applies to.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`Retention`\n        :returns: Retention configuration created from resource.\n        \"\"\"\n        instance = cls(blob)\n        instance.update(resource)\n        return instance\n\n    @property\n    def blob(self):\n        \"\"\"Blob for which this retention configuration applies to.\n\n        :rtype: :class:`Blob`\n        :returns: the instance's blob.\n        \"\"\"\n        return self._blob\n\n    @property\n    def mode(self):\n        \"\"\"The mode of the retention configuration. Options are 'Unlocked' or 'Locked'.\n\n        :rtype: string\n        :returns: The mode of the retention configuration, which can be either set to 'Unlocked' or 'Locked'.\n        \"\"\"\n        return self.get(\"mode\")\n\n    @mode.setter\n    def mode(self, value):\n        self[\"mode\"] = value\n        self.blob._patch_property(\"retention\", self)\n\n    @property\n    def retain_until_time(self):\n        \"\"\"The earliest time that the object can be deleted or replaced, which is the\n        retention configuration set for this object.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self.get(\"retainUntilTime\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @retain_until_time.setter\n    def retain_until_time(self, value):\n        \"\"\"Set the retain_until_time for the object retention configuration.\n\n        :type value: :class:`datetime.datetime`\n        :param value: The earliest time that the object can be deleted or replaced.\n        \"\"\"\n        if value is not None:\n            value = _datetime_to_rfc3339(value)\n        self[\"retainUntilTime\"] = value\n        self.blob._patch_property(\"retention\", self)\n\n    @property\n    def retention_expiration_time(self):\n        \"\"\"The earliest time that the object can be deleted, which depends on any\n        retention configuration set for the object and any retention policy set for\n        the bucket that contains the object.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns:\n            (readonly) The earliest time that the object can be deleted.\n        \"\"\"\n        retention_expiration_time = self.get(\"retentionExpirationTime\")\n        if retention_expiration_time is not None:\n            return _rfc3339_nanos_to_datetime(retention_expiration_time)\n", "google/cloud/storage/bucket.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Create / interact with Google Cloud Storage buckets.\"\"\"\n\nimport base64\nimport copy\nimport datetime\nimport json\nfrom urllib.parse import urlsplit\nimport warnings\n\nfrom google.api_core import datetime_helpers\nfrom google.cloud._helpers import _datetime_to_rfc3339\nfrom google.cloud._helpers import _rfc3339_nanos_to_datetime\nfrom google.cloud.exceptions import NotFound\nfrom google.api_core.iam import Policy\nfrom google.cloud.storage import _signing\nfrom google.cloud.storage._helpers import _add_etag_match_headers\nfrom google.cloud.storage._helpers import _add_generation_match_parameters\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _PropertyMixin\nfrom google.cloud.storage._helpers import _UTC\nfrom google.cloud.storage._helpers import _scalar_property\nfrom google.cloud.storage._helpers import _validate_name\nfrom google.cloud.storage._signing import generate_signed_url_v2\nfrom google.cloud.storage._signing import generate_signed_url_v4\nfrom google.cloud.storage._helpers import _bucket_bound_hostname_url\nfrom google.cloud.storage._helpers import _virtual_hosted_style_base_url\nfrom google.cloud.storage.acl import BucketACL\nfrom google.cloud.storage.acl import DefaultObjectACL\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.constants import ARCHIVE_STORAGE_CLASS\nfrom google.cloud.storage.constants import COLDLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import DUAL_REGION_LOCATION_TYPE\nfrom google.cloud.storage.constants import (\n    DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS,\n)\nfrom google.cloud.storage.constants import MULTI_REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import MULTI_REGION_LOCATION_TYPE\nfrom google.cloud.storage.constants import NEARLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_INHERITED\nfrom google.cloud.storage.constants import REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import REGION_LOCATION_TYPE\nfrom google.cloud.storage.constants import STANDARD_STORAGE_CLASS\nfrom google.cloud.storage.notification import BucketNotification\nfrom google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\n\n_UBLA_BPO_ENABLED_MESSAGE = (\n    \"Pass only one of 'uniform_bucket_level_access_enabled' / \"\n    \"'bucket_policy_only_enabled' to 'IAMConfiguration'.\"\n)\n_BPO_ENABLED_MESSAGE = (\n    \"'IAMConfiguration.bucket_policy_only_enabled' is deprecated.  \"\n    \"Instead, use 'IAMConfiguration.uniform_bucket_level_access_enabled'.\"\n)\n_UBLA_BPO_LOCK_TIME_MESSAGE = (\n    \"Pass only one of 'uniform_bucket_level_access_lock_time' / \"\n    \"'bucket_policy_only_lock_time' to 'IAMConfiguration'.\"\n)\n_BPO_LOCK_TIME_MESSAGE = (\n    \"'IAMConfiguration.bucket_policy_only_lock_time' is deprecated.  \"\n    \"Instead, use 'IAMConfiguration.uniform_bucket_level_access_lock_time'.\"\n)\n_LOCATION_SETTER_MESSAGE = (\n    \"Assignment to 'Bucket.location' is deprecated, as it is only \"\n    \"valid before the bucket is created. Instead, pass the location \"\n    \"to `Bucket.create`.\"\n)\n\n\ndef _blobs_page_start(iterator, page, response):\n    \"\"\"Grab prefixes after a :class:`~google.cloud.iterator.Page` started.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that is currently in use.\n\n    :type page: :class:`~google.cloud.api.core.page_iterator.Page`\n    :param page: The page that was just created.\n\n    :type response: dict\n    :param response: The JSON API response for a page of blobs.\n    \"\"\"\n    page.prefixes = tuple(response.get(\"prefixes\", ()))\n    iterator.prefixes.update(page.prefixes)\n\n\ndef _item_to_blob(iterator, item):\n    \"\"\"Convert a JSON blob to the native object.\n\n    .. note::\n\n        This assumes that the ``bucket`` attribute has been\n        added to the iterator after being created.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a blob.\n\n    :rtype: :class:`.Blob`\n    :returns: The next blob in the page.\n    \"\"\"\n    name = item.get(\"name\")\n    blob = Blob(name, bucket=iterator.bucket)\n    blob._set_properties(item)\n    return blob\n\n\ndef _item_to_notification(iterator, item):\n    \"\"\"Convert a JSON blob to the native object.\n\n    .. note::\n\n        This assumes that the ``bucket`` attribute has been\n        added to the iterator after being created.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a blob.\n\n    :rtype: :class:`.BucketNotification`\n    :returns: The next notification being iterated.\n    \"\"\"\n    return BucketNotification.from_api_repr(item, bucket=iterator.bucket)\n\n\nclass LifecycleRuleConditions(dict):\n    \"\"\"Map a single lifecycle rule for a bucket.\n\n    See: https://cloud.google.com/storage/docs/lifecycle\n\n    :type age: int\n    :param age: (Optional) Apply rule action to items whose age, in days,\n                exceeds this value.\n\n    :type created_before: datetime.date\n    :param created_before: (Optional) Apply rule action to items created\n                           before this date.\n\n    :type is_live: bool\n    :param is_live: (Optional) If true, apply rule action to non-versioned\n                    items, or to items with no newer versions. If false, apply\n                    rule action to versioned items with at least one newer\n                    version.\n\n    :type matches_prefix: list(str)\n    :param matches_prefix: (Optional) Apply rule action to items which\n                                  any prefix matches the beginning of the item name.\n\n    :type matches_storage_class: list(str), one or more of\n                                 :attr:`Bucket.STORAGE_CLASSES`.\n    :param matches_storage_class: (Optional) Apply rule action to items\n                                  whose storage class matches this value.\n\n    :type matches_suffix: list(str)\n    :param matches_suffix: (Optional) Apply rule action to items which\n                                  any suffix matches the end of the item name.\n\n    :type number_of_newer_versions: int\n    :param number_of_newer_versions: (Optional) Apply rule action to versioned\n                                     items having N newer versions.\n\n    :type days_since_custom_time: int\n    :param days_since_custom_time: (Optional) Apply rule action to items whose number of days\n                                   elapsed since the custom timestamp. This condition is relevant\n                                   only for versioned objects. The value of the field must be a non\n                                   negative integer. If it's zero, the object version will become\n                                   eligible for lifecycle action as soon as it becomes custom.\n\n    :type custom_time_before: :class:`datetime.date`\n    :param custom_time_before: (Optional)  Date object parsed from RFC3339 valid date, apply rule action\n                               to items whose custom time is before this date. This condition is relevant\n                               only for versioned objects, e.g., 2019-03-16.\n\n    :type days_since_noncurrent_time: int\n    :param days_since_noncurrent_time: (Optional) Apply rule action to items whose number of days\n                                        elapsed since the non current timestamp. This condition\n                                        is relevant only for versioned objects. The value of the field\n                                        must be a non negative integer. If it's zero, the object version\n                                        will become eligible for lifecycle action as soon as it becomes\n                                        non current.\n\n    :type noncurrent_time_before: :class:`datetime.date`\n    :param noncurrent_time_before: (Optional) Date object parsed from RFC3339 valid date, apply\n                                   rule action to items whose non current time is before this date.\n                                   This condition is relevant only for versioned objects, e.g, 2019-03-16.\n\n    :raises ValueError: if no arguments are passed.\n    \"\"\"\n\n    def __init__(\n        self,\n        age=None,\n        created_before=None,\n        is_live=None,\n        matches_storage_class=None,\n        number_of_newer_versions=None,\n        days_since_custom_time=None,\n        custom_time_before=None,\n        days_since_noncurrent_time=None,\n        noncurrent_time_before=None,\n        matches_prefix=None,\n        matches_suffix=None,\n        _factory=False,\n    ):\n        conditions = {}\n\n        if age is not None:\n            conditions[\"age\"] = age\n\n        if created_before is not None:\n            conditions[\"createdBefore\"] = created_before.isoformat()\n\n        if is_live is not None:\n            conditions[\"isLive\"] = is_live\n\n        if matches_storage_class is not None:\n            conditions[\"matchesStorageClass\"] = matches_storage_class\n\n        if number_of_newer_versions is not None:\n            conditions[\"numNewerVersions\"] = number_of_newer_versions\n\n        if days_since_custom_time is not None:\n            conditions[\"daysSinceCustomTime\"] = days_since_custom_time\n\n        if custom_time_before is not None:\n            conditions[\"customTimeBefore\"] = custom_time_before.isoformat()\n\n        if days_since_noncurrent_time is not None:\n            conditions[\"daysSinceNoncurrentTime\"] = days_since_noncurrent_time\n\n        if noncurrent_time_before is not None:\n            conditions[\"noncurrentTimeBefore\"] = noncurrent_time_before.isoformat()\n\n        if matches_prefix is not None:\n            conditions[\"matchesPrefix\"] = matches_prefix\n\n        if matches_suffix is not None:\n            conditions[\"matchesSuffix\"] = matches_suffix\n\n        if not _factory and not conditions:\n            raise ValueError(\"Supply at least one condition\")\n\n        super(LifecycleRuleConditions, self).__init__(conditions)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleConditions`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(_factory=True)\n        instance.update(resource)\n        return instance\n\n    @property\n    def age(self):\n        \"\"\"Conditon's age value.\"\"\"\n        return self.get(\"age\")\n\n    @property\n    def created_before(self):\n        \"\"\"Conditon's created_before value.\"\"\"\n        before = self.get(\"createdBefore\")\n        if before is not None:\n            return datetime_helpers.from_iso8601_date(before)\n\n    @property\n    def is_live(self):\n        \"\"\"Conditon's 'is_live' value.\"\"\"\n        return self.get(\"isLive\")\n\n    @property\n    def matches_prefix(self):\n        \"\"\"Conditon's 'matches_prefix' value.\"\"\"\n        return self.get(\"matchesPrefix\")\n\n    @property\n    def matches_storage_class(self):\n        \"\"\"Conditon's 'matches_storage_class' value.\"\"\"\n        return self.get(\"matchesStorageClass\")\n\n    @property\n    def matches_suffix(self):\n        \"\"\"Conditon's 'matches_suffix' value.\"\"\"\n        return self.get(\"matchesSuffix\")\n\n    @property\n    def number_of_newer_versions(self):\n        \"\"\"Conditon's 'number_of_newer_versions' value.\"\"\"\n        return self.get(\"numNewerVersions\")\n\n    @property\n    def days_since_custom_time(self):\n        \"\"\"Conditon's 'days_since_custom_time' value.\"\"\"\n        return self.get(\"daysSinceCustomTime\")\n\n    @property\n    def custom_time_before(self):\n        \"\"\"Conditon's 'custom_time_before' value.\"\"\"\n        before = self.get(\"customTimeBefore\")\n        if before is not None:\n            return datetime_helpers.from_iso8601_date(before)\n\n    @property\n    def days_since_noncurrent_time(self):\n        \"\"\"Conditon's 'days_since_noncurrent_time' value.\"\"\"\n        return self.get(\"daysSinceNoncurrentTime\")\n\n    @property\n    def noncurrent_time_before(self):\n        \"\"\"Conditon's 'noncurrent_time_before' value.\"\"\"\n        before = self.get(\"noncurrentTimeBefore\")\n        if before is not None:\n            return datetime_helpers.from_iso8601_date(before)\n\n\nclass LifecycleRuleDelete(dict):\n    \"\"\"Map a lifecycle rule deleting matching items.\n\n    :type kw: dict\n    :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n    \"\"\"\n\n    def __init__(self, **kw):\n        conditions = LifecycleRuleConditions(**kw)\n        rule = {\"action\": {\"type\": \"Delete\"}, \"condition\": dict(conditions)}\n        super().__init__(rule)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleDelete`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(_factory=True)\n        instance.update(resource)\n        return instance\n\n\nclass LifecycleRuleSetStorageClass(dict):\n    \"\"\"Map a lifecycle rule updating storage class of matching items.\n\n    :type storage_class: str, one of :attr:`Bucket.STORAGE_CLASSES`.\n    :param storage_class: new storage class to assign to matching items.\n\n    :type kw: dict\n    :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n    \"\"\"\n\n    def __init__(self, storage_class, **kw):\n        conditions = LifecycleRuleConditions(**kw)\n        rule = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": storage_class},\n            \"condition\": dict(conditions),\n        }\n        super().__init__(rule)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleSetStorageClass`\n        :returns: Instance created from resource.\n        \"\"\"\n        action = resource[\"action\"]\n        instance = cls(action[\"storageClass\"], _factory=True)\n        instance.update(resource)\n        return instance\n\n\nclass LifecycleRuleAbortIncompleteMultipartUpload(dict):\n    \"\"\"Map a rule aborting incomplete multipart uploads of matching items.\n\n    The \"age\" lifecycle condition is the only supported condition for this rule.\n\n    :type kw: dict\n    :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n    \"\"\"\n\n    def __init__(self, **kw):\n        conditions = LifecycleRuleConditions(**kw)\n        rule = {\n            \"action\": {\"type\": \"AbortIncompleteMultipartUpload\"},\n            \"condition\": dict(conditions),\n        }\n        super().__init__(rule)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleAbortIncompleteMultipartUpload`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(_factory=True)\n        instance.update(resource)\n        return instance\n\n\n_default = object()\n\n\nclass IAMConfiguration(dict):\n    \"\"\"Map a bucket's IAM configuration.\n\n    :type bucket: :class:`Bucket`\n    :params bucket: Bucket for which this instance is the policy.\n\n    :type public_access_prevention: str\n    :params public_access_prevention:\n        (Optional) Whether the public access prevention policy is 'inherited' (default) or 'enforced'\n        See: https://cloud.google.com/storage/docs/public-access-prevention\n\n    :type uniform_bucket_level_access_enabled: bool\n    :params bucket_policy_only_enabled:\n        (Optional) Whether the IAM-only policy is enabled for the bucket.\n\n    :type uniform_bucket_level_access_locked_time: :class:`datetime.datetime`\n    :params uniform_bucket_level_locked_time:\n        (Optional) When the bucket's IAM-only policy was enabled.\n        This value should normally only be set by the back-end API.\n\n    :type bucket_policy_only_enabled: bool\n    :params bucket_policy_only_enabled:\n        Deprecated alias for :data:`uniform_bucket_level_access_enabled`.\n\n    :type bucket_policy_only_locked_time: :class:`datetime.datetime`\n    :params bucket_policy_only_locked_time:\n        Deprecated alias for :data:`uniform_bucket_level_access_locked_time`.\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket,\n        public_access_prevention=_default,\n        uniform_bucket_level_access_enabled=_default,\n        uniform_bucket_level_access_locked_time=_default,\n        bucket_policy_only_enabled=_default,\n        bucket_policy_only_locked_time=_default,\n    ):\n        if bucket_policy_only_enabled is not _default:\n            if uniform_bucket_level_access_enabled is not _default:\n                raise ValueError(_UBLA_BPO_ENABLED_MESSAGE)\n\n            warnings.warn(_BPO_ENABLED_MESSAGE, DeprecationWarning, stacklevel=2)\n            uniform_bucket_level_access_enabled = bucket_policy_only_enabled\n\n        if bucket_policy_only_locked_time is not _default:\n            if uniform_bucket_level_access_locked_time is not _default:\n                raise ValueError(_UBLA_BPO_LOCK_TIME_MESSAGE)\n\n            warnings.warn(_BPO_LOCK_TIME_MESSAGE, DeprecationWarning, stacklevel=2)\n            uniform_bucket_level_access_locked_time = bucket_policy_only_locked_time\n\n        if uniform_bucket_level_access_enabled is _default:\n            uniform_bucket_level_access_enabled = False\n\n        if public_access_prevention is _default:\n            public_access_prevention = PUBLIC_ACCESS_PREVENTION_INHERITED\n\n        data = {\n            \"uniformBucketLevelAccess\": {\n                \"enabled\": uniform_bucket_level_access_enabled\n            },\n            \"publicAccessPrevention\": public_access_prevention,\n        }\n        if uniform_bucket_level_access_locked_time is not _default:\n            data[\"uniformBucketLevelAccess\"][\"lockedTime\"] = _datetime_to_rfc3339(\n                uniform_bucket_level_access_locked_time\n            )\n        super(IAMConfiguration, self).__init__(data)\n        self._bucket = bucket\n\n    @classmethod\n    def from_api_repr(cls, resource, bucket):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type bucket: :class:`Bucket`\n        :params bucket: Bucket for which this instance is the policy.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`IAMConfiguration`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(bucket)\n        instance.update(resource)\n        return instance\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket for which this instance is the policy.\n\n        :rtype: :class:`Bucket`\n        :returns: the instance's bucket.\n        \"\"\"\n        return self._bucket\n\n    @property\n    def public_access_prevention(self):\n        \"\"\"Setting for public access prevention policy. Options are 'inherited' (default) or 'enforced'.\n\n            See: https://cloud.google.com/storage/docs/public-access-prevention\n\n        :rtype: string\n        :returns: the public access prevention status, either 'enforced' or 'inherited'.\n        \"\"\"\n        return self[\"publicAccessPrevention\"]\n\n    @public_access_prevention.setter\n    def public_access_prevention(self, value):\n        self[\"publicAccessPrevention\"] = value\n        self.bucket._patch_property(\"iamConfiguration\", self)\n\n    @property\n    def uniform_bucket_level_access_enabled(self):\n        \"\"\"If set, access checks only use bucket-level IAM policies or above.\n\n        :rtype: bool\n        :returns: whether the bucket is configured to allow only IAM.\n        \"\"\"\n        ubla = self.get(\"uniformBucketLevelAccess\", {})\n        return ubla.get(\"enabled\", False)\n\n    @uniform_bucket_level_access_enabled.setter\n    def uniform_bucket_level_access_enabled(self, value):\n        ubla = self.setdefault(\"uniformBucketLevelAccess\", {})\n        ubla[\"enabled\"] = bool(value)\n        self.bucket._patch_property(\"iamConfiguration\", self)\n\n    @property\n    def uniform_bucket_level_access_locked_time(self):\n        \"\"\"Deadline for changing :attr:`uniform_bucket_level_access_enabled` from true to false.\n\n        If the bucket's :attr:`uniform_bucket_level_access_enabled` is true, this property\n        is time time after which that setting becomes immutable.\n\n        If the bucket's :attr:`uniform_bucket_level_access_enabled` is false, this property\n        is ``None``.\n\n        :rtype: Union[:class:`datetime.datetime`, None]\n        :returns:  (readonly) Time after which :attr:`uniform_bucket_level_access_enabled` will\n                   be frozen as true.\n        \"\"\"\n        ubla = self.get(\"uniformBucketLevelAccess\", {})\n        stamp = ubla.get(\"lockedTime\")\n        if stamp is not None:\n            stamp = _rfc3339_nanos_to_datetime(stamp)\n        return stamp\n\n    @property\n    def bucket_policy_only_enabled(self):\n        \"\"\"Deprecated alias for :attr:`uniform_bucket_level_access_enabled`.\n\n        :rtype: bool\n        :returns: whether the bucket is configured to allow only IAM.\n        \"\"\"\n        return self.uniform_bucket_level_access_enabled\n\n    @bucket_policy_only_enabled.setter\n    def bucket_policy_only_enabled(self, value):\n        warnings.warn(_BPO_ENABLED_MESSAGE, DeprecationWarning, stacklevel=2)\n        self.uniform_bucket_level_access_enabled = value\n\n    @property\n    def bucket_policy_only_locked_time(self):\n        \"\"\"Deprecated alias for :attr:`uniform_bucket_level_access_locked_time`.\n\n        :rtype: Union[:class:`datetime.datetime`, None]\n        :returns:\n            (readonly) Time after which :attr:`bucket_policy_only_enabled` will\n            be frozen as true.\n        \"\"\"\n        return self.uniform_bucket_level_access_locked_time\n\n\nclass Bucket(_PropertyMixin):\n    \"\"\"A class representing a Bucket on Cloud Storage.\n\n    :type client: :class:`google.cloud.storage.client.Client`\n    :param client: A client which holds credentials and project configuration\n                   for the bucket (which requires a project).\n\n    :type name: str\n    :param name: The name of the bucket. Bucket names must start and end with a\n                 number or letter.\n\n    :type user_project: str\n    :param user_project: (Optional) the project ID to be billed for API\n                         requests made via this instance.\n    \"\"\"\n\n    _MAX_OBJECTS_FOR_ITERATION = 256\n    \"\"\"Maximum number of existing objects allowed in iteration.\n\n    This is used in Bucket.delete() and Bucket.make_public().\n    \"\"\"\n\n    STORAGE_CLASSES = (\n        STANDARD_STORAGE_CLASS,\n        NEARLINE_STORAGE_CLASS,\n        COLDLINE_STORAGE_CLASS,\n        ARCHIVE_STORAGE_CLASS,\n        MULTI_REGIONAL_LEGACY_STORAGE_CLASS,  # legacy\n        REGIONAL_LEGACY_STORAGE_CLASS,  # legacy\n        DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS,  # legacy\n    )\n    \"\"\"Allowed values for :attr:`storage_class`.\n\n    Default value is :attr:`STANDARD_STORAGE_CLASS`.\n\n    See\n    https://cloud.google.com/storage/docs/json_api/v1/buckets#storageClass\n    https://cloud.google.com/storage/docs/storage-classes\n    \"\"\"\n\n    _LOCATION_TYPES = (\n        MULTI_REGION_LOCATION_TYPE,\n        REGION_LOCATION_TYPE,\n        DUAL_REGION_LOCATION_TYPE,\n    )\n    \"\"\"Allowed values for :attr:`location_type`.\"\"\"\n\n    def __init__(self, client, name=None, user_project=None):\n        \"\"\"\n        property :attr:`name`\n            Get the bucket's name.\n        \"\"\"\n        name = _validate_name(name)\n        super(Bucket, self).__init__(name=name)\n        self._client = client\n        self._acl = BucketACL(self)\n        self._default_object_acl = DefaultObjectACL(self)\n        self._label_removals = set()\n        self._user_project = user_project\n\n    def __repr__(self):\n        return f\"<Bucket: {self.name}>\"\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this bucket.\"\"\"\n        return self._client\n\n    def _set_properties(self, value):\n        \"\"\"Set the properties for the current object.\n\n        :type value: dict or :class:`google.cloud.storage.batch._FutureDict`\n        :param value: The properties to be set.\n        \"\"\"\n        self._label_removals.clear()\n        return super(Bucket, self)._set_properties(value)\n\n    @property\n    def rpo(self):\n        \"\"\"Get the RPO (Recovery Point Objective) of this bucket\n\n        See: https://cloud.google.com/storage/docs/managing-turbo-replication\n\n        \"ASYNC_TURBO\" or \"DEFAULT\"\n        :rtype: str\n        \"\"\"\n        return self._properties.get(\"rpo\")\n\n    @rpo.setter\n    def rpo(self, value):\n        \"\"\"\n        Set the RPO (Recovery Point Objective) of this bucket.\n\n        See: https://cloud.google.com/storage/docs/managing-turbo-replication\n\n        :type value: str\n        :param value: \"ASYNC_TURBO\" or \"DEFAULT\"\n        \"\"\"\n        self._patch_property(\"rpo\", value)\n\n    @property\n    def user_project(self):\n        \"\"\"Project ID to be billed for API requests made via this bucket.\n\n        If unset, API requests are billed to the bucket owner.\n\n        A user project is required for all operations on Requester Pays buckets.\n\n        See https://cloud.google.com/storage/docs/requester-pays#requirements for details.\n\n        :rtype: str\n        \"\"\"\n        return self._user_project\n\n    @classmethod\n    def from_string(cls, uri, client=None):\n        \"\"\"Get a constructor for bucket object by URI.\n\n        .. code-block:: python\n\n            from google.cloud import storage\n            from google.cloud.storage.bucket import Bucket\n            client = storage.Client()\n            bucket = Bucket.from_string(\"gs://bucket\", client=client)\n\n        :type uri: str\n        :param uri: The bucket uri pass to get bucket object.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  Application code should\n            *always* pass ``client``.\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket`\n        :returns: The bucket object created.\n        \"\"\"\n        scheme, netloc, path, query, frag = urlsplit(uri)\n\n        if scheme != \"gs\":\n            raise ValueError(\"URI scheme must be gs\")\n\n        return cls(client, name=netloc)\n\n    def blob(\n        self,\n        blob_name,\n        chunk_size=None,\n        encryption_key=None,\n        kms_key_name=None,\n        generation=None,\n    ):\n        \"\"\"Factory constructor for blob object.\n\n        .. note::\n          This will not make an HTTP request; it simply instantiates\n          a blob object owned by this bucket.\n\n        :type blob_name: str\n        :param blob_name: The name of the blob to be instantiated.\n\n        :type chunk_size: int\n        :param chunk_size: The size of a chunk of data whenever iterating\n                           (in bytes). This must be a multiple of 256 KB per\n                           the API specification.\n\n        :type encryption_key: bytes\n        :param encryption_key:\n            (Optional) 32 byte encryption key for customer-supplied encryption.\n\n        :type kms_key_name: str\n        :param kms_key_name:\n            (Optional) Resource name of KMS key used to encrypt blob's content.\n\n        :type generation: long\n        :param generation: (Optional) If present, selects a specific revision of\n                           this object.\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The blob object created.\n        \"\"\"\n        return Blob(\n            name=blob_name,\n            bucket=self,\n            chunk_size=chunk_size,\n            encryption_key=encryption_key,\n            kms_key_name=kms_key_name,\n            generation=generation,\n        )\n\n    def notification(\n        self,\n        topic_name=None,\n        topic_project=None,\n        custom_attributes=None,\n        event_types=None,\n        blob_name_prefix=None,\n        payload_format=NONE_PAYLOAD_FORMAT,\n        notification_id=None,\n    ):\n        \"\"\"Factory:  create a notification resource for the bucket.\n\n        See: :class:`.BucketNotification` for parameters.\n\n        :rtype: :class:`.BucketNotification`\n        \"\"\"\n        return BucketNotification(\n            self,\n            topic_name=topic_name,\n            topic_project=topic_project,\n            custom_attributes=custom_attributes,\n            event_types=event_types,\n            blob_name_prefix=blob_name_prefix,\n            payload_format=payload_format,\n            notification_id=notification_id,\n        )\n\n    def exists(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Determines whether or not this bucket exists.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match: (Optional) Make the operation conditional on whether the\n                              bucket's current ETag matches the given value.\n\n        :type if_etag_not_match: Union[str, Set[str]])\n        :param if_etag_not_match: (Optional) Make the operation conditional on whether the\n                                  bucket's current ETag does not match the given value.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        bucket's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            bucket's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: bool\n        :returns: True if the bucket exists in Cloud Storage.\n        \"\"\"\n        client = self._require_client(client)\n        # We only need the status code (200 or not) so we seek to\n        # minimize the returned payload.\n        query_params = {\"fields\": \"name\"}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        headers = {}\n        _add_etag_match_headers(\n            headers, if_etag_match=if_etag_match, if_etag_not_match=if_etag_not_match\n        )\n\n        try:\n            # We intentionally pass `_target_object=None` since fields=name\n            # would limit the local properties.\n            client._get_resource(\n                self.path,\n                query_params=query_params,\n                headers=headers,\n                timeout=timeout,\n                retry=retry,\n                _target_object=None,\n            )\n        except NotFound:\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            return False\n        return True\n\n    def create(\n        self,\n        client=None,\n        project=None,\n        location=None,\n        predefined_acl=None,\n        predefined_default_object_acl=None,\n        enable_object_retention=False,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Creates current bucket.\n\n        If the bucket already exists, will raise\n        :class:`google.cloud.exceptions.Conflict`.\n\n        This implements \"storage.buckets.insert\".\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type project: str\n        :param project: (Optional) The project under which the bucket is to\n                        be created. If not passed, uses the project set on\n                        the client.\n        :raises ValueError: if ``project`` is None and client's\n                            :attr:`project` is also None.\n\n        :type location: str\n        :param location: (Optional) The location of the bucket. If not passed,\n                         the default location, US, will be used. See\n                         https://cloud.google.com/storage/docs/bucket-locations\n\n        :type predefined_acl: str\n        :param predefined_acl:\n            (Optional) Name of predefined ACL to apply to bucket. See:\n            https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n\n        :type predefined_default_object_acl: str\n        :param predefined_default_object_acl:\n            (Optional) Name of predefined ACL to apply to bucket's objects. See:\n            https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n\n        :type enable_object_retention: bool\n        :param enable_object_retention:\n            (Optional) Whether object retention should be enabled on this bucket. See:\n            https://cloud.google.com/storage/docs/object-lock\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n\n        client = self._require_client(client)\n        client.create_bucket(\n            bucket_or_name=self,\n            project=project,\n            user_project=self.user_project,\n            location=location,\n            predefined_acl=predefined_acl,\n            predefined_default_object_acl=predefined_default_object_acl,\n            enable_object_retention=enable_object_retention,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def update(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Sends all properties in a PUT request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        super(Bucket, self).update(\n            client=client,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def reload(\n        self,\n        client=None,\n        projection=\"noAcl\",\n        timeout=_DEFAULT_TIMEOUT,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Reload properties from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type projection: str\n        :param projection: (Optional) If used, must be 'full' or 'noAcl'.\n                           Defaults to ``'noAcl'``. Specifies the set of\n                           properties to return.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match: (Optional) Make the operation conditional on whether the\n                              bucket's current ETag matches the given value.\n\n        :type if_etag_not_match: Union[str, Set[str]])\n        :param if_etag_not_match: (Optional) Make the operation conditional on whether the\n                                  bucket's current ETag does not match the given value.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        bucket's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            bucket's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        super(Bucket, self).reload(\n            client=client,\n            projection=projection,\n            timeout=timeout,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def patch(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Sends all changed properties in a PATCH request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        # Special case: For buckets, it is possible that labels are being\n        # removed; this requires special handling.\n        if self._label_removals:\n            self._changes.add(\"labels\")\n            self._properties.setdefault(\"labels\", {})\n            for removed_label in self._label_removals:\n                self._properties[\"labels\"][removed_label] = None\n\n        # Call the superclass method.\n        super(Bucket, self).patch(\n            client=client,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    @property\n    def acl(self):\n        \"\"\"Create our ACL on demand.\"\"\"\n        return self._acl\n\n    @property\n    def default_object_acl(self):\n        \"\"\"Create our defaultObjectACL on demand.\"\"\"\n        return self._default_object_acl\n\n    @staticmethod\n    def path_helper(bucket_name):\n        \"\"\"Relative URL path for a bucket.\n\n        :type bucket_name: str\n        :param bucket_name: The bucket name in the path.\n\n        :rtype: str\n        :returns: The relative URL path for ``bucket_name``.\n        \"\"\"\n        return \"/b/\" + bucket_name\n\n    @property\n    def path(self):\n        \"\"\"The URL path to this bucket.\"\"\"\n        if not self.name:\n            raise ValueError(\"Cannot determine path without bucket name.\")\n\n        return self.path_helper(self.name)\n\n    def get_blob(\n        self,\n        blob_name,\n        client=None,\n        encryption_key=None,\n        generation=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        soft_deleted=None,\n        **kwargs,\n    ):\n        \"\"\"Get a blob object by name.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-get-metadata#storage_get_metadata-python)\n        on how to retrieve metadata of an object.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blob_name: str\n        :param blob_name: The name of the blob to retrieve.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type encryption_key: bytes\n        :param encryption_key:\n            (Optional) 32 byte encryption key for customer-supplied encryption.\n            See\n            https://cloud.google.com/storage/docs/encryption#customer-supplied.\n\n        :type generation: long\n        :param generation:\n            (Optional) If present, selects a specific revision of this object.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If True, looks for a soft-deleted object. Will only return\n            the object metadata if the object exists and is in a soft-deleted state.\n            Object ``generation`` is required if ``soft_deleted`` is set to True.\n            See: https://cloud.google.com/storage/docs/soft-delete\n\n        :param kwargs: Keyword arguments to pass to the\n                       :class:`~google.cloud.storage.blob.Blob` constructor.\n\n        :rtype: :class:`google.cloud.storage.blob.Blob` or None\n        :returns: The blob object if it exists, otherwise None.\n        \"\"\"\n        blob = Blob(\n            bucket=self,\n            name=blob_name,\n            encryption_key=encryption_key,\n            generation=generation,\n            **kwargs,\n        )\n        try:\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            blob.reload(\n                client=client,\n                timeout=timeout,\n                if_etag_match=if_etag_match,\n                if_etag_not_match=if_etag_not_match,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n                soft_deleted=soft_deleted,\n            )\n        except NotFound:\n            return None\n        else:\n            return blob\n\n    def list_blobs(\n        self,\n        max_results=None,\n        page_token=None,\n        prefix=None,\n        delimiter=None,\n        start_offset=None,\n        end_offset=None,\n        include_trailing_delimiter=None,\n        versions=None,\n        projection=\"noAcl\",\n        fields=None,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        match_glob=None,\n        include_folders_as_prefixes=None,\n        soft_deleted=None,\n        page_size=None,\n    ):\n        \"\"\"Return an iterator used to find blobs in the bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type max_results: int\n        :param max_results:\n            (Optional) The maximum number of blobs to return.\n\n        :type page_token: str\n        :param page_token:\n            (Optional) If present, return the next batch of blobs, using the\n            value, which must correspond to the ``nextPageToken`` value\n            returned in the previous response.  Deprecated: use the ``pages``\n            property of the returned iterator instead of manually passing the\n            token.\n\n        :type prefix: str\n        :param prefix: (Optional) Prefix used to filter blobs.\n\n        :type delimiter: str\n        :param delimiter: (Optional) Delimiter, used with ``prefix`` to\n                          emulate hierarchy.\n\n        :type start_offset: str\n        :param start_offset:\n            (Optional) Filter results to objects whose names are\n            lexicographically equal to or after ``startOffset``. If\n            ``endOffset`` is also set, the objects listed will have names\n            between ``startOffset`` (inclusive) and ``endOffset`` (exclusive).\n\n        :type end_offset: str\n        :param end_offset:\n            (Optional) Filter results to objects whose names are\n            lexicographically before ``endOffset``. If ``startOffset`` is also\n            set, the objects listed will have names between ``startOffset``\n            (inclusive) and ``endOffset`` (exclusive).\n\n        :type include_trailing_delimiter: boolean\n        :param include_trailing_delimiter:\n            (Optional) If true, objects that end in exactly one instance of\n            ``delimiter`` will have their metadata included in ``items`` in\n            addition to ``prefixes``.\n\n        :type versions: bool\n        :param versions: (Optional) Whether object versions should be returned\n                         as separate blobs.\n\n        :type projection: str\n        :param projection: (Optional) If used, must be 'full' or 'noAcl'.\n                           Defaults to ``'noAcl'``. Specifies the set of\n                           properties to return.\n\n        :type fields: str\n        :param fields:\n            (Optional) Selector specifying which fields to include\n            in a partial response. Must be a list of fields. For\n            example to get a partial response with just the next\n            page token and the name and language of each blob returned:\n            ``'items(name,contentLanguage),nextPageToken'``.\n            See: https://cloud.google.com/storage/docs/json_api/v1/parameters#fields\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type match_glob: str\n        :param match_glob:\n            (Optional) A glob pattern used to filter results (for example, foo*bar).\n            The string value must be UTF-8 encoded. See:\n            https://cloud.google.com/storage/docs/json_api/v1/objects/list#list-object-glob\n\n        :type include_folders_as_prefixes: bool\n            (Optional) If true, includes Folders and Managed Folders in the set of\n            ``prefixes`` returned by the query. Only applicable if ``delimiter`` is set to /.\n            See: https://cloud.google.com/storage/docs/managed-folders\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If true, only soft-deleted objects will be listed as distinct results in order of increasing\n            generation number. This parameter can only be used successfully if the bucket has a soft delete policy.\n            Note ``soft_deleted`` and ``versions`` cannot be set to True simultaneously. See:\n            https://cloud.google.com/storage/docs/soft-delete\n\n        :type page_size: int\n        :param page_size:\n            (Optional) Maximum number of blobs to return in each page.\n            Defaults to a value set by the API.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of all :class:`~google.cloud.storage.blob.Blob`\n                  in this bucket matching the arguments.\n        \"\"\"\n        client = self._require_client(client)\n        return client.list_blobs(\n            self,\n            max_results=max_results,\n            page_token=page_token,\n            prefix=prefix,\n            delimiter=delimiter,\n            start_offset=start_offset,\n            end_offset=end_offset,\n            include_trailing_delimiter=include_trailing_delimiter,\n            versions=versions,\n            projection=projection,\n            fields=fields,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n            match_glob=match_glob,\n            include_folders_as_prefixes=include_folders_as_prefixes,\n            soft_deleted=soft_deleted,\n        )\n\n    def list_notifications(\n        self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"List Pub / Sub notifications for this bucket.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/list\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: list of :class:`.BucketNotification`\n        :returns: notification instances\n        \"\"\"\n        client = self._require_client(client)\n        path = self.path + \"/notificationConfigs\"\n        iterator = client._list_resource(\n            path,\n            _item_to_notification,\n            timeout=timeout,\n            retry=retry,\n        )\n        iterator.bucket = self\n        return iterator\n\n    def get_notification(\n        self,\n        notification_id,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Get Pub / Sub notification for this bucket.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/notifications/get)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-print-pubsub-bucket-notification#storage_print_pubsub_bucket_notification-python).\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type notification_id: str\n        :param notification_id: The notification id to retrieve the notification configuration.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`.BucketNotification`\n        :returns: notification instance.\n        \"\"\"\n        notification = self.notification(notification_id=notification_id)\n        notification.reload(client=client, timeout=timeout, retry=retry)\n        return notification\n\n    def delete(\n        self,\n        force=False,\n        client=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Delete this bucket.\n\n        The bucket **must** be empty in order to submit a delete request. If\n        ``force=True`` is passed, this will first attempt to delete all the\n        objects / blobs in the bucket (i.e. try to empty the bucket).\n\n        If the bucket doesn't exist, this will raise\n        :class:`google.cloud.exceptions.NotFound`. If the bucket is not empty\n        (and ``force=False``), will raise :class:`google.cloud.exceptions.Conflict`.\n\n        If ``force=True`` and the bucket contains more than 256 objects / blobs\n        this will cowardly refuse to delete the objects (or the bucket). This\n        is to prevent accidental bucket deletion and to prevent extremely long\n        runtime of this method. Also note that ``force=True`` is not supported\n        in a ``Batch`` context.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type force: bool\n        :param force: If True, empties the bucket's objects then deletes it.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises: :class:`ValueError` if ``force`` is ``True`` and the bucket\n                 contains more than 256 objects / blobs.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        if force:\n            blobs = list(\n                self.list_blobs(\n                    max_results=self._MAX_OBJECTS_FOR_ITERATION + 1,\n                    client=client,\n                    timeout=timeout,\n                    retry=retry,\n                    versions=True,\n                )\n            )\n            if len(blobs) > self._MAX_OBJECTS_FOR_ITERATION:\n                message = (\n                    \"Refusing to delete bucket with more than \"\n                    \"%d objects. If you actually want to delete \"\n                    \"this bucket, please delete the objects \"\n                    \"yourself before calling Bucket.delete().\"\n                ) % (self._MAX_OBJECTS_FOR_ITERATION,)\n                raise ValueError(message)\n\n            # Ignore 404 errors on delete.\n            self.delete_blobs(\n                blobs,\n                on_error=lambda blob: None,\n                client=client,\n                timeout=timeout,\n                retry=retry,\n                preserve_generation=True,\n            )\n\n        # We intentionally pass `_target_object=None` since a DELETE\n        # request has no response value (whether in a standard request or\n        # in a batch request).\n        client._delete_resource(\n            self.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def delete_blob(\n        self,\n        blob_name,\n        client=None,\n        generation=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Deletes a blob from the current bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blob_name: str\n        :param blob_name: A blob name to delete.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type generation: long\n        :param generation: (Optional) If present, permanently deletes a specific\n                           revision of this object.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :raises: :class:`google.cloud.exceptions.NotFound` Raises a NotFound\n                 if the blob isn't found. To suppress\n                 the exception, use :meth:`delete_blobs` by passing a no-op\n                 ``on_error`` callback.\n        \"\"\"\n        client = self._require_client(client)\n        blob = Blob(blob_name, bucket=self, generation=generation)\n\n        query_params = copy.deepcopy(blob._query_params)\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        # We intentionally pass `_target_object=None` since a DELETE\n        # request has no response value (whether in a standard request or\n        # in a batch request).\n        client._delete_resource(\n            blob.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def delete_blobs(\n        self,\n        blobs,\n        on_error=None,\n        client=None,\n        preserve_generation=False,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Deletes a list of blobs from the current bucket.\n\n        Uses :meth:`delete_blob` to delete each individual blob.\n\n        By default, any generation information in the list of blobs is ignored, and the\n        live versions of all blobs are deleted. Set `preserve_generation` to True\n        if blob generation should instead be propagated from the list of blobs.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blobs: list\n        :param blobs: A list of :class:`~google.cloud.storage.blob.Blob`-s or\n                      blob names to delete.\n\n        :type on_error: callable\n        :param on_error: (Optional) Takes single argument: ``blob``.\n                         Called once for each blob raising\n                         :class:`~google.cloud.exceptions.NotFound`;\n                         otherwise, the exception is propagated.\n                         Note that ``on_error`` is not supported in a ``Batch`` context.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type preserve_generation: bool\n        :param preserve_generation: (Optional) Deletes only the generation specified on the blob object,\n                                    instead of the live version, if set to True. Only :class:~google.cloud.storage.blob.Blob\n                                    objects can have their generation set in this way.\n                                    Default: False.\n\n        :type if_generation_match: list of long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the length of the list must match the length of\n            The list must match ``blobs`` item-to-item.\n\n        :type if_generation_not_match: list of long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            The list must match ``blobs`` item-to-item.\n\n        :type if_metageneration_match: list of long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            The list must match ``blobs`` item-to-item.\n\n        :type if_metageneration_not_match: list of long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            The list must match ``blobs`` item-to-item.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :raises: :class:`~google.cloud.exceptions.NotFound` (if\n                 `on_error` is not passed).\n        \"\"\"\n        _raise_if_len_differs(\n            len(blobs),\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        if_generation_match = iter(if_generation_match or [])\n        if_generation_not_match = iter(if_generation_not_match or [])\n        if_metageneration_match = iter(if_metageneration_match or [])\n        if_metageneration_not_match = iter(if_metageneration_not_match or [])\n\n        for blob in blobs:\n            try:\n                blob_name = blob\n                generation = None\n                if not isinstance(blob_name, str):\n                    blob_name = blob.name\n                    generation = blob.generation if preserve_generation else None\n\n                self.delete_blob(\n                    blob_name,\n                    client=client,\n                    generation=generation,\n                    if_generation_match=next(if_generation_match, None),\n                    if_generation_not_match=next(if_generation_not_match, None),\n                    if_metageneration_match=next(if_metageneration_match, None),\n                    if_metageneration_not_match=next(if_metageneration_not_match, None),\n                    timeout=timeout,\n                    retry=retry,\n                )\n            except NotFound:\n                if on_error is not None:\n                    on_error(blob)\n                else:\n                    raise\n\n    def copy_blob(\n        self,\n        blob,\n        destination_bucket,\n        new_name=None,\n        client=None,\n        preserve_acl=True,\n        source_generation=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Copy the given blob to the given bucket, optionally with a new name.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/objects/copy)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-copy-file#storage_copy_file-python).\n\n        :type blob: :class:`google.cloud.storage.blob.Blob`\n        :param blob: The blob to be copied.\n\n        :type destination_bucket: :class:`google.cloud.storage.bucket.Bucket`\n        :param destination_bucket: The bucket into which the blob should be\n                                   copied.\n\n        :type new_name: str\n        :param new_name: (Optional) The new name for the copied file.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type preserve_acl: bool\n        :param preserve_acl: DEPRECATED. This argument is not functional!\n                             (Optional) Copies ACL from old blob to new blob.\n                             Default: True.\n                             Note that ``preserve_acl`` is not supported in a\n                             ``Batch`` context.\n\n        :type source_generation: long\n        :param source_generation: (Optional) The generation of the blob to be\n                                  copied.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The new Blob.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        if source_generation is not None:\n            query_params[\"sourceGeneration\"] = source_generation\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n        )\n\n        if new_name is None:\n            new_name = blob.name\n\n        new_blob = Blob(bucket=destination_bucket, name=new_name)\n        api_path = blob.path + \"/copyTo\" + new_blob.path\n        copy_result = client._post_resource(\n            api_path,\n            None,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=new_blob,\n        )\n\n        if not preserve_acl:\n            new_blob.acl.save(acl={}, client=client, timeout=timeout)\n\n        new_blob._set_properties(copy_result)\n        return new_blob\n\n    def rename_blob(\n        self,\n        blob,\n        new_name,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Rename the given blob using copy and delete operations.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        Effectively, copies blob to the same bucket with a new name, then\n        deletes the blob.\n\n        .. warning::\n\n          This method will first duplicate the data and then delete the\n          old blob.  This means that with very large objects renaming\n          could be a very (temporarily) costly or a very slow operation.\n          If you need more control over the copy and deletion, instead\n          use ``google.cloud.storage.blob.Blob.copy_to`` and\n          ``google.cloud.storage.blob.Blob.delete`` directly.\n\n          Also note that this method is not fully supported in a\n          ``Batch`` context.\n\n        :type blob: :class:`google.cloud.storage.blob.Blob`\n        :param blob: The blob to be renamed.\n\n        :type new_name: str\n        :param new_name: The new name for this blob.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value. Also used in the\n            (implied) delete request.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value. Also used in\n            the (implied) delete request.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value. Also used\n            in the (implied) delete request.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n            Also used in the (implied) delete request.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: :class:`Blob`\n        :returns: The newly-renamed blob.\n        \"\"\"\n        same_name = blob.name == new_name\n\n        new_blob = self.copy_blob(\n            blob,\n            self,\n            new_name,\n            client=client,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n            retry=retry,\n        )\n\n        if not same_name:\n            blob.delete(\n                client=client,\n                timeout=timeout,\n                if_generation_match=if_source_generation_match,\n                if_generation_not_match=if_source_generation_not_match,\n                if_metageneration_match=if_source_metageneration_match,\n                if_metageneration_not_match=if_source_metageneration_not_match,\n                retry=retry,\n            )\n        return new_blob\n\n    def restore_blob(\n        self,\n        blob_name,\n        client=None,\n        generation=None,\n        copy_source_acl=None,\n        projection=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Restores a soft-deleted object.\n\n        If :attr:`user_project` is set on the bucket, bills the API request to that project.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/objects/restore)\n\n        :type blob_name: str\n        :param blob_name: The name of the blob to be restored.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type generation: long\n        :param generation: (Optional) If present, selects a specific revision of this object.\n\n        :type copy_source_acl: bool\n        :param copy_source_acl: (Optional) If true, copy the soft-deleted object's access controls.\n\n        :type projection: str\n        :param projection: (Optional) Specifies the set of properties to return.\n                           If used, must be 'full' or 'noAcl'.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, which\n            only restore operations with ``if_generation_match`` or ``generation`` set\n            will be retried.\n\n            Users can configure non-default retry behavior. A ``None`` value will\n            disable retries. A ``DEFAULT_RETRY`` value will enable retries\n            even if restore operations are not guaranteed to be idempotent.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The restored Blob.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n        if generation is not None:\n            query_params[\"generation\"] = generation\n        if copy_source_acl is not None:\n            query_params[\"copySourceAcl\"] = copy_source_acl\n        if projection is not None:\n            query_params[\"projection\"] = projection\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        blob = Blob(bucket=self, name=blob_name)\n        api_response = client._post_resource(\n            f\"{blob.path}/restore\",\n            None,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        blob._set_properties(api_response)\n        return blob\n\n    @property\n    def cors(self):\n        \"\"\"Retrieve or set CORS policies configured for this bucket.\n\n        See http://www.w3.org/TR/cors/ and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        .. note::\n\n           The getter for this property returns a list which contains\n           *copies* of the bucket's CORS policy mappings.  Mutating the list\n           or one of its dicts has no effect unless you then re-assign the\n           dict via the setter.  E.g.:\n\n           >>> policies = bucket.cors\n           >>> policies.append({'origin': '/foo', ...})\n           >>> policies[1]['maxAgeSeconds'] = 3600\n           >>> del policies[0]\n           >>> bucket.cors = policies\n           >>> bucket.update()\n\n        :setter: Set CORS policies for this bucket.\n        :getter: Gets the CORS policies for this bucket.\n\n        :rtype: list of dictionaries\n        :returns: A sequence of mappings describing each CORS policy.\n        \"\"\"\n        return [copy.deepcopy(policy) for policy in self._properties.get(\"cors\", ())]\n\n    @cors.setter\n    def cors(self, entries):\n        \"\"\"Set CORS policies configured for this bucket.\n\n        See http://www.w3.org/TR/cors/ and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :type entries: list of dictionaries\n        :param entries: A sequence of mappings describing each CORS policy.\n        \"\"\"\n        self._patch_property(\"cors\", entries)\n\n    default_event_based_hold = _scalar_property(\"defaultEventBasedHold\")\n    \"\"\"Are uploaded objects automatically placed under an even-based hold?\n\n    If True, uploaded objects will be placed under an event-based hold to\n    be released at a future time. When released an object will then begin\n    the retention period determined by the policy retention period for the\n    object bucket.\n\n    See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n    If the property is not set locally, returns ``None``.\n\n    :rtype: bool or ``NoneType``\n    \"\"\"\n\n    @property\n    def default_kms_key_name(self):\n        \"\"\"Retrieve / set default KMS encryption key for objects in the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :setter: Set default KMS encryption key for items in this bucket.\n        :getter: Get default KMS encryption key for items in this bucket.\n\n        :rtype: str\n        :returns: Default KMS encryption key, or ``None`` if not set.\n        \"\"\"\n        encryption_config = self._properties.get(\"encryption\", {})\n        return encryption_config.get(\"defaultKmsKeyName\")\n\n    @default_kms_key_name.setter\n    def default_kms_key_name(self, value):\n        \"\"\"Set default KMS encryption key for objects in the bucket.\n\n        :type value: str or None\n        :param value: new KMS key name (None to clear any existing key).\n        \"\"\"\n        encryption_config = self._properties.get(\"encryption\", {})\n        encryption_config[\"defaultKmsKeyName\"] = value\n        self._patch_property(\"encryption\", encryption_config)\n\n    @property\n    def labels(self):\n        \"\"\"Retrieve or set labels assigned to this bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets#labels\n\n        .. note::\n\n           The getter for this property returns a dict which is a *copy*\n           of the bucket's labels.  Mutating that dict has no effect unless\n           you then re-assign the dict via the setter.  E.g.:\n\n           >>> labels = bucket.labels\n           >>> labels['new_key'] = 'some-label'\n           >>> del labels['old_key']\n           >>> bucket.labels = labels\n           >>> bucket.update()\n\n        :setter: Set labels for this bucket.\n        :getter: Gets the labels for this bucket.\n\n        :rtype: :class:`dict`\n        :returns: Name-value pairs (string->string) labelling the bucket.\n        \"\"\"\n        labels = self._properties.get(\"labels\")\n        if labels is None:\n            return {}\n        return copy.deepcopy(labels)\n\n    @labels.setter\n    def labels(self, mapping):\n        \"\"\"Set labels assigned to this bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets#labels\n\n        :type mapping: :class:`dict`\n        :param mapping: Name-value pairs (string->string) labelling the bucket.\n        \"\"\"\n        # If any labels have been expressly removed, we need to track this\n        # so that a future .patch() call can do the correct thing.\n        existing = set([k for k in self.labels.keys()])\n        incoming = set([k for k in mapping.keys()])\n        self._label_removals = self._label_removals.union(existing.difference(incoming))\n        mapping = {k: str(v) for k, v in mapping.items()}\n\n        # Actually update the labels on the object.\n        self._patch_property(\"labels\", copy.deepcopy(mapping))\n\n    @property\n    def etag(self):\n        \"\"\"Retrieve the ETag for the bucket.\n\n        See https://tools.ietf.org/html/rfc2616#section-3.11 and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: str or ``NoneType``\n        :returns: The bucket etag or ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"etag\")\n\n    @property\n    def id(self):\n        \"\"\"Retrieve the ID for the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: str or ``NoneType``\n        :returns: The ID of the bucket or ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"id\")\n\n    @property\n    def iam_configuration(self):\n        \"\"\"Retrieve IAM configuration for this bucket.\n\n        :rtype: :class:`IAMConfiguration`\n        :returns: an instance for managing the bucket's IAM configuration.\n        \"\"\"\n        info = self._properties.get(\"iamConfiguration\", {})\n        return IAMConfiguration.from_api_repr(info, self)\n\n    @property\n    def soft_delete_policy(self):\n        \"\"\"Retrieve the soft delete policy for this bucket.\n\n        See https://cloud.google.com/storage/docs/soft-delete\n\n        :rtype: :class:`SoftDeletePolicy`\n        :returns: an instance for managing the bucket's soft delete policy.\n        \"\"\"\n        policy = self._properties.get(\"softDeletePolicy\", {})\n        return SoftDeletePolicy.from_api_repr(policy, self)\n\n    @property\n    def lifecycle_rules(self):\n        \"\"\"Retrieve or set lifecycle rules configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/lifecycle and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        .. note::\n\n           The getter for this property returns a generator which yields\n           *copies* of the bucket's lifecycle rules mappings.  Mutating the\n           output dicts has no effect unless you then re-assign the dict via\n           the setter.  E.g.:\n\n           >>> rules = list(bucket.lifecycle_rules)\n           >>> rules.append({'origin': '/foo', ...})\n           >>> rules[1]['rule']['action']['type'] = 'Delete'\n           >>> del rules[0]\n           >>> bucket.lifecycle_rules = rules\n           >>> bucket.update()\n\n        :setter: Set lifecycle rules for this bucket.\n        :getter: Gets the lifecycle rules for this bucket.\n\n        :rtype: generator(dict)\n        :returns: A sequence of mappings describing each lifecycle rule.\n        \"\"\"\n        info = self._properties.get(\"lifecycle\", {})\n        for rule in info.get(\"rule\", ()):\n            action_type = rule[\"action\"][\"type\"]\n            if action_type == \"Delete\":\n                yield LifecycleRuleDelete.from_api_repr(rule)\n            elif action_type == \"SetStorageClass\":\n                yield LifecycleRuleSetStorageClass.from_api_repr(rule)\n            elif action_type == \"AbortIncompleteMultipartUpload\":\n                yield LifecycleRuleAbortIncompleteMultipartUpload.from_api_repr(rule)\n            else:\n                warnings.warn(\n                    \"Unknown lifecycle rule type received: {}. Please upgrade to the latest version of google-cloud-storage.\".format(\n                        rule\n                    ),\n                    UserWarning,\n                    stacklevel=1,\n                )\n\n    @lifecycle_rules.setter\n    def lifecycle_rules(self, rules):\n        \"\"\"Set lifecycle rules configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/lifecycle and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :type rules: list of dictionaries\n        :param rules: A sequence of mappings describing each lifecycle rule.\n        \"\"\"\n        rules = [dict(rule) for rule in rules]  # Convert helpers if needed\n        self._patch_property(\"lifecycle\", {\"rule\": rules})\n\n    def clear_lifecycle_rules(self):\n        \"\"\"Clear lifecycle rules configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/lifecycle and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n        \"\"\"\n        self.lifecycle_rules = []\n\n    def clear_lifecyle_rules(self):\n        \"\"\"Deprecated alias for clear_lifecycle_rules.\"\"\"\n        return self.clear_lifecycle_rules()\n\n    def add_lifecycle_delete_rule(self, **kw):\n        \"\"\"Add a \"delete\" rule to lifecycle rules configured for this bucket.\n\n        This defines a [lifecycle configuration](https://cloud.google.com/storage/docs/lifecycle),\n        which is set on the bucket. For the general format of a lifecycle configuration, see the\n        [bucket resource representation for JSON](https://cloud.google.com/storage/docs/json_api/v1/buckets).\n        See also a [code sample](https://cloud.google.com/storage/docs/samples/storage-enable-bucket-lifecycle-management#storage_enable_bucket_lifecycle_management-python).\n\n        :type kw: dict\n        :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n        \"\"\"\n        rules = list(self.lifecycle_rules)\n        rules.append(LifecycleRuleDelete(**kw))\n        self.lifecycle_rules = rules\n\n    def add_lifecycle_set_storage_class_rule(self, storage_class, **kw):\n        \"\"\"Add a \"set storage class\" rule to lifecycle rules.\n\n        This defines a [lifecycle configuration](https://cloud.google.com/storage/docs/lifecycle),\n        which is set on the bucket. For the general format of a lifecycle configuration, see the\n        [bucket resource representation for JSON](https://cloud.google.com/storage/docs/json_api/v1/buckets).\n\n        :type storage_class: str, one of :attr:`STORAGE_CLASSES`.\n        :param storage_class: new storage class to assign to matching items.\n\n        :type kw: dict\n        :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n        \"\"\"\n        rules = list(self.lifecycle_rules)\n        rules.append(LifecycleRuleSetStorageClass(storage_class, **kw))\n        self.lifecycle_rules = rules\n\n    def add_lifecycle_abort_incomplete_multipart_upload_rule(self, **kw):\n        \"\"\"Add a \"abort incomplete multipart upload\" rule to lifecycle rules.\n\n        .. note::\n          The \"age\" lifecycle condition is the only supported condition\n          for this rule.\n\n        This defines a [lifecycle configuration](https://cloud.google.com/storage/docs/lifecycle),\n        which is set on the bucket. For the general format of a lifecycle configuration, see the\n        [bucket resource representation for JSON](https://cloud.google.com/storage/docs/json_api/v1/buckets).\n\n        :type kw: dict\n        :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n        \"\"\"\n        rules = list(self.lifecycle_rules)\n        rules.append(LifecycleRuleAbortIncompleteMultipartUpload(**kw))\n        self.lifecycle_rules = rules\n\n    _location = _scalar_property(\"location\")\n\n    @property\n    def location(self):\n        \"\"\"Retrieve location configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets and\n        https://cloud.google.com/storage/docs/locations\n\n        Returns ``None`` if the property has not been set before creation,\n        or if the bucket's resource has not been loaded from the server.\n        :rtype: str or ``NoneType``\n        \"\"\"\n        return self._location\n\n    @location.setter\n    def location(self, value):\n        \"\"\"(Deprecated) Set `Bucket.location`\n\n        This can only be set at bucket **creation** time.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets and\n        https://cloud.google.com/storage/docs/bucket-locations\n\n        .. warning::\n\n            Assignment to 'Bucket.location' is deprecated, as it is only\n            valid before the bucket is created. Instead, pass the location\n            to `Bucket.create`.\n        \"\"\"\n        warnings.warn(_LOCATION_SETTER_MESSAGE, DeprecationWarning, stacklevel=2)\n        self._location = value\n\n    @property\n    def data_locations(self):\n        \"\"\"Retrieve the list of regional locations for custom dual-region buckets.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets and\n        https://cloud.google.com/storage/docs/locations\n\n        Returns ``None`` if the property has not been set before creation,\n        if the bucket's resource has not been loaded from the server,\n        or if the bucket is not a dual-regions bucket.\n        :rtype: list of str or ``NoneType``\n        \"\"\"\n        custom_placement_config = self._properties.get(\"customPlacementConfig\", {})\n        return custom_placement_config.get(\"dataLocations\")\n\n    @property\n    def location_type(self):\n        \"\"\"Retrieve the location type for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :getter: Gets the the location type for this bucket.\n\n        :rtype: str or ``NoneType``\n        :returns:\n            If set, one of\n            :attr:`~google.cloud.storage.constants.MULTI_REGION_LOCATION_TYPE`,\n            :attr:`~google.cloud.storage.constants.REGION_LOCATION_TYPE`, or\n            :attr:`~google.cloud.storage.constants.DUAL_REGION_LOCATION_TYPE`,\n            else ``None``.\n        \"\"\"\n        return self._properties.get(\"locationType\")\n\n    def get_logging(self):\n        \"\"\"Return info about access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs#status\n\n        :rtype: dict or None\n        :returns: a dict w/ keys, ``logBucket`` and ``logObjectPrefix``\n                  (if logging is enabled), or None (if not).\n        \"\"\"\n        info = self._properties.get(\"logging\")\n        return copy.deepcopy(info)\n\n    def enable_logging(self, bucket_name, object_prefix=\"\"):\n        \"\"\"Enable access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs\n\n        :type bucket_name: str\n        :param bucket_name: name of bucket in which to store access logs\n\n        :type object_prefix: str\n        :param object_prefix: prefix for access log filenames\n        \"\"\"\n        info = {\"logBucket\": bucket_name, \"logObjectPrefix\": object_prefix}\n        self._patch_property(\"logging\", info)\n\n    def disable_logging(self):\n        \"\"\"Disable access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs#disabling\n        \"\"\"\n        self._patch_property(\"logging\", None)\n\n    @property\n    def metageneration(self):\n        \"\"\"Retrieve the metageneration for the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: int or ``NoneType``\n        :returns: The metageneration of the bucket or ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        metageneration = self._properties.get(\"metageneration\")\n        if metageneration is not None:\n            return int(metageneration)\n\n    @property\n    def owner(self):\n        \"\"\"Retrieve info about the owner of the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: dict or ``NoneType``\n        :returns: Mapping of owner's role/ID. Returns ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return copy.deepcopy(self._properties.get(\"owner\"))\n\n    @property\n    def project_number(self):\n        \"\"\"Retrieve the number of the project to which the bucket is assigned.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: int or ``NoneType``\n        :returns: The project number that owns the bucket or ``None`` if\n                  the bucket's resource has not been loaded from the server.\n        \"\"\"\n        project_number = self._properties.get(\"projectNumber\")\n        if project_number is not None:\n            return int(project_number)\n\n    @property\n    def retention_policy_effective_time(self):\n        \"\"\"Retrieve the effective time of the bucket's retention policy.\n\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's retention policy is\n                  effective, or ``None`` if the property is not\n                  set locally.\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            timestamp = policy.get(\"effectiveTime\")\n            if timestamp is not None:\n                return _rfc3339_nanos_to_datetime(timestamp)\n\n    @property\n    def retention_policy_locked(self):\n        \"\"\"Retrieve whthere the bucket's retention policy is locked.\n\n        :rtype: bool\n        :returns: True if the bucket's policy is locked, or else False\n                  if the policy is not locked, or the property is not\n                  set locally.\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            return policy.get(\"isLocked\")\n\n    @property\n    def retention_period(self):\n        \"\"\"Retrieve or set the retention period for items in the bucket.\n\n        :rtype: int or ``NoneType``\n        :returns: number of seconds to retain items after upload or release\n                  from event-based lock, or ``None`` if the property is not\n                  set locally.\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            period = policy.get(\"retentionPeriod\")\n            if period is not None:\n                return int(period)\n\n    @retention_period.setter\n    def retention_period(self, value):\n        \"\"\"Set the retention period for items in the bucket.\n\n        :type value: int\n        :param value:\n            number of seconds to retain items after upload or release from\n            event-based lock.\n\n        :raises ValueError: if the bucket's retention policy is locked.\n        \"\"\"\n        policy = self._properties.setdefault(\"retentionPolicy\", {})\n        if value is not None:\n            policy[\"retentionPeriod\"] = str(value)\n        else:\n            policy = None\n        self._patch_property(\"retentionPolicy\", policy)\n\n    @property\n    def self_link(self):\n        \"\"\"Retrieve the URI for the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: str or ``NoneType``\n        :returns: The self link for the bucket or ``None`` if\n                  the bucket's resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"selfLink\")\n\n    @property\n    def storage_class(self):\n        \"\"\"Retrieve or set the storage class for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :setter: Set the storage class for this bucket.\n        :getter: Gets the the storage class for this bucket.\n\n        :rtype: str or ``NoneType``\n        :returns:\n            If set, one of\n            :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`,\n            or\n            :attr:`~google.cloud.storage.constants.DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS`,\n            else ``None``.\n        \"\"\"\n        return self._properties.get(\"storageClass\")\n\n    @storage_class.setter\n    def storage_class(self, value):\n        \"\"\"Set the storage class for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :type value: str\n        :param value:\n            One of\n            :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`,\n            or\n            :attr:`~google.cloud.storage.constants.DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS`,\n        \"\"\"\n        self._patch_property(\"storageClass\", value)\n\n    @property\n    def time_created(self):\n        \"\"\"Retrieve the timestamp at which the bucket was created.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"timeCreated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def updated(self):\n        \"\"\"Retrieve the timestamp at which the bucket was last updated.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"updated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def versioning_enabled(self):\n        \"\"\"Is versioning enabled for this bucket?\n\n        See  https://cloud.google.com/storage/docs/object-versioning for\n        details.\n\n        :setter: Update whether versioning is enabled for this bucket.\n        :getter: Query whether versioning is enabled for this bucket.\n\n        :rtype: bool\n        :returns: True if enabled, else False.\n        \"\"\"\n        versioning = self._properties.get(\"versioning\", {})\n        return versioning.get(\"enabled\", False)\n\n    @versioning_enabled.setter\n    def versioning_enabled(self, value):\n        \"\"\"Enable versioning for this bucket.\n\n        See  https://cloud.google.com/storage/docs/object-versioning for\n        details.\n\n        :type value: convertible to boolean\n        :param value: should versioning be enabled for the bucket?\n        \"\"\"\n        self._patch_property(\"versioning\", {\"enabled\": bool(value)})\n\n    @property\n    def requester_pays(self):\n        \"\"\"Does the requester pay for API requests for this bucket?\n\n        See https://cloud.google.com/storage/docs/requester-pays for\n        details.\n\n        :setter: Update whether requester pays for this bucket.\n        :getter: Query whether requester pays for this bucket.\n\n        :rtype: bool\n        :returns: True if requester pays for API requests for the bucket,\n                  else False.\n        \"\"\"\n        versioning = self._properties.get(\"billing\", {})\n        return versioning.get(\"requesterPays\", False)\n\n    @requester_pays.setter\n    def requester_pays(self, value):\n        \"\"\"Update whether requester pays for API requests for this bucket.\n\n        See https://cloud.google.com/storage/docs/using-requester-pays for\n        details.\n\n        :type value: convertible to boolean\n        :param value: should requester pay for API requests for the bucket?\n        \"\"\"\n        self._patch_property(\"billing\", {\"requesterPays\": bool(value)})\n\n    @property\n    def autoclass_enabled(self):\n        \"\"\"Whether Autoclass is enabled for this bucket.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :setter: Update whether autoclass is enabled for this bucket.\n        :getter: Query whether autoclass is enabled for this bucket.\n\n        :rtype: bool\n        :returns: True if enabled, else False.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        return autoclass.get(\"enabled\", False)\n\n    @autoclass_enabled.setter\n    def autoclass_enabled(self, value):\n        \"\"\"Enable or disable Autoclass at the bucket-level.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :type value: convertible to boolean\n        :param value: If true, enable Autoclass for this bucket.\n                      If false, disable Autoclass for this bucket.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        autoclass[\"enabled\"] = bool(value)\n        self._patch_property(\"autoclass\", autoclass)\n\n    @property\n    def autoclass_toggle_time(self):\n        \"\"\"Retrieve the toggle time when Autoclaass was last enabled or disabled for the bucket.\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's autoclass is toggled, or ``None`` if the property is not set locally.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\")\n        if autoclass is not None:\n            timestamp = autoclass.get(\"toggleTime\")\n            if timestamp is not None:\n                return _rfc3339_nanos_to_datetime(timestamp)\n\n    @property\n    def autoclass_terminal_storage_class(self):\n        \"\"\"The storage class that objects in an Autoclass bucket eventually transition to if\n        they are not read for a certain length of time. Valid values are NEARLINE and ARCHIVE.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :setter: Set the terminal storage class for Autoclass configuration.\n        :getter: Get the terminal storage class for Autoclass configuration.\n\n        :rtype: str\n        :returns: The terminal storage class if Autoclass is enabled, else ``None``.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        return autoclass.get(\"terminalStorageClass\", None)\n\n    @autoclass_terminal_storage_class.setter\n    def autoclass_terminal_storage_class(self, value):\n        \"\"\"The storage class that objects in an Autoclass bucket eventually transition to if\n        they are not read for a certain length of time. Valid values are NEARLINE and ARCHIVE.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :type value: str\n        :param value: The only valid values are `\"NEARLINE\"` and `\"ARCHIVE\"`.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        autoclass[\"terminalStorageClass\"] = value\n        self._patch_property(\"autoclass\", autoclass)\n\n    @property\n    def autoclass_terminal_storage_class_update_time(self):\n        \"\"\"The time at which the Autoclass terminal_storage_class field was last updated for this bucket\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's terminal_storage_class is last updated, or ``None`` if the property is not set locally.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\")\n        if autoclass is not None:\n            timestamp = autoclass.get(\"terminalStorageClassUpdateTime\")\n            if timestamp is not None:\n                return _rfc3339_nanos_to_datetime(timestamp)\n\n    @property\n    def object_retention_mode(self):\n        \"\"\"Retrieve the object retention mode set on the bucket.\n\n        :rtype: str\n        :returns: When set to Enabled, retention configurations can be\n                  set on objects in the bucket.\n        \"\"\"\n        object_retention = self._properties.get(\"objectRetention\")\n        if object_retention is not None:\n            return object_retention.get(\"mode\")\n\n    @property\n    def hierarchical_namespace_enabled(self):\n        \"\"\"Whether hierarchical namespace is enabled for this bucket.\n\n        :setter: Update whether hierarchical namespace is enabled for this bucket.\n        :getter: Query whether hierarchical namespace is enabled for this bucket.\n\n        :rtype: bool\n        :returns: True if enabled, else False.\n        \"\"\"\n        hns = self._properties.get(\"hierarchicalNamespace\", {})\n        return hns.get(\"enabled\")\n\n    @hierarchical_namespace_enabled.setter\n    def hierarchical_namespace_enabled(self, value):\n        \"\"\"Enable or disable hierarchical namespace at the bucket-level.\n\n        :type value: convertible to boolean\n        :param value: If true, enable hierarchical namespace for this bucket.\n                      If false, disable hierarchical namespace for this bucket.\n\n        .. note::\n          To enable hierarchical namespace, you must set it at bucket creation time.\n          Currently, hierarchical namespace configuration cannot be changed after bucket creation.\n        \"\"\"\n        hns = self._properties.get(\"hierarchicalNamespace\", {})\n        hns[\"enabled\"] = bool(value)\n        self._patch_property(\"hierarchicalNamespace\", hns)\n\n    def configure_website(self, main_page_suffix=None, not_found_page=None):\n        \"\"\"Configure website-related properties.\n\n        See https://cloud.google.com/storage/docs/static-website\n\n        .. note::\n          This configures the bucket's website-related properties,controlling how\n          the service behaves when accessing bucket contents as a web site.\n          See [tutorials](https://cloud.google.com/storage/docs/hosting-static-website) and\n          [code samples](https://cloud.google.com/storage/docs/samples/storage-define-bucket-website-configuration#storage_define_bucket_website_configuration-python)\n          for more information.\n\n        :type main_page_suffix: str\n        :param main_page_suffix: The page to use as the main page\n                                 of a directory.\n                                 Typically something like index.html.\n\n        :type not_found_page: str\n        :param not_found_page: The file to use when a page isn't found.\n        \"\"\"\n        data = {\"mainPageSuffix\": main_page_suffix, \"notFoundPage\": not_found_page}\n        self._patch_property(\"website\", data)\n\n    def disable_website(self):\n        \"\"\"Disable the website configuration for this bucket.\n\n        This is really just a shortcut for setting the website-related\n        attributes to ``None``.\n        \"\"\"\n        return self.configure_website(None, None)\n\n    def get_iam_policy(\n        self,\n        client=None,\n        requested_policy_version=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Retrieve the IAM policy for the bucket.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/getIamPolicy)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-view-bucket-iam-members#storage_view_bucket_iam_members-python).\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type requested_policy_version: int or ``NoneType``\n        :param requested_policy_version: (Optional) The version of IAM policies to request.\n                                         If a policy with a condition is requested without\n                                         setting this, the server will return an error.\n                                         This must be set to a value of 3 to retrieve IAM\n                                         policies containing conditions. This is to prevent\n                                         client code that isn't aware of IAM conditions from\n                                         interpreting and modifying policies incorrectly.\n                                         The service might return a policy with version lower\n                                         than the one that was requested, based on the\n                                         feature syntax in the policy fetched.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``getIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        if requested_policy_version is not None:\n            query_params[\"optionsRequestedPolicyVersion\"] = requested_policy_version\n\n        info = client._get_resource(\n            f\"{self.path}/iam\",\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return Policy.from_api_repr(info)\n\n    def set_iam_policy(\n        self,\n        policy,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n    ):\n        \"\"\"Update the IAM policy for the bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets/setIamPolicy\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type policy: :class:`google.api_core.iam.Policy`\n        :param policy: policy instance used to update bucket's IAM policy.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``setIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam\"\n        resource = policy.to_api_repr()\n        resource[\"resourceId\"] = self.path\n\n        info = client._put_resource(\n            path,\n            resource,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n        return Policy.from_api_repr(info)\n\n    def test_iam_permissions(\n        self, permissions, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"API call:  test permissions\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets/testIamPermissions\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type permissions: list of string\n        :param permissions: the permissions to check\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: list of string\n        :returns: the permissions returned by the ``testIamPermissions`` API\n                  request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {\"permissions\": permissions}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam/testPermissions\"\n        resp = client._get_resource(\n            path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return resp.get(\"permissions\", [])\n\n    def make_public(\n        self,\n        recursive=False,\n        future=False,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update bucket's ACL, granting read access to anonymous users.\n\n        :type recursive: bool\n        :param recursive: If True, this will make all blobs inside the bucket\n                          public as well.\n\n        :type future: bool\n        :param future: If True, this will make all objects created in the\n                       future public as well.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError:\n            If ``recursive`` is True, and the bucket contains more than 256\n            blobs.  This is to prevent extremely long runtime of this\n            method.  For such buckets, iterate over the blobs returned by\n            :meth:`list_blobs` and call\n            :meth:`~google.cloud.storage.blob.Blob.make_public`\n            for each blob.\n        \"\"\"\n        self.acl.all().grant_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n        if future:\n            doa = self.default_object_acl\n            if not doa.loaded:\n                doa.reload(client=client, timeout=timeout)\n            doa.all().grant_read()\n            doa.save(\n                client=client,\n                timeout=timeout,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n            )\n\n        if recursive:\n            blobs = list(\n                self.list_blobs(\n                    projection=\"full\",\n                    max_results=self._MAX_OBJECTS_FOR_ITERATION + 1,\n                    client=client,\n                    timeout=timeout,\n                )\n            )\n            if len(blobs) > self._MAX_OBJECTS_FOR_ITERATION:\n                message = (\n                    \"Refusing to make public recursively with more than \"\n                    \"%d objects. If you actually want to make every object \"\n                    \"in this bucket public, iterate through the blobs \"\n                    \"returned by 'Bucket.list_blobs()' and call \"\n                    \"'make_public' on each one.\"\n                ) % (self._MAX_OBJECTS_FOR_ITERATION,)\n                raise ValueError(message)\n\n            for blob in blobs:\n                blob.acl.all().grant_read()\n                blob.acl.save(\n                    client=client,\n                    timeout=timeout,\n                )\n\n    def make_private(\n        self,\n        recursive=False,\n        future=False,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update bucket's ACL, revoking read access for anonymous users.\n\n        :type recursive: bool\n        :param recursive: If True, this will make all blobs inside the bucket\n                          private as well.\n\n        :type future: bool\n        :param future: If True, this will make all objects created in the\n                       future private as well.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError:\n            If ``recursive`` is True, and the bucket contains more than 256\n            blobs.  This is to prevent extremely long runtime of this\n            method.  For such buckets, iterate over the blobs returned by\n            :meth:`list_blobs` and call\n            :meth:`~google.cloud.storage.blob.Blob.make_private`\n            for each blob.\n        \"\"\"\n        self.acl.all().revoke_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n        if future:\n            doa = self.default_object_acl\n            if not doa.loaded:\n                doa.reload(client=client, timeout=timeout)\n            doa.all().revoke_read()\n            doa.save(\n                client=client,\n                timeout=timeout,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n            )\n\n        if recursive:\n            blobs = list(\n                self.list_blobs(\n                    projection=\"full\",\n                    max_results=self._MAX_OBJECTS_FOR_ITERATION + 1,\n                    client=client,\n                    timeout=timeout,\n                )\n            )\n            if len(blobs) > self._MAX_OBJECTS_FOR_ITERATION:\n                message = (\n                    \"Refusing to make private recursively with more than \"\n                    \"%d objects. If you actually want to make every object \"\n                    \"in this bucket private, iterate through the blobs \"\n                    \"returned by 'Bucket.list_blobs()' and call \"\n                    \"'make_private' on each one.\"\n                ) % (self._MAX_OBJECTS_FOR_ITERATION,)\n                raise ValueError(message)\n\n            for blob in blobs:\n                blob.acl.all().revoke_read()\n                blob.acl.save(client=client, timeout=timeout)\n\n    def generate_upload_policy(self, conditions, expiration=None, client=None):\n        \"\"\"Create a signed upload policy for uploading objects.\n\n        This method generates and signs a policy document. You can use\n        [`policy documents`](https://cloud.google.com/storage/docs/xml-api/post-object-forms)\n        to allow visitors to a website to upload files to\n        Google Cloud Storage without giving them direct write access.\n        See a [code sample](https://cloud.google.com/storage/docs/xml-api/post-object-forms#python).\n\n        :type expiration: datetime\n        :param expiration: (Optional) Expiration in UTC. If not specified, the\n                           policy will expire in 1 hour.\n\n        :type conditions: list\n        :param conditions: A list of conditions as described in the\n                          `policy documents` documentation.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :rtype: dict\n        :returns: A dictionary of (form field name, form field value) of form\n                  fields that should be added to your HTML upload form in order\n                  to attach the signature.\n        \"\"\"\n        client = self._require_client(client)\n        credentials = client._credentials\n        _signing.ensure_signed_credentials(credentials)\n\n        if expiration is None:\n            expiration = _NOW(_UTC).replace(tzinfo=None) + datetime.timedelta(hours=1)\n\n        conditions = conditions + [{\"bucket\": self.name}]\n\n        policy_document = {\n            \"expiration\": _datetime_to_rfc3339(expiration),\n            \"conditions\": conditions,\n        }\n\n        encoded_policy_document = base64.b64encode(\n            json.dumps(policy_document).encode(\"utf-8\")\n        )\n        signature = base64.b64encode(credentials.sign_bytes(encoded_policy_document))\n\n        fields = {\n            \"bucket\": self.name,\n            \"GoogleAccessId\": credentials.signer_email,\n            \"policy\": encoded_policy_document.decode(\"utf-8\"),\n            \"signature\": signature.decode(\"utf-8\"),\n        }\n\n        return fields\n\n    def lock_retention_policy(\n        self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"Lock the bucket's retention policy.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError:\n            if the bucket has no metageneration (i.e., new or never reloaded);\n            if the bucket has no retention policy assigned;\n            if the bucket's retention policy is already locked.\n        \"\"\"\n        if \"metageneration\" not in self._properties:\n            raise ValueError(\"Bucket has no retention policy assigned: try 'reload'?\")\n\n        policy = self._properties.get(\"retentionPolicy\")\n\n        if policy is None:\n            raise ValueError(\"Bucket has no retention policy assigned: try 'reload'?\")\n\n        if policy.get(\"isLocked\"):\n            raise ValueError(\"Bucket's retention policy is already locked.\")\n\n        client = self._require_client(client)\n\n        query_params = {\"ifMetagenerationMatch\": self.metageneration}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"/b/{self.name}/lockRetentionPolicy\"\n        api_response = client._post_resource(\n            path,\n            None,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n    def generate_signed_url(\n        self,\n        expiration=None,\n        api_access_endpoint=None,\n        method=\"GET\",\n        headers=None,\n        query_parameters=None,\n        client=None,\n        credentials=None,\n        version=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n    ):\n        \"\"\"Generates a signed URL for this bucket.\n\n        .. note::\n\n            If you are on Google Compute Engine, you can't generate a signed\n            URL using GCE service account. If you'd like to be able to generate\n            a signed URL from GCE, you can use a standard service account from a\n            JSON file rather than a GCE service account.\n\n        If you have a bucket that you want to allow access to for a set\n        amount of time, you can use this method to generate a URL that\n        is only valid within a certain time period.\n\n        If ``bucket_bound_hostname`` is set as an argument of :attr:`api_access_endpoint`,\n        ``https`` works only if using a ``CDN``.\n\n        :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n        :param expiration: Point in time when the signed URL should expire. If\n                           a ``datetime`` instance is passed without an explicit\n                           ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n        :type api_access_endpoint: str\n        :param api_access_endpoint: (Optional) URI base, for instance\n            \"https://storage.googleapis.com\". If not specified, the client's\n            api_endpoint will be used. Incompatible with bucket_bound_hostname.\n\n        :type method: str\n        :param method: The HTTP verb that will be used when requesting the URL.\n\n        :type headers: dict\n        :param headers:\n            (Optional) Additional HTTP headers to be included as part of the\n            signed URLs.  See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers\n            Requests using the signed URL *must* pass the specified header\n            (name and value) with each request for the URL.\n\n        :type query_parameters: dict\n        :param query_parameters:\n            (Optional) Additional query parameters to be included as part of the\n            signed URLs.  See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type credentials: :class:`google.auth.credentials.Credentials` or\n                           :class:`NoneType`\n        :param credentials: The authorization credentials to attach to requests.\n                            These credentials identify this application to the service.\n                            If none are specified, the client will attempt to ascertain\n                            the credentials from the environment.\n\n        :type version: str\n        :param version: (Optional) The version of signed credential to create.\n                        Must be one of 'v2' | 'v4'.\n\n        :type virtual_hosted_style: bool\n        :param virtual_hosted_style:\n            (Optional) If true, then construct the URL relative the bucket's\n            virtual hostname, e.g., '<bucket-name>.storage.googleapis.com'.\n            Incompatible with bucket_bound_hostname.\n\n        :type bucket_bound_hostname: str\n        :param bucket_bound_hostname:\n            (Optional) If passed, then construct the URL relative to the bucket-bound hostname.\n            Value can be a bare or with scheme, e.g., 'example.com' or 'http://example.com'.\n            Incompatible with api_access_endpoint and virtual_hosted_style.\n            See: https://cloud.google.com/storage/docs/request-endpoints#cname\n\n        :type scheme: str\n        :param scheme:\n            (Optional) If ``bucket_bound_hostname`` is passed as a bare hostname, use\n            this value as the scheme.  ``https`` will work only when using a CDN.\n            Defaults to ``\"http\"``.\n\n        :raises: :exc:`ValueError` when version is invalid or mutually exclusive arguments are used.\n        :raises: :exc:`TypeError` when expiration is not a valid type.\n        :raises: :exc:`AttributeError` if credentials is not an instance\n                of :class:`google.auth.credentials.Signing`.\n\n        :rtype: str\n        :returns: A signed URL you can use to access the resource\n                  until expiration.\n        \"\"\"\n        if version is None:\n            version = \"v2\"\n        elif version not in (\"v2\", \"v4\"):\n            raise ValueError(\"'version' must be either 'v2' or 'v4'\")\n\n        if (\n            api_access_endpoint is not None or virtual_hosted_style\n        ) and bucket_bound_hostname:\n            raise ValueError(\n                \"The bucket_bound_hostname argument is not compatible with \"\n                \"either api_access_endpoint or virtual_hosted_style.\"\n            )\n\n        if api_access_endpoint is None:\n            client = self._require_client(client)\n            api_access_endpoint = client.api_endpoint\n\n        # If you are on Google Compute Engine, you can't generate a signed URL\n        # using GCE service account.\n        # See https://github.com/googleapis/google-auth-library-python/issues/50\n        if virtual_hosted_style:\n            api_access_endpoint = _virtual_hosted_style_base_url(\n                api_access_endpoint, self.name\n            )\n            resource = \"/\"\n        elif bucket_bound_hostname:\n            api_access_endpoint = _bucket_bound_hostname_url(\n                bucket_bound_hostname, scheme\n            )\n            resource = \"/\"\n        else:\n            resource = f\"/{self.name}\"\n\n        if credentials is None:\n            client = self._require_client(client)  # May be redundant, but that's ok.\n            credentials = client._credentials\n\n        if version == \"v2\":\n            helper = generate_signed_url_v2\n        else:\n            helper = generate_signed_url_v4\n\n        return helper(\n            credentials,\n            resource=resource,\n            expiration=expiration,\n            api_access_endpoint=api_access_endpoint,\n            method=method.upper(),\n            headers=headers,\n            query_parameters=query_parameters,\n        )\n\n\nclass SoftDeletePolicy(dict):\n    \"\"\"Map a bucket's soft delete policy.\n\n    See https://cloud.google.com/storage/docs/soft-delete\n\n    :type bucket: :class:`Bucket`\n    :param bucket: Bucket for which this instance is the policy.\n\n    :type retention_duration_seconds: int\n    :param retention_duration_seconds:\n        (Optional) The period of time in seconds that soft-deleted objects in the bucket\n        will be retained and cannot be permanently deleted.\n\n    :type effective_time: :class:`datetime.datetime`\n    :param effective_time:\n        (Optional) When the bucket's soft delete policy is effective.\n        This value should normally only be set by the back-end API.\n    \"\"\"\n\n    def __init__(self, bucket, **kw):\n        data = {}\n        retention_duration_seconds = kw.get(\"retention_duration_seconds\")\n        data[\"retentionDurationSeconds\"] = retention_duration_seconds\n\n        effective_time = kw.get(\"effective_time\")\n        if effective_time is not None:\n            effective_time = _datetime_to_rfc3339(effective_time)\n        data[\"effectiveTime\"] = effective_time\n\n        super().__init__(data)\n        self._bucket = bucket\n\n    @classmethod\n    def from_api_repr(cls, resource, bucket):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :type bucket: :class:`Bucket`\n        :params bucket: Bucket for which this instance is the policy.\n\n        :rtype: :class:`SoftDeletePolicy`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(bucket)\n        instance.update(resource)\n        return instance\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket for which this instance is the policy.\n\n        :rtype: :class:`Bucket`\n        :returns: the instance's bucket.\n        \"\"\"\n        return self._bucket\n\n    @property\n    def retention_duration_seconds(self):\n        \"\"\"Get the retention duration of the bucket's soft delete policy.\n\n        :rtype: int or ``NoneType``\n        :returns: The period of time in seconds that soft-deleted objects in the bucket\n                  will be retained and cannot be permanently deleted; Or ``None`` if the\n                  property is not set.\n        \"\"\"\n        duration = self.get(\"retentionDurationSeconds\")\n        if duration is not None:\n            return int(duration)\n\n    @retention_duration_seconds.setter\n    def retention_duration_seconds(self, value):\n        \"\"\"Set the retention duration of the bucket's soft delete policy.\n\n        :type value: int\n        :param value:\n            The period of time in seconds that soft-deleted objects in the bucket\n            will be retained and cannot be permanently deleted.\n        \"\"\"\n        self[\"retentionDurationSeconds\"] = value\n        self.bucket._patch_property(\"softDeletePolicy\", self)\n\n    @property\n    def effective_time(self):\n        \"\"\"Get the effective time of the bucket's soft delete policy.\n\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's soft delte policy is\n                  effective, or ``None`` if the property is not set.\n        \"\"\"\n        timestamp = self.get(\"effectiveTime\")\n        if timestamp is not None:\n            return _rfc3339_nanos_to_datetime(timestamp)\n\n\ndef _raise_if_len_differs(expected_len, **generation_match_args):\n    \"\"\"\n    Raise an error if any generation match argument\n    is set and its len differs from the given value.\n\n    :type expected_len: int\n    :param expected_len: Expected argument length in case it's set.\n\n    :type generation_match_args: dict\n    :param generation_match_args: Lists, which length must be checked.\n\n    :raises: :exc:`ValueError` if any argument set, but has an unexpected length.\n    \"\"\"\n    for name, value in generation_match_args.items():\n        if value is not None and len(value) != expected_len:\n            raise ValueError(f\"'{name}' length must be the same as 'blobs' length\")\n", "google/cloud/storage/client.py": "# Copyright 2015 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Client for interacting with the Google Cloud Storage API.\"\"\"\n\nimport base64\nimport binascii\nimport collections\nimport datetime\nimport functools\nimport json\nimport warnings\nimport google.api_core.client_options\n\nfrom google.auth.credentials import AnonymousCredentials\n\nfrom google.api_core import page_iterator\nfrom google.cloud._helpers import _LocalStack\nfrom google.cloud.client import ClientWithProject\nfrom google.cloud.exceptions import NotFound\n\nfrom google.cloud.storage._helpers import _bucket_bound_hostname_url\nfrom google.cloud.storage._helpers import _get_api_endpoint_override\nfrom google.cloud.storage._helpers import _get_environ_project\nfrom google.cloud.storage._helpers import _get_storage_emulator_override\nfrom google.cloud.storage._helpers import _use_client_cert\nfrom google.cloud.storage._helpers import _virtual_hosted_style_base_url\nfrom google.cloud.storage._helpers import _DEFAULT_UNIVERSE_DOMAIN\nfrom google.cloud.storage._helpers import _DEFAULT_SCHEME\nfrom google.cloud.storage._helpers import _STORAGE_HOST_TEMPLATE\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\n\nfrom google.cloud.storage._http import Connection\nfrom google.cloud.storage._signing import (\n    get_expiration_seconds_v4,\n    get_v4_now_dtstamps,\n    ensure_signed_credentials,\n    _sign_message,\n)\nfrom google.cloud.storage.batch import Batch\nfrom google.cloud.storage.bucket import Bucket, _item_to_blob, _blobs_page_start\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.hmac_key import HMACKeyMetadata\nfrom google.cloud.storage.acl import BucketACL\nfrom google.cloud.storage.acl import DefaultObjectACL\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\n_marker = object()\n\n\nclass Client(ClientWithProject):\n    \"\"\"Client to bundle configuration needed for API requests.\n\n    :type project: str or None\n    :param project: the project which the client acts on behalf of. Will be\n                    passed when creating a topic.  If not passed,\n                    falls back to the default inferred from the environment.\n\n    :type credentials: :class:`~google.auth.credentials.Credentials`\n    :param credentials: (Optional) The OAuth2 Credentials to use for this\n                        client. If not passed (and if no ``_http`` object is\n                        passed), falls back to the default inferred from the\n                        environment.\n\n    :type _http: :class:`~requests.Session`\n    :param _http: (Optional) HTTP object to make requests. Can be any object\n                  that defines ``request()`` with the same interface as\n                  :meth:`requests.Session.request`. If not passed, an\n                  ``_http`` object is created that is bound to the\n                  ``credentials`` for the current object.\n                  This parameter should be considered private, and could\n                  change in the future.\n\n    :type client_info: :class:`~google.api_core.client_info.ClientInfo`\n    :param client_info:\n        The client info used to send a user-agent string along with API\n        requests. If ``None``, then default info will be used. Generally,\n        you only need to set this if you're developing your own library\n        or partner tool.\n\n    :type client_options: :class:`~google.api_core.client_options.ClientOptions` or :class:`dict`\n    :param client_options: (Optional) Client options used to set user options on the client.\n        A non-default universe domain or api endpoint should be set through client_options.\n\n    :type use_auth_w_custom_endpoint: bool\n    :param use_auth_w_custom_endpoint:\n        (Optional) Whether authentication is required under custom endpoints.\n        If false, uses AnonymousCredentials and bypasses authentication.\n        Defaults to True. Note this is only used when a custom endpoint is set in conjunction.\n\n    :type extra_headers: dict\n    :param extra_headers:\n        (Optional) Custom headers to be sent with the requests attached to the client.\n        For example, you can add custom audit logging headers.\n    \"\"\"\n\n    SCOPE = (\n        \"https://www.googleapis.com/auth/devstorage.full_control\",\n        \"https://www.googleapis.com/auth/devstorage.read_only\",\n        \"https://www.googleapis.com/auth/devstorage.read_write\",\n    )\n    \"\"\"The scopes required for authenticating as a Cloud Storage consumer.\"\"\"\n\n    def __init__(\n        self,\n        project=_marker,\n        credentials=None,\n        _http=None,\n        client_info=None,\n        client_options=None,\n        use_auth_w_custom_endpoint=True,\n        extra_headers={},\n    ):\n        self._base_connection = None\n\n        if project is None:\n            no_project = True\n            project = \"<none>\"\n        else:\n            no_project = False\n\n        if project is _marker:\n            project = None\n\n        # Save the initial value of constructor arguments before they\n        # are passed along, for use in __reduce__ defined elsewhere.\n        self._initial_client_info = client_info\n        self._initial_client_options = client_options\n        self._extra_headers = extra_headers\n\n        connection_kw_args = {\"client_info\": client_info}\n\n        if client_options:\n            if isinstance(client_options, dict):\n                client_options = google.api_core.client_options.from_dict(\n                    client_options\n                )\n\n        if client_options and client_options.universe_domain:\n            self._universe_domain = client_options.universe_domain\n        else:\n            self._universe_domain = None\n\n        storage_emulator_override = _get_storage_emulator_override()\n        api_endpoint_override = _get_api_endpoint_override()\n\n        # Determine the api endpoint. The rules are as follows:\n\n        # 1. If the `api_endpoint` is set in `client_options`, use that as the\n        #    endpoint.\n        if client_options and client_options.api_endpoint:\n            api_endpoint = client_options.api_endpoint\n\n        # 2. Elif the \"STORAGE_EMULATOR_HOST\" env var is set, then use that as the\n        #    endpoint.\n        elif storage_emulator_override:\n            api_endpoint = storage_emulator_override\n\n        # 3. Elif the \"API_ENDPOINT_OVERRIDE\" env var is set, then use that as the\n        #    endpoint.\n        elif api_endpoint_override:\n            api_endpoint = api_endpoint_override\n\n        # 4. Elif the `universe_domain` is set in `client_options`,\n        #    create the endpoint using that as the default.\n        #\n        #    Mutual TLS is not compatible with a non-default universe domain\n        #    at this time. If such settings are enabled along with the\n        #    \"GOOGLE_API_USE_CLIENT_CERTIFICATE\" env variable, a ValueError will\n        #    be raised.\n\n        elif self._universe_domain:\n            # The final decision of whether to use mTLS takes place in\n            # google-auth-library-python. We peek at the environment variable\n            # here only to issue an exception in case of a conflict.\n            if _use_client_cert():\n                raise ValueError(\n                    'The \"GOOGLE_API_USE_CLIENT_CERTIFICATE\" env variable is '\n                    'set to \"true\" and a non-default universe domain is '\n                    \"configured. mTLS is not supported in any universe other than\"\n                    \"googleapis.com.\"\n                )\n            api_endpoint = _DEFAULT_SCHEME + _STORAGE_HOST_TEMPLATE.format(\n                universe_domain=self._universe_domain\n            )\n\n        # 5. Else, use the default, which is to use the default\n        #    universe domain of \"googleapis.com\" and create the endpoint\n        #    \"storage.googleapis.com\" from that.\n        else:\n            api_endpoint = None\n\n        connection_kw_args[\"api_endpoint\"] = api_endpoint\n\n        self._is_emulator_set = True if storage_emulator_override else False\n\n        # If a custom endpoint is set, the client checks for credentials\n        # or finds the default credentials based on the current environment.\n        # Authentication may be bypassed under certain conditions:\n        # (1) STORAGE_EMULATOR_HOST is set (for backwards compatibility), OR\n        # (2) use_auth_w_custom_endpoint is set to False.\n        if connection_kw_args[\"api_endpoint\"] is not None:\n            if self._is_emulator_set or not use_auth_w_custom_endpoint:\n                if credentials is None:\n                    credentials = AnonymousCredentials()\n                if project is None:\n                    project = _get_environ_project()\n                if project is None:\n                    no_project = True\n                    project = \"<none>\"\n\n        super(Client, self).__init__(\n            project=project,\n            credentials=credentials,\n            client_options=client_options,\n            _http=_http,\n        )\n\n        # Validate that the universe domain of the credentials matches the\n        # universe domain of the client.\n        if self._credentials.universe_domain != self.universe_domain:\n            raise ValueError(\n                \"The configured universe domain ({client_ud}) does not match \"\n                \"the universe domain found in the credentials ({cred_ud}). If \"\n                \"you haven't configured the universe domain explicitly, \"\n                \"`googleapis.com` is the default.\".format(\n                    client_ud=self.universe_domain,\n                    cred_ud=self._credentials.universe_domain,\n                )\n            )\n\n        if no_project:\n            self.project = None\n\n        # Pass extra_headers to Connection\n        connection = Connection(self, **connection_kw_args)\n        connection.extra_headers = extra_headers\n        self._connection = connection\n        self._batch_stack = _LocalStack()\n\n    @classmethod\n    def create_anonymous_client(cls):\n        \"\"\"Factory: return client with anonymous credentials.\n\n        .. note::\n\n           Such a client has only limited access to \"public\" buckets:\n           listing their contents and downloading their blobs.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: Instance w/ anonymous credentials and no project.\n        \"\"\"\n        client = cls(project=\"<none>\", credentials=AnonymousCredentials())\n        client.project = None\n        return client\n\n    @property\n    def universe_domain(self):\n        return self._universe_domain or _DEFAULT_UNIVERSE_DOMAIN\n\n    @property\n    def api_endpoint(self):\n        return self._connection.API_BASE_URL\n\n    @property\n    def _connection(self):\n        \"\"\"Get connection or batch on the client.\n\n        :rtype: :class:`google.cloud.storage._http.Connection`\n        :returns: The connection set on the client, or the batch\n                  if one is set.\n        \"\"\"\n        if self.current_batch is not None:\n            return self.current_batch\n        else:\n            return self._base_connection\n\n    @_connection.setter\n    def _connection(self, value):\n        \"\"\"Set connection on the client.\n\n        Intended to be used by constructor (since the base class calls)\n            self._connection = connection\n        Will raise if the connection is set more than once.\n\n        :type value: :class:`google.cloud.storage._http.Connection`\n        :param value: The connection set on the client.\n\n        :raises: :class:`ValueError` if connection has already been set.\n        \"\"\"\n        if self._base_connection is not None:\n            raise ValueError(\"Connection already set on client\")\n        self._base_connection = value\n\n    def _push_batch(self, batch):\n        \"\"\"Push a batch onto our stack.\n\n        \"Protected\", intended for use by batch context mgrs.\n\n        :type batch: :class:`google.cloud.storage.batch.Batch`\n        :param batch: newly-active batch\n        \"\"\"\n        self._batch_stack.push(batch)\n\n    def _pop_batch(self):\n        \"\"\"Pop a batch from our stack.\n\n        \"Protected\", intended for use by batch context mgrs.\n\n        :raises: IndexError if the stack is empty.\n        :rtype: :class:`google.cloud.storage.batch.Batch`\n        :returns: the top-most batch/transaction, after removing it.\n        \"\"\"\n        return self._batch_stack.pop()\n\n    @property\n    def current_batch(self):\n        \"\"\"Currently-active batch.\n\n        :rtype: :class:`google.cloud.storage.batch.Batch` or ``NoneType`` (if\n                no batch is active).\n        :returns: The batch at the top of the batch stack.\n        \"\"\"\n        return self._batch_stack.top\n\n    def get_service_account_email(\n        self, project=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"Get the email address of the project's GCS service account\n\n        :type project: str\n        :param project:\n            (Optional) Project ID to use for retreiving GCS service account\n            email address.  Defaults to the client's project.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: str\n        :returns: service account email address\n        \"\"\"\n        if project is None:\n            project = self.project\n\n        path = f\"/projects/{project}/serviceAccount\"\n        api_response = self._get_resource(path, timeout=timeout, retry=retry)\n        return api_response[\"email_address\"]\n\n    def bucket(self, bucket_name, user_project=None):\n        \"\"\"Factory constructor for bucket object.\n\n        .. note::\n          This will not make an HTTP request; it simply instantiates\n          a bucket object owned by this client.\n\n        :type bucket_name: str\n        :param bucket_name: The name of the bucket to be instantiated.\n\n        :type user_project: str\n        :param user_project: (Optional) The project ID to be billed for API\n                             requests made via the bucket.\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket`\n        :returns: The bucket object created.\n        \"\"\"\n        return Bucket(client=self, name=bucket_name, user_project=user_project)\n\n    def batch(self, raise_exception=True):\n        \"\"\"Factory constructor for batch object.\n\n        .. note::\n          This will not make an HTTP request; it simply instantiates\n          a batch object owned by this client.\n\n        :type raise_exception: bool\n        :param raise_exception:\n            (Optional) Defaults to True. If True, instead of adding exceptions\n            to the list of return responses, the final exception will be raised.\n            Note that exceptions are unwrapped after all operations are complete\n            in success or failure, and only the last exception is raised.\n\n        :rtype: :class:`google.cloud.storage.batch.Batch`\n        :returns: The batch object created.\n        \"\"\"\n        return Batch(client=self, raise_exception=raise_exception)\n\n    def _get_resource(\n        self,\n        path,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'GET' calls.\n\n        Args:\n            path str:\n                The path of the resource to fetch.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"GET\",\n            path=path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _list_resource(\n        self,\n        path,\n        item_to_value,\n        page_token=None,\n        max_results=None,\n        extra_params=None,\n        page_start=page_iterator._do_nothing_page_start,\n        page_size=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        api_request = functools.partial(\n            self._connection.api_request, timeout=timeout, retry=retry\n        )\n        return page_iterator.HTTPIterator(\n            client=self,\n            api_request=api_request,\n            path=path,\n            item_to_value=item_to_value,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_start=page_start,\n            page_size=page_size,\n        )\n\n    def _patch_resource(\n        self,\n        path,\n        data,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'PATCH' calls.\n\n        Args:\n            path str:\n                The path of the resource to fetch.\n\n            data dict:\n                The data to be patched.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"PATCH\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _put_resource(\n        self,\n        path,\n        data,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'PUT' calls.\n\n        Args:\n            path str:\n                The path of the resource to fetch.\n\n            data dict:\n                The data to be patched.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"PUT\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _post_resource(\n        self,\n        path,\n        data,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'POST' calls.\n\n        Args:\n            path str:\n                The path of the resource to which to post.\n\n            data dict:\n                The data to be posted.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource returned from the post.\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n\n        return self._connection.api_request(\n            method=\"POST\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _delete_resource(\n        self,\n        path,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'DELETE' calls.\n\n        Args:\n            path str:\n                The path of the resource to delete.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"DELETE\",\n            path=path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _bucket_arg_to_bucket(self, bucket_or_name):\n        \"\"\"Helper to return given bucket or create new by name.\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n\n        Returns:\n            google.cloud.storage.bucket.Bucket\n                The newly created bucket or the given one.\n        \"\"\"\n        if isinstance(bucket_or_name, Bucket):\n            bucket = bucket_or_name\n            if bucket.client is None:\n                bucket._client = self\n        else:\n            bucket = Bucket(self, name=bucket_or_name)\n        return bucket\n\n    def get_bucket(\n        self,\n        bucket_or_name,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Retrieve a bucket via a GET request.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/get) and a [code sample](https://cloud.google.com/storage/docs/samples/storage-get-bucket-metadata#storage_get_bucket_metadata-python).\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            if_metageneration_match (Optional[long]):\n                Make the operation conditional on whether the\n                blob's current metageneration matches the given value.\n\n            if_metageneration_not_match (Optional[long]):\n                Make the operation conditional on whether the blob's\n                current metageneration does not match the given value.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n        Returns:\n            google.cloud.storage.bucket.Bucket\n                The bucket matching the name provided.\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        bucket = self._bucket_arg_to_bucket(bucket_or_name)\n        bucket.reload(\n            client=self,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n        return bucket\n\n    def lookup_bucket(\n        self,\n        bucket_name,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Get a bucket by name, returning None if not found.\n\n        You can use this if you would rather check for a None value\n        than catching a NotFound exception.\n\n        :type bucket_name: str\n        :param bucket_name: The name of the bucket to get.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket` or ``NoneType``\n        :returns: The bucket matching the name provided or None if not found.\n        \"\"\"\n        try:\n            return self.get_bucket(\n                bucket_name,\n                timeout=timeout,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n            )\n        except NotFound:\n            return None\n\n    def create_bucket(\n        self,\n        bucket_or_name,\n        requester_pays=None,\n        project=None,\n        user_project=None,\n        location=None,\n        data_locations=None,\n        predefined_acl=None,\n        predefined_default_object_acl=None,\n        enable_object_retention=False,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Create a new bucket via a POST request.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/insert) and a [code sample](https://cloud.google.com/storage/docs/samples/storage-create-bucket#storage_create_bucket-python).\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n            requester_pays (bool):\n                DEPRECATED. Use Bucket().requester_pays instead.\n                (Optional) Whether requester pays for API requests for\n                this bucket and its blobs.\n            project (str):\n                (Optional) The project under which the bucket is to be created.\n                If not passed, uses the project set on the client.\n            user_project (str):\n                (Optional) The project ID to be billed for API requests\n                made via created bucket.\n            location (str):\n                (Optional) The location of the bucket. If not passed,\n                the default location, US, will be used. If specifying a dual-region,\n                `data_locations` should be set in conjunction. See:\n                https://cloud.google.com/storage/docs/locations\n            data_locations (list of str):\n                (Optional) The list of regional locations of a custom dual-region bucket.\n                Dual-regions require exactly 2 regional locations. See:\n                https://cloud.google.com/storage/docs/locations\n            predefined_acl (str):\n                (Optional) Name of predefined ACL to apply to bucket. See:\n                https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n            predefined_default_object_acl (str):\n                (Optional) Name of predefined ACL to apply to bucket's objects. See:\n                https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n            enable_object_retention (bool):\n                (Optional) Whether object retention should be enabled on this bucket. See:\n                https://cloud.google.com/storage/docs/object-lock\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n        Returns:\n            google.cloud.storage.bucket.Bucket\n                The newly created bucket.\n\n        Raises:\n            google.cloud.exceptions.Conflict\n                If the bucket already exists.\n        \"\"\"\n        bucket = self._bucket_arg_to_bucket(bucket_or_name)\n        query_params = {}\n\n        if project is None:\n            project = self.project\n\n        # Use no project if STORAGE_EMULATOR_HOST is set\n        if self._is_emulator_set:\n            if project is None:\n                project = _get_environ_project()\n            if project is None:\n                project = \"<none>\"\n\n        # Only include the project parameter if a project is set.\n        # If a project is not set, falls back to API validation (BadRequest).\n        if project is not None:\n            query_params = {\"project\": project}\n\n        if requester_pays is not None:\n            warnings.warn(\n                \"requester_pays arg is deprecated. Use Bucket().requester_pays instead.\",\n                PendingDeprecationWarning,\n                stacklevel=1,\n            )\n            bucket.requester_pays = requester_pays\n\n        if predefined_acl is not None:\n            predefined_acl = BucketACL.validate_predefined(predefined_acl)\n            query_params[\"predefinedAcl\"] = predefined_acl\n\n        if predefined_default_object_acl is not None:\n            predefined_default_object_acl = DefaultObjectACL.validate_predefined(\n                predefined_default_object_acl\n            )\n            query_params[\"predefinedDefaultObjectAcl\"] = predefined_default_object_acl\n\n        if user_project is not None:\n            query_params[\"userProject\"] = user_project\n\n        if enable_object_retention:\n            query_params[\"enableObjectRetention\"] = enable_object_retention\n\n        properties = {key: bucket._properties[key] for key in bucket._changes}\n        properties[\"name\"] = bucket.name\n\n        if location is not None:\n            properties[\"location\"] = location\n\n        if data_locations is not None:\n            properties[\"customPlacementConfig\"] = {\"dataLocations\": data_locations}\n\n        api_response = self._post_resource(\n            \"/b\",\n            properties,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=bucket,\n        )\n\n        bucket._set_properties(api_response)\n        return bucket\n\n    def download_blob_to_file(\n        self,\n        blob_or_uri,\n        file_obj,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of a blob object or blob URI into a file-like object.\n\n        See https://cloud.google.com/storage/docs/downloading-objects\n\n        Args:\n            blob_or_uri (Union[ \\\n            :class:`~google.cloud.storage.blob.Blob`, \\\n             str, \\\n            ]):\n                The blob resource to pass or URI to download.\n\n            file_obj (file):\n                A file handle to which to write the blob's data.\n\n            start (int):\n                (Optional) The first byte in a range to be downloaded.\n\n            end (int):\n                (Optional) The last byte in a range to be downloaded.\n\n            raw_download (bool):\n                (Optional) If true, download the object without any expansion.\n\n            if_etag_match (Union[str, Set[str]]):\n                (Optional) See :ref:`using-if-etag-match`\n\n            if_etag_not_match (Union[str, Set[str]]):\n                (Optional) See :ref:`using-if-etag-not-match`\n\n            if_generation_match (long):\n                (Optional) See :ref:`using-if-generation-match`\n\n            if_generation_not_match (long):\n                (Optional) See :ref:`using-if-generation-not-match`\n\n            if_metageneration_match (long):\n                (Optional) See :ref:`using-if-metageneration-match`\n\n            if_metageneration_not_match (long):\n                (Optional) See :ref:`using-if-metageneration-not-match`\n\n            timeout ([Union[float, Tuple[float, float]]]):\n                (Optional) The amount of time, in seconds, to wait\n                for the server response.  See: :ref:`configuring_timeouts`\n\n            checksum (str):\n                (Optional) The type of checksum to compute to verify the integrity\n                of the object. The response headers must contain a checksum of the\n                requested type. If the headers lack an appropriate checksum (for\n                instance in the case of transcoded or ranged downloads where the\n                remote service does not know the correct checksum, including\n                downloads where chunk_size is set) an INFO-level log will be\n                emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n                is \"md5\".\n            retry (google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy)\n                (Optional) How to retry the RPC. A None value will disable\n                retries. A google.api_core.retry.Retry value will enable retries,\n                and the object will define retriable response codes and errors and\n                configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n                Retry object and activates it only if certain conditions are met.\n                This class exists to provide safe defaults for RPC calls that are\n                not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a\n                condition such as if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package\n                (google.cloud.storage.retry) for information on retry types and how\n                to configure them.\n\n                Media operations (downloads and uploads) do not support non-default\n                predicates in a Retry object. The default will always be used. Other\n                configuration changes for Retry objects such as delays and deadlines\n                are respected.\n        \"\"\"\n\n        if not isinstance(blob_or_uri, Blob):\n            blob_or_uri = Blob.from_string(blob_or_uri)\n\n        blob_or_uri._prep_and_do_download(\n            file_obj,\n            client=self,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def list_blobs(\n        self,\n        bucket_or_name,\n        max_results=None,\n        page_token=None,\n        prefix=None,\n        delimiter=None,\n        start_offset=None,\n        end_offset=None,\n        include_trailing_delimiter=None,\n        versions=None,\n        projection=\"noAcl\",\n        fields=None,\n        page_size=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        match_glob=None,\n        include_folders_as_prefixes=None,\n        soft_deleted=None,\n    ):\n        \"\"\"Return an iterator used to find blobs in the bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        .. note::\n          List prefixes (directories) in a bucket using a prefix and delimiter.\n          See a [code sample](https://cloud.google.com/storage/docs/samples/storage-list-files-with-prefix#storage_list_files_with_prefix-python)\n          listing objects using a prefix filter.\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n\n            max_results (int):\n                (Optional) The maximum number of blobs to return.\n\n            page_token (str):\n                (Optional) If present, return the next batch of blobs, using the\n                value, which must correspond to the ``nextPageToken`` value\n                returned in the previous response.  Deprecated: use the ``pages``\n                property of the returned iterator instead of manually passing the\n                token.\n\n            prefix (str):\n                (Optional) Prefix used to filter blobs.\n\n            delimiter (str):\n                (Optional) Delimiter, used with ``prefix`` to\n                emulate hierarchy.\n\n            start_offset (str):\n                (Optional) Filter results to objects whose names are\n                lexicographically equal to or after ``startOffset``. If\n                ``endOffset`` is also set, the objects listed will have names\n                between ``startOffset`` (inclusive) and ``endOffset``\n                (exclusive).\n\n            end_offset (str):\n                (Optional) Filter results to objects whose names are\n                lexicographically before ``endOffset``. If ``startOffset`` is\n                also set, the objects listed will have names between\n                ``startOffset`` (inclusive) and ``endOffset`` (exclusive).\n\n            include_trailing_delimiter (boolean):\n                (Optional) If true, objects that end in exactly one instance of\n                ``delimiter`` will have their metadata included in ``items`` in\n                addition to ``prefixes``.\n\n            versions (bool):\n                (Optional) Whether object versions should be returned\n                as separate blobs.\n\n            projection (str):\n                (Optional) If used, must be 'full' or 'noAcl'.\n                Defaults to ``'noAcl'``. Specifies the set of\n                properties to return.\n\n            fields (str):\n                (Optional) Selector specifying which fields to include\n                in a partial response. Must be a list of fields. For\n                example to get a partial response with just the next\n                page token and the name and language of each blob returned:\n                ``'items(name,contentLanguage),nextPageToken'``.\n                See: https://cloud.google.com/storage/docs/json_api/v1/parameters#fields\n\n            page_size (int):\n                (Optional) Maximum number of blobs to return in each page.\n                Defaults to a value set by the API.\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            match_glob (str):\n                (Optional) A glob pattern used to filter results (for example, foo*bar).\n                The string value must be UTF-8 encoded. See:\n                https://cloud.google.com/storage/docs/json_api/v1/objects/list#list-object-glob\n\n            include_folders_as_prefixes (bool):\n                (Optional) If true, includes Folders and Managed Folders in the set of\n                ``prefixes`` returned by the query. Only applicable if ``delimiter`` is set to /.\n                See: https://cloud.google.com/storage/docs/managed-folders\n\n            soft_deleted (bool):\n                (Optional) If true, only soft-deleted objects will be listed as distinct results in order of increasing\n                generation number. This parameter can only be used successfully if the bucket has a soft delete policy.\n                Note ``soft_deleted`` and ``versions`` cannot be set to True simultaneously. See:\n                https://cloud.google.com/storage/docs/soft-delete\n\n        Returns:\n            Iterator of all :class:`~google.cloud.storage.blob.Blob`\n            in this bucket matching the arguments. The RPC call\n            returns a response when the iterator is consumed.\n\n            As part of the response, you'll also get back an iterator.prefixes entity that lists object names\n            up to and including the requested delimiter. Duplicate entries are omitted from this list.\n        \"\"\"\n        bucket = self._bucket_arg_to_bucket(bucket_or_name)\n\n        extra_params = {\"projection\": projection}\n\n        if prefix is not None:\n            extra_params[\"prefix\"] = prefix\n\n        if delimiter is not None:\n            extra_params[\"delimiter\"] = delimiter\n\n        if match_glob is not None:\n            extra_params[\"matchGlob\"] = match_glob\n\n        if start_offset is not None:\n            extra_params[\"startOffset\"] = start_offset\n\n        if end_offset is not None:\n            extra_params[\"endOffset\"] = end_offset\n\n        if include_trailing_delimiter is not None:\n            extra_params[\"includeTrailingDelimiter\"] = include_trailing_delimiter\n\n        if versions is not None:\n            extra_params[\"versions\"] = versions\n\n        if fields is not None:\n            extra_params[\"fields\"] = fields\n\n        if include_folders_as_prefixes is not None:\n            extra_params[\"includeFoldersAsPrefixes\"] = include_folders_as_prefixes\n\n        if soft_deleted is not None:\n            extra_params[\"softDeleted\"] = soft_deleted\n\n        if bucket.user_project is not None:\n            extra_params[\"userProject\"] = bucket.user_project\n\n        path = bucket.path + \"/o\"\n        iterator = self._list_resource(\n            path,\n            _item_to_blob,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_start=_blobs_page_start,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n        iterator.bucket = bucket\n        iterator.prefixes = set()\n        return iterator\n\n    def list_buckets(\n        self,\n        max_results=None,\n        page_token=None,\n        prefix=None,\n        projection=\"noAcl\",\n        fields=None,\n        project=None,\n        page_size=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Get all buckets in the project associated to the client.\n\n        This will not populate the list of blobs available in each\n        bucket.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/list) and a [code sample](https://cloud.google.com/storage/docs/samples/storage-list-buckets#storage_list_buckets-python).\n\n        :type max_results: int\n        :param max_results: (Optional) The maximum number of buckets to return.\n\n        :type page_token: str\n        :param page_token:\n            (Optional) If present, return the next batch of buckets, using the\n            value, which must correspond to the ``nextPageToken`` value\n            returned in the previous response.  Deprecated: use the ``pages``\n            property of the returned iterator instead of manually passing the\n            token.\n\n        :type prefix: str\n        :param prefix: (Optional) Filter results to buckets whose names begin\n                       with this prefix.\n\n        :type projection: str\n        :param projection:\n            (Optional) Specifies the set of properties to return. If used, must\n            be 'full' or 'noAcl'. Defaults to 'noAcl'.\n\n        :type fields: str\n        :param fields:\n            (Optional) Selector specifying which fields to include in a partial\n            response. Must be a list of fields. For example to get a partial\n            response with just the next page token and the language of each\n            bucket returned: 'items/id,nextPageToken'\n\n        :type project: str\n        :param project: (Optional) The project whose buckets are to be listed.\n                        If not passed, uses the project set on the client.\n\n        :type page_size: int\n        :param page_size: (Optional) Maximum number of buckets to return in each page.\n            Defaults to a value set by the API.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :raises ValueError: if both ``project`` is ``None`` and the client's\n                            project is also ``None``.\n        :returns: Iterator of all :class:`~google.cloud.storage.bucket.Bucket`\n                  belonging to this project.\n        \"\"\"\n        extra_params = {}\n\n        if project is None:\n            project = self.project\n\n        # Use no project if STORAGE_EMULATOR_HOST is set\n        if self._is_emulator_set:\n            if project is None:\n                project = _get_environ_project()\n            if project is None:\n                project = \"<none>\"\n\n        # Only include the project parameter if a project is set.\n        # If a project is not set, falls back to API validation (BadRequest).\n        if project is not None:\n            extra_params = {\"project\": project}\n\n        if prefix is not None:\n            extra_params[\"prefix\"] = prefix\n\n        extra_params[\"projection\"] = projection\n\n        if fields is not None:\n            extra_params[\"fields\"] = fields\n\n        return self._list_resource(\n            \"/b\",\n            _item_to_bucket,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def create_hmac_key(\n        self,\n        service_account_email,\n        project_id=None,\n        user_project=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n    ):\n        \"\"\"Create an HMAC key for a service account.\n\n        :type service_account_email: str\n        :param service_account_email: e-mail address of the service account\n\n        :type project_id: str\n        :param project_id: (Optional) Explicit project ID for the key.\n            Defaults to the client's project.\n\n        :type user_project: str\n        :param user_project: (Optional) This parameter is currently ignored.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable retries.\n            A google.api_core.retry.Retry value will enable retries, and the object will\n            define retriable response codes and errors and configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n            activates it only if certain conditions are met. This class exists to provide safe defaults\n            for RPC calls that are not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a condition such as\n            if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n            information on retry types and how to configure them.\n\n        :rtype:\n            Tuple[:class:`~google.cloud.storage.hmac_key.HMACKeyMetadata`, str]\n        :returns: metadata for the created key, plus the bytes of the key's secret, which is an 40-character base64-encoded string.\n        \"\"\"\n        if project_id is None:\n            project_id = self.project\n\n        path = f\"/projects/{project_id}/hmacKeys\"\n        qs_params = {\"serviceAccountEmail\": service_account_email}\n\n        if user_project is not None:\n            qs_params[\"userProject\"] = user_project\n\n        api_response = self._post_resource(\n            path,\n            None,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        metadata = HMACKeyMetadata(self)\n        metadata._properties = api_response[\"metadata\"]\n        secret = api_response[\"secret\"]\n        return metadata, secret\n\n    def list_hmac_keys(\n        self,\n        max_results=None,\n        service_account_email=None,\n        show_deleted_keys=None,\n        project_id=None,\n        user_project=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"List HMAC keys for a project.\n\n        :type max_results: int\n        :param max_results:\n            (Optional) Max number of keys to return in a given page.\n\n        :type service_account_email: str\n        :param service_account_email:\n            (Optional) Limit keys to those created by the given service account.\n\n        :type show_deleted_keys: bool\n        :param show_deleted_keys:\n            (Optional) Included deleted keys in the list. Default is to\n            exclude them.\n\n        :type project_id: str\n        :param project_id: (Optional) Explicit project ID for the key.\n            Defaults to the client's project.\n\n        :type user_project: str\n        :param user_project: (Optional) This parameter is currently ignored.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype:\n            Tuple[:class:`~google.cloud.storage.hmac_key.HMACKeyMetadata`, str]\n        :returns: metadata for the created key, plus the bytes of the key's secret, which is an 40-character base64-encoded string.\n        \"\"\"\n        if project_id is None:\n            project_id = self.project\n\n        path = f\"/projects/{project_id}/hmacKeys\"\n        extra_params = {}\n\n        if service_account_email is not None:\n            extra_params[\"serviceAccountEmail\"] = service_account_email\n\n        if show_deleted_keys is not None:\n            extra_params[\"showDeletedKeys\"] = show_deleted_keys\n\n        if user_project is not None:\n            extra_params[\"userProject\"] = user_project\n\n        return self._list_resource(\n            path,\n            _item_to_hmac_key_metadata,\n            max_results=max_results,\n            extra_params=extra_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def get_hmac_key_metadata(\n        self, access_id, project_id=None, user_project=None, timeout=_DEFAULT_TIMEOUT\n    ):\n        \"\"\"Return a metadata instance for the given HMAC key.\n\n        :type access_id: str\n        :param access_id: Unique ID of an existing key.\n\n        :type project_id: str\n        :param project_id: (Optional) Project ID of an existing key.\n            Defaults to client's project.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type user_project: str\n        :param user_project: (Optional) This parameter is currently ignored.\n        \"\"\"\n        metadata = HMACKeyMetadata(self, access_id, project_id, user_project)\n        metadata.reload(timeout=timeout)  # raises NotFound for missing key\n        return metadata\n\n    def generate_signed_post_policy_v4(\n        self,\n        bucket_name,\n        blob_name,\n        expiration,\n        conditions=None,\n        fields=None,\n        credentials=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n        service_account_email=None,\n        access_token=None,\n    ):\n        \"\"\"Generate a V4 signed policy object. Generated policy object allows user to upload objects with a POST request.\n\n        .. note::\n\n            Assumes ``credentials`` implements the\n            :class:`google.auth.credentials.Signing` interface. Also assumes\n            ``credentials`` has a ``service_account_email`` property which\n            identifies the credentials.\n\n        See a [code sample](https://github.com/googleapis/python-storage/blob/main/samples/snippets/storage_generate_signed_post_policy_v4.py).\n\n        :type bucket_name: str\n        :param bucket_name: Bucket name.\n\n        :type blob_name: str\n        :param blob_name: Object name.\n\n        :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n        :param expiration: Policy expiration time. If a ``datetime`` instance is\n                           passed without an explicit ``tzinfo`` set,  it will be\n                           assumed to be ``UTC``.\n\n        :type conditions: list\n        :param conditions: (Optional) List of POST policy conditions, which are\n                           used to restrict what is allowed in the request.\n\n        :type fields: dict\n        :param fields: (Optional) Additional elements to include into request.\n\n        :type credentials: :class:`google.auth.credentials.Signing`\n        :param credentials: (Optional) Credentials object with an associated private\n                            key to sign text.\n\n        :type virtual_hosted_style: bool\n        :param virtual_hosted_style:\n            (Optional) If True, construct the URL relative to the bucket\n            virtual hostname, e.g., '<bucket-name>.storage.googleapis.com'.\n            Incompatible with bucket_bound_hostname.\n\n        :type bucket_bound_hostname: str\n        :param bucket_bound_hostname:\n            (Optional) If passed, construct the URL relative to the bucket-bound hostname.\n            Value can be bare or with a scheme, e.g., 'example.com' or 'http://example.com'.\n            Incompatible with virtual_hosted_style.\n            See: https://cloud.google.com/storage/docs/request-endpoints#cname\n\n        :type scheme: str\n        :param scheme:\n            (Optional) If ``bucket_bound_hostname`` is passed as a bare hostname, use\n            this value as a scheme. ``https`` will work only when using a CDN.\n            Defaults to ``\"http\"``.\n\n        :type service_account_email: str\n        :param service_account_email: (Optional) E-mail address of the service account.\n\n        :type access_token: str\n        :param access_token: (Optional) Access token for a service account.\n\n        :raises: :exc:`ValueError` when mutually exclusive arguments are used.\n\n        :rtype: dict\n        :returns: Signed POST policy.\n        \"\"\"\n        if virtual_hosted_style and bucket_bound_hostname:\n            raise ValueError(\n                \"Only one of virtual_hosted_style and bucket_bound_hostname \"\n                \"can be specified.\"\n            )\n\n        credentials = self._credentials if credentials is None else credentials\n        ensure_signed_credentials(credentials)\n\n        # prepare policy conditions and fields\n        timestamp, datestamp = get_v4_now_dtstamps()\n\n        x_goog_credential = \"{email}/{datestamp}/auto/storage/goog4_request\".format(\n            email=credentials.signer_email, datestamp=datestamp\n        )\n        required_conditions = [\n            {\"bucket\": bucket_name},\n            {\"key\": blob_name},\n            {\"x-goog-date\": timestamp},\n            {\"x-goog-credential\": x_goog_credential},\n            {\"x-goog-algorithm\": \"GOOG4-RSA-SHA256\"},\n        ]\n\n        conditions = conditions or []\n        policy_fields = {}\n        for key, value in sorted((fields or {}).items()):\n            if not key.startswith(\"x-ignore-\"):\n                policy_fields[key] = value\n                conditions.append({key: value})\n\n        conditions += required_conditions\n\n        # calculate policy expiration time\n        now = _NOW(_UTC).replace(tzinfo=None)\n        if expiration is None:\n            expiration = now + datetime.timedelta(hours=1)\n\n        policy_expires = now + datetime.timedelta(\n            seconds=get_expiration_seconds_v4(expiration)\n        )\n\n        # encode policy for signing\n        policy = json.dumps(\n            collections.OrderedDict(\n                sorted(\n                    {\n                        \"conditions\": conditions,\n                        \"expiration\": policy_expires.isoformat() + \"Z\",\n                    }.items()\n                )\n            ),\n            separators=(\",\", \":\"),\n        )\n        str_to_sign = base64.b64encode(policy.encode(\"utf-8\"))\n\n        # sign the policy and get its cryptographic signature\n        if access_token and service_account_email:\n            signature = _sign_message(str_to_sign, access_token, service_account_email)\n            signature_bytes = base64.b64decode(signature)\n        else:\n            signature_bytes = credentials.sign_bytes(str_to_sign)\n\n        # get hexadecimal representation of the signature\n        signature = binascii.hexlify(signature_bytes).decode(\"utf-8\")\n\n        policy_fields.update(\n            {\n                \"key\": blob_name,\n                \"x-goog-algorithm\": \"GOOG4-RSA-SHA256\",\n                \"x-goog-credential\": x_goog_credential,\n                \"x-goog-date\": timestamp,\n                \"x-goog-signature\": signature,\n                \"policy\": str_to_sign.decode(\"utf-8\"),\n            }\n        )\n        # designate URL\n        if virtual_hosted_style:\n            url = _virtual_hosted_style_base_url(\n                self.api_endpoint, bucket_name, trailing_slash=True\n            )\n        elif bucket_bound_hostname:\n            url = f\"{_bucket_bound_hostname_url(bucket_bound_hostname, scheme)}/\"\n        else:\n            url = f\"{self.api_endpoint}/{bucket_name}/\"\n\n        return {\"url\": url, \"fields\": policy_fields}\n\n\ndef _item_to_bucket(iterator, item):\n    \"\"\"Convert a JSON bucket to the native object.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a bucket.\n\n    :rtype: :class:`.Bucket`\n    :returns: The next bucket in the page.\n    \"\"\"\n    name = item.get(\"name\")\n    bucket = Bucket(iterator.client, name)\n    bucket._set_properties(item)\n    return bucket\n\n\ndef _item_to_hmac_key_metadata(iterator, item):\n    \"\"\"Convert a JSON key metadata resource to the native object.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a key metadata instance.\n\n    :rtype: :class:`~google.cloud.storage.hmac_key.HMACKeyMetadata`\n    :returns: The next key metadata instance in the page.\n    \"\"\"\n    metadata = HMACKeyMetadata(iterator.client)\n    metadata._properties = item\n    return metadata\n", "google/cloud/storage/batch.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Batch updates / deletes of storage buckets / blobs.\n\nA batch request is a single standard HTTP request containing multiple Cloud Storage JSON API calls.\nWithin this main HTTP request, there are multiple parts which each contain a nested HTTP request.\nThe body of each part is itself a complete HTTP request, with its own verb, URL, headers, and body.\n\nNote that Cloud Storage does not support batch operations for uploading or downloading.\nAdditionally, the current batch design does not support library methods whose return values\ndepend on the response payload. See more details in the [Sending Batch Requests official guide](https://cloud.google.com/storage/docs/batch).\n\nExamples of situations when you might want to use the Batch module:\n``blob.patch()``\n``blob.update()``\n``blob.delete()``\n``bucket.delete_blob()``\n``bucket.patch()``\n``bucket.update()``\n\"\"\"\nfrom email.encoders import encode_noop\nfrom email.generator import Generator\nfrom email.mime.application import MIMEApplication\nfrom email.mime.multipart import MIMEMultipart\nfrom email.parser import Parser\nimport io\nimport json\n\nimport requests\n\nfrom google.cloud import _helpers\nfrom google.cloud import exceptions\nfrom google.cloud.storage._http import Connection\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n\nclass MIMEApplicationHTTP(MIMEApplication):\n    \"\"\"MIME type for ``application/http``.\n\n    Constructs payload from headers and body\n\n    :type method: str\n    :param method: HTTP method\n\n    :type uri: str\n    :param uri: URI for HTTP request\n\n    :type headers:  dict\n    :param headers: HTTP headers\n\n    :type body: str\n    :param body: (Optional) HTTP payload\n\n    \"\"\"\n\n    def __init__(self, method, uri, headers, body):\n        if isinstance(body, dict):\n            body = json.dumps(body)\n            headers[\"Content-Type\"] = \"application/json\"\n            headers[\"Content-Length\"] = len(body)\n        if body is None:\n            body = \"\"\n        lines = [f\"{method} {uri} HTTP/1.1\"]\n        lines.extend([f\"{key}: {value}\" for key, value in sorted(headers.items())])\n        lines.append(\"\")\n        lines.append(body)\n        payload = \"\\r\\n\".join(lines)\n        super().__init__(payload, \"http\", encode_noop)\n\n\nclass _FutureDict(object):\n    \"\"\"Class to hold a future value for a deferred request.\n\n    Used by for requests that get sent in a :class:`Batch`.\n    \"\"\"\n\n    @staticmethod\n    def get(key, default=None):\n        \"\"\"Stand-in for dict.get.\n\n        :type key: object\n        :param key: Hashable dictionary key.\n\n        :type default: object\n        :param default: Fallback value to dict.get.\n\n        :raises: :class:`KeyError` always since the future is intended to fail\n                 as a dictionary.\n        \"\"\"\n        raise KeyError(f\"Cannot get({key!r}, default={default!r}) on a future\")\n\n    def __getitem__(self, key):\n        \"\"\"Stand-in for dict[key].\n\n        :type key: object\n        :param key: Hashable dictionary key.\n\n        :raises: :class:`KeyError` always since the future is intended to fail\n                 as a dictionary.\n        \"\"\"\n        raise KeyError(f\"Cannot get item {key!r} from a future\")\n\n    def __setitem__(self, key, value):\n        \"\"\"Stand-in for dict[key] = value.\n\n        :type key: object\n        :param key: Hashable dictionary key.\n\n        :type value: object\n        :param value: Dictionary value.\n\n        :raises: :class:`KeyError` always since the future is intended to fail\n                 as a dictionary.\n        \"\"\"\n        raise KeyError(f\"Cannot set {key!r} -> {value!r} on a future\")\n\n\nclass _FutureResponse(requests.Response):\n    \"\"\"Reponse that returns a placeholder dictionary for a batched requests.\"\"\"\n\n    def __init__(self, future_dict):\n        super(_FutureResponse, self).__init__()\n        self._future_dict = future_dict\n        self.status_code = 204\n\n    def json(self):\n        return self._future_dict\n\n    @property\n    def content(self):\n        return self._future_dict\n\n\nclass Batch(Connection):\n    \"\"\"Proxy an underlying connection, batching up change operations.\n\n    .. warning::\n\n        Cloud Storage does not support batch operations for uploading or downloading.\n        Additionally, the current batch design does not support library methods whose\n        return values depend on the response payload.\n\n    :type client: :class:`google.cloud.storage.client.Client`\n    :param client: The client to use for making connections.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        (Optional) Defaults to True. If True, instead of adding exceptions\n        to the list of return responses, the final exception will be raised.\n        Note that exceptions are unwrapped after all operations are complete\n        in success or failure, and only the last exception is raised.\n    \"\"\"\n\n    _MAX_BATCH_SIZE = 1000\n\n    def __init__(self, client, raise_exception=True):\n        api_endpoint = client._connection.API_BASE_URL\n        client_info = client._connection._client_info\n        super(Batch, self).__init__(\n            client, client_info=client_info, api_endpoint=api_endpoint\n        )\n        self._requests = []\n        self._target_objects = []\n        self._responses = []\n        self._raise_exception = raise_exception\n\n    def _do_request(\n        self, method, url, headers, data, target_object, timeout=_DEFAULT_TIMEOUT\n    ):\n        \"\"\"Override Connection:  defer actual HTTP request.\n\n        Only allow up to ``_MAX_BATCH_SIZE`` requests to be deferred.\n\n        :type method: str\n        :param method: The HTTP method to use in the request.\n\n        :type url: str\n        :param url: The URL to send the request to.\n\n        :type headers: dict\n        :param headers: A dictionary of HTTP headers to send with the request.\n\n        :type data: str\n        :param data: The data to send as the body of the request.\n\n        :type target_object: object\n        :param target_object:\n            (Optional) This allows us to enable custom behavior in our batch\n            connection. Here we defer an HTTP request and complete\n            initialization of the object at a later time.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :rtype: tuple of ``response`` (a dictionary of sorts)\n                and ``content`` (a string).\n        :returns: The HTTP response object and the content of the response.\n        \"\"\"\n        if len(self._requests) >= self._MAX_BATCH_SIZE:\n            raise ValueError(\n                \"Too many deferred requests (max %d)\" % self._MAX_BATCH_SIZE\n            )\n        self._requests.append((method, url, headers, data, timeout))\n        result = _FutureDict()\n        self._target_objects.append(target_object)\n        if target_object is not None:\n            target_object._properties = result\n        return _FutureResponse(result)\n\n    def _prepare_batch_request(self):\n        \"\"\"Prepares headers and body for a batch request.\n\n        :rtype: tuple (dict, str)\n        :returns: The pair of headers and body of the batch request to be sent.\n        :raises: :class:`ValueError` if no requests have been deferred.\n        \"\"\"\n        if len(self._requests) == 0:\n            raise ValueError(\"No deferred requests\")\n\n        multi = MIMEMultipart()\n\n        # Use timeout of last request, default to _DEFAULT_TIMEOUT\n        timeout = _DEFAULT_TIMEOUT\n        for method, uri, headers, body, _timeout in self._requests:\n            subrequest = MIMEApplicationHTTP(method, uri, headers, body)\n            multi.attach(subrequest)\n            timeout = _timeout\n\n        buf = io.StringIO()\n        generator = Generator(buf, False, 0)\n        generator.flatten(multi)\n        payload = buf.getvalue()\n\n        # Strip off redundant header text\n        _, body = payload.split(\"\\n\\n\", 1)\n        return dict(multi._headers), body, timeout\n\n    def _finish_futures(self, responses, raise_exception=True):\n        \"\"\"Apply all the batch responses to the futures created.\n\n        :type responses: list of (headers, payload) tuples.\n        :param responses: List of headers and payloads from each response in\n                          the batch.\n\n        :type raise_exception: bool\n        :param raise_exception:\n            (Optional) Defaults to True. If True, instead of adding exceptions\n            to the list of return responses, the final exception will be raised.\n            Note that exceptions are unwrapped after all operations are complete\n            in success or failure, and only the last exception is raised.\n\n        :raises: :class:`ValueError` if no requests have been deferred.\n        \"\"\"\n        # If a bad status occurs, we track it, but don't raise an exception\n        # until all futures have been populated.\n        # If raise_exception=False, we add exceptions to the list of responses.\n        exception_args = None\n\n        if len(self._target_objects) != len(responses):  # pragma: NO COVER\n            raise ValueError(\"Expected a response for every request.\")\n\n        for target_object, subresponse in zip(self._target_objects, responses):\n            # For backwards compatibility, only the final exception will be raised.\n            # Set raise_exception=False to include all exceptions to the list of return responses.\n            if not 200 <= subresponse.status_code < 300 and raise_exception:\n                exception_args = exception_args or subresponse\n            elif target_object is not None:\n                try:\n                    target_object._properties = subresponse.json()\n                except ValueError:\n                    target_object._properties = subresponse.content\n\n        if exception_args is not None:\n            raise exceptions.from_http_response(exception_args)\n\n    def finish(self, raise_exception=True):\n        \"\"\"Submit a single `multipart/mixed` request with deferred requests.\n\n        :type raise_exception: bool\n        :param raise_exception:\n            (Optional) Defaults to True. If True, instead of adding exceptions\n            to the list of return responses, the final exception will be raised.\n            Note that exceptions are unwrapped after all operations are complete\n            in success or failure, and only the last exception is raised.\n\n        :rtype: list of tuples\n        :returns: one ``(headers, payload)`` tuple per deferred request.\n        \"\"\"\n        headers, body, timeout = self._prepare_batch_request()\n\n        url = f\"{self.API_BASE_URL}/batch/storage/v1\"\n\n        # Use the private ``_base_connection`` rather than the property\n        # ``_connection``, since the property may be this\n        # current batch.\n        response = self._client._base_connection._make_request(\n            \"POST\", url, data=body, headers=headers, timeout=timeout\n        )\n\n        # Raise exception if the top-level batch request fails\n        if not 200 <= response.status_code < 300:\n            raise exceptions.from_http_response(response)\n\n        responses = list(_unpack_batch_response(response))\n        self._finish_futures(responses, raise_exception=raise_exception)\n        self._responses = responses\n        return responses\n\n    def current(self):\n        \"\"\"Return the topmost batch, or None.\"\"\"\n        return self._client.current_batch\n\n    def __enter__(self):\n        self._client._push_batch(self)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        try:\n            if exc_type is None:\n                self.finish(raise_exception=self._raise_exception)\n        finally:\n            self._client._pop_batch()\n\n\ndef _generate_faux_mime_message(parser, response):\n    \"\"\"Convert response, content -> (multipart) email.message.\n\n    Helper for _unpack_batch_response.\n    \"\"\"\n    # We coerce to bytes to get consistent concat across\n    # Py2 and Py3. Percent formatting is insufficient since\n    # it includes the b in Py3.\n    content_type = _helpers._to_bytes(response.headers.get(\"content-type\", \"\"))\n\n    faux_message = b\"\".join(\n        [b\"Content-Type: \", content_type, b\"\\nMIME-Version: 1.0\\n\\n\", response.content]\n    )\n\n    return parser.parsestr(faux_message.decode(\"utf-8\"))\n\n\ndef _unpack_batch_response(response):\n    \"\"\"Convert requests.Response -> [(headers, payload)].\n\n    Creates a generator of tuples of emulating the responses to\n    :meth:`requests.Session.request`.\n\n    :type response: :class:`requests.Response`\n    :param response: HTTP response / headers from a request.\n    \"\"\"\n    parser = Parser()\n    message = _generate_faux_mime_message(parser, response)\n\n    if not isinstance(message._payload, list):  # pragma: NO COVER\n        raise ValueError(\"Bad response:  not multi-part\")\n\n    for subrequest in message._payload:\n        status_line, rest = subrequest._payload.split(\"\\n\", 1)\n        _, status, _ = status_line.split(\" \", 2)\n        sub_message = parser.parsestr(rest)\n        payload = sub_message._payload\n        msg_headers = dict(sub_message._headers)\n        content_id = msg_headers.get(\"Content-ID\")\n\n        subresponse = requests.Response()\n        subresponse.request = requests.Request(\n            method=\"BATCH\", url=f\"contentid://{content_id}\"\n        ).prepare()\n        subresponse.status_code = int(status)\n        subresponse.headers.update(msg_headers)\n        subresponse._content = payload.encode(\"utf-8\")\n\n        yield subresponse\n", "google/cloud/storage/__init__.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Shortcut methods for getting set up with Google Cloud Storage.\n\nYou'll typically use these to get started with the API:\n\n.. literalinclude:: snippets.py\n    :start-after: START storage_get_started\n    :end-before: END storage_get_started\n    :dedent: 4\n\nThe main concepts with this API are:\n\n- :class:`~google.cloud.storage.bucket.Bucket` which represents a particular\n  bucket (akin to a mounted disk on a computer).\n\n- :class:`~google.cloud.storage.blob.Blob` which represents a pointer to a\n  particular entity in Cloud Storage (akin to a file path on a remote\n  machine).\n\"\"\"\n\nfrom google.cloud.storage.version import __version__\nfrom google.cloud.storage.batch import Batch\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.bucket import Bucket\nfrom google.cloud.storage.client import Client\n\n\n__all__ = [\"__version__\", \"Batch\", \"Blob\", \"Bucket\", \"Client\"]\n", "google/cloud/storage/_helpers.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helper functions for Cloud Storage utility classes.\n\nThese are *not* part of the API.\n\"\"\"\n\nimport base64\nimport datetime\nfrom hashlib import md5\nimport os\nfrom urllib.parse import urlsplit\nfrom urllib.parse import urlunsplit\nfrom uuid import uuid4\n\nfrom google import resumable_media\nfrom google.auth import environment_vars\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\n\nSTORAGE_EMULATOR_ENV_VAR = \"STORAGE_EMULATOR_HOST\"  # Despite name, includes scheme.\n\"\"\"Environment variable defining host for Storage emulator.\"\"\"\n\n_API_ENDPOINT_OVERRIDE_ENV_VAR = \"API_ENDPOINT_OVERRIDE\"  # Includes scheme.\n\"\"\"This is an experimental configuration variable. Use api_endpoint instead.\"\"\"\n\n_API_VERSION_OVERRIDE_ENV_VAR = \"API_VERSION_OVERRIDE\"\n\"\"\"This is an experimental configuration variable used for internal testing.\"\"\"\n\n_DEFAULT_UNIVERSE_DOMAIN = \"googleapis.com\"\n\n_STORAGE_HOST_TEMPLATE = \"storage.{universe_domain}\"\n\n_TRUE_DEFAULT_STORAGE_HOST = _STORAGE_HOST_TEMPLATE.format(\n    universe_domain=_DEFAULT_UNIVERSE_DOMAIN\n)\n\n_DEFAULT_SCHEME = \"https://\"\n\n_API_VERSION = os.getenv(_API_VERSION_OVERRIDE_ENV_VAR, \"v1\")\n\"\"\"API version of the default storage host\"\"\"\n\n# etag match parameters in snake case and equivalent header\n_ETAG_MATCH_PARAMETERS = (\n    (\"if_etag_match\", \"If-Match\"),\n    (\"if_etag_not_match\", \"If-None-Match\"),\n)\n\n# generation match parameters in camel and snake cases\n_GENERATION_MATCH_PARAMETERS = (\n    (\"if_generation_match\", \"ifGenerationMatch\"),\n    (\"if_generation_not_match\", \"ifGenerationNotMatch\"),\n    (\"if_metageneration_match\", \"ifMetagenerationMatch\"),\n    (\"if_metageneration_not_match\", \"ifMetagenerationNotMatch\"),\n    (\"if_source_generation_match\", \"ifSourceGenerationMatch\"),\n    (\"if_source_generation_not_match\", \"ifSourceGenerationNotMatch\"),\n    (\"if_source_metageneration_match\", \"ifSourceMetagenerationMatch\"),\n    (\"if_source_metageneration_not_match\", \"ifSourceMetagenerationNotMatch\"),\n)\n\n_NUM_RETRIES_MESSAGE = (\n    \"`num_retries` has been deprecated and will be removed in a future \"\n    \"release. Use the `retry` argument with a Retry or ConditionalRetryPolicy \"\n    \"object, or None, instead.\"\n)\n\n# _NOW() returns the current local date and time.\n# It is preferred to use timezone-aware datetimes _NOW(_UTC),\n# which returns the current UTC date and time.\n_NOW = datetime.datetime.now\n_UTC = datetime.timezone.utc\n\n\ndef _get_storage_emulator_override():\n    return os.environ.get(STORAGE_EMULATOR_ENV_VAR, None)\n\n\ndef _get_default_storage_base_url():\n    return os.getenv(\n        _API_ENDPOINT_OVERRIDE_ENV_VAR, _DEFAULT_SCHEME + _TRUE_DEFAULT_STORAGE_HOST\n    )\n\n\ndef _get_api_endpoint_override():\n    \"\"\"This is an experimental configuration variable. Use api_endpoint instead.\"\"\"\n    if _get_default_storage_base_url() != _DEFAULT_SCHEME + _TRUE_DEFAULT_STORAGE_HOST:\n        return _get_default_storage_base_url()\n    return None\n\n\ndef _virtual_hosted_style_base_url(url, bucket, trailing_slash=False):\n    \"\"\"Returns the scheme and netloc sections of the url, with the bucket\n    prepended to the netloc.\n\n    Not intended for use with netlocs which include a username and password.\n    \"\"\"\n    parsed_url = urlsplit(url)\n    new_netloc = f\"{bucket}.{parsed_url.netloc}\"\n    base_url = urlunsplit(\n        (parsed_url.scheme, new_netloc, \"/\" if trailing_slash else \"\", \"\", \"\")\n    )\n    return base_url\n\n\ndef _use_client_cert():\n    return os.getenv(\"GOOGLE_API_USE_CLIENT_CERTIFICATE\") == \"true\"\n\n\ndef _get_environ_project():\n    return os.getenv(\n        environment_vars.PROJECT,\n        os.getenv(environment_vars.LEGACY_PROJECT),\n    )\n\n\ndef _validate_name(name):\n    \"\"\"Pre-flight ``Bucket`` name validation.\n\n    :type name: str or :data:`NoneType`\n    :param name: Proposed bucket name.\n\n    :rtype: str or :data:`NoneType`\n    :returns: ``name`` if valid.\n    \"\"\"\n    if name is None:\n        return\n\n    # The first and last characters must be alphanumeric.\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\n        raise ValueError(\"Bucket names must start and end with a number or letter.\")\n    return name\n\n\nclass _PropertyMixin(object):\n    \"\"\"Abstract mixin for cloud storage classes with associated properties.\n\n    Non-abstract subclasses should implement:\n      - path\n      - client\n      - user_project\n\n    :type name: str\n    :param name: The name of the object. Bucket names must start and end with a\n                 number or letter.\n    \"\"\"\n\n    def __init__(self, name=None):\n        self.name = name\n        self._properties = {}\n        self._changes = set()\n\n    @property\n    def path(self):\n        \"\"\"Abstract getter for the object path.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def client(self):\n        \"\"\"Abstract getter for the object client.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def user_project(self):\n        \"\"\"Abstract getter for the object user_project.\"\"\"\n        raise NotImplementedError\n\n    def _require_client(self, client):\n        \"\"\"Check client or verify over-ride.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: The client passed in or the currently bound client.\n        \"\"\"\n        if client is None:\n            client = self.client\n        return client\n\n    def _encryption_headers(self):\n        \"\"\"Return any encryption headers needed to fetch the object.\n\n        .. note::\n           Defined here because :meth:`reload` calls it, but this method is\n           really only relevant for :class:`~google.cloud.storage.blob.Blob`.\n\n        :rtype: dict\n        :returns: a mapping of encryption-related headers.\n        \"\"\"\n        return {}\n\n    @property\n    def _query_params(self):\n        \"\"\"Default query parameters.\"\"\"\n        params = {}\n        if self.user_project is not None:\n            params[\"userProject\"] = self.user_project\n        return params\n\n    def reload(\n        self,\n        client=None,\n        projection=\"noAcl\",\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        soft_deleted=None,\n    ):\n        \"\"\"Reload properties from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type projection: str\n        :param projection: (Optional) If used, must be 'full' or 'noAcl'.\n                           Defaults to ``'noAcl'``. Specifies the set of\n                           properties to return.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match: (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]])\n        :param if_etag_not_match: (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If True, looks for a soft-deleted object. Will only return\n            the object metadata if the object exists and is in a soft-deleted state.\n            :attr:`generation` is required to be set on the blob if ``soft_deleted`` is set to True.\n            See: https://cloud.google.com/storage/docs/soft-delete\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        # Pass only '?projection=noAcl' here because 'acl' and related\n        # are handled via custom endpoints.\n        query_params[\"projection\"] = projection\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        if soft_deleted is not None:\n            query_params[\"softDeleted\"] = soft_deleted\n        headers = self._encryption_headers()\n        _add_etag_match_headers(\n            headers, if_etag_match=if_etag_match, if_etag_not_match=if_etag_not_match\n        )\n        api_response = client._get_resource(\n            self.path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n    def _patch_property(self, name, value):\n        \"\"\"Update field of this object's properties.\n\n        This method will only update the field provided and will not\n        touch the other fields.\n\n        It **will not** reload the properties from the server. The behavior is\n        local only and syncing occurs via :meth:`patch`.\n\n        :type name: str\n        :param name: The field name to update.\n\n        :type value: object\n        :param value: The value being updated.\n        \"\"\"\n        self._changes.add(name)\n        self._properties[name] = value\n\n    def _set_properties(self, value):\n        \"\"\"Set the properties for the current object.\n\n        :type value: dict or :class:`google.cloud.storage.batch._FutureDict`\n        :param value: The properties to be set.\n        \"\"\"\n        self._properties = value\n        # If the values are reset, the changes must as well.\n        self._changes = set()\n\n    def patch(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        override_unlocked_retention=False,\n    ):\n        \"\"\"Sends all changed properties in a PATCH request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type override_unlocked_retention: bool\n        :param override_unlocked_retention:\n            (Optional) override_unlocked_retention must be set to True if the operation includes\n            a retention property that changes the mode from Unlocked to Locked, reduces the\n            retainUntilTime, or removes the retention configuration from the object. See:\n            https://cloud.google.com/storage/docs/json_api/v1/objects/patch\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        # Pass '?projection=full' here because 'PATCH' documented not\n        # to work properly w/ 'noAcl'.\n        query_params[\"projection\"] = \"full\"\n        if override_unlocked_retention:\n            query_params[\"overrideUnlockedRetention\"] = override_unlocked_retention\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        update_properties = {key: self._properties[key] for key in self._changes}\n\n        # Make the API call.\n        api_response = client._patch_resource(\n            self.path,\n            update_properties,\n            query_params=query_params,\n            _target_object=self,\n            timeout=timeout,\n            retry=retry,\n        )\n        self._set_properties(api_response)\n\n    def update(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        override_unlocked_retention=False,\n    ):\n        \"\"\"Sends all properties in a PUT request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type override_unlocked_retention: bool\n        :param override_unlocked_retention:\n            (Optional) override_unlocked_retention must be set to True if the operation includes\n            a retention property that changes the mode from Unlocked to Locked, reduces the\n            retainUntilTime, or removes the retention configuration from the object. See:\n            https://cloud.google.com/storage/docs/json_api/v1/objects/patch\n        \"\"\"\n        client = self._require_client(client)\n\n        query_params = self._query_params\n        query_params[\"projection\"] = \"full\"\n        if override_unlocked_retention:\n            query_params[\"overrideUnlockedRetention\"] = override_unlocked_retention\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        api_response = client._put_resource(\n            self.path,\n            self._properties,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n\ndef _scalar_property(fieldname):\n    \"\"\"Create a property descriptor around the :class:`_PropertyMixin` helpers.\"\"\"\n\n    def _getter(self):\n        \"\"\"Scalar property getter.\"\"\"\n        return self._properties.get(fieldname)\n\n    def _setter(self, value):\n        \"\"\"Scalar property setter.\"\"\"\n        self._patch_property(fieldname, value)\n\n    return property(_getter, _setter)\n\n\ndef _write_buffer_to_hash(buffer_object, hash_obj, digest_block_size=8192):\n    \"\"\"Read blocks from a buffer and update a hash with them.\n\n    :type buffer_object: bytes buffer\n    :param buffer_object: Buffer containing bytes used to update a hash object.\n\n    :type hash_obj: object that implements update\n    :param hash_obj: A hash object (MD5 or CRC32-C).\n\n    :type digest_block_size: int\n    :param digest_block_size: The block size to write to the hash.\n                              Defaults to 8192.\n    \"\"\"\n    block = buffer_object.read(digest_block_size)\n\n    while len(block) > 0:\n        hash_obj.update(block)\n        # Update the block for the next iteration.\n        block = buffer_object.read(digest_block_size)\n\n\ndef _base64_md5hash(buffer_object):\n    \"\"\"Get MD5 hash of bytes (as base64).\n\n    :type buffer_object: bytes buffer\n    :param buffer_object: Buffer containing bytes used to compute an MD5\n                          hash (as base64).\n\n    :rtype: str\n    :returns: A base64 encoded digest of the MD5 hash.\n    \"\"\"\n    hash_obj = md5()\n    _write_buffer_to_hash(buffer_object, hash_obj)\n    digest_bytes = hash_obj.digest()\n    return base64.b64encode(digest_bytes)\n\n\ndef _add_etag_match_headers(headers, **match_parameters):\n    \"\"\"Add generation match parameters into the given parameters list.\n\n    :type headers: dict\n    :param headers: Headers dict.\n\n    :type match_parameters: dict\n    :param match_parameters: if*etag*match parameters to add.\n    \"\"\"\n    for snakecase_name, header_name in _ETAG_MATCH_PARAMETERS:\n        value = match_parameters.get(snakecase_name)\n\n        if value is not None:\n            if isinstance(value, str):\n                value = [value]\n            headers[header_name] = \", \".join(value)\n\n\ndef _add_generation_match_parameters(parameters, **match_parameters):\n    \"\"\"Add generation match parameters into the given parameters list.\n\n    :type parameters: list or dict\n    :param parameters: Parameters list or dict.\n\n    :type match_parameters: dict\n    :param match_parameters: if*generation*match parameters to add.\n\n    :raises: :exc:`ValueError` if ``parameters`` is not a ``list()``\n             or a ``dict()``.\n    \"\"\"\n    for snakecase_name, camelcase_name in _GENERATION_MATCH_PARAMETERS:\n        value = match_parameters.get(snakecase_name)\n\n        if value is not None:\n            if isinstance(parameters, list):\n                parameters.append((camelcase_name, value))\n\n            elif isinstance(parameters, dict):\n                parameters[camelcase_name] = value\n\n            else:\n                raise ValueError(\n                    \"`parameters` argument should be a dict() or a list().\"\n                )\n\n\ndef _raise_if_more_than_one_set(**kwargs):\n    \"\"\"Raise ``ValueError`` exception if more than one parameter was set.\n\n    :type error: :exc:`ValueError`\n    :param error: Description of which fields were set\n\n    :raises: :class:`~ValueError` containing the fields that were set\n    \"\"\"\n    if sum(arg is not None for arg in kwargs.values()) > 1:\n        escaped_keys = [f\"'{name}'\" for name in kwargs.keys()]\n\n        keys_but_last = \", \".join(escaped_keys[:-1])\n        last_key = escaped_keys[-1]\n\n        msg = f\"Pass at most one of {keys_but_last} and {last_key}\"\n\n        raise ValueError(msg)\n\n\ndef _bucket_bound_hostname_url(host, scheme=None):\n    \"\"\"Helper to build bucket bound hostname URL.\n\n    :type host: str\n    :param host: Host name.\n\n    :type scheme: str\n    :param scheme: (Optional) Web scheme. If passed, use it\n                   as a scheme in the result URL.\n\n    :rtype: str\n    :returns: A bucket bound hostname URL.\n    \"\"\"\n    url_parts = urlsplit(host)\n    if url_parts.scheme and url_parts.netloc:\n        return host\n\n    return f\"{scheme}://{host}\"\n\n\ndef _api_core_retry_to_resumable_media_retry(retry, num_retries=None):\n    \"\"\"Convert google.api.core.Retry to google.resumable_media.RetryStrategy.\n\n    Custom predicates are not translated.\n\n    :type retry: google.api_core.Retry\n    :param retry: (Optional) The google.api_core.Retry object to translate.\n\n    :type num_retries: int\n    :param num_retries: (Optional) The number of retries desired. This is\n        supported for backwards compatibility and is mutually exclusive with\n        `retry`.\n\n    :rtype: google.resumable_media.RetryStrategy\n    :returns: A RetryStrategy with all applicable attributes copied from input,\n              or a RetryStrategy with max_retries set to 0 if None was input.\n    \"\"\"\n\n    if retry is not None and num_retries is not None:\n        raise ValueError(\"num_retries and retry arguments are mutually exclusive\")\n\n    elif retry is not None:\n        return resumable_media.RetryStrategy(\n            max_sleep=retry._maximum,\n            max_cumulative_retry=retry._deadline,\n            initial_delay=retry._initial,\n            multiplier=retry._multiplier,\n        )\n    elif num_retries is not None:\n        return resumable_media.RetryStrategy(max_retries=num_retries)\n    else:\n        return resumable_media.RetryStrategy(max_retries=0)\n\n\ndef _get_invocation_id():\n    return \"gccl-invocation-id/\" + str(uuid4())\n\n\ndef _get_default_headers(\n    user_agent,\n    content_type=\"application/json; charset=UTF-8\",\n    x_upload_content_type=None,\n    command=None,\n):\n    \"\"\"Get the headers for a request.\n\n    :type user_agent: str\n    :param user_agent: The user-agent for requests.\n\n    :type command: str\n    :param command:\n        (Optional) Information about which interface for the operation was\n        used, to be included in the X-Goog-API-Client header. Please leave\n        as None unless otherwise directed.\n\n    :rtype: dict\n    :returns: The headers to be used for the request.\n    \"\"\"\n    x_goog_api_client = f\"{user_agent} {_get_invocation_id()}\"\n\n    if command:\n        x_goog_api_client += f\" gccl-gcs-cmd/{command}\"\n\n    return {\n        \"Accept\": \"application/json\",\n        \"Accept-Encoding\": \"gzip, deflate\",\n        \"User-Agent\": user_agent,\n        \"X-Goog-API-Client\": x_goog_api_client,\n        \"content-type\": content_type,\n        \"x-upload-content-type\": x_upload_content_type or content_type,\n    }\n"}