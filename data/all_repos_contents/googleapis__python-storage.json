{"noxfile.py": "# -*- coding: utf-8 -*-\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated by synthtool. DO NOT EDIT!\n\nfrom __future__ import absolute_import\nimport os\nimport pathlib\nimport shutil\n\nimport nox\n\n\nBLACK_VERSION = \"black==23.7.0\"\nBLACK_PATHS = [\"docs\", \"google\", \"tests\", \"noxfile.py\", \"setup.py\"]\n\nDEFAULT_PYTHON_VERSION = \"3.8\"\nSYSTEM_TEST_PYTHON_VERSIONS = [\"3.8\"]\nUNIT_TEST_PYTHON_VERSIONS = [\"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\nCONFORMANCE_TEST_PYTHON_VERSIONS = [\"3.8\"]\n\n_DEFAULT_STORAGE_HOST = \"https://storage.googleapis.com\"\n\nCURRENT_DIRECTORY = pathlib.Path(__file__).parent.absolute()\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef lint(session):\n    \"\"\"Run linters.\n\n    Returns a failure if the linters find linting errors or sufficiently\n    serious code quality issues.\n    \"\"\"\n    # Pin flake8 to 6.0.0\n    # See https://github.com/googleapis/python-storage/issues/1102\n    session.install(\"flake8==6.0.0\", BLACK_VERSION)\n    session.run(\n        \"black\",\n        \"--check\",\n        *BLACK_PATHS,\n    )\n    session.run(\"flake8\", \"google\", \"tests\")\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef blacken(session):\n    \"\"\"Run black.\n\n    Format code to uniform standard.\n    \"\"\"\n    session.install(BLACK_VERSION)\n    session.run(\n        \"black\",\n        *BLACK_PATHS,\n    )\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef lint_setup_py(session):\n    \"\"\"Verify that setup.py is valid (including RST check).\"\"\"\n    session.install(\"docutils\", \"pygments\")\n    session.run(\"python\", \"setup.py\", \"check\", \"--restructuredtext\", \"--strict\")\n\n\ndef default(session):\n    constraints_path = str(\n        CURRENT_DIRECTORY / \"testing\" / f\"constraints-{session.python}.txt\"\n    )\n    # Install all test dependencies, then install this package in-place.\n    session.install(\"mock\", \"pytest\", \"pytest-cov\", \"-c\", constraints_path)\n    session.install(\"-e\", \".\", \"-c\", constraints_path)\n\n    # Run py.test against the unit tests.\n    session.run(\n        \"py.test\",\n        \"--quiet\",\n        f\"--junitxml=unit_{session.python}_sponge_log.xml\",\n        \"--cov=google.cloud.storage\",\n        \"--cov=google.cloud\",\n        \"--cov=tests.unit\",\n        \"--cov-append\",\n        \"--cov-config=.coveragerc\",\n        \"--cov-report=\",\n        \"--cov-fail-under=0\",\n        os.path.join(\"tests\", \"unit\"),\n        *session.posargs,\n    )\n\n\n@nox.session(python=UNIT_TEST_PYTHON_VERSIONS)\ndef unit(session):\n    \"\"\"Run the unit test suite.\"\"\"\n    default(session)\n\n\n@nox.session(python=SYSTEM_TEST_PYTHON_VERSIONS)\ndef system(session):\n    constraints_path = str(\n        CURRENT_DIRECTORY / \"testing\" / f\"constraints-{session.python}.txt\"\n    )\n    \"\"\"Run the system test suite.\"\"\"\n    system_test_path = os.path.join(\"tests\", \"system.py\")\n    system_test_folder_path = os.path.join(\"tests\", \"system\")\n    rerun_count = 0\n\n    # Check the value of `RUN_SYSTEM_TESTS` env var. It defaults to true.\n    if os.environ.get(\"RUN_SYSTEM_TESTS\", \"true\") == \"false\":\n        session.skip(\"RUN_SYSTEM_TESTS is set to false, skipping\")\n    # Environment check: Only run tests if the environment variable is set.\n    if not os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\"):\n        session.skip(\n            \"Credentials must be set via environment variable GOOGLE_APPLICATION_CREDENTIALS\"\n        )\n    # mTLS tests requires pyopenssl.\n    if os.environ.get(\"GOOGLE_API_USE_CLIENT_CERTIFICATE\", \"\") == \"true\":\n        session.install(\"pyopenssl\")\n    # Check if endpoint is being overriden for rerun_count\n    if (\n        os.getenv(\"API_ENDPOINT_OVERRIDE\", \"https://storage.googleapis.com\")\n        != \"https://storage.googleapis.com\"\n    ):\n        rerun_count = 3\n\n    system_test_exists = os.path.exists(system_test_path)\n    system_test_folder_exists = os.path.exists(system_test_folder_path)\n    # Environment check: only run tests if found.\n    if not system_test_exists and not system_test_folder_exists:\n        session.skip(\"System tests were not found\")\n\n    # Use pre-release gRPC for system tests.\n    # TODO: Remove ban of 1.52.0rc1 once grpc/grpc#31885 is resolved.\n    session.install(\"--pre\", \"grpcio!=1.52.0rc1\")\n\n    # Install all test dependencies, then install this package into the\n    # virtualenv's dist-packages.\n    # 2021-05-06: defer installing 'google-cloud-*' to after this package,\n    #             in order to work around Python 2.7 googolapis-common-protos\n    #             issue.\n    session.install(\"mock\", \"pytest\", \"pytest-rerunfailures\", \"-c\", constraints_path)\n    session.install(\"-e\", \".\", \"-c\", constraints_path)\n    session.install(\n        \"google-cloud-testutils\",\n        \"google-cloud-iam\",\n        \"google-cloud-pubsub < 2.0.0\",\n        \"google-cloud-kms < 2.0dev\",\n        \"-c\",\n        constraints_path,\n    )\n\n    # Run py.test against the system tests.\n    if system_test_exists:\n        session.run(\n            \"py.test\",\n            \"--quiet\",\n            f\"--junitxml=system_{session.python}_sponge_log.xml\",\n            \"--reruns={}\".format(rerun_count),\n            system_test_path,\n            *session.posargs,\n        )\n    if system_test_folder_exists:\n        session.run(\n            \"py.test\",\n            \"--quiet\",\n            f\"--junitxml=system_{session.python}_sponge_log.xml\",\n            \"--reruns={}\".format(rerun_count),\n            system_test_folder_path,\n            *session.posargs,\n        )\n\n\n@nox.session(python=CONFORMANCE_TEST_PYTHON_VERSIONS)\ndef conftest_retry(session):\n    \"\"\"Run the retry conformance test suite.\"\"\"\n    conformance_test_folder_path = os.path.join(\"tests\", \"conformance\")\n    conformance_test_folder_exists = os.path.exists(conformance_test_folder_path)\n    # Environment check: only run tests if found.\n    if not conformance_test_folder_exists:\n        session.skip(\"Conformance tests were not found\")\n\n    # Install all test dependencies and pytest plugin to run tests in parallel.\n    # Then install this package in-place.\n    session.install(\"pytest\", \"pytest-xdist\")\n    session.install(\"-e\", \".\")\n\n    # Run #CPU processes in parallel if no test session arguments are passed in.\n    if session.posargs:\n        test_cmd = [\n            \"py.test\",\n            \"--quiet\",\n            conformance_test_folder_path,\n            *session.posargs,\n        ]\n    else:\n        test_cmd = [\"py.test\", \"-n\", \"auto\", \"--quiet\", conformance_test_folder_path]\n\n    # Run py.test against the conformance tests.\n    session.run(*test_cmd)\n\n\n@nox.session(python=DEFAULT_PYTHON_VERSION)\ndef cover(session):\n    \"\"\"Run the final coverage report.\n\n    This outputs the coverage report aggregating coverage from the unit\n    test runs (not system test runs), and then erases coverage data.\n    \"\"\"\n    session.install(\"coverage\", \"pytest-cov\")\n    session.run(\"coverage\", \"report\", \"--show-missing\", \"--fail-under=100\")\n\n    session.run(\"coverage\", \"erase\")\n\n\n@nox.session(python=\"3.9\")\ndef docs(session):\n    \"\"\"Build the docs for this library.\"\"\"\n\n    session.install(\"-e\", \".\")\n    session.install(\n        # We need to pin to specific versions of the `sphinxcontrib-*` packages\n        # which still support sphinx 4.x.\n        # See https://github.com/googleapis/sphinx-docfx-yaml/issues/344\n        # and https://github.com/googleapis/sphinx-docfx-yaml/issues/345.\n        \"sphinxcontrib-applehelp==1.0.4\",\n        \"sphinxcontrib-devhelp==1.0.2\",\n        \"sphinxcontrib-htmlhelp==2.0.1\",\n        \"sphinxcontrib-qthelp==1.0.3\",\n        \"sphinxcontrib-serializinghtml==1.1.5\",\n        \"sphinx==4.5.0\",\n        \"alabaster\",\n        \"recommonmark\",\n    )\n\n    shutil.rmtree(os.path.join(\"docs\", \"_build\"), ignore_errors=True)\n    session.run(\n        \"sphinx-build\",\n        \"-W\",  # warnings as errors\n        \"-T\",  # show full traceback on exception\n        \"-N\",  # no colors\n        \"-b\",\n        \"html\",\n        \"-d\",\n        os.path.join(\"docs\", \"_build\", \"doctrees\", \"\"),\n        os.path.join(\"docs\", \"\"),\n        os.path.join(\"docs\", \"_build\", \"html\", \"\"),\n    )\n\n\n@nox.session(python=\"3.10\")\ndef docfx(session):\n    \"\"\"Build the docfx yaml files for this library.\"\"\"\n\n    session.install(\"-e\", \".\")\n    session.install(\"grpcio\")\n    session.install(\n        # We need to pin to specific versions of the `sphinxcontrib-*` packages\n        # which still support sphinx 4.x.\n        # See https://github.com/googleapis/sphinx-docfx-yaml/issues/344\n        # and https://github.com/googleapis/sphinx-docfx-yaml/issues/345.\n        \"sphinxcontrib-applehelp==1.0.4\",\n        \"sphinxcontrib-devhelp==1.0.2\",\n        \"sphinxcontrib-htmlhelp==2.0.1\",\n        \"sphinxcontrib-qthelp==1.0.3\",\n        \"sphinxcontrib-serializinghtml==1.1.5\",\n        \"gcp-sphinx-docfx-yaml\",\n        \"alabaster\",\n        \"recommonmark\",\n    )\n\n    shutil.rmtree(os.path.join(\"docs\", \"_build\"), ignore_errors=True)\n    session.run(\n        \"sphinx-build\",\n        \"-T\",  # show full traceback on exception\n        \"-N\",  # no colors\n        \"-D\",\n        (\n            \"extensions=sphinx.ext.autodoc,\"\n            \"sphinx.ext.autosummary,\"\n            \"docfx_yaml.extension,\"\n            \"sphinx.ext.intersphinx,\"\n            \"sphinx.ext.coverage,\"\n            \"sphinx.ext.napoleon,\"\n            \"sphinx.ext.todo,\"\n            \"sphinx.ext.viewcode,\"\n            \"recommonmark\"\n        ),\n        \"-b\",\n        \"html\",\n        \"-d\",\n        os.path.join(\"docs\", \"_build\", \"doctrees\", \"\"),\n        os.path.join(\"docs\", \"\"),\n        os.path.join(\"docs\", \"_build\", \"html\", \"\"),\n    )\n", "setup.py": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\n\nimport setuptools\n\n\n# Package metadata.\n\nname = \"google-cloud-storage\"\ndescription = \"Google Cloud Storage API client library\"\n# Should be one of:\n# 'Development Status :: 3 - Alpha'\n# 'Development Status :: 4 - Beta'\n# 'Development Status :: 5 - Production/Stable'\nrelease_status = \"Development Status :: 5 - Production/Stable\"\ndependencies = [\n    \"google-auth >= 2.26.1, < 3.0dev\",\n    \"google-api-core >= 2.15.0, <3.0.0dev\",\n    \"google-cloud-core >= 2.3.0, < 3.0dev\",\n    \"google-resumable-media >= 2.6.0\",\n    \"requests >= 2.18.0, < 3.0.0dev\",\n    \"google-crc32c >= 1.0, < 2.0dev\",\n]\nextras = {\"protobuf\": [\"protobuf<5.0.0dev\"]}\n\n\n# Setup boilerplate below this line.\n\npackage_root = os.path.abspath(os.path.dirname(__file__))\n\nversion = {}\nwith open(os.path.join(package_root, \"google/cloud/storage/version.py\")) as fp:\n    exec(fp.read(), version)\nversion = version[\"__version__\"]\n\nreadme_filename = os.path.join(package_root, \"README.rst\")\nwith io.open(readme_filename, encoding=\"utf-8\") as readme_file:\n    readme = readme_file.read()\n\n# Only include packages under the 'google' namespace. Do not include tests,\n# benchmarks, etc.\npackages = [\n    package\n    for package in setuptools.find_namespace_packages()\n    if package.startswith(\"google\")\n]\n\n\nsetuptools.setup(\n    name=name,\n    version=version,\n    description=description,\n    long_description=readme,\n    author=\"Google LLC\",\n    author_email=\"googleapis-packages@google.com\",\n    license=\"Apache 2.0\",\n    url=\"https://github.com/googleapis/python-storage\",\n    classifiers=[\n        release_status,\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Operating System :: OS Independent\",\n        \"Topic :: Internet\",\n    ],\n    platforms=\"Posix; MacOS X; Windows\",\n    packages=packages,\n    install_requires=dependencies,\n    extras_require=extras,\n    python_requires=\">=3.7\",\n    include_package_data=True,\n    zip_safe=False,\n)\n", "owlbot.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This script is used to synthesize generated parts of this library.\"\"\"\n\nimport synthtool as s\nfrom synthtool import gcp\nfrom synthtool.languages import python\n\ncommon = gcp.CommonTemplates()\n\n# ----------------------------------------------------------------------------\n# Add templated files\n# ----------------------------------------------------------------------------\ntemplated_files = common.py_library(\n    cov_level=100,\n    split_system_tests=True,\n    system_test_external_dependencies=[\n        \"google-cloud-iam\",\n        \"google-cloud-pubsub < 2.0.0\",\n        # See: https://github.com/googleapis/python-storage/issues/226\n        \"google-cloud-kms < 2.0dev\",\n    ],\n    intersphinx_dependencies={\n        # python-requests url temporary change related to\n        # https://github.com/psf/requests/issues/6140#issuecomment-1135071992\n        \"requests\": \"https://requests.readthedocs.io/en/stable/\"\n    },\n)\n\ns.move(\n    templated_files,\n    excludes=[\n        \"docs/multiprocessing.rst\",\n        \"noxfile.py\",\n        \"CONTRIBUTING.rst\",\n        \"README.rst\",\n        \".kokoro/samples/python3.6\", # remove python 3.6 support\n        \".github/workflows\", # exclude gh actions as credentials are needed for tests\n        \".github/release-please.yml\", # special support for a python2 branch in this repo\n    ],\n)\n\ns.replace(\n    \".kokoro/build.sh\",\n    \"export PYTHONUNBUFFERED=1\",\n    \"\"\"export PYTHONUNBUFFERED=1\n\n# Export variable to override api endpoint\nexport API_ENDPOINT_OVERRIDE\n\n# Export variable to override api endpoint version\nexport API_VERSION_OVERRIDE\n\n# Export dual region locations\nexport DUAL_REGION_LOC_1\nexport DUAL_REGION_LOC_2\"\"\")\n\ns.replace(\n    \".coveragerc\",\n    \"omit =\",\n    \"\"\"omit =\n  .nox/*\"\"\")\n\npython.py_samples(skip_readmes=True)\n\ns.shell.run([\"nox\", \"-s\", \"blacken\"], hide_output=False)\n", "pylint.config.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This module is used to configure gcp-devrel-py-tools run-pylint.\"\"\"\n\n# Library configuration\n\n# library_additions = {}\n# library_replacements = {}\n\n# Test configuration\n\n# test_additions = copy.deepcopy(library_additions)\n# test_replacements = copy.deepcopy(library_replacements)\n", "samples/snippets/storage_create_bucket_hierarchical_namespace.py": "#!/usr/bin/env python\n\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket_hierarchical_namespace]\nfrom google.cloud import storage\n\n\ndef create_bucket_hierarchical_namespace(bucket_name):\n    \"\"\"Creates a bucket with hierarchical namespace enabled.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.hierarchical_namespace_enabled = True\n    bucket.create()\n\n    print(f\"Created bucket {bucket_name} with hierarchical namespace enabled.\")\n\n\n# [END storage_create_bucket_hierarchical_namespace]\n\n\nif __name__ == \"__main__\":\n    create_bucket_hierarchical_namespace(bucket_name=sys.argv[1])\n", "samples/snippets/storage_make_public.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_make_public]\nfrom google.cloud import storage\n\n\ndef make_blob_public(bucket_name, blob_name):\n    \"\"\"Makes a blob publicly accessible.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    blob.make_public()\n\n    print(\n        f\"Blob {blob.name} is publicly accessible at {blob.public_url}\"\n    )\n\n\n# [END storage_make_public]\n\nif __name__ == \"__main__\":\n    make_blob_public(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_remove_bucket_iam_member.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_iam_member]\nfrom google.cloud import storage\n\n\ndef remove_bucket_iam_member(bucket_name, role, member):\n    \"\"\"Remove member from bucket IAM Policy\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g. roles/storage.objectViewer\"\n    # member = \"IAM identity, e.g. user: name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    for binding in policy.bindings:\n        print(binding)\n        if binding[\"role\"] == role and binding.get(\"condition\") is None:\n            binding[\"members\"].discard(member)\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Removed {member} with role {role} from {bucket_name}.\")\n\n\n# [END storage_remove_bucket_iam_member]\n\nif __name__ == \"__main__\":\n    remove_bucket_iam_member(\n        bucket_name=sys.argv[1], role=sys.argv[2], member=sys.argv[3]\n    )\n", "samples/snippets/storage_remove_bucket_default_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_default_owner]\nfrom google.cloud import storage\n\n\ndef remove_bucket_default_owner(bucket_name, user_email):\n    \"\"\"Removes a user from the access control list of the given bucket's\n    default object access control list.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # remove access for different types of entities.\n    bucket.default_object_acl.user(user_email).revoke_read()\n    bucket.default_object_acl.user(user_email).revoke_write()\n    bucket.default_object_acl.user(user_email).revoke_owner()\n    bucket.default_object_acl.save()\n\n    print(\n        f\"Removed user {user_email} from the default acl of bucket {bucket_name}.\"\n    )\n\n\n# [END storage_remove_bucket_default_owner]\n\nif __name__ == \"__main__\":\n    remove_bucket_default_owner(\n        bucket_name=sys.argv[1], user_email=sys.argv[2]\n    )\n", "samples/snippets/storage_disable_bucket_lifecycle_management.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_bucket_lifecycle_management]\nfrom google.cloud import storage\n\n\ndef disable_bucket_lifecycle_management(bucket_name):\n    \"\"\"Disable lifecycle management for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.clear_lifecyle_rules()\n    bucket.patch()\n    rules = bucket.lifecycle_rules\n\n    print(f\"Lifecycle management is disable for bucket {bucket_name} and the rules are {list(rules)}\")\n    return bucket\n\n\n# [END storage_disable_bucket_lifecycle_management]\n\nif __name__ == \"__main__\":\n    disable_bucket_lifecycle_management(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_service_account.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_get_service_account]\nfrom google.cloud import storage\n\n\ndef get_service_account():\n    \"\"\"Get the service account email\"\"\"\n    storage_client = storage.Client()\n\n    email = storage_client.get_service_account_email()\n    print(\n        f\"The GCS service account for project {storage_client.project} is: {email} \"\n    )\n\n\n# [END storage_get_service_account]\n\nif __name__ == \"__main__\":\n    get_service_account()\n", "samples/snippets/storage_list_files.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_files]\nfrom google.cloud import storage\n\n\ndef list_blobs(bucket_name):\n    \"\"\"Lists all the blobs in the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    # Note: Client.list_blobs requires at least package version 1.17.0.\n    blobs = storage_client.list_blobs(bucket_name)\n\n    # Note: The call returns a response only when the iterator is consumed.\n    for blob in blobs:\n        print(blob.name)\n\n\n# [END storage_list_files]\n\n\nif __name__ == \"__main__\":\n    list_blobs(bucket_name=sys.argv[1])\n", "samples/snippets/storage_transfer_manager_upload_many.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_upload_many]\ndef upload_many_blobs_with_transfer_manager(\n    bucket_name, filenames, source_directory=\"\", workers=8\n):\n    \"\"\"Upload every file in a list to a bucket, concurrently in a process pool.\n\n    Each blob name is derived from the filename, not including the\n    `source_directory` parameter. For complete control of the blob name for each\n    file (and other aspects of individual blob metadata), use\n    transfer_manager.upload_many() instead.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # A list (or other iterable) of filenames to upload.\n    # filenames = [\"file_1.txt\", \"file_2.txt\"]\n\n    # The directory on your computer that is the root of all of the files in the\n    # list of filenames. This string is prepended (with os.path.join()) to each\n    # filename to get the full path to the file. Relative paths and absolute\n    # paths are both accepted. This string is not included in the name of the\n    # uploaded blob; it is only used to find the source files. An empty string\n    # means \"the current working directory\". Note that this parameter allows\n    # directory traversal (e.g. \"/\", \"../\") and is not intended for unsanitized\n    # end user input.\n    # source_directory=\"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    results = transfer_manager.upload_many_from_filenames(\n        bucket, filenames, source_directory=source_directory, max_workers=workers\n    )\n\n    for name, result in zip(filenames, results):\n        # The results list is either `None` or an exception for each filename in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Uploaded {} to {}.\".format(name, bucket.name))\n# [END storage_transfer_manager_upload_many]\n", "samples/snippets/storage_delete_bucket_notification.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that deletes a notification configuration for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_delete_bucket_notification]\nfrom google.cloud import storage\n\n\ndef delete_bucket_notification(bucket_name, notification_id):\n    \"\"\"Deletes a notification configuration for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of the notification\n    # notification_id = \"your-notification-id\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notification = bucket.notification(notification_id=notification_id)\n    notification.delete()\n\n    print(f\"Successfully deleted notification with ID {notification_id} for bucket {bucket_name}\")\n\n# [END storage_delete_bucket_notification]\n\n\nif __name__ == \"__main__\":\n    delete_bucket_notification(bucket_name=sys.argv[1], notification_id=sys.argv[2])\n", "samples/snippets/hmac_samples_test.py": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nTests for hmac.py. Requires GOOGLE_CLOUD_PROJECT (valid project) and\nHMAC_KEY_TEST_SERVICE_ACCOUNT (valid service account email) env variables to be\nset in order to run.\n\"\"\"\n\n\nimport os\n\nimport google.api_core.exceptions\nfrom google.cloud import storage\nimport pytest\n\nimport storage_activate_hmac_key\nimport storage_create_hmac_key\nimport storage_deactivate_hmac_key\nimport storage_delete_hmac_key\nimport storage_get_hmac_key\nimport storage_list_hmac_keys\n\n# We are reaching maximum number of HMAC keys on the service account.\n# We change the service account based on the value of\n# RUN_TESTS_SESSION in noxfile_config.py.\n# The reason we can not use multiple project is that our new projects\n# are enforced to have\n# 'constraints/iam.disableServiceAccountKeyCreation' policy.\n\nPROJECT_ID = os.environ[\"MAIN_GOOGLE_CLOUD_PROJECT\"]\nSERVICE_ACCOUNT_EMAIL = os.environ[\"HMAC_KEY_TEST_SERVICE_ACCOUNT\"]\nSTORAGE_CLIENT = storage.Client(project=PROJECT_ID)\n\n\n@pytest.fixture(scope=\"module\")\ndef new_hmac_key():\n    \"\"\"\n    Fixture to create a new HMAC key, and to guarantee all keys are deleted at\n    the end of the module.\n\n    NOTE: Due to the module scope, test order in this file is significant\n    \"\"\"\n    hmac_key, secret = STORAGE_CLIENT.create_hmac_key(\n        service_account_email=SERVICE_ACCOUNT_EMAIL, project_id=PROJECT_ID\n    )\n    yield hmac_key\n    # Re-fetch the key metadata in case state has changed during the test.\n    hmac_key = STORAGE_CLIENT.get_hmac_key_metadata(\n        hmac_key.access_id, project_id=PROJECT_ID\n    )\n    if hmac_key.state == \"DELETED\":\n        return\n    if not hmac_key.state == \"INACTIVE\":\n        hmac_key.state = \"INACTIVE\"\n        hmac_key.update()\n    hmac_key.delete()\n\n\ndef test_list_keys(capsys, new_hmac_key):\n    hmac_keys = storage_list_hmac_keys.list_keys(PROJECT_ID)\n    assert \"HMAC Keys:\" in capsys.readouterr().out\n    assert hmac_keys.num_results >= 1\n\n\ndef test_create_key(capsys):\n    hmac_key = storage_create_hmac_key.create_key(\n        PROJECT_ID, SERVICE_ACCOUNT_EMAIL\n    )\n    hmac_key.state = \"INACTIVE\"\n    hmac_key.update()\n    hmac_key.delete()\n    assert \"Key ID:\" in capsys.readouterr().out\n    assert hmac_key.access_id\n\n\ndef test_get_key(capsys, new_hmac_key):\n    hmac_key = storage_get_hmac_key.get_key(new_hmac_key.access_id, PROJECT_ID)\n    assert \"HMAC key metadata\" in capsys.readouterr().out\n    assert hmac_key.access_id == new_hmac_key.access_id\n\n\ndef test_activate_key(capsys, new_hmac_key):\n    new_hmac_key.state = \"INACTIVE\"\n    new_hmac_key.update()\n    hmac_key = storage_activate_hmac_key.activate_key(\n        new_hmac_key.access_id, PROJECT_ID\n    )\n    assert \"State: ACTIVE\" in capsys.readouterr().out\n    assert hmac_key.state == \"ACTIVE\"\n\n\ndef test_deactivate_key(capsys, new_hmac_key):\n    hmac_key = storage_deactivate_hmac_key.deactivate_key(\n        new_hmac_key.access_id, PROJECT_ID\n    )\n    assert \"State: INACTIVE\" in capsys.readouterr().out\n    assert hmac_key.state == \"INACTIVE\"\n\n\ndef test_delete_key(capsys, new_hmac_key):\n    # Due to reuse of the HMAC key for each test function, the previous\n    # test has deactivated the key already.\n    try:\n        new_hmac_key.state = \"INACTIVE\"\n        new_hmac_key.update()\n    except google.api_core.exceptions.BadRequest:\n        pass\n\n    storage_delete_hmac_key.delete_key(new_hmac_key.access_id, PROJECT_ID)\n    assert \"The key is deleted\" in capsys.readouterr().out\n", "samples/snippets/storage_generate_signed_url_v2.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_generate_signed_url_v2]\nimport datetime\n# [END storage_generate_signed_url_v2]\nimport sys\n# [START storage_generate_signed_url_v2]\n\nfrom google.cloud import storage\n\n\ndef generate_signed_url(bucket_name, blob_name):\n    \"\"\"Generates a v2 signed URL for downloading a blob.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        # This URL is valid for 1 hour\n        expiration=datetime.timedelta(hours=1),\n        # Allow GET requests using this URL.\n        method=\"GET\",\n    )\n\n    print(f\"The signed url for {blob.name} is {url}\")\n    return url\n\n\n# [END storage_generate_signed_url_v2]\n\nif __name__ == \"__main__\":\n    generate_signed_url(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_add_bucket_default_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_default_owner]\nfrom google.cloud import storage\n\n\ndef add_bucket_default_owner(bucket_name, user_email):\n    \"\"\"Adds a user as an owner in the given bucket's default object access\n    control list.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # grant access to different types of entities. You can also use\n    # `grant_read` or `grant_write` to grant different roles.\n    bucket.default_object_acl.user(user_email).grant_owner()\n    bucket.default_object_acl.save()\n\n    print(\n        \"Added user {} as an owner in the default acl on bucket {}.\".format(\n            user_email, bucket_name\n        )\n    )\n\n\n# [END storage_add_bucket_default_owner]\n\nif __name__ == \"__main__\":\n    add_bucket_default_owner(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_get_public_access_prevention.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_public_access_prevention]\nfrom google.cloud import storage\n\n\ndef get_public_access_prevention(bucket_name):\n    \"\"\"Gets the public access prevention setting (either 'inherited' or 'enforced') for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    iam_configuration = bucket.iam_configuration\n\n    print(\n        f\"Public access prevention is {iam_configuration.public_access_prevention} for {bucket.name}.\"\n    )\n\n\n# [END storage_get_public_access_prevention]\n\nif __name__ == \"__main__\":\n    get_public_access_prevention(bucket_name=sys.argv[1])\n", "samples/snippets/storage_generate_signed_url_v4.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_generate_signed_url_v4]\nimport datetime\n# [END storage_generate_signed_url_v4]\nimport sys\n# [START storage_generate_signed_url_v4]\n\nfrom google.cloud import storage\n\n\ndef generate_download_signed_url_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 signed URL for downloading a blob.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        version=\"v4\",\n        # This URL is valid for 15 minutes\n        expiration=datetime.timedelta(minutes=15),\n        # Allow GET requests using this URL.\n        method=\"GET\",\n    )\n\n    print(\"Generated GET signed URL:\")\n    print(url)\n    print(\"You can use this URL with any user agent, for example:\")\n    print(f\"curl '{url}'\")\n    return url\n\n\n# [END storage_generate_signed_url_v4]\n\nif __name__ == \"__main__\":\n    generate_download_signed_url_v4(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_print_file_acl_for_user.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_file_acl_for_user]\nfrom google.cloud import storage\n\n\ndef print_blob_acl_for_user(bucket_name, blob_name, user_email):\n    \"\"\"Prints out a blob's access control list for a given user.\"\"\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    blob.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # get the roles for different types of entities.\n    roles = blob.acl.user(user_email).get_roles()\n\n    print(roles)\n\n\n# [END storage_print_file_acl_for_user]\n\nif __name__ == \"__main__\":\n    print_blob_acl_for_user(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2], user_email=sys.argv[3],\n    )\n", "samples/snippets/storage_release_temporary_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_release_temporary_hold]\nfrom google.cloud import storage\n\n\ndef release_temporary_hold(bucket_name, blob_name):\n    \"\"\"Releases the temporary hold on a given blob\"\"\"\n\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.temporary_hold = False\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(\"Temporary hold was release for #{blob_name}\")\n\n\n# [END storage_release_temporary_hold]\n\n\nif __name__ == \"__main__\":\n    release_temporary_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_print_bucket_acl.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_bucket_acl]\nfrom google.cloud import storage\n\n\ndef print_bucket_acl(bucket_name):\n    \"\"\"Prints out a bucket's access control list.\"\"\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    for entry in bucket.acl:\n        print(f\"{entry['role']}: {entry['entity']}\")\n\n\n# [END storage_print_bucket_acl]\n\nif __name__ == \"__main__\":\n    print_bucket_acl(bucket_name=sys.argv[1])\n", "samples/snippets/storage_bucket_delete_default_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_bucket_delete_default_kms_key]\nfrom google.cloud import storage\n\n\ndef bucket_delete_default_kms_key(bucket_name):\n    \"\"\"Delete a default KMS key of bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.default_kms_key_name = None\n    bucket.patch()\n\n    print(f\"Default KMS key was removed from {bucket.name}\")\n    return bucket\n\n\n# [END storage_bucket_delete_default_kms_key]\n\nif __name__ == \"__main__\":\n    bucket_delete_default_kms_key(bucket_name=sys.argv[1])\n", "samples/snippets/storage_compose_file.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_compose_file]\nfrom google.cloud import storage\n\n\ndef compose_file(bucket_name, first_blob_name, second_blob_name, destination_blob_name):\n    \"\"\"Concatenate source blobs into destination blob.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # first_blob_name = \"first-object-name\"\n    # second_blob_name = \"second-blob-name\"\n    # destination_blob_name = \"destination-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    destination = bucket.blob(destination_blob_name)\n    destination.content_type = \"text/plain\"\n\n    # Note sources is a list of Blob instances, up to the max of 32 instances per request\n    sources = [bucket.blob(first_blob_name), bucket.blob(second_blob_name)]\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to compose is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n    destination_generation_match_precondition = 0\n\n    destination.compose(sources, if_generation_match=destination_generation_match_precondition)\n\n    print(\n        \"New composite object {} in the bucket {} was created by combining {} and {}\".format(\n            destination_blob_name, bucket_name, first_blob_name, second_blob_name\n        )\n    )\n    return destination\n\n\n# [END storage_compose_file]\n\nif __name__ == \"__main__\":\n    compose_file(\n        bucket_name=sys.argv[1],\n        first_blob_name=sys.argv[2],\n        second_blob_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n    )\n", "samples/snippets/storage_remove_cors_configuration.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_cors_configuration]\nfrom google.cloud import storage\n\n\ndef remove_cors_configuration(bucket_name):\n    \"\"\"Remove a bucket's CORS policies configuration.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.cors = []\n    bucket.patch()\n\n    print(f\"Remove CORS policies for bucket {bucket.name}.\")\n    return bucket\n\n\n# [END storage_remove_cors_configuration]\n\nif __name__ == \"__main__\":\n    remove_cors_configuration(bucket_name=sys.argv[1])\n", "samples/snippets/storage_add_bucket_iam_member.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_iam_member]\nfrom google.cloud import storage\n\n\ndef add_bucket_iam_member(bucket_name, role, member):\n    \"\"\"Add a new member to an IAM Policy\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g., roles/storage.objectViewer\"\n    # member = \"IAM identity, e.g., user: name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    policy.bindings.append({\"role\": role, \"members\": {member}})\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Added {member} with role {role} to {bucket_name}.\")\n\n\n# [END storage_add_bucket_iam_member]\n\n\nif __name__ == \"__main__\":\n    add_bucket_iam_member(bucket_name=sys.argv[1], role=sys.argv[2], member=sys.argv[3])\n", "samples/snippets/storage_transfer_manager_download_chunks_concurrently.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_download_chunks_concurrently]\ndef download_chunks_concurrently(\n    bucket_name, blob_name, filename, chunk_size=32 * 1024 * 1024, workers=8\n):\n    \"\"\"Download a single file in chunks, concurrently in a process pool.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The file to be downloaded\n    # blob_name = \"target-file\"\n\n    # The destination filename or path\n    # filename = \"\"\n\n    # The size of each chunk. The performance impact of this value depends on\n    # the use case. The remote service has a minimum of 5 MiB and a maximum of\n    # 5 GiB.\n    # chunk_size = 32 * 1024 * 1024 (32 MiB)\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    transfer_manager.download_chunks_concurrently(\n        blob, filename, chunk_size=chunk_size, max_workers=workers\n    )\n\n    print(\"Downloaded {} to {}.\".format(blob_name, filename))\n\n\n# [END storage_transfer_manager_download_chunks_concurrently]\n", "samples/snippets/storage_set_bucket_public_iam.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_bucket_public_iam]\nfrom typing import List\n\nfrom google.cloud import storage\n\n\ndef set_bucket_public_iam(\n    bucket_name: str = \"your-bucket-name\",\n    members: List[str] = [\"allUsers\"],\n):\n    \"\"\"Set a public IAM Policy to bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    policy.bindings.append(\n        {\"role\": \"roles/storage.objectViewer\", \"members\": members}\n    )\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Bucket {bucket.name} is now publicly readable\")\n\n\n# [END storage_set_bucket_public_iam]\n\nif __name__ == \"__main__\":\n    set_bucket_public_iam(\n        bucket_name=sys.argv[1],\n    )\n", "samples/snippets/storage_list_buckets.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_list_buckets]\nfrom google.cloud import storage\n\n\ndef list_buckets():\n    \"\"\"Lists all buckets.\"\"\"\n\n    storage_client = storage.Client()\n    buckets = storage_client.list_buckets()\n\n    for bucket in buckets:\n        print(bucket.name)\n\n\n# [END storage_list_buckets]\n\n\nif __name__ == \"__main__\":\n    list_buckets()\n", "samples/snippets/storage_upload_from_stream.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_stream_file_upload]\nfrom google.cloud import storage\n\n\ndef upload_blob_from_stream(bucket_name, file_obj, destination_blob_name):\n    \"\"\"Uploads bytes from a stream or other file-like object to a blob.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The stream or file (file-like object) from which to read\n    # import io\n    # file_obj = io.BytesIO()\n    # file_obj.write(b\"This is test data.\")\n\n    # The desired name of the uploaded GCS object (blob)\n    # destination_blob_name = \"storage-object-name\"\n\n    # Construct a client-side representation of the blob.\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    # Rewind the stream to the beginning. This step can be omitted if the input\n    # stream will always be at a correct position.\n    file_obj.seek(0)\n\n    # Upload data from the stream to your bucket.\n    blob.upload_from_file(file_obj)\n\n    print(\n        f\"Stream data uploaded to {destination_blob_name} in bucket {bucket_name}.\"\n    )\n\n# [END storage_stream_file_upload]\n", "samples/snippets/storage_download_into_memory.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_file_download_into_memory]\nfrom google.cloud import storage\n\n\ndef download_blob_into_memory(bucket_name, blob_name):\n    \"\"\"Downloads a blob into memory.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object\n    # blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n    # any content from Google Cloud Storage. As we don't need additional data,\n    # using `Bucket.blob` is preferred here.\n    blob = bucket.blob(blob_name)\n    contents = blob.download_as_bytes()\n\n    print(\n        \"Downloaded storage object {} from bucket {} as the following bytes object: {}.\".format(\n            blob_name, bucket_name, contents.decode(\"utf-8\")\n        )\n    )\n\n\n# [END storage_file_download_into_memory]\n\nif __name__ == \"__main__\":\n    download_blob_into_memory(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n    )\n", "samples/snippets/storage_get_requester_pays_status.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_requester_pays_status]\nfrom google.cloud import storage\n\n\ndef get_requester_pays_status(bucket_name):\n    \"\"\"Get a bucket's requester pays metadata\"\"\"\n    # bucket_name = \"my-bucket\"\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    requester_pays_status = bucket.requester_pays\n\n    if requester_pays_status:\n        print(f\"Requester Pays is enabled for {bucket_name}\")\n    else:\n        print(f\"Requester Pays is disabled for {bucket_name}\")\n\n\n# [END storage_get_requester_pays_status]\n\nif __name__ == \"__main__\":\n    get_requester_pays_status(bucket_name=sys.argv[1])\n", "samples/snippets/iam_test.py": "# Copyright 2017 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nimport time\nimport uuid\n\nfrom google.cloud import storage\nimport pytest\n\nimport storage_add_bucket_conditional_iam_binding\nimport storage_add_bucket_iam_member\nimport storage_remove_bucket_conditional_iam_binding\nimport storage_remove_bucket_iam_member\nimport storage_set_bucket_public_iam\nimport storage_view_bucket_iam_members\n\nMEMBER = \"group:dpebot@google.com\"\nROLE = \"roles/storage.legacyBucketReader\"\n\nCONDITION_TITLE = \"match-prefix\"\nCONDITION_DESCRIPTION = \"Applies to objects matching a prefix\"\nCONDITION_EXPRESSION = (\n    'resource.name.startsWith(\"projects/_/buckets/bucket-name/objects/prefix-a-\")'\n)\n\n\n@pytest.fixture(scope=\"module\")\ndef bucket():\n    bucket = None\n    while bucket is None or bucket.exists():\n        storage_client = storage.Client()\n        bucket_name = f\"test-iam-{uuid.uuid4()}\"\n        bucket = storage_client.bucket(bucket_name)\n        bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    storage_client.create_bucket(bucket)\n    yield bucket\n    time.sleep(3)\n    bucket.delete(force=True)\n\n\n@pytest.fixture(scope=\"function\")\ndef public_bucket():\n    # The new projects don't allow to make a bucket available to public, so\n    # we need to use the old main project for now.\n    original_value = os.environ['GOOGLE_CLOUD_PROJECT']\n    os.environ['GOOGLE_CLOUD_PROJECT'] = os.environ['MAIN_GOOGLE_CLOUD_PROJECT']\n    bucket = None\n    while bucket is None or bucket.exists():\n        storage_client = storage.Client()\n        bucket_name = f\"test-iam-{uuid.uuid4()}\"\n        bucket = storage_client.bucket(bucket_name)\n        bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    storage_client.create_bucket(bucket)\n    yield bucket\n    time.sleep(3)\n    bucket.delete(force=True)\n    # Set the value back.\n    os.environ['GOOGLE_CLOUD_PROJECT'] = original_value\n\n\ndef test_view_bucket_iam_members(capsys, bucket):\n    storage_view_bucket_iam_members.view_bucket_iam_members(bucket.name)\n    assert re.match(\"Role: .*, Members: .*\", capsys.readouterr().out)\n\n\ndef test_add_bucket_iam_member(bucket):\n    storage_add_bucket_iam_member.add_bucket_iam_member(bucket.name, ROLE, MEMBER)\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    assert any(\n        binding[\"role\"] == ROLE and MEMBER in binding[\"members\"]\n        for binding in policy.bindings\n    )\n\n\ndef test_add_bucket_conditional_iam_binding(bucket):\n    storage_add_bucket_conditional_iam_binding.add_bucket_conditional_iam_binding(\n        bucket.name,\n        ROLE,\n        CONDITION_TITLE,\n        CONDITION_DESCRIPTION,\n        CONDITION_EXPRESSION,\n        {MEMBER},\n    )\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    assert any(\n        binding[\"role\"] == ROLE\n        and binding[\"members\"] == {MEMBER}\n        and binding[\"condition\"]\n        == {\n            \"title\": CONDITION_TITLE,\n            \"description\": CONDITION_DESCRIPTION,\n            \"expression\": CONDITION_EXPRESSION,\n        }\n        for binding in policy.bindings\n    )\n\n\ndef test_remove_bucket_iam_member(public_bucket):\n    storage_remove_bucket_iam_member.remove_bucket_iam_member(\n        public_bucket.name, ROLE, MEMBER)\n\n    policy = public_bucket.get_iam_policy(requested_policy_version=3)\n    assert not any(\n        binding[\"role\"] == ROLE and MEMBER in binding[\"members\"]\n        for binding in policy.bindings\n    )\n\n\ndef test_remove_bucket_conditional_iam_binding(bucket):\n    storage_remove_bucket_conditional_iam_binding.remove_bucket_conditional_iam_binding(\n        bucket.name, ROLE, CONDITION_TITLE, CONDITION_DESCRIPTION, CONDITION_EXPRESSION\n    )\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    condition = {\n        \"title\": CONDITION_TITLE,\n        \"description\": CONDITION_DESCRIPTION,\n        \"expression\": CONDITION_EXPRESSION,\n    }\n    assert not any(\n        (binding[\"role\"] == ROLE and binding.get(\"condition\") == condition)\n        for binding in policy.bindings\n    )\n\n\ndef test_set_bucket_public_iam(public_bucket):\n    # The test project has org policy restricting identities by domain.\n    # Testing \"domain:google.com\" instead of \"allUsers\"\n    storage_set_bucket_public_iam.set_bucket_public_iam(public_bucket.name, [\"domain:google.com\"])\n    policy = public_bucket.get_iam_policy(requested_policy_version=3)\n\n    assert any(\n        binding[\"role\"] == \"roles/storage.objectViewer\"\n        and \"domain:google.com\" in binding[\"members\"]\n        for binding in policy.bindings\n    )\n", "samples/snippets/storage_delete_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_file]\nfrom google.cloud import storage\n\n\ndef delete_blob(bucket_name, blob_name):\n    \"\"\"Deletes a blob from the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    generation_match_precondition = None\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to delete is aborted if the object's\n    # generation number does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = blob.generation\n\n    blob.delete(if_generation_match=generation_match_precondition)\n\n    print(f\"Blob {blob_name} deleted.\")\n\n\n# [END storage_delete_file]\n\nif __name__ == \"__main__\":\n    delete_blob(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_create_bucket_dual_region.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"\nSample that creates a dual region bucket.\n\"\"\"\n\n# [START storage_create_bucket_dual_region]\nfrom google.cloud import storage\n\n\ndef create_bucket_dual_region(bucket_name, location, region_1, region_2):\n    \"\"\"Creates a Dual-Region Bucket with provided location and regions..\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The bucket's pair of regions. Case-insensitive.\n    # See this documentation for other valid locations:\n    # https://cloud.google.com/storage/docs/locations\n    # region_1 = \"US-EAST1\"\n    # region_2 = \"US-WEST1\"\n    # location = \"US\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.create_bucket(bucket_name, location=location, data_locations=[region_1, region_2])\n\n    print(f\"Created bucket {bucket_name}\")\n    print(f\" - location: {bucket.location}\")\n    print(f\" - location_type: {bucket.location_type}\")\n    print(f\" - customPlacementConfig data_locations: {bucket.data_locations}\")\n\n\n# [END storage_create_bucket_dual_region]\n\n\nif __name__ == \"__main__\":\n    create_bucket_dual_region(\n        bucket_name=sys.argv[1], location=sys.argv[2], region_1=sys.argv[3], region_2=sys.argv[4]\n    )\n", "samples/snippets/storage_define_bucket_website_configuration.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_define_bucket_website_configuration]\nfrom google.cloud import storage\n\n\ndef define_bucket_website_configuration(bucket_name, main_page_suffix, not_found_page):\n    \"\"\"Configure website-related properties of bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # main_page_suffix = \"index.html\"\n    # not_found_page = \"404.html\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.configure_website(main_page_suffix, not_found_page)\n    bucket.patch()\n\n    print(\n        \"Static website bucket {} is set up to use {} as the index page and {} as the 404 page\".format(\n            bucket.name, main_page_suffix, not_found_page\n        )\n    )\n    return bucket\n\n\n# [END storage_define_bucket_website_configuration]\n\nif __name__ == \"__main__\":\n    define_bucket_website_configuration(\n        bucket_name=sys.argv[1],\n        main_page_suffix=sys.argv[2],\n        not_found_page=sys.argv[3],\n    )\n", "samples/snippets/quickstart.py": "#!/usr/bin/env python\n\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef run_quickstart():\n    # [START storage_quickstart]\n    # Imports the Google Cloud client library\n    from google.cloud import storage\n\n    # Instantiates a client\n    storage_client = storage.Client()\n\n    # The name for the new bucket\n    bucket_name = \"my-new-bucket\"\n\n    # Creates the new bucket\n    bucket = storage_client.create_bucket(bucket_name)\n\n    print(f\"Bucket {bucket.name} created.\")\n    # [END storage_quickstart]\n\n\nif __name__ == \"__main__\":\n    run_quickstart()\n", "samples/snippets/storage_get_rpo.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that gets RPO (Recovery Point Objective) of a bucket\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_get_rpo]\n\nfrom google.cloud import storage\n\n\ndef get_rpo(bucket_name):\n    \"\"\"Gets the RPO of the bucket\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    rpo = bucket.rpo\n\n    print(f\"RPO for {bucket.name} is {rpo}.\")\n\n\n# [END storage_get_rpo]\n\nif __name__ == \"__main__\":\n    get_rpo(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_bucket_metadata.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport sys\n\n# [START storage_get_bucket_metadata]\n\nfrom google.cloud import storage\n\n\ndef bucket_metadata(bucket_name):\n    \"\"\"Prints out a bucket's metadata.\"\"\"\n    # bucket_name = 'your-bucket-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    print(f\"ID: {bucket.id}\")\n    print(f\"Name: {bucket.name}\")\n    print(f\"Storage Class: {bucket.storage_class}\")\n    print(f\"Location: {bucket.location}\")\n    print(f\"Location Type: {bucket.location_type}\")\n    print(f\"Cors: {bucket.cors}\")\n    print(f\"Default Event Based Hold: {bucket.default_event_based_hold}\")\n    print(f\"Default KMS Key Name: {bucket.default_kms_key_name}\")\n    print(f\"Metageneration: {bucket.metageneration}\")\n    print(\n        f\"Public Access Prevention: {bucket.iam_configuration.public_access_prevention}\"\n    )\n    print(f\"Retention Effective Time: {bucket.retention_policy_effective_time}\")\n    print(f\"Retention Period: {bucket.retention_period}\")\n    print(f\"Retention Policy Locked: {bucket.retention_policy_locked}\")\n    print(f\"Object Retention Mode: {bucket.object_retention_mode}\")\n    print(f\"Requester Pays: {bucket.requester_pays}\")\n    print(f\"Self Link: {bucket.self_link}\")\n    print(f\"Time Created: {bucket.time_created}\")\n    print(f\"Versioning Enabled: {bucket.versioning_enabled}\")\n    print(f\"Labels: {bucket.labels}\")\n\n\n# [END storage_get_bucket_metadata]\n\nif __name__ == \"__main__\":\n    bucket_metadata(bucket_name=sys.argv[1])\n", "samples/snippets/storage_list_file_archived_generations.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_file_archived_generations]\nfrom google.cloud import storage\n\n\ndef list_file_archived_generations(bucket_name):\n    \"\"\"Lists all the blobs in the bucket with generation.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    blobs = storage_client.list_blobs(bucket_name, versions=True)\n\n    for blob in blobs:\n        print(f\"{blob.name},{blob.generation}\")\n\n\n# [END storage_list_file_archived_generations]\n\n\nif __name__ == \"__main__\":\n    list_file_archived_generations(bucket_name=sys.argv[1])\n", "samples/snippets/storage_add_bucket_conditional_iam_binding.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_conditional_iam_binding]\nfrom google.cloud import storage\n\n\ndef add_bucket_conditional_iam_binding(\n    bucket_name, role, title, description, expression, members\n):\n    \"\"\"Add a conditional IAM binding to a bucket's IAM policy.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g. roles/storage.objectViewer\"\n    # members = {\"IAM identity, e.g. user: name@example.com}\"\n    # title = \"Condition title.\"\n    # description = \"Condition description.\"\n    # expression = \"Condition expression.\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    # Set the policy's version to 3 to use condition in bindings.\n    policy.version = 3\n\n    policy.bindings.append(\n        {\n            \"role\": role,\n            \"members\": members,\n            \"condition\": {\n                \"title\": title,\n                \"description\": description,\n                \"expression\": expression,\n            },\n        }\n    )\n\n    bucket.set_iam_policy(policy)\n\n    print(f\"Added the following member(s) with role {role} to {bucket_name}:\")\n\n    for member in members:\n        print(f\"    {member}\")\n\n    print(\"with condition:\")\n    print(f\"    Title: {title}\")\n    print(f\"    Description: {description}\")\n    print(f\"    Expression: {expression}\")\n\n\n# [END storage_add_bucket_conditional_iam_binding]\n\n\nif __name__ == \"__main__\":\n    add_bucket_conditional_iam_binding(\n        bucket_name=sys.argv[1],\n        role=sys.argv[2],\n        title=sys.argv[3],\n        description=sys.argv[4],\n        expression=sys.argv[5],\n        members=set(sys.argv[6::]),\n    )\n", "samples/snippets/storage_get_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_retention_policy]\nfrom google.cloud import storage\n\n\ndef get_retention_policy(bucket_name):\n    \"\"\"Gets the retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket.reload()\n\n    print(f\"Retention Policy for {bucket_name}\")\n    print(f\"Retention Period: {bucket.retention_period}\")\n    if bucket.retention_policy_locked:\n        print(\"Retention Policy is locked\")\n\n    if bucket.retention_policy_effective_time:\n        print(\n            f\"Effective Time: {bucket.retention_policy_effective_time}\"\n        )\n\n\n# [END storage_get_retention_policy]\n\n\nif __name__ == \"__main__\":\n    get_retention_policy(bucket_name=sys.argv[1])\n", "samples/snippets/storage_remove_file_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_file_owner]\nfrom google.cloud import storage\n\n\ndef remove_blob_owner(bucket_name, blob_name, user_email):\n    \"\"\"Removes a user from the access control list of the given blob in the\n    given bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # remove access for different types of entities.\n    blob.acl.user(user_email).revoke_read()\n    blob.acl.user(user_email).revoke_write()\n    blob.acl.user(user_email).revoke_owner()\n    blob.acl.save()\n\n    print(\n        f\"Removed user {user_email} from blob {blob_name} in bucket {bucket_name}.\"\n    )\n\n\n# [END storage_remove_file_owner]\n\nif __name__ == \"__main__\":\n    remove_blob_owner(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2], user_email=sys.argv[3],\n    )\n", "samples/snippets/storage_disable_default_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_default_event_based_hold]\nfrom google.cloud import storage\n\n\ndef disable_default_event_based_hold(bucket_name):\n    \"\"\"Disables the default event based hold on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.default_event_based_hold = False\n    bucket.patch()\n\n    print(f\"Default event based hold was disabled for {bucket_name}\")\n\n\n# [END storage_disable_default_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    disable_default_event_based_hold(bucket_name=sys.argv[1])\n", "samples/snippets/storage_change_default_storage_class.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_change_default_storage_class]\nfrom google.cloud import storage\nfrom google.cloud.storage import constants\n\n\ndef change_default_storage_class(bucket_name):\n    \"\"\"Change the default storage class of the bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.storage_class = constants.COLDLINE_STORAGE_CLASS\n    bucket.patch()\n\n    print(f\"Default storage class for bucket {bucket_name} has been set to {bucket.storage_class}\")\n    return bucket\n\n\n# [END storage_change_default_storage_class]\n\nif __name__ == \"__main__\":\n    change_default_storage_class(bucket_name=sys.argv[1])\n", "samples/snippets/storage_cors_configuration.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_cors_configuration]\nfrom google.cloud import storage\n\n\ndef cors_configuration(bucket_name):\n    \"\"\"Set a bucket's CORS policies configuration.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.cors = [\n        {\n            \"origin\": [\"*\"],\n            \"responseHeader\": [\n                \"Content-Type\",\n                \"x-goog-resumable\"],\n            \"method\": ['PUT', 'POST'],\n            \"maxAgeSeconds\": 3600\n        }\n    ]\n    bucket.patch()\n\n    print(f\"Set CORS policies for bucket {bucket.name} is {bucket.cors}\")\n    return bucket\n\n\n# [END storage_cors_configuration]\n\nif __name__ == \"__main__\":\n    cors_configuration(bucket_name=sys.argv[1])\n", "samples/snippets/rpo_test.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport uuid\n\nfrom google.cloud import storage\nimport pytest\n\nimport storage_create_bucket_turbo_replication\nimport storage_get_rpo\nimport storage_set_rpo_async_turbo\nimport storage_set_rpo_default\n\n\n@pytest.fixture\ndef dual_region_bucket():\n    \"\"\"Yields a dual region bucket that is deleted after the test completes.\"\"\"\n    bucket = None\n    location = \"NAM4\"\n    while bucket is None or bucket.exists():\n        bucket_name = f\"bucket-lock-{uuid.uuid4()}\"\n        bucket = storage.Client().bucket(bucket_name)\n    bucket.create(location=location)\n    yield bucket\n    bucket.delete(force=True)\n\n\ndef test_get_rpo(dual_region_bucket, capsys):\n    storage_get_rpo.get_rpo(dual_region_bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"RPO for {dual_region_bucket.name} is DEFAULT.\" in out\n\n\ndef test_set_rpo_async_turbo(dual_region_bucket, capsys):\n    storage_set_rpo_async_turbo.set_rpo_async_turbo(dual_region_bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"RPO is set to ASYNC_TURBO for {dual_region_bucket.name}.\" in out\n\n\ndef test_set_rpo_default(dual_region_bucket, capsys):\n    storage_set_rpo_default.set_rpo_default(dual_region_bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"RPO is set to DEFAULT for {dual_region_bucket.name}.\" in out\n\n\ndef test_create_bucket_turbo_replication(capsys):\n    bucket_name = f\"test-rpo-{uuid.uuid4()}\"\n    storage_create_bucket_turbo_replication.create_bucket_turbo_replication(bucket_name)\n    out, _ = capsys.readouterr()\n    assert f\"{bucket_name} created with the recovery point objective (RPO) set to ASYNC_TURBO in NAM4.\" in out\n", "samples/snippets/storage_copy_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_copy_file]\nfrom google.cloud import storage\n\n\ndef copy_blob(\n    bucket_name, blob_name, destination_bucket_name, destination_blob_name,\n):\n    \"\"\"Copies a blob from one bucket to another with a new name.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # destination_bucket_name = \"destination-bucket-name\"\n    # destination_blob_name = \"destination-object-name\"\n\n    storage_client = storage.Client()\n\n    source_bucket = storage_client.bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.bucket(destination_bucket_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to copy is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n    destination_generation_match_precondition = 0\n\n    blob_copy = source_bucket.copy_blob(\n        source_blob, destination_bucket, destination_blob_name, if_generation_match=destination_generation_match_precondition,\n    )\n\n    print(\n        \"Blob {} in bucket {} copied to blob {} in bucket {}.\".format(\n            source_blob.name,\n            source_bucket.name,\n            blob_copy.name,\n            destination_bucket.name,\n        )\n    )\n\n\n# [END storage_copy_file]\n\nif __name__ == \"__main__\":\n    copy_blob(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        destination_bucket_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n    )\n", "samples/snippets/storage_async_upload.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport sys\n\n\n\"\"\"Sample that asynchronously uploads a file to GCS\n\"\"\"\n\n\n# [START storage_async_upload]\n# This sample can be run by calling `async.run(async_upload_blob('bucket_name'))`\nasync def async_upload_blob(bucket_name):\n    \"\"\"Uploads a number of files in parallel to the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    import asyncio\n    from functools import partial\n    from google.cloud import storage\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    loop = asyncio.get_running_loop()\n\n    tasks = []\n    count = 3\n    for x in range(count):\n        blob_name = f\"async_sample_blob_{x}\"\n        content = f\"Hello world #{x}\"\n        blob = bucket.blob(blob_name)\n        # The first arg, None, tells it to use the default loops executor\n        tasks.append(loop.run_in_executor(None, partial(blob.upload_from_string, content)))\n\n    # If the method returns a value (such as download_as_string), gather will return the values\n    await asyncio.gather(*tasks)\n\n    print(f\"Uploaded {count} files to bucket {bucket_name}\")\n\n\n# [END storage_async_upload]\n\n\nif __name__ == \"__main__\":\n    asyncio.run(async_upload_blob(\n        bucket_name=sys.argv[1]\n    ))\n", "samples/snippets/storage_print_bucket_acl_for_user.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_bucket_acl_for_user]\nfrom google.cloud import storage\n\n\ndef print_bucket_acl_for_user(bucket_name, user_email):\n    \"\"\"Prints out a bucket's access control list for a given user.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # get the roles for different types of entities.\n    roles = bucket.acl.user(user_email).get_roles()\n\n    print(roles)\n\n\n# [END storage_print_bucket_acl_for_user]\n\nif __name__ == \"__main__\":\n    print_bucket_acl_for_user(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_print_file_acl.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_print_file_acl]\nfrom google.cloud import storage\n\n\ndef print_blob_acl(bucket_name, blob_name):\n    \"\"\"Prints out a blob's access control list.\"\"\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    for entry in blob.acl:\n        print(f\"{entry['role']}: {entry['entity']}\")\n\n\n# [END storage_print_file_acl]\n\nif __name__ == \"__main__\":\n    print_blob_acl(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_batch_request.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that uses a batch request.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/batch\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_batch_request]\n\nfrom google.cloud import storage\n\n\ndef batch_request(bucket_name, prefix=None):\n    \"\"\"\n    Use a batch request to patch a list of objects with the given prefix in a bucket.\n\n    Note that Cloud Storage does not support batch operations for uploading or downloading.\n    Additionally, the current batch design does not support library methods whose return values\n    depend on the response payload.\n    See https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.batch\n    \"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n    # The prefix of the object paths\n    # prefix = \"directory-prefix/\"\n\n    client = storage.Client()\n    bucket = client.bucket(bucket_name)\n\n    # Accumulate in a list the objects with a given prefix.\n    blobs_to_patch = [blob for blob in bucket.list_blobs(prefix=prefix)]\n\n    # Use a batch context manager to edit metadata in the list of blobs.\n    # The batch request is sent out when the context manager closes.\n    # No more than 100 calls should be included in a single batch request.\n    with client.batch():\n        for blob in blobs_to_patch:\n            metadata = {\"your-metadata-key\": \"your-metadata-value\"}\n            blob.metadata = metadata\n            blob.patch()\n\n    print(\n        f\"Batch request edited metadata for all objects with the given prefix in {bucket.name}.\"\n    )\n\n\n# [END storage_batch_request]\n\nif __name__ == \"__main__\":\n    batch_request(bucket_name=sys.argv[1], prefix=sys.argv[2])\n", "samples/snippets/public_access_prevention_test.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport storage_get_public_access_prevention\nimport storage_set_public_access_prevention_enforced\nimport storage_set_public_access_prevention_inherited\n\n\ndef test_get_public_access_prevention(bucket, capsys):\n    short_name = storage_get_public_access_prevention\n    short_name.get_public_access_prevention(bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"Public access prevention is inherited for {bucket.name}.\" in out\n\n\ndef test_set_public_access_prevention_enforced(bucket, capsys):\n    short_name = storage_set_public_access_prevention_enforced\n    short_name.set_public_access_prevention_enforced(bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"Public access prevention is set to enforced for {bucket.name}.\" in out\n\n\ndef test_set_public_access_prevention_inherited(bucket, capsys):\n    short_name = storage_set_public_access_prevention_inherited\n    short_name.set_public_access_prevention_inherited(bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"Public access prevention is 'inherited' for {bucket.name}.\" in out\n", "samples/snippets/storage_set_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_event_based_hold]\nfrom google.cloud import storage\n\n\ndef set_event_based_hold(bucket_name, blob_name):\n    \"\"\"Sets a event based hold on a given blob\"\"\"\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.event_based_hold = True\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(f\"Event based hold was set for {blob_name}\")\n\n\n# [END storage_set_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    set_event_based_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_enable_default_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_default_event_based_hold]\nfrom google.cloud import storage\n\n\ndef enable_default_event_based_hold(bucket_name):\n    \"\"\"Enables the default event based hold on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    bucket.default_event_based_hold = True\n    bucket.patch()\n\n    print(f\"Default event based hold was enabled for {bucket_name}\")\n\n\n# [END storage_enable_default_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    enable_default_event_based_hold(bucket_name=sys.argv[1])\n", "samples/snippets/fileio_test.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport uuid\n\nimport storage_fileio_pandas\nimport storage_fileio_write_read\n\n\ndef test_fileio_write_read(bucket, capsys):\n    blob_name = f\"test-fileio-{uuid.uuid4()}\"\n    storage_fileio_write_read.write_read(bucket.name, blob_name)\n    out, _ = capsys.readouterr()\n    assert \"Hello world\" in out\n\n\ndef test_fileio_pandas(bucket, capsys):\n    blob_name = f\"test-fileio-{uuid.uuid4()}\"\n    storage_fileio_pandas.pandas_write(bucket.name, blob_name)\n    out, _ = capsys.readouterr()\n    assert f\"Wrote csv with pandas with name {blob_name} from bucket {bucket.name}.\" in out\n    storage_fileio_pandas.pandas_read(bucket.name, blob_name)\n    out, _ = capsys.readouterr()\n    assert f\"Read csv with pandas with name {blob_name} from bucket {bucket.name}.\" in out\n", "samples/snippets/storage_list_bucket_notifications.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that lists notification configurations for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_list_bucket_notifications]\nfrom google.cloud import storage\n\n\ndef list_bucket_notifications(bucket_name):\n    \"\"\"Lists notification configurations for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notifications = bucket.list_notifications()\n\n    for notification in notifications:\n        print(f\"Notification ID: {notification.notification_id}\")\n\n# [END storage_list_bucket_notifications]\n\n\nif __name__ == \"__main__\":\n    list_bucket_notifications(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_uniform_bucket_level_access.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_uniform_bucket_level_access]\nfrom google.cloud import storage\n\n\ndef get_uniform_bucket_level_access(bucket_name):\n    \"\"\"Get uniform bucket-level access for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    iam_configuration = bucket.iam_configuration\n\n    if iam_configuration.uniform_bucket_level_access_enabled:\n        print(\n            f\"Uniform bucket-level access is enabled for {bucket.name}.\"\n        )\n        print(\n            \"Bucket will be locked on {}.\".format(\n                iam_configuration.uniform_bucket_level_locked_time\n            )\n        )\n    else:\n        print(\n            f\"Uniform bucket-level access is disabled for {bucket.name}.\"\n        )\n\n\n# [END storage_get_uniform_bucket_level_access]\n\nif __name__ == \"__main__\":\n    get_uniform_bucket_level_access(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_with_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_upload_with_kms_key]\nfrom google.cloud import storage\n\n\ndef upload_blob_with_kms(\n    bucket_name, source_file_name, destination_blob_name, kms_key_name,\n):\n    \"\"\"Uploads a file to the bucket, encrypting it with the given KMS key.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_file_name = \"local/path/to/file\"\n    # destination_blob_name = \"storage-object-name\"\n    # kms_key_name = \"projects/PROJ/locations/LOC/keyRings/RING/cryptoKey/KEY\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name, kms_key_name=kms_key_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to upload is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    generation_match_precondition = 0\n\n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\n    print(\n        \"File {} uploaded to {} with encryption key {}.\".format(\n            source_file_name, destination_blob_name, kms_key_name\n        )\n    )\n\n\n# [END storage_upload_with_kms_key]\n\nif __name__ == \"__main__\":\n    upload_blob_with_kms(\n        bucket_name=sys.argv[1],\n        source_file_name=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n        kms_key_name=sys.argv[4],\n    )\n", "samples/snippets/storage_rename_file.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_rename_file]\nfrom google.cloud import storage\n\n\ndef rename_blob(bucket_name, blob_name, new_name):\n    \"\"\"Renames a blob.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of the GCS object to rename\n    # blob_name = \"your-object-name\"\n    # The new ID of the GCS object\n    # new_name = \"new-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    new_blob = bucket.rename_blob(blob, new_name)\n\n    print(f\"Blob {blob.name} has been renamed to {new_blob.name}\")\n\n\n# [END storage_rename_file]\n\nif __name__ == \"__main__\":\n    rename_blob(bucket_name=sys.argv[1], blob_name=sys.argv[2], new_name=sys.argv[3])\n", "samples/snippets/notification_polling_test.py": "# Copyright 2017 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom google.cloud.pubsub_v1.subscriber.message import Message\nimport mock\n\nfrom notification_polling import summarize\n\n\nMESSAGE_ID = 12345\n\n\ndef test_parse_json_message():\n    attributes = {\n        \"eventType\": \"OBJECT_FINALIZE\",\n        \"bucketId\": \"mybucket\",\n        \"objectId\": \"myobject\",\n        \"objectGeneration\": 1234567,\n        \"resource\": \"projects/_/buckets/mybucket/objects/myobject#1234567\",\n        \"notificationConfig\": (\n            \"projects/_/buckets/mybucket/\" \"notificationConfigs/5\"\n        ),\n        \"payloadFormat\": \"JSON_API_V1\",\n    }\n    data = (\n        b\"{\"\n        b'  \"size\": 12345,'\n        b'  \"contentType\": \"text/html\",'\n        b'  \"metageneration\": 1'\n        b\"}\"\n    )\n    message = Message(\n        mock.Mock(data=data, attributes=attributes, publish_time=mock.Mock(seconds=0.0, nanos=0.0)), MESSAGE_ID, delivery_attempt=0, request_queue=mock.Mock()\n    )\n    assert summarize(message) == (\n        \"\\tEvent type: OBJECT_FINALIZE\\n\"\n        \"\\tBucket ID: mybucket\\n\"\n        \"\\tObject ID: myobject\\n\"\n        \"\\tGeneration: 1234567\\n\"\n        \"\\tContent type: text/html\\n\"\n        \"\\tSize: 12345\\n\"\n        \"\\tMetageneration: 1\\n\"\n    )\n", "samples/snippets/storage_get_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_hmac_key]\nfrom google.cloud import storage\n\n\ndef get_key(access_id, project_id):\n    \"\"\"\n    Retrieve the HMACKeyMetadata with the given access id.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an HMAC key\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_get_hmac_key]\n\nif __name__ == \"__main__\":\n    get_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_download_encrypted_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_download_encrypted_file]\nimport base64\n# [END storage_download_encrypted_file]\nimport sys\n# [START storage_download_encrypted_file]\n\nfrom google.cloud import storage\n\n\ndef download_encrypted_blob(\n    bucket_name,\n    source_blob_name,\n    destination_file_name,\n    base64_encryption_key,\n):\n    \"\"\"Downloads a previously-encrypted blob from Google Cloud Storage.\n\n    The encryption key provided must be the same key provided when uploading\n    the blob.\n    \"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_blob_name = \"storage-object-name\"\n    # destination_file_name = \"local/path/to/file\"\n    # base64_encryption_key = \"base64-encoded-encryption-key\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Encryption key must be an AES256 key represented as a bytestring with\n    # 32 bytes. Since it's passed in as a base64 encoded string, it needs\n    # to be decoded.\n    encryption_key = base64.b64decode(base64_encryption_key)\n    blob = bucket.blob(source_blob_name, encryption_key=encryption_key)\n\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        f\"Blob {source_blob_name} downloaded to {destination_file_name}.\"\n    )\n\n\n# [END storage_download_encrypted_file]\n\nif __name__ == \"__main__\":\n    download_encrypted_blob(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        destination_file_name=sys.argv[3],\n        base64_encryption_key=sys.argv[4],\n    )\n", "samples/snippets/storage_set_metadata.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_metadata]\nfrom google.cloud import storage\n\n\ndef set_blob_metadata(bucket_name, blob_name):\n    \"\"\"Set a blob's metadata.\"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.get_blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    metadata = {'color': 'Red', 'name': 'Test'}\n    blob.metadata = metadata\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(f\"The metadata for the blob {blob.name} is {blob.metadata}\")\n\n\n# [END storage_set_metadata]\n\nif __name__ == \"__main__\":\n    set_blob_metadata(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/snippets_test.py": "# Copyright 2016 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport io\nimport os\nimport tempfile\nimport time\nimport uuid\n\nfrom google.cloud import storage\nimport google.cloud.exceptions\nimport pytest\nimport requests\n\nimport storage_add_bucket_label\nimport storage_async_upload\nimport storage_batch_request\nimport storage_bucket_delete_default_kms_key\nimport storage_change_default_storage_class\nimport storage_change_file_storage_class\nimport storage_compose_file\nimport storage_configure_retries\nimport storage_copy_file\nimport storage_copy_file_archived_generation\nimport storage_cors_configuration\nimport storage_create_bucket_class_location\nimport storage_create_bucket_dual_region\nimport storage_create_bucket_hierarchical_namespace\nimport storage_create_bucket_object_retention\nimport storage_define_bucket_website_configuration\nimport storage_delete_file\nimport storage_delete_file_archived_generation\nimport storage_disable_bucket_lifecycle_management\nimport storage_disable_versioning\nimport storage_download_byte_range\nimport storage_download_file\nimport storage_download_into_memory\nimport storage_download_public_file\nimport storage_download_to_stream\nimport storage_enable_bucket_lifecycle_management\nimport storage_enable_versioning\nimport storage_generate_signed_post_policy_v4\nimport storage_generate_signed_url_v2\nimport storage_generate_signed_url_v4\nimport storage_generate_upload_signed_url_v4\nimport storage_get_autoclass\nimport storage_get_bucket_labels\nimport storage_get_bucket_metadata\nimport storage_get_metadata\nimport storage_get_service_account\nimport storage_list_buckets\nimport storage_list_file_archived_generations\nimport storage_list_files\nimport storage_list_files_with_prefix\nimport storage_make_public\nimport storage_move_file\nimport storage_object_get_kms_key\nimport storage_remove_bucket_label\nimport storage_remove_cors_configuration\nimport storage_rename_file\nimport storage_set_autoclass\nimport storage_set_bucket_default_kms_key\nimport storage_set_client_endpoint\nimport storage_set_object_retention_policy\nimport storage_set_metadata\nimport storage_transfer_manager_download_bucket\nimport storage_transfer_manager_download_chunks_concurrently\nimport storage_transfer_manager_download_many\nimport storage_transfer_manager_upload_chunks_concurrently\nimport storage_transfer_manager_upload_directory\nimport storage_transfer_manager_upload_many\nimport storage_upload_file\nimport storage_upload_from_memory\nimport storage_upload_from_stream\nimport storage_upload_with_kms_key\n\nKMS_KEY = os.environ.get(\"CLOUD_KMS_KEY\")\n\n\ndef test_enable_default_kms_key(test_bucket):\n    storage_set_bucket_default_kms_key.enable_default_kms_key(\n        bucket_name=test_bucket.name, kms_key_name=KMS_KEY\n    )\n    time.sleep(2)  # Let change propagate as needed\n    bucket = storage.Client().get_bucket(test_bucket.name)\n    assert bucket.default_kms_key_name.startswith(KMS_KEY)\n    bucket.default_kms_key_name = None\n    bucket.patch()\n\n\ndef test_get_bucket_labels(test_bucket):\n    storage_get_bucket_labels.get_bucket_labels(test_bucket.name)\n\n\ndef test_add_bucket_label(test_bucket, capsys):\n    storage_add_bucket_label.add_bucket_label(test_bucket.name)\n    out, _ = capsys.readouterr()\n    assert \"example\" in out\n\n\ndef test_remove_bucket_label(test_bucket, capsys):\n    storage_add_bucket_label.add_bucket_label(test_bucket.name)\n    storage_remove_bucket_label.remove_bucket_label(test_bucket.name)\n    out, _ = capsys.readouterr()\n    assert \"Removed labels\" in out\n\n\n@pytest.fixture(scope=\"module\")\ndef test_bucket():\n    \"\"\"Yields a bucket that is deleted after the test completes.\"\"\"\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"storage-snippets-test-{uuid.uuid4()}\"\n        bucket = storage.Client().bucket(bucket_name)\n    bucket.create()\n    yield bucket\n    bucket.delete(force=True)\n\n\n@pytest.fixture(scope=\"function\")\ndef test_public_bucket():\n    # The new projects don't allow to make a bucket available to public, so\n    # for some tests we need to use the old main project for now.\n    original_value = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = os.environ[\"MAIN_GOOGLE_CLOUD_PROJECT\"]\n    bucket = None\n    while bucket is None or bucket.exists():\n        storage_client = storage.Client()\n        bucket_name = f\"storage-snippets-test-{uuid.uuid4()}\"\n        bucket = storage_client.bucket(bucket_name)\n    storage_client.create_bucket(bucket)\n    yield bucket\n    bucket.delete(force=True)\n    # Set the value back.\n    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = original_value\n\n\n@pytest.fixture(scope=\"module\")\ndef new_bucket_obj():\n    \"\"\"Yields a new bucket object that is deleted after the test completes.\"\"\"\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"storage-snippets-test-{uuid.uuid4()}\"\n        bucket = storage.Client().bucket(bucket_name)\n    yield bucket\n    bucket.delete(force=True)\n\n\n@pytest.fixture\ndef test_blob(test_bucket):\n    \"\"\"Yields a blob that is deleted after the test completes.\"\"\"\n    bucket = test_bucket\n    blob = bucket.blob(f\"storage_snippets_test_sigil-{uuid.uuid4()}\")\n    blob.upload_from_string(\"Hello, is it me you're looking for?\")\n    yield blob\n\n\n@pytest.fixture(scope=\"function\")\ndef test_public_blob(test_public_bucket):\n    \"\"\"Yields a blob that is deleted after the test completes.\"\"\"\n    bucket = test_public_bucket\n    blob = bucket.blob(f\"storage_snippets_test_sigil-{uuid.uuid4()}\")\n    blob.upload_from_string(\"Hello, is it me you're looking for?\")\n    yield blob\n\n\n@pytest.fixture\ndef test_bucket_create():\n    \"\"\"Yields a bucket object that is deleted after the test completes.\"\"\"\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"storage-snippets-test-{uuid.uuid4()}\"\n        bucket = storage.Client().bucket(bucket_name)\n    yield bucket\n    bucket.delete(force=True)\n\n\ndef test_list_buckets(test_bucket, capsys):\n    storage_list_buckets.list_buckets()\n    out, _ = capsys.readouterr()\n    assert test_bucket.name in out\n\n\ndef test_list_blobs(test_blob, capsys):\n    storage_list_files.list_blobs(test_blob.bucket.name)\n    out, _ = capsys.readouterr()\n    assert test_blob.name in out\n\n\ndef test_bucket_metadata(test_bucket, capsys):\n    storage_get_bucket_metadata.bucket_metadata(test_bucket.name)\n    out, _ = capsys.readouterr()\n    assert test_bucket.name in out\n\n\ndef test_list_blobs_with_prefix(test_blob, capsys):\n    storage_list_files_with_prefix.list_blobs_with_prefix(\n        test_blob.bucket.name, prefix=\"storage_snippets\"\n    )\n    out, _ = capsys.readouterr()\n    assert test_blob.name in out\n\n\ndef test_upload_blob(test_bucket):\n    with tempfile.NamedTemporaryFile() as source_file:\n        source_file.write(b\"test\")\n        source_file.flush()\n\n        storage_upload_file.upload_blob(\n            test_bucket.name, source_file.name, \"test_upload_blob\"\n        )\n\n\ndef test_upload_blob_from_memory(test_bucket, capsys):\n    storage_upload_from_memory.upload_blob_from_memory(\n        test_bucket.name, \"Hello, is it me you're looking for?\", \"test_upload_blob\"\n    )\n    out, _ = capsys.readouterr()\n\n    assert \"Hello, is it me you're looking for?\" in out\n\n\ndef test_upload_blob_from_stream(test_bucket, capsys):\n    file_obj = io.StringIO()\n    file_obj.write(\"This is test data.\")\n    storage_upload_from_stream.upload_blob_from_stream(\n        test_bucket.name, file_obj, \"test_upload_blob\"\n    )\n    out, _ = capsys.readouterr()\n\n    assert \"Stream data uploaded to test_upload_blob\" in out\n\n\ndef test_upload_blob_with_kms(test_bucket):\n    blob_name = f\"test_upload_with_kms_{uuid.uuid4().hex}\"\n    with tempfile.NamedTemporaryFile() as source_file:\n        source_file.write(b\"test\")\n        source_file.flush()\n        storage_upload_with_kms_key.upload_blob_with_kms(\n            test_bucket.name,\n            source_file.name,\n            blob_name,\n            KMS_KEY,\n        )\n        bucket = storage.Client().bucket(test_bucket.name)\n        kms_blob = bucket.get_blob(blob_name)\n        assert kms_blob.kms_key_name.startswith(KMS_KEY)\n    test_bucket.delete_blob(blob_name)\n\n\ndef test_async_upload(bucket, capsys):\n    asyncio.run(storage_async_upload.async_upload_blob(bucket.name))\n    out, _ = capsys.readouterr()\n    assert f\"Uploaded 3 files to bucket {bucket.name}\" in out\n\n\ndef test_download_byte_range(test_blob):\n    with tempfile.NamedTemporaryFile() as dest_file:\n        storage_download_byte_range.download_byte_range(\n            test_blob.bucket.name, test_blob.name, 0, 4, dest_file.name\n        )\n        assert dest_file.read() == b\"Hello\"\n\n\ndef test_download_blob(test_blob):\n    with tempfile.NamedTemporaryFile() as dest_file:\n        storage_download_file.download_blob(\n            test_blob.bucket.name, test_blob.name, dest_file.name\n        )\n\n        assert dest_file.read()\n\n\ndef test_download_blob_into_memory(test_blob, capsys):\n    storage_download_into_memory.download_blob_into_memory(\n        test_blob.bucket.name, test_blob.name\n    )\n    out, _ = capsys.readouterr()\n\n    assert \"Hello, is it me you're looking for?\" in out\n\n\ndef test_download_blob_to_stream(test_blob, capsys):\n    file_obj = io.BytesIO()\n    storage_download_to_stream.download_blob_to_stream(\n        test_blob.bucket.name, test_blob.name, file_obj\n    )\n    out, _ = capsys.readouterr()\n\n    file_obj.seek(0)\n    content = file_obj.read()\n\n    assert \"Downloaded blob\" in out\n    assert b\"Hello, is it me you're looking for?\" in content\n\n\ndef test_blob_metadata(test_blob, capsys):\n    storage_get_metadata.blob_metadata(test_blob.bucket.name, test_blob.name)\n    out, _ = capsys.readouterr()\n    assert test_blob.name in out\n\n\ndef test_set_blob_metadata(test_blob, capsys):\n    storage_set_metadata.set_blob_metadata(test_blob.bucket.name, test_blob.name)\n    out, _ = capsys.readouterr()\n    assert test_blob.name in out\n\n\ndef test_delete_blob(test_blob):\n    storage_delete_file.delete_blob(test_blob.bucket.name, test_blob.name)\n\n\ndef test_make_blob_public(test_public_blob):\n    storage_make_public.make_blob_public(\n        test_public_blob.bucket.name, test_public_blob.name\n    )\n\n    r = requests.get(test_public_blob.public_url)\n    assert r.text == \"Hello, is it me you're looking for?\"\n\n\ndef test_generate_signed_url(test_blob, capsys):\n    url = storage_generate_signed_url_v2.generate_signed_url(\n        test_blob.bucket.name, test_blob.name\n    )\n\n    r = requests.get(url)\n    assert r.text == \"Hello, is it me you're looking for?\"\n\n\ndef test_generate_download_signed_url_v4(test_blob, capsys):\n    url = storage_generate_signed_url_v4.generate_download_signed_url_v4(\n        test_blob.bucket.name, test_blob.name\n    )\n\n    r = requests.get(url)\n    assert r.text == \"Hello, is it me you're looking for?\"\n\n\ndef test_generate_upload_signed_url_v4(test_bucket, capsys):\n    blob_name = \"storage_snippets_test_upload\"\n    content = b\"Uploaded via v4 signed url\"\n    url = storage_generate_upload_signed_url_v4.generate_upload_signed_url_v4(\n        test_bucket.name, blob_name\n    )\n\n    requests.put(\n        url,\n        data=content,\n        headers={\"content-type\": \"application/octet-stream\"},\n    )\n\n    bucket = storage.Client().bucket(test_bucket.name)\n    blob = bucket.blob(blob_name)\n    assert blob.download_as_bytes() == content\n\n\ndef test_generate_signed_policy_v4(test_bucket, capsys):\n    blob_name = \"storage_snippets_test_form\"\n    short_name = storage_generate_signed_post_policy_v4\n    form = short_name.generate_signed_post_policy_v4(test_bucket.name, blob_name)\n    assert f\"name='key' value='{blob_name}'\" in form\n    assert \"name='x-goog-signature'\" in form\n    assert \"name='x-goog-date'\" in form\n    assert \"name='x-goog-credential'\" in form\n    assert \"name='x-goog-algorithm' value='GOOG4-RSA-SHA256'\" in form\n    assert \"name='policy'\" in form\n    assert \"name='x-goog-meta-test' value='data'\" in form\n    assert \"type='file' name='file'/>\" in form\n\n\ndef test_rename_blob(test_blob):\n    bucket = storage.Client().bucket(test_blob.bucket.name)\n\n    try:\n        bucket.delete_blob(\"test_rename_blob\")\n    except google.cloud.exceptions.exceptions.NotFound:\n        print(f\"test_rename_blob not found in bucket {bucket.name}\")\n\n    storage_rename_file.rename_blob(bucket.name, test_blob.name, \"test_rename_blob\")\n\n    assert bucket.get_blob(\"test_rename_blob\") is not None\n    assert bucket.get_blob(test_blob.name) is None\n\n\ndef test_move_blob(test_bucket_create, test_blob):\n    bucket = test_blob.bucket\n    storage.Client().create_bucket(test_bucket_create)\n\n    try:\n        test_bucket_create.delete_blob(\"test_move_blob\")\n    except google.cloud.exceptions.NotFound:\n        print(f\"test_move_blob not found in bucket {test_bucket_create.name}\")\n\n    storage_move_file.move_blob(\n        bucket.name,\n        test_blob.name,\n        test_bucket_create.name,\n        \"test_move_blob\",\n    )\n\n    assert test_bucket_create.get_blob(\"test_move_blob\") is not None\n    assert bucket.get_blob(test_blob.name) is None\n\n\ndef test_copy_blob(test_blob):\n    bucket = storage.Client().bucket(test_blob.bucket.name)\n\n    try:\n        bucket.delete_blob(\"test_copy_blob\")\n    except google.cloud.exceptions.NotFound:\n        pass\n\n    storage_copy_file.copy_blob(\n        bucket.name,\n        test_blob.name,\n        bucket.name,\n        \"test_copy_blob\",\n    )\n\n    assert bucket.get_blob(\"test_copy_blob\") is not None\n    assert bucket.get_blob(test_blob.name) is not None\n\n\ndef test_versioning(test_bucket, capsys):\n    bucket = storage_enable_versioning.enable_versioning(test_bucket)\n    out, _ = capsys.readouterr()\n    assert \"Versioning was enabled for bucket\" in out\n    assert bucket.versioning_enabled is True\n\n    bucket = storage_disable_versioning.disable_versioning(test_bucket)\n    out, _ = capsys.readouterr()\n    assert \"Versioning was disabled for bucket\" in out\n    assert bucket.versioning_enabled is False\n\n\ndef test_get_set_autoclass(new_bucket_obj, test_bucket, capsys):\n    # Test default values when Autoclass is unset\n    bucket = storage_get_autoclass.get_autoclass(test_bucket.name)\n    out, _ = capsys.readouterr()\n    assert \"Autoclass enabled is set to False\" in out\n    assert bucket.autoclass_toggle_time is None\n    assert bucket.autoclass_terminal_storage_class_update_time is None\n\n    # Test enabling Autoclass at bucket creation\n    new_bucket_obj.autoclass_enabled = True\n    bucket = storage.Client().create_bucket(new_bucket_obj)\n    assert bucket.autoclass_enabled is True\n    assert bucket.autoclass_terminal_storage_class == \"NEARLINE\"\n\n    # Test set terminal_storage_class to ARCHIVE\n    bucket = storage_set_autoclass.set_autoclass(bucket.name)\n    out, _ = capsys.readouterr()\n    assert \"Autoclass enabled is set to True\" in out\n    assert bucket.autoclass_enabled is True\n    assert bucket.autoclass_terminal_storage_class == \"ARCHIVE\"\n\n    # Test get Autoclass\n    bucket = storage_get_autoclass.get_autoclass(bucket.name)\n    out, _ = capsys.readouterr()\n    assert \"Autoclass enabled is set to True\" in out\n    assert bucket.autoclass_toggle_time is not None\n    assert bucket.autoclass_terminal_storage_class_update_time is not None\n\n\ndef test_bucket_lifecycle_management(test_bucket, capsys):\n    bucket = (\n        storage_enable_bucket_lifecycle_management.enable_bucket_lifecycle_management(\n            test_bucket\n        )\n    )\n    out, _ = capsys.readouterr()\n    assert \"[]\" in out\n    assert \"Lifecycle management is enable\" in out\n    assert len(list(bucket.lifecycle_rules)) > 0\n\n    bucket = (\n        storage_disable_bucket_lifecycle_management.disable_bucket_lifecycle_management(\n            test_bucket\n        )\n    )\n    out, _ = capsys.readouterr()\n    assert \"[]\" in out\n    assert len(list(bucket.lifecycle_rules)) == 0\n\n\ndef test_create_bucket_class_location(test_bucket_create):\n    bucket = storage_create_bucket_class_location.create_bucket_class_location(\n        test_bucket_create.name\n    )\n\n    assert bucket.location == \"US\"\n    assert bucket.storage_class == \"COLDLINE\"\n\n\ndef test_create_bucket_dual_region(test_bucket_create, capsys):\n    location = \"US\"\n    region_1 = \"US-EAST1\"\n    region_2 = \"US-WEST1\"\n    storage_create_bucket_dual_region.create_bucket_dual_region(\n        test_bucket_create.name, location, region_1, region_2\n    )\n    out, _ = capsys.readouterr()\n    assert f\"Created bucket {test_bucket_create.name}\" in out\n    assert location in out\n    assert region_1 in out\n    assert region_2 in out\n    assert \"dual-region\" in out\n\n\ndef test_bucket_delete_default_kms_key(test_bucket, capsys):\n    test_bucket.default_kms_key_name = KMS_KEY\n    test_bucket.patch()\n\n    assert test_bucket.default_kms_key_name == KMS_KEY\n\n    bucket = storage_bucket_delete_default_kms_key.bucket_delete_default_kms_key(\n        test_bucket.name\n    )\n\n    out, _ = capsys.readouterr()\n    assert bucket.default_kms_key_name is None\n    assert bucket.name in out\n\n\ndef test_get_service_account(capsys):\n    storage_get_service_account.get_service_account()\n\n    out, _ = capsys.readouterr()\n\n    assert \"@gs-project-accounts.iam.gserviceaccount.com\" in out\n\n\ndef test_download_public_file(test_public_blob):\n    storage_make_public.make_blob_public(\n        test_public_blob.bucket.name, test_public_blob.name\n    )\n    with tempfile.NamedTemporaryFile() as dest_file:\n        storage_download_public_file.download_public_file(\n            test_public_blob.bucket.name, test_public_blob.name, dest_file.name\n        )\n\n        assert dest_file.read() == b\"Hello, is it me you're looking for?\"\n\n\ndef test_define_bucket_website_configuration(test_bucket):\n    bucket = (\n        storage_define_bucket_website_configuration.define_bucket_website_configuration(\n            test_bucket.name, \"index.html\", \"404.html\"\n        )\n    )\n\n    website_val = {\"mainPageSuffix\": \"index.html\", \"notFoundPage\": \"404.html\"}\n\n    assert bucket._properties[\"website\"] == website_val\n\n\ndef test_object_get_kms_key(test_bucket):\n    with tempfile.NamedTemporaryFile() as source_file:\n        storage_upload_with_kms_key.upload_blob_with_kms(\n            test_bucket.name,\n            source_file.name,\n            \"test_upload_blob_encrypted\",\n            KMS_KEY,\n        )\n    kms_key = storage_object_get_kms_key.object_get_kms_key(\n        test_bucket.name, \"test_upload_blob_encrypted\"\n    )\n\n    assert kms_key.startswith(KMS_KEY)\n\n\ndef test_storage_compose_file(test_bucket):\n    source_files = [\"test_upload_blob_1\", \"test_upload_blob_2\"]\n    for source in source_files:\n        blob = test_bucket.blob(source)\n        blob.upload_from_string(source)\n\n    with tempfile.NamedTemporaryFile() as dest_file:\n        destination = storage_compose_file.compose_file(\n            test_bucket.name,\n            source_files[0],\n            source_files[1],\n            dest_file.name,\n        )\n        composed = destination.download_as_bytes()\n\n        assert composed.decode(\"utf-8\") == source_files[0] + source_files[1]\n\n\ndef test_cors_configuration(test_bucket, capsys):\n    bucket = storage_cors_configuration.cors_configuration(test_bucket)\n    out, _ = capsys.readouterr()\n    assert \"Set CORS policies for bucket\" in out\n    assert len(bucket.cors) > 0\n\n    bucket = storage_remove_cors_configuration.remove_cors_configuration(test_bucket)\n    out, _ = capsys.readouterr()\n    assert \"Remove CORS policies for bucket\" in out\n    assert len(bucket.cors) == 0\n\n\ndef test_delete_blobs_archived_generation(test_blob, capsys):\n    storage_delete_file_archived_generation.delete_file_archived_generation(\n        test_blob.bucket.name, test_blob.name, test_blob.generation\n    )\n    out, _ = capsys.readouterr()\n    assert \"blob \" + test_blob.name + \" was deleted\" in out\n    blob = test_blob.bucket.get_blob(test_blob.name, generation=test_blob.generation)\n    assert blob is None\n\n\ndef test_change_default_storage_class(test_bucket, capsys):\n    bucket = storage_change_default_storage_class.change_default_storage_class(\n        test_bucket\n    )\n    out, _ = capsys.readouterr()\n    assert \"Default storage class for bucket\" in out\n    assert bucket.storage_class == \"COLDLINE\"\n\n\ndef test_change_file_storage_class(test_blob, capsys):\n    blob = storage_change_file_storage_class.change_file_storage_class(\n        test_blob.bucket.name,\n        test_blob.name,\n    )\n    out, _ = capsys.readouterr()\n    assert f\"Blob {blob.name} in bucket {blob.bucket.name}\" in out\n    assert blob.storage_class == \"NEARLINE\"\n\n\ndef test_copy_file_archived_generation(test_blob):\n    bucket = storage.Client().bucket(test_blob.bucket.name)\n\n    try:\n        bucket.delete_blob(\"test_copy_blob\")\n    except google.cloud.exceptions.NotFound:\n        pass\n\n    storage_copy_file_archived_generation.copy_file_archived_generation(\n        bucket.name, test_blob.name, bucket.name, \"test_copy_blob\", test_blob.generation\n    )\n\n    assert bucket.get_blob(\"test_copy_blob\") is not None\n    assert bucket.get_blob(test_blob.name) is not None\n\n\ndef test_list_blobs_archived_generation(test_blob, capsys):\n    storage_list_file_archived_generations.list_file_archived_generations(\n        test_blob.bucket.name\n    )\n    out, _ = capsys.readouterr()\n    assert str(test_blob.generation) in out\n\n\ndef test_storage_configure_retries(test_blob, capsys):\n    storage_configure_retries.configure_retries(test_blob.bucket.name, test_blob.name)\n\n    # This simply checks if the retry configurations were set and printed as intended.\n    out, _ = capsys.readouterr()\n    assert \"The following library method is customized to be retried\" in out\n    assert \"_should_retry\" in out\n    assert \"initial=1.5, maximum=45.0, multiplier=1.2\" in out\n    assert \"500\" in out  # \"deadline\" or \"timeout\" depending on dependency ver.\n\n\ndef test_batch_request(test_bucket):\n    blob1 = test_bucket.blob(\"b/1.txt\")\n    blob2 = test_bucket.blob(\"b/2.txt\")\n    blob1.upload_from_string(\"hello world\")\n    blob2.upload_from_string(\"hello world\")\n\n    storage_batch_request.batch_request(test_bucket.name, \"b/\")\n    blob1.reload()\n    blob2.reload()\n\n    assert blob1.metadata.get(\"your-metadata-key\") == \"your-metadata-value\"\n    assert blob2.metadata.get(\"your-metadata-key\") == \"your-metadata-value\"\n\n\ndef test_storage_set_client_endpoint(capsys):\n    storage_set_client_endpoint.set_client_endpoint(\"https://storage.googleapis.com\")\n    out, _ = capsys.readouterr()\n\n    assert \"client initiated with endpoint: https://storage.googleapis.com\" in out\n\n\ndef test_transfer_manager_snippets(test_bucket, capsys):\n    BLOB_NAMES = [\n        \"test.txt\",\n        \"test2.txt\",\n        \"blobs/test.txt\",\n        \"blobs/nesteddir/test.txt\",\n    ]\n\n    with tempfile.TemporaryDirectory() as uploads:\n        # Create dirs and nested dirs\n        for name in BLOB_NAMES:\n            relpath = os.path.dirname(name)\n            os.makedirs(os.path.join(uploads, relpath), exist_ok=True)\n\n        # Create files with nested dirs to exercise directory handling.\n        for name in BLOB_NAMES:\n            with open(os.path.join(uploads, name), \"w\") as f:\n                f.write(name)\n\n        storage_transfer_manager_upload_many.upload_many_blobs_with_transfer_manager(\n            test_bucket.name,\n            BLOB_NAMES,\n            source_directory=\"{}/\".format(uploads),\n            workers=8,\n        )\n        out, _ = capsys.readouterr()\n\n        for name in BLOB_NAMES:\n            assert \"Uploaded {}\".format(name) in out\n\n    with tempfile.TemporaryDirectory() as downloads:\n        # Download the files.\n        storage_transfer_manager_download_bucket.download_bucket_with_transfer_manager(\n            test_bucket.name,\n            destination_directory=os.path.join(downloads, \"\"),\n            workers=8,\n            max_results=10000,\n        )\n        out, _ = capsys.readouterr()\n\n        for name in BLOB_NAMES:\n            assert \"Downloaded {}\".format(name) in out\n\n    with tempfile.TemporaryDirectory() as downloads:\n        # Download the files.\n        storage_transfer_manager_download_many.download_many_blobs_with_transfer_manager(\n            test_bucket.name,\n            blob_names=BLOB_NAMES,\n            destination_directory=os.path.join(downloads, \"\"),\n            workers=8,\n        )\n        out, _ = capsys.readouterr()\n\n        for name in BLOB_NAMES:\n            assert \"Downloaded {}\".format(name) in out\n\n\ndef test_transfer_manager_directory_upload(test_bucket, capsys):\n    BLOB_NAMES = [\n        \"dirtest/test.txt\",\n        \"dirtest/test2.txt\",\n        \"dirtest/blobs/test.txt\",\n        \"dirtest/blobs/nesteddir/test.txt\",\n    ]\n\n    with tempfile.TemporaryDirectory() as uploads:\n        # Create dirs and nested dirs\n        for name in BLOB_NAMES:\n            relpath = os.path.dirname(name)\n            os.makedirs(os.path.join(uploads, relpath), exist_ok=True)\n\n        # Create files with nested dirs to exercise directory handling.\n        for name in BLOB_NAMES:\n            with open(os.path.join(uploads, name), \"w\") as f:\n                f.write(name)\n\n        storage_transfer_manager_upload_directory.upload_directory_with_transfer_manager(\n            test_bucket.name, source_directory=\"{}/\".format(uploads)\n        )\n        out, _ = capsys.readouterr()\n\n        assert \"Found {}\".format(len(BLOB_NAMES)) in out\n        for name in BLOB_NAMES:\n            assert \"Uploaded {}\".format(name) in out\n\n\ndef test_transfer_manager_download_chunks_concurrently(test_bucket, capsys):\n    BLOB_NAME = \"test_file.txt\"\n\n    with tempfile.NamedTemporaryFile() as file:\n        file.write(b\"test\")\n        file.flush()\n\n        storage_upload_file.upload_blob(test_bucket.name, file.name, BLOB_NAME)\n\n    with tempfile.TemporaryDirectory() as downloads:\n        # Download the file.\n        storage_transfer_manager_download_chunks_concurrently.download_chunks_concurrently(\n            test_bucket.name,\n            BLOB_NAME,\n            os.path.join(downloads, BLOB_NAME),\n            workers=8,\n        )\n        out, _ = capsys.readouterr()\n\n        assert (\n            \"Downloaded {} to {}\".format(BLOB_NAME, os.path.join(downloads, BLOB_NAME))\n            in out\n        )\n\n\ndef test_transfer_manager_upload_chunks_concurrently(test_bucket, capsys):\n    BLOB_NAME = \"test_file.txt\"\n\n    with tempfile.NamedTemporaryFile() as file:\n        file.write(b\"test\")\n        file.flush()\n\n        storage_transfer_manager_upload_chunks_concurrently.upload_chunks_concurrently(\n            test_bucket.name, file.name, BLOB_NAME\n        )\n\n        out, _ = capsys.readouterr()\n        assert \"File {} uploaded to {}\".format(file.name, BLOB_NAME) in out\n\n\ndef test_object_retention_policy(test_bucket_create, capsys):\n    storage_create_bucket_object_retention.create_bucket_object_retention(\n        test_bucket_create.name\n    )\n    out, _ = capsys.readouterr()\n    assert f\"Created bucket {test_bucket_create.name} with object retention enabled setting\" in out\n\n    blob_name = \"test_object_retention\"\n    storage_set_object_retention_policy.set_object_retention_policy(\n        test_bucket_create.name, \"hello world\", blob_name\n    )\n    out, _ = capsys.readouterr()\n    assert f\"Retention policy for file {blob_name}\" in out\n\n    # Remove retention policy for test cleanup\n    blob = test_bucket_create.blob(blob_name)\n    blob.retention.mode = None\n    blob.retention.retain_until_time = None\n    blob.patch(override_unlocked_retention=True)\n\n\ndef test_create_bucket_hierarchical_namespace(test_bucket_create, capsys):\n    storage_create_bucket_hierarchical_namespace.create_bucket_hierarchical_namespace(\n        test_bucket_create.name\n    )\n    out, _ = capsys.readouterr()\n    assert f\"Created bucket {test_bucket_create.name} with hierarchical namespace enabled\" in out\n", "samples/snippets/storage_disable_versioning.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_versioning]\nfrom google.cloud import storage\n\n\ndef disable_versioning(bucket_name):\n    \"\"\"Disable versioning for this bucket.\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.versioning_enabled = False\n    bucket.patch()\n\n    print(f\"Versioning was disabled for bucket {bucket}\")\n    return bucket\n\n\n# [END storage_disable_versioning]\n\nif __name__ == \"__main__\":\n    disable_versioning(bucket_name=sys.argv[1])\n", "samples/snippets/storage_delete_file_archived_generation.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_file_archived_generation]\nfrom google.cloud import storage\n\n\ndef delete_file_archived_generation(bucket_name, blob_name, generation):\n    \"\"\"Delete a blob in the bucket with the given generation.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # generation = 1579287380533984\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.delete_blob(blob_name, generation=generation)\n    print(\n        f\"Generation {generation} of blob {blob_name} was deleted from {bucket_name}\"\n    )\n\n\n# [END storage_delete_file_archived_generation]\n\n\nif __name__ == \"__main__\":\n    delete_file_archived_generation(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        generation=sys.argv[3]\n    )\n", "samples/snippets/storage_change_file_storage_class.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_change_file_storage_class]\nfrom google.cloud import storage\n\n\ndef change_file_storage_class(bucket_name, blob_name):\n    \"\"\"Change the default storage class of the blob\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    generation_match_precondition = None\n\n    # Optional: set a generation-match precondition to avoid potential race\n    # conditions and data corruptions. The request is aborted if the\n    # object's generation number does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = blob.generation\n\n    blob.update_storage_class(\"NEARLINE\", if_generation_match=generation_match_precondition)\n\n    print(\n        \"Blob {} in bucket {} had its storage class set to {}\".format(\n            blob_name,\n            bucket_name,\n            blob.storage_class\n        )\n    )\n    return blob\n# [END storage_change_file_storage_class]\n\n\nif __name__ == \"__main__\":\n    change_file_storage_class(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_download_byte_range.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_byte_range]\nfrom google.cloud import storage\n\n\ndef download_byte_range(\n    bucket_name, source_blob_name, start_byte, end_byte, destination_file_name\n):\n    \"\"\"Downloads a blob from the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object\n    # source_blob_name = \"storage-object-name\"\n\n    # The starting byte at which to begin the download\n    # start_byte = 0\n\n    # The ending byte at which to end the download\n    # end_byte = 20\n\n    # The path to which the file should be downloaded\n    # destination_file_name = \"local/path/to/file\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n    # any content from Google Cloud Storage. As we don't need additional data,\n    # using `Bucket.blob` is preferred here.\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name, start=start_byte, end=end_byte)\n\n    print(\n        \"Downloaded bytes {} to {} of object {} from bucket {} to local file {}.\".format(\n            start_byte, end_byte, source_blob_name, bucket_name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_byte_range]\n\nif __name__ == \"__main__\":\n    download_byte_range(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        start_byte=sys.argv[3],\n        end_byte=sys.argv[4],\n        destination_file_name=sys.argv[5],\n    )\n", "samples/snippets/storage_download_public_file.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_public_file]\nfrom google.cloud import storage\n\n\ndef download_public_file(bucket_name, source_blob_name, destination_file_name):\n    \"\"\"Downloads a public blob from the bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_blob_name = \"storage-object-name\"\n    # destination_file_name = \"local/path/to/file\"\n\n    storage_client = storage.Client.create_anonymous_client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        \"Downloaded public blob {} from bucket {} to {}.\".format(\n            source_blob_name, bucket.name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_public_file]\n\nif __name__ == \"__main__\":\n    download_public_file(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        destination_file_name=sys.argv[3],\n    )\n", "samples/snippets/storage_remove_bucket_label.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_remove_bucket_label]\nimport pprint\n# [END storage_remove_bucket_label]\nimport sys\n# [START storage_remove_bucket_label]\n\nfrom google.cloud import storage\n\n\ndef remove_bucket_label(bucket_name):\n    \"\"\"Remove a label from a bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    labels = bucket.labels\n\n    if \"example\" in labels:\n        del labels[\"example\"]\n\n    bucket.labels = labels\n    bucket.patch()\n\n    print(f\"Removed labels on {bucket.name}.\")\n    pprint.pprint(bucket.labels)\n\n\n# [END storage_remove_bucket_label]\n\nif __name__ == \"__main__\":\n    remove_bucket_label(bucket_name=sys.argv[1])\n", "samples/snippets/storage_transfer_manager_download_many.py": "# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_download_many]\ndef download_many_blobs_with_transfer_manager(\n    bucket_name, blob_names, destination_directory=\"\", workers=8\n):\n    \"\"\"Download blobs in a list by name, concurrently in a process pool.\n\n    The filename of each blob once downloaded is derived from the blob name and\n    the `destination_directory `parameter. For complete control of the filename\n    of each blob, use transfer_manager.download_many() instead.\n\n    Directories will be created automatically as needed to accommodate blob\n    names that include slashes.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The list of blob names to download. The names of each blobs will also\n    # be the name of each destination file (use transfer_manager.download_many()\n    # instead to control each destination file name). If there is a \"/\" in the\n    # blob name, then corresponding directories will be created on download.\n    # blob_names = [\"myblob\", \"myblob2\"]\n\n    # The directory on your computer to which to download all of the files. This\n    # string is prepended (with os.path.join()) to the name of each blob to form\n    # the full path. Relative paths and absolute paths are both accepted. An\n    # empty string means \"the current working directory\". Note that this\n    # parameter allows accepts directory traversal (\"../\" etc.) and is not\n    # intended for unsanitized end user input.\n    # destination_directory = \"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    results = transfer_manager.download_many_to_path(\n        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n    )\n\n    for name, result in zip(blob_names, results):\n        # The results list is either `None` or an exception for each blob in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to download {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n# [END storage_transfer_manager_download_many]\n", "samples/snippets/storage_fileio_write_read.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that writes and read a blob in GCS using file-like IO\n\"\"\"\n\n# [START storage_fileio_write_read]\nfrom google.cloud import storage\n\n\ndef write_read(bucket_name, blob_name):\n    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your new GCS object\n    # blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Mode can be specified as wb/rb for bytes mode.\n    # See: https://docs.python.org/3/library/io.html\n    with blob.open(\"w\") as f:\n        f.write(\"Hello world\")\n\n    with blob.open(\"r\") as f:\n        print(f.read())\n\n\n# [END storage_fileio_write_read]\n\nif __name__ == \"__main__\":\n    write_read(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2]\n    )\n", "samples/snippets/noxfile.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport glob\nimport os\nfrom pathlib import Path\nimport sys\nfrom typing import Callable, Dict, Optional\n\nimport nox\n\n\n# WARNING - WARNING - WARNING - WARNING - WARNING\n# WARNING - WARNING - WARNING - WARNING - WARNING\n#           DO NOT EDIT THIS FILE EVER!\n# WARNING - WARNING - WARNING - WARNING - WARNING\n# WARNING - WARNING - WARNING - WARNING - WARNING\n\nBLACK_VERSION = \"black==22.3.0\"\nISORT_VERSION = \"isort==5.10.1\"\n\n# Copy `noxfile_config.py` to your directory and modify it instead.\n\n# `TEST_CONFIG` dict is a configuration hook that allows users to\n# modify the test configurations. The values here should be in sync\n# with `noxfile_config.py`. Users will copy `noxfile_config.py` into\n# their directory and modify it.\n\nTEST_CONFIG = {\n    # You can opt out from the test for specific Python versions.\n    \"ignored_versions\": [],\n    # Old samples are opted out of enforcing Python type hints\n    # All new samples should feature them\n    \"enforce_type_hints\": False,\n    # An envvar key for determining the project id to use. Change it\n    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a\n    # build specific Cloud project. You can also use your own string\n    # to use your own Cloud project.\n    \"gcloud_project_env\": \"GOOGLE_CLOUD_PROJECT\",\n    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',\n    # If you need to use a specific version of pip,\n    # change pip_version_override to the string representation\n    # of the version number, for example, \"20.2.4\"\n    \"pip_version_override\": None,\n    # A dictionary you want to inject into your test. Don't put any\n    # secrets here. These values will override predefined values.\n    \"envs\": {},\n}\n\n\ntry:\n    # Ensure we can import noxfile_config in the project's directory.\n    sys.path.append(\".\")\n    from noxfile_config import TEST_CONFIG_OVERRIDE\nexcept ImportError as e:\n    print(\"No user noxfile_config found: detail: {}\".format(e))\n    TEST_CONFIG_OVERRIDE = {}\n\n# Update the TEST_CONFIG with the user supplied values.\nTEST_CONFIG.update(TEST_CONFIG_OVERRIDE)\n\n\ndef get_pytest_env_vars() -> Dict[str, str]:\n    \"\"\"Returns a dict for pytest invocation.\"\"\"\n    ret = {}\n\n    # Override the GCLOUD_PROJECT and the alias.\n    env_key = TEST_CONFIG[\"gcloud_project_env\"]\n    # This should error out if not set.\n    ret[\"GOOGLE_CLOUD_PROJECT\"] = os.environ[env_key]\n\n    # Apply user supplied envs.\n    ret.update(TEST_CONFIG[\"envs\"])\n    return ret\n\n\n# DO NOT EDIT - automatically generated.\n# All versions used to test samples.\nALL_VERSIONS = [\"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n# Any default versions that should be ignored.\nIGNORED_VERSIONS = TEST_CONFIG[\"ignored_versions\"]\n\nTESTED_VERSIONS = sorted([v for v in ALL_VERSIONS if v not in IGNORED_VERSIONS])\n\nINSTALL_LIBRARY_FROM_SOURCE = os.environ.get(\"INSTALL_LIBRARY_FROM_SOURCE\", False) in (\n    \"True\",\n    \"true\",\n)\n\n# Error if a python version is missing\nnox.options.error_on_missing_interpreters = True\n\n#\n# Style Checks\n#\n\n\n# Linting with flake8.\n#\n# We ignore the following rules:\n#   E203: whitespace before \u2018:\u2019\n#   E266: too many leading \u2018#\u2019 for block comment\n#   E501: line too long\n#   I202: Additional newline in a section of imports\n#\n# We also need to specify the rules which are ignored by default:\n# ['E226', 'W504', 'E126', 'E123', 'W503', 'E24', 'E704', 'E121']\nFLAKE8_COMMON_ARGS = [\n    \"--show-source\",\n    \"--builtin=gettext\",\n    \"--max-complexity=20\",\n    \"--exclude=.nox,.cache,env,lib,generated_pb2,*_pb2.py,*_pb2_grpc.py\",\n    \"--ignore=E121,E123,E126,E203,E226,E24,E266,E501,E704,W503,W504,I202\",\n    \"--max-line-length=88\",\n]\n\n\n@nox.session\ndef lint(session: nox.sessions.Session) -> None:\n    if not TEST_CONFIG[\"enforce_type_hints\"]:\n        session.install(\"flake8\")\n    else:\n        session.install(\"flake8\", \"flake8-annotations\")\n\n    args = FLAKE8_COMMON_ARGS + [\n        \".\",\n    ]\n    session.run(\"flake8\", *args)\n\n\n#\n# Black\n#\n\n\n@nox.session\ndef blacken(session: nox.sessions.Session) -> None:\n    \"\"\"Run black. Format code to uniform standard.\"\"\"\n    session.install(BLACK_VERSION)\n    python_files = [path for path in os.listdir(\".\") if path.endswith(\".py\")]\n\n    session.run(\"black\", *python_files)\n\n\n#\n# format = isort + black\n#\n\n@nox.session\ndef format(session: nox.sessions.Session) -> None:\n    \"\"\"\n    Run isort to sort imports. Then run black\n    to format code to uniform standard.\n    \"\"\"\n    session.install(BLACK_VERSION, ISORT_VERSION)\n    python_files = [path for path in os.listdir(\".\") if path.endswith(\".py\")]\n\n    # Use the --fss option to sort imports using strict alphabetical order.\n    # See https://pycqa.github.io/isort/docs/configuration/options.html#force-sort-within-sections\n    session.run(\"isort\", \"--fss\", *python_files)\n    session.run(\"black\", *python_files)\n\n\n#\n# Sample Tests\n#\n\n\nPYTEST_COMMON_ARGS = [\"--junitxml=sponge_log.xml\"]\n\n\ndef _session_tests(\n    session: nox.sessions.Session, post_install: Callable = None\n) -> None:\n    # check for presence of tests\n    test_list = glob.glob(\"**/*_test.py\", recursive=True) + glob.glob(\"**/test_*.py\", recursive=True)\n    test_list.extend(glob.glob(\"**/tests\", recursive=True))\n\n    if len(test_list) == 0:\n        print(\"No tests found, skipping directory.\")\n        return\n\n    if TEST_CONFIG[\"pip_version_override\"]:\n        pip_version = TEST_CONFIG[\"pip_version_override\"]\n        session.install(f\"pip=={pip_version}\")\n    \"\"\"Runs py.test for a particular project.\"\"\"\n    concurrent_args = []\n    if os.path.exists(\"requirements.txt\"):\n        if os.path.exists(\"constraints.txt\"):\n            session.install(\"-r\", \"requirements.txt\", \"-c\", \"constraints.txt\")\n        else:\n            session.install(\"-r\", \"requirements.txt\")\n        with open(\"requirements.txt\") as rfile:\n            packages = rfile.read()\n\n    if os.path.exists(\"requirements-test.txt\"):\n        if os.path.exists(\"constraints-test.txt\"):\n            session.install(\n                \"-r\", \"requirements-test.txt\", \"-c\", \"constraints-test.txt\"\n            )\n        else:\n            session.install(\"-r\", \"requirements-test.txt\")\n        with open(\"requirements-test.txt\") as rtfile:\n            packages += rtfile.read()\n\n    if INSTALL_LIBRARY_FROM_SOURCE:\n        session.install(\"-e\", _get_repo_root())\n\n    if post_install:\n        post_install(session)\n\n    if \"pytest-parallel\" in packages:\n        concurrent_args.extend(['--workers', 'auto', '--tests-per-worker', 'auto'])\n    elif \"pytest-xdist\" in packages:\n        concurrent_args.extend(['-n', 'auto'])\n\n    session.run(\n        \"pytest\",\n        *(PYTEST_COMMON_ARGS + session.posargs + concurrent_args),\n        # Pytest will return 5 when no tests are collected. This can happen\n        # on travis where slow and flaky tests are excluded.\n        # See http://doc.pytest.org/en/latest/_modules/_pytest/main.html\n        success_codes=[0, 5],\n        env=get_pytest_env_vars(),\n    )\n\n\n@nox.session(python=ALL_VERSIONS)\ndef py(session: nox.sessions.Session) -> None:\n    \"\"\"Runs py.test for a sample using the specified version of Python.\"\"\"\n    if session.python in TESTED_VERSIONS:\n        _session_tests(session)\n    else:\n        session.skip(\n            \"SKIPPED: {} tests are disabled for this sample.\".format(session.python)\n        )\n\n\n#\n# Readmegen\n#\n\n\ndef _get_repo_root() -> Optional[str]:\n    \"\"\" Returns the root folder of the project. \"\"\"\n    # Get root of this repository. Assume we don't have directories nested deeper than 10 items.\n    p = Path(os.getcwd())\n    for i in range(10):\n        if p is None:\n            break\n        if Path(p / \".git\").exists():\n            return str(p)\n        # .git is not available in repos cloned via Cloud Build\n        # setup.py is always in the library's root, so use that instead\n        # https://github.com/googleapis/synthtool/issues/792\n        if Path(p / \"setup.py\").exists():\n            return str(p)\n        p = p.parent\n    raise Exception(\"Unable to detect repository root.\")\n\n\nGENERATED_READMES = sorted([x for x in Path(\".\").rglob(\"*.rst.in\")])\n\n\n@nox.session\n@nox.parametrize(\"path\", GENERATED_READMES)\ndef readmegen(session: nox.sessions.Session, path: str) -> None:\n    \"\"\"(Re-)generates the readme for a sample.\"\"\"\n    session.install(\"jinja2\", \"pyyaml\")\n    dir_ = os.path.dirname(path)\n\n    if os.path.exists(os.path.join(dir_, \"requirements.txt\")):\n        session.install(\"-r\", os.path.join(dir_, \"requirements.txt\"))\n\n    in_file = os.path.join(dir_, \"README.rst.in\")\n    session.run(\n        \"python\", _get_repo_root() + \"/scripts/readme-gen/readme_gen.py\", in_file\n    )\n", "samples/snippets/storage_object_csek_to_cmek.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport sys\n\n# [START storage_object_csek_to_cmek]\nfrom google.cloud import storage\n\n\ndef object_csek_to_cmek(bucket_name, blob_name, encryption_key, kms_key_name):\n    \"\"\"Change a blob's customer-supplied encryption key to KMS key\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # encryption_key = \"TIbv/fjexq+VmtXzAlc63J4z5kFmWJ6NdAPQulQBT7g=\"\n    # kms_key_name = \"projects/PROJ/locations/LOC/keyRings/RING/cryptoKey/KEY\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    current_encryption_key = base64.b64decode(encryption_key)\n    source_blob = bucket.blob(blob_name, encryption_key=current_encryption_key)\n    destination_blob = bucket.blob(blob_name, kms_key_name=kms_key_name)\n    generation_match_precondition = None\n    token = None\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to rewrite is aborted if the object's\n    # generation number does not match your precondition.\n    source_blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = source_blob.generation\n\n    while True:\n        token, bytes_rewritten, total_bytes = destination_blob.rewrite(\n            source_blob, token=token, if_generation_match=generation_match_precondition\n        )\n        if token is None:\n            break\n\n    print(\n        \"Blob {} in bucket {} is now managed by the KMS key {} instead of a customer-supplied encryption key\".format(\n            blob_name, bucket_name, kms_key_name\n        )\n    )\n    return destination_blob\n\n\n# [END storage_object_csek_to_cmek]\n\nif __name__ == \"__main__\":\n    object_csek_to_cmek(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        encryption_key=sys.argv[3],\n        kms_key_name=sys.argv[4],\n    )\n", "samples/snippets/storage_remove_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_retention_policy]\nfrom google.cloud import storage\n\n\ndef remove_retention_policy(bucket_name):\n    \"\"\"Removes the retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket.reload()\n\n    if bucket.retention_policy_locked:\n        print(\n            \"Unable to remove retention period as retention policy is locked.\"\n        )\n        return\n\n    bucket.retention_period = None\n    bucket.patch()\n\n    print(f\"Removed bucket {bucket.name} retention policy\")\n\n\n# [END storage_remove_retention_policy]\n\n\nif __name__ == \"__main__\":\n    remove_retention_policy(bucket_name=sys.argv[1])\n", "samples/snippets/storage_download_file_requester_pays.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_file_requester_pays]\nfrom google.cloud import storage\n\n\ndef download_file_requester_pays(\n    bucket_name, project_id, source_blob_name, destination_file_name\n):\n    \"\"\"Download file using specified project as the requester\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # project_id = \"your-project-id\"\n    # source_blob_name = \"source-blob-name\"\n    # destination_file_name = \"local-destination-file-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name, user_project=project_id)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        \"Blob {} downloaded to {} using a requester-pays request.\".format(\n            source_blob_name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_file_requester_pays]\n\nif __name__ == \"__main__\":\n    download_file_requester_pays(\n        bucket_name=sys.argv[1],\n        project_id=sys.argv[2],\n        source_blob_name=sys.argv[3],\n        destination_file_name=sys.argv[4],\n    )\n", "samples/snippets/acl_test.py": "# Copyright 2016 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport uuid\n\nimport backoff\nfrom google.api_core.exceptions import GoogleAPIError\nfrom google.cloud import storage\nimport pytest\n\nimport storage_add_bucket_default_owner\nimport storage_add_bucket_owner\nimport storage_add_file_owner\nimport storage_print_bucket_acl\nimport storage_print_bucket_acl_for_user\nimport storage_print_file_acl\nimport storage_print_file_acl_for_user\nimport storage_remove_bucket_default_owner\nimport storage_remove_bucket_owner\nimport storage_remove_file_owner\n\n# Typically we'd use a @example.com address, but GCS requires a real Google\n# account. Retrieve a service account email with storage admin permissions.\nTEST_EMAIL = \"py38-storage-test\" \"@python-docs-samples-tests.iam.gserviceaccount.com\"\n\n\n@pytest.fixture(scope=\"module\")\ndef test_bucket():\n    \"\"\"Yields a bucket that is deleted after the test completes.\"\"\"\n\n    # The new projects have uniform bucket-level access and our tests don't\n    # pass with those buckets. We need to use the old main project for now.\n    original_value = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = os.environ[\"MAIN_GOOGLE_CLOUD_PROJECT\"]\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"acl-test-{uuid.uuid4()}\"\n        bucket = storage.Client().bucket(bucket_name)\n    bucket.create()\n    yield bucket\n    bucket.delete(force=True)\n    # Set the value back.\n    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = original_value\n\n\n@pytest.fixture\ndef test_blob(test_bucket):\n    \"\"\"Yields a blob that is deleted after the test completes.\"\"\"\n    bucket = test_bucket\n    blob = bucket.blob(f\"storage_acl_test_sigil-{uuid.uuid4()}\")\n    blob.upload_from_string(\"Hello, is it me you're looking for?\")\n    yield blob\n\n\ndef test_print_bucket_acl(test_bucket, capsys):\n    storage_print_bucket_acl.print_bucket_acl(test_bucket.name)\n    out, _ = capsys.readouterr()\n    assert out\n\n\ndef test_print_bucket_acl_for_user(test_bucket, capsys):\n    test_bucket.acl.user(TEST_EMAIL).grant_owner()\n    test_bucket.acl.save()\n\n    storage_print_bucket_acl_for_user.print_bucket_acl_for_user(\n        test_bucket.name, TEST_EMAIL\n    )\n\n    out, _ = capsys.readouterr()\n    assert \"OWNER\" in out\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_add_bucket_owner(test_bucket):\n    storage_add_bucket_owner.add_bucket_owner(test_bucket.name, TEST_EMAIL)\n\n    test_bucket.acl.reload()\n    assert \"OWNER\" in test_bucket.acl.user(TEST_EMAIL).get_roles()\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_remove_bucket_owner(test_bucket):\n    test_bucket.acl.user(TEST_EMAIL).grant_owner()\n    test_bucket.acl.save()\n\n    storage_remove_bucket_owner.remove_bucket_owner(test_bucket.name, TEST_EMAIL)\n\n    test_bucket.acl.reload()\n    assert \"OWNER\" not in test_bucket.acl.user(TEST_EMAIL).get_roles()\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_add_bucket_default_owner(test_bucket):\n    storage_add_bucket_default_owner.add_bucket_default_owner(\n        test_bucket.name, TEST_EMAIL\n    )\n\n    test_bucket.default_object_acl.reload()\n    roles = test_bucket.default_object_acl.user(TEST_EMAIL).get_roles()\n    assert \"OWNER\" in roles\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_remove_bucket_default_owner(test_bucket):\n    test_bucket.acl.user(TEST_EMAIL).grant_owner()\n    test_bucket.acl.save()\n\n    storage_remove_bucket_default_owner.remove_bucket_default_owner(\n        test_bucket.name, TEST_EMAIL\n    )\n\n    test_bucket.default_object_acl.reload()\n    roles = test_bucket.default_object_acl.user(TEST_EMAIL).get_roles()\n    assert \"OWNER\" not in roles\n\n\ndef test_print_blob_acl(test_blob, capsys):\n    storage_print_file_acl.print_blob_acl(test_blob.bucket.name, test_blob.name)\n    out, _ = capsys.readouterr()\n    assert out\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_print_blob_acl_for_user(test_blob, capsys):\n    test_blob.acl.user(TEST_EMAIL).grant_owner()\n    test_blob.acl.save()\n\n    storage_print_file_acl_for_user.print_blob_acl_for_user(\n        test_blob.bucket.name, test_blob.name, TEST_EMAIL\n    )\n\n    out, _ = capsys.readouterr()\n    assert \"OWNER\" in out\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_add_blob_owner(test_blob):\n    storage_add_file_owner.add_blob_owner(\n        test_blob.bucket.name, test_blob.name, TEST_EMAIL\n    )\n\n    test_blob.acl.reload()\n    assert \"OWNER\" in test_blob.acl.user(TEST_EMAIL).get_roles()\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_remove_blob_owner(test_blob):\n    test_blob.acl.user(TEST_EMAIL).grant_owner()\n    test_blob.acl.save()\n\n    storage_remove_file_owner.remove_blob_owner(\n        test_blob.bucket.name, test_blob.name, TEST_EMAIL\n    )\n\n    test_blob.acl.reload()\n    assert \"OWNER\" not in test_blob.acl.user(TEST_EMAIL).get_roles()\n", "samples/snippets/storage_enable_versioning.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_versioning]\nfrom google.cloud import storage\n\n\ndef enable_versioning(bucket_name):\n    \"\"\"Enable versioning for this bucket.\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.versioning_enabled = True\n    bucket.patch()\n\n    print(f\"Versioning was enabled for bucket {bucket.name}\")\n    return bucket\n\n\n# [END storage_enable_versioning]\n\nif __name__ == \"__main__\":\n    enable_versioning(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_default_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_default_event_based_hold]\nfrom google.cloud import storage\n\n\ndef get_default_event_based_hold(bucket_name):\n    \"\"\"Gets the default event based hold on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n\n    if bucket.default_event_based_hold:\n        print(f\"Default event-based hold is enabled for {bucket_name}\")\n    else:\n        print(\n            f\"Default event-based hold is not enabled for {bucket_name}\"\n        )\n\n\n# [END storage_get_default_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    get_default_event_based_hold(bucket_name=sys.argv[1])\n", "samples/snippets/storage_disable_requester_pays.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_requester_pays]\nfrom google.cloud import storage\n\n\ndef disable_requester_pays(bucket_name):\n    \"\"\"Disable a bucket's requesterpays metadata\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.requester_pays = False\n    bucket.patch()\n\n    print(f\"Requester Pays has been disabled for {bucket_name}\")\n\n\n# [END storage_disable_requester_pays]\n\n\nif __name__ == \"__main__\":\n    disable_requester_pays(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket_object_retention.py": "#!/usr/bin/env python\n\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket_with_object_retention]\nfrom google.cloud import storage\n\n\ndef create_bucket_object_retention(bucket_name):\n    \"\"\"Creates a bucket with object retention enabled.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.create_bucket(bucket_name, enable_object_retention=True)\n\n    print(f\"Created bucket {bucket_name} with object retention enabled setting: {bucket.object_retention_mode}\")\n\n\n# [END storage_create_bucket_with_object_retention]\n\n\nif __name__ == \"__main__\":\n    create_bucket_object_retention(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket_class_location.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket_class_location]\nfrom google.cloud import storage\n\n\ndef create_bucket_class_location(bucket_name):\n    \"\"\"\n    Create a new bucket in the US region with the coldline storage\n    class\n    \"\"\"\n    # bucket_name = \"your-new-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    bucket.storage_class = \"COLDLINE\"\n    new_bucket = storage_client.create_bucket(bucket, location=\"us\")\n\n    print(\n        \"Created bucket {} in {} with storage class {}\".format(\n            new_bucket.name, new_bucket.location, new_bucket.storage_class\n        )\n    )\n    return new_bucket\n\n\n# [END storage_create_bucket_class_location]\n\nif __name__ == \"__main__\":\n    create_bucket_class_location(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_from_memory.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_file_upload_from_memory]\nfrom google.cloud import storage\n\n\ndef upload_blob_from_memory(bucket_name, contents, destination_blob_name):\n    \"\"\"Uploads a file to the bucket.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The contents to upload to the file\n    # contents = \"these are my contents\"\n\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    blob.upload_from_string(contents)\n\n    print(\n        f\"{destination_blob_name} with contents {contents} uploaded to {bucket_name}.\"\n    )\n\n# [END storage_file_upload_from_memory]\n\n\nif __name__ == \"__main__\":\n    upload_blob_from_memory(\n        bucket_name=sys.argv[1],\n        contents=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n    )\n", "samples/snippets/storage_remove_bucket_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_owner]\nfrom google.cloud import storage\n\n\ndef remove_bucket_owner(bucket_name, user_email):\n    \"\"\"Removes a user from the access control list of the given bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # remove access for different types of entities.\n    bucket.acl.user(user_email).revoke_read()\n    bucket.acl.user(user_email).revoke_write()\n    bucket.acl.user(user_email).revoke_owner()\n    bucket.acl.save()\n\n    print(f\"Removed user {user_email} from bucket {bucket_name}.\")\n\n\n# [END storage_remove_bucket_owner]\n\nif __name__ == \"__main__\":\n    remove_bucket_owner(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_set_public_access_prevention_inherited.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that sets public access prevention to inherited.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/using-public-access-prevention\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_set_public_access_prevention_inherited]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_INHERITED\n\n\ndef set_public_access_prevention_inherited(bucket_name):\n    \"\"\"Sets the public access prevention status to inherited, so that the bucket inherits its setting from its parent project.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.public_access_prevention = (\n        PUBLIC_ACCESS_PREVENTION_INHERITED\n    )\n    bucket.patch()\n\n    print(f\"Public access prevention is 'inherited' for {bucket.name}.\")\n\n\n# [END storage_set_public_access_prevention_inherited]\n\nif __name__ == \"__main__\":\n    set_public_access_prevention_inherited(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket_turbo_replication.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates a new bucket with dual-region and turbo replication.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_create_bucket_turbo_replication]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import RPO_ASYNC_TURBO\n\n\ndef create_bucket_turbo_replication(bucket_name):\n    \"\"\"Creates dual-region bucket with turbo replication enabled.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    bucket_location = \"NAM4\"\n    bucket.rpo = RPO_ASYNC_TURBO\n    bucket.create(location=bucket_location)\n\n    print(f\"{bucket.name} created with the recovery point objective (RPO) set to {bucket.rpo} in {bucket.location}.\")\n\n\n# [END storage_create_bucket_turbo_replication]\n\nif __name__ == \"__main__\":\n    create_bucket_turbo_replication(bucket_name=sys.argv[1])\n", "samples/snippets/storage_activate_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_activate_hmac_key]\nfrom google.cloud import storage\n\n\ndef activate_key(access_id, project_id):\n    \"\"\"\n    Activate the HMAC key with the given access ID.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an inactive HMAC key\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n    hmac_key.state = \"ACTIVE\"\n    hmac_key.update()\n\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_activate_hmac_key]\n\nif __name__ == \"__main__\":\n    activate_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_list_hmac_keys.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_hmac_keys]\nfrom google.cloud import storage\n\n\ndef list_keys(project_id):\n    \"\"\"\n    List all HMAC keys associated with the project.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n\n    storage_client = storage.Client(project=project_id)\n    hmac_keys = storage_client.list_hmac_keys(project_id=project_id)\n    print(\"HMAC Keys:\")\n    for hmac_key in hmac_keys:\n        print(\n            f\"Service Account Email: {hmac_key.service_account_email}\"\n        )\n        print(f\"Access ID: {hmac_key.access_id}\")\n    return hmac_keys\n\n\n# [END storage_list_hmac_keys]\n\nif __name__ == \"__main__\":\n    list_keys(project_id=sys.argv[1])\n", "samples/snippets/storage_fileio_pandas.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates and consumes a GCS blob using pandas with file-like IO\n\"\"\"\n\n# [START storage_fileio_pandas_write]\n\n\ndef pandas_write(bucket_name, blob_name):\n    \"\"\"Use pandas to interact with GCS using file-like IO\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your new GCS object\n    # blob_name = \"storage-object-name\"\n\n    from google.cloud import storage\n    import pandas as pd\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    with blob.open(\"w\") as f:\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n        f.write(df.to_csv(index=False))\n\n    print(f\"Wrote csv with pandas with name {blob_name} from bucket {bucket.name}.\")\n\n\n# [END storage_fileio_pandas_write]\n\n\n# [START storage_fileio_pandas_read]\n\n\ndef pandas_read(bucket_name, blob_name):\n    \"\"\"Use pandas to interact with GCS using file-like IO\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your new GCS object\n    # blob_name = \"storage-object-name\"\n\n    from google.cloud import storage\n    import pandas as pd\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    with blob.open(\"r\") as f:\n        pd.read_csv(f)\n\n    print(f\"Read csv with pandas with name {blob_name} from bucket {bucket.name}.\")\n\n\n# [END storage_fileio_pandas_read]\n\n\nif __name__ == \"__main__\":\n    pandas_write(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2]\n    )\n\n    pandas_read(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_add_bucket_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_bucket_owner]\nfrom google.cloud import storage\n\n\ndef add_bucket_owner(bucket_name, user_email):\n    \"\"\"Adds a user as an owner on the given bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    bucket.acl.reload()\n\n    # You can also use `group()`, `domain()`, `all_authenticated()` and `all()`\n    # to grant access to different types of entities.\n    # You can also use `grant_read()` or `grant_write()` to grant different\n    # roles.\n    bucket.acl.user(user_email).grant_owner()\n    bucket.acl.save()\n\n    print(\n        f\"Added user {user_email} as an owner on bucket {bucket_name}.\"\n    )\n\n\n# [END storage_add_bucket_owner]\n\nif __name__ == \"__main__\":\n    add_bucket_owner(bucket_name=sys.argv[1], user_email=sys.argv[2])\n", "samples/snippets/storage_remove_bucket_conditional_iam_binding.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_remove_bucket_conditional_iam_binding]\nfrom google.cloud import storage\n\n\ndef remove_bucket_conditional_iam_binding(\n    bucket_name, role, title, description, expression\n):\n    \"\"\"Remove a conditional IAM binding from a bucket's IAM policy.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # role = \"IAM role, e.g. roles/storage.objectViewer\"\n    # title = \"Condition title.\"\n    # description = \"Condition description.\"\n    # expression = \"Condition expression.\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    # Set the policy's version to 3 to use condition in bindings.\n    policy.version = 3\n\n    condition = {\n        \"title\": title,\n        \"description\": description,\n        \"expression\": expression,\n    }\n    policy.bindings = [\n        binding\n        for binding in policy.bindings\n        if not (binding[\"role\"] == role and binding.get(\"condition\") == condition)\n    ]\n\n    bucket.set_iam_policy(policy)\n\n    print(\"Conditional Binding was removed.\")\n\n\n# [END storage_remove_bucket_conditional_iam_binding]\n\n\nif __name__ == \"__main__\":\n    remove_bucket_conditional_iam_binding(\n        bucket_name=sys.argv[1],\n        role=sys.argv[2],\n        title=sys.argv[3],\n        description=sys.argv[4],\n        expression=sys.argv[5],\n    )\n", "samples/snippets/storage_copy_file_archived_generation.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_copy_file_archived_generation]\nfrom google.cloud import storage\n\n\ndef copy_file_archived_generation(\n        bucket_name, blob_name, destination_bucket_name, destination_blob_name, generation\n):\n    \"\"\"Copies a blob from one bucket to another with a new name with the same generation.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # destination_bucket_name = \"destination-bucket-name\"\n    # destination_blob_name = \"destination-object-name\"\n    # generation = 1579287380533984\n\n    storage_client = storage.Client()\n\n    source_bucket = storage_client.bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.bucket(destination_bucket_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to copy is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    destination_generation_match_precondition = 0\n\n    # source_generation selects a specific revision of the source object, as opposed to the latest version.\n    blob_copy = source_bucket.copy_blob(\n        source_blob, destination_bucket, destination_blob_name, source_generation=generation, if_generation_match=destination_generation_match_precondition\n    )\n\n    print(\n        \"Generation {} of the blob {} in bucket {} copied to blob {} in bucket {}.\".format(\n            generation,\n            source_blob.name,\n            source_bucket.name,\n            blob_copy.name,\n            destination_bucket.name,\n        )\n    )\n\n\n# [END storage_copy_file_archived_generation]\n\nif __name__ == \"__main__\":\n    copy_file_archived_generation(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        destination_bucket_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n        generation=sys.argv[5]\n    )\n", "samples/snippets/storage_generate_upload_signed_url_v4.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_generate_upload_signed_url_v4]\nimport datetime\n# [END storage_generate_upload_signed_url_v4]\nimport sys\n# [START storage_generate_upload_signed_url_v4]\n\nfrom google.cloud import storage\n\n\ndef generate_upload_signed_url_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 signed URL for uploading a blob using HTTP PUT.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        version=\"v4\",\n        # This URL is valid for 15 minutes\n        expiration=datetime.timedelta(minutes=15),\n        # Allow PUT requests using this URL.\n        method=\"PUT\",\n        content_type=\"application/octet-stream\",\n    )\n\n    print(\"Generated PUT signed URL:\")\n    print(url)\n    print(\"You can use this URL with any user agent, for example:\")\n    print(\n        \"curl -X PUT -H 'Content-Type: application/octet-stream' \"\n        \"--upload-file my-file '{}'\".format(url)\n    )\n    return url\n\n\n# [END storage_generate_upload_signed_url_v4]\n\n\nif __name__ == \"__main__\":\n    generate_upload_signed_url_v4(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2]\n    )\n", "samples/snippets/notification_test.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport uuid\n\nfrom google.api_core.exceptions import NotFound\nfrom google.cloud import storage\n\nimport pytest\n\nimport storage_create_bucket_notifications\nimport storage_delete_bucket_notification\nimport storage_list_bucket_notifications\nimport storage_print_pubsub_bucket_notification\n\n_topic_name = f\"notification-{uuid.uuid4()}\"\n\n\n@pytest.fixture(scope=\"module\")\ndef storage_client():\n    return storage.Client()\n\n\n@pytest.fixture(scope=\"module\")\ndef publisher_client():\n    try:\n        from google.cloud.pubsub_v1 import PublisherClient\n    except ImportError:\n        pytest.skip(\"Cannot import pubsub\")\n\n    return PublisherClient()\n\n\n@pytest.fixture(scope=\"module\")\ndef _notification_topic(storage_client, publisher_client):\n    topic_path = publisher_client.topic_path(storage_client.project, _topic_name)\n    try:\n        topic = publisher_client.get_topic(request={\"topic\": topic_path})\n    except NotFound:\n        topic = publisher_client.create_topic(request={\"name\": topic_path})\n\n    policy = publisher_client.get_iam_policy(request={\"resource\": topic_path})\n    binding = policy.bindings.add()\n    binding.role = \"roles/pubsub.publisher\"\n    binding.members.append(\n        f\"serviceAccount:{storage_client.get_service_account_email()}\"\n    )\n    publisher_client.set_iam_policy(request={\"resource\": topic_path, \"policy\": policy})\n\n    yield topic\n\n    try:\n        publisher_client.delete_topic(request={\"topic\": topic.name})\n    except NotFound:\n        pass\n\n\n@pytest.fixture(scope=\"module\")\ndef bucket_w_notification(storage_client, _notification_topic):\n    \"\"\"Yields a bucket with notification that is deleted after the tests complete.\"\"\"\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"notification-test-{uuid.uuid4()}\"\n        bucket = storage_client.bucket(bucket_name)\n    bucket.create()\n\n    notification = bucket.notification(topic_name=_topic_name)\n    notification.create()\n\n    yield bucket\n\n    bucket.delete(force=True)\n\n\ndef test_list_bucket_notifications(bucket_w_notification, capsys):\n    storage_list_bucket_notifications.list_bucket_notifications(bucket_w_notification.name)\n    out, _ = capsys.readouterr()\n    assert \"Notification ID\" in out\n\n\ndef test_print_pubsub_bucket_notification(bucket_w_notification, capsys):\n    notification_id = 1\n    storage_print_pubsub_bucket_notification.print_pubsub_bucket_notification(bucket_w_notification.name, notification_id)\n    out, _ = capsys.readouterr()\n    assert \"Notification ID: 1\" in out\n\n\ndef test_create_bucket_notifications(bucket_w_notification, capsys):\n    # test only bucket notification ID 1 was created in the fixture\n    assert bucket_w_notification.notification(notification_id=1).exists() is True\n    assert bucket_w_notification.notification(notification_id=2).exists() is False\n\n    storage_create_bucket_notifications.create_bucket_notifications(bucket_w_notification.name, _topic_name)\n    out, _ = capsys.readouterr()\n    assert \"Successfully created notification\" in out\n    # test succesfully creates new bucket notification with ID 2\n    assert bucket_w_notification.notification(notification_id=2).exists() is True\n\n\ndef test_delete_bucket_notification(bucket_w_notification, capsys):\n    # test bucket notification ID 1 was created in the fixture\n    notification_id = 1\n    assert bucket_w_notification.notification(notification_id=notification_id).exists() is True\n\n    storage_delete_bucket_notification.delete_bucket_notification(bucket_w_notification.name, notification_id)\n    out, _ = capsys.readouterr()\n    assert \"Successfully deleted notification\" in out\n    assert bucket_w_notification.notification(notification_id=notification_id).exists() is False\n", "samples/snippets/storage_rotate_encryption_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_rotate_encryption_key]\nimport base64\n# [END storage_rotate_encryption_key]\nimport sys\n# [START storage_rotate_encryption_key]\n\nfrom google.cloud import storage\n\n\ndef rotate_encryption_key(\n    bucket_name, blob_name, base64_encryption_key, base64_new_encryption_key\n):\n    \"\"\"Performs a key rotation by re-writing an encrypted blob with a new\n    encryption key.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    current_encryption_key = base64.b64decode(base64_encryption_key)\n    new_encryption_key = base64.b64decode(base64_new_encryption_key)\n\n    # Both source_blob and destination_blob refer to the same storage object,\n    # but destination_blob has the new encryption key.\n    source_blob = bucket.blob(\n        blob_name, encryption_key=current_encryption_key\n    )\n    destination_blob = bucket.blob(\n        blob_name, encryption_key=new_encryption_key\n    )\n    generation_match_precondition = None\n    token = None\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to rewrite is aborted if the object's\n    # generation number does not match your precondition.\n    source_blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n    generation_match_precondition = source_blob.generation\n\n    while True:\n        token, bytes_rewritten, total_bytes = destination_blob.rewrite(\n            source_blob, token=token, if_generation_match=generation_match_precondition\n        )\n        if token is None:\n            break\n\n    print(f\"Key rotation complete for Blob {blob_name}\")\n\n\n# [END storage_rotate_encryption_key]\n\nif __name__ == \"__main__\":\n    rotate_encryption_key(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        base64_encryption_key=sys.argv[3],\n        base64_new_encryption_key=sys.argv[4],\n    )\n", "samples/snippets/storage_set_rpo_async_turbo.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that sets RPO (Recovery Point Objective) to ASYNC_TURBO\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_set_rpo_async_turbo]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import RPO_ASYNC_TURBO\n\n\ndef set_rpo_async_turbo(bucket_name):\n    \"\"\"Sets the RPO to ASYNC_TURBO, enabling the turbo replication feature\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.rpo = RPO_ASYNC_TURBO\n    bucket.patch()\n\n    print(f\"RPO is set to ASYNC_TURBO for {bucket.name}.\")\n\n\n# [END storage_set_rpo_async_turbo]\n\nif __name__ == \"__main__\":\n    set_rpo_async_turbo(bucket_name=sys.argv[1])\n", "samples/snippets/noxfile_config.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Default TEST_CONFIG_OVERRIDE for python repos.\n\n# You can copy this file into your directory, then it will be imported from\n# the noxfile.py.\n\n# The source of truth:\n# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/noxfile_config.py\n\nimport os\n\n\n# We are reaching maximum number of HMAC keys on the service account.\n# We change the service account based on the value of\n# RUN_TESTS_SESSION. The reason we can not use multiple project is\n# that our new projects are enforced to have\n# 'constraints/iam.disableServiceAccountKeyCreation' policy.\ndef get_service_account_email():\n    session = os.environ.get('RUN_TESTS_SESSION')\n    if session == 'py-3.6':\n        return ('py36-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.7':\n        return ('py37-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.8':\n        return ('py38-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.9':\n        return ('py39-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    if session == 'py-3.10':\n        return ('py310-storage-test@'\n                'python-docs-samples-tests.iam.gserviceaccount.com')\n    return os.environ['HMAC_KEY_TEST_SERVICE_ACCOUNT']\n\n\n# We change the value of CLOUD_KMS_KEY based on the value of\n# RUN_TESTS_SESSION.\ndef get_cloud_kms_key():\n    session = os.environ.get('RUN_TESTS_SESSION')\n    if session == 'py-3.6':\n        return ('projects/python-docs-samples-tests-py36/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.7':\n        return ('projects/python-docs-samples-tests-py37/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.8':\n        return ('projects/python-docs-samples-tests-py38/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.9':\n        return ('projects/python-docs-samples-tests-py39/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.10':\n        return ('projects/python-docs-samples-tests-310/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.11':\n        return ('projects/python-docs-samples-tests-311/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    if session == 'py-3.12':\n        return ('projects/python-docs-samples-tests-312/locations/us/'\n                'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    return os.environ['CLOUD_KMS_KEY']\n\n\nTEST_CONFIG_OVERRIDE = {\n    # You can opt out from the test for specific Python versions.\n    'ignored_versions': [\"2.7\", \"3.6\", \"3.7\", \"3.11\", \"3.12\"],\n\n    # An envvar key for determining the project id to use. Change it\n    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a\n    # build specific Cloud project. You can also use your own string\n    # to use your own Cloud project.\n    # 'gcloud_project_env': 'GOOGLE_CLOUD_PROJECT',\n    'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',\n\n    # A dictionary you want to inject into your test. Don't put any\n    # secrets here. These values will override predefined values.\n    'envs': {\n        'HMAC_KEY_TEST_SERVICE_ACCOUNT': get_service_account_email(),\n        'CLOUD_KMS_KEY': get_cloud_kms_key(),\n        # Some tests can not use multiple projects because of several reasons:\n        # 1. The new projects is enforced to have the\n        # 'constraints/iam.disableServiceAccountKeyCreation' policy.\n        # 2. The new projects buckets need to have universal permission model.\n        # For those tests, we'll use the original project.\n        'MAIN_GOOGLE_CLOUD_PROJECT': 'python-docs-samples-tests',\n        'MAIN_CLOUD_KMS_KEY': ('projects/python-docs-samples-tests/locations/us/'\n                               'keyRings/gcs-kms-key-ring/cryptoKeys/gcs-kms-key')\n    },\n}\n", "samples/snippets/bucket_lock_test.py": "# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport uuid\n\nfrom google.cloud import storage\nimport pytest\n\nimport storage_disable_default_event_based_hold\nimport storage_enable_default_event_based_hold\nimport storage_get_default_event_based_hold\nimport storage_get_retention_policy\nimport storage_lock_retention_policy\nimport storage_release_event_based_hold\nimport storage_release_temporary_hold\nimport storage_remove_retention_policy\nimport storage_set_event_based_hold\nimport storage_set_retention_policy\nimport storage_set_temporary_hold\n\n\nBLOB_NAME = \"storage_snippets_test_sigil\"\nBLOB_CONTENT = \"Hello, is it me you're looking for?\"\n# Retention policy for 5 seconds\nRETENTION_POLICY = 5\n\n\n@pytest.fixture\ndef bucket():\n    \"\"\"Yields a bucket that is deleted after the test completes.\"\"\"\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"bucket-lock-{uuid.uuid4()}\"\n        bucket = storage.Client().bucket(bucket_name)\n    bucket.create()\n    yield bucket\n    bucket.delete(force=True)\n\n\ndef test_retention_policy_no_lock(bucket, capsys):\n    storage_set_retention_policy.set_retention_policy(\n        bucket.name, RETENTION_POLICY\n    )\n    bucket.reload()\n\n    assert bucket.retention_period is RETENTION_POLICY\n    assert bucket.retention_policy_effective_time is not None\n    assert bucket.retention_policy_locked is None\n\n    storage_get_retention_policy.get_retention_policy(bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"Retention Policy for {bucket.name}\" in out\n    assert \"Retention Period: 5\" in out\n    assert \"Effective Time: \" in out\n    assert \"Retention Policy is locked\" not in out\n\n    blob = bucket.blob(BLOB_NAME)\n    blob.upload_from_string(BLOB_CONTENT)\n\n    assert blob.retention_expiration_time is not None\n\n    storage_remove_retention_policy.remove_retention_policy(bucket.name)\n    bucket.reload()\n    assert bucket.retention_period is None\n\n    time.sleep(RETENTION_POLICY)\n\n\ndef test_retention_policy_lock(bucket, capsys):\n    storage_set_retention_policy.set_retention_policy(\n        bucket.name, RETENTION_POLICY\n    )\n    bucket.reload()\n    assert bucket.retention_policy_locked is None\n\n    storage_lock_retention_policy.lock_retention_policy(bucket.name)\n    bucket.reload()\n    assert bucket.retention_policy_locked is True\n\n    storage_get_retention_policy.get_retention_policy(bucket.name)\n    out, _ = capsys.readouterr()\n    assert \"Retention Policy is locked\" in out\n\n\ndef test_enable_disable_bucket_default_event_based_hold(bucket, capsys):\n    storage_get_default_event_based_hold.get_default_event_based_hold(\n        bucket.name\n    )\n    out, _ = capsys.readouterr()\n    assert (\n        f\"Default event-based hold is not enabled for {bucket.name}\"\n        in out\n    )\n    assert (\n        f\"Default event-based hold is enabled for {bucket.name}\"\n        not in out\n    )\n\n    storage_enable_default_event_based_hold.enable_default_event_based_hold(\n        bucket.name\n    )\n    bucket.reload()\n\n    assert bucket.default_event_based_hold is True\n\n    storage_get_default_event_based_hold.get_default_event_based_hold(\n        bucket.name\n    )\n    out, _ = capsys.readouterr()\n    assert (\n        f\"Default event-based hold is enabled for {bucket.name}\" in out\n    )\n\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    time.sleep(10)\n\n    blob = bucket.blob(BLOB_NAME)\n    blob.upload_from_string(BLOB_CONTENT)\n    assert blob.event_based_hold is True\n\n    storage_release_event_based_hold.release_event_based_hold(\n        bucket.name, blob.name\n    )\n    blob.reload()\n    assert blob.event_based_hold is False\n\n    storage_disable_default_event_based_hold.disable_default_event_based_hold(\n        bucket.name\n    )\n    bucket.reload()\n    assert bucket.default_event_based_hold is False\n\n\ndef test_enable_disable_temporary_hold(bucket):\n    blob = bucket.blob(BLOB_NAME)\n    blob.upload_from_string(BLOB_CONTENT)\n    assert blob.temporary_hold is None\n\n    storage_set_temporary_hold.set_temporary_hold(bucket.name, blob.name)\n    blob.reload()\n    assert blob.temporary_hold is True\n\n    storage_release_temporary_hold.release_temporary_hold(\n        bucket.name, blob.name\n    )\n    blob.reload()\n    assert blob.temporary_hold is False\n\n\ndef test_enable_disable_event_based_hold(bucket):\n    blob = bucket.blob(BLOB_NAME)\n    blob.upload_from_string(BLOB_CONTENT)\n    assert blob.event_based_hold is None\n\n    storage_set_event_based_hold.set_event_based_hold(bucket.name, blob.name)\n    blob.reload()\n    assert blob.event_based_hold is True\n\n    storage_release_event_based_hold.release_event_based_hold(\n        bucket.name, blob.name\n    )\n    blob.reload()\n    assert blob.event_based_hold is False\n", "samples/snippets/encryption_test.py": "# Copyright 2016 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport os\nimport tempfile\nimport uuid\n\nfrom google.api_core.exceptions import NotFound\nfrom google.cloud import storage\nfrom google.cloud.storage import Blob\nimport pytest\n\nimport storage_download_encrypted_file\nimport storage_generate_encryption_key\nimport storage_object_csek_to_cmek\nimport storage_rotate_encryption_key\nimport storage_upload_encrypted_file\n\nBUCKET = os.environ[\"CLOUD_STORAGE_BUCKET\"]\nKMS_KEY = os.environ[\"MAIN_CLOUD_KMS_KEY\"]\n\nTEST_ENCRYPTION_KEY = \"brtJUWneL92g5q0N2gyDSnlPSYAiIVZ/cWgjyZNeMy0=\"\nTEST_ENCRYPTION_KEY_DECODED = base64.b64decode(TEST_ENCRYPTION_KEY)\n\nTEST_ENCRYPTION_KEY_2 = \"o4OD7SWCaPjfeEGhAY+YCgMdY9UW+OJ8mvfWD9lNtO4=\"\nTEST_ENCRYPTION_KEY_2_DECODED = base64.b64decode(TEST_ENCRYPTION_KEY_2)\n\n\ndef test_generate_encryption_key(capsys):\n    storage_generate_encryption_key.generate_encryption_key()\n    out, _ = capsys.readouterr()\n    encoded_key = out.split(\":\", 1).pop().strip()\n    key = base64.b64decode(encoded_key)\n    assert len(key) == 32, \"Returned key should be 32 bytes\"\n\n\ndef test_upload_encrypted_blob():\n    blob_name = f\"test_upload_encrypted_{uuid.uuid4().hex}\"\n    with tempfile.NamedTemporaryFile() as source_file:\n        source_file.write(b\"test\")\n\n        storage_upload_encrypted_file.upload_encrypted_blob(\n            BUCKET,\n            source_file.name,\n            blob_name,\n            TEST_ENCRYPTION_KEY,\n        )\n    bucket = storage.Client().bucket(BUCKET)\n    bucket.delete_blob(blob_name)\n\n\n@pytest.fixture(scope=\"module\")\ndef test_blob():\n    \"\"\"Provides a pre-existing blob in the test bucket.\"\"\"\n    bucket = storage.Client().bucket(BUCKET)\n    blob_name = f\"test_blob_{uuid.uuid4().hex}\"\n    blob = Blob(\n        blob_name,\n        bucket,\n        encryption_key=TEST_ENCRYPTION_KEY_DECODED,\n    )\n    content = \"Hello, is it me you're looking for?\"\n    blob.upload_from_string(content)\n\n    yield blob.name, content\n\n    # To delete an encrypted blob, you have to provide the same key\n    # used for the blob. When you provide a wrong key, you'll get\n    # NotFound.\n    try:\n        # Clean up for the case that the rotation didn't occur.\n        blob.delete()\n    except NotFound as e:\n        # For the case that the rotation succeeded.\n        print(f\"Ignoring 404, detail: {e}\")\n        blob = Blob(\n            blob_name,\n            bucket,\n            encryption_key=TEST_ENCRYPTION_KEY_2_DECODED\n        )\n        blob.delete()\n\n\ndef test_download_blob(test_blob):\n    test_blob_name, test_blob_content = test_blob\n    with tempfile.NamedTemporaryFile() as dest_file:\n        storage_download_encrypted_file.download_encrypted_blob(\n            BUCKET, test_blob_name, dest_file.name, TEST_ENCRYPTION_KEY\n        )\n\n        downloaded_content = dest_file.read().decode(\"utf-8\")\n        assert downloaded_content == test_blob_content\n\n\ndef test_rotate_encryption_key(test_blob):\n    test_blob_name, test_blob_content = test_blob\n    storage_rotate_encryption_key.rotate_encryption_key(\n        BUCKET, test_blob_name, TEST_ENCRYPTION_KEY, TEST_ENCRYPTION_KEY_2\n    )\n\n    with tempfile.NamedTemporaryFile() as dest_file:\n        storage_download_encrypted_file.download_encrypted_blob(\n            BUCKET, test_blob_name, dest_file.name, TEST_ENCRYPTION_KEY_2\n        )\n\n        downloaded_content = dest_file.read().decode(\"utf-8\")\n        assert downloaded_content == test_blob_content\n\n\ndef test_object_csek_to_cmek(test_blob):\n    test_blob_name, test_blob_content = test_blob\n    cmek_blob = storage_object_csek_to_cmek.object_csek_to_cmek(\n        BUCKET, test_blob_name, TEST_ENCRYPTION_KEY_2, KMS_KEY\n    )\n\n    assert cmek_blob.download_as_bytes(), test_blob_content\n", "samples/snippets/storage_get_autoclass.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_autoclass]\nfrom google.cloud import storage\n\n\ndef get_autoclass(bucket_name):\n    \"\"\"Get the Autoclass setting for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    autoclass_enabled = bucket.autoclass_enabled\n    autoclass_toggle_time = bucket.autoclass_toggle_time\n    terminal_storage_class = bucket.autoclass_terminal_storage_class\n    tsc_update_time = bucket.autoclass_terminal_storage_class_update_time\n\n    print(f\"Autoclass enabled is set to {autoclass_enabled} for {bucket.name} at {autoclass_toggle_time}.\")\n    print(f\"Autoclass terminal storage class is set to {terminal_storage_class} for {bucket.name} at {tsc_update_time}.\")\n\n    return bucket\n\n\n# [END storage_get_autoclass]\n\nif __name__ == \"__main__\":\n    get_autoclass(bucket_name=sys.argv[1])\n", "samples/snippets/storage_release_event_based_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_release_event_based_hold]\nfrom google.cloud import storage\n\n\ndef release_event_based_hold(bucket_name, blob_name):\n    \"\"\"Releases the event based hold on a given blob\"\"\"\n\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.event_based_hold = False\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(f\"Event based hold was released for {blob_name}\")\n\n\n# [END storage_release_event_based_hold]\n\n\nif __name__ == \"__main__\":\n    release_event_based_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_object_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport sys\n\n# [START storage_set_object_retention_policy]\nfrom google.cloud import storage\n\n\ndef set_object_retention_policy(bucket_name, contents, destination_blob_name):\n    \"\"\"Set the object retention policy of a file.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The contents to upload to the file\n    # contents = \"these are my contents\"\n\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_string(contents)\n\n    # Set the retention policy for the file.\n    blob.retention.mode = \"Unlocked\"\n    retention_date = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=10)\n    blob.retention.retain_until_time = retention_date\n    blob.patch()\n    print(\n        f\"Retention policy for file {destination_blob_name} was set to: {blob.retention.mode}.\"\n    )\n\n    # To modify an existing policy on an unlocked file object, pass in the override parameter.\n    new_retention_date = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=9)\n    blob.retention.retain_until_time = new_retention_date\n    blob.patch(override_unlocked_retention=True)\n    print(\n        f\"Retention policy for file {destination_blob_name} was updated to: {blob.retention.retain_until_time}.\"\n    )\n\n\n# [END storage_set_object_retention_policy]\n\n\nif __name__ == \"__main__\":\n    set_object_retention_policy(\n        bucket_name=sys.argv[1],\n        contents=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n    )\n", "samples/snippets/conftest.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport time\nimport uuid\n\nfrom google.cloud import storage\nimport pytest\n\n\n@pytest.fixture(scope=\"function\")\ndef bucket():\n    \"\"\"Yields a bucket that is deleted after the test completes.\"\"\"\n    # The new projects enforces uniform bucket level access, so\n    # we need to use the old main project for now.\n    original_value = os.environ['GOOGLE_CLOUD_PROJECT']\n    os.environ['GOOGLE_CLOUD_PROJECT'] = os.environ['MAIN_GOOGLE_CLOUD_PROJECT']\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = f\"uniform-bucket-level-access-{uuid.uuid4().hex}\"\n        bucket = storage.Client().bucket(bucket_name)\n    bucket.create()\n    yield bucket\n    time.sleep(3)\n    bucket.delete(force=True)\n    # Set the value back.\n    os.environ['GOOGLE_CLOUD_PROJECT'] = original_value\n", "samples/snippets/uniform_bucket_level_access_test.py": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport storage_disable_uniform_bucket_level_access\nimport storage_enable_uniform_bucket_level_access\nimport storage_get_uniform_bucket_level_access\n\n\ndef test_get_uniform_bucket_level_access(bucket, capsys):\n    storage_get_uniform_bucket_level_access.get_uniform_bucket_level_access(\n        bucket.name\n    )\n    out, _ = capsys.readouterr()\n    assert (\n        f\"Uniform bucket-level access is disabled for {bucket.name}.\"\n        in out\n    )\n\n\ndef test_enable_uniform_bucket_level_access(bucket, capsys):\n    short_name = storage_enable_uniform_bucket_level_access\n    short_name.enable_uniform_bucket_level_access(\n        bucket.name\n    )\n    out, _ = capsys.readouterr()\n    assert (\n        f\"Uniform bucket-level access was enabled for {bucket.name}.\"\n        in out\n    )\n\n\ndef test_disable_uniform_bucket_level_access(bucket, capsys):\n    short_name = storage_disable_uniform_bucket_level_access\n    short_name.disable_uniform_bucket_level_access(\n        bucket.name\n    )\n    out, _ = capsys.readouterr()\n    assert (\n        f\"Uniform bucket-level access was disabled for {bucket.name}.\"\n        in out\n    )\n", "samples/snippets/storage_add_file_owner.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_add_file_owner]\nfrom google.cloud import storage\n\n\ndef add_blob_owner(bucket_name, blob_name, user_email):\n    \"\"\"Adds a user as an owner on the given blob.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n    # user_email = \"name@example.com\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Reload fetches the current ACL from Cloud Storage.\n    blob.acl.reload()\n\n    # You can also use `group`, `domain`, `all_authenticated` and `all` to\n    # grant access to different types of entities. You can also use\n    # `grant_read` or `grant_write` to grant different roles.\n    blob.acl.user(user_email).grant_owner()\n    blob.acl.save()\n\n    print(\n        \"Added user {} as an owner on blob {} in bucket {}.\".format(\n            user_email, blob_name, bucket_name\n        )\n    )\n\n\n# [END storage_add_file_owner]\n\nif __name__ == \"__main__\":\n    add_blob_owner(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2], user_email=sys.argv[3],\n    )\n", "samples/snippets/storage_generate_encryption_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_generate_encryption_key]\nimport base64\nimport os\n\n\ndef generate_encryption_key():\n    \"\"\"Generates a 256 bit (32 byte) AES encryption key and prints the\n    base64 representation.\n\n    This is included for demonstration purposes. You should generate your own\n    key. Please remember that encryption keys should be handled with a\n    comprehensive security policy.\n    \"\"\"\n    key = os.urandom(32)\n    encoded_key = base64.b64encode(key).decode(\"utf-8\")\n\n    print(f\"Base 64 encoded encryption key: {encoded_key}\")\n\n\n# [END storage_generate_encryption_key]\n\nif __name__ == \"__main__\":\n    generate_encryption_key()\n", "samples/snippets/storage_deactivate_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_deactivate_hmac_key]\nfrom google.cloud import storage\n\n\ndef deactivate_key(access_id, project_id):\n    \"\"\"\n    Deactivate the HMAC key with the given access ID.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an active HMAC key\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n    hmac_key.state = \"INACTIVE\"\n    hmac_key.update()\n\n    print(\"The HMAC key is now inactive.\")\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_deactivate_hmac_key]\n\nif __name__ == \"__main__\":\n    deactivate_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_list_files_with_prefix.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_list_files_with_prefix]\nfrom google.cloud import storage\n\n\ndef list_blobs_with_prefix(bucket_name, prefix, delimiter=None):\n    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n\n    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n\n    The delimiter argument can be used to restrict the results to only the\n    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n    the prefix is returned. For example, given these blobs:\n\n        a/1.txt\n        a/b/2.txt\n\n    If you specify prefix ='a/', without a delimiter, you'll get back:\n\n        a/1.txt\n        a/b/2.txt\n\n    However, if you specify prefix='a/' and delimiter='/', you'll get back\n    only the file directly under 'a/':\n\n        a/1.txt\n\n    As part of the response, you'll also get back a blobs.prefixes entity\n    that lists the \"subfolders\" under `a/`:\n\n        a/b/\n    \"\"\"\n\n    storage_client = storage.Client()\n\n    # Note: Client.list_blobs requires at least package version 1.17.0.\n    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n\n    # Note: The call returns a response only when the iterator is consumed.\n    print(\"Blobs:\")\n    for blob in blobs:\n        print(blob.name)\n\n    if delimiter:\n        print(\"Prefixes:\")\n        for prefix in blobs.prefixes:\n            print(prefix)\n\n\n# [END storage_list_files_with_prefix]\n\nif __name__ == \"__main__\":\n    list_blobs_with_prefix(\n        bucket_name=sys.argv[1], prefix=sys.argv[2], delimiter=sys.argv[3]\n    )\n", "samples/snippets/storage_transfer_manager_upload_directory.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_upload_directory]\ndef upload_directory_with_transfer_manager(bucket_name, source_directory, workers=8):\n    \"\"\"Upload every file in a directory, including all files in subdirectories.\n\n    Each blob name is derived from the filename, not including the `directory`\n    parameter itself. For complete control of the blob name for each file (and\n    other aspects of individual blob metadata), use\n    transfer_manager.upload_many() instead.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The directory on your computer to upload. Files in the directory and its\n    # subdirectories will be uploaded. An empty string means \"the current\n    # working directory\".\n    # source_directory=\"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from pathlib import Path\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Generate a list of paths (in string form) relative to the `directory`.\n    # This can be done in a single list comprehension, but is expanded into\n    # multiple lines here for clarity.\n\n    # First, recursively get all files in `directory` as Path objects.\n    directory_as_path_obj = Path(source_directory)\n    paths = directory_as_path_obj.rglob(\"*\")\n\n    # Filter so the list only includes files, not directories themselves.\n    file_paths = [path for path in paths if path.is_file()]\n\n    # These paths are relative to the current working directory. Next, make them\n    # relative to `directory`\n    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n\n    # Finally, convert them all to strings.\n    string_paths = [str(path) for path in relative_paths]\n\n    print(\"Found {} files.\".format(len(string_paths)))\n\n    # Start the upload.\n    results = transfer_manager.upload_many_from_filenames(\n        bucket, string_paths, source_directory=source_directory, max_workers=workers\n    )\n\n    for name, result in zip(string_paths, results):\n        # The results list is either `None` or an exception for each filename in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Uploaded {} to {}.\".format(name, bucket.name))\n# [END storage_transfer_manager_upload_directory]\n", "samples/snippets/notification_polling.py": "#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"This application demonstrates how to poll for GCS notifications from a\nCloud Pub/Sub subscription, parse the incoming message, and acknowledge the\nsuccessful processing of the message.\n\nThis application will work with any subscription configured for pull rather\nthan push notifications. If you do not already have notifications configured,\nyou may consult the docs at\nhttps://cloud.google.com/storage/docs/reporting-changes or follow the steps\nbelow:\n\n1. First, follow the common setup steps for these snippets, specically\n   configuring auth and installing dependencies. See the README's \"Setup\"\n   section.\n\n2. Activate the Google Cloud Pub/Sub API, if you have not already done so.\n   https://console.cloud.google.com/flows/enableapi?apiid=pubsub\n\n3. Create a Google Cloud Storage bucket:\n   $ gsutil mb gs://testbucket\n\n4. Create a Cloud Pub/Sub topic and publish bucket notifications there:\n   $ gsutil notification create -f json -t testtopic gs://testbucket\n\n5. Create a subscription for your new topic:\n   $ gcloud pubsub subscriptions create testsubscription --topic=testtopic\n\n6. Run this program:\n   $ python notification_polling.py my-project-id testsubscription\n\n7. While the program is running, upload and delete some files in the testbucket\n   bucket (you could use the console or gsutil) and watch as changes scroll by\n   in the app.\n\"\"\"\n\nimport argparse\nimport json\nimport time\n\nfrom google.cloud import pubsub_v1\n\n\ndef summarize(message):\n    data = message.data.decode(\"utf-8\")\n    attributes = message.attributes\n\n    event_type = attributes[\"eventType\"]\n    bucket_id = attributes[\"bucketId\"]\n    object_id = attributes[\"objectId\"]\n    generation = attributes[\"objectGeneration\"]\n    description = (\n        \"\\tEvent type: {event_type}\\n\"\n        \"\\tBucket ID: {bucket_id}\\n\"\n        \"\\tObject ID: {object_id}\\n\"\n        \"\\tGeneration: {generation}\\n\"\n    ).format(\n        event_type=event_type,\n        bucket_id=bucket_id,\n        object_id=object_id,\n        generation=generation,\n    )\n\n    if \"overwroteGeneration\" in attributes:\n        description += f\"\\tOverwrote generation: {attributes['overwroteGeneration']}\\n\"\n    if \"overwrittenByGeneration\" in attributes:\n        description += f\"\\tOverwritten by generation: {attributes['overwrittenByGeneration']}\\n\"\n\n    payload_format = attributes[\"payloadFormat\"]\n    if payload_format == \"JSON_API_V1\":\n        object_metadata = json.loads(data)\n        size = object_metadata[\"size\"]\n        content_type = object_metadata[\"contentType\"]\n        metageneration = object_metadata[\"metageneration\"]\n        description += (\n            \"\\tContent type: {content_type}\\n\"\n            \"\\tSize: {object_size}\\n\"\n            \"\\tMetageneration: {metageneration}\\n\"\n        ).format(\n            content_type=content_type,\n            object_size=size,\n            metageneration=metageneration,\n        )\n    return description\n\n\ndef poll_notifications(project, subscription_name):\n    \"\"\"Polls a Cloud Pub/Sub subscription for new GCS events for display.\"\"\"\n    subscriber = pubsub_v1.SubscriberClient()\n    subscription_path = subscriber.subscription_path(\n        project, subscription_name\n    )\n\n    def callback(message):\n        print(f\"Received message:\\n{summarize(message)}\")\n        message.ack()\n\n    subscriber.subscribe(subscription_path, callback=callback)\n\n    # The subscriber is non-blocking, so we must keep the main thread from\n    # exiting to allow it to process messages in the background.\n    print(f\"Listening for messages on {subscription_path}\")\n    while True:\n        time.sleep(60)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\n        \"project\", help=\"The ID of the project that owns the subscription\"\n    )\n    parser.add_argument(\n        \"subscription\", help=\"The ID of the Pub/Sub subscription\"\n    )\n    args = parser.parse_args()\n    poll_notifications(args.project, args.subscription)\n", "samples/snippets/storage_enable_requester_pays.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_requester_pays]\nfrom google.cloud import storage\n\n\ndef enable_requester_pays(bucket_name):\n    \"\"\"Enable a bucket's requesterpays metadata\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.requester_pays = True\n    bucket.patch()\n\n    print(f\"Requester Pays has been enabled for {bucket_name}\")\n\n\n# [END storage_enable_requester_pays]\n\nif __name__ == \"__main__\":\n    enable_requester_pays(bucket_name=sys.argv[1])\n", "samples/snippets/requester_pays_test.py": "# Copyright 2017 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport backoff\nimport os\nimport tempfile\n\nfrom google.api_core.exceptions import GoogleAPIError\nfrom google.cloud import storage\nimport pytest\n\nimport storage_disable_requester_pays\nimport storage_download_file_requester_pays\nimport storage_enable_requester_pays\nimport storage_get_requester_pays_status\n\n\n# We use a different bucket from other tests.\n# The service account for the test needs to have Billing Project Manager role\n# in order to make changes on buckets with requester pays enabled.\nBUCKET = os.environ[\"REQUESTER_PAYS_TEST_BUCKET\"]\nPROJECT = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_enable_requester_pays(capsys):\n    storage_enable_requester_pays.enable_requester_pays(BUCKET)\n    out, _ = capsys.readouterr()\n    assert f\"Requester Pays has been enabled for {BUCKET}\" in out\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_disable_requester_pays(capsys):\n    storage_disable_requester_pays.disable_requester_pays(BUCKET)\n    out, _ = capsys.readouterr()\n    assert f\"Requester Pays has been disabled for {BUCKET}\" in out\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_get_requester_pays_status(capsys):\n    storage_get_requester_pays_status.get_requester_pays_status(BUCKET)\n    out, _ = capsys.readouterr()\n    assert f\"Requester Pays is disabled for {BUCKET}\" in out\n\n\n@pytest.fixture\ndef test_blob():\n    \"\"\"Provides a pre-existing blob in the test bucket.\"\"\"\n    bucket = storage.Client().bucket(BUCKET)\n    blob = bucket.blob(\"storage_snippets_test_sigil\")\n    blob.upload_from_string(\"Hello, is it me you're looking for?\")\n    return blob\n\n\n@backoff.on_exception(backoff.expo, GoogleAPIError, max_time=60)\ndef test_download_file_requester_pays(test_blob, capsys):\n    with tempfile.NamedTemporaryFile() as dest_file:\n        storage_download_file_requester_pays.download_file_requester_pays(\n            BUCKET, PROJECT, test_blob.name, dest_file.name\n        )\n\n        assert dest_file.read()\n", "samples/snippets/storage_set_autoclass.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_autoclass]\nfrom google.cloud import storage\n\n\ndef set_autoclass(bucket_name):\n    \"\"\"Configure the Autoclass setting for a bucket.\n\n    terminal_storage_class field is optional and defaults to NEARLINE if not otherwise specified.\n    Valid terminal_storage_class values are NEARLINE and ARCHIVE.\n    \"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n    # Enable Autoclass for a bucket. Set enabled to false to disable Autoclass.\n    # Set Autoclass.TerminalStorageClass, valid values are NEARLINE and ARCHIVE.\n    enabled = True\n    terminal_storage_class = \"ARCHIVE\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.autoclass_enabled = enabled\n    bucket.autoclass_terminal_storage_class = terminal_storage_class\n    bucket.patch()\n    print(f\"Autoclass enabled is set to {bucket.autoclass_enabled} for {bucket.name} at {bucket.autoclass_toggle_time}.\")\n    print(f\"Autoclass terminal storage class is {bucket.autoclass_terminal_storage_class}.\")\n\n    return bucket\n\n\n# [END storage_set_autoclass]\n\nif __name__ == \"__main__\":\n    set_autoclass(bucket_name=sys.argv[1])\n", "samples/snippets/storage_configure_retries.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that configures retries on an operation call.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/retry-strategy\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_configure_retries]\nfrom google.cloud import storage\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\ndef configure_retries(bucket_name, blob_name):\n    \"\"\"Configures retries with customizations.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of your GCS object\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    # Customize retry with a deadline of 500 seconds (default=120 seconds).\n    modified_retry = DEFAULT_RETRY.with_deadline(500.0)\n    # Customize retry with an initial wait time of 1.5 (default=1.0).\n    # Customize retry with a wait time multiplier per iteration of 1.2 (default=2.0).\n    # Customize retry with a maximum wait time of 45.0 (default=60.0).\n    modified_retry = modified_retry.with_delay(initial=1.5, multiplier=1.2, maximum=45.0)\n\n    # blob.delete() uses DEFAULT_RETRY_IF_GENERATION_SPECIFIED by default.\n    # Override with modified_retry so the function retries even if the generation\n    # number is not specified.\n    print(\n        f\"The following library method is customized to be retried according to the following configurations: {modified_retry}\"\n    )\n\n    blob.delete(retry=modified_retry)\n    print(f\"Blob {blob_name} deleted with a customized retry strategy.\")\n\n\n# [END storage_configure_retries]\n\n\nif __name__ == \"__main__\":\n    configure_retries(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_temporary_hold.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_temporary_hold]\nfrom google.cloud import storage\n\n\ndef set_temporary_hold(bucket_name, blob_name):\n    \"\"\"Sets a temporary hold on a given blob\"\"\"\n    # bucket_name = \"my-bucket\"\n    # blob_name = \"my-blob\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    metageneration_match_precondition = None\n\n    # Optional: set a metageneration-match precondition to avoid potential race\n    # conditions and data corruptions. The request to patch is aborted if the\n    # object's metageneration does not match your precondition.\n    blob.reload()  # Fetch blob metadata to use in metageneration_match_precondition.\n    metageneration_match_precondition = blob.metageneration\n\n    blob.temporary_hold = True\n    blob.patch(if_metageneration_match=metageneration_match_precondition)\n\n    print(\"Temporary hold was set for #{blob_name}\")\n\n\n# [END storage_set_temporary_hold]\n\n\nif __name__ == \"__main__\":\n    set_temporary_hold(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_generate_signed_post_policy_v4.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_generate_signed_post_policy_v4]\nimport datetime\n# [END storage_generate_signed_post_policy_v4]\nimport sys\n# [START storage_generate_signed_post_policy_v4]\n\nfrom google.cloud import storage\n\n\ndef generate_signed_post_policy_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 POST Policy and prints an HTML form.\"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n\n    policy = storage_client.generate_signed_post_policy_v4(\n        bucket_name,\n        blob_name,\n        expiration=datetime.timedelta(minutes=10),\n        fields={\n          'x-goog-meta-test': 'data'\n        }\n    )\n\n    # Create an HTML form with the provided policy\n    header = \"<form action='{}' method='POST' enctype='multipart/form-data'>\\n\"\n    form = header.format(policy[\"url\"])\n\n    # Include all fields returned in the HTML form as they're required\n    for key, value in policy[\"fields\"].items():\n        form += f\"  <input name='{key}' value='{value}' type='hidden'/>\\n\"\n\n    form += \"  <input type='file' name='file'/><br />\\n\"\n    form += \"  <input type='submit' value='Upload File' /><br />\\n\"\n    form += \"</form>\"\n\n    print(form)\n\n    return form\n\n\n# [END storage_generate_signed_post_policy_v4]\n\nif __name__ == \"__main__\":\n    generate_signed_post_policy_v4(\n        bucket_name=sys.argv[1], blob_name=sys.argv[2]\n    )\n", "samples/snippets/storage_create_bucket_notifications.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates a notification configuration for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_create_bucket_notifications]\nfrom google.cloud import storage\n\n\ndef create_bucket_notifications(bucket_name, topic_name):\n    \"\"\"Creates a notification configuration for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The name of a topic\n    # topic_name = \"your-topic-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notification = bucket.notification(topic_name=topic_name)\n    notification.create()\n\n    print(f\"Successfully created notification with ID {notification.notification_id} for bucket {bucket_name}\")\n\n# [END storage_create_bucket_notifications]\n\n\nif __name__ == \"__main__\":\n    create_bucket_notifications(bucket_name=sys.argv[1], topic_name=sys.argv[2])\n", "samples/snippets/storage_download_to_stream.py": "#!/usr/bin/env python\n\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_stream_file_download]\nfrom google.cloud import storage\n\n\ndef download_blob_to_stream(bucket_name, source_blob_name, file_obj):\n    \"\"\"Downloads a blob to a stream or other file-like object.\"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object (blob)\n    # source_blob_name = \"storage-object-name\"\n\n    # The stream or file (file-like object) to which the blob will be written\n    # import io\n    # file_obj = io.BytesIO()\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client-side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` in that it doesn't\n    # retrieve metadata from Google Cloud Storage. As we don't use metadata in\n    # this example, using `Bucket.blob` is preferred here.\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_file(file_obj)\n\n    print(f\"Downloaded blob {source_blob_name} to file-like object.\")\n\n    return file_obj\n    # Before reading from file_obj, remember to rewind with file_obj.seek(0).\n\n# [END storage_stream_file_download]\n", "samples/snippets/storage_enable_bucket_lifecycle_management.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_bucket_lifecycle_management]\nfrom google.cloud import storage\n\n\ndef enable_bucket_lifecycle_management(bucket_name):\n    \"\"\"Enable lifecycle management for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    rules = bucket.lifecycle_rules\n\n    print(f\"Lifecycle management rules for bucket {bucket_name} are {list(rules)}\")\n    bucket.add_lifecycle_delete_rule(age=2)\n    bucket.patch()\n\n    rules = bucket.lifecycle_rules\n    print(f\"Lifecycle management is enable for bucket {bucket_name} and the rules are {list(rules)}\")\n\n    return bucket\n\n\n# [END storage_enable_bucket_lifecycle_management]\n\nif __name__ == \"__main__\":\n    enable_bucket_lifecycle_management(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_bucket.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_bucket]\nfrom google.cloud import storage\n\n\ndef create_bucket(bucket_name):\n    \"\"\"Creates a new bucket.\"\"\"\n    # bucket_name = \"your-new-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.create_bucket(bucket_name)\n\n    print(f\"Bucket {bucket.name} created\")\n\n\n# [END storage_create_bucket]\n\nif __name__ == \"__main__\":\n    create_bucket(bucket_name=sys.argv[1])\n", "samples/snippets/storage_create_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_create_hmac_key]\nfrom google.cloud import storage\n\n\ndef create_key(project_id, service_account_email):\n    \"\"\"\n    Create a new HMAC key using the given project and service account.\n    \"\"\"\n    # project_id = 'Your Google Cloud project ID'\n    # service_account_email = 'Service account used to generate the HMAC key'\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key, secret = storage_client.create_hmac_key(\n        service_account_email=service_account_email, project_id=project_id\n    )\n\n    print(f\"The base64 encoded secret is {secret}\")\n    print(\"Do not miss that secret, there is no API to recover it.\")\n    print(\"The HMAC key metadata is:\")\n    print(f\"Service Account Email: {hmac_key.service_account_email}\")\n    print(f\"Key ID: {hmac_key.id}\")\n    print(f\"Access ID: {hmac_key.access_id}\")\n    print(f\"Project ID: {hmac_key.project}\")\n    print(f\"State: {hmac_key.state}\")\n    print(f\"Created At: {hmac_key.time_created}\")\n    print(f\"Updated At: {hmac_key.updated}\")\n    print(f\"Etag: {hmac_key.etag}\")\n    return hmac_key\n\n\n# [END storage_create_hmac_key]\n\nif __name__ == \"__main__\":\n    create_key(project_id=sys.argv[1], service_account_email=sys.argv[2])\n", "samples/snippets/storage_delete_bucket.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_bucket]\nfrom google.cloud import storage\n\n\ndef delete_bucket(bucket_name):\n    \"\"\"Deletes a bucket. The bucket must be empty.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.delete()\n\n    print(f\"Bucket {bucket.name} deleted\")\n\n\n# [END storage_delete_bucket]\n\nif __name__ == \"__main__\":\n    delete_bucket(bucket_name=sys.argv[1])\n", "samples/snippets/storage_download_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_download_file]\nfrom google.cloud import storage\n\n\ndef download_blob(bucket_name, source_blob_name, destination_file_name):\n    \"\"\"Downloads a blob from the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The ID of your GCS object\n    # source_blob_name = \"storage-object-name\"\n\n    # The path to which the file should be downloaded\n    # destination_file_name = \"local/path/to/file\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n\n    # Construct a client side representation of a blob.\n    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n    # any content from Google Cloud Storage. As we don't need additional data,\n    # using `Bucket.blob` is preferred here.\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\n    print(\n        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n            source_blob_name, bucket_name, destination_file_name\n        )\n    )\n\n\n# [END storage_download_file]\n\nif __name__ == \"__main__\":\n    download_blob(\n        bucket_name=sys.argv[1],\n        source_blob_name=sys.argv[2],\n        destination_file_name=sys.argv[3],\n    )\n", "samples/snippets/storage_lock_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_lock_retention_policy]\nfrom google.cloud import storage\n\n\ndef lock_retention_policy(bucket_name):\n    \"\"\"Locks the retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    # get_bucket gets the current metageneration value for the bucket,\n    # required by lock_retention_policy.\n    bucket = storage_client.get_bucket(bucket_name)\n\n    # Warning: Once a retention policy is locked it cannot be unlocked\n    # and retention period can only be increased.\n    bucket.lock_retention_policy()\n\n    print(f\"Retention policy for {bucket_name} is now locked\")\n    print(\n        f\"Retention policy effective as of {bucket.retention_policy_effective_time}\"\n    )\n\n\n# [END storage_lock_retention_policy]\n\n\nif __name__ == \"__main__\":\n    lock_retention_policy(bucket_name=sys.argv[1])\n", "samples/snippets/storage_enable_uniform_bucket_level_access.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_enable_uniform_bucket_level_access]\nfrom google.cloud import storage\n\n\ndef enable_uniform_bucket_level_access(bucket_name):\n    \"\"\"Enable uniform bucket-level access for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.patch()\n\n    print(\n        f\"Uniform bucket-level access was enabled for {bucket.name}.\"\n    )\n\n\n# [END storage_enable_uniform_bucket_level_access]\n\nif __name__ == \"__main__\":\n    enable_uniform_bucket_level_access(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_upload_file]\nfrom google.cloud import storage\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The path to your file to upload\n    # source_file_name = \"local/path/to/file\"\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to upload is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    generation_match_precondition = 0\n\n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\n    print(\n        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n    )\n\n\n# [END storage_upload_file]\n\nif __name__ == \"__main__\":\n    upload_blob(\n        bucket_name=sys.argv[1],\n        source_file_name=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n    )\n", "samples/snippets/storage_print_pubsub_bucket_notification.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that gets a notification configuration for a bucket.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/reporting-changes\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_print_pubsub_bucket_notification]\nfrom google.cloud import storage\n\n\ndef print_pubsub_bucket_notification(bucket_name, notification_id):\n    \"\"\"Gets a notification configuration for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of the notification\n    # notification_id = \"your-notification-id\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    notification = bucket.get_notification(notification_id)\n\n    print(f\"Notification ID: {notification.notification_id}\")\n    print(f\"Topic Name: {notification.topic_name}\")\n    print(f\"Event Types: {notification.event_types}\")\n    print(f\"Custom Attributes: {notification.custom_attributes}\")\n    print(f\"Payload Format: {notification.payload_format}\")\n    print(f\"Blob Name Prefix: {notification.blob_name_prefix}\")\n    print(f\"Etag: {notification.etag}\")\n    print(f\"Self Link: {notification.self_link}\")\n\n# [END storage_print_pubsub_bucket_notification]\n\n\nif __name__ == \"__main__\":\n    print_pubsub_bucket_notification(bucket_name=sys.argv[1], notification_id=sys.argv[2])\n", "samples/snippets/storage_set_rpo_default.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that sets the replication behavior or recovery point objective (RPO) to default.\nThis sample is used on this page:\n    https://cloud.google.com/storage/docs/managing-turbo-replication\nFor more information, see README.md.\n\"\"\"\n\n# [START storage_set_rpo_default]\n\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import RPO_DEFAULT\n\n\ndef set_rpo_default(bucket_name):\n    \"\"\"Sets the RPO to DEFAULT, disabling the turbo replication feature\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.rpo = RPO_DEFAULT\n    bucket.patch()\n\n    print(f\"RPO is set to DEFAULT for {bucket.name}.\")\n\n\n# [END storage_set_rpo_default]\n\nif __name__ == \"__main__\":\n    set_rpo_default(bucket_name=sys.argv[1])\n", "samples/snippets/storage_transfer_manager_download_bucket.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_download_bucket]\ndef download_bucket_with_transfer_manager(\n    bucket_name, destination_directory=\"\", workers=8, max_results=1000\n):\n    \"\"\"Download all of the blobs in a bucket, concurrently in a process pool.\n\n    The filename of each blob once downloaded is derived from the blob name and\n    the `destination_directory `parameter. For complete control of the filename\n    of each blob, use transfer_manager.download_many() instead.\n\n    Directories will be created automatically as needed, for instance to\n    accommodate blob names that include slashes.\n    \"\"\"\n\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The directory on your computer to which to download all of the files. This\n    # string is prepended (with os.path.join()) to the name of each blob to form\n    # the full path. Relative paths and absolute paths are both accepted. An\n    # empty string means \"the current working directory\". Note that this\n    # parameter allows accepts directory traversal (\"../\" etc.) and is not\n    # intended for unsanitized end user input.\n    # destination_directory = \"\"\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case, but smaller files usually\n    # benefit from a higher number of processes. Each additional process occupies\n    # some CPU and memory resources until finished. Threads can be used instead\n    # of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    # The maximum number of results to fetch from bucket.list_blobs(). This\n    # sample code fetches all of the blobs up to max_results and queues them all\n    # for download at once. Though they will still be executed in batches up to\n    # the processes limit, queueing them all at once can be taxing on system\n    # memory if buckets are very large. Adjust max_results as needed for your\n    # system environment, or set it to None if you are sure the bucket is not\n    # too large to hold in memory easily.\n    # max_results=1000\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n\n    results = transfer_manager.download_many_to_path(\n        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n    )\n\n    for name, result in zip(blob_names, results):\n        # The results list is either `None` or an exception for each blob in\n        # the input list, in order.\n\n        if isinstance(result, Exception):\n            print(\"Failed to download {} due to exception: {}\".format(name, result))\n        else:\n            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n# [END storage_transfer_manager_download_bucket]\n", "samples/snippets/storage_get_bucket_labels.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_get_bucket_labels]\nimport pprint\n# [END storage_get_bucket_labels]\nimport sys\n# [START storage_get_bucket_labels]\n\nfrom google.cloud import storage\n\n\ndef get_bucket_labels(bucket_name):\n    \"\"\"Prints out a bucket's labels.\"\"\"\n    # bucket_name = 'your-bucket-name'\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n\n    labels = bucket.labels\n    pprint.pprint(labels)\n\n\n# [END storage_get_bucket_labels]\n\nif __name__ == \"__main__\":\n    get_bucket_labels(bucket_name=sys.argv[1])\n", "samples/snippets/storage_set_public_access_prevention_enforced.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_public_access_prevention_enforced]\nfrom google.cloud import storage\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_ENFORCED\n\n\ndef set_public_access_prevention_enforced(bucket_name):\n    \"\"\"Enforce public access prevention for a bucket.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.public_access_prevention = (\n        PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n    bucket.patch()\n\n    print(f\"Public access prevention is set to enforced for {bucket.name}.\")\n\n\n# [END storage_set_public_access_prevention_enforced]\n\nif __name__ == \"__main__\":\n    set_public_access_prevention_enforced(bucket_name=sys.argv[1])\n", "samples/snippets/storage_set_bucket_default_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_bucket_default_kms_key]\nfrom google.cloud import storage\n\n\ndef enable_default_kms_key(bucket_name, kms_key_name):\n    \"\"\"Sets a bucket's default KMS key.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # kms_key_name = \"projects/PROJ/locations/LOC/keyRings/RING/cryptoKey/KEY\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    bucket.default_kms_key_name = kms_key_name\n    bucket.patch()\n\n    print(\n        \"Set default KMS key for bucket {} to {}.\".format(\n            bucket.name, bucket.default_kms_key_name\n        )\n    )\n\n\n# [END storage_set_bucket_default_kms_key]\n\nif __name__ == \"__main__\":\n    enable_default_kms_key(bucket_name=sys.argv[1], kms_key_name=sys.argv[2])\n", "samples/snippets/storage_object_get_kms_key.py": "#!/usr/bin/env python\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_object_get_kms_key]\nfrom google.cloud import storage\n\n\ndef object_get_kms_key(bucket_name, blob_name):\n    \"\"\"Retrieve the KMS key of a blob\"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # blob_name = \"your-object-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.get_blob(blob_name)\n\n    kms_key = blob.kms_key_name\n\n    print(f\"The KMS key of a blob is {blob.kms_key_name}\")\n    return kms_key\n\n\n# [END storage_object_get_kms_key]\n\nif __name__ == \"__main__\":\n    object_get_kms_key(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_client_endpoint.py": "#!/usr/bin/env python\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n\"\"\"Sample that creates a new bucket in a specified region\n\"\"\"\n\n# [START storage_set_client_endpoint]\n\nfrom google.cloud import storage\n\n\ndef set_client_endpoint(api_endpoint):\n    \"\"\"Initiates client with specified endpoint.\"\"\"\n    # api_endpoint = 'https://storage.googleapis.com'\n\n    storage_client = storage.Client(client_options={'api_endpoint': api_endpoint})\n\n    print(f\"client initiated with endpoint: {storage_client._connection.API_BASE_URL}\")\n\n    return storage_client\n\n\n# [END storage_set_client_endpoint]\n\nif __name__ == \"__main__\":\n    set_client_endpoint(api_endpoint=sys.argv[1])\n", "samples/snippets/storage_disable_uniform_bucket_level_access.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_disable_uniform_bucket_level_access]\nfrom google.cloud import storage\n\n\ndef disable_uniform_bucket_level_access(bucket_name):\n    \"\"\"Disable uniform bucket-level access for a bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = False\n    bucket.patch()\n\n    print(\n        f\"Uniform bucket-level access was disabled for {bucket.name}.\"\n    )\n\n\n# [END storage_disable_uniform_bucket_level_access]\n\nif __name__ == \"__main__\":\n    disable_uniform_bucket_level_access(bucket_name=sys.argv[1])\n", "samples/snippets/quickstart_test.py": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport mock\n\nimport quickstart\n\n\n@mock.patch(\"google.cloud.storage.client.Client.create_bucket\")\ndef test_quickstart(create_bucket_mock, capsys):\n    # Unlike other quickstart tests, this one mocks out the creation\n    # because buckets are expensive, globally-namespaced object.\n    create_bucket_mock.return_value = mock.sentinel.bucket\n\n    quickstart.run_quickstart()\n\n    create_bucket_mock.assert_called_with(\"my-new-bucket\")\n", "samples/snippets/storage_view_bucket_iam_members.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_view_bucket_iam_members]\nfrom google.cloud import storage\n\n\ndef view_bucket_iam_members(bucket_name):\n    \"\"\"View IAM Policy for a bucket\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n\n    for binding in policy.bindings:\n        print(f\"Role: {binding['role']}, Members: {binding['members']}\")\n\n\n# [END storage_view_bucket_iam_members]\n\n\nif __name__ == \"__main__\":\n    view_bucket_iam_members(bucket_name=sys.argv[1])\n", "samples/snippets/storage_upload_encrypted_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_upload_encrypted_file]\nimport base64\n# [END storage_upload_encrypted_file]\nimport sys\n# [START storage_upload_encrypted_file]\n\nfrom google.cloud import storage\n\n\ndef upload_encrypted_blob(\n    bucket_name,\n    source_file_name,\n    destination_blob_name,\n    base64_encryption_key,\n):\n    \"\"\"Uploads a file to a Google Cloud Storage bucket using a custom\n    encryption key.\n\n    The file will be encrypted by Google Cloud Storage and only\n    retrievable using the provided encryption key.\n    \"\"\"\n    # bucket_name = \"your-bucket-name\"\n    # source_file_name = \"local/path/to/file\"\n    # destination_blob_name = \"storage-object-name\"\n    # base64_encryption_key = \"TIbv/fjexq+VmtXzAlc63J4z5kFmWJ6NdAPQulQBT7g=\"\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Encryption key must be an AES256 key represented as a bytestring with\n    # 32 bytes. Since it's passed in as a base64 encoded string, it needs\n    # to be decoded.\n    encryption_key = base64.b64decode(base64_encryption_key)\n    blob = bucket.blob(\n        destination_blob_name, encryption_key=encryption_key\n    )\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request to upload is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    generation_match_precondition = 0\n\n    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n\n    print(\n        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n    )\n\n\n# [END storage_upload_encrypted_file]\n\nif __name__ == \"__main__\":\n    upload_encrypted_blob(\n        bucket_name=sys.argv[1],\n        source_file_name=sys.argv[2],\n        destination_blob_name=sys.argv[3],\n        base64_encryption_key=sys.argv[4],\n    )\n", "samples/snippets/storage_move_file.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_move_file]\nfrom google.cloud import storage\n\n\ndef move_blob(bucket_name, blob_name, destination_bucket_name, destination_blob_name,):\n    \"\"\"Moves a blob from one bucket to another with a new name.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n    # The ID of your GCS object\n    # blob_name = \"your-object-name\"\n    # The ID of the bucket to move the object to\n    # destination_bucket_name = \"destination-bucket-name\"\n    # The ID of your new GCS object (optional)\n    # destination_blob_name = \"destination-object-name\"\n\n    storage_client = storage.Client()\n\n    source_bucket = storage_client.bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.bucket(destination_bucket_name)\n\n    # Optional: set a generation-match precondition to avoid potential race conditions\n    # and data corruptions. The request is aborted if the object's\n    # generation number does not match your precondition. For a destination\n    # object that does not yet exist, set the if_generation_match precondition to 0.\n    # If the destination object already exists in your bucket, set instead a\n    # generation-match precondition using its generation number.\n    # There is also an `if_source_generation_match` parameter, which is not used in this example.\n    destination_generation_match_precondition = 0\n\n    blob_copy = source_bucket.copy_blob(\n        source_blob, destination_bucket, destination_blob_name, if_generation_match=destination_generation_match_precondition,\n    )\n    source_bucket.delete_blob(blob_name)\n\n    print(\n        \"Blob {} in bucket {} moved to blob {} in bucket {}.\".format(\n            source_blob.name,\n            source_bucket.name,\n            blob_copy.name,\n            destination_bucket.name,\n        )\n    )\n\n\n# [END storage_move_file]\n\nif __name__ == \"__main__\":\n    move_blob(\n        bucket_name=sys.argv[1],\n        blob_name=sys.argv[2],\n        destination_bucket_name=sys.argv[3],\n        destination_blob_name=sys.argv[4],\n    )\n", "samples/snippets/storage_delete_hmac_key.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_delete_hmac_key]\nfrom google.cloud import storage\n\n\ndef delete_key(access_id, project_id):\n    \"\"\"\n    Delete the HMAC key with the given access ID. Key must have state INACTIVE\n    in order to succeed.\n    \"\"\"\n    # project_id = \"Your Google Cloud project ID\"\n    # access_id = \"ID of an HMAC key (must be in INACTIVE state)\"\n\n    storage_client = storage.Client(project=project_id)\n\n    hmac_key = storage_client.get_hmac_key_metadata(\n        access_id, project_id=project_id\n    )\n    hmac_key.delete()\n\n    print(\n        \"The key is deleted, though it may still appear in list_hmac_keys()\"\n        \" results.\"\n    )\n\n\n# [END storage_delete_hmac_key]\n\nif __name__ == \"__main__\":\n    delete_key(access_id=sys.argv[1], project_id=sys.argv[2])\n", "samples/snippets/storage_transfer_manager_upload_chunks_concurrently.py": "# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START storage_transfer_manager_upload_chunks_concurrently]\ndef upload_chunks_concurrently(\n    bucket_name,\n    source_filename,\n    destination_blob_name,\n    chunk_size=32 * 1024 * 1024,\n    workers=8,\n):\n    \"\"\"Upload a single file, in chunks, concurrently in a process pool.\"\"\"\n    # The ID of your GCS bucket\n    # bucket_name = \"your-bucket-name\"\n\n    # The path to your file to upload\n    # source_filename = \"local/path/to/file\"\n\n    # The ID of your GCS object\n    # destination_blob_name = \"storage-object-name\"\n\n    # The size of each chunk. The performance impact of this value depends on\n    # the use case. The remote service has a minimum of 5 MiB and a maximum of\n    # 5 GiB.\n    # chunk_size = 32 * 1024 * 1024 (32 MiB)\n\n    # The maximum number of processes to use for the operation. The performance\n    # impact of this value depends on the use case. Each additional process\n    # occupies some CPU and memory resources until finished. Threads can be used\n    # instead of processes by passing `worker_type=transfer_manager.THREAD`.\n    # workers=8\n\n    from google.cloud.storage import Client, transfer_manager\n\n    storage_client = Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    transfer_manager.upload_chunks_concurrently(\n        source_filename, blob, chunk_size=chunk_size, max_workers=workers\n    )\n\n    print(f\"File {source_filename} uploaded to {destination_blob_name}.\")\n\n\n# [END storage_transfer_manager_upload_chunks_concurrently]\n", "samples/snippets/storage_add_bucket_label.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# [START storage_add_bucket_label]\nimport pprint\n# [END storage_add_bucket_label]\nimport sys\n# [START storage_add_bucket_label]\n\nfrom google.cloud import storage\n\n\ndef add_bucket_label(bucket_name):\n    \"\"\"Add a label to a bucket.\"\"\"\n    # bucket_name = \"your-bucket-name\"\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(bucket_name)\n    labels = bucket.labels\n    labels[\"example\"] = \"label\"\n    bucket.labels = labels\n    bucket.patch()\n\n    print(f\"Updated labels on {bucket.name}.\")\n    pprint.pprint(bucket.labels)\n\n\n# [END storage_add_bucket_label]\n\nif __name__ == \"__main__\":\n    add_bucket_label(bucket_name=sys.argv[1])\n", "samples/snippets/storage_get_metadata.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_get_metadata]\nfrom google.cloud import storage\n\n\ndef blob_metadata(bucket_name, blob_name):\n    \"\"\"Prints out a blob's metadata.\"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    # Retrieve a blob, and its metadata, from Google Cloud Storage.\n    # Note that `get_blob` differs from `Bucket.blob`, which does not\n    # make an HTTP request.\n    blob = bucket.get_blob(blob_name)\n\n    print(f\"Blob: {blob.name}\")\n    print(f\"Bucket: {blob.bucket.name}\")\n    print(f\"Storage class: {blob.storage_class}\")\n    print(f\"ID: {blob.id}\")\n    print(f\"Size: {blob.size} bytes\")\n    print(f\"Updated: {blob.updated}\")\n    print(f\"Generation: {blob.generation}\")\n    print(f\"Metageneration: {blob.metageneration}\")\n    print(f\"Etag: {blob.etag}\")\n    print(f\"Owner: {blob.owner}\")\n    print(f\"Component count: {blob.component_count}\")\n    print(f\"Crc32c: {blob.crc32c}\")\n    print(f\"md5_hash: {blob.md5_hash}\")\n    print(f\"Cache-control: {blob.cache_control}\")\n    print(f\"Content-type: {blob.content_type}\")\n    print(f\"Content-disposition: {blob.content_disposition}\")\n    print(f\"Content-encoding: {blob.content_encoding}\")\n    print(f\"Content-language: {blob.content_language}\")\n    print(f\"Metadata: {blob.metadata}\")\n    print(f\"Medialink: {blob.media_link}\")\n    print(f\"Custom Time: {blob.custom_time}\")\n    print(\"Temporary hold: \", \"enabled\" if blob.temporary_hold else \"disabled\")\n    print(\n        \"Event based hold: \",\n        \"enabled\" if blob.event_based_hold else \"disabled\",\n    )\n    print(f\"Retention mode: {blob.retention.mode}\")\n    print(f\"Retention retain until time: {blob.retention.retain_until_time}\")\n    if blob.retention_expiration_time:\n        print(\n            f\"retentionExpirationTime: {blob.retention_expiration_time}\"\n        )\n\n\n# [END storage_get_metadata]\n\nif __name__ == \"__main__\":\n    blob_metadata(bucket_name=sys.argv[1], blob_name=sys.argv[2])\n", "samples/snippets/storage_set_retention_policy.py": "#!/usr/bin/env python\n\n# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\n# [START storage_set_retention_policy]\nfrom google.cloud import storage\n\n\ndef set_retention_policy(bucket_name, retention_period):\n    \"\"\"Defines a retention policy on a given bucket\"\"\"\n    # bucket_name = \"my-bucket\"\n    # retention_period = 10\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    bucket.retention_period = retention_period\n    bucket.patch()\n\n    print(\n        \"Bucket {} retention period set for {} seconds\".format(\n            bucket.name, bucket.retention_period\n        )\n    )\n\n\n# [END storage_set_retention_policy]\n\n\nif __name__ == \"__main__\":\n    set_retention_policy(bucket_name=sys.argv[1], retention_period=sys.argv[2])\n", "scripts/readme-gen/readme_gen.py": "#!/usr/bin/env python\n\n# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Generates READMEs using configuration defined in yaml.\"\"\"\n\nimport argparse\nimport io\nimport os\nimport subprocess\n\nimport jinja2\nimport yaml\n\n\njinja_env = jinja2.Environment(\n    trim_blocks=True,\n    loader=jinja2.FileSystemLoader(\n        os.path.abspath(os.path.join(os.path.dirname(__file__), \"templates\"))\n    ),\n    autoescape=True,\n)\n\nREADME_TMPL = jinja_env.get_template(\"README.tmpl.rst\")\n\n\ndef get_help(file):\n    return subprocess.check_output([\"python\", file, \"--help\"]).decode()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"source\")\n    parser.add_argument(\"--destination\", default=\"README.rst\")\n\n    args = parser.parse_args()\n\n    source = os.path.abspath(args.source)\n    root = os.path.dirname(source)\n    destination = os.path.join(root, args.destination)\n\n    jinja_env.globals[\"get_help\"] = get_help\n\n    with io.open(source, \"r\") as f:\n        config = yaml.load(f)\n\n    # This allows get_help to execute in the right directory.\n    os.chdir(root)\n\n    output = README_TMPL.render(config)\n\n    with io.open(destination, \"w\") as f:\n        f.write(output)\n\n\nif __name__ == \"__main__\":\n    main()\n", "docs/conf.py": "# -*- coding: utf-8 -*-\n# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# google-cloud-storage documentation build configuration file\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nimport shlex\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\"..\"))\n\n# For plugins that can not read conf.py.\n# See also: https://github.com/docascode/sphinx-docfx-yaml/issues/85\nsys.path.insert(0, os.path.abspath(\".\"))\n\n__version__ = \"\"\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\nneeds_sphinx = \"1.5.5\"\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.coverage\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.viewcode\",\n    \"recommonmark\",\n]\n\n# autodoc/autosummary flags\nautoclass_content = \"both\"\nautodoc_default_options = {\"members\": True}\nautosummary_generate = True\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = ['.rst', '.md']\nsource_suffix = [\".rst\", \".md\"]\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n\n# The root toctree document.\nroot_doc = \"index\"\n\n# General information about the project.\nproject = \"google-cloud-storage\"\ncopyright = \"2019, Google\"\nauthor = \"Google APIs\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n# The short X.Y version.\nversion = \".\".join(release.split(\".\")[0:2])\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\n    \"_build\",\n    \"**/.nox/**/*\",\n    \"samples/AUTHORING_GUIDE.md\",\n    \"samples/CONTRIBUTING.md\",\n    \"samples/snippets/README.rst\",\n]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"alabaster\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n    \"description\": \"Google Cloud Client Libraries for google-cloud-storage\",\n    \"github_user\": \"googleapis\",\n    \"github_repo\": \"python-storage\",\n    \"github_banner\": True,\n    \"font_family\": \"'Roboto', Georgia, sans\",\n    \"head_font_family\": \"'Roboto', Georgia, serif\",\n    \"code_font_family\": \"'Roboto Mono', 'Consolas', monospace\",\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'\n#   'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'\n# html_search_language = 'en'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only 'ja' uses this config value\n# html_search_options = {'type': 'default'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n# html_search_scorer = 'scorer.js'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"google-cloud-storage-doc\"\n\n# -- Options for warnings ------------------------------------------------------\n\n\nsuppress_warnings = [\n    # Temporarily suppress this to avoid \"more than one target found for\n    # cross-reference\" warning, which are intractable for us to avoid while in\n    # a mono-repo.\n    # See https://github.com/sphinx-doc/sphinx/blob\n    # /2a65ffeef5c107c19084fabdd706cdff3f52d93c/sphinx/domains/python.py#L843\n    \"ref.python\"\n]\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #'preamble': '',\n    # Latex figure (float) alignment\n    #'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        root_doc,\n        \"google-cloud-storage.tex\",\n        \"google-cloud-storage Documentation\",\n        author,\n        \"manual\",\n    )\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\n        root_doc,\n        \"google-cloud-storage\",\n        \"google-cloud-storage Documentation\",\n        [author],\n        1,\n    )\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        root_doc,\n        \"google-cloud-storage\",\n        \"google-cloud-storage Documentation\",\n        author,\n        \"google-cloud-storage\",\n        \"google-cloud-storage Library\",\n        \"APIs\",\n    )\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n# texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n# texinfo_no_detailmenu = False\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \"python\": (\"https://python.readthedocs.org/en/latest/\", None),\n    \"google-auth\": (\"https://googleapis.dev/python/google-auth/latest/\", None),\n    \"google.api_core\": (\n        \"https://googleapis.dev/python/google-api-core/latest/\",\n        None,\n    ),\n    \"grpc\": (\"https://grpc.github.io/grpc/python/\", None),\n    \"proto-plus\": (\"https://proto-plus-python.readthedocs.io/en/latest/\", None),\n    \"protobuf\": (\"https://googleapis.dev/python/protobuf/latest/\", None),\n    \"requests\": (\"https://requests.readthedocs.io/en/stable/\", None),\n}\n\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n", "docs/snippets.py": "# Copyright 2016 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Testable usage examples for Google Cloud Storage API wrapper\n\nEach example function takes a ``client`` argument (which must be an instance\nof :class:`google.cloud.storage.client.Client`) and uses it to perform a task\nwith the API.\n\nTo facilitate running the examples as system tests, each example is also passed\na ``to_delete`` list;  the function adds to the list any objects created which\nneed to be deleted during teardown.\n\"\"\"\n\nfrom google.cloud import storage\n\n\ndef snippet(func):\n    \"\"\"Mark ``func`` as a snippet example function.\"\"\"\n    func._snippet = True\n    return func\n\n\n@snippet\ndef storage_get_started(to_delete):\n    # START storage_get_started\n    client = storage.Client()\n    bucket = client.get_bucket(\"bucket-id-here\")\n    # Then do other things...\n    blob = bucket.get_blob(\"/remote/path/to/file.txt\")\n    assert blob.download_as_bytes() == b\"My old contents!\"\n    blob.upload_from_string(\"New contents!\")\n    blob2 = bucket.blob(\"/remote/path/storage.txt\")\n    blob2.upload_from_filename(filename=\"/local/path.txt\")\n    # END storage_get_started\n\n    to_delete.append(bucket)\n\n\n@snippet\ndef client_bucket_acl(client, to_delete):\n    bucket_name = \"system-test-bucket\"\n    client.create_bucket(bucket_name)\n\n    # START client_bucket_acl\n    client = storage.Client()\n    bucket = client.get_bucket(bucket_name)\n    acl = bucket.acl\n    # END client_bucket_acl\n    to_delete.append(bucket)\n\n    # START acl_user_settings\n    acl.user(\"me@example.org\").grant_read()\n    acl.all_authenticated().grant_write()\n    # END acl_user_settings\n\n    # START acl_save\n    acl.save()\n    # END acl_save\n\n    # START acl_revoke_write\n    acl.all().grant_read()\n    acl.all().revoke_write()\n    # END acl_revoke_write\n\n    # START acl_save_bucket\n    bucket.acl.save(acl=acl)\n    # END acl_save_bucket\n\n    # START acl_print\n    print(list(acl))\n    # [{'role': 'OWNER', 'entity': 'allUsers'}, ...]\n    # END acl_print\n\n\n@snippet\ndef download_to_file(to_delete):\n    # START download_to_file\n    from google.cloud.storage import Blob\n\n    client = storage.Client(project=\"my-project\")\n    bucket = client.get_bucket(\"my-bucket\")\n    encryption_key = \"c7f32af42e45e85b9848a6a14dd2a8f6\"\n    blob = Blob(\"secure-data\", bucket, encryption_key=encryption_key)\n    blob.upload_from_string(\"my secret message.\")\n    with open(\"/tmp/my-secure-file\", \"wb\") as file_obj:\n        client.download_to_file(blob, file_obj)\n    # END download_to_file\n\n    to_delete.append(blob)\n\n\n@snippet\ndef upload_from_file(to_delete):\n    # START upload_from_file\n    from google.cloud.storage import Blob\n\n    client = storage.Client(project=\"my-project\")\n    bucket = client.get_bucket(\"my-bucket\")\n    encryption_key = \"aa426195405adee2c8081bb9e7e74b19\"\n    blob = Blob(\"secure-data\", bucket, encryption_key=encryption_key)\n    with open(\"my-file\", \"rb\") as my_file:\n        blob.upload_from_file(my_file)\n    # END upload_from_file\n\n    to_delete.append(blob)\n\n\n@snippet\ndef get_blob(to_delete):\n    from google.cloud.storage.blob import Blob\n\n    # START get_blob\n    client = storage.Client()\n    bucket = client.get_bucket(\"my-bucket\")\n    assert isinstance(bucket.get_blob(\"/path/to/blob.txt\"), Blob)\n    # <Blob: my-bucket, /path/to/blob.txt>\n    assert not bucket.get_blob(\"/does-not-exist.txt\")\n    # None\n    # END get_blob\n\n    to_delete.append(bucket)\n\n\n@snippet\ndef delete_blob(to_delete):\n    # START delete_blob\n    from google.cloud.exceptions import NotFound\n\n    client = storage.Client()\n    bucket = client.get_bucket(\"my-bucket\")\n    blobs = list(client.list_blobs(bucket))\n    assert len(blobs) > 0\n    # [<Blob: my-bucket, my-file.txt>]\n    bucket.delete_blob(\"my-file.txt\")\n    try:\n        bucket.delete_blob(\"doesnt-exist\")\n    except NotFound:\n        pass\n    # END delete_blob\n\n    blob = None\n    # START delete_blobs\n    bucket.delete_blobs([blob], on_error=lambda blob: None)\n    # END delete_blobs\n\n    to_delete.append(bucket)\n\n\n@snippet\ndef configure_website(to_delete):\n    bucket_name = \"test-bucket\"\n    # START configure_website\n    client = storage.Client()\n    bucket = client.get_bucket(bucket_name)\n    bucket.configure_website(\"index.html\", \"404.html\")\n    # END configure_website\n\n    # START make_public\n    bucket.make_public(recursive=True, future=True)\n    # END make_public\n\n    to_delete.append(bucket)\n\n\n@snippet\ndef get_bucket(client, to_delete):\n    import google\n\n    # START get_bucket\n    try:\n        bucket = client.get_bucket(\"my-bucket\")\n    except google.cloud.exceptions.NotFound:\n        print(\"Sorry, that bucket does not exist!\")\n    # END get_bucket\n    to_delete.append(bucket)\n\n\n@snippet\ndef add_lifecycle_delete_rule(client, to_delete):\n    # START add_lifecycle_delete_rule\n    bucket = client.get_bucket(\"my-bucket\")\n    bucket.add_lifecycle_delete_rule(age=2)\n    bucket.patch()\n    # END add_lifecycle_delete_rule\n    to_delete.append(bucket)\n\n\n@snippet\ndef add_lifecycle_set_storage_class_rule(client, to_delete):\n    # START add_lifecycle_set_storage_class_rule\n    bucket = client.get_bucket(\"my-bucket\")\n    bucket.add_lifecycle_set_storage_class_rule(\n        \"COLD_LINE\", matches_storage_class=[\"NEARLINE\"]\n    )\n    bucket.patch()\n    # END add_lifecycle_set_storage_class_rule\n    to_delete.append(bucket)\n\n\n@snippet\ndef lookup_bucket(client, to_delete):\n    from google.cloud.storage.bucket import Bucket\n\n    # START lookup_bucket\n    bucket = client.lookup_bucket(\"doesnt-exist\")\n    assert not bucket\n    # None\n    bucket = client.lookup_bucket(\"my-bucket\")\n    assert isinstance(bucket, Bucket)\n    # <Bucket: my-bucket>\n    # END lookup_bucket\n\n    to_delete.append(bucket)\n\n\n@snippet\ndef create_bucket(client, to_delete):\n    from google.cloud.storage import Bucket\n\n    # START create_bucket\n    bucket = client.create_bucket(\"my-bucket\")\n    assert isinstance(bucket, Bucket)\n    # <Bucket: my-bucket>\n    # END create_bucket\n\n    to_delete.append(bucket)\n\n\n@snippet\ndef list_buckets(client, to_delete):\n    # START list_buckets\n    for bucket in client.list_buckets():\n        print(bucket)\n    # END list_buckets\n\n    for bucket in client.list_buckets():\n        to_delete.append(bucket)\n\n\n@snippet\ndef policy_document(client):\n    # pylint: disable=unused-argument\n    # START policy_document\n    bucket = client.bucket(\"my-bucket\")\n    conditions = [[\"starts-with\", \"$key\", \"\"], {\"acl\": \"public-read\"}]\n\n    policy = bucket.generate_upload_policy(conditions)\n\n    # Generate an upload form using the form fields.\n    policy_fields = \"\".join(\n        f'<input type=\"hidden\" name=\"{key}\" value=\"{value}\">'\n        for key, value in policy.items()\n    )\n\n    upload_form = (\n        '<form action=\"http://{bucket_name}.storage.googleapis.com\"'\n        '   method=\"post\" enctype=\"multipart/form-data\">'\n        '<input type=\"text\" name=\"key\" value=\"my-test-key\">'\n        '<input type=\"hidden\" name=\"bucket\" value=\"{bucket_name}\">'\n        '<input type=\"hidden\" name=\"acl\" value=\"public-read\">'\n        '<input name=\"file\" type=\"file\">'\n        '<input type=\"submit\" value=\"Upload\">'\n        \"{policy_fields}\"\n        \"</form>\"\n    ).format(bucket_name=bucket.name, policy_fields=policy_fields)\n\n    print(upload_form)\n    # END policy_document\n\n\ndef _line_no(func):\n    code = getattr(func, \"__code__\", None) or getattr(func, \"func_code\")\n    return code.co_firstlineno\n\n\ndef _find_examples():\n    funcs = [obj for obj in globals().values() if getattr(obj, \"_snippet\", False)]\n    for func in sorted(funcs, key=_line_no):\n        yield func\n\n\ndef _name_and_doc(func):\n    return func.__name__, func.__doc__\n\n\ndef main():\n    client = storage.Client()\n    for example in _find_examples():\n        to_delete = []\n        name, doc = _name_and_doc(example)\n        print(f\"{name:>25}: {doc}\")\n\n        try:\n            example(client, to_delete)\n        except AssertionError as failure:\n            print(f\"   FAIL: {failure}\")\n        except Exception as error:  # pylint: disable=broad-except\n            print(f\"  ERROR: {error!r}\")\n        for item in to_delete:\n            item.delete()\n\n\nif __name__ == \"__main__\":\n    main()\n", "tests/__init__.py": "", "tests/system/test_blob.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport gzip\nimport io\nimport os\nimport tempfile\nimport uuid\nimport warnings\n\nimport pytest\nimport mock\n\nfrom google import resumable_media\nfrom google.api_core import exceptions\nfrom google.cloud.storage._helpers import _base64_md5hash\nfrom . import _helpers\n\nencryption_key = \"b23ff11bba187db8c37077e6af3b25b8\"\n\n\ndef _check_blob_hash(blob, info):\n    md5_hash = blob.md5_hash\n    if not isinstance(md5_hash, bytes):\n        md5_hash = md5_hash.encode(\"utf-8\")\n\n    assert md5_hash == info[\"hash\"]\n\n\ndef test_large_file_write_from_stream(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"LargeFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"big\"]\n    with open(info[\"path\"], \"rb\") as file_obj:\n        blob.upload_from_file(file_obj)\n        blobs_to_delete.append(blob)\n\n    _check_blob_hash(blob, info)\n\n\ndef test_large_file_write_from_stream_w_checksum(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"LargeFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"big\"]\n    with open(info[\"path\"], \"rb\") as file_obj:\n        blob.upload_from_file(file_obj, checksum=\"crc32c\")\n        blobs_to_delete.append(blob)\n\n    _check_blob_hash(blob, info)\n\n\ndef test_large_file_write_from_stream_w_failed_checksum(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"LargeFile{uuid.uuid4().hex}\")\n\n    # Intercept the digest processing at the last stage and replace it\n    # with garbage.  This is done with a patch to monkey-patch the\n    # resumable media library's checksum # processing;\n    # it does not mock a remote interface like a unit test would.\n    # The # remote API is still exercised.\n    info = file_data[\"big\"]\n    with open(info[\"path\"], \"rb\") as file_obj:\n        with mock.patch(\n            \"google.resumable_media._helpers.prepare_checksum_digest\",\n            return_value=\"FFFFFF==\",\n        ):\n            with pytest.raises(resumable_media.DataCorruption):\n                blob.upload_from_file(file_obj, checksum=\"crc32c\")\n\n    assert not blob.exists()\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\ndef test_large_file_write_from_stream_w_encryption_key(\n    storage_client,\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(\"LargeFile\", encryption_key=encryption_key)\n\n    info = file_data[\"big\"]\n    with open(info[\"path\"], \"rb\") as file_obj:\n        blob.upload_from_file(file_obj)\n        blobs_to_delete.append(blob)\n\n    _check_blob_hash(blob, info)\n\n    blob_without_key = shared_bucket.blob(\"LargeFile\")\n    with tempfile.TemporaryFile() as tmp:\n        with pytest.raises(exceptions.BadRequest):\n            storage_client.download_blob_to_file(blob_without_key, tmp)\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        with open(temp_f.name, \"wb\") as file_obj:\n            storage_client.download_blob_to_file(blob, file_obj)\n\n        with open(temp_f.name, \"rb\") as file_obj:\n            md5_temp_hash = _base64_md5hash(file_obj)\n\n    assert md5_temp_hash == info[\"hash\"]\n\n\ndef test_small_file_write_from_filename(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"SmallFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"simple\"]\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n\n    _check_blob_hash(blob, info)\n\n\ndef test_small_file_write_from_filename_with_checksum(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"SmallFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"simple\"]\n    blob.upload_from_filename(info[\"path\"], checksum=\"crc32c\")\n    blobs_to_delete.append(blob)\n\n    _check_blob_hash(blob, info)\n\n\ndef test_small_file_write_from_filename_with_failed_checksum(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"SmallFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"simple\"]\n    # Intercept the digest processing at the last stage and replace\n    # it with garbage\n    with mock.patch(\n        \"google.resumable_media._helpers.prepare_checksum_digest\",\n        return_value=\"FFFFFF==\",\n    ):\n        with pytest.raises(exceptions.BadRequest):\n            blob.upload_from_filename(info[\"path\"], checksum=\"crc32c\")\n\n    assert not blob.exists()\n\n\ndef test_blob_crud_w_user_project(\n    storage_client,\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n    user_project,\n):\n    gen1_payload = b\"gen1\"\n    with_user_project = storage_client.bucket(\n        shared_bucket.name, user_project=user_project\n    )\n    blob = with_user_project.blob(\"SmallFile\")\n\n    info = file_data[\"simple\"]\n    with open(info[\"path\"], mode=\"rb\") as to_read:\n        gen0_payload = to_read.read()\n\n    # Exercise 'objects.insert' w/ userProject.\n    blob.upload_from_filename(info[\"path\"])\n    gen0 = blob.generation\n    etag0 = blob.etag\n\n    # Upload a second generation of the blob\n    blob.upload_from_string(gen1_payload)\n    gen1 = blob.generation\n    etag1 = blob.etag\n\n    blob0 = with_user_project.blob(\"SmallFile\", generation=gen0)\n    blob1 = with_user_project.blob(\"SmallFile\", generation=gen1)\n\n    # Exercise 'objects.get' w/ generation\n    blob1 = with_user_project.get_blob(blob.name)\n    assert blob1.generation == gen1\n    assert blob1.etag == etag1\n    blob0 = with_user_project.get_blob(blob.name, generation=gen0)\n    assert blob0.generation == gen0\n    assert blob0.etag == etag0\n\n    try:\n        # Exercise 'objects.get' (metadata) w/ userProject.\n        assert blob.exists()\n        blob.reload()\n\n        # Exercise 'objects.get' (media) w/ userProject.\n        blob0 = with_user_project.blob(\"SmallFile\", generation=gen0)\n        blob1 = with_user_project.blob(\"SmallFile\", generation=gen1)\n        assert blob0.etag is None\n        assert blob1.etag is None\n        assert blob0.download_as_bytes() == gen0_payload\n        assert blob1.download_as_bytes() == gen1_payload\n        assert blob0.etag == etag0\n        assert blob1.etag == etag1\n\n        # Exercise 'objects.patch' w/ userProject.\n        blob0.content_language = \"en\"\n        blob0.patch()\n        assert blob0.content_language == \"en\"\n        assert blob1.content_language is None\n\n        # Exercise 'objects.update' w/ userProject.\n        metadata = {\"foo\": \"Foo\", \"bar\": \"Bar\"}\n        blob0.metadata = metadata\n        blob0.update()\n        assert blob0.metadata == metadata\n        assert blob1.metadata is None\n\n    finally:\n        # Exercise 'objects.delete' (metadata) w/ userProject.\n        blobs = storage_client.list_blobs(\n            with_user_project, prefix=blob.name, versions=True\n        )\n        assert [each.generation for each in blobs] == [gen0, gen1]\n\n        blob0.delete()\n        blobs = storage_client.list_blobs(\n            with_user_project, prefix=blob.name, versions=True\n        )\n        assert [each.generation for each in blobs] == [gen1]\n\n        blob1.delete()\n\n\ndef test_blob_crud_w_etag_match(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    wrong_etag = \"kittens\"\n\n    blob = shared_bucket.blob(\"SmallFile\")\n\n    info = file_data[\"simple\"]\n    with open(info[\"path\"], mode=\"rb\") as to_read:\n        payload = to_read.read()\n\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n    etag = blob.etag\n\n    fresh_blob = shared_bucket.blob(\"SmallFile\")\n\n    # Exercise 'objects.get' (metadata) w/ etag match.\n    with pytest.raises(exceptions.PreconditionFailed):\n        fresh_blob.exists(if_etag_match=wrong_etag)\n\n    with pytest.raises(exceptions.NotModified):\n        fresh_blob.exists(if_etag_not_match=etag)\n\n    assert fresh_blob.exists(if_etag_match=etag)\n    assert fresh_blob.exists(if_etag_not_match=wrong_etag)\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        fresh_blob.reload(if_etag_match=wrong_etag)\n\n    with pytest.raises(exceptions.NotModified):\n        fresh_blob.reload(if_etag_not_match=etag)\n\n    fresh_blob.reload(if_etag_match=etag)  # no raise\n    fresh_blob.reload(if_etag_not_match=wrong_etag)  # no raise\n\n    # Exercise 'objects.get' (media) w/ etag match.\n    assert fresh_blob.download_as_bytes(if_etag_match=etag) == payload\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        fresh_blob.download_as_bytes(if_etag_match=wrong_etag)\n\n    with pytest.raises(exceptions.NotModified):\n        fresh_blob.download_as_bytes(if_etag_not_match=etag)\n\n\ndef test_blob_crud_w_generation_match(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    wrong_generation_number = 6\n    wrong_metageneration_number = 9\n    gen1_payload = b\"gen1\"\n\n    blob = shared_bucket.blob(\"SmallFile\")\n\n    info = file_data[\"simple\"]\n    with open(info[\"path\"], mode=\"rb\") as to_read:\n        gen0_payload = to_read.read()\n\n    blob.upload_from_filename(info[\"path\"])\n    gen0 = blob.generation\n\n    # Upload a second generation of the blob\n    blob.upload_from_string(gen1_payload)\n    gen1 = blob.generation\n\n    blob0 = shared_bucket.blob(\"SmallFile\", generation=gen0)\n    blob1 = shared_bucket.blob(\"SmallFile\", generation=gen1)\n\n    try:\n        # Exercise 'objects.get' (metadata) w/ generation match.\n        with pytest.raises(exceptions.PreconditionFailed):\n            blob.exists(if_generation_match=wrong_generation_number)\n\n        assert blob.exists(if_generation_match=gen1)\n\n        with pytest.raises(exceptions.PreconditionFailed):\n            blob.reload(if_metageneration_match=wrong_metageneration_number)\n\n        blob.reload(if_generation_match=gen1)\n\n        # Exercise 'objects.get' (media) w/ generation match.\n        assert blob0.download_as_bytes(if_generation_match=gen0) == gen0_payload\n        assert blob1.download_as_bytes(if_generation_not_match=gen0) == gen1_payload\n\n        # Exercise 'objects.patch' w/ generation match.\n        blob0.content_language = \"en\"\n        blob0.patch(if_generation_match=gen0)\n\n        assert blob0.content_language == \"en\"\n        assert blob1.content_language is None\n\n        # Exercise 'objects.update' w/ generation match.\n        metadata = {\"foo\": \"Foo\", \"bar\": \"Bar\"}\n        blob0.metadata = metadata\n        blob0.update(if_generation_match=gen0)\n\n        assert blob0.metadata == metadata\n        assert blob1.metadata is None\n    finally:\n        # Exercise 'objects.delete' (metadata) w/ generation match.\n        with pytest.raises(exceptions.PreconditionFailed):\n            blob0.delete(if_metageneration_match=wrong_metageneration_number)\n\n        blob0.delete(if_generation_match=gen0)\n        blob1.delete(if_metageneration_not_match=wrong_metageneration_number)\n\n\ndef test_blob_acl_w_user_project(\n    storage_client,\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n    user_project,\n):\n    with_user_project = storage_client.bucket(\n        shared_bucket.name, user_project=user_project\n    )\n    blob = with_user_project.blob(f\"SmallFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"simple\"]\n\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n\n    # Exercise blob ACL w/ userProject\n    acl = blob.acl\n    acl.reload()\n    acl.all().grant_read()\n    acl.save()\n    assert \"READER\" in acl.all().get_roles()\n\n    del acl.entities[\"allUsers\"]\n    acl.save()\n    assert not acl.has_entity(\"allUsers\")\n\n\ndef test_blob_acl_w_metageneration_match(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    wrong_metageneration_number = 9\n    wrong_generation_number = 6\n\n    blob = shared_bucket.blob(\"FilePatchACL\")\n    info = file_data[\"simple\"]\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n\n    # Exercise blob ACL with metageneration/generation match\n    acl = blob.acl\n    blob.reload()\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        acl.save_predefined(\n            \"publicRead\", if_metageneration_match=wrong_metageneration_number\n        )\n        assert \"READER\" not in acl.all().get_roles()\n\n    acl.save_predefined(\"publicRead\", if_metageneration_match=blob.metageneration)\n    assert \"READER\" in acl.all().get_roles()\n\n    blob.reload()\n    del acl.entities[\"allUsers\"]\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        acl.save(if_generation_match=wrong_generation_number)\n        assert acl.has_entity(\"allUsers\")\n\n    acl.save(if_generation_match=blob.generation)\n    assert not acl.has_entity(\"allUsers\")\n\n\ndef test_blob_acl_upload_predefined(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    control = shared_bucket.blob(f\"logo{uuid.uuid4().hex}\")\n    control_info = file_data[\"logo\"]\n\n    blob = shared_bucket.blob(f\"SmallFile{uuid.uuid4().hex}\")\n    info = file_data[\"simple\"]\n\n    try:\n        control.upload_from_filename(control_info[\"path\"])\n    finally:\n        blobs_to_delete.append(control)\n\n    try:\n        blob.upload_from_filename(info[\"path\"], predefined_acl=\"publicRead\")\n    finally:\n        blobs_to_delete.append(blob)\n\n    control_acl = control.acl\n    assert \"READER\" not in control_acl.all().get_roles()\n\n    acl = blob.acl\n    assert \"READER\" in acl.all().get_roles()\n\n    acl.all().revoke_read()\n    assert acl.all().get_roles() == set()\n    assert control_acl.all().get_roles() == acl.all().get_roles()\n\n\ndef test_blob_patch_metadata(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    filename = file_data[\"logo\"][\"path\"]\n    blob_name = os.path.basename(filename)\n\n    blob = shared_bucket.blob(blob_name)\n    blob.upload_from_filename(filename)\n    blobs_to_delete.append(blob)\n\n    # NOTE: This should not be necessary. We should be able to pass\n    #       it in to upload_file and also to upload_from_string.\n    blob.content_type = \"image/png\"\n    assert blob.content_type == \"image/png\"\n\n    metadata = {\"foo\": \"Foo\", \"bar\": \"Bar\"}\n    blob.metadata = metadata\n    blob.patch()\n    blob.reload()\n    assert blob.metadata == metadata\n\n    # Ensure that metadata keys can be deleted by setting equal to None.\n    new_metadata = {\"foo\": \"Foo\", \"bar\": None}\n    blob.metadata = new_metadata\n    blob.patch()\n    blob.reload()\n    assert blob.metadata == {\"foo\": \"Foo\"}\n\n\ndef test_blob_direct_write_and_read_into_file(\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    payload = b\"Hello World\"\n    blob = shared_bucket.blob(\"MyBuffer\")\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    same_blob = shared_bucket.blob(\"MyBuffer\")\n    same_blob.reload()  # Initialize properties.\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        with open(temp_f.name, \"wb\") as file_obj:\n            same_blob.download_to_file(file_obj)\n\n        with open(temp_f.name, \"rb\") as file_obj:\n            stored_contents = file_obj.read()\n\n    assert stored_contents == payload\n\n\ndef test_blob_download_w_generation_match(\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    wrong_generation_number = 6\n\n    blob = shared_bucket.blob(\"MyBuffer\")\n    payload = b\"Hello World\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    same_blob = shared_bucket.blob(\"MyBuffer\")\n    same_blob.reload()  # Initialize properties.\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        with open(temp_f.name, \"wb\") as file_obj:\n            with pytest.raises(exceptions.PreconditionFailed):\n                same_blob.download_to_file(\n                    file_obj, if_generation_match=wrong_generation_number\n                )\n\n            same_blob.download_to_file(\n                file_obj,\n                if_generation_match=blob.generation,\n                if_metageneration_match=blob.metageneration,\n            )\n\n        with open(temp_f.name, \"rb\") as file_obj:\n            stored_contents = file_obj.read()\n\n    assert stored_contents == payload\n\n\ndef test_blob_download_w_failed_crc32c_checksum(\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    blob = shared_bucket.blob(\"FailedChecksumBlob\")\n    payload = b\"Hello World\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        # Intercept the digest processing at the last stage and replace\n        # it with garbage.  This is done with a patch to monkey-patch\n        # the resumable media library's checksum processing; it does not\n        # mock a remote interface like a unit test would.\n        # The remote API is still exercised.\n        with mock.patch(\n            \"google.resumable_media._helpers.prepare_checksum_digest\",\n            return_value=\"FFFFFF==\",\n        ):\n            with pytest.raises(resumable_media.DataCorruption):\n                blob.download_to_filename(temp_f.name, checksum=\"crc32c\")\n\n            # Confirm the file was deleted on failure\n            assert not os.path.isfile(temp_f.name)\n\n            # Now download with checksumming turned off\n            blob.download_to_filename(temp_f.name, checksum=None)\n\n        with open(temp_f.name, \"rb\") as file_obj:\n            stored_contents = file_obj.read()\n\n        assert stored_contents == payload\n\n\ndef test_blob_download_as_text(\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    blob = shared_bucket.blob(\"MyBuffer\")\n    payload = \"Hello World\"\n    blob.upload_from_string(payload)\n    etag = blob.etag\n    blobs_to_delete.append(blob)\n\n    blob = shared_bucket.blob(\"MyBuffer\")\n    assert blob.etag is None\n    stored_contents = blob.download_as_text()\n    assert stored_contents == payload\n    assert blob.etag == etag\n\n    # Test download with byte range\n    end_byte = 5\n    stored_contents = blob.download_as_text(start=0, end=end_byte - 1)\n    assert stored_contents == payload[0:end_byte]\n\n\ndef test_blob_upload_w_gzip_encoded_download_raw(\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    payload = b\"DEADBEEF\" * 1000\n    raw_stream = io.BytesIO()\n    with gzip.GzipFile(fileobj=raw_stream, mode=\"wb\") as gzip_stream:\n        gzip_stream.write(payload)\n    zipped = raw_stream.getvalue()\n\n    blob = shared_bucket.blob(\"test_gzipped.gz\")\n    blob.content_encoding = \"gzip\"\n    blob.upload_from_file(raw_stream, rewind=True)\n    blobs_to_delete.append(blob)\n\n    expanded = blob.download_as_bytes()\n    assert expanded == payload\n\n    raw = blob.download_as_bytes(raw_download=True)\n    assert raw == zipped\n\n\ndef test_blob_upload_from_file_resumable_with_generation(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(f\"LargeFile{uuid.uuid4().hex}\")\n    wrong_generation = 3\n    wrong_meta_generation = 3\n\n    # uploading the file\n    info = file_data[\"big\"]\n    with open(info[\"path\"], \"rb\") as file_obj:\n        blob.upload_from_file(file_obj)\n        blobs_to_delete.append(blob)\n\n    # reuploading with correct generations numbers\n    with open(info[\"path\"], \"rb\") as file_obj:\n        blob.upload_from_file(\n            file_obj,\n            if_generation_match=blob.generation,\n            if_metageneration_match=blob.metageneration,\n        )\n\n    # reuploading with generations numbers that doesn't match original\n    with pytest.raises(exceptions.PreconditionFailed):\n        with open(info[\"path\"], \"rb\") as file_obj:\n            blob.upload_from_file(\n                file_obj,\n                if_generation_match=wrong_generation,\n            )\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        with open(info[\"path\"], \"rb\") as file_obj:\n            blob.upload_from_file(\n                file_obj,\n                if_metageneration_match=wrong_meta_generation,\n            )\n\n\ndef test_blob_upload_from_string_w_owner(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(\"MyBuffer\")\n    payload = b\"Hello World\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    same_blob = shared_bucket.blob(\"MyBuffer\")\n    same_blob.reload(projection=\"full\")  # Initialize properties.\n    user_email = service_account.service_account_email\n    owner = same_blob.owner\n    assert user_email in owner[\"entity\"]\n\n\ndef test_blob_upload_from_string_w_custom_time(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(\"CustomTimeBlob\")\n    payload = b\"Hello World\"\n    current_time = datetime.datetime.now()\n    blob.custom_time = current_time\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    same_blob = shared_bucket.blob(\"CustomTimeBlob\")\n    same_blob.reload(projection=\"full\")\n    custom_time = same_blob.custom_time.replace(tzinfo=None)\n    assert custom_time == current_time\n\n\ndef test_blob_upload_from_string_w_custom_time_no_micros(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    # Test that timestamps without microseconds are treated correctly by\n    # custom_time encoding/decoding.\n    blob = shared_bucket.blob(\"CustomTimeNoMicrosBlob\")\n    payload = b\"Hello World\"\n    time_without_micros = datetime.datetime(2021, 2, 10, 12, 30)\n    blob.custom_time = time_without_micros\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    same_blob = shared_bucket.blob((\"CustomTimeNoMicrosBlob\"))\n    same_blob.reload(projection=\"full\")\n    custom_time = same_blob.custom_time.replace(tzinfo=None)\n    assert custom_time == time_without_micros\n\n\ndef test_blob_upload_download_crc32_md5_hash(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(\"MyBuffer\")\n    payload = b\"Hello World\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    download_blob = shared_bucket.blob(\"MyBuffer\")\n\n    assert download_blob.download_as_bytes() == payload\n    assert download_blob.crc32c == blob.crc32c\n    assert download_blob.md5_hash == blob.md5_hash\n\n\n@pytest.mark.parametrize(\n    \"blob_name,payload\",\n    [\n        (\"Caf\\u00e9\", b\"Normalization Form C\"),\n        (\"Cafe\\u0301\", b\"Normalization Form D\"),\n    ],\n)\ndef test_blob_w_unicode_names(blob_name, payload, shared_bucket, blobs_to_delete):\n    # Historical note: This test when originally written accessed public\n    # files with Unicode names. These files are no longer available, so it\n    # was rewritten to upload them first.\n\n    # Normalization form C: a single character for e-acute;\n    # URL should end with Cafe%CC%81\n    # Normalization Form D: an ASCII e followed by U+0301 combining\n    # character; URL should end with Caf%C3%A9\n\n    blob = shared_bucket.blob(blob_name)\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    same_blob = shared_bucket.blob(blob_name)\n    assert same_blob.download_as_bytes() == payload\n    assert same_blob.name == blob_name\n\n\ndef test_blob_compose_new_blob(shared_bucket, blobs_to_delete):\n    payload_1 = b\"AAA\\n\"\n    source_1 = shared_bucket.blob(\"source-1\")\n    source_1.upload_from_string(payload_1)\n    blobs_to_delete.append(source_1)\n\n    payload_2 = b\"BBB\\n\"\n    source_2 = shared_bucket.blob(\"source-2\")\n    source_2.upload_from_string(payload_2)\n    blobs_to_delete.append(source_2)\n\n    destination = shared_bucket.blob(\"destination\")\n    destination.content_type = \"text/plain\"\n    destination.compose([source_1, source_2])\n    blobs_to_delete.append(destination)\n\n    assert destination.download_as_bytes() == payload_1 + payload_2\n\n\ndef test_blob_compose_new_blob_wo_content_type(shared_bucket, blobs_to_delete):\n    payload_1 = b\"AAA\\n\"\n    source_1 = shared_bucket.blob(\"source-1\")\n    source_1.upload_from_string(payload_1)\n    blobs_to_delete.append(source_1)\n\n    payload_2 = b\"BBB\\n\"\n    source_2 = shared_bucket.blob(\"source-2\")\n    source_2.upload_from_string(payload_2)\n    blobs_to_delete.append(source_2)\n\n    destination = shared_bucket.blob(\"destination\")\n\n    destination.compose([source_1, source_2])\n    blobs_to_delete.append(destination)\n\n    assert destination.content_type is None\n    assert destination.download_as_bytes() == payload_1 + payload_2\n\n\ndef test_blob_compose_replace_existing_blob(shared_bucket, blobs_to_delete):\n    payload_before = b\"AAA\\n\"\n    original = shared_bucket.blob(uuid.uuid4().hex)\n    original.content_type = \"text/plain\"\n    original.upload_from_string(payload_before)\n    blobs_to_delete.append(original)\n\n    payload_to_append = b\"BBB\\n\"\n    to_append = shared_bucket.blob(uuid.uuid4().hex)\n    to_append.upload_from_string(payload_to_append)\n    blobs_to_delete.append(to_append)\n\n    original.compose([original, to_append])\n\n    assert original.download_as_bytes() == payload_before + payload_to_append\n\n\ndef test_blob_compose_w_generation_match_list(shared_bucket, blobs_to_delete):\n    payload_before = b\"AAA\\n\"\n    original = shared_bucket.blob(uuid.uuid4().hex)\n    original.content_type = \"text/plain\"\n    original.upload_from_string(payload_before)\n    blobs_to_delete.append(original)\n    wrong_generations = [6, 7]\n    wrong_metagenerations = [8, 9]\n\n    payload_to_append = b\"BBB\\n\"\n    to_append = shared_bucket.blob(uuid.uuid4().hex)\n    to_append.upload_from_string(payload_to_append)\n    blobs_to_delete.append(to_append)\n\n    with warnings.catch_warnings(record=True) as log:\n        with pytest.raises(exceptions.PreconditionFailed):\n            original.compose(\n                [original, to_append],\n                if_generation_match=wrong_generations,\n                if_metageneration_match=wrong_metagenerations,\n            )\n    assert len(log) == 2\n\n    with warnings.catch_warnings(record=True) as log:\n        original.compose(\n            [original, to_append],\n            if_generation_match=[original.generation, to_append.generation],\n            if_metageneration_match=[original.metageneration, to_append.metageneration],\n        )\n    assert len(log) == 2\n\n    assert original.download_as_bytes() == payload_before + payload_to_append\n\n\ndef test_blob_compose_w_generation_match_long(shared_bucket, blobs_to_delete):\n    payload_before = b\"AAA\\n\"\n    original = shared_bucket.blob(uuid.uuid4().hex)\n    original.content_type = \"text/plain\"\n    original.upload_from_string(payload_before)\n    blobs_to_delete.append(original)\n\n    payload_to_append = b\"BBB\\n\"\n    to_append = shared_bucket.blob(uuid.uuid4().hex)\n    to_append.upload_from_string(payload_to_append)\n    blobs_to_delete.append(to_append)\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        original.compose([original, to_append], if_generation_match=0)\n\n    original.compose([original, to_append], if_generation_match=original.generation)\n\n    assert original.download_as_bytes() == payload_before + payload_to_append\n\n\ndef test_blob_compose_w_source_generation_match(shared_bucket, blobs_to_delete):\n    payload_before = b\"AAA\\n\"\n    original = shared_bucket.blob(uuid.uuid4().hex)\n    original.content_type = \"text/plain\"\n    original.upload_from_string(payload_before)\n    blobs_to_delete.append(original)\n    wrong_source_generations = [6, 7]\n\n    payload_to_append = b\"BBB\\n\"\n    to_append = shared_bucket.blob(uuid.uuid4().hex)\n    to_append.upload_from_string(payload_to_append)\n    blobs_to_delete.append(to_append)\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        original.compose(\n            [original, to_append],\n            if_source_generation_match=wrong_source_generations,\n        )\n\n    original.compose(\n        [original, to_append],\n        if_source_generation_match=[original.generation, to_append.generation],\n    )\n\n    assert original.download_as_bytes() == payload_before + payload_to_append\n\n\ndef test_blob_compose_w_user_project(storage_client, buckets_to_delete, user_project):\n    new_bucket_name = _helpers.unique_name(\"compose-user-project\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(new_bucket_name)\n    buckets_to_delete.append(created)\n    created.requester_pays = True\n\n    payload_1 = b\"AAA\\n\"\n    source_1 = created.blob(uuid.uuid4().hex)\n    source_1.upload_from_string(payload_1)\n\n    payload_2 = b\"BBB\\n\"\n    source_2 = created.blob(uuid.uuid4().hex)\n    source_2.upload_from_string(payload_2)\n\n    with_user_project = storage_client.bucket(\n        new_bucket_name, user_project=user_project\n    )\n\n    destination = with_user_project.blob(uuid.uuid4().hex)\n    destination.content_type = \"text/plain\"\n    destination.compose([source_1, source_2])\n\n    assert destination.download_as_bytes() == payload_1 + payload_2\n\n\ndef test_blob_rewrite_new_blob_add_key(shared_bucket, blobs_to_delete, file_data):\n    info = file_data[\"simple\"]\n    source = shared_bucket.blob(uuid.uuid4().hex)\n    source.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(source)\n    source_data = source.download_as_bytes()\n\n    key = os.urandom(32)\n    dest = shared_bucket.blob(uuid.uuid4().hex, encryption_key=key)\n    token, rewritten, total = dest.rewrite(source)\n    blobs_to_delete.append(dest)\n\n    assert token is None\n    assert rewritten == len(source_data)\n    assert total == len(source_data)\n    assert dest.download_as_bytes() == source_data\n\n\ndef test_blob_rewrite_rotate_key(shared_bucket, blobs_to_delete, file_data):\n    blob_name = \"rotating-keys\"\n    info = file_data[\"simple\"]\n\n    source_key = os.urandom(32)\n    source = shared_bucket.blob(blob_name, encryption_key=source_key)\n    source.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(source)\n    source_data = source.download_as_bytes()\n\n    dest_key = os.urandom(32)\n    dest = shared_bucket.blob(blob_name, encryption_key=dest_key)\n    token, rewritten, total = dest.rewrite(source)\n    # Not adding 'dest' to 'blobs_to_delete':  it is the\n    # same object as 'source'.\n\n    assert token is None\n    assert rewritten == len(source_data)\n    assert total == len(source_data)\n    assert dest.download_as_bytes() == source_data\n\n\ndef test_blob_rewrite_add_key_w_user_project(\n    storage_client, buckets_to_delete, user_project, file_data\n):\n    info = file_data[\"simple\"]\n    new_bucket_name = _helpers.unique_name(\"rewrite-key-up\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(new_bucket_name)\n    buckets_to_delete.append(created)\n    created.requester_pays = True\n\n    with_user_project = storage_client.bucket(\n        new_bucket_name, user_project=user_project\n    )\n\n    source = with_user_project.blob(\"source\")\n    source.upload_from_filename(info[\"path\"])\n    source_data = source.download_as_bytes()\n\n    key = os.urandom(32)\n    dest = with_user_project.blob(\"dest\", encryption_key=key)\n    token, rewritten, total = dest.rewrite(source)\n\n    assert token is None\n    assert rewritten == len(source_data)\n    assert total == len(source_data)\n    assert dest.download_as_bytes() == source_data\n\n\ndef test_blob_rewrite_rotate_key_w_user_project(\n    storage_client, buckets_to_delete, user_project, file_data\n):\n    blob_name = \"rotating-keys\"\n    info = file_data[\"simple\"]\n    new_bucket_name = _helpers.unique_name(\"rewrite-key-up\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(new_bucket_name)\n    buckets_to_delete.append(created)\n    created.requester_pays = True\n\n    with_user_project = storage_client.bucket(\n        new_bucket_name, user_project=user_project\n    )\n\n    source_key = os.urandom(32)\n    source = with_user_project.blob(blob_name, encryption_key=source_key)\n    source.upload_from_filename(info[\"path\"])\n    source_data = source.download_as_bytes()\n\n    dest_key = os.urandom(32)\n    dest = with_user_project.blob(blob_name, encryption_key=dest_key)\n    token, rewritten, total = dest.rewrite(source)\n\n    assert token is None\n    assert rewritten == len(source_data)\n    assert total == len(source_data)\n    assert dest.download_as_bytes() == source_data\n\n\ndef test_blob_rewrite_w_generation_match(shared_bucket, blobs_to_delete, file_data):\n    wrong_generation_number = 6\n    blob_name = \"generation-match\"\n    info = file_data[\"simple\"]\n\n    source = shared_bucket.blob(blob_name)\n    source.upload_from_filename(info[\"path\"])\n    source_data = source.download_as_bytes()\n    blobs_to_delete.append(source)\n\n    dest = shared_bucket.blob(blob_name)\n    dest.reload()\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        dest.rewrite(source, if_generation_match=wrong_generation_number)\n\n    token, rewritten, total = dest.rewrite(\n        source,\n        if_generation_match=dest.generation,\n        if_source_generation_match=source.generation,\n        if_source_metageneration_match=source.metageneration,\n    )\n\n    assert token is None\n    assert rewritten == len(source_data)\n    assert total == len(source_data)\n    assert dest.download_as_bytes() == source_data\n\n\ndef test_blob_update_storage_class_small_file(\n    shared_bucket, blobs_to_delete, file_data\n):\n    from google.cloud.storage import constants\n\n    blob = shared_bucket.blob(\"SmallFile\")\n\n    info = file_data[\"simple\"]\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n\n    blob.update_storage_class(constants.NEARLINE_STORAGE_CLASS)\n    blob.reload()\n    assert blob.storage_class == constants.NEARLINE_STORAGE_CLASS\n\n    blob.update_storage_class(constants.COLDLINE_STORAGE_CLASS)\n    blob.reload()\n    assert blob.storage_class == constants.COLDLINE_STORAGE_CLASS\n\n\ndef test_blob_update_storage_class_large_file(\n    shared_bucket, blobs_to_delete, file_data\n):\n    from google.cloud.storage import constants\n\n    blob = shared_bucket.blob(f\"BigFile{uuid.uuid4().hex}\")\n\n    info = file_data[\"big\"]\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n\n    blob.update_storage_class(constants.NEARLINE_STORAGE_CLASS)\n    blob.reload()\n    assert blob.storage_class == constants.NEARLINE_STORAGE_CLASS\n\n    blob.update_storage_class(constants.COLDLINE_STORAGE_CLASS)\n    blob.reload()\n    assert blob.storage_class == constants.COLDLINE_STORAGE_CLASS\n\n\ndef test_object_retention_lock(storage_client, buckets_to_delete, blobs_to_delete):\n    from google.cloud.storage._helpers import _NOW\n    from google.cloud.storage._helpers import _UTC\n\n    # Test bucket created with object retention enabled\n    new_bucket_name = _helpers.unique_name(\"object-retention\")\n    created_bucket = _helpers.retry_429_503(storage_client.create_bucket)(\n        new_bucket_name, enable_object_retention=True\n    )\n    buckets_to_delete.append(created_bucket)\n    assert created_bucket.object_retention_mode == \"Enabled\"\n\n    # Test create object with object retention enabled\n    payload = b\"Hello World\"\n    mode = \"Unlocked\"\n    current_time = _NOW(_UTC).replace(tzinfo=None)\n    expiration_time = current_time + datetime.timedelta(seconds=10)\n    blob = created_bucket.blob(\"object-retention-lock\")\n    blob.retention.mode = mode\n    blob.retention.retain_until_time = expiration_time\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n    blob.reload()\n    assert blob.retention.mode == mode\n\n    # Test patch object to disable object retention\n    blob.retention.mode = None\n    blob.retention.retain_until_time = None\n    blob.patch(override_unlocked_retention=True)\n    assert blob.retention.mode is None\n", "tests/system/test_hmac_key_metadata.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\n\nimport pytest\n\nfrom . import _helpers\n\n\ndef ensure_hmac_key_deleted(hmac_key):\n    from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n    if hmac_key.state != HMACKeyMetadata.INACTIVE_STATE:\n        hmac_key.state = HMACKeyMetadata.INACTIVE_STATE\n        hmac_key.update()\n    _helpers.retry_429_harder(hmac_key.delete)()\n\n\n@pytest.fixture\ndef scrubbed_hmac_keys(storage_client):\n    from google.cloud.storage._helpers import _NOW\n    from google.cloud.storage._helpers import _UTC\n\n    before_hmac_keys = set(storage_client.list_hmac_keys())\n\n    now = _NOW(_UTC)\n    yesterday = now - datetime.timedelta(days=1)\n\n    # Delete any HMAC keys older than a day.\n    for hmac_key in list(before_hmac_keys):\n        if hmac_key.time_created < yesterday:\n            ensure_hmac_key_deleted(hmac_key)\n            before_hmac_keys.remove(hmac_key)\n\n    hmac_keys_to_delete = []\n    yield before_hmac_keys, hmac_keys_to_delete\n\n    # Delete any HMAC keys we created\n    for hmac_key in hmac_keys_to_delete:\n        ensure_hmac_key_deleted(hmac_key)\n\n\ndef test_hmac_key_crud(storage_client, scrubbed_hmac_keys, service_account):\n    from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n    before_hmac_keys, hmac_keys_to_delete = scrubbed_hmac_keys\n\n    email = service_account.service_account_email\n\n    metadata, secret = storage_client.create_hmac_key(email)\n    hmac_keys_to_delete.append(metadata)\n\n    assert isinstance(secret, str)\n    assert len(secret) == 40\n\n    after_hmac_keys = set(storage_client.list_hmac_keys())\n    assert metadata not in before_hmac_keys\n    assert metadata in after_hmac_keys\n\n    another = HMACKeyMetadata(storage_client)\n    another._properties[\"accessId\"] = \"nonesuch\"\n\n    assert not another.exists()\n\n    another._properties[\"accessId\"] = metadata.access_id\n    assert another.exists()\n\n    another.reload()\n\n    assert another._properties == metadata._properties\n\n    metadata.state = HMACKeyMetadata.INACTIVE_STATE\n    metadata.update()\n\n    metadata.delete()\n    hmac_keys_to_delete.remove(metadata)\n", "tests/system/test__signing.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport datetime\nimport hashlib\nimport os\nimport time\nimport pytest\nimport requests\n\nfrom google.api_core import path_template\nfrom google.cloud import iam_credentials_v1\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\nfrom . import _helpers\n\n\ndef _morph_expiration(version, expiration):\n    if expiration is not None:\n        return expiration\n\n    if version == \"v2\":\n        return int(time.time()) + 10\n\n    return 10\n\n\ndef _create_signed_list_blobs_url_helper(\n    client, bucket, version, expiration=None, method=\"GET\"\n):\n    expiration = _morph_expiration(version, expiration)\n\n    signed_url = bucket.generate_signed_url(\n        expiration=expiration,\n        method=method,\n        client=client,\n        version=version,\n        api_access_endpoint=_helpers._get_default_storage_base_url(),\n    )\n\n    response = requests.get(signed_url)\n    assert response.status_code == 200\n\n\ndef test_create_signed_list_blobs_url_v2(storage_client, signing_bucket, no_mtls):\n    _create_signed_list_blobs_url_helper(\n        storage_client,\n        signing_bucket,\n        version=\"v2\",\n    )\n\n\ndef test_create_signed_list_blobs_url_v2_w_expiration(\n    storage_client, signing_bucket, no_mtls\n):\n    now = _NOW(_UTC).replace(tzinfo=None)\n    delta = datetime.timedelta(seconds=10)\n\n    _create_signed_list_blobs_url_helper(\n        storage_client,\n        signing_bucket,\n        expiration=now + delta,\n        version=\"v2\",\n    )\n\n\ndef test_create_signed_list_blobs_url_v4(storage_client, signing_bucket, no_mtls):\n    _create_signed_list_blobs_url_helper(\n        storage_client,\n        signing_bucket,\n        version=\"v4\",\n    )\n\n\ndef test_create_signed_list_blobs_url_v4_w_expiration(\n    storage_client, signing_bucket, no_mtls\n):\n    now = _NOW(_UTC).replace(tzinfo=None)\n    delta = datetime.timedelta(seconds=10)\n    _create_signed_list_blobs_url_helper(\n        storage_client,\n        signing_bucket,\n        expiration=now + delta,\n        version=\"v4\",\n    )\n\n\ndef _create_signed_read_url_helper(\n    client,\n    bucket,\n    blob_name=\"LogoToSign.jpg\",\n    method=\"GET\",\n    version=\"v2\",\n    payload=None,\n    expiration=None,\n    encryption_key=None,\n    service_account_email=None,\n    access_token=None,\n):\n    expiration = _morph_expiration(version, expiration)\n\n    if payload is not None:\n        blob = bucket.blob(blob_name, encryption_key=encryption_key)\n        blob.upload_from_string(payload)\n    else:\n        blob = bucket.get_blob(\"README.txt\")\n\n    signed_url = blob.generate_signed_url(\n        expiration=expiration,\n        method=method,\n        client=client,\n        version=version,\n        service_account_email=service_account_email,\n        access_token=access_token,\n    )\n\n    headers = {}\n\n    if encryption_key is not None:\n        headers[\"x-goog-encryption-algorithm\"] = \"AES256\"\n        encoded_key = base64.b64encode(encryption_key).decode(\"utf-8\")\n        headers[\"x-goog-encryption-key\"] = encoded_key\n        key_hash = hashlib.sha256(encryption_key).digest()\n        key_hash = base64.b64encode(key_hash).decode(\"utf-8\")\n        headers[\"x-goog-encryption-key-sha256\"] = key_hash\n\n    response = requests.get(signed_url, headers=headers)\n    assert response.status_code == 200\n\n    if payload is not None:\n        assert response.content == payload\n    else:\n        assert response.content == _helpers.signing_blob_content\n\n\ndef test_create_signed_read_url_v2(storage_client, signing_bucket, no_mtls):\n    _create_signed_read_url_helper(storage_client, signing_bucket)\n\n\ndef test_create_signed_read_url_v4(storage_client, signing_bucket, no_mtls):\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        version=\"v4\",\n    )\n\n\ndef test_create_signed_read_url_v2_w_expiration(\n    storage_client, signing_bucket, no_mtls\n):\n    now = _NOW(_UTC).replace(tzinfo=None)\n    delta = datetime.timedelta(seconds=10)\n\n    _create_signed_read_url_helper(\n        storage_client, signing_bucket, expiration=now + delta\n    )\n\n\ndef test_create_signed_read_url_v4_w_expiration(\n    storage_client, signing_bucket, no_mtls\n):\n    now = _NOW(_UTC).replace(tzinfo=None)\n    delta = datetime.timedelta(seconds=10)\n    _create_signed_read_url_helper(\n        storage_client, signing_bucket, expiration=now + delta, version=\"v4\"\n    )\n\n\ndef test_create_signed_read_url_v2_lowercase_method(\n    storage_client, signing_bucket, no_mtls\n):\n    _create_signed_read_url_helper(storage_client, signing_bucket, method=\"get\")\n\n\ndef test_create_signed_read_url_v4_lowercase_method(\n    storage_client, signing_bucket, no_mtls\n):\n    _create_signed_read_url_helper(\n        storage_client, signing_bucket, method=\"get\", version=\"v4\"\n    )\n\n\ndef test_create_signed_read_url_v2_w_non_ascii_name(\n    storage_client, signing_bucket, no_mtls\n):\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        blob_name=\"Caf\\xe9.txt\",\n        payload=b\"Test signed URL for blob w/ non-ASCII name\",\n    )\n\n\ndef test_create_signed_read_url_v4_w_non_ascii_name(\n    storage_client, signing_bucket, no_mtls\n):\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        blob_name=\"Caf\\xe9.txt\",\n        payload=b\"Test signed URL for blob w/ non-ASCII name\",\n        version=\"v4\",\n    )\n\n\ndef test_create_signed_read_url_v2_w_csek(storage_client, signing_bucket, no_mtls):\n    encryption_key = os.urandom(32)\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        blob_name=\"v2-w-csek.txt\",\n        payload=b\"Test signed URL for blob w/ CSEK\",\n        encryption_key=encryption_key,\n    )\n\n\ndef test_create_signed_read_url_v4_w_csek(storage_client, signing_bucket, no_mtls):\n    encryption_key = os.urandom(32)\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        blob_name=\"v2-w-csek.txt\",\n        payload=b\"Test signed URL for blob w/ CSEK\",\n        encryption_key=encryption_key,\n        version=\"v4\",\n    )\n\n\ndef test_create_signed_read_url_v2_w_access_token(\n    storage_client, signing_bucket, service_account, no_mtls\n):\n    client = iam_credentials_v1.IAMCredentialsClient()\n    service_account_email = service_account.service_account_email\n    name = path_template.expand(\n        \"projects/{project}/serviceAccounts/{service_account}\",\n        project=\"-\",\n        service_account=service_account_email,\n    )\n    scope = [\n        \"https://www.googleapis.com/auth/devstorage.read_write\",\n        \"https://www.googleapis.com/auth/iam\",\n    ]\n    response = client.generate_access_token(name=name, scope=scope)\n\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        service_account_email=service_account_email,\n        access_token=response.access_token,\n    )\n\n\ndef test_create_signed_read_url_v4_w_access_token(\n    storage_client, signing_bucket, service_account, no_mtls\n):\n    client = iam_credentials_v1.IAMCredentialsClient()\n    service_account_email = service_account.service_account_email\n    name = path_template.expand(\n        \"projects/{project}/serviceAccounts/{service_account}\",\n        project=\"-\",\n        service_account=service_account_email,\n    )\n    scope = [\n        \"https://www.googleapis.com/auth/devstorage.read_write\",\n        \"https://www.googleapis.com/auth/iam\",\n    ]\n    response = client.generate_access_token(name=name, scope=scope)\n\n    _create_signed_read_url_helper(\n        storage_client,\n        signing_bucket,\n        version=\"v4\",\n        service_account_email=service_account_email,\n        access_token=response.access_token,\n    )\n\n\ndef _create_signed_delete_url_helper(client, bucket, version=\"v2\", expiration=None):\n    expiration = _morph_expiration(version, expiration)\n\n    blob = bucket.blob(\"DELETE_ME.txt\")\n    blob.upload_from_string(b\"DELETE ME!\")\n\n    signed_delete_url = blob.generate_signed_url(\n        expiration=expiration,\n        method=\"DELETE\",\n        client=client,\n        version=version,\n    )\n\n    response = requests.request(\"DELETE\", signed_delete_url)\n\n    assert response.status_code == 204\n    assert response.content == b\"\"\n    assert not blob.exists()\n\n\ndef test_create_signed_delete_url_v2(storage_client, signing_bucket, no_mtls):\n    _create_signed_delete_url_helper(storage_client, signing_bucket)\n\n\ndef test_create_signed_delete_url_v4(storage_client, signing_bucket, no_mtls):\n    _create_signed_delete_url_helper(storage_client, signing_bucket, version=\"v4\")\n\n\ndef _create_signed_resumable_upload_url_helper(\n    client, bucket, version=\"v2\", expiration=None\n):\n    expiration = _morph_expiration(version, expiration)\n    blob = bucket.blob(\"cruddy.txt\")\n    payload = b\"DEADBEEF\"\n\n    # Initiate the upload using a signed URL.\n    signed_resumable_upload_url = blob.generate_signed_url(\n        expiration=expiration,\n        method=\"RESUMABLE\",\n        client=client,\n        version=version,\n    )\n\n    post_headers = {\"x-goog-resumable\": \"start\"}\n    post_response = requests.post(signed_resumable_upload_url, headers=post_headers)\n    assert post_response.status_code == 201\n\n    # Finish uploading the body.\n    location = post_response.headers[\"Location\"]\n    put_headers = {\"content-length\": str(len(payload))}\n    put_response = requests.put(location, headers=put_headers, data=payload)\n    assert put_response.status_code == 200\n\n    # Download using a signed URL and verify.\n    signed_download_url = blob.generate_signed_url(\n        expiration=expiration, method=\"GET\", client=client, version=version\n    )\n\n    get_response = requests.get(signed_download_url)\n    assert get_response.status_code == 200\n    assert get_response.content == payload\n\n    # Finally, delete the blob using a signed URL.\n    signed_delete_url = blob.generate_signed_url(\n        expiration=expiration,\n        method=\"DELETE\",\n        client=client,\n        version=version,\n    )\n\n    delete_response = requests.delete(signed_delete_url)\n    assert delete_response.status_code == 204\n\n\ndef test_create_signed_resumable_upload_url_v2(storage_client, signing_bucket, no_mtls):\n    _create_signed_resumable_upload_url_helper(\n        storage_client,\n        signing_bucket,\n        version=\"v2\",\n    )\n\n\ndef test_create_signed_resumable_upload_url_v4(storage_client, signing_bucket, no_mtls):\n    _create_signed_resumable_upload_url_helper(\n        storage_client,\n        signing_bucket,\n        version=\"v4\",\n    )\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\ndef test_generate_signed_post_policy_v4(\n    storage_client, buckets_to_delete, blobs_to_delete, service_account, no_mtls\n):\n    bucket_name = _helpers.unique_name(\"post_policy\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    blob_name = \"post_policy_obj.txt\"\n    payload = b\"DEADBEEF\"\n    with open(blob_name, \"wb\") as f:\n        f.write(payload)\n\n    now = _NOW(_UTC).replace(tzinfo=None)\n    policy = storage_client.generate_signed_post_policy_v4(\n        bucket_name,\n        blob_name,\n        conditions=[\n            {\"bucket\": bucket_name},\n            [\"starts-with\", \"$Content-Type\", \"text/pla\"],\n        ],\n        expiration=now + datetime.timedelta(hours=1),\n        fields={\"content-type\": \"text/plain\"},\n    )\n    with open(blob_name, \"r\") as f:\n        files = {\"file\": (blob_name, f)}\n        response = requests.post(policy[\"url\"], data=policy[\"fields\"], files=files)\n\n    os.remove(blob_name)\n    assert response.status_code == 204\n\n    blob = bucket.get_blob(blob_name)\n    assert blob.download_as_bytes() == payload\n\n\ndef test_generate_signed_post_policy_v4_invalid_field(\n    storage_client, buckets_to_delete, blobs_to_delete, service_account, no_mtls\n):\n    bucket_name = _helpers.unique_name(\"post_policy-invalid\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    blob_name = \"post_policy_obj.txt\"\n    payload = b\"DEADBEEF\"\n    with open(blob_name, \"wb\") as f:\n        f.write(payload)\n\n    now = _NOW(_UTC).replace(tzinfo=None)\n    policy = storage_client.generate_signed_post_policy_v4(\n        bucket_name,\n        blob_name,\n        conditions=[\n            {\"bucket\": bucket_name},\n            [\"starts-with\", \"$Content-Type\", \"text/pla\"],\n        ],\n        expiration=now + datetime.timedelta(hours=1),\n        fields={\"x-goog-random\": \"invalid_field\", \"content-type\": \"text/plain\"},\n    )\n    with open(blob_name, \"r\") as f:\n        files = {\"file\": (blob_name, f)}\n        response = requests.post(policy[\"url\"], data=policy[\"fields\"], files=files)\n\n    os.remove(blob_name)\n    assert response.status_code == 400\n\n    assert list(bucket.list_blobs()) == []\n", "tests/system/test_transfer_manager.py": "# coding=utf-8\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tempfile\nimport os\n\nimport pytest\n\nfrom google.cloud.storage import transfer_manager\nfrom google.cloud.storage._helpers import _base64_md5hash\n\nfrom google.api_core import exceptions\n\nDEADLINE = 30\n\nencryption_key = \"b23ff11bba187db8c37077e6af3b25b8\"\n\n\ndef _check_blob_hash(blob, info):\n    md5_hash = blob.md5_hash\n    if not isinstance(md5_hash, bytes):\n        md5_hash = md5_hash.encode(\"utf-8\")\n\n    assert md5_hash == info[\"hash\"]\n\n\ndef test_upload_many(shared_bucket, file_data, blobs_to_delete):\n    FILE_BLOB_PAIRS = [\n        (file_data[\"simple\"][\"path\"], shared_bucket.blob(\"simple1\")),\n        (file_data[\"simple\"][\"path\"], shared_bucket.blob(\"simple2\")),\n    ]\n\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS,\n        worker_type=transfer_manager.PROCESS,\n        deadline=DEADLINE,\n    )\n    assert results == [None, None]\n\n    blobs = shared_bucket.list_blobs()\n    for blob in blobs:\n        if blob.name.startswith(\"simple\"):\n            blobs_to_delete.append(blob)\n    assert len(blobs_to_delete) == 2\n\n\ndef test_upload_many_with_threads_and_file_objs(\n    shared_bucket, file_data, blobs_to_delete\n):\n    FILE_BLOB_PAIRS = [\n        (open(file_data[\"simple\"][\"path\"], \"rb\"), shared_bucket.blob(\"simple1\")),\n        (open(file_data[\"simple\"][\"path\"], \"rb\"), shared_bucket.blob(\"simple2\")),\n    ]\n\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS,\n        worker_type=transfer_manager.THREAD,\n        deadline=DEADLINE,\n    )\n    assert results == [None, None]\n\n    blobs = shared_bucket.list_blobs()\n    for blob in blobs:\n        if blob.name.startswith(\"simple\"):\n            blobs_to_delete.append(blob)\n    assert len(blobs_to_delete) == 2\n\n\ndef test_upload_many_skip_if_exists(\n    listable_bucket, listable_filenames, file_data, blobs_to_delete\n):\n    FILE_BLOB_PAIRS = [\n        (file_data[\"logo\"][\"path\"], listable_bucket.blob(listable_filenames[0])),\n        (file_data[\"simple\"][\"path\"], listable_bucket.blob(\"simple\")),\n    ]\n\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS,\n        skip_if_exists=True,\n        raise_exception=True,\n        deadline=DEADLINE,\n    )\n    assert isinstance(results[0], exceptions.PreconditionFailed)\n    assert results[1] is None\n\n    blobs = listable_bucket.list_blobs()\n    for blob in blobs:\n        if blob.name.startswith(\"simple\"):\n            blobs_to_delete.append(blob)\n    assert len(blobs_to_delete) == 1\n\n\ndef test_upload_many_from_filenames_with_attributes(\n    listable_bucket, listable_filenames, file_data, blobs_to_delete\n):\n    SOURCE_DIRECTORY, FILENAME = os.path.split(file_data[\"logo\"][\"path\"])\n\n    transfer_manager.upload_many_from_filenames(\n        listable_bucket,\n        [FILENAME],\n        source_directory=SOURCE_DIRECTORY,\n        additional_blob_attributes={\"cache_control\": \"no-cache\"},\n        raise_exception=True,\n    )\n\n    blob = listable_bucket.blob(FILENAME)\n    blob.reload()\n    blobs_to_delete.append(blob)\n    assert blob.cache_control == \"no-cache\"\n\n\ndef test_download_many(listable_bucket):\n    blobs = list(listable_bucket.list_blobs())\n    with tempfile.TemporaryDirectory() as tempdir:\n        filenames = [\n            os.path.join(tempdir, \"file_a.txt\"),\n            os.path.join(tempdir, \"file_b.txt\"),\n        ]\n        BLOB_FILE_PAIRS = zip(blobs[:2], filenames)\n\n        results = transfer_manager.download_many(\n            BLOB_FILE_PAIRS,\n            worker_type=transfer_manager.PROCESS,\n            deadline=DEADLINE,\n        )\n        assert results == [None, None]\n        for count, filename in enumerate(filenames):\n            with open(filename, \"rb\") as fp:\n                assert len(fp.read()) == blobs[count].size\n\n\ndef test_download_many_with_threads_and_file_objs(listable_bucket):\n    blobs = list(listable_bucket.list_blobs())\n    with tempfile.TemporaryFile() as file_a, tempfile.TemporaryFile() as file_b:\n        tempfiles = [file_a, file_b]\n        BLOB_FILE_PAIRS = zip(blobs[:2], tempfiles)\n\n        results = transfer_manager.download_many(\n            BLOB_FILE_PAIRS,\n            worker_type=transfer_manager.THREAD,\n            deadline=DEADLINE,\n        )\n        assert results == [None, None]\n        for fp in tempfiles:\n            assert fp.tell() != 0\n\n\ndef test_download_chunks_concurrently(shared_bucket, file_data):\n    # Upload a big file\n    source_file = file_data[\"big\"]\n    upload_blob = shared_bucket.blob(\"chunky_file\")\n    upload_blob.upload_from_filename(source_file[\"path\"])\n    upload_blob.reload()\n    size = upload_blob.size\n    chunk_size = size // 32\n\n    # Get a fresh blob obj w/o metadata for testing purposes\n    download_blob = shared_bucket.blob(\"chunky_file\")\n\n    with tempfile.TemporaryDirectory() as tempdir:\n        full_filename = os.path.join(tempdir, \"chunky_file_1\")\n        transfer_manager.download_chunks_concurrently(\n            download_blob,\n            full_filename,\n            chunk_size=chunk_size,\n            deadline=DEADLINE,\n        )\n        with open(full_filename, \"rb\") as file_obj:\n            assert _base64_md5hash(file_obj) == source_file[\"hash\"]\n\n        # Now test for case where last chunk is exactly 1 byte.\n        trailing_chunk_filename = os.path.join(tempdir, \"chunky_file_2\")\n        transfer_manager.download_chunks_concurrently(\n            download_blob,\n            trailing_chunk_filename,\n            chunk_size=size - 1,\n            deadline=DEADLINE,\n        )\n        with open(trailing_chunk_filename, \"rb\") as file_obj:\n            assert _base64_md5hash(file_obj) == source_file[\"hash\"]\n\n        # And for a case where there is only one chunk.\n        trailing_chunk_filename = os.path.join(tempdir, \"chunky_file_3\")\n        transfer_manager.download_chunks_concurrently(\n            download_blob,\n            trailing_chunk_filename,\n            chunk_size=size,\n            deadline=DEADLINE,\n        )\n        with open(trailing_chunk_filename, \"rb\") as file_obj:\n            assert _base64_md5hash(file_obj) == source_file[\"hash\"]\n\n        # Also test threaded mode.\n        threaded_filename = os.path.join(tempdir, \"chunky_file_4\")\n        transfer_manager.download_chunks_concurrently(\n            download_blob,\n            threaded_filename,\n            chunk_size=chunk_size,\n            deadline=DEADLINE,\n            worker_type=transfer_manager.THREAD,\n        )\n        with open(threaded_filename, \"rb\") as file_obj:\n            assert _base64_md5hash(file_obj) == source_file[\"hash\"]\n\n\ndef test_upload_chunks_concurrently(shared_bucket, file_data, blobs_to_delete):\n    source_file = file_data[\"big\"]\n    filename = source_file[\"path\"]\n    blob_name = \"mpu_file\"\n    upload_blob = shared_bucket.blob(blob_name)\n    chunk_size = 5 * 1024 * 1024  # Minimum supported by XML MPU API\n    assert os.path.getsize(filename) > chunk_size  # Won't make a good test otherwise\n\n    blobs_to_delete.append(upload_blob)\n\n    transfer_manager.upload_chunks_concurrently(\n        filename, upload_blob, chunk_size=chunk_size, deadline=DEADLINE\n    )\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        download_blob = shared_bucket.blob(blob_name)\n        download_blob.download_to_file(tmp)\n        tmp.seek(0)\n\n        with open(source_file[\"path\"], \"rb\") as sf:\n            source_contents = sf.read()\n            temp_contents = tmp.read()\n            assert source_contents == temp_contents\n\n    # Also test threaded mode\n    blob_name = \"mpu_threaded\"\n    upload_blob = shared_bucket.blob(blob_name)\n    chunk_size = 5 * 1024 * 1024  # Minimum supported by XML MPU API\n    assert os.path.getsize(filename) > chunk_size  # Won't make a good test otherwise\n\n    transfer_manager.upload_chunks_concurrently(\n        filename,\n        upload_blob,\n        chunk_size=chunk_size,\n        deadline=DEADLINE,\n        worker_type=transfer_manager.THREAD,\n    )\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        download_blob = shared_bucket.blob(blob_name)\n        download_blob.download_to_file(tmp)\n        tmp.seek(0)\n\n        with open(source_file[\"path\"], \"rb\") as sf:\n            source_contents = sf.read()\n            temp_contents = tmp.read()\n            assert source_contents == temp_contents\n\n\ndef test_upload_chunks_concurrently_with_metadata(\n    shared_bucket, file_data, blobs_to_delete\n):\n    from google.cloud.storage._helpers import _NOW\n    from google.cloud.storage._helpers import _UTC\n\n    now = _NOW(_UTC)\n    custom_metadata = {\"key_a\": \"value_a\", \"key_b\": \"value_b\"}\n\n    METADATA = {\n        \"cache_control\": \"private\",\n        \"content_disposition\": \"inline\",\n        \"content_language\": \"en-US\",\n        \"custom_time\": now,\n        \"metadata\": custom_metadata,\n        \"storage_class\": \"NEARLINE\",\n    }\n\n    source_file = file_data[\"big\"]\n    filename = source_file[\"path\"]\n    blob_name = \"mpu_file_with_metadata\"\n    upload_blob = shared_bucket.blob(blob_name)\n\n    for key, value in METADATA.items():\n        setattr(upload_blob, key, value)\n\n    chunk_size = 5 * 1024 * 1024  # Minimum supported by XML MPU API\n    assert os.path.getsize(filename) > chunk_size  # Won't make a good test otherwise\n\n    transfer_manager.upload_chunks_concurrently(\n        filename, upload_blob, chunk_size=chunk_size, deadline=DEADLINE\n    )\n    blobs_to_delete.append(upload_blob)\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        download_blob = shared_bucket.get_blob(blob_name)\n\n        for key, value in METADATA.items():\n            assert getattr(download_blob, key) == value\n\n        download_blob.download_to_file(tmp)\n        tmp.seek(0)\n\n        with open(source_file[\"path\"], \"rb\") as sf:\n            source_contents = sf.read()\n            temp_contents = tmp.read()\n            assert source_contents == temp_contents\n\n\ndef test_upload_chunks_concurrently_with_content_encoding(\n    shared_bucket, file_data, blobs_to_delete\n):\n    import gzip\n\n    METADATA = {\n        \"content_encoding\": \"gzip\",\n    }\n\n    source_file = file_data[\"big\"]\n    filename = source_file[\"path\"]\n    blob_name = \"mpu_file_encoded\"\n    upload_blob = shared_bucket.blob(blob_name)\n\n    for key, value in METADATA.items():\n        setattr(upload_blob, key, value)\n\n    chunk_size = 5 * 1024 * 1024  # Minimum supported by XML MPU API\n\n    with tempfile.NamedTemporaryFile() as tmp_gzip:\n        with open(filename, \"rb\") as f:\n            compressed_bytes = gzip.compress(f.read())\n\n        tmp_gzip.write(compressed_bytes)\n        tmp_gzip.seek(0)\n        transfer_manager.upload_chunks_concurrently(\n            tmp_gzip.name, upload_blob, chunk_size=chunk_size, deadline=DEADLINE\n        )\n        blobs_to_delete.append(upload_blob)\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        download_blob = shared_bucket.get_blob(blob_name)\n\n        for key, value in METADATA.items():\n            assert getattr(download_blob, key) == value\n\n        download_blob.download_to_file(tmp)\n        tmp.seek(0)\n\n        with open(source_file[\"path\"], \"rb\") as sf:\n            source_contents = sf.read()\n            temp_contents = tmp.read()\n            assert source_contents == temp_contents\n\n\ndef test_upload_chunks_concurrently_with_encryption_key(\n    shared_bucket, file_data, blobs_to_delete\n):\n    source_file = file_data[\"big\"]\n    filename = source_file[\"path\"]\n    blob_name = \"mpu_file_encrypted\"\n    upload_blob = shared_bucket.blob(blob_name, encryption_key=encryption_key)\n\n    chunk_size = 5 * 1024 * 1024  # Minimum supported by XML MPU API\n    assert os.path.getsize(filename) > chunk_size  # Won't make a good test otherwise\n\n    transfer_manager.upload_chunks_concurrently(\n        filename, upload_blob, chunk_size=chunk_size, deadline=DEADLINE\n    )\n    blobs_to_delete.append(upload_blob)\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        download_blob = shared_bucket.get_blob(blob_name, encryption_key=encryption_key)\n\n        download_blob.download_to_file(tmp)\n        tmp.seek(0)\n\n        with open(source_file[\"path\"], \"rb\") as sf:\n            source_contents = sf.read()\n            temp_contents = tmp.read()\n            assert source_contents == temp_contents\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        keyless_blob = shared_bucket.get_blob(blob_name)\n\n        with pytest.raises(exceptions.BadRequest):\n            keyless_blob.download_to_file(tmp)\n\n\ndef test_upload_chunks_concurrently_with_kms(\n    kms_bucket, file_data, blobs_to_delete, kms_key_name\n):\n    source_file = file_data[\"big\"]\n    filename = source_file[\"path\"]\n    blob_name = \"mpu_file_kms\"\n    blob = kms_bucket.blob(blob_name, kms_key_name=kms_key_name)\n\n    chunk_size = 5 * 1024 * 1024  # Minimum supported by XML MPU API\n    assert os.path.getsize(filename) > chunk_size  # Won't make a good test otherwise\n\n    transfer_manager.upload_chunks_concurrently(\n        filename, blob, chunk_size=chunk_size, deadline=DEADLINE\n    )\n    blobs_to_delete.append(blob)\n    blob.reload()\n    assert blob.kms_key_name.startswith(kms_key_name)\n\n    with tempfile.NamedTemporaryFile() as tmp:\n        blob.download_to_file(tmp)\n        tmp.seek(0)\n\n        with open(source_file[\"path\"], \"rb\") as sf:\n            source_contents = sf.read()\n            temp_contents = tmp.read()\n            assert source_contents == temp_contents\n", "tests/system/test_client.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport re\nimport os\nimport tempfile\n\nimport pytest\n\nfrom google.cloud import exceptions\nfrom test_utils.vpcsc_config import vpcsc_config\nfrom . import _helpers\n\n\ndual_data_loc_1 = os.getenv(\"DUAL_REGION_LOC_1\", \"US-EAST1\")\ndual_data_loc_2 = os.getenv(\"DUAL_REGION_LOC_2\", \"US-WEST1\")\npublic_bucket = \"gcp-public-data-landsat\"\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\n@vpcsc_config.skip_if_inside_vpcsc\ndef test_anonymous_client_access_to_public_bucket():\n    from google.cloud.storage.client import Client\n\n    anonymous_client = Client.create_anonymous_client()\n    bucket = anonymous_client.bucket(public_bucket)\n    (blob,) = _helpers.retry_429_503(anonymous_client.list_blobs)(\n        bucket,\n        max_results=1,\n    )\n    with tempfile.TemporaryFile() as stream:\n        _helpers.retry_429_503(blob.download_to_file)(stream)\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\ndef test_get_service_account_email(storage_client, service_account):\n    domain = \"gs-project-accounts.iam.gserviceaccount.com\"\n    email = storage_client.get_service_account_email()\n\n    new_style = re.compile(r\"service-(?P<projnum>[^@]+)@{}\".format(domain))\n    old_style = re.compile(r\"{}@{}\".format(storage_client.project, domain))\n    patterns = [new_style, old_style]\n    matches = [pattern.match(email) for pattern in patterns]\n\n    assert any(match for match in matches if match is not None)\n\n\ndef test_create_bucket_simple(storage_client, buckets_to_delete):\n    new_bucket_name = _helpers.unique_name(\"a-new-bucket\")\n\n    with pytest.raises(exceptions.NotFound):\n        storage_client.get_bucket(new_bucket_name)\n\n    created = _helpers.retry_429_503(storage_client.create_bucket)(new_bucket_name)\n    buckets_to_delete.append(created)\n\n    assert created.name == new_bucket_name\n\n\ndef test_create_bucket_dual_region(storage_client, buckets_to_delete):\n    from google.cloud.storage.constants import DUAL_REGION_LOCATION_TYPE\n\n    new_bucket_name = _helpers.unique_name(\"dual-region-bucket\")\n    location = \"US\"\n\n    data_locations = [dual_data_loc_1, dual_data_loc_2]\n\n    with pytest.raises(exceptions.NotFound):\n        storage_client.get_bucket(new_bucket_name)\n\n    created = _helpers.retry_429_503(storage_client.create_bucket)(\n        new_bucket_name, location=location, data_locations=data_locations\n    )\n    buckets_to_delete.append(created)\n\n    assert created.name == new_bucket_name\n    assert created.location == location\n    assert created.location_type == DUAL_REGION_LOCATION_TYPE\n    assert created.data_locations == data_locations\n\n\ndef test_list_buckets(storage_client, buckets_to_delete):\n    buckets_to_create = [\n        _helpers.unique_name(\"new\"),\n        _helpers.unique_name(\"newer\"),\n        _helpers.unique_name(\"newest\"),\n    ]\n    created_buckets = []\n\n    for bucket_name in buckets_to_create:\n        bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n        buckets_to_delete.append(bucket)\n\n    all_buckets = storage_client.list_buckets()\n\n    created_buckets = [\n        bucket.name for bucket in all_buckets if bucket.name in buckets_to_create\n    ]\n\n    assert sorted(created_buckets) == sorted(buckets_to_create)\n\n\ndef test_download_blob_to_file_w_uri(\n    storage_client,\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    blob = shared_bucket.blob(\"MyBuffer\")\n    payload = b\"Hello World\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        with open(temp_f.name, \"wb\") as file_obj:\n            storage_client.download_blob_to_file(\n                \"gs://\" + shared_bucket.name + \"/MyBuffer\", file_obj\n            )\n\n        with open(temp_f.name, \"rb\") as file_obj:\n            stored_contents = file_obj.read()\n\n    assert stored_contents == payload\n\n\ndef test_download_blob_to_file_w_etag(\n    storage_client,\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    filename = \"kittens\"\n    blob = shared_bucket.blob(filename)\n    payload = b\"fluffy\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    buffer = io.BytesIO()\n    with pytest.raises(exceptions.NotModified):\n        storage_client.download_blob_to_file(\n            \"gs://\" + shared_bucket.name + \"/\" + filename,\n            buffer,\n            if_etag_not_match=blob.etag,\n        )\n\n    buffer = io.BytesIO()\n    with pytest.raises(exceptions.PreconditionFailed):\n        storage_client.download_blob_to_file(\n            \"gs://\" + shared_bucket.name + \"/\" + filename,\n            buffer,\n            if_etag_match=\"kittens\",\n        )\n\n    buffer = io.BytesIO()\n    storage_client.download_blob_to_file(\n        \"gs://\" + shared_bucket.name + \"/\" + filename,\n        buffer,\n        if_etag_not_match=\"kittens\",\n    )\n    assert buffer.getvalue() == payload\n\n    buffer = io.BytesIO()\n    storage_client.download_blob_to_file(\n        \"gs://\" + shared_bucket.name + \"/\" + filename,\n        buffer,\n        if_etag_match=blob.etag,\n    )\n    assert buffer.getvalue() == payload\n", "tests/system/conftest.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport os\n\nimport pytest\n\nfrom google.api_core import exceptions\nfrom google.cloud import kms\nfrom google.cloud.storage._helpers import _base64_md5hash\nfrom . import _helpers\n\n\ndirname = os.path.realpath(os.path.dirname(__file__))\ndata_dirname = os.path.abspath(os.path.join(dirname, \"..\", \"data\"))\n_filenames = [\n    (\"logo\", \"CloudPlatform_128px_Retina.png\"),\n    (\"big\", \"five-point-one-mb-file.zip\"),\n    (\"simple\", \"simple.txt\"),\n]\n_file_data = {\n    key: {\"path\": os.path.join(data_dirname, file_name)}\n    for key, file_name in _filenames\n}\n\n_listable_filenames = [\"CloudLogo1\", \"CloudLogo2\", \"CloudLogo3\", \"CloudLogo4\"]\n_hierarchy_filenames = [\n    \"file01.txt\",\n    \"parent/\",\n    \"parent/file11.txt\",\n    \"parent/child/file21.txt\",\n    \"parent/child/file22.txt\",\n    \"parent/child/grand/file31.txt\",\n    \"parent/child/other/file32.txt\",\n]\n\nebh_bucket_iteration = 0\n\n_key_name_format = \"projects/{}/locations/{}/keyRings/{}/cryptoKeys/{}\"\n\nkeyring_name = \"gcs-test\"\ndefault_key_name = \"gcs-test\"\nalt_key_name = \"gcs-test-alternate\"\n\n\ndef _kms_key_name(client, bucket, key_name):\n    return _key_name_format.format(\n        client.project,\n        bucket.location.lower(),\n        keyring_name,\n        key_name,\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef storage_client():\n    from google.cloud.storage import Client\n\n    client = Client()\n    with contextlib.closing(client):\n        yield client\n\n\n@pytest.fixture(scope=\"session\")\ndef user_project():\n    if _helpers.user_project is None:\n        pytest.skip(\"USER_PROJECT not set in environment.\")\n    return _helpers.user_project\n\n\n@pytest.fixture(scope=\"session\")\ndef no_mtls():\n    if _helpers.testing_mtls:\n        pytest.skip(\"Test incompatible with mTLS.\")\n\n\n@pytest.fixture(scope=\"session\")\ndef service_account(storage_client):\n    from google.oauth2.service_account import Credentials\n\n    if not isinstance(storage_client._credentials, Credentials):\n        pytest.skip(\"These tests require a service account credential\")\n    return storage_client._credentials\n\n\n@pytest.fixture(scope=\"session\")\ndef shared_bucket_name():\n    return _helpers.unique_name(\"gcp-systest\")\n\n\n@pytest.fixture(scope=\"session\")\ndef shared_bucket(storage_client, shared_bucket_name):\n    bucket = storage_client.bucket(shared_bucket_name)\n    bucket.versioning_enabled = True\n    _helpers.retry_429_503(bucket.create)()\n\n    yield bucket\n\n    _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"session\")\ndef listable_bucket_name():\n    return _helpers.unique_name(\"gcp-systest-listable\")\n\n\n@pytest.fixture(scope=\"session\")\ndef listable_bucket(storage_client, listable_bucket_name, file_data):\n    bucket = storage_client.bucket(listable_bucket_name)\n    _helpers.retry_429_503(bucket.create)()\n\n    info = file_data[\"logo\"]\n    source_blob = bucket.blob(_listable_filenames[0])\n    source_blob.upload_from_filename(info[\"path\"])\n\n    for filename in _listable_filenames[1:]:\n        _helpers.retry_bad_copy(bucket.copy_blob)(\n            source_blob,\n            bucket,\n            filename,\n        )\n\n    yield bucket\n\n    _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"session\")\ndef listable_filenames():\n    return _listable_filenames\n\n\n@pytest.fixture(scope=\"session\")\ndef hierarchy_bucket_name():\n    return _helpers.unique_name(\"gcp-systest-hierarchy\")\n\n\n@pytest.fixture(scope=\"session\")\ndef hierarchy_bucket(storage_client, hierarchy_bucket_name, file_data):\n    bucket = storage_client.bucket(hierarchy_bucket_name)\n    _helpers.retry_429_503(bucket.create)()\n\n    simple_path = _file_data[\"simple\"][\"path\"]\n    for filename in _hierarchy_filenames:\n        blob = bucket.blob(filename)\n        blob.upload_from_filename(simple_path)\n\n    yield bucket\n\n    _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"session\")\ndef hierarchy_filenames():\n    return _hierarchy_filenames\n\n\n@pytest.fixture(scope=\"session\")\ndef signing_bucket_name():\n    return _helpers.unique_name(\"gcp-systest-signing\")\n\n\n@pytest.fixture(scope=\"session\")\ndef signing_bucket(storage_client, signing_bucket_name):\n    bucket = storage_client.bucket(signing_bucket_name)\n    _helpers.retry_429_503(bucket.create)()\n    blob = bucket.blob(\"README.txt\")\n    blob.upload_from_string(_helpers.signing_blob_content)\n\n    yield bucket\n\n    _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"function\")\ndef default_ebh_bucket_name():\n    # Keep track of how many ebh buckets have been created so we can get a\n    # clean one each rerun. \"unique_name\" is unique per test iteration, not\n    # per test rerun.\n    global ebh_bucket_iteration\n    ebh_bucket_iteration += 1\n    return _helpers.unique_name(\"gcp-systest-default-ebh\") + \"-{}\".format(\n        ebh_bucket_iteration\n    )\n\n\n# ebh_bucket/name are not scope=session because the bucket is modified in test.\n@pytest.fixture(scope=\"function\")\ndef default_ebh_bucket(storage_client, default_ebh_bucket_name):\n    bucket = storage_client.bucket(default_ebh_bucket_name)\n    bucket.default_event_based_hold = True\n    _helpers.retry_429_503(bucket.create)()\n\n    yield bucket\n\n    _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"function\")\ndef buckets_to_delete():\n    buckets_to_delete = []\n\n    yield buckets_to_delete\n\n    for bucket in buckets_to_delete:\n        _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"function\")\ndef blobs_to_delete():\n    blobs_to_delete = []\n\n    yield blobs_to_delete\n\n    for blob in blobs_to_delete:\n        _helpers.delete_blob(blob)\n\n\n@pytest.fixture(scope=\"session\")\ndef file_data():\n    for file_data in _file_data.values():\n        with open(file_data[\"path\"], \"rb\") as file_obj:\n            file_data[\"hash\"] = _base64_md5hash(file_obj)\n\n    return _file_data\n\n\n@pytest.fixture(scope=\"function\")\ndef kms_bucket_name():\n    return _helpers.unique_name(\"gcp-systest-kms\")\n\n\n@pytest.fixture(scope=\"function\")\ndef kms_bucket(storage_client, kms_bucket_name, no_mtls):\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(kms_bucket_name)\n\n    yield bucket\n\n    _helpers.delete_bucket(bucket)\n\n\n@pytest.fixture(scope=\"function\")\ndef kms_key_name(storage_client, kms_bucket):\n    return _kms_key_name(storage_client, kms_bucket, default_key_name)\n\n\n@pytest.fixture(scope=\"function\")\ndef alt_kms_key_name(storage_client, kms_bucket):\n    return _kms_key_name(storage_client, kms_bucket, alt_key_name)\n\n\n@pytest.fixture(scope=\"session\")\ndef kms_client():\n    return kms.KeyManagementServiceClient()\n\n\n@pytest.fixture(scope=\"function\")\ndef keyring(storage_client, kms_bucket, kms_client):\n    project = storage_client.project\n    location = kms_bucket.location.lower()\n    purpose = kms.enums.CryptoKey.CryptoKeyPurpose.ENCRYPT_DECRYPT\n\n    # If the keyring doesn't exist create it.\n    keyring_path = kms_client.key_ring_path(project, location, keyring_name)\n\n    try:\n        kms_client.get_key_ring(keyring_path)\n    except exceptions.NotFound:\n        parent = kms_client.location_path(project, location)\n        kms_client.create_key_ring(parent, keyring_name, {})\n\n        # Mark this service account as an owner of the new keyring\n        service_account_email = storage_client.get_service_account_email()\n        policy = {\n            \"bindings\": [\n                {\n                    \"role\": \"roles/cloudkms.cryptoKeyEncrypterDecrypter\",\n                    \"members\": [\"serviceAccount:\" + service_account_email],\n                }\n            ]\n        }\n        kms_client.set_iam_policy(keyring_path, policy)\n\n    # Populate the keyring with the keys we use in the tests\n    key_names = [\n        \"gcs-test\",\n        \"gcs-test-alternate\",\n        \"explicit-kms-key-name\",\n        \"default-kms-key-name\",\n        \"override-default-kms-key-name\",\n        \"alt-default-kms-key-name\",\n    ]\n    for key_name in key_names:\n        key_path = kms_client.crypto_key_path(project, location, keyring_name, key_name)\n        try:\n            kms_client.get_crypto_key(key_path)\n        except exceptions.NotFound:\n            key = {\"purpose\": purpose}\n            kms_client.create_crypto_key(keyring_path, key_name, key)\n", "tests/system/test_fileio.py": "# coding=utf-8\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom .test_blob import _check_blob_hash\n\n\ndef test_blobwriter_and_blobreader(\n    shared_bucket,\n    blobs_to_delete,\n    file_data,\n    service_account,\n):\n    blob = shared_bucket.blob(\"LargeFile\")\n\n    # Test BlobWriter works.\n    info = file_data[\"big\"]\n    with open(info[\"path\"], \"rb\") as file_obj:\n        with blob.open(\"wb\", chunk_size=256 * 1024) as writer:\n            writer.write(file_obj.read(100))\n            writer.write(file_obj.read(256 * 1024))\n            writer.write(file_obj.read())\n        blobs_to_delete.append(blob)\n\n    blob.reload()\n    _check_blob_hash(blob, info)\n\n    # Test BlobReader read and seek behave identically to filesystem file.\n    with open(info[\"path\"], \"rb\") as file_obj:\n        with blob.open(\"rb\", chunk_size=256 * 1024) as reader:\n            assert file_obj.read(100) == reader.read(100)\n            assert file_obj.read(256 * 1024) == reader.read(256 * 1024)\n            reader.seek(20)\n            file_obj.seek(20)\n            assert file_obj.read(256 * 1024 * 2) == reader.read(256 * 1024 * 2)\n            assert file_obj.read() == reader.read()\n            # End of file reached; further reads should be blank but not\n            # raise an error.\n            assert reader.read() == b\"\"\n\n\ndef test_blobwriter_and_blobreader_text_mode(\n    shared_bucket,\n    blobs_to_delete,\n    service_account,\n):\n    blob = shared_bucket.blob(\"MultibyteTextFile\")\n\n    # Construct a multibyte text_data sample file.\n    base_multibyte_text_string = \"abcde \u3042\u3044\u3046\u3048\u304a line: \"\n    text_data = \"\\n\".join([base_multibyte_text_string + str(x) for x in range(100)])\n\n    # Test text BlobWriter works.\n    with blob.open(\"wt\") as writer:\n        writer.write(text_data[:100])\n        writer.write(text_data[100:])\n    blobs_to_delete.append(blob)\n\n    # Test text BlobReader read and seek to 0. Seeking to an non-0 byte on a\n    # multibyte text stream is not safe in Python but the API expects\n    # seek() to work regadless.\n    with blob.open(\"rt\") as reader:\n        # This should produce 100 characters, not 100 bytes.\n        assert text_data[:100] == reader.read(100)\n        assert 0 == reader.seek(0)\n        assert reader.read() == text_data\n", "tests/system/test_notification.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nfrom . import _helpers\n\ncustom_attributes = {\"attr1\": \"value1\", \"attr2\": \"value2\"}\nblob_name_prefix = \"blob-name-prefix/\"\n\n\n@pytest.fixture(scope=\"session\")\ndef event_types():\n    from google.cloud.storage.notification import (\n        OBJECT_FINALIZE_EVENT_TYPE,\n        OBJECT_DELETE_EVENT_TYPE,\n    )\n\n    return [OBJECT_FINALIZE_EVENT_TYPE, OBJECT_DELETE_EVENT_TYPE]\n\n\n@pytest.fixture(scope=\"session\")\ndef payload_format():\n    from google.cloud.storage.notification import JSON_API_V1_PAYLOAD_FORMAT\n\n    return JSON_API_V1_PAYLOAD_FORMAT\n\n\n@pytest.fixture(scope=\"session\")\ndef publisher_client():\n    try:\n        from google.cloud.pubsub_v1 import PublisherClient\n    except ImportError:\n        pytest.skip(\"Cannot import pubsub\")\n\n    return PublisherClient()\n\n\n@pytest.fixture(scope=\"session\")\ndef topic_name():\n    return _helpers.unique_name(\"notification\")\n\n\n@pytest.fixture(scope=\"session\")\ndef topic_path(storage_client, topic_name):\n    return f\"projects/{storage_client.project}/topics/{topic_name}\"\n\n\n@pytest.fixture(scope=\"session\")\ndef notification_topic(storage_client, publisher_client, topic_path, no_mtls):\n    _helpers.retry_429(publisher_client.create_topic)(topic_path)\n    policy = publisher_client.get_iam_policy(topic_path)\n    binding = policy.bindings.add()\n    binding.role = \"roles/pubsub.publisher\"\n    binding.members.append(\n        f\"serviceAccount:{storage_client.get_service_account_email()}\"\n    )\n    publisher_client.set_iam_policy(topic_path, policy)\n\n\ndef test_notification_create_minimal(\n    storage_client,\n    buckets_to_delete,\n    topic_name,\n    notification_topic,\n):\n    bucket_name = _helpers.unique_name(\"notification-minimal\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    assert list(bucket.list_notifications()) == []\n\n    notification = bucket.notification(topic_name)\n    _helpers.retry_429_503(notification.create)()\n\n    try:\n        assert notification.exists()\n        assert notification.notification_id is not None\n        notifications = list(bucket.list_notifications())\n        assert len(notifications) == 1\n        assert notifications[0].topic_name == topic_name\n    finally:\n        notification.delete()\n\n\ndef test_notification_create_explicit(\n    storage_client,\n    buckets_to_delete,\n    topic_name,\n    notification_topic,\n    event_types,\n    payload_format,\n):\n    bucket_name = _helpers.unique_name(\"notification-explicit\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    assert list(bucket.list_notifications()) == []\n\n    notification = bucket.notification(\n        topic_name=topic_name,\n        custom_attributes=custom_attributes,\n        event_types=event_types,\n        blob_name_prefix=blob_name_prefix,\n        payload_format=payload_format,\n    )\n    _helpers.retry_429_503(notification.create)()\n\n    try:\n        assert notification.exists()\n        assert notification.notification_id is not None\n        assert notification.custom_attributes == custom_attributes\n        assert notification.event_types == event_types\n        assert notification.blob_name_prefix == blob_name_prefix\n        assert notification.payload_format == payload_format\n    finally:\n        notification.delete()\n\n\ndef test_notification_create_w_user_project(\n    storage_client,\n    buckets_to_delete,\n    topic_name,\n    notification_topic,\n    user_project,\n):\n    bucket_name = _helpers.unique_name(\"notification-w-up\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    with_user_project = storage_client.bucket(bucket_name, user_project=user_project)\n\n    assert list(with_user_project.list_notifications()) == []\n\n    notification = with_user_project.notification(topic_name)\n    _helpers.retry_429_503(notification.create)()\n\n    try:\n        assert notification.exists()\n        assert notification.notification_id is not None\n        notifications = list(bucket.list_notifications())\n        assert len(notifications) == 1\n        assert notifications[0].topic_name == topic_name\n    finally:\n        notification.delete()\n\n\ndef test_notification_create_wo_topic_name(\n    storage_client,\n    buckets_to_delete,\n    topic_name,\n    notification_topic,\n    event_types,\n    payload_format,\n):\n    from google.cloud.exceptions import BadRequest\n\n    bucket_name = _helpers.unique_name(\"notification-wo-name\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    assert list(bucket.list_notifications()) == []\n\n    notification = bucket.notification(\n        topic_name=None,\n        custom_attributes=custom_attributes,\n        event_types=event_types,\n        blob_name_prefix=blob_name_prefix,\n        payload_format=payload_format,\n    )\n\n    with pytest.raises(BadRequest):\n        notification.create()\n\n\ndef test_bucket_get_notification(\n    storage_client,\n    buckets_to_delete,\n    topic_name,\n    notification_topic,\n    event_types,\n    payload_format,\n):\n    bucket_name = _helpers.unique_name(\"notification-get\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    notification = bucket.notification(\n        topic_name=topic_name,\n        custom_attributes=custom_attributes,\n        payload_format=payload_format,\n    )\n    _helpers.retry_429_503(notification.create)()\n    try:\n        assert notification.exists()\n        assert notification.notification_id is not None\n\n        fetched = bucket.get_notification(notification.notification_id)\n\n        assert fetched.notification_id == notification.notification_id\n        assert fetched.custom_attributes == custom_attributes\n        assert fetched.payload_format == payload_format\n    finally:\n        notification.delete()\n", "tests/system/__init__.py": "", "tests/system/test_kms_integration.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nfrom . import _helpers\n\nkeyring_name = \"gcs-test\"\ndefault_key_name = \"gcs-test\"\nalt_key_name = \"gcs-test-alternate\"\n\n\ndef test_blob_w_explicit_kms_key_name(\n    kms_bucket, blobs_to_delete, kms_key_name, file_data\n):\n    blob_name = \"explicit-kms-key-name\"\n    info = file_data[\"simple\"]\n    blob = kms_bucket.blob(blob_name, kms_key_name=kms_key_name)\n    blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(blob)\n\n    with open(info[\"path\"], \"rb\") as file_obj:\n        assert blob.download_as_bytes() == file_obj.read()\n\n    # We don't know the current version of the key.\n    assert blob.kms_key_name.startswith(kms_key_name)\n\n    (listed,) = list(kms_bucket.list_blobs())\n    assert listed.kms_key_name.startswith(kms_key_name)\n\n\n@_helpers.retry_failures\ndef test_bucket_w_default_kms_key_name(\n    kms_bucket,\n    blobs_to_delete,\n    kms_key_name,\n    alt_kms_key_name,\n    file_data,\n):\n    blob_name = \"default-kms-key-name\"\n    info = file_data[\"simple\"]\n\n    with open(info[\"path\"], \"rb\") as file_obj:\n        payload = file_obj.read()\n\n    kms_bucket.default_kms_key_name = kms_key_name\n    kms_bucket.patch()\n    assert kms_bucket.default_kms_key_name == kms_key_name\n\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    _helpers.await_config_changes_propagate()\n\n    defaulted_blob = kms_bucket.blob(blob_name)\n    defaulted_blob.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(defaulted_blob)\n\n    assert defaulted_blob.download_as_bytes() == payload\n    _helpers.retry_429_harder(_helpers.retry_has_kms_key_name(defaulted_blob.reload))()\n    # We don't know the current version of the key.\n    assert defaulted_blob.kms_key_name.startswith(kms_key_name)\n\n    # Test changing the default KMS key.\n    kms_bucket.default_kms_key_name = alt_kms_key_name\n    kms_bucket.patch()\n    assert kms_bucket.default_kms_key_name == alt_kms_key_name\n\n    # Test removing the default KMS key.\n    kms_bucket.default_kms_key_name = None\n    kms_bucket.patch()\n    assert kms_bucket.default_kms_key_name is None\n\n\ndef test_blob_rewrite_rotate_csek_to_cmek(\n    kms_bucket,\n    blobs_to_delete,\n    kms_key_name,\n    file_data,\n):\n    blob_name = \"rotating-keys\"\n    source_key = os.urandom(32)\n    info = file_data[\"simple\"]\n\n    source = kms_bucket.blob(blob_name, encryption_key=source_key)\n    source.upload_from_filename(info[\"path\"])\n    blobs_to_delete.append(source)\n    source_data = source.download_as_bytes()\n\n    # We can't verify it, but ideally we would check that the following\n    # URL was resolvable with our credentials\n    # KEY_URL = 'https://cloudkms.googleapis.com/v1/{}'.format(\n    #     kms_key_name)\n\n    dest = kms_bucket.blob(blob_name, kms_key_name=kms_key_name)\n    token, rewritten, total = dest.rewrite(source)\n\n    while token is not None:\n        token, rewritten, total = dest.rewrite(source, token=token)\n\n    # Not adding 'dest' to 'self.case_blobs_to_delete':  it is the\n    # same object as 'source'.\n\n    assert token is None\n    assert rewritten == len(source_data)\n    assert total == len(source_data)\n\n    assert dest.download_as_bytes() == source_data\n\n    # Test existing kmsKeyName version is ignored in the rewrite request\n    dest = kms_bucket.get_blob(blob_name)\n    source = kms_bucket.get_blob(blob_name)\n    token, rewritten, total = dest.rewrite(source)\n\n    while token is not None:\n        token, rewritten, total = dest.rewrite(source, token=token)\n\n    assert rewritten == len(source_data)\n    assert dest.download_as_bytes() == source_data\n\n\ndef test_blob_upload_w_bucket_cmek_enabled(\n    kms_bucket,\n    blobs_to_delete,\n    kms_key_name,\n    alt_kms_key_name,\n):\n    blob_name = \"test-blob\"\n    override_blob_name = \"override-default-kms-key-name\"\n    payload = b\"DEADBEEF\"\n    alt_payload = b\"NEWDEADBEEF\"\n\n    kms_bucket.default_kms_key_name = kms_key_name\n    kms_bucket.patch()\n    assert kms_bucket.default_kms_key_name == kms_key_name\n\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    _helpers.await_config_changes_propagate()\n\n    blob = kms_bucket.blob(blob_name)\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    _helpers.retry_429_harder(_helpers.retry_has_kms_key_name(blob.reload))()\n    assert blob.kms_key_name.startswith(kms_key_name)\n\n    blob.upload_from_string(alt_payload, if_generation_match=blob.generation)\n    assert blob.download_as_bytes() == alt_payload\n\n    # Test the specific key is used to encrypt the object if you have both\n    # a default KMS key set on your bucket and a specific key included in your request.\n    override_blob = kms_bucket.blob(override_blob_name, kms_key_name=alt_kms_key_name)\n    override_blob.upload_from_string(payload)\n    blobs_to_delete.append(override_blob)\n\n    assert override_blob.download_as_bytes() == payload\n    assert override_blob.kms_key_name.startswith(alt_kms_key_name)\n\n    kms_bucket.default_kms_key_name = None\n    _helpers.retry_429_harder(kms_bucket.patch)()\n    assert kms_bucket.default_kms_key_name is None\n", "tests/system/_helpers.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport time\n\nfrom google.api_core import exceptions\n\nfrom test_utils.retry import RetryErrors\nfrom test_utils.retry import RetryInstanceState\nfrom test_utils.system import unique_resource_id\nfrom google.cloud.storage._helpers import _get_default_storage_base_url\n\nretry_429 = RetryErrors(exceptions.TooManyRequests)\nretry_429_harder = RetryErrors(exceptions.TooManyRequests, max_tries=10)\nretry_429_503 = RetryErrors(\n    (exceptions.TooManyRequests, exceptions.ServiceUnavailable), max_tries=10\n)\nretry_failures = RetryErrors(AssertionError)\n\nuser_project = os.environ.get(\"GOOGLE_CLOUD_TESTS_USER_PROJECT\")\ntesting_mtls = os.getenv(\"GOOGLE_API_USE_CLIENT_CERTIFICATE\") == \"true\"\nsigning_blob_content = b\"This time for sure, Rocky!\"\nis_api_endpoint_override = (\n    _get_default_storage_base_url() != \"https://storage.googleapis.com\"\n)\n\n\ndef _bad_copy(bad_request):\n    \"\"\"Predicate: pass only exceptions for a failed copyTo.\"\"\"\n    err_msg = bad_request.message\n    return err_msg.startswith(\"No file found in request. (POST\") and \"copyTo\" in err_msg\n\n\ndef _no_event_based_hold(blob):\n    return not blob.event_based_hold\n\n\ndef _has_kms_key_name(blob):\n    return blob.kms_key_name is not None\n\n\ndef _has_retention_expiration(blob):\n    return blob.retention_expiration_time is not None\n\n\ndef _no_retention_expiration(blob):\n    return blob.retention_expiration_time is None\n\n\ndef _has_retetion_period(bucket):\n    return bucket.retention_period is not None\n\n\ndef _no_retetion_period(bucket):\n    return bucket.retention_period is None\n\n\nretry_bad_copy = RetryErrors(exceptions.BadRequest, error_predicate=_bad_copy)\nretry_no_event_based_hold = RetryInstanceState(_no_event_based_hold, max_tries=5)\nretry_has_kms_key_name = RetryInstanceState(_has_kms_key_name, max_tries=5)\nretry_has_retention_expiration = RetryInstanceState(\n    _has_retention_expiration, max_tries=5\n)\nretry_no_retention_expiration = RetryInstanceState(\n    _no_retention_expiration, max_tries=5\n)\nretry_has_retention_period = RetryInstanceState(_has_retetion_period, max_tries=5)\nretry_no_retention_period = RetryInstanceState(_no_retetion_period, max_tries=5)\n\n\ndef unique_name(prefix):\n    return prefix + unique_resource_id(\"-\")\n\n\ndef empty_bucket(bucket):\n    for blob in list(bucket.list_blobs(versions=True)):\n        try:\n            blob.delete()\n        except exceptions.NotFound:\n            pass\n\n\ndef delete_blob(blob):\n    errors = (\n        exceptions.Conflict,\n        exceptions.TooManyRequests,\n        exceptions.ServiceUnavailable,\n    )\n    retry = RetryErrors(errors)\n    try:\n        retry(blob.delete)(timeout=120)  # seconds\n    except exceptions.NotFound:  # race\n        pass\n    except exceptions.Forbidden:  # event-based hold\n        blob.event_based_hold = False\n        blob.patch()\n        retry_no_event_based_hold(blob.reload)()\n        retry(blob.delete)(timeout=120)  # seconds\n\n\ndef delete_bucket(bucket):\n    errors = (\n        exceptions.Conflict,\n        exceptions.TooManyRequests,\n        exceptions.ServiceUnavailable,\n    )\n    retry = RetryErrors(errors, max_tries=15)\n    retry(empty_bucket)(bucket)\n    retry(bucket.delete)(force=True)\n\n\ndef await_config_changes_propagate(sec=12):\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    # See https://cloud.google.com/storage/docs/json_api/v1/buckets/patch\n    #\n    # The default was changed from 3 to 12 in May 2023 due to changes in bucket\n    # metadata handling. Note that the documentation recommends waiting \"30\n    # seconds\".\n    time.sleep(sec)\n", "tests/system/test_bucket.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport pytest\n\nfrom google.api_core import exceptions\nfrom . import _helpers\n\n\ndef test_bucket_create_w_alt_storage_class(storage_client, buckets_to_delete):\n    from google.cloud.storage import constants\n\n    bucket_name = _helpers.unique_name(\"bucket-w-archive\")\n\n    with pytest.raises(exceptions.NotFound):\n        storage_client.get_bucket(bucket_name)\n\n    bucket = storage_client.bucket(bucket_name)\n    bucket.storage_class = constants.ARCHIVE_STORAGE_CLASS\n\n    _helpers.retry_429_503(bucket.create)()\n    buckets_to_delete.append(bucket)\n\n    created = storage_client.get_bucket(bucket_name)\n    assert created.storage_class == constants.ARCHIVE_STORAGE_CLASS\n\n\ndef test_bucket_lifecycle_rules(storage_client, buckets_to_delete):\n    from google.cloud.storage import constants\n    from google.cloud.storage.bucket import LifecycleRuleDelete\n    from google.cloud.storage.bucket import LifecycleRuleSetStorageClass\n    from google.cloud.storage.bucket import LifecycleRuleAbortIncompleteMultipartUpload\n\n    bucket_name = _helpers.unique_name(\"w-lifcycle-rules\")\n    custom_time_before = datetime.date(2018, 8, 1)\n    noncurrent_before = datetime.date(2018, 8, 1)\n    matches_prefix = [\"storage-sys-test\", \"gcs-sys-test\"]\n    matches_suffix = [\"suffix-test\"]\n\n    with pytest.raises(exceptions.NotFound):\n        storage_client.get_bucket(bucket_name)\n\n    bucket = storage_client.bucket(bucket_name)\n    bucket.add_lifecycle_delete_rule(\n        age=42,\n        number_of_newer_versions=3,\n        days_since_custom_time=2,\n        custom_time_before=custom_time_before,\n        days_since_noncurrent_time=2,\n        noncurrent_time_before=noncurrent_before,\n        matches_prefix=matches_prefix,\n        matches_suffix=matches_suffix,\n    )\n    bucket.add_lifecycle_set_storage_class_rule(\n        constants.COLDLINE_STORAGE_CLASS,\n        is_live=False,\n        matches_storage_class=[constants.NEARLINE_STORAGE_CLASS],\n    )\n    bucket.add_lifecycle_abort_incomplete_multipart_upload_rule(\n        age=42,\n    )\n\n    expected_rules = [\n        LifecycleRuleDelete(\n            age=42,\n            number_of_newer_versions=3,\n            days_since_custom_time=2,\n            custom_time_before=custom_time_before,\n            days_since_noncurrent_time=2,\n            noncurrent_time_before=noncurrent_before,\n            matches_prefix=matches_prefix,\n            matches_suffix=matches_suffix,\n        ),\n        LifecycleRuleSetStorageClass(\n            constants.COLDLINE_STORAGE_CLASS,\n            is_live=False,\n            matches_storage_class=[constants.NEARLINE_STORAGE_CLASS],\n        ),\n        LifecycleRuleAbortIncompleteMultipartUpload(\n            age=42,\n        ),\n    ]\n\n    _helpers.retry_429_503(bucket.create)(location=\"us\")\n    buckets_to_delete.append(bucket)\n\n    assert bucket.name == bucket_name\n    assert list(bucket.lifecycle_rules) == expected_rules\n\n    # Test modifying lifecycle rules\n    expected_rules[0] = LifecycleRuleDelete(\n        age=30,\n        matches_prefix=[\"new-prefix\"],\n        matches_suffix=[\"new-suffix\"],\n    )\n    rules = list(bucket.lifecycle_rules)\n    rules[0][\"condition\"] = {\n        \"age\": 30,\n        \"matchesPrefix\": [\"new-prefix\"],\n        \"matchesSuffix\": [\"new-suffix\"],\n    }\n    bucket.lifecycle_rules = rules\n    bucket.patch()\n\n    assert list(bucket.lifecycle_rules) == expected_rules\n\n    # Test clearing lifecycle rules\n    bucket.clear_lifecyle_rules()\n    bucket.patch()\n\n    assert list(bucket.lifecycle_rules) == []\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\ndef test_bucket_update_labels(storage_client, buckets_to_delete):\n    bucket_name = _helpers.unique_name(\"update-labels\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n    assert bucket.exists()\n\n    updated_labels = {\"test-label\": \"label-value\"}\n    bucket.labels = updated_labels\n    bucket.update()\n    assert bucket.labels == updated_labels\n\n    new_labels = {\"another-label\": \"another-value\"}\n    bucket.labels = new_labels\n    bucket.patch()\n    assert bucket.labels == new_labels\n\n    bucket.labels = {}\n    # See https://github.com/googleapis/python-storage/issues/541\n    retry_400 = _helpers.RetryErrors(exceptions.BadRequest)\n    retry_400(bucket.update)()\n    assert bucket.labels == {}\n\n\ndef test_bucket_get_set_iam_policy(\n    storage_client,\n    buckets_to_delete,\n    service_account,\n):\n    from google.cloud.storage.iam import STORAGE_OBJECT_VIEWER_ROLE\n    from google.api_core.exceptions import BadRequest\n    from google.api_core.exceptions import PreconditionFailed\n\n    bucket_name = _helpers.unique_name(\"iam-policy\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n    assert bucket.exists()\n\n    policy_no_version = bucket.get_iam_policy()\n    assert policy_no_version.version == 1\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    assert policy == policy_no_version\n\n    member = f\"serviceAccount:{storage_client.get_service_account_email()}\"\n\n    binding_w_condition = {\n        \"role\": STORAGE_OBJECT_VIEWER_ROLE,\n        \"members\": {member},\n        \"condition\": {\n            \"title\": \"always-true\",\n            \"description\": \"test condition always-true\",\n            \"expression\": \"true\",\n        },\n    }\n    policy.bindings.append(binding_w_condition)\n\n    with pytest.raises(PreconditionFailed, match=\"enable uniform bucket-level access\"):\n        bucket.set_iam_policy(policy)\n\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.patch()\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    policy.bindings.append(binding_w_condition)\n\n    with pytest.raises(BadRequest, match=\"at least 3\"):\n        bucket.set_iam_policy(policy)\n\n    policy.version = 3\n    returned_policy = bucket.set_iam_policy(policy)\n    assert returned_policy.version == 3\n    assert returned_policy.bindings == policy.bindings\n\n    fetched_policy = bucket.get_iam_policy(requested_policy_version=3)\n    assert fetched_policy.bindings == returned_policy.bindings\n\n\ndef test_bucket_crud_w_requester_pays(storage_client, buckets_to_delete, user_project):\n    bucket_name = _helpers.unique_name(\"w-requester-pays\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(\n        bucket_name, requester_pays=True\n    )\n    buckets_to_delete.append(created)\n    assert created.name == bucket_name\n    assert created.requester_pays\n\n    with_user_project = storage_client.bucket(\n        bucket_name,\n        user_project=user_project,\n    )\n\n    try:\n        # Exercise 'buckets.get' w/ userProject.\n        assert with_user_project.exists()\n        with_user_project.reload()\n        assert with_user_project.requester_pays\n\n        # Exercise 'buckets.patch' w/ userProject.\n        with_user_project.configure_website(\n            main_page_suffix=\"index.html\", not_found_page=\"404.html\"\n        )\n        with_user_project.patch()\n        expected_website = {\"mainPageSuffix\": \"index.html\", \"notFoundPage\": \"404.html\"}\n        assert with_user_project._properties[\"website\"] == expected_website\n\n        # Exercise 'buckets.update' w/ userProject.\n        new_labels = {\"another-label\": \"another-value\"}\n        with_user_project.labels = new_labels\n        with_user_project.update()\n        assert with_user_project.labels == new_labels\n\n    finally:\n        # Exercise 'buckets.delete' w/ userProject.\n        with_user_project.delete()\n        buckets_to_delete.remove(created)\n\n\ndef test_bucket_acls_iam_w_user_project(\n    storage_client, buckets_to_delete, user_project\n):\n    bucket_name = _helpers.unique_name(\"acl-w-user-project\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(\n        bucket_name,\n        requester_pays=True,\n    )\n    buckets_to_delete.append(created)\n\n    with_user_project = storage_client.bucket(bucket_name, user_project=user_project)\n\n    # Exercise bucket ACL w/ userProject\n    acl = with_user_project.acl\n    acl.reload()\n    acl.all().grant_read()\n    acl.save()\n    assert \"READER\" in acl.all().get_roles()\n\n    del acl.entities[\"allUsers\"]\n    acl.save()\n    assert not acl.has_entity(\"allUsers\")\n\n    # Exercise default object ACL w/ userProject\n    doa = with_user_project.default_object_acl\n    doa.reload()\n    doa.all().grant_read()\n    doa.save()\n    assert \"READER\" in doa.all().get_roles()\n\n    # Exercise IAM w/ userProject\n    test_permissions = [\"storage.buckets.get\"]\n    found = with_user_project.test_iam_permissions(test_permissions)\n    assert found == test_permissions\n\n    policy = with_user_project.get_iam_policy()\n    viewers = policy.setdefault(\"roles/storage.objectViewer\", set())\n    viewers.add(policy.all_users())\n    with_user_project.set_iam_policy(policy)\n\n\ndef test_bucket_acls_w_metageneration_match(storage_client, buckets_to_delete):\n    wrong_metageneration_number = 9\n    bucket_name = _helpers.unique_name(\"acl-w-metageneration-match\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    # Exercise bucket ACL with metageneration match\n    acl = bucket.acl\n    acl.group(\"cloud-developer-relations@google.com\").grant_read()\n    bucket.reload()\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        acl.save(if_metageneration_match=wrong_metageneration_number)\n        assert (\n            \"READER\"\n            not in acl.group(\"cloud-developer-relations@google.com\").get_roles()\n        )\n\n    acl.save(if_metageneration_match=bucket.metageneration)\n    assert \"READER\" in acl.group(\"cloud-developer-relations@google.com\").get_roles()\n\n    # Exercise default object ACL w/ metageneration match\n    doa = bucket.default_object_acl\n    doa.group(\"cloud-developer-relations@google.com\").grant_owner()\n    bucket.reload()\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        doa.save(if_metageneration_match=wrong_metageneration_number)\n        assert (\n            \"OWNER\" not in doa.group(\"cloud-developer-relations@google.com\").get_roles()\n        )\n\n    doa.save(if_metageneration_match=bucket.metageneration)\n    assert \"OWNER\" in doa.group(\"cloud-developer-relations@google.com\").get_roles()\n\n\ndef test_bucket_copy_blob(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n    user_project,\n):\n    payload = b\"DEADBEEF\"\n    bucket_name = _helpers.unique_name(\"copy-blob\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(created)\n    assert created.name == bucket_name\n\n    blob = created.blob(\"CloudLogo\")\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    new_blob = _helpers.retry_bad_copy(created.copy_blob)(\n        blob, created, \"CloudLogoCopy\"\n    )\n    blobs_to_delete.append(new_blob)\n\n    copied_contents = new_blob.download_as_bytes()\n    assert copied_contents == payload\n\n\ndef test_bucket_copy_blob_w_user_project(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n    user_project,\n):\n    payload = b\"DEADBEEF\"\n    bucket_name = _helpers.unique_name(\"copy-w-requester-pays\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(\n        bucket_name, requester_pays=True\n    )\n    buckets_to_delete.append(created)\n    assert created.name == bucket_name\n    assert created.requester_pays\n\n    blob = created.blob(\"simple\")\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    with_user_project = storage_client.bucket(bucket_name, user_project=user_project)\n\n    new_blob = _helpers.retry_bad_copy(with_user_project.copy_blob)(\n        blob, with_user_project, \"simple-copy\"\n    )\n    blobs_to_delete.append(new_blob)\n\n    assert new_blob.download_as_bytes() == payload\n\n\ndef test_bucket_copy_blob_w_generation_match(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    payload = b\"DEADBEEF\"\n    bucket_name = _helpers.unique_name(\"generation-match\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(created)\n    assert created.name == bucket_name\n\n    blob = created.blob(\"simple\")\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    dest_bucket = storage_client.bucket(bucket_name)\n\n    new_blob = dest_bucket.copy_blob(\n        blob,\n        dest_bucket,\n        \"simple-copy\",\n        if_source_generation_match=blob.generation,\n    )\n    blobs_to_delete.append(new_blob)\n\n    assert new_blob.download_as_bytes() == payload\n\n\ndef test_bucket_copy_blob_w_metageneration_match(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    payload = b\"DEADBEEF\"\n    bucket_name = _helpers.unique_name(\"generation-match\")\n    bucket = storage_client.bucket(bucket_name)\n    bucket.requester_pays = True\n    created = _helpers.retry_429_503(storage_client.create_bucket)(bucket)\n    buckets_to_delete.append(created)\n    assert created.name == bucket_name\n\n    blob = created.blob(\"simple\")\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    dest_bucket = storage_client.bucket(bucket_name)\n\n    new_blob = dest_bucket.copy_blob(\n        blob,\n        dest_bucket,\n        \"simple-copy\",\n        if_source_metageneration_match=blob.metageneration,\n    )\n    blobs_to_delete.append(new_blob)\n\n    assert new_blob.download_as_bytes() == payload\n\n\ndef test_bucket_get_blob_with_user_project(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n    user_project,\n):\n    blob_name = \"blob-name\"\n    payload = b\"DEADBEEF\"\n    bucket_name = _helpers.unique_name(\"w-requester-pays\")\n    created = _helpers.retry_429_503(storage_client.create_bucket)(\n        bucket_name, requester_pays=True\n    )\n    buckets_to_delete.append(created)\n    assert created.name == bucket_name\n    assert created.requester_pays\n\n    with_user_project = storage_client.bucket(bucket_name, user_project=user_project)\n\n    assert with_user_project.get_blob(\"nonesuch\") is None\n\n    to_add = created.blob(blob_name)\n    to_add.upload_from_string(payload)\n    blobs_to_delete.append(to_add)\n\n    found = with_user_project.get_blob(blob_name)\n    assert found.download_as_bytes() == payload\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs(listable_bucket, listable_filenames):\n    all_blobs = list(listable_bucket.list_blobs())\n    assert sorted(blob.name for blob in all_blobs) == sorted(listable_filenames)\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_w_user_project(\n    storage_client,\n    listable_bucket,\n    listable_filenames,\n    user_project,\n):\n    with_user_project = storage_client.bucket(\n        listable_bucket.name, user_project=user_project\n    )\n    all_blobs = list(with_user_project.list_blobs())\n    assert sorted(blob.name for blob in all_blobs) == sorted(listable_filenames)\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_paginated(listable_bucket, listable_filenames):\n    truncation_size = 1\n    count = len(listable_filenames) - truncation_size\n    iterator = listable_bucket.list_blobs(max_results=count)\n    page_iter = iterator.pages\n\n    page1 = next(page_iter)\n    blobs = list(page1)\n    assert len(blobs) == count\n    assert iterator.next_page_token is not None\n    # Technically the iterator is exhausted.\n    assert iterator.num_results == iterator.max_results\n    # But we modify the iterator to continue paging after\n    # artificially stopping after ``count`` items.\n    iterator.max_results = None\n\n    page2 = next(page_iter)\n    last_blobs = list(page2)\n    assert len(last_blobs) == truncation_size\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_paginated_w_offset(listable_bucket, listable_filenames):\n    truncation_size = 1\n    inclusive_start_offset = listable_filenames[1]\n    exclusive_end_offset = listable_filenames[-1]\n    desired_files = listable_filenames[1:-1]\n    count = len(desired_files) - truncation_size\n    iterator = listable_bucket.list_blobs(\n        max_results=count,\n        start_offset=inclusive_start_offset,\n        end_offset=exclusive_end_offset,\n    )\n    page_iter = iterator.pages\n\n    page1 = next(page_iter)\n    blobs = list(page1)\n    assert len(blobs) == count\n    assert blobs[0].name == desired_files[0]\n    assert iterator.next_page_token is not None\n    # Technically the iterator is exhausted.\n    assert iterator.num_results == iterator.max_results\n    # But we modify the iterator to continue paging after\n    # artificially stopping after ``count`` items.\n    iterator.max_results = None\n\n    page2 = next(page_iter)\n    last_blobs = list(page2)\n    assert len(last_blobs) == truncation_size\n    assert last_blobs[-1].name == desired_files[-1]\n\n\n@_helpers.retry_failures\ndef test_blob_exists_hierarchy(hierarchy_bucket, hierarchy_filenames):\n    for filename in hierarchy_filenames:\n        blob = hierarchy_bucket.blob(filename)\n        assert blob.exists()\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_hierarchy_root_level(hierarchy_bucket, hierarchy_filenames):\n    expected_names = [\"file01.txt\"]\n    expected_prefixes = set([\"parent/\"])\n\n    iterator = hierarchy_bucket.list_blobs(delimiter=\"/\")\n    page = next(iterator.pages)\n    blobs = list(page)\n\n    assert [blob.name for blob in blobs] == expected_names\n    assert iterator.next_page_token is None\n    assert iterator.prefixes == expected_prefixes\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_hierarchy_first_level(hierarchy_bucket, hierarchy_filenames):\n    expected_names = [\"parent/\", \"parent/file11.txt\"]\n    expected_prefixes = set([\"parent/child/\"])\n\n    iterator = hierarchy_bucket.list_blobs(delimiter=\"/\", prefix=\"parent/\")\n    page = next(iterator.pages)\n    blobs = list(page)\n\n    assert [blob.name for blob in blobs] == expected_names\n    assert iterator.next_page_token is None\n    assert iterator.prefixes == expected_prefixes\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_hierarchy_second_level(\n    hierarchy_bucket, hierarchy_filenames\n):\n    expected_names = [\"parent/child/file21.txt\", \"parent/child/file22.txt\"]\n    expected_prefixes = set([\"parent/child/grand/\", \"parent/child/other/\"])\n\n    iterator = hierarchy_bucket.list_blobs(delimiter=\"/\", prefix=\"parent/child/\")\n    page = next(iterator.pages)\n    blobs = list(page)\n    assert [blob.name for blob in blobs] == expected_names\n    assert iterator.next_page_token is None\n    assert iterator.prefixes == expected_prefixes\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_hierarchy_third_level(hierarchy_bucket, hierarchy_filenames):\n    # Pseudo-hierarchy can be arbitrarily deep, subject to the limit\n    # of 1024 characters in the UTF-8 encoded name:\n    # https://cloud.google.com/storage/docs/bucketnaming#objectnames\n    # Exercise a layer deeper to illustrate this.\n    expected_names = [\"parent/child/grand/file31.txt\"]\n    expected_prefixes = set()\n\n    iterator = hierarchy_bucket.list_blobs(delimiter=\"/\", prefix=\"parent/child/grand/\")\n    page = next(iterator.pages)\n    blobs = list(page)\n\n    assert [blob.name for blob in blobs] == expected_names\n    assert iterator.next_page_token is None\n    assert iterator.prefixes == expected_prefixes\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_hierarchy_w_include_trailing_delimiter(\n    hierarchy_bucket,\n    hierarchy_filenames,\n):\n    expected_names = [\"file01.txt\", \"parent/\"]\n    expected_prefixes = set([\"parent/\"])\n\n    iterator = hierarchy_bucket.list_blobs(\n        delimiter=\"/\", include_trailing_delimiter=True\n    )\n    page = next(iterator.pages)\n    blobs = list(page)\n\n    assert [blob.name for blob in blobs] == expected_names\n    assert iterator.next_page_token is None\n    assert iterator.prefixes == expected_prefixes\n\n\n@_helpers.retry_failures\ndef test_bucket_list_blobs_w_match_glob(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    bucket_name = _helpers.unique_name(\"w-matchglob\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    payload = b\"helloworld\"\n    blob_names = [\"foo/bar\", \"foo/baz\", \"foo/foobar\", \"foobar\"]\n    for name in blob_names:\n        blob = bucket.blob(name)\n        blob.upload_from_string(payload)\n        blobs_to_delete.append(blob)\n\n    match_glob_results = {\n        \"foo*bar\": [\"foobar\"],\n        \"foo**bar\": [\"foo/bar\", \"foo/foobar\", \"foobar\"],\n        \"**/foobar\": [\"foo/foobar\", \"foobar\"],\n        \"*/ba[rz]\": [\"foo/bar\", \"foo/baz\"],\n        \"*/ba[!a-y]\": [\"foo/baz\"],\n        \"**/{foobar,baz}\": [\"foo/baz\", \"foo/foobar\", \"foobar\"],\n        \"foo/{foo*,*baz}\": [\"foo/baz\", \"foo/foobar\"],\n    }\n    for match_glob, expected_names in match_glob_results.items():\n        blob_iter = bucket.list_blobs(match_glob=match_glob)\n        blobs = list(blob_iter)\n        assert [blob.name for blob in blobs] == expected_names\n\n\ndef test_bucket_list_blobs_include_managed_folders(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n    hierarchy_filenames,\n):\n    bucket_name = _helpers.unique_name(\"ubla-mf\")\n    bucket = storage_client.bucket(bucket_name)\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    _helpers.retry_429_503(bucket.create)()\n    buckets_to_delete.append(bucket)\n\n    payload = b\"helloworld\"\n    for filename in hierarchy_filenames:\n        blob = bucket.blob(filename)\n        blob.upload_from_string(payload)\n        blobs_to_delete.append(blob)\n\n    # Make API call to create a managed folder.\n    # TODO: change to use storage control client once available.\n    path = f\"/b/{bucket_name}/managedFolders\"\n    properties = {\"name\": \"managedfolder1\"}\n    storage_client._post_resource(path, properties)\n\n    expected_prefixes = set([\"parent/\"])\n    blob_iter = bucket.list_blobs(delimiter=\"/\")\n    list(blob_iter)\n    assert blob_iter.prefixes == expected_prefixes\n\n    # Test that managed folders are only included when IncludeFoldersAsPrefixes is set.\n    expected_prefixes = set([\"parent/\", \"managedfolder1/\"])\n    blob_iter = bucket.list_blobs(delimiter=\"/\", include_folders_as_prefixes=True)\n    list(blob_iter)\n    assert blob_iter.prefixes == expected_prefixes\n\n    # Cleanup: API call to delete a managed folder.\n    # TODO: change to use storage control client once available.\n    path = f\"/b/{bucket_name}/managedFolders/managedfolder1\"\n    storage_client._delete_resource(path)\n\n\ndef test_bucket_update_retention_period(\n    storage_client,\n    buckets_to_delete,\n):\n    period_secs = 3\n    bucket_name = _helpers.unique_name(\"w-retention-period\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    bucket.retention_period = period_secs\n    bucket.default_event_based_hold = False\n    bucket.patch()\n\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    _helpers.retry_has_retention_period(bucket.reload)()\n\n    assert bucket.retention_period == period_secs\n    assert isinstance(bucket.retention_policy_effective_time, datetime.datetime)\n    assert not bucket.default_event_based_hold\n    assert not bucket.retention_policy_locked\n\n    bucket.retention_period = None\n    bucket.patch()\n\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    _helpers.retry_no_retention_period(bucket.reload)()\n\n    assert bucket.retention_period is None\n    assert bucket.retention_policy_effective_time is None\n    assert not bucket.default_event_based_hold\n    assert not bucket.retention_policy_locked\n\n\ndef test_delete_object_bucket_w_retention_period(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    # Create a bucket with retention period.\n    period_secs = 12\n    bucket = storage_client.bucket(_helpers.unique_name(\"w-retention-period\"))\n    bucket.retention_period = period_secs\n    bucket.default_event_based_hold = False\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket)\n    buckets_to_delete.append(bucket)\n\n    _helpers.retry_has_retention_period(bucket.reload)()\n    assert bucket.retention_period == period_secs\n    assert isinstance(bucket.retention_policy_effective_time, datetime.datetime)\n\n    payload = b\"DEADBEEF\"\n    blob = bucket.blob(_helpers.unique_name(\"w-retention\"))\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    _helpers.retry_has_retention_expiration(blob.reload)()\n    assert isinstance(blob.retention_expiration_time, datetime.datetime)\n    assert not blob.event_based_hold\n    assert not blob.temporary_hold\n\n    # Attempts to delete objects whose age is less than the retention period should fail.\n    with pytest.raises(exceptions.Forbidden):\n        blob.delete()\n\n    # Object can be deleted once it reaches the age defined in the retention policy.\n    _helpers.await_config_changes_propagate(sec=period_secs)\n    blob.delete()\n    blobs_to_delete.pop()\n\n\ndef test_bucket_w_default_event_based_hold(\n    storage_client,\n    blobs_to_delete,\n    default_ebh_bucket,\n):\n    bucket = storage_client.get_bucket(default_ebh_bucket)\n    assert bucket.default_event_based_hold\n    assert bucket.retention_period is None\n    assert bucket.retention_policy_effective_time is None\n    assert not bucket.retention_policy_locked\n\n    blob_name = \"test-blob\"\n    payload = b\"DEADBEEF\"\n    blob = bucket.blob(blob_name)\n    blob.upload_from_string(payload)\n\n    blobs_to_delete.append(blob)\n\n    other = bucket.get_blob(blob_name)\n\n    assert other.event_based_hold\n    assert not other.temporary_hold\n    assert other.retention_expiration_time is None\n\n    with pytest.raises(exceptions.Forbidden):\n        other.delete()\n\n    other.event_based_hold = False\n    other.patch()\n    other.delete()\n\n    bucket.default_event_based_hold = False\n    bucket.patch()\n\n    assert not bucket.default_event_based_hold\n    assert bucket.retention_period is None\n    assert bucket.retention_policy_effective_time is None\n    assert not bucket.retention_policy_locked\n\n    # Changes to the bucket will be readable immediately after writing,\n    # but configuration changes may take time to propagate.\n    _helpers.await_config_changes_propagate()\n\n    blob.upload_from_string(payload)\n\n    # https://github.com/googleapis/python-storage/issues/435\n    _helpers.retry_no_event_based_hold(blob.reload)()\n\n    assert not blob.event_based_hold\n    assert not blob.temporary_hold\n    assert blob.retention_expiration_time is None\n\n    blob.delete()\n    blobs_to_delete.pop()\n\n\ndef test_blob_w_temporary_hold(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    bucket_name = _helpers.unique_name(\"w-tmp-hold\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    blob_name = \"test-blob\"\n    payload = b\"DEADBEEF\"\n    blob = bucket.blob(blob_name)\n    blob.upload_from_string(payload)\n\n    blobs_to_delete.append(blob)\n\n    other = bucket.get_blob(blob_name)\n    other.temporary_hold = True\n    other.patch()\n\n    assert other.temporary_hold\n    assert not other.event_based_hold\n    assert other.retention_expiration_time is None\n\n    with pytest.raises(exceptions.Forbidden):\n        other.delete()\n\n    other.temporary_hold = False\n    other.patch()\n\n    other.delete()\n    blobs_to_delete.pop()\n\n\ndef test_bucket_lock_retention_policy(\n    storage_client,\n    buckets_to_delete,\n):\n    period_secs = 10\n    bucket_name = _helpers.unique_name(\"loc-ret-policy\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    bucket.retention_period = period_secs\n    bucket.patch()\n\n    assert bucket.retention_period == period_secs\n    assert isinstance(bucket.retention_policy_effective_time, datetime.datetime)\n    assert not bucket.default_event_based_hold\n    assert not bucket.retention_policy_locked\n\n    bucket.lock_retention_policy()\n\n    bucket.reload()\n    assert bucket.retention_policy_locked\n\n    bucket.retention_period = None\n    with pytest.raises(exceptions.Forbidden):\n        bucket.patch()\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\ndef test_new_bucket_w_ubla(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    bucket_name = _helpers.unique_name(\"new-w-ubla\")\n    bucket = storage_client.bucket(bucket_name)\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    _helpers.retry_429_503(bucket.create)()\n    buckets_to_delete.append(bucket)\n\n    bucket_acl = bucket.acl\n    with pytest.raises(exceptions.BadRequest):\n        bucket_acl.reload()\n\n    bucket_acl.loaded = True  # Fake that we somehow loaded the ACL\n    bucket_acl.group(\"cloud-developer-relations@google.com\").grant_read()\n    with pytest.raises(exceptions.BadRequest):\n        bucket_acl.save()\n\n    blob_name = \"my-blob.txt\"\n    blob = bucket.blob(blob_name)\n    payload = b\"DEADBEEF\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    found = bucket.get_blob(blob_name)\n    assert found.download_as_bytes() == payload\n\n    blob_acl = blob.acl\n    with pytest.raises(exceptions.BadRequest):\n        blob_acl.reload()\n\n    blob_acl.loaded = True  # Fake that we somehow loaded the ACL\n    blob_acl.group(\"cloud-developer-relations@google.com\").grant_read()\n    with pytest.raises(exceptions.BadRequest):\n        blob_acl.save()\n\n\ndef test_ubla_set_unset_preserves_acls(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    bucket_name = _helpers.unique_name(\"ubla-acls\")\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket_name)\n    buckets_to_delete.append(bucket)\n\n    blob_name = \"my-blob.txt\"\n    blob = bucket.blob(blob_name)\n    payload = b\"DEADBEEF\"\n    blob.upload_from_string(payload)\n    blobs_to_delete.append(blob)\n\n    # Preserve ACLs before setting UBLA\n    bucket_acl_before = list(bucket.acl)\n    blob_acl_before = list(bucket.acl)\n\n    # Set UBLA\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.patch()\n\n    assert bucket.iam_configuration.uniform_bucket_level_access_enabled\n\n    # While UBLA is set, cannot get / set ACLs\n    with pytest.raises(exceptions.BadRequest):\n        bucket.acl.reload()\n\n    # Clear UBLA\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = False\n    bucket.patch()\n    _helpers.await_config_changes_propagate()\n\n    # Query ACLs after clearing UBLA\n    bucket.acl.reload()\n    bucket_acl_after = list(bucket.acl)\n    blob.acl.reload()\n    blob_acl_after = list(bucket.acl)\n\n    assert bucket_acl_before == bucket_acl_after\n    assert blob_acl_before == blob_acl_after\n\n\ndef test_new_bucket_created_w_inherited_pap(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    from google.cloud.storage import constants\n\n    bucket_name = _helpers.unique_name(\"new-w-pap-inherited\")\n    bucket = storage_client.bucket(bucket_name)\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket.create()\n    buckets_to_delete.append(bucket)\n\n    # TODO: Remove unspecified after changeover is complete\n    assert bucket.iam_configuration.public_access_prevention in [\n        constants.PUBLIC_ACCESS_PREVENTION_UNSPECIFIED,\n        constants.PUBLIC_ACCESS_PREVENTION_INHERITED,\n    ]\n\n    bucket.iam_configuration.public_access_prevention = (\n        constants.PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n    bucket.patch()\n    assert (\n        bucket.iam_configuration.public_access_prevention\n        == constants.PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n    assert bucket.iam_configuration.uniform_bucket_level_access_enabled\n\n    bucket.iam_configuration.uniform_bucket_level_access_enabled = False\n    bucket.patch()\n\n    _helpers.await_config_changes_propagate()\n\n    assert (\n        bucket.iam_configuration.public_access_prevention\n        == constants.PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n\n    with pytest.raises(exceptions.BadRequest):\n        bucket.iam_configuration.public_access_prevention = \"unexpected value\"\n        bucket.patch()\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        bucket.make_public()\n\n    blob_name = \"my-blob.txt\"\n    blob = bucket.blob(blob_name)\n    payload = b\"DEADBEEF\"\n    blob.upload_from_string(payload)\n\n    with pytest.raises(exceptions.PreconditionFailed):\n        blob.make_public()\n\n\n@pytest.mark.skip(reason=\"Unspecified PAP is changing to inherited\")\ndef test_new_bucket_created_w_enforced_pap(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    from google.cloud.storage import constants\n\n    bucket_name = _helpers.unique_name(\"new-w-pap-enforced\")\n    bucket = storage_client.bucket(bucket_name)\n    bucket.iam_configuration.public_access_prevention = (\n        constants.PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n    bucket.create()\n    buckets_to_delete.append(bucket)\n\n    assert (\n        bucket.iam_configuration.public_access_prevention\n        == constants.PUBLIC_ACCESS_PREVENTION_ENFORCED\n    )\n\n    bucket.iam_configuration.public_access_prevention = (\n        constants.PUBLIC_ACCESS_PREVENTION_INHERITED\n    )\n    bucket.patch()\n\n    # TODO: Remove unspecified after changeover is complete\n    assert bucket.iam_configuration.public_access_prevention in [\n        constants.PUBLIC_ACCESS_PREVENTION_UNSPECIFIED,\n        constants.PUBLIC_ACCESS_PREVENTION_INHERITED,\n    ]\n    assert not bucket.iam_configuration.uniform_bucket_level_access_enabled\n\n\n@pytest.mark.skipif(\n    _helpers.is_api_endpoint_override,\n    reason=\"Test does not yet support endpoint override\",\n)\ndef test_new_bucket_with_rpo(\n    storage_client,\n    buckets_to_delete,\n    blobs_to_delete,\n):\n    from google.cloud.storage import constants\n\n    bucket_name = _helpers.unique_name(\"new-w-turbo-replication\")\n    bucket = storage_client.create_bucket(bucket_name, location=\"NAM4\")\n    buckets_to_delete.append(bucket)\n\n    assert bucket.rpo == constants.RPO_DEFAULT\n\n    bucket.rpo = constants.RPO_ASYNC_TURBO\n    bucket.patch()\n\n    bucket_from_server = storage_client.get_bucket(bucket_name)\n\n    assert bucket_from_server.rpo == constants.RPO_ASYNC_TURBO\n\n\ndef test_new_bucket_with_autoclass(\n    storage_client,\n    buckets_to_delete,\n):\n    from google.cloud.storage import constants\n\n    # Autoclass can be enabled via bucket create\n    bucket_name = _helpers.unique_name(\"new-w-autoclass\")\n    bucket_obj = storage_client.bucket(bucket_name)\n    bucket_obj.autoclass_enabled = True\n    bucket = storage_client.create_bucket(bucket_obj)\n    previous_toggle_time = bucket.autoclass_toggle_time\n    buckets_to_delete.append(bucket)\n\n    # Autoclass terminal_storage_class is defaulted to NEARLINE if not specified\n    assert bucket.autoclass_enabled is True\n    assert bucket.autoclass_terminal_storage_class == constants.NEARLINE_STORAGE_CLASS\n\n    # Autoclass can be enabled/disabled via bucket patch\n    bucket.autoclass_enabled = False\n    bucket.patch(if_metageneration_match=bucket.metageneration)\n\n    assert bucket.autoclass_enabled is False\n    assert bucket.autoclass_toggle_time != previous_toggle_time\n\n\ndef test_bucket_delete_force(storage_client):\n    bucket_name = _helpers.unique_name(\"version-disabled\")\n    bucket_obj = storage_client.bucket(bucket_name)\n    bucket = storage_client.create_bucket(bucket_obj)\n\n    BLOB_NAME = \"my_object\"\n    blob = bucket.blob(BLOB_NAME)\n    blob.upload_from_string(\"abcd\")\n    blob.upload_from_string(\"efgh\")\n\n    blobs = bucket.list_blobs(versions=True)\n    counter = 0\n    for blob in blobs:\n        counter += 1\n        assert blob.name == BLOB_NAME\n    assert counter == 1\n\n    bucket.delete(force=True)  # Will fail with 409 if blobs aren't deleted\n\n\ndef test_bucket_delete_force_works_with_versions(storage_client):\n    bucket_name = _helpers.unique_name(\"version-enabled\")\n    bucket_obj = storage_client.bucket(bucket_name)\n    bucket_obj.versioning_enabled = True\n    bucket = storage_client.create_bucket(bucket_obj)\n    assert bucket.versioning_enabled\n\n    BLOB_NAME = \"my_versioned_object\"\n    blob = bucket.blob(BLOB_NAME)\n    blob.upload_from_string(\"abcd\")\n    blob.upload_from_string(\"efgh\")\n\n    blobs = bucket.list_blobs(versions=True)\n    counter = 0\n    for blob in blobs:\n        counter += 1\n        assert blob.name == BLOB_NAME\n    assert counter == 2\n\n    bucket.delete(force=True)  # Will fail with 409 if versions aren't deleted\n\n\ndef test_config_autoclass_w_existing_bucket(\n    storage_client,\n    buckets_to_delete,\n):\n    from google.cloud.storage import constants\n\n    bucket_name = _helpers.unique_name(\"for-autoclass\")\n    bucket = storage_client.create_bucket(bucket_name)\n    buckets_to_delete.append(bucket)\n    assert bucket.autoclass_enabled is False\n    assert bucket.autoclass_toggle_time is None\n    assert bucket.autoclass_terminal_storage_class is None\n    assert bucket.autoclass_terminal_storage_class_update_time is None\n\n    # Enable Autoclass on existing buckets with terminal_storage_class set to ARCHIVE\n    bucket.autoclass_enabled = True\n    bucket.autoclass_terminal_storage_class = constants.ARCHIVE_STORAGE_CLASS\n    bucket.patch(if_metageneration_match=bucket.metageneration)\n    previous_tsc_update_time = bucket.autoclass_terminal_storage_class_update_time\n    assert bucket.autoclass_enabled is True\n    assert bucket.autoclass_terminal_storage_class == constants.ARCHIVE_STORAGE_CLASS\n\n    # Configure Autoclass terminal_storage_class to NEARLINE\n    bucket.autoclass_terminal_storage_class = constants.NEARLINE_STORAGE_CLASS\n    bucket.patch(if_metageneration_match=bucket.metageneration)\n    assert bucket.autoclass_enabled is True\n    assert bucket.autoclass_terminal_storage_class == constants.NEARLINE_STORAGE_CLASS\n    assert (\n        bucket.autoclass_terminal_storage_class_update_time != previous_tsc_update_time\n    )\n\n\ndef test_soft_delete_policy(\n    storage_client,\n    buckets_to_delete,\n):\n    from google.cloud.storage.bucket import SoftDeletePolicy\n\n    # Create a bucket with soft delete policy.\n    duration_secs = 7 * 86400\n    bucket = storage_client.bucket(_helpers.unique_name(\"w-soft-delete\"))\n    bucket.soft_delete_policy.retention_duration_seconds = duration_secs\n    bucket = _helpers.retry_429_503(storage_client.create_bucket)(bucket)\n    buckets_to_delete.append(bucket)\n\n    policy = bucket.soft_delete_policy\n    assert isinstance(policy, SoftDeletePolicy)\n    assert policy.retention_duration_seconds == duration_secs\n    assert isinstance(policy.effective_time, datetime.datetime)\n\n    # Insert an object and get object metadata prior soft-deleted.\n    payload = b\"DEADBEEF\"\n    blob_name = _helpers.unique_name(\"soft-delete\")\n    blob = bucket.blob(blob_name)\n    blob.upload_from_string(payload)\n\n    blob = bucket.get_blob(blob_name)\n    gen = blob.generation\n    assert blob.soft_delete_time is None\n    assert blob.hard_delete_time is None\n\n    # Delete the object to enter soft-deleted state.\n    blob.delete()\n\n    iter_default = bucket.list_blobs()\n    assert len(list(iter_default)) == 0\n    iter_w_soft_delete = bucket.list_blobs(soft_deleted=True)\n    assert len(list(iter_w_soft_delete)) > 0\n\n    # Get the soft-deleted object.\n    soft_deleted_blob = bucket.get_blob(blob_name, generation=gen, soft_deleted=True)\n    assert soft_deleted_blob.soft_delete_time is not None\n    assert soft_deleted_blob.hard_delete_time is not None\n\n    # Restore the soft-deleted object.\n    restored_blob = bucket.restore_blob(blob_name, generation=gen)\n    assert restored_blob.exists() is True\n    assert restored_blob.generation != gen\n\n    # Patch the soft delete policy on an existing bucket.\n    new_duration_secs = 10 * 86400\n    bucket.soft_delete_policy.retention_duration_seconds = new_duration_secs\n    bucket.patch()\n    assert bucket.soft_delete_policy.retention_duration_seconds == new_duration_secs\n\n\ndef test_new_bucket_with_hierarchical_namespace(\n    storage_client,\n    buckets_to_delete,\n):\n    # Test new bucket without specifying hierarchical namespace\n    bucket_name = _helpers.unique_name(\"new-wo-hns\")\n    bucket_obj = storage_client.bucket(bucket_name)\n    bucket = storage_client.create_bucket(bucket_obj)\n    buckets_to_delete.append(bucket)\n    assert bucket.hierarchical_namespace_enabled is None\n\n    # Test new bucket with hierarchical namespace disabled\n    bucket_name = _helpers.unique_name(\"new-hns-disabled\")\n    bucket_obj = storage_client.bucket(bucket_name)\n    bucket_obj.hierarchical_namespace_enabled = False\n    bucket = storage_client.create_bucket(bucket_obj)\n    buckets_to_delete.append(bucket)\n    assert bucket.hierarchical_namespace_enabled is False\n\n    # Test new bucket with hierarchical namespace enabled\n    bucket_name = _helpers.unique_name(\"new-hns-enabled\")\n    bucket_obj = storage_client.bucket(bucket_name)\n    bucket_obj.hierarchical_namespace_enabled = True\n    bucket_obj.iam_configuration.uniform_bucket_level_access_enabled = True\n    bucket = storage_client.create_bucket(bucket_obj)\n    buckets_to_delete.append(bucket)\n    assert bucket.hierarchical_namespace_enabled is True\n", "tests/perf/profile_w1r3.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Workload W1R3 profiling script. This is not an officially supported Google product.\"\"\"\n\nimport logging\nimport os\nimport random\nimport time\nimport uuid\n\nfrom functools import partial, update_wrapper\n\nfrom google.cloud import storage\n\nimport _perf_utils as _pu\n\n\ndef WRITE(bucket, blob_name, checksum, size, args, **kwargs):\n    \"\"\"Perform an upload and return latency.\"\"\"\n    blob = bucket.blob(blob_name)\n    file_path = f\"{os.getcwd()}/{uuid.uuid4().hex}\"\n    # Create random file locally on disk\n    with open(file_path, \"wb\") as file_obj:\n        file_obj.write(os.urandom(size))\n\n    start_time = time.monotonic_ns()\n    blob.upload_from_filename(file_path, checksum=checksum, if_generation_match=0)\n    end_time = time.monotonic_ns()\n\n    elapsed_time = round(\n        (end_time - start_time) / 1000\n    )  # convert nanoseconds to microseconds\n\n    # Clean up local file\n    _pu.cleanup_file(file_path)\n\n    return elapsed_time\n\n\ndef READ(bucket, blob_name, checksum, args, **kwargs):\n    \"\"\"Perform a download and return latency.\"\"\"\n    blob = bucket.blob(blob_name)\n    if not blob.exists():\n        raise Exception(\"Blob does not exist. Previous WRITE failed.\")\n\n    range_read_size = args.range_read_size\n    range_read_offset = kwargs.get(\"range_read_offset\")\n    # Perfor range read if range_read_size is specified, else get full object.\n    if range_read_size != 0:\n        start = range_read_offset\n        end = start + range_read_size - 1\n    else:\n        start = 0\n        end = -1\n\n    file_path = f\"{os.getcwd()}/{blob_name}\"\n    with open(file_path, \"wb\") as file_obj:\n        start_time = time.monotonic_ns()\n        blob.download_to_file(file_obj, checksum=checksum, start=start, end=end)\n        end_time = time.monotonic_ns()\n\n    elapsed_time = round(\n        (end_time - start_time) / 1000\n    )  # convert nanoseconds to microseconds\n\n    # Clean up local file\n    _pu.cleanup_file(file_path)\n\n    return elapsed_time\n\n\ndef _wrapped_partial(func, *args, **kwargs):\n    \"\"\"Helper method to create partial and propagate function name and doc from original function.\"\"\"\n    partial_func = partial(func, *args, **kwargs)\n    update_wrapper(partial_func, func)\n    return partial_func\n\n\ndef _generate_func_list(args):\n    \"\"\"Generate Write-1-Read-3 workload.\"\"\"\n    bucket_name = args.bucket\n    blob_name = f\"{_pu.TIMESTAMP}-{uuid.uuid4().hex}\"\n\n    # parse min_size and max_size from object_size\n    min_size, max_size = _pu.get_min_max_size(args.object_size)\n    # generate randmon size in bytes using a uniform distribution\n    size = random.randint(min_size, max_size)\n\n    # generate random checksumming type: md5, crc32c or None\n    idx_checksum = random.choice([0, 1, 2])\n    checksum = _pu.CHECKSUM[idx_checksum]\n\n    # generated random read_offset\n    range_read_offset = random.randint(\n        args.minimum_read_offset, args.maximum_read_offset\n    )\n\n    func_list = [\n        _wrapped_partial(\n            WRITE,\n            storage.Client().bucket(bucket_name),\n            blob_name,\n            size=size,\n            checksum=checksum,\n            args=args,\n        ),\n        *[\n            _wrapped_partial(\n                READ,\n                storage.Client().bucket(bucket_name),\n                blob_name,\n                size=size,\n                checksum=checksum,\n                args=args,\n                num=i,\n                range_read_offset=range_read_offset,\n            )\n            for i in range(3)\n        ],\n    ]\n    return func_list\n\n\ndef log_performance(func, args, elapsed_time, status, failure_msg):\n    \"\"\"Hold benchmarking results per operation call.\"\"\"\n    size = func.keywords.get(\"size\")\n    checksum = func.keywords.get(\"checksum\", None)\n    num = func.keywords.get(\"num\", None)\n    range_read_size = args.range_read_size\n\n    res = {\n        \"Op\": func.__name__,\n        \"ElapsedTimeUs\": elapsed_time,\n        \"ApiName\": args.api,\n        \"RunID\": _pu.TIMESTAMP,\n        \"CpuTimeUs\": _pu.NOT_SUPPORTED,\n        \"AppBufferSize\": _pu.NOT_SUPPORTED,\n        \"LibBufferSize\": _pu.DEFAULT_LIB_BUFFER_SIZE,\n        \"ChunkSize\": 0,\n        \"ObjectSize\": size,\n        \"TransferSize\": size,\n        \"TransferOffset\": 0,\n        \"RangeReadSize\": range_read_size,\n        \"BucketName\": args.bucket,\n        \"Library\": \"python-storage\",\n        \"Crc32cEnabled\": checksum == \"crc32c\",\n        \"MD5Enabled\": checksum == \"md5\",\n        \"FailureMsg\": failure_msg,\n        \"Status\": status,\n    }\n\n    if res[\"Op\"] == \"READ\":\n        res[\"Op\"] += f\"[{num}]\"\n\n        # For range reads (workload 2), record additional outputs\n        if range_read_size > 0:\n            res[\"TransferSize\"] = range_read_size\n            res[\"TransferOffset\"] = func.keywords.get(\"range_read_offset\", 0)\n\n    return res\n\n\ndef run_profile_w1r3(args):\n    \"\"\"Run w1r3 benchmarking. This is a wrapper used with the main benchmarking framework.\"\"\"\n    results = []\n\n    for func in _generate_func_list(args):\n        failure_msg = \"\"\n        try:\n            elapsed_time = func()\n        except Exception as e:\n            failure_msg = (\n                f\"Caught an exception while running operation {func.__name__}\\n {e}\"\n            )\n            logging.exception(failure_msg)\n            status = [\"FAIL\"]\n            elapsed_time = _pu.NOT_SUPPORTED\n        else:\n            status = [\"OK\"]\n\n        res = log_performance(func, args, elapsed_time, status, failure_msg)\n        results.append(res)\n\n    return results\n\n\ndef run_profile_range_read(args):\n    \"\"\"Run range read W2 benchmarking. This is a wrapper used with the main benchmarking framework.\"\"\"\n    results = []\n\n    for func in _generate_func_list(args):\n        failure_msg = \"\"\n        try:\n            elapsed_time = func()\n        except Exception as e:\n            failure_msg = (\n                f\"Caught an exception while running operation {func.__name__}\\n {e}\"\n            )\n            logging.exception(failure_msg)\n            status = [\"FAIL\"]\n            elapsed_time = _pu.NOT_SUPPORTED\n        else:\n            status = [\"OK\"]\n\n    # Only measure the last read\n    res = log_performance(func, args, elapsed_time, status, failure_msg)\n    results.append(res)\n\n    return results\n", "tests/perf/benchmarking.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Performance benchmarking main script. This is not an officially supported Google product.\"\"\"\n\nimport argparse\nimport logging\nimport multiprocessing\nimport sys\n\nfrom google.cloud import storage\n\nimport _perf_utils as _pu\nimport profile_w1r3 as w1r3\n\n\n##### PROFILE BENCHMARKING TEST TYPES #####\nPROFILE_WRITE_ONE_READ_THREE = \"w1r3\"\nPROFILE_RANGE_READ = \"range\"\n\n\ndef main(args):\n    # Track error logging for BBMC reporting.\n    counter = _pu.logCount()\n    logging.basicConfig(\n        level=logging.ERROR,\n        handlers=[counter, logging.StreamHandler(sys.stderr)],\n    )\n\n    # Create a storage bucket to run benchmarking.\n    if args.project is not None:\n        client = storage.Client(project=args.project)\n    else:\n        client = storage.Client()\n\n    bucket = client.bucket(args.bucket)\n    if not bucket.exists():\n        bucket = client.create_bucket(bucket, location=args.bucket_region)\n\n    # Define test type and number of processes to run benchmarking.\n    # Note that transfer manager tests defaults to using 1 process.\n    num_processes = 1\n    test_type = args.test_type\n    if test_type == PROFILE_WRITE_ONE_READ_THREE:\n        num_processes = args.workers\n        benchmark_runner = w1r3.run_profile_w1r3\n        logging.info(\n            f\"A total of {num_processes} processes are created to run benchmarking {test_type}\"\n        )\n    elif test_type == PROFILE_RANGE_READ:\n        num_processes = args.workers\n        benchmark_runner = w1r3.run_profile_range_read\n        logging.info(\n            f\"A total of {num_processes} processes are created to run benchmarking {test_type}\"\n        )\n\n    # Allow multiprocessing to speed up benchmarking tests; Defaults to 1 for no concurrency.\n    p = multiprocessing.Pool(num_processes)\n    pool_output = p.map(benchmark_runner, [args for _ in range(args.samples)])\n\n    # Output to Cloud Monitoring or CSV file.\n    output_type = args.output_type\n    if output_type == \"cloud-monitoring\":\n        _pu.convert_to_cloud_monitoring(args.bucket, pool_output, num_processes)\n    elif output_type == \"csv\":\n        _pu.convert_to_csv(args.output_file, pool_output, num_processes)\n        logging.info(\n            f\"Succesfully ran benchmarking. Please find your output log at {args.output_file}\"\n        )\n\n    # Cleanup and delete blobs.\n    _pu.cleanup_bucket(bucket, delete_bucket=args.delete_bucket)\n\n    # BBMC will not surface errors unless the process is terminated with a non zero code.\n    if counter.count.errors != 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--project\",\n        type=str,\n        default=None,\n        help=\"GCP project identifier\",\n    )\n    parser.add_argument(\n        \"--api\",\n        type=str,\n        default=\"JSON\",\n        help=\"API to use\",\n    )\n    parser.add_argument(\n        \"--test_type\",\n        type=str,\n        default=PROFILE_WRITE_ONE_READ_THREE,\n        help=\"Benchmarking test type\",\n    )\n    parser.add_argument(\n        \"--object_size\",\n        type=str,\n        default=_pu.DEFAULT_OBJECT_RANGE_SIZE_BYTES,\n        help=\"Object size in bytes; can be a range min..max\",\n    )\n    parser.add_argument(\n        \"--range_read_size\",\n        type=int,\n        default=0,\n        help=\"Size of the range to read in bytes\",\n    )\n    parser.add_argument(\n        \"--minimum_read_offset\",\n        type=int,\n        default=0,\n        help=\"Minimum offset for the start of the range to be read in bytes\",\n    )\n    parser.add_argument(\n        \"--maximum_read_offset\",\n        type=int,\n        default=0,\n        help=\"Maximum offset for the start of the range to be read in bytes\",\n    )\n    parser.add_argument(\n        \"--samples\",\n        type=int,\n        default=_pu.DEFAULT_NUM_SAMPLES,\n        help=\"Number of samples to report\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=_pu.DEFAULT_NUM_PROCESSES,\n        help=\"Number of processes- multiprocessing enabled\",\n    )\n    parser.add_argument(\n        \"--bucket\",\n        type=str,\n        default=_pu.DEFAULT_BUCKET_NAME,\n        help=\"Storage bucket name\",\n    )\n    parser.add_argument(\n        \"--bucket_region\",\n        type=str,\n        default=_pu.DEFAULT_BUCKET_REGION,\n        help=\"Bucket region\",\n    )\n    parser.add_argument(\n        \"--output_type\",\n        type=str,\n        default=\"cloud-monitoring\",\n        help=\"Ouput format, csv or cloud-monitoring\",\n    )\n    parser.add_argument(\n        \"--output_file\",\n        type=str,\n        default=_pu.DEFAULT_OUTPUT_FILE,\n        help=\"File to output results to\",\n    )\n    parser.add_argument(\n        \"--tmp_dir\",\n        type=str,\n        default=_pu.DEFAULT_BASE_DIR,\n        help=\"Temp directory path on file system\",\n    )\n    parser.add_argument(\n        \"--delete_bucket\",\n        type=bool,\n        default=False,\n        help=\"Whether or not to delete GCS bucket used for benchmarking\",\n    )\n    args = parser.parse_args()\n\n    main(args)\n", "tests/perf/_perf_utils.py": "# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Performance benchmarking helper methods. This is not an officially supported Google product.\"\"\"\n\nimport csv\nimport logging\nimport os\nimport random\nimport shutil\nimport time\nimport uuid\n\nfrom google.cloud import storage\n\n\n##### DEFAULTS & CONSTANTS #####\nHEADER = [\n    \"Op\",\n    \"ObjectSize\",\n    \"AppBufferSize\",\n    \"LibBufferSize\",\n    \"Crc32cEnabled\",\n    \"MD5Enabled\",\n    \"ApiName\",\n    \"ElapsedTimeUs\",\n    \"CpuTimeUs\",\n    \"Status\",\n]\nCHECKSUM = [\"md5\", \"crc32c\", None]\nTIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\nDEFAULT_API = \"JSON\"\nDEFAULT_BUCKET_NAME = f\"pybench{TIMESTAMP}\"\nDEFAULT_BUCKET_REGION = \"US-WEST1\"\nDEFAULT_OBJECT_RANGE_SIZE_BYTES = \"1048576\"  # 1 MiB\nDEFAULT_NUM_SAMPLES = 8000\nDEFAULT_NUM_PROCESSES = 16\nDEFAULT_LIB_BUFFER_SIZE = 104857600  # 100MB\nDEFAULT_CHUNKSIZE = 104857600  # 100 MB https://github.com/googleapis/python-storage/blob/main/google/cloud/storage/blob.py#L139\nNOT_SUPPORTED = -1\nDEFAULT_BASE_DIR = \"tm-perf-metrics\"\nDEFAULT_OUTPUT_FILE = f\"output_bench{TIMESTAMP}.csv\"\nDEFAULT_CREATE_SUBDIR_PROBABILITY = 0.1\nSSB_SIZE_THRESHOLD_BYTES = 1048576\n\n\n##### UTILITY METHODS #####\n\n\n# Returns a boolean value with the provided probability.\ndef weighted_random_boolean(create_subdir_probability):\n    return random.uniform(0.0, 1.0) <= create_subdir_probability\n\n\n# Creates a random file with the given file name, path and size.\ndef generate_random_file(file_name, file_path, size):\n    with open(os.path.join(file_path, file_name), \"wb\") as file_obj:\n        file_obj.write(os.urandom(size))\n\n\n# Creates a random directory structure consisting of subdirectories and random files.\n# Returns an array of all the generated paths and total size in bytes of all generated files.\ndef generate_random_directory(\n    max_objects,\n    min_file_size,\n    max_file_size,\n    base_dir,\n    create_subdir_probability=DEFAULT_CREATE_SUBDIR_PROBABILITY,\n):\n    directory_info = {\n        \"paths\": [],\n        \"total_size_in_bytes\": 0,\n    }\n\n    file_path = base_dir\n    os.makedirs(file_path, exist_ok=True)\n    for i in range(max_objects):\n        if weighted_random_boolean(create_subdir_probability):\n            file_path = f\"{file_path}/{uuid.uuid4().hex}\"\n            os.makedirs(file_path, exist_ok=True)\n            directory_info[\"paths\"].append(file_path)\n        else:\n            file_name = uuid.uuid4().hex\n            rand_size = random.randint(min_file_size, max_file_size)\n            generate_random_file(file_name, file_path, rand_size)\n            directory_info[\"total_size_in_bytes\"] += rand_size\n            directory_info[\"paths\"].append(os.path.join(file_path, file_name))\n\n    return directory_info\n\n\ndef results_to_csv(res):\n    results = []\n    for metric in HEADER:\n        results.append(res.get(metric, -1))\n    return results\n\n\ndef convert_to_csv(filename, results, workers):\n    with open(filename, \"w\") as file:\n        writer = csv.writer(file)\n        writer.writerow(HEADER)\n        # Benchmarking main script uses Multiprocessing Pool.map(),\n        # thus results is structured as List[List[Dict[str, any]]].\n        for result in results:\n            for row in result:\n                writer.writerow(results_to_csv(row))\n\n\ndef convert_to_cloud_monitoring(bucket_name, results, workers):\n    # Benchmarking main script uses Multiprocessing Pool.map(),\n    # thus results is structured as List[List[Dict[str, any]]].\n    for result in results:\n        for res in result:\n            # Only output successful benchmarking runs to cloud monitoring.\n            status = res.get(\"Status\").pop()  # convert [\"OK\"] --> \"OK\"\n            if status != \"OK\":\n                continue\n\n            range_read_size = res.get(\"RangeReadSize\", 0)\n            object_size = res.get(\"ObjectSize\")\n            elapsed_time_us = res.get(\"ElapsedTimeUs\")\n\n            # Handle range reads and calculate throughput using range_read_size.\n            if range_read_size > 0:\n                size = range_read_size\n            else:\n                size = object_size\n\n            # If size is greater than the defined threshold, report in MiB/s, otherwise report in KiB/s.\n            if size >= SSB_SIZE_THRESHOLD_BYTES:\n                throughput = (size / 1024 / 1024) / (elapsed_time_us / 1_000_000)\n            else:\n                throughput = (size / 1024) / (elapsed_time_us / 1_000_000)\n\n            cloud_monitoring_output = (\n                \"throughput{\"\n                + \"library=python-storage,\"\n                + \"api={},\".format(res.get(\"ApiName\"))\n                + \"op={},\".format(res.get(\"Op\"))\n                + \"workers={},\".format(workers)\n                + \"object_size={},\".format(object_size)\n                + \"transfer_offset={},\".format(res.get(\"TransferOffset\", 0))\n                + \"transfer_size={},\".format(res.get(\"TransferSize\", object_size))\n                + \"app_buffer_size={},\".format(res.get(\"AppBufferSize\"))\n                + \"chunksize={},\".format(res.get(\"TransferSize\", object_size))\n                + \"crc32c_enabled={},\".format(res.get(\"Crc32cEnabled\"))\n                + \"md5_enabled={},\".format(res.get(\"MD5Enabled\"))\n                + \"cpu_time_us={},\".format(res.get(\"CpuTimeUs\"))\n                + \"peer='',\"\n                + f\"bucket_name={bucket_name},\"\n                + \"retry_count='',\"\n                + f\"status={status}\"\n                + \"}\"\n                f\"{throughput}\"\n            )\n\n            print(cloud_monitoring_output)\n\n\ndef cleanup_directory_tree(directory):\n    \"\"\"Clean up directory tree on disk.\"\"\"\n    try:\n        shutil.rmtree(directory)\n    except Exception as e:\n        logging.exception(f\"Caught an exception while deleting local directory\\n {e}\")\n\n\ndef cleanup_file(file_path):\n    \"\"\"Clean up local file on disk.\"\"\"\n    try:\n        os.remove(file_path)\n    except Exception as e:\n        logging.exception(f\"Caught an exception while deleting local file\\n {e}\")\n\n\ndef get_bucket_instance(bucket_name):\n    client = storage.Client()\n    bucket = client.bucket(bucket_name)\n    if not bucket.exists():\n        client.create_bucket(bucket)\n    return bucket\n\n\ndef cleanup_bucket(bucket, delete_bucket=False):\n    # Delete blobs first as the bucket may contain more than 256 blobs.\n    try:\n        blobs = bucket.list_blobs()\n        for blob in blobs:\n            blob.delete()\n    except Exception as e:\n        logging.exception(f\"Caught an exception while deleting blobs\\n {e}\")\n    # Delete bucket if delete_bucket is set to True\n    if delete_bucket:\n        try:\n            bucket.delete(force=True)\n        except Exception as e:\n            logging.exception(f\"Caught an exception while deleting bucket\\n {e}\")\n\n\ndef get_min_max_size(object_size):\n    # Object size accepts a single value in bytes or a range in bytes min..max\n    if object_size.find(\"..\") < 0:\n        min_size = int(object_size)\n        max_size = int(object_size)\n    else:\n        split_sizes = object_size.split(\"..\")\n        min_size = int(split_sizes[0])\n        max_size = int(split_sizes[1])\n    return min_size, max_size\n\n\nclass logCount(logging.Handler):\n    class LogType:\n        def __init__(self):\n            self.errors = 0\n\n    def __init__(self):\n        super().__init__()\n        self.count = self.LogType()\n\n    def emit(self, record):\n        if record.levelname == \"ERROR\":\n            self.count.errors += 1\n", "tests/unit/test_hmac_key.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport mock\n\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\n\n\nclass TestHMACKeyMetadata(unittest.TestCase):\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n        return HMACKeyMetadata\n\n    def _make_one(self, client=None, *args, **kw):\n        if client is None:\n            client = _Client()\n        return self._get_target_class()(client, *args, **kw)\n\n    def test_ctor_defaults(self):\n        client = object()\n        metadata = self._make_one(client)\n        self.assertIs(metadata._client, client)\n        self.assertEqual(metadata._properties, {})\n        self.assertIsNone(metadata.access_id)\n        self.assertIsNone(metadata.etag)\n        self.assertIsNone(metadata.id)\n        self.assertIsNone(metadata.project)\n        self.assertIsNone(metadata.service_account_email)\n        self.assertIsNone(metadata.state)\n        self.assertIsNone(metadata.time_created)\n        self.assertIsNone(metadata.updated)\n\n    def test_ctor_explicit(self):\n        OTHER_PROJECT = \"other-project-456\"\n        ACCESS_ID = \"access-id-123456789\"\n        USER_PROJECT = \"billed-project\"\n        client = _Client()\n        metadata = self._make_one(\n            client,\n            access_id=ACCESS_ID,\n            project_id=OTHER_PROJECT,\n            user_project=USER_PROJECT,\n        )\n        self.assertIs(metadata._client, client)\n        expected = {\"accessId\": ACCESS_ID, \"projectId\": OTHER_PROJECT}\n        self.assertEqual(metadata._properties, expected)\n        self.assertEqual(metadata.access_id, ACCESS_ID)\n        self.assertEqual(metadata.user_project, USER_PROJECT)\n        self.assertIsNone(metadata.etag)\n        self.assertIsNone(metadata.id)\n        self.assertEqual(metadata.project, OTHER_PROJECT)\n        self.assertIsNone(metadata.service_account_email)\n        self.assertIsNone(metadata.state)\n        self.assertIsNone(metadata.time_created)\n        self.assertIsNone(metadata.updated)\n\n    def test___eq___other_type(self):\n        metadata = self._make_one()\n        for bogus in (None, \"bogus\", 123, 456.78, [], (), {}, set()):\n            self.assertNotEqual(metadata, bogus)\n\n    def test___eq___mismatched_client(self):\n        metadata = self._make_one()\n        other_client = _Client(project=\"other-project-456\")\n        other = self._make_one(other_client)\n        self.assertNotEqual(metadata, other)\n\n    def test___eq___mismatched_access_id(self):\n        metadata = self._make_one()\n        metadata._properties[\"accessId\"] = \"ABC123\"\n        other = self._make_one(metadata._client)\n        metadata._properties[\"accessId\"] = \"DEF456\"\n        self.assertNotEqual(metadata, other)\n\n    def test___eq___hit(self):\n        metadata = self._make_one()\n        metadata._properties[\"accessId\"] = \"ABC123\"\n        other = self._make_one(metadata._client)\n        other._properties[\"accessId\"] = metadata.access_id\n        self.assertEqual(metadata, other)\n\n    def test___hash__(self):\n        client = _Client()\n        metadata = self._make_one(client)\n        metadata._properties[\"accessId\"] = \"ABC123\"\n        self.assertIsInstance(hash(metadata), int)\n        other = self._make_one(client)\n        metadata._properties[\"accessId\"] = \"DEF456\"\n        self.assertNotEqual(hash(metadata), hash(other))\n\n    def test_access_id_getter(self):\n        metadata = self._make_one()\n        expected = \"ACCESS-ID\"\n        metadata._properties[\"accessId\"] = expected\n        self.assertEqual(metadata.access_id, expected)\n\n    def test_etag_getter(self):\n        metadata = self._make_one()\n        expected = \"ETAG\"\n        metadata._properties[\"etag\"] = expected\n        self.assertEqual(metadata.etag, expected)\n\n    def test_id_getter(self):\n        metadata = self._make_one()\n        expected = \"ID\"\n        metadata._properties[\"id\"] = expected\n        self.assertEqual(metadata.id, expected)\n\n    def test_project_getter(self):\n        metadata = self._make_one()\n        expected = \"PROJECT-ID\"\n        metadata._properties[\"projectId\"] = expected\n        self.assertEqual(metadata.project, expected)\n\n    def test_service_account_email_getter(self):\n        metadata = self._make_one()\n        expected = \"service_account@example.com\"\n        metadata._properties[\"serviceAccountEmail\"] = expected\n        self.assertEqual(metadata.service_account_email, expected)\n\n    def test_state_getter(self):\n        metadata = self._make_one()\n        expected = \"STATE\"\n        metadata._properties[\"state\"] = expected\n        self.assertEqual(metadata.state, expected)\n\n    def test_state_setter_invalid_state(self):\n        metadata = self._make_one()\n        expected = \"INVALID\"\n        metadata.state = expected\n\n        # Test that invalid states are allowed without client side validation.\n        # Fall back to server side validation and errors.\n        self.assertEqual(metadata.state, expected)\n        self.assertEqual(metadata._properties[\"state\"], expected)\n\n    def test_state_setter_inactive(self):\n        metadata = self._make_one()\n        metadata._properties[\"state\"] = \"ACTIVE\"\n        expected = \"INACTIVE\"\n        metadata.state = expected\n        self.assertEqual(metadata.state, expected)\n        self.assertEqual(metadata._properties[\"state\"], expected)\n\n    def test_state_setter_active(self):\n        metadata = self._make_one()\n        metadata._properties[\"state\"] = \"INACTIVE\"\n        expected = \"ACTIVE\"\n        metadata.state = expected\n        self.assertEqual(metadata.state, expected)\n        self.assertEqual(metadata._properties[\"state\"], expected)\n\n    def test_time_created_getter(self):\n        metadata = self._make_one()\n        now = _NOW()\n        now_stamp = f\"{now.isoformat()}Z\"\n        metadata._properties[\"timeCreated\"] = now_stamp\n        self.assertEqual(metadata.time_created, now.replace(tzinfo=_UTC))\n\n    def test_updated_getter(self):\n        metadata = self._make_one()\n        now = _NOW()\n        now_stamp = f\"{now.isoformat()}Z\"\n        metadata._properties[\"updated\"] = now_stamp\n        self.assertEqual(metadata.updated, now.replace(tzinfo=_UTC))\n\n    def test_path_wo_access_id(self):\n        metadata = self._make_one()\n\n        with self.assertRaises(ValueError):\n            metadata.path\n\n    def test_path_w_access_id_wo_project(self):\n        access_id = \"ACCESS-ID\"\n        client = _Client()\n        metadata = self._make_one()\n        metadata._properties[\"accessId\"] = access_id\n\n        expected_path = f\"/projects/{client.DEFAULT_PROJECT}/hmacKeys/{access_id}\"\n        self.assertEqual(metadata.path, expected_path)\n\n    def test_path_w_access_id_w_explicit_project(self):\n        access_id = \"ACCESS-ID\"\n        project = \"OTHER-PROJECT\"\n        metadata = self._make_one()\n        metadata._properties[\"accessId\"] = access_id\n        metadata._properties[\"projectId\"] = project\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        self.assertEqual(metadata.path, expected_path)\n\n    def test_exists_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        access_id = \"ACCESS-ID\"\n        project = \"PROJECT\"\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        client.project = project\n        metadata = self._make_one(client)\n        metadata._properties[\"accessId\"] = access_id\n\n        self.assertFalse(metadata.exists())\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_exists_hit_w_explicit_w_user_project(self):\n        project = \"PROJECT-ID\"\n        access_id = \"ACCESS-ID\"\n        user_project = \"billed-project\"\n        email = \"service-account@example.com\"\n        resource = {\n            \"kind\": \"storage#hmacKeyMetadata\",\n            \"accessId\": access_id,\n            \"serviceAccountEmail\": email,\n        }\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = resource\n        metadata = self._make_one(client, user_project=user_project)\n        metadata._properties[\"accessId\"] = access_id\n        metadata._properties[\"projectId\"] = project\n\n        self.assertTrue(metadata.exists(timeout=timeout, retry=retry))\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_reload_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        access_id = \"ACCESS-ID\"\n        project = \"PROJECT\"\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        client.project = project\n        metadata = self._make_one(client)\n        metadata._properties[\"accessId\"] = access_id\n\n        with self.assertRaises(NotFound):\n            metadata.reload()\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_reload_hit_w_project_set(self):\n        project = \"PROJECT-ID\"\n        access_id = \"ACCESS-ID\"\n        user_project = \"billed-project\"\n        email = \"service-account@example.com\"\n        resource = {\n            \"kind\": \"storage#hmacKeyMetadata\",\n            \"accessId\": access_id,\n            \"serviceAccountEmail\": email,\n        }\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = resource\n        metadata = self._make_one(client, user_project=user_project)\n        metadata._properties[\"accessId\"] = access_id\n        metadata._properties[\"projectId\"] = project\n\n        metadata.reload(timeout=timeout, retry=retry)\n\n        self.assertEqual(metadata._properties, resource)\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_update_miss_no_project_set_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"PROJECT\"\n        access_id = \"ACCESS-ID\"\n        client = mock.Mock(spec=[\"_put_resource\", \"project\"])\n        client._put_resource.side_effect = NotFound(\"testing\")\n        client.project = project\n        metadata = self._make_one(client)\n        metadata._properties[\"accessId\"] = access_id\n        metadata.state = \"INACTIVE\"\n\n        with self.assertRaises(NotFound):\n            metadata.update()\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_data = {\"state\": \"INACTIVE\"}\n        expected_query_params = {}\n        client._put_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n        )\n\n    def test_update_hit_w_project_set_w_timeout_w_retry(self):\n        project = \"PROJECT-ID\"\n        access_id = \"ACCESS-ID\"\n        user_project = \"billed-project\"\n        email = \"service-account@example.com\"\n        resource = {\n            \"kind\": \"storage#hmacKeyMetadata\",\n            \"accessId\": access_id,\n            \"serviceAccountEmail\": email,\n            \"state\": \"ACTIVE\",\n        }\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = resource\n        metadata = self._make_one(client, user_project=user_project)\n        metadata._properties[\"accessId\"] = access_id\n        metadata._properties[\"projectId\"] = project\n        metadata.state = \"ACTIVE\"\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        metadata.update(timeout=42, retry=retry)\n\n        self.assertEqual(metadata._properties, resource)\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_data = {\"state\": \"ACTIVE\"}\n        expected_query_params = {\"userProject\": user_project}\n        client._put_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_delete_not_inactive(self):\n        client = mock.Mock(spec=[\"_delete_resource\", \"project\"])\n        client.project = \"PROJECT\"\n        metadata = self._make_one(client)\n\n        for state in (\"ACTIVE\", \"DELETED\"):\n            metadata._properties[\"state\"] = state\n\n            with self.assertRaises(ValueError):\n                metadata.delete()\n\n        client._delete_resource.assert_not_called()\n\n    def test_delete_miss_no_project_set_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        access_id = \"ACCESS-ID\"\n        client = mock.Mock(spec=[\"_delete_resource\", \"project\"])\n        client._delete_resource.side_effect = NotFound(\"testing\")\n        client.project = \"PROJECT\"\n        metadata = self._make_one(client)\n        metadata._properties[\"accessId\"] = access_id\n        metadata.state = \"INACTIVE\"\n\n        with self.assertRaises(NotFound):\n            metadata.delete()\n\n        expected_path = f\"/projects/{client.project}/hmacKeys/{access_id}\"\n        expected_query_params = {}\n        client._delete_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_delete_hit_w_project_set_w_explicit_timeout_retry(self):\n        project = \"PROJECT-ID\"\n        access_id = \"ACCESS-ID\"\n        user_project = \"billed-project\"\n        client = mock.Mock(spec=[\"_delete_resource\", \"project\"])\n        client.project = \"CLIENT-PROJECT\"\n        client._delete_resource.return_value = {}\n        metadata = self._make_one(client, user_project=user_project)\n        metadata._properties[\"accessId\"] = access_id\n        metadata._properties[\"projectId\"] = project\n        metadata.state = \"INACTIVE\"\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        metadata.delete(timeout=timeout, retry=retry)\n\n        expected_path = f\"/projects/{project}/hmacKeys/{access_id}\"\n        expected_query_params = {\"userProject\": user_project}\n        client._delete_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n\nclass _Client(object):\n    DEFAULT_PROJECT = \"project-123\"\n\n    def __init__(self, connection=None, project=DEFAULT_PROJECT):\n        self._connection = connection\n        self.project = project\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):  # pragma: NO COVER\n            return NotImplemented\n        return self._connection == other._connection and self.project == other.project\n\n    def __hash__(self):\n        return hash(self._connection) + hash(self.project)\n", "tests/unit/test_blob.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport datetime\nimport hashlib\nimport io\nimport json\nimport os\nimport tempfile\nimport unittest\nimport http.client\nfrom unittest.mock import patch\nfrom urllib.parse import urlencode\n\nimport mock\nimport pytest\n\nfrom google.cloud.storage import _helpers\nfrom google.cloud.storage._helpers import _get_default_headers\nfrom google.cloud.storage._helpers import _get_default_storage_base_url\nfrom google.cloud.storage._helpers import _DEFAULT_UNIVERSE_DOMAIN\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\nfrom google.cloud.storage.retry import (\n    DEFAULT_RETRY,\n    DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n)\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom tests.unit.test__helpers import GCCL_INVOCATION_TEST_CONST\n\n\ndef _make_credentials():\n    import google.auth.credentials\n\n    return mock.Mock(spec=google.auth.credentials.Credentials)\n\n\nclass Test_Blob(unittest.TestCase):\n    @staticmethod\n    def _make_one(*args, **kw):\n        from google.cloud.storage.blob import Blob\n\n        properties = kw.pop(\"properties\", {})\n        blob = Blob(*args, **kw)\n        blob._properties.update(properties)\n        return blob\n\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    @staticmethod\n    def _make_client(*args, **kw):\n        from google.cloud.storage.client import Client\n\n        kw[\"api_endpoint\"] = kw.get(\"api_endpoint\") or _get_default_storage_base_url()\n        return mock.create_autospec(Client, instance=True, **kw)\n\n    def test_ctor_wo_encryption_key(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {\"key\": \"value\"}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, BLOB_NAME)\n        self.assertEqual(blob._properties, properties)\n        self.assertFalse(blob._acl.loaded)\n        self.assertIs(blob._acl.blob, blob)\n        self.assertEqual(blob._encryption_key, None)\n        self.assertEqual(blob.kms_key_name, None)\n\n    def test_ctor_with_encoded_unicode(self):\n        blob_name = b\"wet \\xe2\\x9b\\xb5\"\n        blob = self._make_one(blob_name, bucket=None)\n        unicode_name = \"wet \\N{sailboat}\"\n        self.assertNotIsInstance(blob.name, bytes)\n        self.assertIsInstance(blob.name, str)\n        self.assertEqual(blob.name, unicode_name)\n\n    def test_ctor_w_encryption_key(self):\n        KEY = b\"01234567890123456789012345678901\"  # 32 bytes\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket, encryption_key=KEY)\n        self.assertEqual(blob._encryption_key, KEY)\n        self.assertEqual(blob.kms_key_name, None)\n\n    def test_ctor_w_kms_key_name_and_encryption_key(self):\n        KEY = b\"01234567890123456789012345678901\"  # 32 bytes\n        KMS_RESOURCE = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n\n        with self.assertRaises(ValueError):\n            self._make_one(\n                BLOB_NAME, bucket=bucket, encryption_key=KEY, kms_key_name=KMS_RESOURCE\n            )\n\n    def test_ctor_w_kms_key_name(self):\n        KMS_RESOURCE = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket, kms_key_name=KMS_RESOURCE)\n        self.assertEqual(blob._encryption_key, None)\n        self.assertEqual(blob.kms_key_name, KMS_RESOURCE)\n\n    def test_ctor_with_generation(self):\n        BLOB_NAME = \"blob-name\"\n        GENERATION = 12345\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket, generation=GENERATION)\n        self.assertEqual(blob.generation, GENERATION)\n\n    def _set_properties_helper(self, kms_key_name=None):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        now = _NOW(_UTC)\n        NOW = now.strftime(_RFC3339_MICROS)\n        BLOB_NAME = \"blob-name\"\n        GENERATION = 12345\n        BLOB_ID = f\"name/{BLOB_NAME}/{GENERATION}\"\n        SELF_LINK = \"http://example.com/self/\"\n        METAGENERATION = 23456\n        SIZE = 12345\n        MD5_HASH = \"DEADBEEF\"\n        MEDIA_LINK = \"http://example.com/media/\"\n        ENTITY = \"project-owner-12345\"\n        ENTITY_ID = \"23456\"\n        CRC32C = \"FACE0DAC\"\n        COMPONENT_COUNT = 2\n        ETAG = \"ETAG\"\n        resource = {\n            \"id\": BLOB_ID,\n            \"selfLink\": SELF_LINK,\n            \"generation\": GENERATION,\n            \"metageneration\": METAGENERATION,\n            \"contentType\": \"text/plain\",\n            \"timeCreated\": NOW,\n            \"updated\": NOW,\n            \"timeDeleted\": NOW,\n            \"storageClass\": \"NEARLINE\",\n            \"timeStorageClassUpdated\": NOW,\n            \"size\": SIZE,\n            \"md5Hash\": MD5_HASH,\n            \"mediaLink\": MEDIA_LINK,\n            \"contentEncoding\": \"gzip\",\n            \"contentDisposition\": \"inline\",\n            \"contentLanguage\": \"en-US\",\n            \"cacheControl\": \"private\",\n            \"metadata\": {\"foo\": \"Foo\"},\n            \"owner\": {\"entity\": ENTITY, \"entityId\": ENTITY_ID},\n            \"crc32c\": CRC32C,\n            \"componentCount\": COMPONENT_COUNT,\n            \"etag\": ETAG,\n            \"customTime\": NOW,\n        }\n\n        if kms_key_name is not None:\n            resource[\"kmsKeyName\"] = kms_key_name\n\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n\n        blob._set_properties(resource)\n\n        self.assertEqual(blob.id, BLOB_ID)\n        self.assertEqual(blob.self_link, SELF_LINK)\n        self.assertEqual(blob.generation, GENERATION)\n        self.assertEqual(blob.metageneration, METAGENERATION)\n        self.assertEqual(blob.content_type, \"text/plain\")\n        self.assertEqual(blob.time_created, now)\n        self.assertEqual(blob.updated, now)\n        self.assertEqual(blob.time_deleted, now)\n        self.assertEqual(blob.storage_class, \"NEARLINE\")\n        self.assertEqual(blob.size, SIZE)\n        self.assertEqual(blob.md5_hash, MD5_HASH)\n        self.assertEqual(blob.media_link, MEDIA_LINK)\n        self.assertEqual(blob.content_encoding, \"gzip\")\n        self.assertEqual(blob.content_disposition, \"inline\")\n        self.assertEqual(blob.content_language, \"en-US\")\n        self.assertEqual(blob.cache_control, \"private\")\n        self.assertEqual(blob.metadata, {\"foo\": \"Foo\"})\n        self.assertEqual(blob.owner, {\"entity\": ENTITY, \"entityId\": ENTITY_ID})\n        self.assertEqual(blob.crc32c, CRC32C)\n        self.assertEqual(blob.component_count, COMPONENT_COUNT)\n        self.assertEqual(blob.etag, ETAG)\n        self.assertEqual(blob.custom_time, now)\n\n        if kms_key_name is not None:\n            self.assertEqual(blob.kms_key_name, kms_key_name)\n        else:\n            self.assertIsNone(blob.kms_key_name)\n\n    def test__set_properties_wo_kms_key_name(self):\n        self._set_properties_helper()\n\n    def test__set_properties_w_kms_key_name(self):\n        kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        self._set_properties_helper(kms_key_name=kms_resource)\n\n    def test_chunk_size_ctor(self):\n        from google.cloud.storage.blob import Blob\n\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        chunk_size = 10 * Blob._CHUNK_SIZE_MULTIPLE\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET, chunk_size=chunk_size)\n        self.assertEqual(blob._chunk_size, chunk_size)\n\n    def test_chunk_size_getter(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob.chunk_size)\n        VALUE = object()\n        blob._chunk_size = VALUE\n        self.assertIs(blob.chunk_size, VALUE)\n\n    def test_chunk_size_setter(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob._chunk_size)\n        blob._CHUNK_SIZE_MULTIPLE = 10\n        blob.chunk_size = 20\n        self.assertEqual(blob._chunk_size, 20)\n\n    def test_chunk_size_setter_bad_value(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob._chunk_size)\n        blob._CHUNK_SIZE_MULTIPLE = 10\n        with self.assertRaises(ValueError):\n            blob.chunk_size = 11\n\n    def test_acl_property(self):\n        from google.cloud.storage.acl import ObjectACL\n\n        fake_bucket = _Bucket()\n        blob = self._make_one(\"name\", bucket=fake_bucket)\n        acl = blob.acl\n        self.assertIsInstance(acl, ObjectACL)\n        self.assertIs(acl, blob._acl)\n\n    def test_encryption_key_getter(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob.encryption_key)\n        VALUE = object()\n        blob._encryption_key = VALUE\n        self.assertIs(blob.encryption_key, VALUE)\n\n    def test_encryption_key_setter(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob._encryption_key)\n        key = b\"12345678901234567890123456789012\"\n        blob.encryption_key = key\n        self.assertEqual(blob._encryption_key, key)\n\n    def test_kms_key_name_getter(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob.kms_key_name)\n        VALUE = object()\n        blob._patch_property(\"kmsKeyName\", VALUE)\n        self.assertIs(blob.kms_key_name, VALUE)\n\n    def test_kms_key_name_setter(self):\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n        self.assertIsNone(blob._properties.get(\"kmsKeyName\"))\n        kms_key_name = \"cryptoKeys/test-key\"\n        blob.kms_key_name = kms_key_name\n        self.assertEqual(blob._properties.get(\"kmsKeyName\"), kms_key_name)\n\n    def test_path_bad_bucket(self):\n        fake_bucket = object()\n        name = \"blob-name\"\n        blob = self._make_one(name, bucket=fake_bucket)\n        self.assertRaises(AttributeError, getattr, blob, \"path\")\n\n    def test_path_no_name(self):\n        bucket = _Bucket()\n        blob = self._make_one(\"\", bucket=bucket)\n        self.assertRaises(ValueError, getattr, blob, \"path\")\n\n    def test_path_normal(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(blob.path, f\"/b/name/o/{BLOB_NAME}\")\n\n    def test_path_w_slash_in_name(self):\n        BLOB_NAME = \"parent/child\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(blob.path, \"/b/name/o/parent%2Fchild\")\n\n    def test_path_with_non_ascii(self):\n        blob_name = \"Caf\\xe9\"\n        bucket = _Bucket()\n        blob = self._make_one(blob_name, bucket=bucket)\n        self.assertEqual(blob.path, \"/b/name/o/Caf%C3%A9\")\n\n    def test_bucket_readonly_property(self):\n        blob_name = \"BLOB\"\n        bucket = _Bucket()\n        other = _Bucket()\n        blob = self._make_one(blob_name, bucket=bucket)\n        with self.assertRaises(AttributeError):\n            blob.bucket = other\n\n    def test_client(self):\n        blob_name = \"BLOB\"\n        bucket = _Bucket()\n        blob = self._make_one(blob_name, bucket=bucket)\n        self.assertIs(blob.client, bucket.client)\n\n    def test_user_project(self):\n        user_project = \"user-project-123\"\n        blob_name = \"BLOB\"\n        bucket = _Bucket(user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n        self.assertEqual(blob.user_project, user_project)\n\n    def test__encryption_headers_wo_encryption_key(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        expected = {}\n        self.assertEqual(blob._encryption_headers(), expected)\n\n    def test__encryption_headers_w_encryption_key(self):\n        key = b\"aa426195405adee2c8081bb9e7e74b19\"\n        header_key_value = \"YWE0MjYxOTU0MDVhZGVlMmM4MDgxYmI5ZTdlNzRiMTk=\"\n        header_key_hash_value = \"V3Kwe46nKc3xLv96+iJ707YfZfFvlObta8TQcx2gpm0=\"\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket, encryption_key=key)\n        expected = {\n            \"X-Goog-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Encryption-Key\": header_key_value,\n            \"X-Goog-Encryption-Key-Sha256\": header_key_hash_value,\n        }\n        self.assertEqual(blob._encryption_headers(), expected)\n\n    def test__query_params_default(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(blob._query_params, {})\n\n    def test__query_params_w_user_project(self):\n        user_project = \"user-project-123\"\n        BLOB_NAME = \"BLOB\"\n        bucket = _Bucket(user_project=user_project)\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(blob._query_params, {\"userProject\": user_project})\n\n    def test__query_params_w_generation(self):\n        generation = 123456\n        BLOB_NAME = \"BLOB\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket, generation=generation)\n        self.assertEqual(blob._query_params, {\"generation\": generation})\n\n    def test_public_url(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(\n            blob.public_url, f\"https://storage.googleapis.com/name/{BLOB_NAME}\"\n        )\n\n    def test_public_url_w_slash_in_name(self):\n        BLOB_NAME = \"parent/child\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(\n            blob.public_url, \"https://storage.googleapis.com/name/parent/child\"\n        )\n\n    def test_public_url_w_tilde_in_name(self):\n        BLOB_NAME = \"foo~bar\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(blob.public_url, \"https://storage.googleapis.com/name/foo~bar\")\n\n    def test_public_url_with_non_ascii(self):\n        blob_name = \"winter \\N{snowman}\"\n        bucket = _Bucket()\n        blob = self._make_one(blob_name, bucket=bucket)\n        expected_url = \"https://storage.googleapis.com/name/winter%20%E2%98%83\"\n        self.assertEqual(blob.public_url, expected_url)\n\n    def test_public_url_without_client(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        bucket.client = None\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertEqual(\n            blob.public_url, f\"https://storage.googleapis.com/name/{BLOB_NAME}\"\n        )\n\n    def test_generate_signed_url_w_invalid_version(self):\n        BLOB_NAME = \"blob-name\"\n        EXPIRATION = \"2014-10-16T20:34:37.000Z\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n\n        with self.assertRaises(ValueError):\n            blob.generate_signed_url(EXPIRATION, version=\"nonesuch\")\n\n    def _generate_signed_url_helper(\n        self,\n        version=None,\n        blob_name=\"blob-name\",\n        api_access_endpoint=None,\n        method=\"GET\",\n        content_md5=None,\n        content_type=None,\n        response_type=None,\n        response_disposition=None,\n        generation=None,\n        headers=None,\n        query_parameters=None,\n        credentials=None,\n        expiration=None,\n        encryption_key=None,\n        access_token=None,\n        service_account_email=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n    ):\n        from urllib import parse\n        from google.cloud.storage._helpers import _bucket_bound_hostname_url\n        from google.cloud.storage._helpers import _get_default_storage_base_url\n        from google.cloud.storage.blob import _get_encryption_headers\n\n        delta = datetime.timedelta(hours=1)\n\n        if expiration is None:\n            expiration = _NOW(_UTC) + delta\n\n        if credentials is None:\n            expected_creds = _make_credentials()\n            client = self._make_client(_credentials=expected_creds)\n        else:\n            expected_creds = credentials\n            client = self._make_client(_credentials=object())\n\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket, encryption_key=encryption_key)\n\n        if version is None:\n            effective_version = \"v2\"\n        else:\n            effective_version = version\n\n        to_patch = f\"google.cloud.storage.blob.generate_signed_url_{effective_version}\"\n\n        with mock.patch(to_patch) as signer:\n            signed_uri = blob.generate_signed_url(\n                expiration=expiration,\n                api_access_endpoint=api_access_endpoint,\n                method=method,\n                credentials=credentials,\n                content_md5=content_md5,\n                content_type=content_type,\n                response_type=response_type,\n                response_disposition=response_disposition,\n                generation=generation,\n                headers=headers,\n                query_parameters=query_parameters,\n                version=version,\n                access_token=access_token,\n                service_account_email=service_account_email,\n                virtual_hosted_style=virtual_hosted_style,\n                bucket_bound_hostname=bucket_bound_hostname,\n            )\n\n        self.assertEqual(signed_uri, signer.return_value)\n\n        encoded_name = blob_name.encode(\"utf-8\")\n        quoted_name = parse.quote(encoded_name, safe=b\"/~\")\n\n        if virtual_hosted_style:\n            expected_api_access_endpoint = \"https://{}.storage.googleapis.com\".format(\n                bucket.name\n            )\n        elif bucket_bound_hostname:\n            expected_api_access_endpoint = _bucket_bound_hostname_url(\n                bucket_bound_hostname, scheme\n            )\n        else:\n            expected_api_access_endpoint = (\n                api_access_endpoint\n                if api_access_endpoint\n                else _get_default_storage_base_url()\n            )\n            expected_resource = f\"/{bucket.name}/{quoted_name}\"\n\n        if virtual_hosted_style or bucket_bound_hostname:\n            expected_resource = f\"/{quoted_name}\"\n\n        if encryption_key is not None:\n            expected_headers = headers or {}\n            if effective_version == \"v2\":\n                expected_headers[\"X-Goog-Encryption-Algorithm\"] = \"AES256\"\n            else:\n                expected_headers.update(_get_encryption_headers(encryption_key))\n        else:\n            expected_headers = headers\n\n        expected_kwargs = {\n            \"resource\": expected_resource,\n            \"expiration\": expiration,\n            \"api_access_endpoint\": expected_api_access_endpoint,\n            \"method\": method.upper(),\n            \"content_md5\": content_md5,\n            \"content_type\": content_type,\n            \"response_type\": response_type,\n            \"response_disposition\": response_disposition,\n            \"generation\": generation,\n            \"headers\": expected_headers,\n            \"query_parameters\": query_parameters,\n            \"access_token\": access_token,\n            \"service_account_email\": service_account_email,\n        }\n        signer.assert_called_once_with(expected_creds, **expected_kwargs)\n\n    def test_generate_signed_url_no_version_passed_warning(self):\n        self._generate_signed_url_helper()\n\n    def _generate_signed_url_v2_helper(self, **kw):\n        version = \"v2\"\n        self._generate_signed_url_helper(version, **kw)\n\n    def test_generate_signed_url_v2_w_defaults(self):\n        self._generate_signed_url_v2_helper()\n\n    def test_generate_signed_url_v2_w_expiration(self):\n        expiration = _NOW(_UTC)\n        self._generate_signed_url_v2_helper(expiration=expiration)\n\n    def test_generate_signed_url_v2_w_non_ascii_name(self):\n        BLOB_NAME = \"\\u0410\\u043a\\u043a\\u043e\\u0440\\u0434\\u044b.txt\"\n        self._generate_signed_url_v2_helper(blob_name=BLOB_NAME)\n\n    def test_generate_signed_url_v2_w_slash_in_name(self):\n        BLOB_NAME = \"parent/child\"\n        self._generate_signed_url_v2_helper(blob_name=BLOB_NAME)\n\n    def test_generate_signed_url_v2_w_tilde_in_name(self):\n        BLOB_NAME = \"foo~bar\"\n        self._generate_signed_url_v2_helper(blob_name=BLOB_NAME)\n\n    def test_generate_signed_url_v2_w_endpoint(self):\n        self._generate_signed_url_v2_helper(\n            api_access_endpoint=\"https://api.example.com/v1\"\n        )\n\n    def test_generate_signed_url_v2_w_method(self):\n        self._generate_signed_url_v2_helper(method=\"POST\")\n\n    def test_generate_signed_url_v2_w_lowercase_method(self):\n        self._generate_signed_url_v2_helper(method=\"get\")\n\n    def test_generate_signed_url_v2_w_content_md5(self):\n        self._generate_signed_url_v2_helper(content_md5=\"FACEDACE\")\n\n    def test_generate_signed_url_v2_w_content_type(self):\n        self._generate_signed_url_v2_helper(content_type=\"text.html\")\n\n    def test_generate_signed_url_v2_w_response_type(self):\n        self._generate_signed_url_v2_helper(response_type=\"text.html\")\n\n    def test_generate_signed_url_v2_w_response_disposition(self):\n        self._generate_signed_url_v2_helper(response_disposition=\"inline\")\n\n    def test_generate_signed_url_v2_w_generation(self):\n        self._generate_signed_url_v2_helper(generation=12345)\n\n    def test_generate_signed_url_v2_w_headers(self):\n        self._generate_signed_url_v2_helper(headers={\"x-goog-foo\": \"bar\"})\n\n    def test_generate_signed_url_v2_w_csek(self):\n        self._generate_signed_url_v2_helper(encryption_key=os.urandom(32))\n\n    def test_generate_signed_url_v2_w_csek_and_headers(self):\n        self._generate_signed_url_v2_helper(\n            encryption_key=os.urandom(32), headers={\"x-goog-foo\": \"bar\"}\n        )\n\n    def test_generate_signed_url_v2_w_credentials(self):\n        credentials = object()\n        self._generate_signed_url_v2_helper(credentials=credentials)\n\n    def _generate_signed_url_v4_helper(self, **kw):\n        version = \"v4\"\n        self._generate_signed_url_helper(version, **kw)\n\n    def test_generate_signed_url_v4_w_defaults(self):\n        self._generate_signed_url_v4_helper()\n\n    def test_generate_signed_url_v4_w_non_ascii_name(self):\n        BLOB_NAME = \"\\u0410\\u043a\\u043a\\u043e\\u0440\\u0434\\u044b.txt\"\n        self._generate_signed_url_v4_helper(blob_name=BLOB_NAME)\n\n    def test_generate_signed_url_v4_w_slash_in_name(self):\n        BLOB_NAME = \"parent/child\"\n        self._generate_signed_url_v4_helper(blob_name=BLOB_NAME)\n\n    def test_generate_signed_url_v4_w_tilde_in_name(self):\n        BLOB_NAME = \"foo~bar\"\n        self._generate_signed_url_v4_helper(blob_name=BLOB_NAME)\n\n    def test_generate_signed_url_v4_w_endpoint(self):\n        self._generate_signed_url_v4_helper(\n            api_access_endpoint=\"https://api.example.com/v1\"\n        )\n\n    def test_generate_signed_url_v4_w_method(self):\n        self._generate_signed_url_v4_helper(method=\"POST\")\n\n    def test_generate_signed_url_v4_w_lowercase_method(self):\n        self._generate_signed_url_v4_helper(method=\"get\")\n\n    def test_generate_signed_url_v4_w_content_md5(self):\n        self._generate_signed_url_v4_helper(content_md5=\"FACEDACE\")\n\n    def test_generate_signed_url_v4_w_content_type(self):\n        self._generate_signed_url_v4_helper(content_type=\"text.html\")\n\n    def test_generate_signed_url_v4_w_response_type(self):\n        self._generate_signed_url_v4_helper(response_type=\"text.html\")\n\n    def test_generate_signed_url_v4_w_response_disposition(self):\n        self._generate_signed_url_v4_helper(response_disposition=\"inline\")\n\n    def test_generate_signed_url_v4_w_generation(self):\n        self._generate_signed_url_v4_helper(generation=12345)\n\n    def test_generate_signed_url_v4_w_headers(self):\n        self._generate_signed_url_v4_helper(headers={\"x-goog-foo\": \"bar\"})\n\n    def test_generate_signed_url_v4_w_csek(self):\n        self._generate_signed_url_v4_helper(encryption_key=os.urandom(32))\n\n    def test_generate_signed_url_v4_w_csek_and_headers(self):\n        self._generate_signed_url_v4_helper(\n            encryption_key=os.urandom(32), headers={\"x-goog-foo\": \"bar\"}\n        )\n\n    def test_generate_signed_url_v4_w_virtual_hostname(self):\n        self._generate_signed_url_v4_helper(virtual_hosted_style=True)\n\n    def test_generate_signed_url_v4_w_bucket_bound_hostname_w_scheme(self):\n        self._generate_signed_url_v4_helper(\n            bucket_bound_hostname=\"http://cdn.example.com\"\n        )\n\n    def test_generate_signed_url_v4_w_bucket_bound_hostname_w_bare_hostname(self):\n        self._generate_signed_url_v4_helper(bucket_bound_hostname=\"cdn.example.com\")\n\n    def test_generate_signed_url_v4_w_credentials(self):\n        credentials = object()\n        self._generate_signed_url_v4_helper(credentials=credentials)\n\n    def test_generate_signed_url_v4_w_incompatible_params(self):\n        with self.assertRaises(ValueError):\n            self._generate_signed_url_v4_helper(\n                api_access_endpoint=\"example.com\",\n                bucket_bound_hostname=\"cdn.example.com\",\n            )\n        with self.assertRaises(ValueError):\n            self._generate_signed_url_v4_helper(\n                virtual_hosted_style=True, bucket_bound_hostname=\"cdn.example.com\"\n            )\n\n    def test_exists_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        blob_name = \"nonesuch\"\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertFalse(blob.exists())\n\n        expected_query_params = {\"fields\": \"name\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            blob.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_exists_hit_w_user_project_w_timeout(self):\n        blob_name = \"blob-name\"\n        user_project = \"user-project-123\"\n        timeout = 42\n        api_response = {\"name\": blob_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client, user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertTrue(blob.exists(timeout=timeout))\n\n        expected_query_params = {\"fields\": \"name\", \"userProject\": user_project}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            blob.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_exists_hit_w_generation_w_retry(self):\n        blob_name = \"blob-name\"\n        generation = 123456\n        api_response = {\"name\": blob_name}\n        retry = mock.Mock(spec=[])\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket, generation=generation)\n\n        self.assertTrue(blob.exists(retry=retry))\n\n        expected_query_params = {\"fields\": \"name\", \"generation\": generation}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            blob.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_exists_hit_w_generation_w_soft_deleted(self):\n        blob_name = \"blob-name\"\n        generation = 123456\n        api_response = {\"name\": blob_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket, generation=generation)\n\n        self.assertTrue(blob.exists(retry=None, soft_deleted=True))\n\n        expected_query_params = {\n            \"fields\": \"name\",\n            \"generation\": generation,\n            \"softDeleted\": True,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            blob.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=None,\n        )\n\n    def test_exists_w_etag_match(self):\n        blob_name = \"blob-name\"\n        etag = \"kittens\"\n        api_response = {\"name\": blob_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertTrue(\n            blob.exists(\n                if_etag_match=etag,\n                retry=None,\n            )\n        )\n\n        expected_query_params = {\n            \"fields\": \"name\",\n        }\n        expected_headers = {\n            \"If-Match\": etag,\n        }\n        client._get_resource.assert_called_once_with(\n            blob.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=None,\n        )\n\n    def test_exists_w_generation_match(self):\n        blob_name = \"blob-name\"\n        generation_number = 123456\n        metageneration_number = 6\n        api_response = {\"name\": blob_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertTrue(\n            blob.exists(\n                if_generation_match=generation_number,\n                if_metageneration_match=metageneration_number,\n                retry=None,\n            )\n        )\n\n        expected_query_params = {\n            \"fields\": \"name\",\n            \"ifGenerationMatch\": generation_number,\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            blob.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=None,\n        )\n\n    def test_delete_wo_generation(self):\n        BLOB_NAME = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        bucket._blobs[BLOB_NAME] = 1\n\n        blob.delete()\n\n        self.assertEqual(\n            bucket._deleted,\n            [\n                (\n                    BLOB_NAME,\n                    None,\n                    None,\n                    self._get_default_timeout(),\n                    None,\n                    None,\n                    None,\n                    None,\n                    DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n                )\n            ],\n        )\n\n    def test_delete_w_generation(self):\n        BLOB_NAME = \"blob-name\"\n        GENERATION = 123456\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(BLOB_NAME, bucket=bucket, generation=GENERATION)\n        bucket._blobs[BLOB_NAME] = 1\n\n        blob.delete(timeout=42)\n\n        self.assertEqual(\n            bucket._deleted,\n            [\n                (\n                    BLOB_NAME,\n                    None,\n                    GENERATION,\n                    42,\n                    None,\n                    None,\n                    None,\n                    None,\n                    DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n                )\n            ],\n        )\n\n    def test_delete_w_generation_match(self):\n        BLOB_NAME = \"blob-name\"\n        GENERATION = 123456\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(BLOB_NAME, bucket=bucket, generation=GENERATION)\n        bucket._blobs[BLOB_NAME] = 1\n\n        blob.delete(timeout=42, if_generation_match=GENERATION)\n\n        self.assertEqual(\n            bucket._deleted,\n            [\n                (\n                    BLOB_NAME,\n                    None,\n                    GENERATION,\n                    42,\n                    GENERATION,\n                    None,\n                    None,\n                    None,\n                    DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n                )\n            ],\n        )\n\n    def test__get_transport(self):\n        client = mock.Mock(spec=[\"_credentials\", \"_http\"])\n        client._http = mock.sentinel.transport\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        transport = blob._get_transport(client)\n\n        self.assertIs(transport, mock.sentinel.transport)\n\n    def test__get_download_url_with_media_link(self):\n        blob_name = \"something.txt\"\n        bucket = _Bucket(name=\"IRRELEVANT\")\n        blob = self._make_one(blob_name, bucket=bucket)\n        media_link = \"http://test.invalid\"\n        # Set the media link on the blob\n        blob._properties[\"mediaLink\"] = media_link\n\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(client)\n\n        self.assertEqual(download_url, media_link)\n\n    def test__get_download_url_with_generation_match(self):\n        GENERATION_NUMBER = 6\n        MEDIA_LINK = \"http://test.invalid\"\n\n        blob = self._make_one(\"something.txt\", bucket=_Bucket(name=\"IRRELEVANT\"))\n        # Set the media link on the blob\n        blob._properties[\"mediaLink\"] = MEDIA_LINK\n\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(\n            client, if_generation_match=GENERATION_NUMBER\n        )\n        self.assertEqual(\n            download_url,\n            f\"{MEDIA_LINK}?ifGenerationMatch={GENERATION_NUMBER}\",\n        )\n\n    def test__get_download_url_with_media_link_w_user_project(self):\n        blob_name = \"something.txt\"\n        user_project = \"user-project-123\"\n        bucket = _Bucket(name=\"IRRELEVANT\", user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n        media_link = \"http://test.invalid\"\n        # Set the media link on the blob\n        blob._properties[\"mediaLink\"] = media_link\n\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(client)\n\n        self.assertEqual(download_url, f\"{media_link}?userProject={user_project}\")\n\n    def test__get_download_url_on_the_fly(self):\n        blob_name = \"bzzz-fly.txt\"\n        bucket = _Bucket(name=\"buhkit\")\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertIsNone(blob.media_link)\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(client)\n        expected_url = (\n            \"https://storage.googleapis.com/download/storage/v1/b/\"\n            \"buhkit/o/bzzz-fly.txt?alt=media\"\n        )\n        self.assertEqual(download_url, expected_url)\n\n    def test__get_download_url_mtls(self):\n        blob_name = \"bzzz-fly.txt\"\n        bucket = _Bucket(name=\"buhkit\")\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertIsNone(blob.media_link)\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._connection.get_api_base_url_for_mtls = mock.Mock(\n            return_value=\"https://foo.mtls\"\n        )\n        download_url = blob._get_download_url(client)\n        del client._connection.get_api_base_url_for_mtls\n        expected_url = (\n            \"https://foo.mtls/download/storage/v1/b/\" \"buhkit/o/bzzz-fly.txt?alt=media\"\n        )\n        self.assertEqual(download_url, expected_url)\n\n    def test__get_download_url_on_the_fly_with_generation(self):\n        blob_name = \"pretend.txt\"\n        bucket = _Bucket(name=\"fictional\")\n        blob = self._make_one(blob_name, bucket=bucket)\n        generation = 1493058489532987\n        # Set the media link on the blob\n        blob._properties[\"generation\"] = str(generation)\n\n        self.assertIsNone(blob.media_link)\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(client)\n        expected_url = (\n            \"https://storage.googleapis.com/download/storage/v1/b/\"\n            \"fictional/o/pretend.txt?alt=media&generation=1493058489532987\"\n        )\n        self.assertEqual(download_url, expected_url)\n\n    def test__get_download_url_on_the_fly_with_user_project(self):\n        blob_name = \"pretend.txt\"\n        user_project = \"user-project-123\"\n        bucket = _Bucket(name=\"fictional\", user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        self.assertIsNone(blob.media_link)\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(client)\n        expected_url = (\n            \"https://storage.googleapis.com/download/storage/v1/b/\"\n            \"fictional/o/pretend.txt?alt=media&userProject={}\".format(user_project)\n        )\n        self.assertEqual(download_url, expected_url)\n\n    def test__get_download_url_on_the_fly_with_kms_key_name(self):\n        kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        blob_name = \"bzzz-fly.txt\"\n        bucket = _Bucket(name=\"buhkit\")\n        blob = self._make_one(blob_name, bucket=bucket, kms_key_name=kms_resource)\n\n        self.assertIsNone(blob.media_link)\n        client = mock.Mock(_connection=_Connection)\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        download_url = blob._get_download_url(client)\n        expected_url = (\n            \"https://storage.googleapis.com/download/storage/v1/b/\"\n            \"buhkit/o/bzzz-fly.txt?alt=media\"\n        )\n        self.assertEqual(download_url, expected_url)\n\n    @staticmethod\n    def _mock_requests_response(status_code, headers, content=b\"\"):\n        import requests\n\n        response = requests.Response()\n        response.status_code = status_code\n        response.headers.update(headers)\n        response.raw = None\n        response._content = content\n\n        response.request = requests.Request(\"POST\", \"http://example.com\").prepare()\n        return response\n\n    def test__extract_headers_from_download_gzipped(self):\n        blob_name = \"blob-name\"\n        client = mock.Mock(spec=[\"_http\"])\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        response = self._mock_requests_response(\n            http.client.OK,\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Content-Language\": \"ko-kr\",\n                \"Cache-Control\": \"max-age=1337;public\",\n                \"Content-Encoding\": \"gzip\",\n                \"Etag\": \"kittens\",\n                \"X-Goog-Storage-Class\": \"STANDARD\",\n                \"X-Goog-Hash\": \"crc32c=4gcgLQ==,md5=CS9tHYTtyFntzj7B9nkkJQ==\",\n                \"X-goog-generation\": 42,\n                \"X-goog-metageneration\": 4,\n            },\n            # { \"x\": 5 } gzipped\n            content=b\"\\x1f\\x8b\\x08\\x00\\xcfo\\x17_\\x02\\xff\\xabVP\\xaaP\\xb2R0U\\xa8\\x05\\x00\\xa1\\xcaQ\\x93\\n\\x00\\x00\\x00\",\n        )\n        blob._extract_headers_from_download(response)\n\n        self.assertEqual(blob.content_type, \"application/json\")\n        self.assertEqual(blob.content_language, \"ko-kr\")\n        self.assertEqual(blob.content_encoding, \"gzip\")\n        self.assertEqual(blob.cache_control, \"max-age=1337;public\")\n        self.assertEqual(blob.storage_class, \"STANDARD\")\n        self.assertEqual(blob.md5_hash, \"CS9tHYTtyFntzj7B9nkkJQ==\")\n        self.assertEqual(blob.crc32c, \"4gcgLQ==\")\n        self.assertEqual(blob.etag, \"kittens\")\n        self.assertEqual(blob.generation, 42)\n        self.assertEqual(blob.metageneration, 4)\n        self.assertEqual(blob._changes, set())\n\n    def test__extract_headers_from_download_empty(self):\n        blob_name = \"blob-name\"\n        client = mock.Mock(spec=[\"_http\"])\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        response = self._mock_requests_response(\n            http.client.OK,\n            headers={\n                \"Content-Type\": \"application/octet-stream\",\n                \"Content-Language\": \"en-US\",\n                \"Cache-Control\": \"max-age=1337;public\",\n                \"Content-Encoding\": \"gzip\",\n                \"Etag\": \"kittens\",\n                \"X-Goog-Storage-Class\": \"STANDARD\",\n                \"X-Goog-Hash\": \"crc32c=4/c+LQ==,md5=CS9tHYTt/+ntzj7B9nkkJQ==\",\n                \"X-goog-generation\": 42,\n                \"X-goog-metageneration\": 4,\n            },\n            content=b\"\",\n        )\n        blob._extract_headers_from_download(response)\n        self.assertEqual(blob.content_type, \"application/octet-stream\")\n        self.assertEqual(blob.content_language, \"en-US\")\n        self.assertEqual(blob.md5_hash, \"CS9tHYTt/+ntzj7B9nkkJQ==\")\n        self.assertEqual(blob.crc32c, \"4/c+LQ==\")\n        self.assertEqual(blob.etag, \"kittens\")\n        self.assertEqual(blob.generation, 42)\n        self.assertEqual(blob.metageneration, 4)\n        self.assertEqual(blob._changes, set())\n\n    def test__extract_headers_from_download_w_hash_response_header_none(self):\n        blob_name = \"blob-name\"\n        md5_hash = \"CS9tHYTtyFntzj7B9nkkJQ==\"\n        crc32c = \"4gcgLQ==\"\n        client = mock.Mock(spec=[\"_http\"])\n        bucket = _Bucket(client)\n        properties = {\n            \"md5Hash\": md5_hash,\n            \"crc32c\": crc32c,\n        }\n        blob = self._make_one(blob_name, bucket=bucket, properties=properties)\n\n        response = self._mock_requests_response(\n            http.client.OK,\n            headers={\"X-Goog-Hash\": \"\"},\n            # { \"x\": 5 } gzipped\n            content=b\"\\x1f\\x8b\\x08\\x00\\xcfo\\x17_\\x02\\xff\\xabVP\\xaaP\\xb2R0U\\xa8\\x05\\x00\\xa1\\xcaQ\\x93\\n\\x00\\x00\\x00\",\n        )\n        blob._extract_headers_from_download(response)\n\n        self.assertEqual(blob.md5_hash, md5_hash)\n        self.assertEqual(blob.crc32c, crc32c)\n\n    def test__extract_headers_from_download_w_response_headers_not_match(self):\n        blob_name = \"blob-name\"\n        client = mock.Mock(spec=[\"_http\"])\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        response = self._mock_requests_response(\n            http.client.OK,\n            headers={\"X-Goog-Hash\": \"bogus=4gcgLQ==,\"},\n            # { \"x\": 5 } gzipped\n            content=b\"\",\n        )\n        blob._extract_headers_from_download(response)\n\n        self.assertIsNone(blob.md5_hash)\n        self.assertIsNone(blob.crc32c)\n\n    def _do_download_helper_wo_chunks(\n        self, w_range, raw_download, timeout=None, **extra_kwargs\n    ):\n        blob_name = \"blob-name\"\n        client = mock.Mock()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        self.assertIsNone(blob.chunk_size)\n\n        transport = object()\n        file_obj = io.BytesIO()\n        download_url = \"http://test.invalid\"\n        headers = extra_kwargs.pop(\"headers\", {})\n\n        if raw_download:\n            patch = mock.patch(\"google.cloud.storage.blob.RawDownload\")\n        else:\n            patch = mock.patch(\"google.cloud.storage.blob.Download\")\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n\n        extra_kwargs.update(timeout_kwarg)\n\n        with patch as patched:\n            if w_range:\n                blob._do_download(\n                    transport,\n                    file_obj,\n                    download_url,\n                    headers,\n                    start=1,\n                    end=3,\n                    raw_download=raw_download,\n                    **extra_kwargs,\n                )\n            else:\n                blob._do_download(\n                    transport,\n                    file_obj,\n                    download_url,\n                    headers,\n                    raw_download=raw_download,\n                    **extra_kwargs,\n                )\n\n        if w_range:\n            patched.assert_called_once_with(\n                download_url,\n                stream=file_obj,\n                headers=headers,\n                start=1,\n                end=3,\n                checksum=\"md5\",\n            )\n        else:\n            patched.assert_called_once_with(\n                download_url,\n                stream=file_obj,\n                headers=headers,\n                start=None,\n                end=None,\n                checksum=\"md5\",\n            )\n\n        patched.return_value.consume.assert_called_once_with(\n            transport, timeout=expected_timeout\n        )\n\n        retry_strategy = patched.return_value._retry_strategy\n        retry = extra_kwargs.get(\"retry\", None)\n        if retry is None:\n            self.assertEqual(retry_strategy.max_retries, 0)\n        else:\n            self.assertEqual(retry_strategy.max_sleep, retry._maximum)\n\n    def test__do_download_wo_chunks_wo_range_wo_raw(self):\n        self._do_download_helper_wo_chunks(w_range=False, raw_download=False)\n\n    def test__do_download_wo_chunks_wo_range_wo_raw_w_headers(self):\n        self._do_download_helper_wo_chunks(\n            w_range=False, raw_download=False, headers={\"If-Match\": \"kittens\"}\n        )\n\n    def test__do_download_wo_chunks_wo_range_wo_raw_w_retry(self):\n        self._do_download_helper_wo_chunks(\n            w_range=False, raw_download=False, retry=DEFAULT_RETRY\n        )\n\n    def test__do_download_wo_chunks_wo_range_wo_raw_w_retry_w_headers(self):\n        self._do_download_helper_wo_chunks(\n            w_range=False,\n            raw_download=False,\n            retry=DEFAULT_RETRY,\n            headers={\"If-Match\": \"kittens\"},\n        )\n\n    def test__do_download_wo_chunks_w_range_wo_raw(self):\n        self._do_download_helper_wo_chunks(w_range=True, raw_download=False)\n\n    def test__do_download_wo_chunks_w_range_wo_raw_w_headers(self):\n        self._do_download_helper_wo_chunks(\n            w_range=True, raw_download=False, headers={\"If-Match\": \"kittens\"}\n        )\n\n    def test__do_download_wo_chunks_wo_range_w_raw(self):\n        self._do_download_helper_wo_chunks(w_range=False, raw_download=True)\n\n    def test__do_download_wo_chunks_wo_range_w_raw_w_headers(self):\n        self._do_download_helper_wo_chunks(\n            w_range=False, raw_download=True, headers={\"If-Match\": \"kittens\"}\n        )\n\n    def test__do_download_wo_chunks_w_range_w_raw(self):\n        self._do_download_helper_wo_chunks(w_range=True, raw_download=True)\n\n    def test__do_download_wo_chunks_w_range_w_raw_w_headers(self):\n        self._do_download_helper_wo_chunks(\n            w_range=True, raw_download=True, headers={\"If-Match\": \"kittens\"}\n        )\n\n    def test__do_download_wo_chunks_w_custom_timeout(self):\n        self._do_download_helper_wo_chunks(\n            w_range=False, raw_download=False, timeout=9.58\n        )\n\n    def _do_download_helper_w_chunks(\n        self, w_range, raw_download, timeout=None, checksum=\"md5\"\n    ):\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob._CHUNK_SIZE_MULTIPLE = 1\n        chunk_size = blob.chunk_size = 3\n\n        transport = object()\n        file_obj = io.BytesIO()\n        download_url = \"http://test.invalid\"\n        headers = {}\n\n        download = mock.Mock(finished=False, spec=[\"finished\", \"consume_next_chunk\"])\n\n        def side_effect(*args, **kwargs):\n            download.finished = True\n\n        download.consume_next_chunk.side_effect = side_effect\n\n        if raw_download:\n            patch = mock.patch(\"google.cloud.storage.blob.RawChunkedDownload\")\n        else:\n            patch = mock.patch(\"google.cloud.storage.blob.ChunkedDownload\")\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n\n        with patch as patched:\n            patched.return_value = download\n            if w_range:\n                blob._do_download(\n                    transport,\n                    file_obj,\n                    download_url,\n                    headers,\n                    start=1,\n                    end=3,\n                    raw_download=raw_download,\n                    checksum=checksum,\n                    **timeout_kwarg,\n                )\n            else:\n                blob._do_download(\n                    transport,\n                    file_obj,\n                    download_url,\n                    headers,\n                    raw_download=raw_download,\n                    checksum=checksum,\n                    **timeout_kwarg,\n                )\n\n        if w_range:\n            patched.assert_called_once_with(\n                download_url, chunk_size, file_obj, headers=headers, start=1, end=3\n            )\n        else:\n            patched.assert_called_once_with(\n                download_url, chunk_size, file_obj, headers=headers, start=0, end=None\n            )\n        download.consume_next_chunk.assert_called_once_with(\n            transport, timeout=expected_timeout\n        )\n\n    def test__do_download_w_chunks_wo_range_wo_raw(self):\n        self._do_download_helper_w_chunks(w_range=False, raw_download=False)\n\n    def test__do_download_w_chunks_w_range_wo_raw(self):\n        self._do_download_helper_w_chunks(w_range=True, raw_download=False)\n\n    def test__do_download_w_chunks_wo_range_w_raw(self):\n        self._do_download_helper_w_chunks(w_range=False, raw_download=True)\n\n    def test__do_download_w_chunks_w_range_w_raw(self):\n        self._do_download_helper_w_chunks(w_range=True, raw_download=True)\n\n    def test__do_download_w_chunks_w_custom_timeout(self):\n        self._do_download_helper_w_chunks(w_range=True, raw_download=True, timeout=9.58)\n\n    def test__do_download_w_chunks_w_checksum(self):\n        from google.cloud.storage import blob as blob_module\n\n        with mock.patch.object(blob_module._logger, \"info\") as patch:\n            self._do_download_helper_w_chunks(\n                w_range=False, raw_download=False, checksum=\"md5\"\n            )\n        patch.assert_called_once_with(\n            blob_module._CHUNKED_DOWNLOAD_CHECKSUM_MESSAGE.format(\"md5\")\n        )\n\n    def test__do_download_w_chunks_wo_checksum(self):\n        from google.cloud.storage import blob as blob_module\n\n        with mock.patch.object(blob_module._logger, \"info\") as patch:\n            self._do_download_helper_w_chunks(\n                w_range=False, raw_download=False, checksum=None\n            )\n        patch.assert_not_called()\n\n    def test_download_to_file_with_failure(self):\n        from google.cloud.exceptions import NotFound\n\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        file_obj = io.BytesIO()\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            blob._prep_and_do_download.side_effect = NotFound(\"testing\")\n\n            with self.assertRaises(NotFound):\n                blob.download_to_file(file_obj)\n\n            self.assertEqual(file_obj.tell(), 0)\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                file_obj,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n\n    def test_download_to_file_wo_media_link(self):\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        file_obj = io.BytesIO()\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            blob.download_to_file(file_obj)\n\n            # Make sure the media link is still unknown.\n            self.assertIsNone(blob.media_link)\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                file_obj,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n\n    def test_download_to_file_w_etag_match(self):\n        etag = \"kittens\"\n        client = self._make_client()\n        blob = self._make_one(\"blob-name\", bucket=_Bucket(client))\n        file_obj = io.BytesIO()\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            blob.download_to_file(file_obj, if_etag_not_match=etag)\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                file_obj,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=etag,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n\n    def test_download_to_file_w_generation_match(self):\n        generation_number = 6\n        client = self._make_client()\n        blob = self._make_one(\"blob-name\", bucket=_Bucket(client))\n        file_obj = io.BytesIO()\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            blob.download_to_file(file_obj, if_generation_not_match=generation_number)\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                file_obj,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=generation_number,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n\n    def _download_to_file_helper(\n        self, use_chunks, raw_download, timeout=None, **extra_kwargs\n    ):\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        media_link = \"http://example.com/media/\"\n        properties = {\"mediaLink\": media_link}\n        blob = self._make_one(blob_name, bucket=bucket, properties=properties)\n        if use_chunks:\n            blob._CHUNK_SIZE_MULTIPLE = 1\n            blob.chunk_size = 3\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n\n        extra_kwargs.update(timeout_kwarg)\n\n        file_obj = io.BytesIO()\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            if raw_download:\n                blob.download_to_file(file_obj, raw_download=True, **extra_kwargs)\n            else:\n                blob.download_to_file(file_obj, **extra_kwargs)\n\n            expected_retry = extra_kwargs.get(\"retry\", DEFAULT_RETRY)\n            blob._prep_and_do_download.assert_called_once_with(\n                file_obj,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=raw_download,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=expected_retry,\n            )\n\n    def test_download_to_file_wo_chunks_wo_raw(self):\n        self._download_to_file_helper(use_chunks=False, raw_download=False)\n\n    def test_download_to_file_wo_chunks_no_retry(self):\n        self._download_to_file_helper(use_chunks=False, raw_download=False, retry=None)\n\n    def test_download_to_file_w_chunks_wo_raw(self):\n        self._download_to_file_helper(use_chunks=True, raw_download=False)\n\n    def test_download_to_file_wo_chunks_w_raw(self):\n        self._download_to_file_helper(use_chunks=False, raw_download=True)\n\n    def test_download_to_file_w_chunks_w_raw(self):\n        self._download_to_file_helper(use_chunks=True, raw_download=True)\n\n    def test_download_to_file_w_custom_timeout(self):\n        self._download_to_file_helper(\n            use_chunks=False, raw_download=False, timeout=9.58\n        )\n\n    def _download_to_filename_helper(\n        self, updated, raw_download, timeout=None, **extra_kwargs\n    ):\n        import os\n        from google.cloud._testing import _NamedTemporaryFile\n\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        properties = {}\n        if updated is not None:\n            properties[\"updated\"] = updated\n\n        blob = self._make_one(blob_name, bucket=bucket, properties=properties)\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            with _NamedTemporaryFile() as temp:\n                if timeout is None:\n                    blob.download_to_filename(\n                        temp.name, raw_download=raw_download, **extra_kwargs\n                    )\n                else:\n                    blob.download_to_filename(\n                        temp.name,\n                        raw_download=raw_download,\n                        timeout=timeout,\n                        **extra_kwargs,\n                    )\n\n                if updated is None:\n                    self.assertIsNone(blob.updated)\n                else:\n                    mtime = os.path.getmtime(temp.name)\n                    updated_time = blob.updated.timestamp()\n                    self.assertEqual(mtime, updated_time)\n\n            expected_timeout = (\n                self._get_default_timeout() if timeout is None else timeout\n            )\n\n            expected_retry = extra_kwargs.get(\"retry\", DEFAULT_RETRY)\n\n            blob._prep_and_do_download.assert_called_once_with(\n                mock.ANY,\n                client=None,\n                start=None,\n                end=None,\n                raw_download=raw_download,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=expected_retry,\n            )\n            stream = blob._prep_and_do_download.mock_calls[0].args[0]\n            self.assertEqual(stream.name, temp.name)\n\n    def test_download_to_filename_w_updated_wo_raw(self):\n        updated = \"2014-12-06T13:13:50.690Z\"\n        self._download_to_filename_helper(updated=updated, raw_download=False)\n\n    def test_download_to_filename_w_updated_no_retry(self):\n        updated = \"2014-12-06T13:13:50.690Z\"\n        self._download_to_filename_helper(\n            updated=updated, raw_download=False, retry=None\n        )\n\n    def test_download_to_filename_wo_updated_wo_raw(self):\n        self._download_to_filename_helper(updated=None, raw_download=False)\n\n    def test_download_to_filename_w_updated_w_raw(self):\n        updated = \"2014-12-06T13:13:50.690Z\"\n        self._download_to_filename_helper(updated=updated, raw_download=True)\n\n    def test_download_to_filename_wo_updated_w_raw(self):\n        self._download_to_filename_helper(updated=None, raw_download=True)\n\n    def test_download_to_filename_w_custom_timeout(self):\n        self._download_to_filename_helper(\n            updated=None, raw_download=False, timeout=9.58\n        )\n\n    def test_download_to_filename_w_etag_match(self):\n        from google.cloud._testing import _NamedTemporaryFile\n\n        etag = \"kittens\"\n        client = self._make_client()\n        blob = self._make_one(\"blob-name\", bucket=_Bucket(client))\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            with _NamedTemporaryFile() as temp:\n                blob.download_to_filename(temp.name, if_etag_match=etag)\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                mock.ANY,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=etag,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n            stream = blob._prep_and_do_download.mock_calls[0].args[0]\n            self.assertEqual(stream.name, temp.name)\n\n    def test_download_to_filename_w_generation_match(self):\n        from google.cloud._testing import _NamedTemporaryFile\n\n        generation_number = 6\n        client = self._make_client()\n        blob = self._make_one(\"blob-name\", bucket=_Bucket(client))\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            with _NamedTemporaryFile() as temp:\n                blob.download_to_filename(\n                    temp.name, if_generation_match=generation_number\n                )\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                mock.ANY,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=generation_number,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n            stream = blob._prep_and_do_download.mock_calls[0].args[0]\n            self.assertEqual(stream.name, temp.name)\n\n    def test_download_to_filename_corrupted(self):\n        from google.resumable_media import DataCorruption\n\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            blob._prep_and_do_download.side_effect = DataCorruption(\"testing\")\n\n            # Try to download into a temporary file (don't use\n            # `_NamedTemporaryFile` it will try to remove after the file is\n            # already removed)\n            filehandle, filename = tempfile.mkstemp()\n            os.close(filehandle)\n            self.assertTrue(os.path.exists(filename))\n\n            with self.assertRaises(DataCorruption):\n                blob.download_to_filename(filename)\n\n            # Make sure the file was cleaned up.\n            self.assertFalse(os.path.exists(filename))\n\n            expected_timeout = self._get_default_timeout()\n            blob._prep_and_do_download.assert_called_once_with(\n                mock.ANY,\n                client=None,\n                start=None,\n                end=None,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                raw_download=False,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=DEFAULT_RETRY,\n            )\n            stream = blob._prep_and_do_download.mock_calls[0].args[0]\n            self.assertEqual(stream.name, filename)\n\n    def _download_as_bytes_helper(self, raw_download, timeout=None, **extra_kwargs):\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        with mock.patch.object(blob, \"_prep_and_do_download\"):\n            if timeout is None:\n                expected_timeout = self._get_default_timeout()\n                fetched = blob.download_as_bytes(\n                    raw_download=raw_download, **extra_kwargs\n                )\n            else:\n                expected_timeout = timeout\n                fetched = blob.download_as_bytes(\n                    raw_download=raw_download, timeout=timeout, **extra_kwargs\n                )\n            self.assertEqual(fetched, b\"\")\n\n            expected_retry = extra_kwargs.get(\"retry\", DEFAULT_RETRY)\n\n            blob._prep_and_do_download.assert_called_once_with(\n                mock.ANY,\n                client=None,\n                start=None,\n                end=None,\n                raw_download=raw_download,\n                if_etag_match=None,\n                if_etag_not_match=None,\n                if_generation_match=None,\n                if_generation_not_match=None,\n                if_metageneration_match=None,\n                if_metageneration_not_match=None,\n                timeout=expected_timeout,\n                checksum=\"md5\",\n                retry=expected_retry,\n            )\n            stream = blob._prep_and_do_download.mock_calls[0].args[0]\n            self.assertIsInstance(stream, io.BytesIO)\n\n    def test_download_as_bytes_w_custom_timeout(self):\n        self._download_as_bytes_helper(raw_download=False, timeout=9.58)\n\n    def test_download_as_bytes_w_etag_match(self):\n        ETAG = \"kittens\"\n        MEDIA_LINK = \"http://example.com/media/\"\n\n        client = self._make_client()\n        blob = self._make_one(\n            \"blob-name\", bucket=_Bucket(client), properties={\"mediaLink\": MEDIA_LINK}\n        )\n        blob._prep_and_do_download = mock.Mock()\n\n        fetched = blob.download_as_bytes(if_etag_match=ETAG)\n        self.assertEqual(fetched, b\"\")\n\n        blob._prep_and_do_download.assert_called_once_with(\n            mock.ANY,\n            client=None,\n            start=None,\n            end=None,\n            raw_download=False,\n            if_etag_match=ETAG,\n            if_etag_not_match=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            checksum=\"md5\",\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_download_as_bytes_w_generation_match(self):\n        GENERATION_NUMBER = 6\n        MEDIA_LINK = \"http://example.com/media/\"\n\n        client = self._make_client()\n        blob = self._make_one(\n            \"blob-name\", bucket=_Bucket(client), properties={\"mediaLink\": MEDIA_LINK}\n        )\n        blob._prep_and_do_download = mock.Mock()\n\n        fetched = blob.download_as_bytes(if_generation_match=GENERATION_NUMBER)\n        self.assertEqual(fetched, b\"\")\n\n        blob._prep_and_do_download.assert_called_once_with(\n            mock.ANY,\n            client=None,\n            start=None,\n            end=None,\n            raw_download=False,\n            if_etag_match=None,\n            if_etag_not_match=None,\n            if_generation_match=GENERATION_NUMBER,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            checksum=\"md5\",\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_download_as_bytes_wo_raw(self):\n        self._download_as_bytes_helper(raw_download=False)\n\n    def test_download_as_bytes_no_retry(self):\n        self._download_as_bytes_helper(raw_download=False, retry=None)\n\n    def test_download_as_bytes_w_raw(self):\n        self._download_as_bytes_helper(raw_download=True)\n\n    def test_download_as_byte_w_custom_timeout(self):\n        self._download_as_bytes_helper(raw_download=False, timeout=9.58)\n\n    def _download_as_text_helper(\n        self,\n        raw_download,\n        client=None,\n        start=None,\n        end=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=None,\n        encoding=None,\n        charset=None,\n        no_charset=False,\n        expected_value=\"DEADBEEF\",\n        payload=None,\n        **extra_kwargs,\n    ):\n        if payload is None:\n            if encoding is not None:\n                payload = expected_value.encode(encoding)\n            else:\n                payload = expected_value.encode()\n\n        blob_name = \"blob-name\"\n        bucket_client = self._make_client()\n        bucket = _Bucket(bucket_client)\n\n        properties = {}\n        if charset is not None:\n            properties[\"contentType\"] = f\"text/plain; charset={charset}\"\n        elif no_charset:\n            properties = {\"contentType\": \"text/plain\"}\n\n        blob = self._make_one(blob_name, bucket=bucket, properties=properties)\n        blob.download_as_bytes = mock.Mock(return_value=payload)\n\n        kwargs = {\"raw_download\": raw_download}\n\n        if client is not None:\n            kwargs[\"client\"] = client\n\n        if start is not None:\n            kwargs[\"start\"] = start\n\n        if end is not None:\n            kwargs[\"end\"] = end\n\n        if encoding is not None:\n            kwargs[\"encoding\"] = encoding\n\n        if if_etag_match is not None:\n            kwargs[\"if_etag_match\"] = if_etag_match\n\n        if if_etag_not_match is not None:\n            kwargs[\"if_etag_not_match\"] = if_etag_not_match\n\n        if if_generation_match is not None:\n            kwargs[\"if_generation_match\"] = if_generation_match\n\n        if if_generation_not_match is not None:\n            kwargs[\"if_generation_not_match\"] = if_generation_not_match\n\n        if if_metageneration_match is not None:\n            kwargs[\"if_metageneration_match\"] = if_metageneration_match\n\n        if if_metageneration_not_match is not None:\n            kwargs[\"if_metageneration_not_match\"] = if_metageneration_not_match\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n        else:\n            kwargs[\"timeout\"] = expected_timeout = timeout\n\n        kwargs.update(extra_kwargs)\n\n        fetched = blob.download_as_text(**kwargs)\n\n        self.assertEqual(fetched, expected_value)\n\n        expected_retry = extra_kwargs.get(\"retry\", DEFAULT_RETRY)\n\n        blob.download_as_bytes.assert_called_once_with(\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            timeout=expected_timeout,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=expected_retry,\n        )\n\n    def test_download_as_text_wo_raw(self):\n        self._download_as_text_helper(raw_download=False)\n\n    def test_download_as_text_w_no_retry(self):\n        self._download_as_text_helper(raw_download=False, retry=None)\n\n    def test_download_as_text_w_raw(self):\n        self._download_as_text_helper(raw_download=True)\n\n    def test_download_as_text_w_client(self):\n        self._download_as_text_helper(raw_download=False, client=object())\n\n    def test_download_as_text_w_start(self):\n        self._download_as_text_helper(raw_download=False, start=123)\n\n    def test_download_as_text_w_end(self):\n        self._download_as_text_helper(raw_download=False, end=456)\n\n    def test_download_as_text_w_custom_timeout(self):\n        self._download_as_text_helper(raw_download=False, timeout=9.58)\n\n    def test_download_as_text_w_if_etag_match_str(self):\n        self._download_as_text_helper(\n            raw_download=False,\n            if_etag_match=\"kittens\",\n        )\n\n    def test_download_as_text_w_if_etag_match_list(self):\n        self._download_as_text_helper(\n            raw_download=False,\n            if_etag_match=[\"kittens\", \"fluffy\"],\n        )\n\n    def test_download_as_text_w_if_etag_not_match_str(self):\n        self._download_as_text_helper(\n            raw_download=False,\n            if_etag_not_match=\"kittens\",\n        )\n\n    def test_download_as_text_w_if_etag_not_match_list(self):\n        self._download_as_text_helper(\n            raw_download=False,\n            if_etag_not_match=[\"kittens\", \"fluffy\"],\n        )\n\n    def test_download_as_text_w_if_generation_match(self):\n        self._download_as_text_helper(raw_download=False, if_generation_match=6)\n\n    def test_download_as_text_w_if_generation_not_match(self):\n        self._download_as_text_helper(raw_download=False, if_generation_not_match=6)\n\n    def test_download_as_text_w_if_metageneration_match(self):\n        self._download_as_text_helper(raw_download=False, if_metageneration_match=6)\n\n    def test_download_as_text_w_if_metageneration_not_match(self):\n        self._download_as_text_helper(raw_download=False, if_metageneration_not_match=6)\n\n    def test_download_as_text_w_encoding(self):\n        encoding = \"utf-16\"\n        self._download_as_text_helper(\n            raw_download=False,\n            encoding=encoding,\n        )\n\n    def test_download_as_text_w_no_charset(self):\n        self._download_as_text_helper(\n            raw_download=False,\n            no_charset=True,\n        )\n\n    def test_download_as_text_w_non_ascii_w_explicit_encoding(self):\n        expected_value = \"\\x0AFe\"\n        encoding = \"utf-16\"\n        charset = \"latin1\"\n        payload = expected_value.encode(encoding)\n        self._download_as_text_helper(\n            raw_download=False,\n            expected_value=expected_value,\n            payload=payload,\n            encoding=encoding,\n            charset=charset,\n        )\n\n    def test_download_as_text_w_non_ascii_wo_explicit_encoding_w_charset(self):\n        expected_value = \"\\x0AFe\"\n        charset = \"utf-16\"\n        payload = expected_value.encode(charset)\n        self._download_as_text_helper(\n            raw_download=False,\n            expected_value=expected_value,\n            payload=payload,\n            charset=charset,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_download_as_string(self, mock_warn):\n        from google.cloud.storage.blob import _DOWNLOAD_AS_STRING_DEPRECATED\n\n        MEDIA_LINK = \"http://example.com/media/\"\n\n        client = self._make_client()\n        blob = self._make_one(\n            \"blob-name\", bucket=_Bucket(client), properties={\"mediaLink\": MEDIA_LINK}\n        )\n        blob._prep_and_do_download = mock.Mock()\n\n        fetched = blob.download_as_string()\n        self.assertEqual(fetched, b\"\")\n\n        blob._prep_and_do_download.assert_called_once_with(\n            mock.ANY,\n            client=None,\n            start=None,\n            end=None,\n            raw_download=False,\n            if_etag_match=None,\n            if_etag_not_match=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            checksum=\"md5\",\n            retry=DEFAULT_RETRY,\n        )\n\n        mock_warn.assert_called_once_with(\n            _DOWNLOAD_AS_STRING_DEPRECATED,\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_download_as_string_no_retry(self, mock_warn):\n        from google.cloud.storage.blob import _DOWNLOAD_AS_STRING_DEPRECATED\n\n        MEDIA_LINK = \"http://example.com/media/\"\n\n        client = self._make_client()\n        blob = self._make_one(\n            \"blob-name\", bucket=_Bucket(client), properties={\"mediaLink\": MEDIA_LINK}\n        )\n        blob._prep_and_do_download = mock.Mock()\n\n        fetched = blob.download_as_string(retry=None)\n        self.assertEqual(fetched, b\"\")\n\n        blob._prep_and_do_download.assert_called_once_with(\n            mock.ANY,\n            client=None,\n            start=None,\n            end=None,\n            raw_download=False,\n            if_etag_match=None,\n            if_etag_not_match=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            checksum=\"md5\",\n            retry=None,\n        )\n\n        mock_warn.assert_called_once_with(\n            _DOWNLOAD_AS_STRING_DEPRECATED,\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n\n    def test__get_content_type_explicit(self):\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        content_type = \"text/plain\"\n        return_value = blob._get_content_type(content_type)\n        self.assertEqual(return_value, content_type)\n\n    def test__get_content_type_from_blob(self):\n        blob = self._make_one(\"blob-name\", bucket=None)\n        blob.content_type = \"video/mp4\"\n\n        return_value = blob._get_content_type(None)\n        self.assertEqual(return_value, blob.content_type)\n\n    def test__get_content_type_from_filename(self):\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        return_value = blob._get_content_type(None, filename=\"archive.tar\")\n        self.assertEqual(return_value, \"application/x-tar\")\n\n    def test__get_content_type_default(self):\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        return_value = blob._get_content_type(None)\n        self.assertEqual(return_value, \"application/octet-stream\")\n\n    def test__get_writable_metadata_no_changes(self):\n        name = \"blob-name\"\n        blob = self._make_one(name, bucket=None)\n\n        object_metadata = blob._get_writable_metadata()\n        expected = {\"name\": name}\n        self.assertEqual(object_metadata, expected)\n\n    def test__get_writable_metadata_with_changes(self):\n        name = \"blob-name\"\n        blob = self._make_one(name, bucket=None)\n        blob.storage_class = \"NEARLINE\"\n        blob.cache_control = \"max-age=3600\"\n        blob.metadata = {\"color\": \"red\"}\n\n        object_metadata = blob._get_writable_metadata()\n        expected = {\n            \"cacheControl\": blob.cache_control,\n            \"metadata\": blob.metadata,\n            \"name\": name,\n            \"storageClass\": blob.storage_class,\n        }\n        self.assertEqual(object_metadata, expected)\n\n    def test__get_writable_metadata_unwritable_field(self):\n        name = \"blob-name\"\n        properties = {\"updated\": \"2016-10-16T18:18:18.181Z\"}\n        blob = self._make_one(name, bucket=None, properties=properties)\n        # Fake that `updated` is in changes.\n        blob._changes.add(\"updated\")\n\n        object_metadata = blob._get_writable_metadata()\n        expected = {\"name\": name}\n        self.assertEqual(object_metadata, expected)\n\n    def test__set_metadata_to_none(self):\n        name = \"blob-name\"\n        blob = self._make_one(name, bucket=None)\n        blob.storage_class = \"NEARLINE\"\n        blob.cache_control = \"max-age=3600\"\n\n        with mock.patch(\"google.cloud.storage.blob.Blob._patch_property\") as patch_prop:\n            blob.metadata = None\n            patch_prop.assert_called_once_with(\"metadata\", None)\n\n    def test__get_upload_arguments(self):\n        name = \"blob-name\"\n        key = b\"[pXw@,p@@AfBfrR3x-2b2SCHR,.?YwRO\"\n        custom_headers = {\n            \"x-goog-custom-audit-foo\": \"bar\",\n            \"x-goog-custom-audit-user\": \"baz\",\n        }\n        client = mock.Mock(_connection=_Connection)\n        client._connection.user_agent = \"testing 1.2.3\"\n        client._extra_headers = custom_headers\n        blob = self._make_one(name, bucket=None, encryption_key=key)\n        blob.content_disposition = \"inline\"\n\n        COMMAND = \"tm.upload_many\"\n        content_type = \"image/jpeg\"\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            info = blob._get_upload_arguments(client, content_type, command=COMMAND)\n\n        headers, object_metadata, new_content_type = info\n        header_key_value = \"W3BYd0AscEBAQWZCZnJSM3gtMmIyU0NIUiwuP1l3Uk8=\"\n        header_key_hash_value = \"G0++dxF4q5rG4o9kE8gvEKn15RH6wLm0wXV1MgAlXOg=\"\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            expected_headers = {\n                **_get_default_headers(\n                    client._connection.user_agent, content_type, command=COMMAND\n                ),\n                \"X-Goog-Encryption-Algorithm\": \"AES256\",\n                \"X-Goog-Encryption-Key\": header_key_value,\n                \"X-Goog-Encryption-Key-Sha256\": header_key_hash_value,\n                **custom_headers,\n            }\n        self.assertEqual(\n            headers[\"X-Goog-API-Client\"],\n            f\"{client._connection.user_agent} {GCCL_INVOCATION_TEST_CONST} gccl-gcs-cmd/{COMMAND}\",\n        )\n        self.assertEqual(headers, expected_headers)\n        expected_metadata = {\n            \"contentDisposition\": blob.content_disposition,\n            \"name\": name,\n        }\n        self.assertEqual(object_metadata, expected_metadata)\n        self.assertEqual(new_content_type, content_type)\n\n    def _mock_transport(self, status_code, headers, content=b\"\"):\n        fake_transport = mock.Mock(spec=[\"request\"])\n        fake_response = self._mock_requests_response(\n            status_code, headers, content=content\n        )\n        fake_transport.request.return_value = fake_response\n        return fake_transport\n\n    def _do_multipart_success(\n        self,\n        mock_get_boundary,\n        client=None,\n        size=None,\n        num_retries=None,\n        user_project=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        kms_key_name=None,\n        timeout=None,\n        metadata=None,\n        mtls=False,\n        retry=None,\n    ):\n        bucket = _Bucket(name=\"w00t\", user_project=user_project)\n        blob = self._make_one(\"blob-name\", bucket=bucket, kms_key_name=kms_key_name)\n        self.assertIsNone(blob.chunk_size)\n        if metadata:\n            self.assertIsNone(blob.metadata)\n            blob._properties[\"metadata\"] = metadata\n            self.assertEqual(len(blob._changes), 0)\n\n        # Create some mock arguments.\n        if not client:\n            # Create mocks to be checked for doing transport.\n            transport = self._mock_transport(http.client.OK, {})\n\n            client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n            client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n            client._extra_headers = {}\n\n        # Mock get_api_base_url_for_mtls function.\n        mtls_url = \"https://foo.mtls\"\n        if mtls:\n            client._connection.get_api_base_url_for_mtls = mock.Mock(\n                return_value=mtls_url\n            )\n\n        data = b\"data here hear hier\"\n        stream = io.BytesIO(data)\n        content_type = \"application/xml\"\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            response = blob._do_multipart_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                retry=retry,\n                **timeout_kwarg,\n            )\n\n        # Clean up the get_api_base_url_for_mtls mock.\n        if mtls:\n            del client._connection.get_api_base_url_for_mtls\n\n        # Check the mocks and the returned value.\n        self.assertIs(response, client._http.request.return_value)\n        if size is None:\n            data_read = data\n            self.assertEqual(stream.tell(), len(data))\n        else:\n            data_read = data[:size]\n            self.assertEqual(stream.tell(), size)\n\n        mock_get_boundary.assert_called_once_with()\n\n        upload_url = (\n            \"https://storage.googleapis.com/upload/storage/v1\" + bucket.path + \"/o\"\n        )\n        if mtls:\n            upload_url = mtls_url + \"/upload/storage/v1\" + bucket.path + \"/o\"\n\n        qs_params = [(\"uploadType\", \"multipart\")]\n\n        if user_project is not None:\n            qs_params.append((\"userProject\", user_project))\n\n        if predefined_acl is not None:\n            qs_params.append((\"predefinedAcl\", predefined_acl))\n\n        if kms_key_name is not None and \"cryptoKeyVersions\" not in kms_key_name:\n            qs_params.append((\"kmsKeyName\", kms_key_name))\n\n        if if_generation_match is not None:\n            qs_params.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            qs_params.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            qs_params.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            qs_params.append((\"ifMetaGenerationNotMatch\", if_metageneration_not_match))\n\n        upload_url += \"?\" + urlencode(qs_params)\n\n        blob_data = {\"name\": \"blob-name\"}\n        if metadata:\n            blob_data[\"metadata\"] = metadata\n            self.assertEqual(blob._changes, set([\"metadata\"]))\n        payload = (\n            b\"--==0==\\r\\n\"\n            + b\"content-type: application/json; charset=UTF-8\\r\\n\\r\\n\"\n            + json.dumps(blob_data).encode(\"utf-8\")\n            + b\"\\r\\n--==0==\\r\\n\"\n            + b\"content-type: application/xml\\r\\n\\r\\n\"\n            + data_read\n            + b\"\\r\\n--==0==--\"\n        )\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            headers = {\n                **_get_default_headers(\n                    client._connection.user_agent,\n                    b'multipart/related; boundary=\"==0==\"',\n                    \"application/xml\",\n                ),\n                **client._extra_headers,\n            }\n        client._http.request.assert_called_once_with(\n            \"POST\", upload_url, data=payload, headers=headers, timeout=expected_timeout\n        )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_no_size(self, mock_get_boundary):\n        self._do_multipart_success(mock_get_boundary, predefined_acl=\"private\")\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_no_size_retry(self, mock_get_boundary):\n        self._do_multipart_success(\n            mock_get_boundary, predefined_acl=\"private\", retry=DEFAULT_RETRY\n        )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_no_size_num_retries(self, mock_get_boundary):\n        self._do_multipart_success(\n            mock_get_boundary, predefined_acl=\"private\", num_retries=2\n        )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_no_size_retry_conflict(self, mock_get_boundary):\n        with self.assertRaises(ValueError):\n            self._do_multipart_success(\n                mock_get_boundary,\n                predefined_acl=\"private\",\n                num_retries=2,\n                retry=DEFAULT_RETRY,\n            )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_no_size_mtls(self, mock_get_boundary):\n        self._do_multipart_success(\n            mock_get_boundary, predefined_acl=\"private\", mtls=True\n        )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_size(self, mock_get_boundary):\n        self._do_multipart_success(mock_get_boundary, size=10)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_user_project(self, mock_get_boundary):\n        user_project = \"user-project-123\"\n        self._do_multipart_success(mock_get_boundary, user_project=user_project)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_kms(self, mock_get_boundary):\n        kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        self._do_multipart_success(mock_get_boundary, kms_key_name=kms_resource)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_kms_with_version(self, mock_get_boundary):\n        kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n            \"cryptoKeyVersions/1\"\n        )\n        self._do_multipart_success(mock_get_boundary, kms_key_name=kms_resource)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_retry(self, mock_get_boundary):\n        self._do_multipart_success(mock_get_boundary, retry=DEFAULT_RETRY)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_generation_match(self, mock_get_boundary):\n        self._do_multipart_success(\n            mock_get_boundary, if_generation_match=4, if_metageneration_match=4\n        )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_custom_timeout(self, mock_get_boundary):\n        self._do_multipart_success(mock_get_boundary, timeout=9.58)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_generation_not_match(self, mock_get_boundary):\n        self._do_multipart_success(\n            mock_get_boundary, if_generation_not_match=4, if_metageneration_not_match=4\n        )\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_client(self, mock_get_boundary):\n        transport = self._mock_transport(http.client.OK, {})\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._extra_headers = {}\n        self._do_multipart_success(mock_get_boundary, client=client)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_client_custom_headers(self, mock_get_boundary):\n        custom_headers = {\n            \"x-goog-custom-audit-foo\": \"bar\",\n            \"x-goog-custom-audit-user\": \"baz\",\n        }\n        transport = self._mock_transport(http.client.OK, {})\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._extra_headers = custom_headers\n        self._do_multipart_success(mock_get_boundary, client=client)\n\n    @mock.patch(\"google.resumable_media._upload.get_boundary\", return_value=b\"==0==\")\n    def test__do_multipart_upload_with_metadata(self, mock_get_boundary):\n        self._do_multipart_success(mock_get_boundary, metadata={\"test\": \"test\"})\n\n    def test__do_multipart_upload_bad_size(self):\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        data = b\"data here hear hier\"\n        stream = io.BytesIO(data)\n        size = 50\n        self.assertGreater(size, len(data))\n\n        with self.assertRaises(ValueError) as exc_info:\n            blob._do_multipart_upload(\n                None, stream, None, size, None, None, None, None, None, None\n            )\n\n        exc_contents = str(exc_info.exception)\n        self.assertIn(\"was specified but the file-like object only had\", exc_contents)\n        self.assertEqual(stream.tell(), len(data))\n\n    def _initiate_resumable_helper(\n        self,\n        client=None,\n        size=None,\n        extra_headers=None,\n        chunk_size=None,\n        num_retries=None,\n        user_project=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        blob_chunk_size=786432,\n        kms_key_name=None,\n        timeout=None,\n        metadata=None,\n        mtls=False,\n        retry=None,\n    ):\n        from google.resumable_media.requests import ResumableUpload\n        from google.cloud.storage.blob import _DEFAULT_CHUNKSIZE\n\n        bucket = _Bucket(name=\"whammy\", user_project=user_project)\n        blob = self._make_one(\"blob-name\", bucket=bucket, kms_key_name=kms_key_name)\n        if metadata:\n            self.assertIsNone(blob.metadata)\n            blob._properties[\"metadata\"] = metadata\n            self.assertEqual(len(blob._changes), 0)\n        else:\n            blob.metadata = {\"rook\": \"takes knight\"}\n        blob.chunk_size = blob_chunk_size\n        if blob_chunk_size is not None:\n            self.assertIsNotNone(blob.chunk_size)\n        else:\n            self.assertIsNone(blob.chunk_size)\n\n        # Need to make sure **same** dict is used because ``json.dumps()``\n        # will depend on the hash order.\n        if not metadata:\n            object_metadata = blob._get_writable_metadata()\n            blob._get_writable_metadata = mock.Mock(\n                return_value=object_metadata, spec=[]\n            )\n\n        resumable_url = \"http://test.invalid?upload_id=hey-you\"\n        if not client:\n            # Create mocks to be checked for doing transport.\n            response_headers = {\"location\": resumable_url}\n            transport = self._mock_transport(http.client.OK, response_headers)\n\n            # Create some mock arguments and call the method under test.\n            client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n            client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n            client._extra_headers = {}\n\n        # Mock get_api_base_url_for_mtls function.\n        mtls_url = \"https://foo.mtls\"\n        if mtls:\n            client._connection.get_api_base_url_for_mtls = mock.Mock(\n                return_value=mtls_url\n            )\n\n        data = b\"hello hallo halo hi-low\"\n        stream = io.BytesIO(data)\n        content_type = \"text/plain\"\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            upload, transport = blob._initiate_resumable_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                extra_headers=extra_headers,\n                chunk_size=chunk_size,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n                **timeout_kwarg,\n            )\n\n        # Clean up the get_api_base_url_for_mtls mock.\n        if mtls:\n            del client._connection.get_api_base_url_for_mtls\n\n        # Check the returned values.\n        self.assertIsInstance(upload, ResumableUpload)\n\n        upload_url = (\n            \"https://storage.googleapis.com/upload/storage/v1\" + bucket.path + \"/o\"\n        )\n        if mtls:\n            upload_url = mtls_url + \"/upload/storage/v1\" + bucket.path + \"/o\"\n        qs_params = [(\"uploadType\", \"resumable\")]\n\n        if user_project is not None:\n            qs_params.append((\"userProject\", user_project))\n\n        if predefined_acl is not None:\n            qs_params.append((\"predefinedAcl\", predefined_acl))\n\n        if kms_key_name is not None and \"cryptoKeyVersions\" not in kms_key_name:\n            qs_params.append((\"kmsKeyName\", kms_key_name))\n\n        if if_generation_match is not None:\n            qs_params.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            qs_params.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            qs_params.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            qs_params.append((\"ifMetaGenerationNotMatch\", if_metageneration_not_match))\n\n        upload_url += \"?\" + urlencode(qs_params)\n\n        self.assertEqual(upload.upload_url, upload_url)\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            if extra_headers is None:\n                expected_headers = {\n                    **_get_default_headers(client._connection.user_agent, content_type),\n                    **client._extra_headers,\n                }\n                self.assertEqual(upload._headers, expected_headers)\n            else:\n                expected_headers = {\n                    **_get_default_headers(client._connection.user_agent, content_type),\n                    **client._extra_headers,\n                    **extra_headers,\n                }\n                self.assertEqual(upload._headers, expected_headers)\n                self.assertIsNot(upload._headers, expected_headers)\n        self.assertFalse(upload.finished)\n        if chunk_size is None:\n            if blob_chunk_size is None:\n                self.assertEqual(upload._chunk_size, _DEFAULT_CHUNKSIZE)\n            else:\n                self.assertEqual(upload._chunk_size, blob.chunk_size)\n        else:\n            self.assertNotEqual(blob.chunk_size, chunk_size)\n            self.assertEqual(upload._chunk_size, chunk_size)\n        self.assertIs(upload._stream, stream)\n        if metadata:\n            self.assertEqual(blob._changes, set([\"metadata\"]))\n        if size is None:\n            self.assertIsNone(upload._total_bytes)\n        else:\n            self.assertEqual(upload._total_bytes, size)\n        self.assertEqual(upload._content_type, content_type)\n        self.assertEqual(upload.resumable_url, resumable_url)\n        retry_strategy = upload._retry_strategy\n        self.assertFalse(num_retries is not None and retry is not None)\n        if num_retries is not None and retry is None:\n            self.assertEqual(retry_strategy.max_retries, num_retries)\n        elif retry is None:\n            self.assertEqual(retry_strategy.max_retries, 0)\n        else:\n            self.assertEqual(retry_strategy.max_sleep, 60.0)\n            self.assertEqual(retry_strategy.max_cumulative_retry, 120.0)\n            self.assertIsNone(retry_strategy.max_retries)\n        self.assertIs(client._http, transport)\n        # Make sure we never read from the stream.\n        self.assertEqual(stream.tell(), 0)\n\n        if metadata:\n            object_metadata = {\"name\": \"blob-name\", \"metadata\": metadata}\n        else:\n            # Check the mocks.\n            blob._get_writable_metadata.assert_called_once_with()\n        payload = json.dumps(object_metadata).encode(\"utf-8\")\n\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            expected_headers = {\n                **_get_default_headers(\n                    client._connection.user_agent, x_upload_content_type=content_type\n                ),\n                **client._extra_headers,\n            }\n        if size is not None:\n            expected_headers[\"x-upload-content-length\"] = str(size)\n        if extra_headers is not None:\n            expected_headers.update(extra_headers)\n        transport.request.assert_called_once_with(\n            \"POST\",\n            upload_url,\n            data=payload,\n            headers=expected_headers,\n            timeout=expected_timeout,\n        )\n\n    def test__initiate_resumable_upload_with_metadata(self):\n        self._initiate_resumable_helper(metadata={\"test\": \"test\"})\n\n    def test__initiate_resumable_upload_with_custom_timeout(self):\n        self._initiate_resumable_helper(timeout=9.58)\n\n    def test__initiate_resumable_upload_no_size(self):\n        self._initiate_resumable_helper()\n\n    def test__initiate_resumable_upload_no_size_mtls(self):\n        self._initiate_resumable_helper(mtls=True)\n\n    def test__initiate_resumable_upload_with_size(self):\n        self._initiate_resumable_helper(size=10000)\n\n    def test__initiate_resumable_upload_with_user_project(self):\n        user_project = \"user-project-123\"\n        self._initiate_resumable_helper(user_project=user_project)\n\n    def test__initiate_resumable_upload_with_kms(self):\n        kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        self._initiate_resumable_helper(kms_key_name=kms_resource)\n\n    def test__initiate_resumable_upload_with_kms_with_version(self):\n        kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n            \"cryptoKeyVersions/1\"\n        )\n        self._initiate_resumable_helper(kms_key_name=kms_resource)\n\n    def test__initiate_resumable_upload_without_chunk_size(self):\n        self._initiate_resumable_helper(blob_chunk_size=None)\n\n    def test__initiate_resumable_upload_with_chunk_size(self):\n        one_mb = 1048576\n        self._initiate_resumable_helper(chunk_size=one_mb)\n\n    def test__initiate_resumable_upload_with_extra_headers(self):\n        extra_headers = {\"origin\": \"http://not-in-kansas-anymore.invalid\"}\n        self._initiate_resumable_helper(extra_headers=extra_headers)\n\n    def test__initiate_resumable_upload_with_retry(self):\n        self._initiate_resumable_helper(retry=DEFAULT_RETRY)\n\n    def test__initiate_resumable_upload_w_num_retries(self):\n        self._initiate_resumable_helper(num_retries=11)\n\n    def test__initiate_resumable_upload_with_retry_conflict(self):\n        with self.assertRaises(ValueError):\n            self._initiate_resumable_helper(retry=DEFAULT_RETRY, num_retries=2)\n\n    def test__initiate_resumable_upload_with_generation_match(self):\n        self._initiate_resumable_helper(\n            if_generation_match=4, if_metageneration_match=4\n        )\n\n    def test__initiate_resumable_upload_with_generation_not_match(self):\n        self._initiate_resumable_helper(\n            if_generation_not_match=4, if_metageneration_not_match=4\n        )\n\n    def test__initiate_resumable_upload_with_predefined_acl(self):\n        self._initiate_resumable_helper(predefined_acl=\"private\")\n\n    def test__initiate_resumable_upload_with_client(self):\n        resumable_url = \"http://test.invalid?upload_id=hey-you\"\n        response_headers = {\"location\": resumable_url}\n        transport = self._mock_transport(http.client.OK, response_headers)\n\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._extra_headers = {}\n        self._initiate_resumable_helper(client=client)\n\n    def test__initiate_resumable_upload_with_client_custom_headers(self):\n        custom_headers = {\n            \"x-goog-custom-audit-foo\": \"bar\",\n            \"x-goog-custom-audit-user\": \"baz\",\n        }\n        resumable_url = \"http://test.invalid?upload_id=hey-you\"\n        response_headers = {\"location\": resumable_url}\n        transport = self._mock_transport(http.client.OK, response_headers)\n\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._extra_headers = custom_headers\n        self._initiate_resumable_helper(client=client)\n\n    def _make_resumable_transport(\n        self, headers1, headers2, headers3, total_bytes, data_corruption=False\n    ):\n        from google import resumable_media\n\n        fake_transport = mock.Mock(spec=[\"request\"])\n\n        fake_response1 = self._mock_requests_response(http.client.OK, headers1)\n        fake_response2 = self._mock_requests_response(\n            resumable_media.PERMANENT_REDIRECT, headers2\n        )\n        json_body = f'{{\"size\": \"{total_bytes:d}\"}}'\n        if data_corruption:\n            fake_response3 = resumable_media.DataCorruption(None)\n        else:\n            fake_response3 = self._mock_requests_response(\n                http.client.OK, headers3, content=json_body.encode(\"utf-8\")\n            )\n\n        responses = [fake_response1, fake_response2, fake_response3]\n        fake_transport.request.side_effect = responses\n        return fake_transport, responses\n\n    @staticmethod\n    def _do_resumable_upload_call0(\n        client,\n        blob,\n        content_type,\n        size=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=None,\n    ):\n        # First mock transport.request() does initiates upload.\n        upload_url = (\n            \"https://storage.googleapis.com/upload/storage/v1\"\n            + blob.bucket.path\n            + \"/o?uploadType=resumable\"\n        )\n        if predefined_acl is not None:\n            upload_url += f\"&predefinedAcl={predefined_acl}\"\n        expected_headers = _get_default_headers(\n            client._connection.user_agent, x_upload_content_type=content_type\n        )\n        if size is not None:\n            expected_headers[\"x-upload-content-length\"] = str(size)\n        payload = json.dumps({\"name\": blob.name}).encode(\"utf-8\")\n        return mock.call(\n            \"POST\", upload_url, data=payload, headers=expected_headers, timeout=timeout\n        )\n\n    @staticmethod\n    def _do_resumable_upload_call1(\n        client,\n        blob,\n        content_type,\n        data,\n        resumable_url,\n        size=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=None,\n    ):\n        # Second mock transport.request() does sends first chunk.\n        if size is None:\n            content_range = f\"bytes 0-{blob.chunk_size - 1:}/*\"\n        else:\n            content_range = f\"bytes 0-{blob.chunk_size - 1}/{size}\"\n\n        expected_headers = {\n            **_get_default_headers(\n                client._connection.user_agent, x_upload_content_type=content_type\n            ),\n            \"content-type\": content_type,\n            \"content-range\": content_range,\n        }\n        payload = data[: blob.chunk_size]\n        return mock.call(\n            \"PUT\",\n            resumable_url,\n            data=payload,\n            headers=expected_headers,\n            timeout=timeout,\n        )\n\n    @staticmethod\n    def _do_resumable_upload_call2(\n        client,\n        blob,\n        content_type,\n        data,\n        resumable_url,\n        total_bytes,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=None,\n    ):\n        # Third mock transport.request() does sends last chunk.\n        content_range = f\"bytes {blob.chunk_size:d}-{total_bytes - 1:d}/{total_bytes:d}\"\n        expected_headers = {\n            **_get_default_headers(\n                client._connection.user_agent, x_upload_content_type=content_type\n            ),\n            \"content-type\": content_type,\n            \"content-range\": content_range,\n        }\n        payload = data[blob.chunk_size :]\n        return mock.call(\n            \"PUT\",\n            resumable_url,\n            data=payload,\n            headers=expected_headers,\n            timeout=timeout,\n        )\n\n    def _do_resumable_helper(\n        self,\n        use_size=False,\n        num_retries=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=None,\n        data_corruption=False,\n        retry=None,\n    ):\n        CHUNK_SIZE = 256 * 1024\n        USER_AGENT = \"testing 1.2.3\"\n        content_type = \"text/html\"\n        # Data to be uploaded.\n        data = b\"<html>\" + (b\"A\" * CHUNK_SIZE) + b\"</html>\"\n        total_bytes = len(data)\n        if use_size:\n            size = total_bytes\n        else:\n            size = None\n\n        # Create mocks to be checked for doing transport.\n        resumable_url = \"http://test.invalid?upload_id=and-then-there-was-1\"\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            headers1 = {\n                **_get_default_headers(USER_AGENT, content_type),\n                \"location\": resumable_url,\n            }\n            headers2 = {\n                **_get_default_headers(USER_AGENT, content_type),\n                \"range\": f\"bytes=0-{CHUNK_SIZE - 1:d}\",\n            }\n            headers3 = _get_default_headers(USER_AGENT, content_type)\n            transport, responses = self._make_resumable_transport(\n                headers1,\n                headers2,\n                headers3,\n                total_bytes,\n                data_corruption=data_corruption,\n            )\n\n        # Create some mock arguments and call the method under test.\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._connection.user_agent = USER_AGENT\n        client._extra_headers = {}\n        stream = io.BytesIO(data)\n\n        bucket = _Bucket(name=\"yesterday\")\n        blob = self._make_one(\"blob-name\", bucket=bucket)\n        blob.chunk_size = blob._CHUNK_SIZE_MULTIPLE\n        self.assertIsNotNone(blob.chunk_size)\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            response = blob._do_resumable_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                retry=retry,\n                **timeout_kwarg,\n            )\n\n            # Check the returned values.\n            self.assertIs(response, responses[2])\n            self.assertEqual(stream.tell(), total_bytes)\n\n            # Check the mocks.\n            call0 = self._do_resumable_upload_call0(\n                client,\n                blob,\n                content_type,\n                size=size,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                timeout=expected_timeout,\n            )\n            call1 = self._do_resumable_upload_call1(\n                client,\n                blob,\n                content_type,\n                data,\n                resumable_url,\n                size=size,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                timeout=expected_timeout,\n            )\n            call2 = self._do_resumable_upload_call2(\n                client,\n                blob,\n                content_type,\n                data,\n                resumable_url,\n                total_bytes,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                timeout=expected_timeout,\n            )\n        self.assertEqual(transport.request.mock_calls, [call0, call1, call2])\n\n    def test__do_resumable_upload_with_custom_timeout(self):\n        self._do_resumable_helper(timeout=9.58)\n\n    def test__do_resumable_upload_no_size(self):\n        self._do_resumable_helper()\n\n    def test__do_resumable_upload_with_size(self):\n        self._do_resumable_helper(use_size=True)\n\n    def test__do_resumable_upload_with_retry(self):\n        self._do_resumable_helper(retry=DEFAULT_RETRY)\n\n    def test__do_resumable_upload_w_num_retries(self):\n        self._do_resumable_helper(num_retries=8)\n\n    def test__do_resumable_upload_with_retry_conflict(self):\n        with self.assertRaises(ValueError):\n            self._do_resumable_helper(num_retries=9, retry=DEFAULT_RETRY)\n\n    def test__do_resumable_upload_with_predefined_acl(self):\n        self._do_resumable_helper(predefined_acl=\"private\")\n\n    def test__do_resumable_upload_with_data_corruption(self):\n        from google.resumable_media import DataCorruption\n\n        with mock.patch(\"google.cloud.storage.blob.Blob.delete\") as patch:\n            try:\n                self._do_resumable_helper(data_corruption=True)\n            except Exception as e:\n                self.assertTrue(patch.called)\n                self.assertIsInstance(e, DataCorruption)\n\n    def _do_upload_helper(\n        self,\n        chunk_size=None,\n        num_retries=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        size=None,\n        timeout=None,\n        retry=None,\n    ):\n        from google.cloud.storage.blob import _MAX_MULTIPART_SIZE\n\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        # Create a fake response.\n        response = mock.Mock(spec=[\"json\"])\n        response.json.return_value = mock.sentinel.json\n        # Mock **both** helpers.\n        blob._do_multipart_upload = mock.Mock(return_value=response, spec=[])\n        blob._do_resumable_upload = mock.Mock(return_value=response, spec=[])\n\n        if chunk_size is None:\n            self.assertIsNone(blob.chunk_size)\n        else:\n            blob.chunk_size = chunk_size\n            self.assertIsNotNone(blob.chunk_size)\n\n        client = mock.sentinel.client\n        stream = mock.sentinel.stream\n        content_type = \"video/mp4\"\n        if size is None:\n            size = 12345654321\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n\n        # Make the request and check the mocks.\n        created_json = blob._do_upload(\n            client,\n            stream,\n            content_type,\n            size,\n            num_retries,\n            predefined_acl,\n            if_generation_match,\n            if_generation_not_match,\n            if_metageneration_match,\n            if_metageneration_not_match,\n            retry=retry,\n            **timeout_kwarg,\n        )\n\n        if retry is DEFAULT_RETRY_IF_GENERATION_SPECIFIED:\n            retry = DEFAULT_RETRY if if_generation_match else None\n\n        self.assertIs(created_json, mock.sentinel.json)\n        response.json.assert_called_once_with()\n        if size is not None and size <= _MAX_MULTIPART_SIZE:\n            blob._do_multipart_upload.assert_called_once_with(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=expected_timeout,\n                checksum=None,\n                retry=retry,\n                command=None,\n            )\n            blob._do_resumable_upload.assert_not_called()\n        else:\n            blob._do_multipart_upload.assert_not_called()\n            blob._do_resumable_upload.assert_called_once_with(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=expected_timeout,\n                checksum=None,\n                retry=retry,\n                command=None,\n            )\n\n    def test__do_upload_uses_multipart(self):\n        from google.cloud.storage.blob import _MAX_MULTIPART_SIZE\n\n        self._do_upload_helper(size=_MAX_MULTIPART_SIZE)\n\n    def test__do_upload_uses_multipart_w_custom_timeout(self):\n        from google.cloud.storage.blob import _MAX_MULTIPART_SIZE\n\n        self._do_upload_helper(size=_MAX_MULTIPART_SIZE, timeout=9.58)\n\n    def test__do_upload_uses_resumable(self):\n        from google.cloud.storage.blob import _MAX_MULTIPART_SIZE\n\n        chunk_size = 256 * 1024  # 256KB\n        self._do_upload_helper(chunk_size=chunk_size, size=_MAX_MULTIPART_SIZE + 1)\n\n    def test__do_upload_uses_resumable_w_custom_timeout(self):\n        from google.cloud.storage.blob import _MAX_MULTIPART_SIZE\n\n        chunk_size = 256 * 1024  # 256KB\n        self._do_upload_helper(\n            chunk_size=chunk_size, size=_MAX_MULTIPART_SIZE + 1, timeout=9.58\n        )\n\n    def test__do_upload_with_retry(self):\n        self._do_upload_helper(retry=DEFAULT_RETRY)\n\n    def test__do_upload_w_num_retries(self):\n        self._do_upload_helper(num_retries=2)\n\n    def test__do_upload_with_conditional_retry_success(self):\n        self._do_upload_helper(\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED, if_generation_match=123456\n        )\n\n    def test__do_upload_with_conditional_retry_failure(self):\n        self._do_upload_helper(retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED)\n\n    def _upload_from_file_helper(self, side_effect=None, **kwargs):\n        blob = self._make_one(\"blob-name\", bucket=None)\n        # Mock low-level upload helper on blob (it is tested elsewhere).\n        created_json = {\"updated\": \"2017-01-01T09:09:09.081Z\"}\n        blob._do_upload = mock.Mock(return_value=created_json, spec=[])\n        if side_effect is not None:\n            blob._do_upload.side_effect = side_effect\n        # Make sure `updated` is empty before the request.\n        self.assertIsNone(blob.updated)\n\n        data = b\"data is here\"\n        stream = io.BytesIO(data)\n        stream.seek(2)  # Not at zero.\n        content_type = \"font/woff\"\n        client = mock.sentinel.client\n        predefined_acl = kwargs.get(\"predefined_acl\", None)\n        if_generation_match = kwargs.get(\"if_generation_match\", None)\n        if_generation_not_match = kwargs.get(\"if_generation_not_match\", None)\n        if_metageneration_match = kwargs.get(\"if_metageneration_match\", None)\n        if_metageneration_not_match = kwargs.get(\"if_metageneration_not_match\", None)\n        num_retries = kwargs.get(\"num_retries\", None)\n        default_retry = (\n            DEFAULT_RETRY_IF_GENERATION_SPECIFIED if not num_retries else None\n        )\n        retry = kwargs.get(\"retry\", default_retry)\n        ret_val = blob.upload_from_file(\n            stream, size=len(data), content_type=content_type, client=client, **kwargs\n        )\n\n        # Check the response and side-effects.\n        self.assertIsNone(ret_val)\n        new_updated = datetime.datetime(2017, 1, 1, 9, 9, 9, 81000, tzinfo=_UTC)\n        self.assertEqual(blob.updated, new_updated)\n\n        expected_timeout = kwargs.get(\"timeout\", self._get_default_timeout())\n\n        blob._do_upload.assert_called_once_with(\n            client,\n            stream,\n            content_type,\n            len(data),\n            num_retries,\n            predefined_acl,\n            if_generation_match,\n            if_generation_not_match,\n            if_metageneration_match,\n            if_metageneration_not_match,\n            timeout=expected_timeout,\n            checksum=None,\n            retry=retry,\n            command=None,\n        )\n        return stream\n\n    def test_upload_from_file_success(self):\n        stream = self._upload_from_file_helper(predefined_acl=\"private\")\n        assert stream.tell() == 2\n\n    def test_upload_from_file_with_retry(self):\n        self._upload_from_file_helper(retry=DEFAULT_RETRY)\n\n    @mock.patch(\"warnings.warn\")\n    def test_upload_from_file_w_num_retries(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        self._upload_from_file_helper(num_retries=2)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_upload_from_file_with_retry_conflict(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        # Special case here: in a conflict this method should NOT raise an error\n        # as that's handled further downstream. It should pass both options\n        # through.\n        self._upload_from_file_helper(retry=DEFAULT_RETRY, num_retries=2)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def test_upload_from_file_with_rewind(self):\n        stream = self._upload_from_file_helper(rewind=True)\n        assert stream.tell() == 0\n\n    def test_upload_from_file_with_custom_timeout(self):\n        self._upload_from_file_helper(timeout=9.58)\n\n    def test_upload_from_file_failure(self):\n        import requests\n\n        from google.resumable_media import InvalidResponse\n        from google.cloud import exceptions\n\n        message = \"Someone is already in this spot.\"\n        response = requests.Response()\n        response.status_code = http.client.CONFLICT\n        response.request = requests.Request(\"POST\", \"http://example.com\").prepare()\n        side_effect = InvalidResponse(response, message)\n\n        with self.assertRaises(exceptions.Conflict) as exc_info:\n            self._upload_from_file_helper(side_effect=side_effect)\n\n        self.assertIn(message, exc_info.exception.message)\n        self.assertEqual(exc_info.exception.errors, [])\n\n    def _do_upload_mock_call_helper(\n        self,\n        blob,\n        client,\n        content_type,\n        size,\n        timeout=None,\n        num_retries=None,\n        retry=None,\n    ):\n        self.assertEqual(blob._do_upload.call_count, 1)\n        mock_call = blob._do_upload.mock_calls[0]\n        call_name, pos_args, kwargs = mock_call\n        self.assertEqual(call_name, \"\")\n        self.assertEqual(len(pos_args), 10)\n        self.assertEqual(pos_args[0], client)\n        self.assertEqual(pos_args[2], content_type)\n        self.assertEqual(pos_args[3], size)\n        self.assertEqual(pos_args[4], num_retries)  # num_retries\n        self.assertIsNone(pos_args[5])  # predefined_acl\n        self.assertIsNone(pos_args[6])  # if_generation_match\n        self.assertIsNone(pos_args[7])  # if_generation_not_match\n        self.assertIsNone(pos_args[8])  # if_metageneration_match\n        self.assertIsNone(pos_args[9])  # if_metageneration_not_match\n\n        expected_timeout = self._get_default_timeout() if timeout is None else timeout\n        if not retry:\n            retry = DEFAULT_RETRY_IF_GENERATION_SPECIFIED if not num_retries else None\n        self.assertEqual(\n            kwargs,\n            {\n                \"timeout\": expected_timeout,\n                \"checksum\": None,\n                \"retry\": retry,\n                \"command\": None,\n            },\n        )\n\n        return pos_args[1]\n\n    def test_upload_from_filename(self):\n        from google.cloud._testing import _NamedTemporaryFile\n\n        blob = self._make_one(\"blob-name\", bucket=None)\n        # Mock low-level upload helper on blob (it is tested elsewhere).\n        created_json = {\"metadata\": {\"mint\": \"ice-cream\"}}\n        blob._do_upload = mock.Mock(return_value=created_json, spec=[])\n        # Make sure `metadata` is empty before the request.\n        self.assertIsNone(blob.metadata)\n\n        data = b\"soooo much data\"\n        content_type = \"image/svg+xml\"\n        client = mock.sentinel.client\n        with _NamedTemporaryFile() as temp:\n            with open(temp.name, \"wb\") as file_obj:\n                file_obj.write(data)\n\n            ret_val = blob.upload_from_filename(\n                temp.name, content_type=content_type, client=client\n            )\n\n        # Check the response and side-effects.\n        self.assertIsNone(ret_val)\n        self.assertEqual(blob.metadata, created_json[\"metadata\"])\n\n        # Check the mock.\n        stream = self._do_upload_mock_call_helper(blob, client, content_type, len(data))\n        self.assertTrue(stream.closed)\n        self.assertEqual(stream.mode, \"rb\")\n        self.assertEqual(stream.name, temp.name)\n\n    def test_upload_from_filename_with_retry(self):\n        from google.cloud._testing import _NamedTemporaryFile\n\n        blob = self._make_one(\"blob-name\", bucket=None)\n        # Mock low-level upload helper on blob (it is tested elsewhere).\n        created_json = {\"metadata\": {\"mint\": \"ice-cream\"}}\n        blob._do_upload = mock.Mock(return_value=created_json, spec=[])\n        # Make sure `metadata` is empty before the request.\n        self.assertIsNone(blob.metadata)\n\n        data = b\"soooo much data\"\n        content_type = \"image/svg+xml\"\n        client = mock.sentinel.client\n        with _NamedTemporaryFile() as temp:\n            with open(temp.name, \"wb\") as file_obj:\n                file_obj.write(data)\n\n            ret_val = blob.upload_from_filename(\n                temp.name, content_type=content_type, client=client, retry=DEFAULT_RETRY\n            )\n\n        # Check the response and side-effects.\n        self.assertIsNone(ret_val)\n        self.assertEqual(blob.metadata, created_json[\"metadata\"])\n\n        # Check the mock.\n        stream = self._do_upload_mock_call_helper(\n            blob, client, content_type, len(data), retry=DEFAULT_RETRY\n        )\n        self.assertTrue(stream.closed)\n        self.assertEqual(stream.mode, \"rb\")\n        self.assertEqual(stream.name, temp.name)\n\n    @mock.patch(\"warnings.warn\")\n    def test_upload_from_filename_w_num_retries(self, mock_warn):\n        from google.cloud._testing import _NamedTemporaryFile\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        blob = self._make_one(\"blob-name\", bucket=None)\n        # Mock low-level upload helper on blob (it is tested elsewhere).\n        created_json = {\"metadata\": {\"mint\": \"ice-cream\"}}\n        blob._do_upload = mock.Mock(return_value=created_json, spec=[])\n        # Make sure `metadata` is empty before the request.\n        self.assertIsNone(blob.metadata)\n\n        data = b\"soooo much data\"\n        content_type = \"image/svg+xml\"\n        client = mock.sentinel.client\n        with _NamedTemporaryFile() as temp:\n            with open(temp.name, \"wb\") as file_obj:\n                file_obj.write(data)\n\n            ret_val = blob.upload_from_filename(\n                temp.name, content_type=content_type, client=client, num_retries=2\n            )\n\n        # Check the response and side-effects.\n        self.assertIsNone(ret_val)\n        self.assertEqual(blob.metadata, created_json[\"metadata\"])\n\n        # Check the mock.\n        stream = self._do_upload_mock_call_helper(\n            blob, client, content_type, len(data), num_retries=2\n        )\n        self.assertTrue(stream.closed)\n        self.assertEqual(stream.mode, \"rb\")\n        self.assertEqual(stream.name, temp.name)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def test_upload_from_filename_w_custom_timeout(self):\n        from google.cloud._testing import _NamedTemporaryFile\n\n        blob = self._make_one(\"blob-name\", bucket=None)\n        # Mock low-level upload helper on blob (it is tested elsewhere).\n        created_json = {\"metadata\": {\"mint\": \"ice-cream\"}}\n        blob._do_upload = mock.Mock(return_value=created_json, spec=[])\n        # Make sure `metadata` is empty before the request.\n        self.assertIsNone(blob.metadata)\n\n        data = b\"soooo much data\"\n        content_type = \"image/svg+xml\"\n        client = mock.sentinel.client\n        with _NamedTemporaryFile() as temp:\n            with open(temp.name, \"wb\") as file_obj:\n                file_obj.write(data)\n\n            blob.upload_from_filename(\n                temp.name, content_type=content_type, client=client, timeout=9.58\n            )\n\n        # Check the mock.\n        self._do_upload_mock_call_helper(\n            blob, client, content_type, len(data), timeout=9.58\n        )\n\n    def _upload_from_string_helper(self, data, **kwargs):\n        from google.cloud._helpers import _to_bytes\n\n        blob = self._make_one(\"blob-name\", bucket=None)\n\n        # Mock low-level upload helper on blob (it is tested elsewhere).\n        created_json = {\"componentCount\": \"5\"}\n        blob._do_upload = mock.Mock(return_value=created_json, spec=[])\n        # Make sure `metadata` is empty before the request.\n        self.assertIsNone(blob.component_count)\n\n        client = mock.sentinel.client\n        ret_val = blob.upload_from_string(data, client=client, **kwargs)\n\n        # Check the response and side-effects.\n        self.assertIsNone(ret_val)\n        self.assertEqual(blob.component_count, 5)\n\n        extra_kwargs = {}\n        if \"retry\" in kwargs:\n            extra_kwargs[\"retry\"] = kwargs[\"retry\"]\n        if \"num_retries\" in kwargs:\n            extra_kwargs[\"num_retries\"] = kwargs[\"num_retries\"]\n        # Check the mock.\n        payload = _to_bytes(data, encoding=\"utf-8\")\n        stream = self._do_upload_mock_call_helper(\n            blob,\n            client,\n            \"text/plain\",\n            len(payload),\n            kwargs.get(\"timeout\", self._get_default_timeout()),\n            **extra_kwargs,\n        )\n        self.assertIsInstance(stream, io.BytesIO)\n        self.assertEqual(stream.getvalue(), payload)\n\n    def test_upload_from_string_w_custom_timeout(self):\n        data = b\"XB]jb\\xb8tad\\xe0\"\n        self._upload_from_string_helper(data, timeout=9.58)\n\n    def test_upload_from_string_w_bytes(self):\n        data = b\"XB]jb\\xb8tad\\xe0\"\n        self._upload_from_string_helper(data)\n\n    def test_upload_from_string_w_text(self):\n        data = \"\\N{snowman} \\N{sailboat}\"\n        self._upload_from_string_helper(data)\n\n    def test_upload_from_string_w_text_w_retry(self):\n        data = \"\\N{snowman} \\N{sailboat}\"\n        self._upload_from_string_helper(data, retry=DEFAULT_RETRY)\n\n    @mock.patch(\"warnings.warn\")\n    def test_upload_from_string_with_num_retries(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        data = \"\\N{snowman} \\N{sailboat}\"\n        self._upload_from_string_helper(data, num_retries=2)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def _create_resumable_upload_session_helper(\n        self,\n        origin=None,\n        side_effect=None,\n        timeout=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=None,\n        client=None,\n    ):\n        bucket = _Bucket(name=\"alex-trebek\")\n        blob = self._make_one(\"blob-name\", bucket=bucket)\n        chunk_size = 99 * blob._CHUNK_SIZE_MULTIPLE\n        blob.chunk_size = chunk_size\n        resumable_url = \"http://test.invalid?upload_id=clean-up-everybody\"\n        content_type = \"text/plain\"\n        size = 10000\n        transport = None\n\n        if not client:\n            # Create mocks to be checked for doing transport.\n            response_headers = {\"location\": resumable_url}\n            transport = self._mock_transport(http.client.OK, response_headers)\n\n            # Create some mock arguments and call the method under test.\n            client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n            client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n            client._connection.user_agent = \"testing 1.2.3\"\n            client._extra_headers = {}\n\n        if transport is None:\n            transport = client._http\n        if side_effect is not None:\n            transport.request.side_effect = side_effect\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n            timeout_kwarg = {}\n        else:\n            expected_timeout = timeout\n            timeout_kwarg = {\"timeout\": timeout}\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            new_url = blob.create_resumable_upload_session(\n                content_type=content_type,\n                size=size,\n                origin=origin,\n                client=client,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n                **timeout_kwarg,\n            )\n\n        # Check the returned value and (lack of) side-effect.\n        self.assertEqual(new_url, resumable_url)\n        self.assertEqual(blob.chunk_size, chunk_size)\n\n        # Check the mocks.\n        upload_url = (\n            \"https://storage.googleapis.com/upload/storage/v1\" + bucket.path + \"/o\"\n        )\n\n        qs_params = [(\"uploadType\", \"resumable\")]\n        if predefined_acl is not None:\n            qs_params.append((\"predefinedAcl\", predefined_acl))\n\n        if if_generation_match is not None:\n            qs_params.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            qs_params.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            qs_params.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            qs_params.append((\"ifMetaGenerationNotMatch\", if_metageneration_not_match))\n\n        upload_url += \"?\" + urlencode(qs_params)\n        payload = b'{\"name\": \"blob-name\"}'\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            expected_headers = {\n                **_get_default_headers(\n                    client._connection.user_agent, x_upload_content_type=content_type\n                ),\n                **client._extra_headers,\n                \"x-upload-content-length\": str(size),\n                \"x-upload-content-type\": content_type,\n            }\n        if origin is not None:\n            expected_headers[\"Origin\"] = origin\n        transport.request.assert_called_once_with(\n            \"POST\",\n            upload_url,\n            data=payload,\n            headers=expected_headers,\n            timeout=expected_timeout,\n        )\n\n    def test_create_resumable_upload_session(self):\n        self._create_resumable_upload_session_helper()\n\n    def test_create_resumable_upload_session_with_custom_timeout(self):\n        self._create_resumable_upload_session_helper(timeout=9.58)\n\n    def test_create_resumable_upload_session_with_origin(self):\n        self._create_resumable_upload_session_helper(origin=\"http://google.com\")\n\n    def test_create_resumable_upload_session_with_predefined_acl(self):\n        self._create_resumable_upload_session_helper(predefined_acl=\"private\")\n\n    def test_create_resumable_upload_session_with_generation_match(self):\n        self._create_resumable_upload_session_helper(\n            if_generation_match=123456, if_metageneration_match=2\n        )\n\n    def test_create_resumable_upload_session_with_generation_not_match(self):\n        self._create_resumable_upload_session_helper(\n            if_generation_not_match=0, if_metageneration_not_match=3\n        )\n\n    def test_create_resumable_upload_session_with_conditional_retry_success(self):\n        self._create_resumable_upload_session_helper(\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED, if_generation_match=123456\n        )\n\n    def test_create_resumable_upload_session_with_conditional_retry_failure(self):\n        self._create_resumable_upload_session_helper(\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED\n        )\n\n    def test_create_resumable_upload_session_with_failure(self):\n        from google.resumable_media import InvalidResponse\n        from google.cloud import exceptions\n\n        message = \"5-oh-3 woe is me.\"\n        response = self._mock_requests_response(\n            status_code=http.client.SERVICE_UNAVAILABLE, headers={}\n        )\n        side_effect = InvalidResponse(response, message)\n\n        with self.assertRaises(exceptions.ServiceUnavailable) as exc_info:\n            self._create_resumable_upload_session_helper(side_effect=side_effect)\n\n        self.assertIn(message, exc_info.exception.message)\n        self.assertEqual(exc_info.exception.errors, [])\n\n    def test_create_resumable_upload_session_with_client(self):\n        resumable_url = \"http://test.invalid?upload_id=clean-up-everybody\"\n        response_headers = {\"location\": resumable_url}\n        transport = self._mock_transport(http.client.OK, response_headers)\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._extra_headers = {}\n        self._create_resumable_upload_session_helper(client=client)\n\n    def test_create_resumable_upload_session_with_client_custom_headers(self):\n        custom_headers = {\n            \"x-goog-custom-audit-foo\": \"bar\",\n            \"x-goog-custom-audit-user\": \"baz\",\n        }\n        resumable_url = \"http://test.invalid?upload_id=clean-up-everybody\"\n        response_headers = {\"location\": resumable_url}\n        transport = self._mock_transport(http.client.OK, response_headers)\n        client = mock.Mock(_http=transport, _connection=_Connection, spec=[\"_http\"])\n        client._connection.API_BASE_URL = \"https://storage.googleapis.com\"\n        client._extra_headers = custom_headers\n        self._create_resumable_upload_session_helper(client=client)\n\n    def test_get_iam_policy_defaults(self):\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n        from google.cloud.storage.iam import STORAGE_EDITOR_ROLE\n        from google.cloud.storage.iam import STORAGE_VIEWER_ROLE\n        from google.api_core.iam import Policy\n\n        blob_name = \"blob-name\"\n        path = f\"/b/name/o/{blob_name}\"\n        etag = \"DEADBEEF\"\n        version = 1\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        editor1 = \"domain:google.com\"\n        editor2 = \"user:phred@example.com\"\n        viewer1 = \"serviceAccount:1234-abcdef@service.example.com\"\n        viewer2 = \"user:phred@example.com\"\n        api_response = {\n            \"resourceId\": path,\n            \"etag\": etag,\n            \"version\": version,\n            \"bindings\": [\n                {\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]},\n                {\"role\": STORAGE_EDITOR_ROLE, \"members\": [editor1, editor2]},\n                {\"role\": STORAGE_VIEWER_ROLE, \"members\": [viewer1, viewer2]},\n            ],\n        }\n        expected_policy = {\n            binding[\"role\"]: set(binding[\"members\"])\n            for binding in api_response[\"bindings\"]\n        }\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        policy = blob.get_iam_policy()\n\n        self.assertIsInstance(policy, Policy)\n        self.assertEqual(policy.etag, api_response[\"etag\"])\n        self.assertEqual(policy.version, api_response[\"version\"])\n        self.assertEqual(dict(policy), expected_policy)\n\n        expected_path = f\"{path}/iam\"\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_get_iam_policy_w_user_project_w_timeout(self):\n        from google.api_core.iam import Policy\n\n        blob_name = \"blob-name\"\n        user_project = \"user-project-123\"\n        timeout = 42\n        path = f\"/b/name/o/{blob_name}\"\n        etag = \"DEADBEEF\"\n        version = 1\n        api_response = {\n            \"resourceId\": path,\n            \"etag\": etag,\n            \"version\": version,\n            \"bindings\": [],\n        }\n        expected_policy = {}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client=client, user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        policy = blob.get_iam_policy(timeout=42)\n\n        self.assertIsInstance(policy, Policy)\n        self.assertEqual(policy.etag, api_response[\"etag\"])\n        self.assertEqual(policy.version, api_response[\"version\"])\n        self.assertEqual(dict(policy), expected_policy)\n\n        expected_path = f\"{path}/iam\"\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_get_iam_policy_w_requested_policy_version(self):\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n\n        blob_name = \"blob-name\"\n        path = f\"/b/name/o/{blob_name}\"\n        etag = \"DEADBEEF\"\n        version = 3\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        api_response = {\n            \"resourceId\": path,\n            \"etag\": etag,\n            \"version\": version,\n            \"bindings\": [{\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]}],\n        }\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        policy = blob.get_iam_policy(requested_policy_version=version)\n\n        self.assertEqual(policy.version, version)\n\n        expected_path = f\"{path}/iam\"\n        expected_query_params = {\"optionsRequestedPolicyVersion\": version}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_set_iam_policy(self):\n        import operator\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n        from google.cloud.storage.iam import STORAGE_EDITOR_ROLE\n        from google.cloud.storage.iam import STORAGE_VIEWER_ROLE\n        from google.api_core.iam import Policy\n\n        blob_name = \"blob-name\"\n        path = f\"/b/name/o/{blob_name}\"\n        etag = \"DEADBEEF\"\n        version = 1\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        editor1 = \"domain:google.com\"\n        editor2 = \"user:phred@example.com\"\n        viewer1 = \"serviceAccount:1234-abcdef@service.example.com\"\n        viewer2 = \"user:phred@example.com\"\n        bindings = [\n            {\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]},\n            {\"role\": STORAGE_EDITOR_ROLE, \"members\": [editor1, editor2]},\n            {\"role\": STORAGE_VIEWER_ROLE, \"members\": [viewer1, viewer2]},\n        ]\n        api_response = {\"etag\": etag, \"version\": version, \"bindings\": bindings}\n        policy = Policy()\n        for binding in bindings:\n            policy[binding[\"role\"]] = binding[\"members\"]\n\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        returned = blob.set_iam_policy(policy)\n\n        self.assertEqual(returned.etag, etag)\n        self.assertEqual(returned.version, version)\n        self.assertEqual(dict(returned), dict(policy))\n\n        expected_path = f\"{path}/iam\"\n        expected_data = {\n            \"resourceId\": path,\n            \"bindings\": mock.ANY,\n        }\n        expected_query_params = {}\n        client._put_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n            _target_object=None,\n        )\n\n        sent_bindings = client._put_resource.call_args.args[1][\"bindings\"]\n        key = operator.itemgetter(\"role\")\n        for found, expected in zip(\n            sorted(sent_bindings, key=key), sorted(bindings, key=key)\n        ):\n            self.assertEqual(found[\"role\"], expected[\"role\"])\n            self.assertEqual(sorted(found[\"members\"]), sorted(expected[\"members\"]))\n\n    def test_set_iam_policy_w_user_project_w_explicit_client_w_timeout_retry(self):\n        from google.api_core.iam import Policy\n\n        blob_name = \"blob-name\"\n        user_project = \"user-project-123\"\n        path = f\"/b/name/o/{blob_name}\"\n        etag = \"DEADBEEF\"\n        version = 1\n        bindings = []\n        policy = Policy()\n\n        api_response = {\"etag\": etag, \"version\": version, \"bindings\": bindings}\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n        bucket = _Bucket(client=None, user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        returned = blob.set_iam_policy(\n            policy,\n            client=client,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.assertEqual(returned.etag, etag)\n        self.assertEqual(returned.version, version)\n        self.assertEqual(dict(returned), dict(policy))\n\n        expected_path = f\"{path}/iam\"\n        expected_data = {  # bindings omitted\n            \"resourceId\": path,\n        }\n        expected_query_params = {\"userProject\": user_project}\n        client._put_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_test_iam_permissions_defaults(self):\n        from google.cloud.storage.iam import STORAGE_OBJECTS_LIST\n        from google.cloud.storage.iam import STORAGE_BUCKETS_GET\n        from google.cloud.storage.iam import STORAGE_BUCKETS_UPDATE\n\n        blob_name = \"blob-name\"\n        permissions = [\n            STORAGE_OBJECTS_LIST,\n            STORAGE_BUCKETS_GET,\n            STORAGE_BUCKETS_UPDATE,\n        ]\n        expected = permissions[1:]\n        api_response = {\"permissions\": expected}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        found = blob.test_iam_permissions(permissions)\n\n        self.assertEqual(found, expected)\n\n        expected_path = f\"/b/name/o/{blob_name}/iam/testPermissions\"\n        expected_query_params = {\"permissions\": permissions}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_test_iam_permissions_w_user_project_w_timeout_w_retry(self):\n        from google.cloud.storage.iam import STORAGE_OBJECTS_LIST\n        from google.cloud.storage.iam import STORAGE_BUCKETS_GET\n        from google.cloud.storage.iam import STORAGE_BUCKETS_UPDATE\n\n        blob_name = \"blob-name\"\n        user_project = \"user-project-123\"\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        permissions = [\n            STORAGE_OBJECTS_LIST,\n            STORAGE_BUCKETS_GET,\n            STORAGE_BUCKETS_UPDATE,\n        ]\n        expected = permissions[1:]\n        api_response = {\"permissions\": expected}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = _Bucket(client=client, user_project=user_project)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        found = blob.test_iam_permissions(permissions, timeout=timeout, retry=retry)\n\n        self.assertEqual(found, expected)\n\n        expected_path = f\"/b/name/o/{blob_name}/iam/testPermissions\"\n        expected_query_params = {\n            \"permissions\": permissions,\n            \"userProject\": user_project,\n        }\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_make_public_w_defaults(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        blob_name = \"blob-name\"\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n        api_response = {\"acl\": permissive}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.acl.loaded = True\n\n        blob.make_public()\n\n        self.assertEqual(list(blob.acl), permissive)\n\n        expected_patch_data = {\"acl\": permissive}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            blob.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_public_w_timeout(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        blob_name = \"blob-name\"\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n        api_response = {\"acl\": permissive}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.acl.loaded = True\n        timeout = 42\n\n        blob.make_public(timeout=timeout)\n\n        self.assertEqual(list(blob.acl), permissive)\n\n        expected_patch_data = {\"acl\": permissive}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            blob.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_public_w_preconditions(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        blob_name = \"blob-name\"\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n        api_response = {\"acl\": permissive}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.acl.loaded = True\n\n        blob.make_public(if_metageneration_match=2, if_metageneration_not_match=1)\n\n        self.assertEqual(list(blob.acl), permissive)\n\n        expected_patch_data = {\"acl\": permissive}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            blob.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_private_w_defaults(self):\n        blob_name = \"blob-name\"\n        no_permissions = []\n        api_response = {\"acl\": no_permissions}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.acl.loaded = True\n\n        blob.make_private()\n\n        self.assertEqual(list(blob.acl), no_permissions)\n\n        expected_patch_data = {\"acl\": no_permissions}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            blob.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_private_w_timeout(self):\n        blob_name = \"blob-name\"\n        no_permissions = []\n        api_response = {\"acl\": no_permissions}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.acl.loaded = True\n        timeout = 42\n\n        blob.make_private(timeout=timeout)\n\n        self.assertEqual(list(blob.acl), no_permissions)\n\n        expected_patch_data = {\"acl\": no_permissions}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            blob.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_private_w_preconditions(self):\n        blob_name = \"blob-name\"\n        no_permissions = []\n        api_response = {\"acl\": no_permissions}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.acl.loaded = True\n\n        blob.make_private(if_metageneration_match=2, if_metageneration_not_match=1)\n\n        self.assertEqual(list(blob.acl), no_permissions)\n\n        expected_patch_data = {\"acl\": no_permissions}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            blob.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_compose_wo_content_type_set(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n        destination = self._make_one(destination_name, bucket=bucket)\n        # no destination.content_type set\n\n        destination.compose(sources=[source_1, source_2])\n\n        self.assertIsNone(destination.content_type)\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\"name\": source_1.name, \"generation\": source_1.generation},\n                {\"name\": source_2.name, \"generation\": source_2.generation},\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n    def test_compose_minimal_w_user_project_w_timeout(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        api_response = {\"etag\": \"DEADBEEF\"}\n        user_project = \"user-project-123\"\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client, user_project=user_project)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n        destination = self._make_one(destination_name, bucket=bucket)\n        destination.content_type = \"text/plain\"\n        timeout = 42\n\n        destination.compose(sources=[source_1, source_2], timeout=timeout)\n\n        self.assertEqual(destination.etag, \"DEADBEEF\")\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\"name\": source_1.name, \"generation\": source_1.generation},\n                {\"name\": source_2.name, \"generation\": source_2.generation},\n            ],\n            \"destination\": {\"contentType\": \"text/plain\"},\n        }\n        expected_query_params = {\"userProject\": user_project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n    def test_compose_w_additional_property_changes_w_retry(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        api_response = {\"etag\": \"DEADBEEF\"}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n        destination = self._make_one(destination_name, bucket=bucket)\n        destination.content_type = \"text/plain\"\n        destination.content_language = \"en-US\"\n        destination.metadata = {\"my-key\": \"my-value\"}\n        retry = mock.Mock(spec=[])\n\n        destination.compose(sources=[source_1, source_2], retry=retry)\n\n        self.assertEqual(destination.etag, \"DEADBEEF\")\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\"name\": source_1.name, \"generation\": source_1.generation},\n                {\"name\": source_2.name, \"generation\": source_2.generation},\n            ],\n            \"destination\": {\n                \"contentType\": \"text/plain\",\n                \"contentLanguage\": \"en-US\",\n                \"metadata\": {\"my-key\": \"my-value\"},\n            },\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=destination,\n        )\n\n    def test_compose_w_source_generation_match(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        api_response = {}\n        source_generation_numbers = [6, 9]\n\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n\n        destination = self._make_one(destination_name, bucket=bucket)\n        destination.compose(\n            sources=[source_1, source_2],\n            if_source_generation_match=source_generation_numbers,\n        )\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\n                    \"name\": source_1.name,\n                    \"generation\": source_1.generation,\n                    \"objectPreconditions\": {\n                        \"ifGenerationMatch\": source_generation_numbers[0],\n                    },\n                },\n                {\n                    \"name\": source_2.name,\n                    \"generation\": source_2.generation,\n                    \"objectPreconditions\": {\n                        \"ifGenerationMatch\": source_generation_numbers[1],\n                    },\n                },\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n    def test_compose_w_source_generation_match_bad_length(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        source_generation_numbers = [6]\n        client = mock.Mock(spec=[\"_post_resource\"])\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n\n        destination = self._make_one(destination_name, bucket=bucket)\n\n        with self.assertRaises(ValueError):\n            destination.compose(\n                sources=[source_1, source_2],\n                if_source_generation_match=source_generation_numbers,\n            )\n\n        client._post_resource.assert_not_called()\n\n    def test_compose_w_source_generation_match_nones(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        source_generation_numbers = [6, None]\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n        destination = self._make_one(destination_name, bucket=bucket)\n\n        destination.compose(\n            sources=[source_1, source_2],\n            if_source_generation_match=source_generation_numbers,\n        )\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\n                    \"name\": source_1.name,\n                    \"generation\": source_1.generation,\n                    \"objectPreconditions\": {\n                        \"ifGenerationMatch\": source_generation_numbers[0],\n                    },\n                },\n                {\"name\": source_2.name, \"generation\": source_2.generation},\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n    def test_compose_w_generation_match(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        generation_number = 1\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n        destination = self._make_one(destination_name, bucket=bucket)\n\n        destination.compose(\n            sources=[source_1, source_2],\n            if_generation_match=generation_number,\n        )\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\"name\": source_1.name, \"generation\": source_1.generation},\n                {\"name\": source_2.name, \"generation\": source_2.generation},\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {\"ifGenerationMatch\": generation_number}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_compose_w_if_generation_match_list_w_warning(self, mock_warn):\n        from google.cloud.storage.blob import _COMPOSE_IF_GENERATION_LIST_DEPRECATED\n\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        api_response = {}\n        generation_numbers = [6, 9]\n\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n\n        destination = self._make_one(destination_name, bucket=bucket)\n        destination.compose(\n            sources=[source_1, source_2],\n            if_generation_match=generation_numbers,\n        )\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\n                    \"name\": source_1_name,\n                    \"generation\": None,\n                    \"objectPreconditions\": {\n                        \"ifGenerationMatch\": generation_numbers[0],\n                    },\n                },\n                {\n                    \"name\": source_2_name,\n                    \"generation\": None,\n                    \"objectPreconditions\": {\n                        \"ifGenerationMatch\": generation_numbers[1],\n                    },\n                },\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n        mock_warn.assert_called_with(\n            _COMPOSE_IF_GENERATION_LIST_DEPRECATED,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_compose_w_if_generation_match_and_if_s_generation_match(self, mock_warn):\n        from google.cloud.storage.blob import _COMPOSE_IF_GENERATION_LIST_DEPRECATED\n\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        source_generation_numbers = [6, 8]\n        client = mock.Mock(spec=[\"_post_resource\"])\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n\n        destination = self._make_one(destination_name, bucket=bucket)\n\n        with self.assertRaises(ValueError):\n            destination.compose(\n                sources=[source_1, source_2],\n                if_generation_match=source_generation_numbers,\n                if_source_generation_match=source_generation_numbers,\n            )\n\n        client._post_resource.assert_not_called()\n\n        mock_warn.assert_called_with(\n            _COMPOSE_IF_GENERATION_LIST_DEPRECATED,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_compose_w_if_metageneration_match_list_w_warning(self, mock_warn):\n        from google.cloud.storage.blob import _COMPOSE_IF_METAGENERATION_LIST_DEPRECATED\n\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        metageneration_number = [6]\n        client = mock.Mock(spec=[\"_post_resource\"])\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n\n        destination = self._make_one(destination_name, bucket=bucket)\n\n        destination.compose(\n            sources=[source_1, source_2],\n            if_metageneration_match=metageneration_number,\n        )\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\"name\": source_1_name, \"generation\": None},\n                {\"name\": source_2_name, \"generation\": None},\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n        mock_warn.assert_called_with(\n            _COMPOSE_IF_METAGENERATION_LIST_DEPRECATED,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def test_compose_w_metageneration_match(self):\n        source_1_name = \"source-1\"\n        source_2_name = \"source-2\"\n        destination_name = \"destination\"\n        metageneration_number = 1\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source_1 = self._make_one(source_1_name, bucket=bucket)\n        source_2 = self._make_one(source_2_name, bucket=bucket)\n        destination = self._make_one(destination_name, bucket=bucket)\n\n        destination.compose(\n            sources=[source_1, source_2],\n            if_metageneration_match=metageneration_number,\n        )\n\n        expected_path = f\"/b/name/o/{destination_name}/compose\"\n        expected_data = {\n            \"sourceObjects\": [\n                {\"name\": source_1.name, \"generation\": source_1.generation},\n                {\"name\": source_2.name, \"generation\": source_2.generation},\n            ],\n            \"destination\": {},\n        }\n        expected_query_params = {\"ifMetagenerationMatch\": metageneration_number}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=destination,\n        )\n\n    def test_rewrite_w_response_wo_resource(self):\n        source_name = \"source\"\n        dest_name = \"dest\"\n        other_bucket_name = \"other-bucket\"\n        bytes_rewritten = 33\n        object_size = 52\n        rewrite_token = \"TOKEN\"\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": False,\n            \"rewriteToken\": rewrite_token,\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source_bucket = _Bucket(client=client)\n        source_blob = self._make_one(source_name, bucket=source_bucket)\n        dest_bucket = _Bucket(client=client, name=other_bucket_name)\n        dest_blob = self._make_one(dest_name, bucket=dest_bucket)\n\n        token, rewritten, size = dest_blob.rewrite(source_blob)\n\n        self.assertEqual(token, rewrite_token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = \"/b/%s/o/%s/rewriteTo/b/%s/o/%s\" % (\n            source_bucket.name,\n            source_name,\n            other_bucket_name,\n            dest_name,\n        )\n        expected_data = {}\n        expected_query_params = {}\n        expected_headers = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=dest_blob,\n        )\n\n    def test_rewrite_w_generations_w_timeout(self):\n        source_name = \"source\"\n        source_generation = 22\n        dest_name = \"dest\"\n        other_bucket_name = \"other-bucket\"\n        dest_generation = 23\n        bytes_rewritten = 33\n        object_size = 52\n        rewrite_token = \"TOKEN\"\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": False,\n            \"rewriteToken\": rewrite_token,\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source_bucket = _Bucket(client=client)\n        source_blob = self._make_one(\n            source_name, bucket=source_bucket, generation=source_generation\n        )\n        dest_bucket = _Bucket(client=client, name=other_bucket_name)\n        dest_blob = self._make_one(\n            dest_name, bucket=dest_bucket, generation=dest_generation\n        )\n        timeout = 42\n\n        token, rewritten, size = dest_blob.rewrite(source_blob, timeout=timeout)\n\n        self.assertEqual(token, rewrite_token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = \"/b/%s/o/%s/rewriteTo/b/%s/o/%s\" % (\n            source_bucket.name,\n            source_name,\n            other_bucket_name,\n            dest_name,\n        )\n        expected_data = {\"generation\": dest_generation}\n        expected_query_params = {\"sourceGeneration\": source_generation}\n        expected_headers = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=dest_blob,\n        )\n\n    def test_rewrite_w_generation_match_w_retry(self):\n        source_name = \"source\"\n        source_generation = 42\n        dest_name = \"dest\"\n        other_bucket_name = \"other-bucket\"\n        dest_generation = 16\n        bytes_rewritten = 33\n        object_size = 52\n        rewrite_token = \"TOKEN\"\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": False,\n            \"rewriteToken\": rewrite_token,\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source_bucket = _Bucket(client=client)\n        source_blob = self._make_one(\n            source_name, bucket=source_bucket, generation=source_generation\n        )\n        dest_bucket = _Bucket(client=client, name=other_bucket_name)\n        dest_blob = self._make_one(\n            dest_name, bucket=dest_bucket, generation=dest_generation\n        )\n        retry = mock.Mock(spec=[])\n\n        token, rewritten, size = dest_blob.rewrite(\n            source_blob,\n            if_generation_match=dest_blob.generation,\n            if_source_generation_match=source_blob.generation,\n            retry=retry,\n        )\n\n        self.assertEqual(token, rewrite_token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = \"/b/%s/o/%s/rewriteTo/b/%s/o/%s\" % (\n            source_bucket.name,\n            source_name,\n            other_bucket_name,\n            dest_name,\n        )\n        expected_data = {\"generation\": dest_generation}\n        expected_query_params = {\n            \"ifSourceGenerationMatch\": source_generation,\n            \"ifGenerationMatch\": dest_generation,\n            \"sourceGeneration\": source_generation,\n        }\n        expected_headers = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=dest_blob,\n        )\n\n    def test_rewrite_other_bucket_other_name_no_encryption_partial(self):\n        source_name = \"source\"\n        dest_name = \"dest\"\n        other_bucket_name = \"other-bucket\"\n        bytes_rewritten = 33\n        object_size = 52\n        rewrite_token = \"TOKEN\"\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": False,\n            \"rewriteToken\": rewrite_token,\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source_bucket = _Bucket(client=client)\n        source_blob = self._make_one(source_name, bucket=source_bucket)\n        dest_bucket = _Bucket(client=client, name=other_bucket_name)\n        dest_blob = self._make_one(dest_name, bucket=dest_bucket)\n\n        token, rewritten, size = dest_blob.rewrite(source_blob)\n\n        self.assertEqual(token, rewrite_token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = \"/b/name/o/%s/rewriteTo/b/%s/o/%s\" % (\n            source_name,\n            other_bucket_name,\n            dest_name,\n        )\n        expected_query_params = {}\n        expected_data = {}\n        expected_headers = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=dest_blob,\n        )\n\n    def test_rewrite_same_name_no_old_key_new_key_done_w_user_project(self):\n        blob_name = \"blob\"\n        user_project = \"user-project-123\"\n        key = b\"01234567890123456789012345678901\"  # 32 bytes\n        key_b64 = base64.b64encode(key).rstrip().decode(\"ascii\")\n        key_hash = hashlib.sha256(key).digest()\n        key_hash_b64 = base64.b64encode(key_hash).rstrip().decode(\"ascii\")\n        bytes_rewritten = object_size = 52\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": True,\n            \"resource\": {\"etag\": \"DEADBEEF\"},\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client, user_project=user_project)\n        plain = self._make_one(blob_name, bucket=bucket)\n        encrypted = self._make_one(blob_name, bucket=bucket, encryption_key=key)\n\n        token, rewritten, size = encrypted.rewrite(plain)\n\n        self.assertIsNone(token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = f\"/b/name/o/{blob_name}/rewriteTo/b/name/o/{blob_name}\"\n        expected_query_params = {\"userProject\": user_project}\n        expected_data = {}\n        expected_headers = {\n            \"X-Goog-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Encryption-Key\": key_b64,\n            \"X-Goog-Encryption-Key-Sha256\": key_hash_b64,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=encrypted,\n        )\n\n    def test_rewrite_same_name_no_key_new_key_w_token(self):\n        blob_name = \"blob\"\n        source_key = b\"01234567890123456789012345678901\"  # 32 bytes\n        source_key_b64 = base64.b64encode(source_key).rstrip().decode(\"ascii\")\n        source_key_hash = hashlib.sha256(source_key).digest()\n        source_key_hash_b64 = base64.b64encode(source_key_hash).rstrip().decode(\"ascii\")\n        dest_key = b\"90123456789012345678901234567890\"  # 32 bytes\n        dest_key_b64 = base64.b64encode(dest_key).rstrip().decode(\"ascii\")\n        dest_key_hash = hashlib.sha256(dest_key).digest()\n        dest_key_hash_b64 = base64.b64encode(dest_key_hash).rstrip().decode(\"ascii\")\n        previous_token = \"TOKEN\"\n        bytes_rewritten = object_size = 52\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": True,\n            \"resource\": {\"etag\": \"DEADBEEF\"},\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source = self._make_one(blob_name, bucket=bucket, encryption_key=source_key)\n        dest = self._make_one(blob_name, bucket=bucket, encryption_key=dest_key)\n\n        token, rewritten, size = dest.rewrite(source, token=previous_token)\n\n        self.assertIsNone(token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = f\"/b/name/o/{blob_name}/rewriteTo/b/name/o/{blob_name}\"\n        expected_data = {}\n        expected_query_params = {\"rewriteToken\": previous_token}\n        expected_headers = {\n            \"X-Goog-Copy-Source-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Copy-Source-Encryption-Key\": source_key_b64,\n            \"X-Goog-Copy-Source-Encryption-Key-Sha256\": source_key_hash_b64,\n            \"X-Goog-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Encryption-Key\": dest_key_b64,\n            \"X-Goog-Encryption-Key-Sha256\": dest_key_hash_b64,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=dest,\n        )\n\n    def test_rewrite_same_name_w_old_key_new_kms_key(self):\n        blob_name = \"blob\"\n        source_key = b\"01234567890123456789012345678901\"  # 32 bytes\n        source_key_b64 = base64.b64encode(source_key).rstrip().decode(\"ascii\")\n        source_key_hash = hashlib.sha256(source_key).digest()\n        source_key_hash_b64 = base64.b64encode(source_key_hash).rstrip().decode(\"ascii\")\n        dest_kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        bytes_rewritten = object_size = 42\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": True,\n            \"resource\": {\"etag\": \"DEADBEEF\"},\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source = self._make_one(blob_name, bucket=bucket, encryption_key=source_key)\n        dest = self._make_one(blob_name, bucket=bucket, kms_key_name=dest_kms_resource)\n\n        token, rewritten, size = dest.rewrite(source)\n\n        self.assertIsNone(token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = f\"/b/name/o/{blob_name}/rewriteTo/b/name/o/{blob_name}\"\n        expected_data = {\"kmsKeyName\": dest_kms_resource}\n        expected_query_params = {\"destinationKmsKeyName\": dest_kms_resource}\n        expected_headers = {\n            \"X-Goog-Copy-Source-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Copy-Source-Encryption-Key\": source_key_b64,\n            \"X-Goog-Copy-Source-Encryption-Key-Sha256\": source_key_hash_b64,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=dest,\n        )\n\n    def test_rewrite_same_name_w_kms_key_w_version(self):\n        blob_name = \"blob\"\n        source_key = b\"01234567890123456789012345678901\"  # 32 bytes\n        source_key_b64 = base64.b64encode(source_key).rstrip().decode(\"ascii\")\n        source_key_hash = hashlib.sha256(source_key).digest()\n        source_key_hash_b64 = base64.b64encode(source_key_hash).rstrip().decode(\"ascii\")\n        dest_kms_resource = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n            \"cryptoKeyVersions/1\"\n        )\n        bytes_rewritten = object_size = 42\n        api_response = {\n            \"totalBytesRewritten\": bytes_rewritten,\n            \"objectSize\": object_size,\n            \"done\": True,\n            \"resource\": {\"etag\": \"DEADBEEF\"},\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = _Bucket(client=client)\n        source = self._make_one(blob_name, bucket=bucket, encryption_key=source_key)\n        dest = self._make_one(blob_name, bucket=bucket, kms_key_name=dest_kms_resource)\n\n        token, rewritten, size = dest.rewrite(source)\n\n        self.assertIsNone(token)\n        self.assertEqual(rewritten, bytes_rewritten)\n        self.assertEqual(size, object_size)\n\n        expected_path = f\"/b/name/o/{blob_name}/rewriteTo/b/name/o/{blob_name}\"\n        expected_data = {\"kmsKeyName\": dest_kms_resource}\n        # The kmsKeyName version value can't be used in the rewrite request,\n        # so the client instead ignores it.\n        expected_query_params = {}\n        expected_headers = {\n            \"X-Goog-Copy-Source-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Copy-Source-Encryption-Key\": source_key_b64,\n            \"X-Goog-Copy-Source-Encryption-Key-Sha256\": source_key_hash_b64,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=dest,\n        )\n\n    def _update_storage_class_multi_pass_helper(self, **kw):\n        blob_name = \"blob-name\"\n        storage_class = \"NEARLINE\"\n        rewrite_token = \"TOKEN\"\n        bytes_rewritten = 42\n        object_size = 84\n        client = mock.Mock(spec=[])\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.rewrite = mock.Mock(spec=[])\n        blob.rewrite.side_effect = [\n            (rewrite_token, bytes_rewritten, object_size),\n            (None, object_size, object_size),\n        ]\n\n        expected_i_g_m = kw.get(\"if_generation_match\")\n        expected_i_g_n_m = kw.get(\"if_generation_not_match\")\n        expected_i_m_m = kw.get(\"if_metageneration_match\")\n        expected_i_m_n_m = kw.get(\"if_metageneration_not_match\")\n        expected_i_s_g_m = kw.get(\"if_source_generation_match\")\n        expected_i_s_g_n_m = kw.get(\"if_source_generation_not_match\")\n        expected_i_s_m_m = kw.get(\"if_source_metageneration_match\")\n        expected_i_s_m_n_m = kw.get(\"if_source_metageneration_not_match\")\n        expected_timeout = kw.get(\"timeout\", self._get_default_timeout())\n        expected_retry = kw.get(\"retry\", DEFAULT_RETRY_IF_GENERATION_SPECIFIED)\n\n        blob.update_storage_class(storage_class, **kw)\n\n        self.assertEqual(blob.storage_class, storage_class)\n\n        call1 = mock.call(\n            blob,\n            if_generation_match=expected_i_g_m,\n            if_generation_not_match=expected_i_g_n_m,\n            if_metageneration_match=expected_i_m_m,\n            if_metageneration_not_match=expected_i_m_n_m,\n            if_source_generation_match=expected_i_s_g_m,\n            if_source_generation_not_match=expected_i_s_g_n_m,\n            if_source_metageneration_match=expected_i_s_m_m,\n            if_source_metageneration_not_match=expected_i_s_m_n_m,\n            timeout=expected_timeout,\n            retry=expected_retry,\n        )\n        call2 = mock.call(\n            blob,\n            token=rewrite_token,\n            if_generation_match=expected_i_g_m,\n            if_generation_not_match=expected_i_g_n_m,\n            if_metageneration_match=expected_i_m_m,\n            if_metageneration_not_match=expected_i_m_n_m,\n            if_source_generation_match=expected_i_s_g_m,\n            if_source_generation_not_match=expected_i_s_g_n_m,\n            if_source_metageneration_match=expected_i_s_m_m,\n            if_source_metageneration_not_match=expected_i_s_m_n_m,\n            timeout=expected_timeout,\n            retry=expected_retry,\n        )\n        blob.rewrite.assert_has_calls([call1, call2])\n\n    def test_update_storage_class_multi_pass_w_defaults(self):\n        self._update_storage_class_multi_pass_helper()\n\n    def test_update_storage_class_multi_pass_w_i_g_m(self):\n        generation = 16\n        self._update_storage_class_multi_pass_helper(if_generation_match=generation)\n\n    def test_update_storage_class_multi_pass_w_i_g_n_m(self):\n        generation = 16\n        self._update_storage_class_multi_pass_helper(if_generation_not_match=generation)\n\n    def test_update_storage_class_multi_pass_w_i_m_m(self):\n        metageneration = 16\n        self._update_storage_class_multi_pass_helper(\n            if_metageneration_match=metageneration,\n        )\n\n    def test_update_storage_class_multi_pass_w_i_m_n_m(self):\n        metageneration = 16\n        self._update_storage_class_multi_pass_helper(\n            if_metageneration_not_match=metageneration,\n        )\n\n    def test_update_storage_class_multi_pass_w_i_s_g_m(self):\n        generation = 16\n        self._update_storage_class_multi_pass_helper(\n            if_source_generation_match=generation\n        )\n\n    def test_update_storage_class_multi_pass_w_i_s_g_n_m(self):\n        generation = 16\n        self._update_storage_class_multi_pass_helper(\n            if_source_generation_not_match=generation\n        )\n\n    def test_update_storage_class_multi_pass_w_i_s_m_m(self):\n        metageneration = 16\n        self._update_storage_class_multi_pass_helper(\n            if_source_metageneration_match=metageneration,\n        )\n\n    def test_update_storage_class_multi_pass_w_i_s_m_n_m(self):\n        metageneration = 16\n        self._update_storage_class_multi_pass_helper(\n            if_source_metageneration_not_match=metageneration,\n        )\n\n    def test_update_storage_class_multi_pass_w_timeout(self):\n        timeout = 42\n        self._update_storage_class_multi_pass_helper(timeout=timeout)\n\n    def test_update_storage_class_multi_pass_w_retry(self):\n        retry = mock.Mock(spec=[])\n        self._update_storage_class_multi_pass_helper(retry=retry)\n\n    def _update_storage_class_single_pass_helper(self, **kw):\n        blob_name = \"blob-name\"\n        storage_class = \"NEARLINE\"\n        object_size = 84\n        client = mock.Mock(spec=[])\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.rewrite = mock.Mock(spec=[])\n        blob.rewrite.return_value = (None, object_size, object_size)\n\n        expected_i_g_m = kw.get(\"if_generation_match\")\n        expected_i_g_n_m = kw.get(\"if_generation_not_match\")\n        expected_i_m_m = kw.get(\"if_metageneration_match\")\n        expected_i_m_n_m = kw.get(\"if_metageneration_not_match\")\n        expected_i_s_g_m = kw.get(\"if_source_generation_match\")\n        expected_i_s_g_n_m = kw.get(\"if_source_generation_not_match\")\n        expected_i_s_m_m = kw.get(\"if_source_metageneration_match\")\n        expected_i_s_m_n_m = kw.get(\"if_source_metageneration_not_match\")\n        expected_timeout = kw.get(\"timeout\", self._get_default_timeout())\n        expected_retry = kw.get(\"retry\", DEFAULT_RETRY_IF_GENERATION_SPECIFIED)\n\n        blob.update_storage_class(storage_class, **kw)\n\n        self.assertEqual(blob.storage_class, storage_class)\n\n        blob.rewrite.assert_called_once_with(\n            blob,\n            if_generation_match=expected_i_g_m,\n            if_generation_not_match=expected_i_g_n_m,\n            if_metageneration_match=expected_i_m_m,\n            if_metageneration_not_match=expected_i_m_n_m,\n            if_source_generation_match=expected_i_s_g_m,\n            if_source_generation_not_match=expected_i_s_g_n_m,\n            if_source_metageneration_match=expected_i_s_m_m,\n            if_source_metageneration_not_match=expected_i_s_m_n_m,\n            timeout=expected_timeout,\n            retry=expected_retry,\n        )\n\n    def test_update_storage_class_single_pass_w_defaults(self):\n        self._update_storage_class_single_pass_helper()\n\n    def test_update_storage_class_single_pass_w_i_g_m(self):\n        generation = 16\n        self._update_storage_class_single_pass_helper(if_generation_match=generation)\n\n    def test_update_storage_class_single_pass_w_i_g_n_m(self):\n        generation = 16\n        self._update_storage_class_single_pass_helper(\n            if_generation_not_match=generation\n        )\n\n    def test_update_storage_class_single_pass_w_i_m_m(self):\n        metageneration = 16\n        self._update_storage_class_single_pass_helper(\n            if_metageneration_match=metageneration,\n        )\n\n    def test_update_storage_class_single_pass_w_i_m_n_m(self):\n        metageneration = 16\n        self._update_storage_class_single_pass_helper(\n            if_metageneration_not_match=metageneration,\n        )\n\n    def test_update_storage_class_single_pass_w_i_s_g_m(self):\n        generation = 16\n        self._update_storage_class_single_pass_helper(\n            if_source_generation_match=generation\n        )\n\n    def test_update_storage_class_single_pass_w_i_s_g_n_m(self):\n        generation = 16\n        self._update_storage_class_single_pass_helper(\n            if_source_generation_not_match=generation\n        )\n\n    def test_update_storage_class_single_pass_w_i_s_m_m(self):\n        metageneration = 16\n        self._update_storage_class_single_pass_helper(\n            if_source_metageneration_match=metageneration,\n        )\n\n    def test_update_storage_class_single_pass_w_i_s_m_n_m(self):\n        metageneration = 16\n        self._update_storage_class_single_pass_helper(\n            if_source_metageneration_not_match=metageneration,\n        )\n\n    def test_update_storage_class_single_pass_w_timeout(self):\n        timeout = 42\n        self._update_storage_class_single_pass_helper(timeout=timeout)\n\n    def test_update_storage_class_single_pass_w_retry(self):\n        retry = mock.Mock(spec=[])\n        self._update_storage_class_single_pass_helper(retry=retry)\n\n    def test_update_storage_class_invalid(self):\n        from google.cloud.exceptions import BadRequest\n\n        storage_class = \"BOGUS\"\n        blob_name = \"blob-name\"\n        client = mock.Mock(spec=[])\n        bucket = _Bucket(client=client)\n        blob = self._make_one(blob_name, bucket=bucket)\n        blob.rewrite = mock.Mock(spec=[])\n        blob.rewrite.side_effect = BadRequest(\"Invalid storage class\")\n\n        with self.assertRaises(BadRequest):\n            blob.update_storage_class(storage_class)\n\n        # Test that invalid classes are allowed without client side validation.\n        # Fall back to server side validation and errors.\n        self.assertEqual(blob.storage_class, storage_class)\n\n        blob.rewrite.assert_called_once_with(\n            blob,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            if_source_generation_match=None,\n            if_source_generation_not_match=None,\n            if_source_metageneration_match=None,\n            if_source_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n\n    def test_cache_control_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        CACHE_CONTROL = \"no-cache\"\n        properties = {\"cacheControl\": CACHE_CONTROL}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.cache_control, CACHE_CONTROL)\n\n    def test_cache_control_setter(self):\n        BLOB_NAME = \"blob-name\"\n        CACHE_CONTROL = \"no-cache\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.cache_control)\n        blob.cache_control = CACHE_CONTROL\n        self.assertEqual(blob.cache_control, CACHE_CONTROL)\n\n    def test_component_count(self):\n        BUCKET = object()\n        COMPONENT_COUNT = 42\n        blob = self._make_one(\n            \"blob-name\", bucket=BUCKET, properties={\"componentCount\": COMPONENT_COUNT}\n        )\n        self.assertEqual(blob.component_count, COMPONENT_COUNT)\n\n    def test_component_count_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.component_count)\n\n    def test_component_count_string_val(self):\n        BUCKET = object()\n        COMPONENT_COUNT = 42\n        blob = self._make_one(\n            \"blob-name\",\n            bucket=BUCKET,\n            properties={\"componentCount\": str(COMPONENT_COUNT)},\n        )\n        self.assertEqual(blob.component_count, COMPONENT_COUNT)\n\n    def test_content_disposition_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        CONTENT_DISPOSITION = \"Attachment; filename=example.jpg\"\n        properties = {\"contentDisposition\": CONTENT_DISPOSITION}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.content_disposition, CONTENT_DISPOSITION)\n\n    def test_content_disposition_setter(self):\n        BLOB_NAME = \"blob-name\"\n        CONTENT_DISPOSITION = \"Attachment; filename=example.jpg\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.content_disposition)\n        blob.content_disposition = CONTENT_DISPOSITION\n        self.assertEqual(blob.content_disposition, CONTENT_DISPOSITION)\n\n    def test_content_encoding_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        CONTENT_ENCODING = \"gzip\"\n        properties = {\"contentEncoding\": CONTENT_ENCODING}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.content_encoding, CONTENT_ENCODING)\n\n    def test_content_encoding_setter(self):\n        BLOB_NAME = \"blob-name\"\n        CONTENT_ENCODING = \"gzip\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.content_encoding)\n        blob.content_encoding = CONTENT_ENCODING\n        self.assertEqual(blob.content_encoding, CONTENT_ENCODING)\n\n    def test_content_language_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        CONTENT_LANGUAGE = \"pt-BR\"\n        properties = {\"contentLanguage\": CONTENT_LANGUAGE}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.content_language, CONTENT_LANGUAGE)\n\n    def test_content_language_setter(self):\n        BLOB_NAME = \"blob-name\"\n        CONTENT_LANGUAGE = \"pt-BR\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.content_language)\n        blob.content_language = CONTENT_LANGUAGE\n        self.assertEqual(blob.content_language, CONTENT_LANGUAGE)\n\n    def test_content_type_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        CONTENT_TYPE = \"image/jpeg\"\n        properties = {\"contentType\": CONTENT_TYPE}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.content_type, CONTENT_TYPE)\n\n    def test_content_type_setter(self):\n        BLOB_NAME = \"blob-name\"\n        CONTENT_TYPE = \"image/jpeg\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.content_type)\n        blob.content_type = CONTENT_TYPE\n        self.assertEqual(blob.content_type, CONTENT_TYPE)\n\n    def test_crc32c_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        CRC32C = \"DEADBEEF\"\n        properties = {\"crc32c\": CRC32C}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.crc32c, CRC32C)\n\n    def test_crc32c_setter(self):\n        BLOB_NAME = \"blob-name\"\n        CRC32C = \"DEADBEEF\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.crc32c)\n        blob.crc32c = CRC32C\n        self.assertEqual(blob.crc32c, CRC32C)\n\n    def test_etag(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        ETAG = \"ETAG\"\n        properties = {\"etag\": ETAG}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.etag, ETAG)\n\n    def test_event_based_hold_getter_missing(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertIsNone(blob.event_based_hold)\n\n    def test_event_based_hold_getter_false(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {\"eventBasedHold\": False}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertFalse(blob.event_based_hold)\n\n    def test_event_based_hold_getter_true(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {\"eventBasedHold\": True}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertTrue(blob.event_based_hold)\n\n    def test_event_based_hold_setter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.event_based_hold)\n        blob.event_based_hold = True\n        self.assertEqual(blob.event_based_hold, True)\n\n    def test_generation(self):\n        BUCKET = object()\n        GENERATION = 42\n        blob = self._make_one(\n            \"blob-name\", bucket=BUCKET, properties={\"generation\": GENERATION}\n        )\n        self.assertEqual(blob.generation, GENERATION)\n\n    def test_generation_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.generation)\n\n    def test_generation_string_val(self):\n        BUCKET = object()\n        GENERATION = 42\n        blob = self._make_one(\n            \"blob-name\", bucket=BUCKET, properties={\"generation\": str(GENERATION)}\n        )\n        self.assertEqual(blob.generation, GENERATION)\n\n    def test_id(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        ID = \"ID\"\n        properties = {\"id\": ID}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.id, ID)\n\n    def test_md5_hash_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        MD5_HASH = \"DEADBEEF\"\n        properties = {\"md5Hash\": MD5_HASH}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.md5_hash, MD5_HASH)\n\n    def test_md5_hash_setter(self):\n        BLOB_NAME = \"blob-name\"\n        MD5_HASH = \"DEADBEEF\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.md5_hash)\n        blob.md5_hash = MD5_HASH\n        self.assertEqual(blob.md5_hash, MD5_HASH)\n\n    def test_media_link(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        MEDIA_LINK = \"http://example.com/media/\"\n        properties = {\"mediaLink\": MEDIA_LINK}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.media_link, MEDIA_LINK)\n\n    def test_metadata_getter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        METADATA = {\"foo\": \"Foo\"}\n        properties = {\"metadata\": METADATA}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.metadata, METADATA)\n\n    def test_metadata_setter(self):\n        BLOB_NAME = \"blob-name\"\n        METADATA = {\"foo\": \"Foo\"}\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.metadata)\n        blob.metadata = METADATA\n        self.assertEqual(blob.metadata, METADATA)\n        self.assertIn(\"metadata\", blob._changes)\n\n    def test_metadata_setter_w_nan(self):\n        BLOB_NAME = \"blob-name\"\n        METADATA = {\"foo\": float(\"nan\")}\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.metadata)\n        blob.metadata = METADATA\n        value = blob.metadata[\"foo\"]\n        self.assertIsInstance(value, str)\n        self.assertIn(\"metadata\", blob._changes)\n\n    def test_metageneration(self):\n        BUCKET = object()\n        METAGENERATION = 42\n        blob = self._make_one(\n            \"blob-name\", bucket=BUCKET, properties={\"metageneration\": METAGENERATION}\n        )\n        self.assertEqual(blob.metageneration, METAGENERATION)\n\n    def test_metageneration_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.metageneration)\n\n    def test_metageneration_string_val(self):\n        BUCKET = object()\n        METAGENERATION = 42\n        blob = self._make_one(\n            \"blob-name\",\n            bucket=BUCKET,\n            properties={\"metageneration\": str(METAGENERATION)},\n        )\n        self.assertEqual(blob.metageneration, METAGENERATION)\n\n    def test_owner(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        OWNER = {\"entity\": \"project-owner-12345\", \"entityId\": \"23456\"}\n        properties = {\"owner\": OWNER}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        owner = blob.owner\n        self.assertEqual(owner[\"entity\"], \"project-owner-12345\")\n        self.assertEqual(owner[\"entityId\"], \"23456\")\n\n    def test_retention_expiration_time(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        TIME_CREATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"retentionExpirationTime\": TIME_CREATED}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.retention_expiration_time, TIMESTAMP)\n\n    def test_retention_expiration_time_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.retention_expiration_time)\n\n    def test_self_link(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        SELF_LINK = \"http://example.com/self/\"\n        properties = {\"selfLink\": SELF_LINK}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.self_link, SELF_LINK)\n\n    def test_size(self):\n        BUCKET = object()\n        SIZE = 42\n        blob = self._make_one(\"blob-name\", bucket=BUCKET, properties={\"size\": SIZE})\n        self.assertEqual(blob.size, SIZE)\n\n    def test_size_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.size)\n\n    def test_size_string_val(self):\n        BUCKET = object()\n        SIZE = 42\n        blob = self._make_one(\n            \"blob-name\", bucket=BUCKET, properties={\"size\": str(SIZE)}\n        )\n        self.assertEqual(blob.size, SIZE)\n\n    def test_storage_class_getter(self):\n        blob_name = \"blob-name\"\n        bucket = _Bucket()\n        storage_class = \"COLDLINE\"\n        properties = {\"storageClass\": storage_class}\n        blob = self._make_one(blob_name, bucket=bucket, properties=properties)\n        self.assertEqual(blob.storage_class, storage_class)\n\n    def test_storage_class_setter(self):\n        blob_name = \"blob-name\"\n        bucket = _Bucket()\n        storage_class = \"COLDLINE\"\n        blob = self._make_one(blob_name, bucket=bucket)\n        self.assertIsNone(blob.storage_class)\n        blob.storage_class = storage_class\n        self.assertEqual(blob.storage_class, storage_class)\n        self.assertEqual(blob._properties, {\"storageClass\": storage_class})\n\n    def test_temporary_hold_getter_missing(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertIsNone(blob.temporary_hold)\n\n    def test_temporary_hold_getter_false(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {\"temporaryHold\": False}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertFalse(blob.temporary_hold)\n\n    def test_temporary_hold_getter_true(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        properties = {\"temporaryHold\": True}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertTrue(blob.temporary_hold)\n\n    def test_temporary_hold_setter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.temporary_hold)\n        blob.temporary_hold = True\n        self.assertEqual(blob.temporary_hold, True)\n\n    def test_time_deleted(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        TIME_DELETED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"timeDeleted\": TIME_DELETED}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.time_deleted, TIMESTAMP)\n\n    def test_time_deleted_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.time_deleted)\n\n    def test_time_created(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        TIME_CREATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"timeCreated\": TIME_CREATED}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.time_created, TIMESTAMP)\n\n    def test_time_created_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.time_created)\n\n    def test_updated(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        UPDATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"updated\": UPDATED}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.updated, TIMESTAMP)\n\n    def test_updated_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.updated)\n\n    def test_custom_time_getter(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        TIME_CREATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"customTime\": TIME_CREATED}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.custom_time, TIMESTAMP)\n\n    def test_custom_time_setter(self):\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsNone(blob.custom_time)\n        blob.custom_time = TIMESTAMP\n        self.assertEqual(blob.custom_time, TIMESTAMP)\n        self.assertIn(\"customTime\", blob._changes)\n\n    def test_custom_time_setter_none_value(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        TIME_CREATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"customTime\": TIME_CREATED}\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.custom_time, TIMESTAMP)\n        blob.custom_time = None\n        self.assertIsNone(blob.custom_time)\n\n    def test_custom_time_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.custom_time)\n\n    def test_soft_hard_delete_time_getter(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        soft_timstamp = datetime.datetime(2024, 1, 5, 20, 34, 37, tzinfo=_UTC)\n        soft_delete = soft_timstamp.strftime(_RFC3339_MICROS)\n        hard_timstamp = datetime.datetime(2024, 1, 15, 20, 34, 37, tzinfo=_UTC)\n        hard_delete = hard_timstamp.strftime(_RFC3339_MICROS)\n        properties = {\n            \"softDeleteTime\": soft_delete,\n            \"hardDeleteTime\": hard_delete,\n        }\n        blob = self._make_one(BLOB_NAME, bucket=bucket, properties=properties)\n        self.assertEqual(blob.soft_delete_time, soft_timstamp)\n        self.assertEqual(blob.hard_delete_time, hard_timstamp)\n\n    def test_soft_hard_delte_time_unset(self):\n        BUCKET = object()\n        blob = self._make_one(\"blob-name\", bucket=BUCKET)\n        self.assertIsNone(blob.soft_delete_time)\n        self.assertIsNone(blob.hard_delete_time)\n\n    def test_from_string_w_valid_uri(self):\n        from google.cloud.storage.blob import Blob\n\n        client = self._make_client()\n        basic_uri = \"gs://bucket_name/b\"\n        blob = Blob.from_string(basic_uri, client)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.client, client)\n        self.assertEqual(blob.name, \"b\")\n        self.assertEqual(blob.bucket.name, \"bucket_name\")\n\n        nested_uri = \"gs://bucket_name/path1/path2/b#name\"\n        blob = Blob.from_string(nested_uri, client)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.client, client)\n        self.assertEqual(blob.name, \"path1/path2/b#name\")\n        self.assertEqual(blob.bucket.name, \"bucket_name\")\n\n    def test_from_string_w_invalid_uri(self):\n        from google.cloud.storage.blob import Blob\n\n        client = self._make_client()\n\n        with pytest.raises(ValueError):\n            Blob.from_string(\"http://bucket_name/b\", client)\n\n    def test_from_string_w_domain_name_bucket(self):\n        from google.cloud.storage.blob import Blob\n\n        client = self._make_client()\n        uri = \"gs://buckets.example.com/b\"\n        blob = Blob.from_string(uri, client)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.client, client)\n        self.assertEqual(blob.name, \"b\")\n        self.assertEqual(blob.bucket.name, \"buckets.example.com\")\n\n    def test_open(self):\n        from io import TextIOWrapper\n        from google.cloud.storage.fileio import BlobReader\n        from google.cloud.storage.fileio import BlobWriter\n\n        blob_name = \"blob-name\"\n        client = self._make_client()\n        bucket = _Bucket(client)\n        blob = self._make_one(blob_name, bucket=bucket)\n\n        f = blob.open(\"r\")\n        self.assertEqual(type(f), TextIOWrapper)\n        self.assertEqual(type(f.buffer), BlobReader)\n        f = blob.open(\"rt\")\n        self.assertEqual(type(f), TextIOWrapper)\n        self.assertEqual(type(f.buffer), BlobReader)\n        f = blob.open(\"rb\")\n        self.assertEqual(type(f), BlobReader)\n        f = blob.open(\"w\")\n        self.assertEqual(type(f), TextIOWrapper)\n        self.assertEqual(type(f.buffer), BlobWriter)\n        f = blob.open(\"wt\")\n        self.assertEqual(type(f), TextIOWrapper)\n        self.assertEqual(type(f.buffer), BlobWriter)\n        f = blob.open(\"wb\")\n        self.assertEqual(type(f), BlobWriter)\n        f = blob.open(\"wb\", ignore_flush=True)\n        self.assertTrue(f._ignore_flush)\n\n        with self.assertRaises(NotImplementedError):\n            blob.open(\"a\")\n        with self.assertRaises(ValueError):\n            blob.open(\"rb\", encoding=\"utf-8\")\n        with self.assertRaises(ValueError):\n            blob.open(\"wb\", encoding=\"utf-8\")\n        with self.assertRaises(ValueError):\n            blob.open(\"r\", ignore_flush=True)\n        with self.assertRaises(ValueError):\n            blob.open(\"rb\", ignore_flush=True)\n        with self.assertRaises(ValueError):\n            blob.open(\"w\", ignore_flush=False)\n\n    def test_downloads_w_client_custom_headers(self):\n        import google.auth.credentials\n        from google.cloud.storage import Client\n\n        custom_headers = {\n            \"x-goog-custom-audit-foo\": \"bar\",\n            \"x-goog-custom-audit-user\": \"baz\",\n        }\n        credentials = mock.Mock(\n            spec=google.auth.credentials.Credentials,\n            universe_domain=_DEFAULT_UNIVERSE_DOMAIN,\n        )\n        client = Client(\n            project=\"project\", credentials=credentials, extra_headers=custom_headers\n        )\n        blob = self._make_one(\"blob-name\", bucket=_Bucket(client))\n        file_obj = io.BytesIO()\n\n        downloads = {\n            client.download_blob_to_file: (blob, file_obj),\n            blob.download_to_file: (file_obj,),\n            blob.download_as_bytes: (),\n        }\n        for method, args in downloads.items():\n            with mock.patch.object(blob, \"_do_download\"):\n                method(*args)\n                blob._do_download.assert_called()\n                called_headers = blob._do_download.call_args.args[-4]\n                self.assertIsInstance(called_headers, dict)\n                self.assertLessEqual(custom_headers.items(), called_headers.items())\n\n    def test_object_lock_retention_configuration(self):\n        from google.cloud.storage.blob import Retention\n\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET)\n\n        retention = blob.retention\n\n        self.assertIsInstance(retention, Retention)\n        self.assertIs(retention.blob, blob)\n        self.assertIsNone(retention.mode)\n        self.assertIsNone(retention.retain_until_time)\n        self.assertIsNone(retention.retention_expiration_time)\n\n    def test_object_lock_retention_configuration_w_entry(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n        from google.cloud.storage.blob import Retention\n\n        now = _NOW(_UTC)\n        expiration_time = now + datetime.timedelta(hours=1)\n        expiration = expiration_time.strftime(_RFC3339_MICROS)\n        mode = \"Locked\"\n        properties = {\n            \"retention\": {\n                \"mode\": mode,\n                \"retainUntilTime\": expiration,\n                \"retentionExpirationTime\": expiration,\n            }\n        }\n        BLOB_NAME = \"blob-name\"\n        BUCKET = object()\n        blob = self._make_one(BLOB_NAME, bucket=BUCKET, properties=properties)\n        retention_config = Retention(\n            blob=blob,\n            mode=mode,\n            retain_until_time=expiration_time,\n            retention_expiration_time=expiration_time,\n        )\n\n        retention = blob.retention\n\n        self.assertIsInstance(retention, Retention)\n        self.assertEqual(retention, retention_config)\n        self.assertIs(retention.blob, blob)\n        self.assertEqual(retention.mode, mode)\n        self.assertEqual(retention.retain_until_time, expiration_time)\n        self.assertEqual(retention.retention_expiration_time, expiration_time)\n\n    def test_object_lock_retention_configuration_setter(self):\n        from google.cloud.storage.blob import Retention\n\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket()\n        blob = self._make_one(BLOB_NAME, bucket=bucket)\n        self.assertIsInstance(blob.retention, Retention)\n\n        mode = \"Locked\"\n        now = _NOW(_UTC)\n        expiration_time = now + datetime.timedelta(hours=1)\n        retention_config = Retention(\n            blob=blob, mode=mode, retain_until_time=expiration_time\n        )\n        blob.retention.mode = mode\n        blob.retention.retain_until_time = expiration_time\n        self.assertEqual(blob.retention, retention_config)\n        self.assertIn(\"retention\", blob._changes)\n        blob.retention.retain_until_time = None\n        self.assertIsNone(blob.retention.retain_until_time)\n        self.assertIn(\"retention\", blob._changes)\n\n\nclass Test__quote(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kw):\n        from google.cloud.storage.blob import _quote\n\n        return _quote(*args, **kw)\n\n    def test_bytes(self):\n        quoted = self._call_fut(b\"\\xDE\\xAD\\xBE\\xEF\")\n        self.assertEqual(quoted, \"%DE%AD%BE%EF\")\n\n    def test_unicode(self):\n        helicopter = \"\\U0001f681\"\n        quoted = self._call_fut(helicopter)\n        self.assertEqual(quoted, \"%F0%9F%9A%81\")\n\n    def test_bad_type(self):\n        with self.assertRaises(TypeError):\n            self._call_fut(None)\n\n    def test_w_slash_default(self):\n        with_slash = \"foo/bar/baz\"\n        quoted = self._call_fut(with_slash)\n        self.assertEqual(quoted, \"foo%2Fbar%2Fbaz\")\n\n    def test_w_slash_w_safe(self):\n        with_slash = \"foo/bar/baz\"\n        quoted_safe = self._call_fut(with_slash, safe=b\"/\")\n        self.assertEqual(quoted_safe, with_slash)\n\n    def test_w_tilde(self):\n        with_tilde = \"bam~qux\"\n        quoted = self._call_fut(with_tilde, safe=b\"~\")\n        self.assertEqual(quoted, with_tilde)\n\n\nclass Test__maybe_rewind(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage.blob import _maybe_rewind\n\n        return _maybe_rewind(*args, **kwargs)\n\n    def test_default(self):\n        stream = mock.Mock(spec=[\"seek\"])\n        ret_val = self._call_fut(stream)\n        self.assertIsNone(ret_val)\n\n        stream.seek.assert_not_called()\n\n    def test_do_not_rewind(self):\n        stream = mock.Mock(spec=[\"seek\"])\n        ret_val = self._call_fut(stream, rewind=False)\n        self.assertIsNone(ret_val)\n\n        stream.seek.assert_not_called()\n\n    def test_do_rewind(self):\n        stream = mock.Mock(spec=[\"seek\"])\n        ret_val = self._call_fut(stream, rewind=True)\n        self.assertIsNone(ret_val)\n\n        stream.seek.assert_called_once_with(0, os.SEEK_SET)\n\n\nclass Test__raise_from_invalid_response(unittest.TestCase):\n    @staticmethod\n    def _call_fut(error):\n        from google.cloud.storage.blob import _raise_from_invalid_response\n\n        return _raise_from_invalid_response(error)\n\n    def _helper(self, message, code=http.client.BAD_REQUEST, reason=None, args=()):\n        import requests\n\n        from google.resumable_media import InvalidResponse\n        from google.api_core import exceptions\n\n        response = requests.Response()\n        response.request = requests.Request(\"GET\", \"http://example.com\").prepare()\n        response._content = reason\n        response.status_code = code\n        error = InvalidResponse(response, message, *args)\n\n        with self.assertRaises(exceptions.GoogleAPICallError) as exc_info:\n            self._call_fut(error)\n\n        return exc_info\n\n    def test_default(self):\n        message = \"Failure\"\n        exc_info = self._helper(message)\n        expected = f\"GET http://example.com/: {message}\"\n        self.assertEqual(exc_info.exception.message, expected)\n        self.assertEqual(exc_info.exception.errors, [])\n\n    def test_w_206_and_args(self):\n        message = \"Failure\"\n        reason = b\"Not available\"\n        args = (\"one\", \"two\")\n        exc_info = self._helper(\n            message, code=http.client.PARTIAL_CONTENT, reason=reason, args=args\n        )\n        expected = \"GET http://example.com/: {}: {}\".format(\n            reason.decode(\"utf-8\"), (message,) + args\n        )\n        self.assertEqual(exc_info.exception.message, expected)\n        self.assertEqual(exc_info.exception.errors, [])\n\n\nclass Test__add_query_parameters(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage.blob import _add_query_parameters\n\n        return _add_query_parameters(*args, **kwargs)\n\n    def test_w_empty_list(self):\n        BASE_URL = \"https://test.example.com/base\"\n        self.assertEqual(self._call_fut(BASE_URL, []), BASE_URL)\n\n    def test_wo_existing_qs(self):\n        BASE_URL = \"https://test.example.com/base\"\n        NV_LIST = [(\"one\", \"One\"), (\"two\", \"Two\")]\n        expected = \"&\".join([f\"{name}={value}\" for name, value in NV_LIST])\n        self.assertEqual(self._call_fut(BASE_URL, NV_LIST), f\"{BASE_URL}?{expected}\")\n\n    def test_w_existing_qs(self):\n        BASE_URL = \"https://test.example.com/base?one=Three\"\n        NV_LIST = [(\"one\", \"One\"), (\"two\", \"Two\")]\n        expected = \"&\".join([f\"{name}={value}\" for name, value in NV_LIST])\n        self.assertEqual(self._call_fut(BASE_URL, NV_LIST), f\"{BASE_URL}&{expected}\")\n\n\nclass _Connection(object):\n    API_BASE_URL = \"http://example.com\"\n    USER_AGENT = \"testing 1.2.3\"\n    user_agent = \"testing 1.2.3\"\n    credentials = object()\n\n\nclass _Bucket(object):\n    def __init__(self, client=None, name=\"name\", user_project=None):\n        if client is None:\n            client = Test_Blob._make_client()\n\n        self.client = client\n        self._blobs = {}\n        self._copied = []\n        self._deleted = []\n        self.name = name\n        self.path = \"/b/\" + name\n        self.user_project = user_project\n\n    def delete_blob(\n        self,\n        blob_name,\n        client=None,\n        generation=None,\n        timeout=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        del self._blobs[blob_name]\n        self._deleted.append(\n            (\n                blob_name,\n                client,\n                generation,\n                timeout,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                retry,\n            )\n        )\n", "tests/unit/test_retry.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom google.cloud.storage import _helpers\n\nimport mock\n\n\nclass Test_should_retry(unittest.TestCase):\n    def _call_fut(self, exc):\n        from google.cloud.storage import retry\n\n        return retry._should_retry(exc)\n\n    def test_w_retryable_transport_error(self):\n        from google.cloud.storage import retry\n        from google.auth.exceptions import TransportError as eTransportError\n        from requests import ConnectionError as rConnectionError\n\n        caught_exc = rConnectionError(\"Remote end closed connection unexpected\")\n        exc = eTransportError(caught_exc)\n        self.assertTrue(retry._should_retry(exc))\n\n    def test_w_retryable_types(self):\n        from google.cloud.storage import retry\n\n        for exc_type in retry._RETRYABLE_TYPES:\n            exc = exc_type(\"testing\")\n            self.assertTrue(self._call_fut(exc))\n\n    def test_w_google_api_call_error_hit(self):\n        from google.api_core import exceptions\n\n        exc = exceptions.GoogleAPICallError(\"testing\")\n        exc.code = 408\n        self.assertTrue(self._call_fut(exc))\n\n    def test_w_google_api_call_error_miss(self):\n        from google.api_core import exceptions\n\n        exc = exceptions.GoogleAPICallError(\"testing\")\n        exc.code = 999\n        self.assertFalse(self._call_fut(exc))\n\n    def test_w_stdlib_error_miss(self):\n        exc = ValueError(\"testing\")\n        self.assertFalse(self._call_fut(exc))\n\n\nclass TestConditionalRetryPolicy(unittest.TestCase):\n    def _make_one(self, retry_policy, conditional_predicate, required_kwargs):\n        from google.cloud.storage import retry\n\n        return retry.ConditionalRetryPolicy(\n            retry_policy, conditional_predicate, required_kwargs\n        )\n\n    def test_ctor(self):\n        retry_policy = mock.Mock()\n        conditional_predicate = mock.Mock()\n        required_kwargs = (\"kwarg\",)\n\n        policy = self._make_one(retry_policy, conditional_predicate, required_kwargs)\n\n        self.assertIs(policy.retry_policy, retry_policy)\n        self.assertIs(policy.conditional_predicate, conditional_predicate)\n        self.assertEqual(policy.required_kwargs, required_kwargs)\n\n    def test_get_retry_policy_if_conditions_met_single_kwarg_hit(self):\n        retry_policy = mock.Mock()\n        conditional_predicate = mock.Mock(return_value=True)\n        required_kwargs = (\"foo\",)\n        policy = self._make_one(retry_policy, conditional_predicate, required_kwargs)\n\n        kwargs = {\"foo\": 1, \"bar\": 2, \"baz\": 3}\n        result = policy.get_retry_policy_if_conditions_met(**kwargs)\n\n        self.assertIs(result, retry_policy)\n\n        conditional_predicate.assert_called_once_with(1)\n\n    def test_get_retry_policy_if_conditions_met_multiple_kwargs_miss(self):\n        retry_policy = mock.Mock()\n        conditional_predicate = mock.Mock(return_value=False)\n        required_kwargs = (\"foo\", \"bar\")\n        policy = self._make_one(retry_policy, conditional_predicate, required_kwargs)\n\n        kwargs = {\"foo\": 1, \"bar\": 2, \"baz\": 3}\n        result = policy.get_retry_policy_if_conditions_met(**kwargs)\n\n        self.assertIsNone(result)\n\n        conditional_predicate.assert_called_once_with(1, 2)\n\n\nclass Test_is_generation_specified(unittest.TestCase):\n    def _call_fut(self, query_params):\n        from google.cloud.storage import retry\n\n        return retry.is_generation_specified(query_params)\n\n    def test_w_empty(self):\n        query_params = {}\n\n        self.assertFalse(self._call_fut(query_params))\n\n    def test_w_generation(self):\n        query_params = {\"generation\": 123}\n\n        self.assertTrue(self._call_fut(query_params))\n\n    def test_wo_generation_w_if_generation_match(self):\n        query_params = {\"ifGenerationMatch\": 123}\n\n        self.assertTrue(self._call_fut(query_params))\n\n\nclass Test_is_metageneration_specified(unittest.TestCase):\n    def _call_fut(self, query_params):\n        from google.cloud.storage import retry\n\n        return retry.is_metageneration_specified(query_params)\n\n    def test_w_empty(self):\n        query_params = {}\n\n        self.assertFalse(self._call_fut(query_params))\n\n    def test_w_if_metageneration_match(self):\n        query_params = {\"ifMetagenerationMatch\": 123}\n\n        self.assertTrue(self._call_fut(query_params))\n\n\nclass Test_is_etag_in_data(unittest.TestCase):\n    def _call_fut(self, data):\n        from google.cloud.storage import retry\n\n        return retry.is_etag_in_data(data)\n\n    def test_w_none(self):\n        data = None\n\n        self.assertFalse(self._call_fut(data))\n\n    def test_w_etag_in_data(self):\n        data = {\"etag\": \"123\"}\n\n        self.assertTrue(self._call_fut(data))\n\n    def test_w_empty_data(self):\n        data = {}\n\n        self.assertFalse(self._call_fut(data))\n\n\nclass Test_default_conditional_retry_policies(unittest.TestCase):\n    def test_is_generation_specified_match_generation_match(self):\n        from google.cloud.storage import retry\n\n        query_dict = {}\n        _helpers._add_generation_match_parameters(query_dict, if_generation_match=1)\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_GENERATION_SPECIFIED\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params=query_dict\n        )\n        self.assertEqual(policy, retry.DEFAULT_RETRY)\n\n    def test_is_generation_specified_match_generation(self):\n        from google.cloud.storage import retry\n\n        query_dict = {\"generation\": 1}\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_GENERATION_SPECIFIED\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params=query_dict\n        )\n        self.assertEqual(policy, retry.DEFAULT_RETRY)\n\n    def test_is_generation_specified_mismatch(self):\n        from google.cloud.storage import retry\n\n        query_dict = {}\n        _helpers._add_generation_match_parameters(query_dict, if_metageneration_match=1)\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_GENERATION_SPECIFIED\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params=query_dict\n        )\n        self.assertEqual(policy, None)\n\n    def test_is_metageneration_specified_match(self):\n        from google.cloud.storage import retry\n\n        query_dict = {}\n        _helpers._add_generation_match_parameters(query_dict, if_metageneration_match=1)\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params=query_dict\n        )\n        self.assertEqual(policy, retry.DEFAULT_RETRY)\n\n    def test_is_metageneration_specified_mismatch(self):\n        from google.cloud.storage import retry\n\n        query_dict = {}\n        _helpers._add_generation_match_parameters(query_dict, if_generation_match=1)\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params=query_dict\n        )\n        self.assertEqual(policy, None)\n\n    def test_is_etag_in_json_etag_match(self):\n        from google.cloud.storage import retry\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_ETAG_IN_JSON\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params={\"ifGenerationMatch\": 1}, data='{\"etag\": \"12345678\"}'\n        )\n        self.assertEqual(policy, retry.DEFAULT_RETRY)\n\n    def test_is_etag_in_json_mismatch(self):\n        from google.cloud.storage import retry\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_ETAG_IN_JSON\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params={\"ifGenerationMatch\": 1}, data=\"{}\"\n        )\n        self.assertEqual(policy, None)\n\n    def test_is_meta_or_etag_in_json_invalid(self):\n        from google.cloud.storage import retry\n\n        conditional_policy = retry.DEFAULT_RETRY_IF_ETAG_IN_JSON\n        policy = conditional_policy.get_retry_policy_if_conditions_met(\n            query_params={\"ifGenerationMatch\": 1}, data=\"I am invalid JSON!\"\n        )\n        self.assertEqual(policy, None)\n", "tests/unit/test__http.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom unittest.mock import patch\n\nimport mock\n\nfrom google.cloud.storage import _helpers\n\nGCCL_INVOCATION_TEST_CONST = \"gccl-invocation-id/test-invocation-123\"\n\n\nclass TestConnection(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage._http import Connection\n\n        return Connection\n\n    def _make_one(self, *args, **kw):\n        if \"api_endpoint\" not in kw:\n            kw[\"api_endpoint\"] = \"https://storage.googleapis.com\"\n        return self._get_target_class()(*args, **kw)\n\n    def test_extra_headers(self):\n        import requests\n        from google.cloud import _http as base_http\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        http = mock.create_autospec(requests.Session, instance=True)\n        response = requests.Response()\n        response.status_code = 200\n        data = b\"brent-spiner\"\n        response._content = data\n        http.request.return_value = response\n        client = mock.Mock(_http=http, spec=[\"_http\"])\n\n        conn = self._make_one(client)\n        req_data = \"hey-yoooouuuuu-guuuuuyyssss\"\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            result = conn.api_request(\n                \"GET\", \"/rainbow\", data=req_data, expect_json=False\n            )\n        self.assertEqual(result, data)\n\n        expected_headers = {\n            \"Accept-Encoding\": \"gzip\",\n            base_http.CLIENT_INFO_HEADER: f\"{conn.user_agent} {GCCL_INVOCATION_TEST_CONST}\",\n            \"User-Agent\": conn.user_agent,\n        }\n        expected_uri = conn.build_api_url(\"/rainbow\")\n        http.request.assert_called_once_with(\n            data=req_data,\n            headers=expected_headers,\n            method=\"GET\",\n            url=expected_uri,\n            timeout=_DEFAULT_TIMEOUT,\n        )\n\n    def test_metadata_op_has_client_custom_headers(self):\n        import requests\n        import google.auth.credentials\n        from google.cloud import _http as base_http\n        from google.cloud.storage import Client\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        custom_headers = {\n            \"x-goog-custom-audit-foo\": \"bar\",\n            \"x-goog-custom-audit-user\": \"baz\",\n        }\n        http = mock.create_autospec(requests.Session, instance=True)\n        response = requests.Response()\n        response.status_code = 200\n        data = b\"brent-spiner\"\n        response._content = data\n        http.is_mtls = False\n        http.request.return_value = response\n        credentials = mock.Mock(\n            spec=google.auth.credentials.Credentials,\n            universe_domain=_helpers._DEFAULT_UNIVERSE_DOMAIN,\n        )\n        client = Client(\n            project=\"project\",\n            credentials=credentials,\n            _http=http,\n            extra_headers=custom_headers,\n        )\n        req_data = \"hey-yoooouuuuu-guuuuuyyssss\"\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            result = client._connection.api_request(\n                \"GET\", \"/rainbow\", data=req_data, expect_json=False\n            )\n        self.assertEqual(result, data)\n\n        expected_headers = {\n            **custom_headers,\n            \"Accept-Encoding\": \"gzip\",\n            base_http.CLIENT_INFO_HEADER: f\"{client._connection.user_agent} {GCCL_INVOCATION_TEST_CONST}\",\n            \"User-Agent\": client._connection.user_agent,\n        }\n        expected_uri = client._connection.build_api_url(\"/rainbow\")\n        http.request.assert_called_once_with(\n            data=req_data,\n            headers=expected_headers,\n            method=\"GET\",\n            url=expected_uri,\n            timeout=_DEFAULT_TIMEOUT,\n        )\n\n    def test_build_api_url_no_extra_query_params(self):\n        from urllib.parse import parse_qsl\n        from urllib.parse import urlsplit\n\n        conn = self._make_one(object())\n        uri = conn.build_api_url(\"/foo\")\n        scheme, netloc, path, qs, _ = urlsplit(uri)\n        self.assertEqual(f\"{scheme}://{netloc}\", conn.API_BASE_URL)\n        self.assertEqual(path, \"/\".join([\"\", \"storage\", conn.API_VERSION, \"foo\"]))\n        parms = dict(parse_qsl(qs))\n        pretty_print = parms.pop(\"prettyPrint\", \"false\")\n        self.assertEqual(pretty_print, \"false\")\n        self.assertEqual(parms, {})\n\n    def test_build_api_url_w_custom_endpoint(self):\n        from urllib.parse import parse_qsl\n        from urllib.parse import urlsplit\n\n        custom_endpoint = \"https://foo-storage.googleapis.com\"\n        conn = self._make_one(object(), api_endpoint=custom_endpoint)\n        uri = conn.build_api_url(\"/foo\")\n        scheme, netloc, path, qs, _ = urlsplit(uri)\n        self.assertEqual(f\"{scheme}://{netloc}\", custom_endpoint)\n        self.assertEqual(path, \"/\".join([\"\", \"storage\", conn.API_VERSION, \"foo\"]))\n        parms = dict(parse_qsl(qs))\n        pretty_print = parms.pop(\"prettyPrint\", \"false\")\n        self.assertEqual(pretty_print, \"false\")\n        self.assertEqual(parms, {})\n\n    def test_build_api_url_w_extra_query_params(self):\n        from urllib.parse import parse_qsl\n        from urllib.parse import urlsplit\n\n        conn = self._make_one(object())\n        uri = conn.build_api_url(\"/foo\", {\"bar\": \"baz\"})\n        scheme, netloc, path, qs, _ = urlsplit(uri)\n        self.assertEqual(f\"{scheme}://{netloc}\", conn.API_BASE_URL)\n        self.assertEqual(path, \"/\".join([\"\", \"storage\", conn.API_VERSION, \"foo\"]))\n        parms = dict(parse_qsl(qs))\n        self.assertEqual(parms[\"bar\"], \"baz\")\n\n    def test_api_request_no_retry(self):\n        import requests\n\n        http = mock.create_autospec(requests.Session, instance=True)\n        client = mock.Mock(_http=http, spec=[\"_http\"])\n\n        conn = self._make_one(client)\n        response = requests.Response()\n        response.status_code = 200\n        data = b\"brent-spiner\"\n        response._content = data\n        http.request.return_value = response\n\n        req_data = \"hey-yoooouuuuu-guuuuuyyssss\"\n        conn.api_request(\"GET\", \"/rainbow\", data=req_data, expect_json=False)\n        http.request.assert_called_once()\n\n    def test_api_request_basic_retry(self):\n        # For this test, the \"retry\" function will just short-circuit.\n        FAKE_RESPONSE_STRING = \"fake_response\"\n\n        def retry(_):\n            def fake_response():\n                return FAKE_RESPONSE_STRING\n\n            return fake_response\n\n        import requests\n\n        http = mock.create_autospec(requests.Session, instance=True)\n        client = mock.Mock(_http=http, spec=[\"_http\"])\n\n        # Some of this is unnecessary if the test succeeds, but we'll leave it\n        # to ensure a failure produces a less confusing error message.\n        conn = self._make_one(client)\n        response = requests.Response()\n        response.status_code = 200\n        data = b\"brent-spiner\"\n        response._content = data\n        http.request.return_value = response\n\n        req_data = \"hey-yoooouuuuu-guuuuuyyssss\"\n        result = conn.api_request(\n            \"GET\", \"/rainbow\", data=req_data, expect_json=False, retry=retry\n        )\n        http.request.assert_not_called()\n        self.assertEqual(result, FAKE_RESPONSE_STRING)\n\n    def test_api_request_conditional_retry(self):\n        # For this test, the \"retry\" function will short-circuit.\n        FAKE_RESPONSE_STRING = \"fake_response\"\n\n        def retry(_):\n            def fake_response():\n                return FAKE_RESPONSE_STRING\n\n            return fake_response\n\n        conditional_retry_mock = mock.MagicMock()\n        conditional_retry_mock.get_retry_policy_if_conditions_met.return_value = retry\n\n        import requests\n\n        http = mock.create_autospec(requests.Session, instance=True)\n        client = mock.Mock(_http=http, spec=[\"_http\"])\n\n        # Some of this is unnecessary if the test succeeds, but we'll leave it\n        # to ensure a failure produces a less confusing error message.\n        conn = self._make_one(client)\n        response = requests.Response()\n        response.status_code = 200\n        data = b\"brent-spiner\"\n        response._content = data\n        http.request.return_value = response\n\n        req_data = \"hey-yoooouuuuu-guuuuuyyssss\"\n        result = conn.api_request(\n            \"GET\",\n            \"/rainbow\",\n            data=req_data,\n            expect_json=False,\n            retry=conditional_retry_mock,\n        )\n        http.request.assert_not_called()\n        self.assertEqual(result, FAKE_RESPONSE_STRING)\n\n    def test_api_request_conditional_retry_failed(self):\n        conditional_retry_mock = mock.MagicMock()\n        conditional_retry_mock.get_retry_policy_if_conditions_met.return_value = None\n\n        import requests\n\n        http = mock.create_autospec(requests.Session, instance=True)\n        client = mock.Mock(_http=http, spec=[\"_http\"])\n\n        # Some of this is unnecessary if the test succeeds, but we'll leave it\n        # to ensure a failure produces a less confusing error message.\n        conn = self._make_one(client)\n        response = requests.Response()\n        response.status_code = 200\n        data = b\"brent-spiner\"\n        response._content = data\n        http.request.return_value = response\n\n        req_data = \"hey-yoooouuuuu-guuuuuyyssss\"\n        conn.api_request(\n            \"GET\",\n            \"/rainbow\",\n            data=req_data,\n            expect_json=False,\n            retry=conditional_retry_mock,\n        )\n        http.request.assert_called_once()\n\n    def test_mtls(self):\n        client = object()\n\n        conn = self._make_one(client, api_endpoint=None)\n        self.assertEqual(conn.ALLOW_AUTO_SWITCH_TO_MTLS_URL, True)\n        self.assertEqual(conn.API_BASE_URL, \"https://storage.googleapis.com\")\n        self.assertEqual(conn.API_BASE_MTLS_URL, \"https://storage.mtls.googleapis.com\")\n\n        conn = self._make_one(client, api_endpoint=\"http://foo\")\n        self.assertEqual(conn.ALLOW_AUTO_SWITCH_TO_MTLS_URL, False)\n        self.assertEqual(conn.API_BASE_URL, \"http://foo\")\n        self.assertEqual(conn.API_BASE_MTLS_URL, \"https://storage.mtls.googleapis.com\")\n\n    def test_duplicate_user_agent(self):\n        # Regression test for issue #565\n        from google.cloud._http import ClientInfo\n        from google.cloud.storage.batch import Batch\n        from google.cloud.storage import __version__\n\n        client_info = ClientInfo(user_agent=\"test/123\")\n        conn = self._make_one(object(), client_info=client_info)\n        expected_user_agent = f\"test/123 gcloud-python/{__version__} \"\n        self.assertEqual(conn._client_info.user_agent, expected_user_agent)\n\n        client = mock.Mock(_connection=conn, spec=[\"_connection\"])\n        batch = Batch(client)\n        self.assertEqual(batch._client_info.user_agent, expected_user_agent)\n", "tests/unit/test__signing.py": "# -*- coding: utf-8 -*-\n#\n# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport binascii\nimport calendar\nimport datetime\nimport json\nimport time\nimport unittest\nimport urllib.parse\n\nimport mock\nimport pytest\n\nfrom google.cloud.storage._helpers import _UTC\nfrom . import _read_local_json\n\n\n_SERVICE_ACCOUNT_JSON = _read_local_json(\"url_signer_v4_test_account.json\")\n_CONFORMANCE_TESTS = _read_local_json(\"url_signer_v4_test_data.json\")[\"signingV4Tests\"]\n_BUCKET_TESTS = [\n    test for test in _CONFORMANCE_TESTS if \"bucket\" in test and not test.get(\"object\")\n]\n_BLOB_TESTS = [\n    test for test in _CONFORMANCE_TESTS if \"bucket\" in test and test.get(\"object\")\n]\n\n\ndef _utc_seconds(when):\n    return int(calendar.timegm(when.timetuple()))\n\n\ndef _make_cet_timezone():\n    from datetime import timezone\n    from datetime import timedelta\n\n    return timezone(timedelta(hours=1), name=\"CET\")\n\n\nclass Test_get_expiration_seconds_v2(unittest.TestCase):\n    @staticmethod\n    def _call_fut(expiration):\n        from google.cloud.storage._signing import get_expiration_seconds_v2\n\n        return get_expiration_seconds_v2(expiration)\n\n    def test_w_invalid_expiration_type(self):\n        with self.assertRaises(TypeError):\n            self._call_fut(object(), None)\n\n    def test_w_expiration_none(self):\n        with self.assertRaises(TypeError):\n            self._call_fut(None)\n\n    def test_w_expiration_int(self):\n        self.assertEqual(self._call_fut(123), 123)\n\n    def test_w_expiration_naive_datetime(self):\n        expiration_no_tz = datetime.datetime(2004, 8, 19, 0, 0, 0, 0)\n        utc_seconds = _utc_seconds(expiration_no_tz)\n        self.assertEqual(self._call_fut(expiration_no_tz), utc_seconds)\n\n    def test_w_expiration_utc_datetime(self):\n        expiration_utc = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        utc_seconds = _utc_seconds(expiration_utc)\n        self.assertEqual(self._call_fut(expiration_utc), utc_seconds)\n\n    def test_w_expiration_other_zone_datetime(self):\n        zone = _make_cet_timezone()\n        expiration_other = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, zone)\n        utc_seconds = _utc_seconds(expiration_other)\n        cet_seconds = utc_seconds - (60 * 60)  # CET one hour earlier than UTC\n        self.assertEqual(self._call_fut(expiration_other), cet_seconds)\n\n    def test_w_expiration_timedelta_seconds(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        utc_seconds = _utc_seconds(fake_utcnow)\n        expiration_as_delta = datetime.timedelta(seconds=10)\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n        with patch as utcnow:\n            result = self._call_fut(expiration_as_delta)\n\n        self.assertEqual(result, utc_seconds + 10)\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n    def test_w_expiration_timedelta_days(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        utc_seconds = _utc_seconds(fake_utcnow)\n        expiration_as_delta = datetime.timedelta(days=1)\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n        with patch as utcnow:\n            result = self._call_fut(expiration_as_delta)\n\n        self.assertEqual(result, utc_seconds + 86400)\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n\nclass Test_get_expiration_seconds_v4(unittest.TestCase):\n    @staticmethod\n    def _call_fut(expiration):\n        from google.cloud.storage._signing import get_expiration_seconds_v4\n\n        return get_expiration_seconds_v4(expiration)\n\n    def test_w_invalid_expiration_type(self):\n        with self.assertRaises(TypeError):\n            self._call_fut(object(), None)\n\n    def test_w_expiration_none(self):\n        with self.assertRaises(TypeError):\n            self._call_fut(None)\n\n    def test_w_expiration_int_gt_seven_days(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0)\n        delta = datetime.timedelta(days=10)\n        expiration_utc = fake_utcnow + delta\n        expiration_seconds = _utc_seconds(expiration_utc)\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n\n        with patch as utcnow:\n            with self.assertRaises(ValueError):\n                self._call_fut(expiration_seconds)\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n    def test_w_expiration_int(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0)\n        expiration_seconds = 10\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n\n        with patch as utcnow:\n            result = self._call_fut(expiration_seconds)\n\n        self.assertEqual(result, expiration_seconds)\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n    def test_w_expiration_naive_datetime(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        delta = datetime.timedelta(seconds=10)\n        expiration_no_tz = fake_utcnow + delta\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n        with patch as utcnow:\n            result = self._call_fut(expiration_no_tz)\n\n        self.assertEqual(result, delta.seconds)\n        utcnow.assert_called_once()\n\n    def test_w_expiration_utc_datetime(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        delta = datetime.timedelta(seconds=10)\n        expiration_utc = fake_utcnow + delta\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n        with patch as utcnow:\n            result = self._call_fut(expiration_utc)\n\n        self.assertEqual(result, delta.seconds)\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n    def test_w_expiration_other_zone_datetime(self):\n        zone = _make_cet_timezone()\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        fake_cetnow = fake_utcnow.astimezone(zone)\n        delta = datetime.timedelta(seconds=10)\n        expiration_other = fake_cetnow + delta\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n        with patch as utcnow:\n            result = self._call_fut(expiration_other)\n        self.assertEqual(result, delta.seconds)\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n    def test_w_expiration_timedelta(self):\n        fake_utcnow = datetime.datetime(2004, 8, 19, 0, 0, 0, 0, _UTC)\n        expiration_as_delta = datetime.timedelta(seconds=10)\n\n        patch = mock.patch(\n            \"google.cloud.storage._signing._NOW\", return_value=fake_utcnow\n        )\n        with patch as utcnow:\n            result = self._call_fut(expiration_as_delta)\n\n        self.assertEqual(result, expiration_as_delta.total_seconds())\n        utcnow.assert_called_once_with(datetime.timezone.utc)\n\n\nclass Test_get_signed_query_params_v2(unittest.TestCase):\n    @staticmethod\n    def _call_fut(credentials, expiration, string_to_sign):\n        from google.cloud.storage._signing import get_signed_query_params_v2\n\n        return get_signed_query_params_v2(credentials, expiration, string_to_sign)\n\n    def test_it(self):\n        sig_bytes = b\"DEADBEEF\"\n        account_name = mock.sentinel.service_account_email\n        credentials = _make_credentials(signer_email=account_name)\n        credentials.sign_bytes.return_value = sig_bytes\n        expiration = 100\n        string_to_sign = \"fake_signature\"\n        result = self._call_fut(credentials, expiration, string_to_sign)\n\n        expected = {\n            \"GoogleAccessId\": account_name,\n            \"Expires\": expiration,\n            \"Signature\": base64.b64encode(sig_bytes),\n        }\n        self.assertEqual(result, expected)\n        credentials.sign_bytes.assert_called_once_with(string_to_sign.encode(\"ascii\"))\n\n\nclass Test_get_canonical_headers(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage._signing import get_canonical_headers\n\n        return get_canonical_headers(*args, **kwargs)\n\n    def test_w_none(self):\n        headers = None\n        expected_canonical = []\n        expected_ordered = []\n        canonical, ordered = self._call_fut(headers)\n        self.assertEqual(canonical, expected_canonical)\n        self.assertEqual(ordered, expected_ordered)\n\n    def test_w_dict(self):\n        headers = {\"foo\": \"Foo 1.2.3\", \"Bar\": \" baz,bam,qux   \"}\n        expected_canonical = [\"bar:baz,bam,qux\", \"foo:Foo 1.2.3\"]\n        expected_ordered = [tuple(item.split(\":\")) for item in expected_canonical]\n        canonical, ordered = self._call_fut(headers)\n        self.assertEqual(canonical, expected_canonical)\n        self.assertEqual(ordered, expected_ordered)\n\n    def test_w_list_and_multiples(self):\n        headers = [\n            (\"foo\", \"Foo 1.2.3\"),\n            (\"Bar\", \" baz\"),\n            (\"Bar\", \"bam\"),\n            (\"Bar\", \"qux   \"),\n        ]\n        expected_canonical = [\"bar:baz,bam,qux\", \"foo:Foo 1.2.3\"]\n        expected_ordered = [tuple(item.split(\":\")) for item in expected_canonical]\n        canonical, ordered = self._call_fut(headers)\n        self.assertEqual(canonical, expected_canonical)\n        self.assertEqual(ordered, expected_ordered)\n\n    def test_w_embedded_ws(self):\n        headers = {\"foo\": \"Foo\\n1.2.3\", \"Bar\": \"   baz   bam   qux   \"}\n        expected_canonical = [\"bar:baz bam qux\", \"foo:Foo 1.2.3\"]\n        expected_ordered = [tuple(item.split(\":\")) for item in expected_canonical]\n        canonical, ordered = self._call_fut(headers)\n        self.assertEqual(canonical, expected_canonical)\n        self.assertEqual(ordered, expected_ordered)\n\n\nclass Test_canonicalize_v2(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage._signing import canonicalize_v2\n\n        return canonicalize_v2(*args, **kwargs)\n\n    def test_wo_headers_or_query_parameters(self):\n        method = \"GET\"\n        resource = \"/bucket/blob\"\n        canonical = self._call_fut(method, resource, None, None)\n        self.assertEqual(canonical.method, method)\n        self.assertEqual(canonical.resource, resource)\n        self.assertEqual(canonical.query_parameters, [])\n        self.assertEqual(canonical.headers, [])\n\n    def test_w_headers_and_resumable(self):\n        method = \"RESUMABLE\"\n        resource = \"/bucket/blob\"\n        headers = [(\"x-goog-extension\", \"foobar\")]\n        canonical = self._call_fut(method, resource, None, headers)\n        self.assertEqual(canonical.method, \"POST\")\n        self.assertEqual(canonical.resource, resource)\n        self.assertEqual(canonical.query_parameters, [])\n        self.assertEqual(\n            canonical.headers, [\"x-goog-extension:foobar\", \"x-goog-resumable:start\"]\n        )\n\n    def test_w_query_parameters(self):\n        method = \"GET\"\n        resource = \"/bucket/blob\"\n        query_parameters = {\"foo\": \"bar\", \"baz\": \"qux\"}\n        canonical = self._call_fut(method, resource, query_parameters, None)\n        self.assertEqual(canonical.method, method)\n        self.assertEqual(canonical.resource, f\"{resource}?baz=qux&foo=bar\")\n        self.assertEqual(canonical.query_parameters, [(\"baz\", \"qux\"), (\"foo\", \"bar\")])\n        self.assertEqual(canonical.headers, [])\n\n\nclass Test_generate_signed_url_v2(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage._signing import generate_signed_url_v2\n\n        return generate_signed_url_v2(*args, **kwargs)\n\n    def _generate_helper(\n        self,\n        api_access_endpoint=\"\",\n        method=\"GET\",\n        content_md5=None,\n        content_type=None,\n        response_type=None,\n        response_disposition=None,\n        generation=None,\n        headers=None,\n        query_parameters=None,\n    ):\n        from urllib.parse import urlencode\n\n        resource = \"/name/path\"\n        credentials = _make_credentials(signer_email=\"service@example.com\")\n        credentials.sign_bytes.return_value = b\"DEADBEEF\"\n        signed = base64.b64encode(credentials.sign_bytes.return_value)\n        signed = signed.decode(\"ascii\")\n\n        expiration = 1000\n\n        url = self._call_fut(\n            credentials,\n            resource,\n            expiration=expiration,\n            api_access_endpoint=api_access_endpoint,\n            method=method,\n            content_md5=content_md5,\n            content_type=content_type,\n            response_type=response_type,\n            response_disposition=response_disposition,\n            generation=generation,\n            headers=headers,\n            query_parameters=query_parameters,\n            service_account_email=None,\n            access_token=None,\n        )\n\n        # Check the mock was called.\n        method = method.upper()\n\n        if headers is None:\n            headers = []\n        elif isinstance(headers, dict):\n            headers = sorted(headers.items())\n\n        elements = []\n        expected_resource = resource\n        if method == \"RESUMABLE\":\n            elements.append(\"POST\")\n            headers.append((\"x-goog-resumable\", \"start\"))\n        else:\n            elements.append(method)\n\n        if query_parameters is not None:\n            normalized_qp = {\n                key.lower(): value and value.strip() or \"\"\n                for key, value in query_parameters.items()\n            }\n            expected_qp = urlencode(sorted(normalized_qp.items()))\n            expected_resource = f\"{resource}?{expected_qp}\"\n\n        elements.append(content_md5 or \"\")\n        elements.append(content_type or \"\")\n        elements.append(str(expiration))\n        elements.extend([\"{}:{}\".format(*header) for header in headers])\n        elements.append(expected_resource)\n\n        string_to_sign = \"\\n\".join(elements)\n\n        credentials.sign_bytes.assert_called_once_with(string_to_sign.encode(\"ascii\"))\n\n        scheme, netloc, path, qs, frag = urllib.parse.urlsplit(url)\n        expected_scheme, expected_netloc, _, _, _ = urllib.parse.urlsplit(\n            api_access_endpoint\n        )\n        self.assertEqual(scheme, expected_scheme)\n        self.assertEqual(netloc, expected_netloc)\n        self.assertEqual(path, resource)\n        self.assertEqual(frag, \"\")\n\n        # Check the URL parameters.\n        params = dict(urllib.parse.parse_qsl(qs, keep_blank_values=True))\n\n        self.assertEqual(params[\"GoogleAccessId\"], credentials.signer_email)\n        self.assertEqual(params[\"Expires\"], str(expiration))\n        self.assertEqual(params[\"Signature\"], signed)\n\n        if response_type is not None:\n            self.assertEqual(params[\"response-content-type\"], response_type)\n\n        if response_disposition is not None:\n            self.assertEqual(\n                params[\"response-content-disposition\"], response_disposition\n            )\n\n        if generation is not None:\n            self.assertEqual(params[\"generation\"], str(generation))\n\n        if query_parameters is not None:\n            for key, value in query_parameters.items():\n                value = value.strip() if value else \"\"\n                self.assertEqual(params[key].lower(), value)\n\n    def test_w_expiration_int(self):\n        self._generate_helper()\n\n    def test_w_endpoint(self):\n        api_access_endpoint = \"https://api.example.com\"\n        self._generate_helper(api_access_endpoint=api_access_endpoint)\n\n    def test_w_method(self):\n        method = \"POST\"\n        self._generate_helper(method=method)\n\n    def test_w_method_resumable(self):\n        method = \"RESUMABLE\"\n        self._generate_helper(method=method)\n\n    def test_w_response_type(self):\n        response_type = \"text/plain\"\n        self._generate_helper(response_type=response_type)\n\n    def test_w_response_disposition(self):\n        response_disposition = \"attachment; filename=blob.png\"\n        self._generate_helper(response_disposition=response_disposition)\n\n    def test_w_generation(self):\n        generation = \"123\"\n        self._generate_helper(generation=generation)\n\n    def test_w_custom_headers_dict(self):\n        self._generate_helper(headers={\"x-goog-foo\": \"bar\"})\n\n    def test_w_custom_headers_list(self):\n        self._generate_helper(headers=[(\"x-goog-foo\", \"bar\")])\n\n    def test_w_custom_query_parameters_w_string_value(self):\n        self._generate_helper(query_parameters={\"bar\": \"/\"})\n\n    def test_w_custom_query_parameters_w_none_value(self):\n        self._generate_helper(query_parameters={\"qux\": None})\n\n    def test_with_google_credentials(self):\n        resource = \"/name/path\"\n        credentials = _make_credentials()\n        expiration = int(time.time() + 5)\n        with self.assertRaises(AttributeError):\n            self._call_fut(credentials, resource=resource, expiration=expiration)\n\n    def test_with_access_token(self):\n        resource = \"/name/path\"\n        credentials = _make_credentials()\n        expiration = int(time.time() + 5)\n        email = mock.sentinel.service_account_email\n        with mock.patch(\n            \"google.cloud.storage._signing._sign_message\", return_value=b\"DEADBEEF\"\n        ):\n            self._call_fut(\n                credentials,\n                resource=resource,\n                expiration=expiration,\n                service_account_email=email,\n                access_token=\"token\",\n            )\n\n\nclass Test_generate_signed_url_v4(unittest.TestCase):\n    DEFAULT_EXPIRATION = 1000\n\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage._signing import generate_signed_url_v4\n\n        return generate_signed_url_v4(*args, **kwargs)\n\n    def _generate_helper(\n        self,\n        expiration=DEFAULT_EXPIRATION,\n        api_access_endpoint=\"\",\n        method=\"GET\",\n        content_type=None,\n        content_md5=None,\n        response_type=None,\n        response_disposition=None,\n        generation=None,\n        headers=None,\n        query_parameters=None,\n    ):\n        now = datetime.datetime(2019, 2, 26, 19, 53, 27)\n        resource = \"/name/path\"\n        signer_email = \"service@example.com\"\n        credentials = _make_credentials(signer_email=signer_email)\n        credentials.sign_bytes.return_value = b\"DEADBEEF\"\n\n        with mock.patch(\"google.cloud.storage._signing._NOW\", lambda tz: now):\n            url = self._call_fut(\n                credentials,\n                resource,\n                expiration=expiration,\n                api_access_endpoint=api_access_endpoint,\n                method=method,\n                content_type=content_type,\n                content_md5=content_md5,\n                response_type=response_type,\n                response_disposition=response_disposition,\n                generation=generation,\n                headers=headers,\n                query_parameters=query_parameters,\n            )\n\n        # Check the mock was called.\n        credentials.sign_bytes.assert_called_once()\n\n        scheme, netloc, path, qs, frag = urllib.parse.urlsplit(url)\n\n        expected_scheme, expected_netloc, _, _, _ = urllib.parse.urlsplit(\n            api_access_endpoint\n        )\n        self.assertEqual(scheme, expected_scheme)\n        self.assertEqual(netloc, expected_netloc)\n        self.assertEqual(path, resource)\n        self.assertEqual(frag, \"\")\n\n        # Check the URL parameters.\n        params = dict(urllib.parse.parse_qsl(qs, keep_blank_values=True))\n        self.assertEqual(params[\"X-Goog-Algorithm\"], \"GOOG4-RSA-SHA256\")\n\n        now_date = now.date().strftime(\"%Y%m%d\")\n        expected_cred = f\"{signer_email}/{now_date}/auto/storage/goog4_request\"\n        self.assertEqual(params[\"X-Goog-Credential\"], expected_cred)\n\n        now_stamp = now.strftime(\"%Y%m%dT%H%M%SZ\")\n        self.assertEqual(params[\"X-Goog-Date\"], now_stamp)\n        self.assertEqual(params[\"X-Goog-Expires\"], str(self.DEFAULT_EXPIRATION))\n\n        signed = binascii.hexlify(credentials.sign_bytes.return_value).decode(\"ascii\")\n        self.assertEqual(params[\"X-Goog-Signature\"], signed)\n\n        if response_type is not None:\n            self.assertEqual(params[\"response-content-type\"], response_type)\n\n        if response_disposition is not None:\n            self.assertEqual(\n                params[\"response-content-disposition\"], response_disposition\n            )\n\n        if generation is not None:\n            self.assertEqual(params[\"generation\"], str(generation))\n\n        if query_parameters is not None:\n            for key, value in query_parameters.items():\n                value = value.strip() if value else \"\"\n                self.assertEqual(params[key].lower(), value)\n\n    def test_w_expiration_too_long(self):\n        with self.assertRaises(ValueError):\n            self._generate_helper(expiration=datetime.timedelta(days=8))\n\n    def test_w_defaults(self):\n        self._generate_helper()\n\n    def test_w_api_access_endpoint(self):\n        self._generate_helper(api_access_endpoint=\"http://api.example.com\")\n\n    def test_w_method(self):\n        self._generate_helper(method=\"PUT\")\n\n    def test_w_method_resumable(self):\n        self._generate_helper(method=\"RESUMABLE\")\n\n    def test_w_content_type(self):\n        self._generate_helper(content_type=\"text/plain\")\n\n    def test_w_content_md5(self):\n        self._generate_helper(content_md5=\"FACEDACE\")\n\n    def test_w_response_type(self):\n        self._generate_helper(response_type=\"application/octets\")\n\n    def test_w_response_disposition(self):\n        self._generate_helper(response_disposition=\"attachment\")\n\n    def test_w_generation(self):\n        self._generate_helper(generation=12345)\n\n    def test_w_custom_host_header(self):\n        self._generate_helper(headers={\"Host\": \"api.example.com\"})\n\n    def test_w_custom_headers(self):\n        self._generate_helper(headers={\"x-goog-foo\": \"bar\"})\n\n    def test_w_custom_payload_hash_goog(self):\n        self._generate_helper(headers={\"x-goog-content-sha256\": \"DEADBEEF\"})\n\n    def test_w_custom_query_parameters_w_string_value(self):\n        self._generate_helper(query_parameters={\"bar\": \"/\"})\n\n    def test_w_custom_query_parameters_w_none_value(self):\n        self._generate_helper(query_parameters={\"qux\": None})\n\n    def test_with_access_token_and_service_account_email(self):\n        resource = \"/name/path\"\n        credentials = _make_credentials()\n        email = mock.sentinel.service_account_email\n        with mock.patch(\n            \"google.cloud.storage._signing._sign_message\", return_value=b\"DEADBEEF\"\n        ):\n            self._call_fut(\n                credentials,\n                resource=resource,\n                expiration=datetime.timedelta(days=5),\n                service_account_email=email,\n                access_token=\"token\",\n            )\n\n    def test_with_access_token_and_service_account_email_and_signer_email(self):\n        resource = \"/name/path\"\n        signer_email = \"service@example.com\"\n        credentials = _make_credentials(signer_email=signer_email)\n        with mock.patch(\n            \"google.cloud.storage._signing._sign_message\", return_value=b\"DEADBEEF\"\n        ):\n            self._call_fut(\n                credentials,\n                resource=resource,\n                expiration=datetime.timedelta(days=5),\n                service_account_email=signer_email,\n                access_token=\"token\",\n            )\n\n    def test_with_signer_email(self):\n        resource = \"/name/path\"\n        signer_email = \"service@example.com\"\n        credentials = _make_credentials(signer_email=signer_email)\n        credentials.sign_bytes.return_value = b\"DEADBEEF\"\n        self._call_fut(\n            credentials,\n            resource=resource,\n            expiration=datetime.timedelta(days=5),\n        )\n\n    def test_with_service_account_email_and_signer_email(self):\n        resource = \"/name/path\"\n        signer_email = \"service@example.com\"\n        credentials = _make_credentials(signer_email=signer_email)\n        credentials.sign_bytes.return_value = b\"DEADBEEF\"\n        self._call_fut(\n            credentials,\n            resource=resource,\n            expiration=datetime.timedelta(days=5),\n            service_account_email=signer_email,\n        )\n\n    def test_with_token_and_signer_email(self):\n        resource = \"/name/path\"\n        signer_email = \"service@example.com\"\n        credentials = _make_credentials(signer_email=signer_email)\n        credentials.sign_bytes.return_value = b\"DEADBEEF\"\n        self._call_fut(\n            credentials,\n            resource=resource,\n            expiration=datetime.timedelta(days=5),\n            access_token=\"token\",\n        )\n\n\nclass Test_sign_message(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage._signing import _sign_message\n\n        return _sign_message(*args, **kwargs)\n\n    def test_sign_bytes(self):\n        signature = \"DEADBEEF\"\n        data = {\"signedBlob\": signature}\n        request = make_request(200, data)\n        with mock.patch(\"google.auth.transport.requests.Request\", return_value=request):\n            returned_signature = self._call_fut(\n                \"123\", service_account_email=\"service@example.com\", access_token=\"token\"\n            )\n            assert returned_signature == signature\n\n    def test_sign_bytes_failure(self):\n        from google.auth import exceptions\n\n        request = make_request(401)\n        with mock.patch(\"google.auth.transport.requests.Request\", return_value=request):\n            with pytest.raises(exceptions.TransportError):\n                self._call_fut(\n                    \"123\",\n                    service_account_email=\"service@example.com\",\n                    access_token=\"token\",\n                )\n\n\nclass TestCustomURLEncoding(unittest.TestCase):\n    def test_url_encode(self):\n        from google.cloud.storage._signing import _url_encode\n\n        # param1 includes safe symbol ~\n        # param# includes symbols, which must be encoded\n        query_params = {\"param1\": \"value~1-2\", \"param#\": \"*value+value/\"}\n\n        self.assertEqual(\n            _url_encode(query_params), \"param%23=%2Avalue%2Bvalue%2F&param1=value~1-2\"\n        )\n\n\nclass TestQuoteParam(unittest.TestCase):\n    def test_ascii_symbols(self):\n        from google.cloud.storage._signing import _quote_param\n\n        encoded_param = _quote_param(\"param\")\n        self.assertIsInstance(encoded_param, str)\n        self.assertEqual(encoded_param, \"param\")\n\n    def test_quoted_symbols(self):\n        from google.cloud.storage._signing import _quote_param\n\n        encoded_param = _quote_param(\"!#$%&'()*+,/:;=?@[]\")\n        self.assertIsInstance(encoded_param, str)\n        self.assertEqual(\n            encoded_param, \"%21%23%24%25%26%27%28%29%2A%2B%2C%2F%3A%3B%3D%3F%40%5B%5D\"\n        )\n\n    def test_unquoted_symbols(self):\n        from google.cloud.storage._signing import _quote_param\n        import string\n\n        UNQUOTED = string.ascii_letters + string.digits + \".~_-\"\n\n        encoded_param = _quote_param(UNQUOTED)\n        self.assertIsInstance(encoded_param, str)\n        self.assertEqual(encoded_param, UNQUOTED)\n\n    def test_unicode_symbols(self):\n        from google.cloud.storage._signing import _quote_param\n\n        encoded_param = _quote_param(\"\u0401\u0419\u0426\u042f\u0429\u042f\u0429\")\n        self.assertIsInstance(encoded_param, str)\n        self.assertEqual(encoded_param, \"%D0%81%D0%99%D0%A6%D0%AF%D0%A9%D0%AF%D0%A9\")\n\n    def test_bytes(self):\n        from google.cloud.storage._signing import _quote_param\n\n        encoded_param = _quote_param(b\"bytes\")\n        self.assertIsInstance(encoded_param, str)\n        self.assertEqual(encoded_param, \"bytes\")\n\n\nclass TestV4Stamps(unittest.TestCase):\n    def test_get_v4_now_dtstamps(self):\n        import datetime\n        from google.cloud.storage._signing import get_v4_now_dtstamps\n\n        with mock.patch(\n            \"google.cloud.storage._signing._NOW\",\n            return_value=datetime.datetime(2020, 3, 12, 13, 14, 15),\n        ) as now_mock:\n            timestamp, datestamp = get_v4_now_dtstamps()\n            now_mock.assert_called_once()\n\n        self.assertEqual(timestamp, \"20200312T131415Z\")\n        self.assertEqual(datestamp, \"20200312\")\n\n\n\"\"\"Conformance tests for v4 signed URLs.\"\"\"\n\n_FAKE_SERVICE_ACCOUNT = None\n\n\ndef fake_service_account():\n    global _FAKE_SERVICE_ACCOUNT\n\n    from google.oauth2.service_account import Credentials\n\n    if _FAKE_SERVICE_ACCOUNT is None:\n        _FAKE_SERVICE_ACCOUNT = Credentials.from_service_account_info(\n            _SERVICE_ACCOUNT_JSON\n        )\n\n    return _FAKE_SERVICE_ACCOUNT\n\n\n_API_ACCESS_ENDPOINT = \"https://storage.googleapis.com\"\n\n\ndef _run_conformance_test(\n    resource, test_data, api_access_endpoint=_API_ACCESS_ENDPOINT\n):\n    credentials = fake_service_account()\n    url = Test_generate_signed_url_v4._call_fut(\n        credentials,\n        resource,\n        expiration=test_data[\"expiration\"],\n        api_access_endpoint=api_access_endpoint,\n        method=test_data[\"method\"],\n        _request_timestamp=test_data[\"timestamp\"],\n        headers=test_data.get(\"headers\"),\n        query_parameters=test_data.get(\"queryParameters\"),\n    )\n\n    assert url == test_data[\"expectedUrl\"]\n\n\n@pytest.mark.parametrize(\"test_data\", _BUCKET_TESTS)\ndef test_conformance_bucket(test_data):\n    global _API_ACCESS_ENDPOINT\n    if \"urlStyle\" in test_data and test_data[\"urlStyle\"] == \"BUCKET_BOUND_HOSTNAME\":\n        _API_ACCESS_ENDPOINT = \"{scheme}://{bucket_bound_hostname}\".format(\n            scheme=test_data[\"scheme\"],\n            bucket_bound_hostname=test_data[\"bucketBoundHostname\"],\n        )\n        resource = \"/\"\n        _run_conformance_test(resource, test_data, _API_ACCESS_ENDPOINT)\n    else:\n        resource = f\"/{test_data['bucket']}\"\n        _run_conformance_test(resource, test_data)\n\n\n@pytest.mark.parametrize(\"test_data\", _BLOB_TESTS)\ndef test_conformance_blob(test_data):\n    global _API_ACCESS_ENDPOINT\n    if \"urlStyle\" in test_data:\n        if test_data[\"urlStyle\"] == \"BUCKET_BOUND_HOSTNAME\":\n            _API_ACCESS_ENDPOINT = \"{scheme}://{bucket_bound_hostname}\".format(\n                scheme=test_data[\"scheme\"],\n                bucket_bound_hostname=test_data[\"bucketBoundHostname\"],\n            )\n\n        # For the VIRTUAL_HOSTED_STYLE\n        else:\n            _API_ACCESS_ENDPOINT = (\n                f\"{test_data['scheme']}://{test_data['bucket']}.storage.googleapis.com\"\n            )\n        resource = f\"/{test_data['object']}\"\n        _run_conformance_test(resource, test_data, _API_ACCESS_ENDPOINT)\n    else:\n        resource = f\"/{test_data['bucket']}/{test_data['object']}\"\n        _run_conformance_test(resource, test_data)\n\n\ndef _make_credentials(signer_email=None):\n    import google.auth.credentials\n\n    if signer_email:\n        credentials = mock.Mock(spec=google.auth.credentials.Signing)\n        credentials.signer_email = signer_email\n        return credentials\n    else:\n        return mock.Mock(spec=google.auth.credentials.Credentials)\n\n\ndef make_request(status, data=None):\n    from google.auth import transport\n\n    response = mock.create_autospec(transport.Response, instance=True)\n    response.status = status\n    if data is not None:\n        response.data = json.dumps(data).encode(\"utf-8\")\n\n    request = mock.create_autospec(transport.Request)\n    request.return_value = response\n    return request\n", "tests/unit/test_transfer_manager.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nfrom google.cloud.storage import Blob\nfrom google.cloud.storage import Client\nfrom google.cloud.storage import transfer_manager\n\nfrom google.api_core import exceptions\n\nfrom google.resumable_media.common import DataCorruption\n\nimport os\nimport tempfile\nimport mock\nimport pickle\n\nBLOB_TOKEN_STRING = \"blob token\"\nFAKE_CONTENT_TYPE = \"text/fake\"\nUPLOAD_KWARGS = {\"content-type\": FAKE_CONTENT_TYPE}\nFAKE_RESULT = \"nothing to see here\"\nFAKE_ENCODING = \"fake_gzip\"\nDOWNLOAD_KWARGS = {\"accept-encoding\": FAKE_ENCODING}\nCHUNK_SIZE = 8\nHOSTNAME = \"https://example.com\"\nURL = \"https://example.com/bucket/blob\"\nUSER_AGENT = \"agent\"\nEXPECTED_UPLOAD_KWARGS = {\n    \"command\": \"tm.upload_many\",\n    **UPLOAD_KWARGS,\n}\nEXPECTED_DOWNLOAD_KWARGS = {\n    \"command\": \"tm.download_many\",\n    **DOWNLOAD_KWARGS,\n}\n\n\n# Used in subprocesses only, so excluded from coverage\ndef _validate_blob_token_in_subprocess(\n    maybe_pickled_blob, method_name, path_or_file, **kwargs\n):  # pragma: NO COVER\n    assert pickle.loads(maybe_pickled_blob) == BLOB_TOKEN_STRING\n    assert \"filename\" in method_name\n    assert path_or_file.startswith(\"file\")\n    assert kwargs == EXPECTED_UPLOAD_KWARGS or kwargs == EXPECTED_DOWNLOAD_KWARGS\n    return FAKE_RESULT\n\n\ndef test_upload_many_with_filenames():\n    FILE_BLOB_PAIRS = [\n        (\"file_a.txt\", mock.Mock(spec=Blob)),\n        (\"file_b.txt\", mock.Mock(spec=Blob)),\n    ]\n    expected_upload_kwargs = EXPECTED_UPLOAD_KWARGS.copy()\n    expected_upload_kwargs[\"if_generation_match\"] = 0\n\n    for _, blob_mock in FILE_BLOB_PAIRS:\n        blob_mock._handle_filename_and_upload.return_value = FAKE_RESULT\n\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS,\n        skip_if_exists=True,\n        upload_kwargs=UPLOAD_KWARGS,\n        worker_type=transfer_manager.THREAD,\n    )\n    for filename, mock_blob in FILE_BLOB_PAIRS:\n        mock_blob._handle_filename_and_upload.assert_any_call(\n            filename, **expected_upload_kwargs\n        )\n    for result in results:\n        assert result == FAKE_RESULT\n\n\ndef test_upload_many_with_file_objs():\n    FILE_BLOB_PAIRS = [\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n    ]\n    expected_upload_kwargs = EXPECTED_UPLOAD_KWARGS.copy()\n    expected_upload_kwargs[\"if_generation_match\"] = 0\n\n    for _, blob_mock in FILE_BLOB_PAIRS:\n        blob_mock._prep_and_do_upload.return_value = FAKE_RESULT\n\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS,\n        skip_if_exists=True,\n        upload_kwargs=UPLOAD_KWARGS,\n        worker_type=transfer_manager.THREAD,\n    )\n    for file, mock_blob in FILE_BLOB_PAIRS:\n        mock_blob._prep_and_do_upload.assert_any_call(file, **expected_upload_kwargs)\n    for result in results:\n        assert result == FAKE_RESULT\n\n\ndef test_upload_many_passes_concurrency_options():\n    FILE_BLOB_PAIRS = [\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n    ]\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    with mock.patch(\"concurrent.futures.ThreadPoolExecutor\") as pool_patch, mock.patch(\n        \"concurrent.futures.wait\"\n    ) as wait_patch:\n        transfer_manager.upload_many(\n            FILE_BLOB_PAIRS,\n            deadline=DEADLINE,\n            worker_type=transfer_manager.THREAD,\n            max_workers=MAX_WORKERS,\n        )\n        pool_patch.assert_called_with(max_workers=MAX_WORKERS)\n        wait_patch.assert_called_with(mock.ANY, timeout=DEADLINE, return_when=mock.ANY)\n\n\ndef test_threads_deprecation_with_upload():\n    FILE_BLOB_PAIRS = [\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n    ]\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    with mock.patch(\"concurrent.futures.ThreadPoolExecutor\") as pool_patch, mock.patch(\n        \"concurrent.futures.wait\"\n    ) as wait_patch:\n        with pytest.warns():\n            transfer_manager.upload_many(\n                FILE_BLOB_PAIRS, deadline=DEADLINE, threads=MAX_WORKERS\n            )\n        pool_patch.assert_called_with(max_workers=MAX_WORKERS)\n        wait_patch.assert_called_with(mock.ANY, timeout=DEADLINE, return_when=mock.ANY)\n\n\ndef test_threads_deprecation_conflict_with_upload():\n    FILE_BLOB_PAIRS = [\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n        (tempfile.TemporaryFile(), mock.Mock(spec=Blob)),\n    ]\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    with pytest.raises(ValueError):\n        transfer_manager.upload_many(\n            FILE_BLOB_PAIRS,\n            deadline=DEADLINE,\n            threads=5,\n            worker_type=transfer_manager.THREAD,\n            max_workers=MAX_WORKERS,\n        )\n\n\ndef test_upload_many_suppresses_exceptions():\n    FILE_BLOB_PAIRS = [\n        (\"file_a.txt\", mock.Mock(spec=Blob)),\n        (\"file_b.txt\", mock.Mock(spec=Blob)),\n    ]\n    for _, mock_blob in FILE_BLOB_PAIRS:\n        mock_blob._handle_filename_and_upload.side_effect = ConnectionError()\n\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS, worker_type=transfer_manager.THREAD\n    )\n    for result in results:\n        assert isinstance(result, ConnectionError)\n\n\ndef test_upload_many_raises_exceptions():\n    FILE_BLOB_PAIRS = [\n        (\"file_a.txt\", mock.Mock(spec=Blob)),\n        (\"file_b.txt\", mock.Mock(spec=Blob)),\n    ]\n    for _, mock_blob in FILE_BLOB_PAIRS:\n        mock_blob._handle_filename_and_upload.side_effect = ConnectionError()\n\n    with pytest.raises(ConnectionError):\n        transfer_manager.upload_many(\n            FILE_BLOB_PAIRS, raise_exception=True, worker_type=transfer_manager.THREAD\n        )\n\n\ndef test_upload_many_suppresses_412_with_skip_if_exists():\n    FILE_BLOB_PAIRS = [\n        (\"file_a.txt\", mock.Mock(spec=Blob)),\n        (\"file_b.txt\", mock.Mock(spec=Blob)),\n    ]\n    for _, mock_blob in FILE_BLOB_PAIRS:\n        mock_blob._handle_filename_and_upload.side_effect = (\n            exceptions.PreconditionFailed(\"412\")\n        )\n    results = transfer_manager.upload_many(\n        FILE_BLOB_PAIRS,\n        skip_if_exists=True,\n        raise_exception=True,\n        worker_type=transfer_manager.THREAD,\n    )\n    for result in results:\n        assert isinstance(result, exceptions.PreconditionFailed)\n\n\ndef test_upload_many_with_processes():\n    # Mocks are not pickleable, so we send token strings over the wire.\n    FILE_BLOB_PAIRS = [\n        (\"file_a.txt\", BLOB_TOKEN_STRING),\n        (\"file_b.txt\", BLOB_TOKEN_STRING),\n    ]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._call_method_on_maybe_pickled_blob\",\n        new=_validate_blob_token_in_subprocess,\n    ):\n        results = transfer_manager.upload_many(\n            FILE_BLOB_PAIRS,\n            upload_kwargs=UPLOAD_KWARGS,\n            worker_type=transfer_manager.PROCESS,\n            raise_exception=True,\n        )\n    for result in results:\n        assert result == FAKE_RESULT\n\n\ndef test_upload_many_with_processes_rejects_file_obj():\n    # Mocks are not pickleable, so we send token strings over the wire.\n    FILE_BLOB_PAIRS = [\n        (\"file_a.txt\", BLOB_TOKEN_STRING),\n        (tempfile.TemporaryFile(), BLOB_TOKEN_STRING),\n    ]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._call_method_on_maybe_pickled_blob\",\n        new=_validate_blob_token_in_subprocess,\n    ):\n        with pytest.raises(ValueError):\n            transfer_manager.upload_many(\n                FILE_BLOB_PAIRS,\n                upload_kwargs=UPLOAD_KWARGS,\n                worker_type=transfer_manager.PROCESS,\n            )\n\n\ndef test_download_many_with_filenames():\n    BLOB_FILE_PAIRS = [\n        (mock.Mock(spec=Blob), \"file_a.txt\"),\n        (mock.Mock(spec=Blob), \"file_b.txt\"),\n    ]\n\n    for blob_mock, _ in BLOB_FILE_PAIRS:\n        blob_mock._handle_filename_and_download.return_value = FAKE_RESULT\n\n    results = transfer_manager.download_many(\n        BLOB_FILE_PAIRS,\n        download_kwargs=DOWNLOAD_KWARGS,\n        worker_type=transfer_manager.THREAD,\n    )\n    for mock_blob, file in BLOB_FILE_PAIRS:\n        mock_blob._handle_filename_and_download.assert_any_call(\n            file, **EXPECTED_DOWNLOAD_KWARGS\n        )\n    for result in results:\n        assert result == FAKE_RESULT\n\n\ndef test_download_many_with_skip_if_exists():\n    with tempfile.NamedTemporaryFile() as tf:\n        BLOB_FILE_PAIRS = [\n            (mock.Mock(spec=Blob), \"file_a.txt\"),\n            (mock.Mock(spec=Blob), tf.name),\n        ]\n\n        for blob_mock, _ in BLOB_FILE_PAIRS:\n            blob_mock._handle_filename_and_download.return_value = FAKE_RESULT\n\n        results = transfer_manager.download_many(\n            BLOB_FILE_PAIRS,\n            download_kwargs=DOWNLOAD_KWARGS,\n            worker_type=transfer_manager.THREAD,\n            skip_if_exists=True,\n        )\n        mock_blob, file = BLOB_FILE_PAIRS[0]\n        mock_blob._handle_filename_and_download.assert_any_call(\n            file, **EXPECTED_DOWNLOAD_KWARGS\n        )\n        mock_blob, _ = BLOB_FILE_PAIRS[1]\n        mock_blob._handle_filename_and_download.assert_not_called()\n        for result in results:\n            assert result == FAKE_RESULT\n\n\ndef test_download_many_with_file_objs():\n    BLOB_FILE_PAIRS = [\n        (mock.Mock(spec=Blob), tempfile.TemporaryFile()),\n        (mock.Mock(spec=Blob), tempfile.TemporaryFile()),\n    ]\n\n    for blob_mock, _ in BLOB_FILE_PAIRS:\n        blob_mock._prep_and_do_download.return_value = FAKE_RESULT\n\n    results = transfer_manager.download_many(\n        BLOB_FILE_PAIRS,\n        download_kwargs=DOWNLOAD_KWARGS,\n        worker_type=transfer_manager.THREAD,\n    )\n    for mock_blob, file in BLOB_FILE_PAIRS:\n        mock_blob._prep_and_do_download.assert_any_call(file, **DOWNLOAD_KWARGS)\n    for result in results:\n        assert result == FAKE_RESULT\n\n\ndef test_download_many_passes_concurrency_options():\n    BLOB_FILE_PAIRS = [\n        (mock.Mock(spec=Blob), tempfile.TemporaryFile()),\n        (mock.Mock(spec=Blob), tempfile.TemporaryFile()),\n    ]\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    with mock.patch(\"concurrent.futures.ThreadPoolExecutor\") as pool_patch, mock.patch(\n        \"concurrent.futures.wait\"\n    ) as wait_patch:\n        transfer_manager.download_many(\n            BLOB_FILE_PAIRS,\n            deadline=DEADLINE,\n            worker_type=transfer_manager.THREAD,\n            max_workers=MAX_WORKERS,\n        )\n        pool_patch.assert_called_with(max_workers=MAX_WORKERS)\n        wait_patch.assert_called_with(mock.ANY, timeout=DEADLINE, return_when=mock.ANY)\n\n\ndef test_download_many_suppresses_exceptions():\n    BLOB_FILE_PAIRS = [\n        (mock.Mock(spec=Blob), \"file_a.txt\"),\n        (mock.Mock(spec=Blob), \"file_b.txt\"),\n    ]\n    for mock_blob, _ in BLOB_FILE_PAIRS:\n        mock_blob._handle_filename_and_download.side_effect = ConnectionError()\n\n    results = transfer_manager.download_many(\n        BLOB_FILE_PAIRS, worker_type=transfer_manager.THREAD\n    )\n    for result in results:\n        assert isinstance(result, ConnectionError)\n\n\ndef test_download_many_raises_exceptions():\n    BLOB_FILE_PAIRS = [\n        (mock.Mock(spec=Blob), \"file_a.txt\"),\n        (mock.Mock(spec=Blob), \"file_b.txt\"),\n    ]\n    for mock_blob, _ in BLOB_FILE_PAIRS:\n        mock_blob._handle_filename_and_download.side_effect = ConnectionError()\n\n    with pytest.raises(ConnectionError):\n        transfer_manager.download_many(\n            BLOB_FILE_PAIRS, raise_exception=True, worker_type=transfer_manager.THREAD\n        )\n\n\ndef test_download_many_with_processes():\n    # Mocks are not pickleable, so we send token strings over the wire.\n    BLOB_FILE_PAIRS = [\n        (BLOB_TOKEN_STRING, \"file_a.txt\"),\n        (BLOB_TOKEN_STRING, \"file_b.txt\"),\n    ]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._call_method_on_maybe_pickled_blob\",\n        new=_validate_blob_token_in_subprocess,\n    ):\n        results = transfer_manager.download_many(\n            BLOB_FILE_PAIRS,\n            download_kwargs=DOWNLOAD_KWARGS,\n            worker_type=transfer_manager.PROCESS,\n        )\n    for result in results:\n        assert result == FAKE_RESULT\n\n\ndef test_download_many_with_processes_rejects_file_obj():\n    # Mocks are not pickleable, so we send token strings over the wire.\n    BLOB_FILE_PAIRS = [\n        (BLOB_TOKEN_STRING, \"file_a.txt\"),\n        (BLOB_TOKEN_STRING, tempfile.TemporaryFile()),\n    ]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._call_method_on_maybe_pickled_blob\",\n        new=_validate_blob_token_in_subprocess,\n    ):\n        with pytest.raises(ValueError):\n            transfer_manager.download_many(\n                BLOB_FILE_PAIRS,\n                download_kwargs=DOWNLOAD_KWARGS,\n                worker_type=transfer_manager.PROCESS,\n            )\n\n\ndef test_upload_many_from_filenames():\n    bucket = mock.Mock()\n\n    FILENAMES = [\"file_a.txt\", \"file_b.txt\"]\n    ROOT = \"mypath/\"\n    PREFIX = \"myprefix/\"\n    KEY_NAME = \"keyname\"\n    BLOB_CONSTRUCTOR_KWARGS = {\"kms_key_name\": KEY_NAME}\n    UPLOAD_KWARGS = {\"content-type\": \"text/fake\"}\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    WORKER_TYPE = transfer_manager.THREAD\n\n    EXPECTED_FILE_BLOB_PAIRS = [\n        (os.path.join(ROOT, filename), mock.ANY) for filename in FILENAMES\n    ]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.upload_many\"\n    ) as mock_upload_many:\n        transfer_manager.upload_many_from_filenames(\n            bucket,\n            FILENAMES,\n            source_directory=ROOT,\n            blob_name_prefix=PREFIX,\n            skip_if_exists=True,\n            blob_constructor_kwargs=BLOB_CONSTRUCTOR_KWARGS,\n            upload_kwargs=UPLOAD_KWARGS,\n            deadline=DEADLINE,\n            raise_exception=True,\n            worker_type=WORKER_TYPE,\n            max_workers=MAX_WORKERS,\n        )\n\n    mock_upload_many.assert_called_once_with(\n        EXPECTED_FILE_BLOB_PAIRS,\n        skip_if_exists=True,\n        upload_kwargs=UPLOAD_KWARGS,\n        deadline=DEADLINE,\n        raise_exception=True,\n        worker_type=WORKER_TYPE,\n        max_workers=MAX_WORKERS,\n    )\n    bucket.blob.assert_any_call(PREFIX + FILENAMES[0], **BLOB_CONSTRUCTOR_KWARGS)\n    bucket.blob.assert_any_call(PREFIX + FILENAMES[1], **BLOB_CONSTRUCTOR_KWARGS)\n\n\ndef test_upload_many_from_filenames_minimal_args():\n    bucket = mock.Mock()\n\n    FILENAMES = [\"file_a.txt\", \"file_b.txt\"]\n\n    EXPECTED_FILE_BLOB_PAIRS = [(filename, mock.ANY) for filename in FILENAMES]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.upload_many\"\n    ) as mock_upload_many:\n        transfer_manager.upload_many_from_filenames(\n            bucket,\n            FILENAMES,\n        )\n\n    mock_upload_many.assert_called_once_with(\n        EXPECTED_FILE_BLOB_PAIRS,\n        skip_if_exists=False,\n        upload_kwargs=None,\n        deadline=None,\n        raise_exception=False,\n        worker_type=transfer_manager.PROCESS,\n        max_workers=8,\n    )\n    bucket.blob.assert_any_call(FILENAMES[0])\n    bucket.blob.assert_any_call(FILENAMES[1])\n\n\ndef test_upload_many_from_filenames_additional_properties():\n    bucket = mock.Mock()\n    blob = mock.Mock()\n    bucket_blob = mock.Mock(return_value=blob)\n    blob.cache_control = None\n    bucket.blob = bucket_blob\n\n    FILENAME = \"file_a.txt\"\n    ADDITIONAL_BLOB_ATTRIBUTES = {\"cache_control\": \"no-cache\"}\n    EXPECTED_FILE_BLOB_PAIRS = [(FILENAME, mock.ANY)]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.upload_many\"\n    ) as mock_upload_many:\n        transfer_manager.upload_many_from_filenames(\n            bucket, [FILENAME], additional_blob_attributes=ADDITIONAL_BLOB_ATTRIBUTES\n        )\n\n    mock_upload_many.assert_called_once_with(\n        EXPECTED_FILE_BLOB_PAIRS,\n        skip_if_exists=False,\n        upload_kwargs=None,\n        deadline=None,\n        raise_exception=False,\n        worker_type=transfer_manager.PROCESS,\n        max_workers=8,\n    )\n\n    for attrib, value in ADDITIONAL_BLOB_ATTRIBUTES.items():\n        assert getattr(blob, attrib) == value\n\n\ndef test_download_many_to_path():\n    bucket = mock.Mock()\n\n    BLOBNAMES = [\"file_a.txt\", \"file_b.txt\", \"dir_a/file_c.txt\"]\n    PATH_ROOT = \"mypath/\"\n    BLOB_NAME_PREFIX = \"myprefix/\"\n    DOWNLOAD_KWARGS = {\"accept-encoding\": \"fake-gzip\"}\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    WORKER_TYPE = transfer_manager.THREAD\n\n    EXPECTED_BLOB_FILE_PAIRS = [\n        (mock.ANY, os.path.join(PATH_ROOT, blobname)) for blobname in BLOBNAMES\n    ]\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.download_many\"\n    ) as mock_download_many:\n        transfer_manager.download_many_to_path(\n            bucket,\n            BLOBNAMES,\n            destination_directory=PATH_ROOT,\n            blob_name_prefix=BLOB_NAME_PREFIX,\n            download_kwargs=DOWNLOAD_KWARGS,\n            deadline=DEADLINE,\n            create_directories=False,\n            raise_exception=True,\n            max_workers=MAX_WORKERS,\n            worker_type=WORKER_TYPE,\n            skip_if_exists=True,\n        )\n\n    mock_download_many.assert_called_once_with(\n        EXPECTED_BLOB_FILE_PAIRS,\n        download_kwargs=DOWNLOAD_KWARGS,\n        deadline=DEADLINE,\n        raise_exception=True,\n        max_workers=MAX_WORKERS,\n        worker_type=WORKER_TYPE,\n        skip_if_exists=True,\n    )\n    for blobname in BLOBNAMES:\n        bucket.blob.assert_any_call(BLOB_NAME_PREFIX + blobname)\n\n\ndef test_download_many_to_path_creates_directories():\n    bucket = mock.Mock()\n\n    with tempfile.TemporaryDirectory() as tempdir:\n        DIR_NAME = \"dir_a/dir_b\"\n        BLOBNAMES = [\n            \"file_a.txt\",\n            \"file_b.txt\",\n            os.path.join(DIR_NAME, \"file_c.txt\"),\n        ]\n\n        EXPECTED_BLOB_FILE_PAIRS = [\n            (mock.ANY, os.path.join(tempdir, blobname)) for blobname in BLOBNAMES\n        ]\n\n        with mock.patch(\n            \"google.cloud.storage.transfer_manager.download_many\"\n        ) as mock_download_many:\n            transfer_manager.download_many_to_path(\n                bucket,\n                BLOBNAMES,\n                destination_directory=tempdir,\n                create_directories=True,\n                raise_exception=True,\n            )\n\n        mock_download_many.assert_called_once_with(\n            EXPECTED_BLOB_FILE_PAIRS,\n            download_kwargs=None,\n            deadline=None,\n            raise_exception=True,\n            worker_type=transfer_manager.PROCESS,\n            max_workers=8,\n            skip_if_exists=False,\n        )\n        for blobname in BLOBNAMES:\n            bucket.blob.assert_any_call(blobname)\n\n        assert os.path.isdir(os.path.join(tempdir, DIR_NAME))\n\n\ndef test_download_chunks_concurrently():\n    blob_mock = mock.Mock(spec=Blob)\n    FILENAME = \"file_a.txt\"\n    MULTIPLE = 4\n    blob_mock.size = CHUNK_SIZE * MULTIPLE\n\n    expected_download_kwargs = EXPECTED_DOWNLOAD_KWARGS.copy()\n    expected_download_kwargs[\"command\"] = \"tm.download_sharded\"\n\n    with mock.patch(\"google.cloud.storage.transfer_manager.open\", mock.mock_open()):\n        result = transfer_manager.download_chunks_concurrently(\n            blob_mock,\n            FILENAME,\n            chunk_size=CHUNK_SIZE,\n            download_kwargs=DOWNLOAD_KWARGS,\n            worker_type=transfer_manager.THREAD,\n            crc32c_checksum=False,\n        )\n    for x in range(MULTIPLE):\n        blob_mock._prep_and_do_download.assert_any_call(\n            mock.ANY,\n            **expected_download_kwargs,\n            start=x * CHUNK_SIZE,\n            end=((x + 1) * CHUNK_SIZE) - 1,\n        )\n    assert blob_mock._prep_and_do_download.call_count == 4\n    assert result is None\n\n\ndef test_download_chunks_concurrently_with_crc32c():\n    blob_mock = mock.Mock(spec=Blob)\n    FILENAME = \"file_a.txt\"\n    MULTIPLE = 4\n    BLOB_CHUNK = b\"abcdefgh\"\n    BLOB_CONTENTS = BLOB_CHUNK * MULTIPLE\n    blob_mock.size = len(BLOB_CONTENTS)\n    blob_mock.crc32c = \"eOVVVw==\"\n\n    expected_download_kwargs = EXPECTED_DOWNLOAD_KWARGS.copy()\n    expected_download_kwargs[\"command\"] = \"tm.download_sharded\"\n\n    def write_to_file(f, *args, **kwargs):\n        f.write(BLOB_CHUNK)\n\n    blob_mock._prep_and_do_download.side_effect = write_to_file\n\n    with mock.patch(\"google.cloud.storage.transfer_manager.open\", mock.mock_open()):\n        transfer_manager.download_chunks_concurrently(\n            blob_mock,\n            FILENAME,\n            chunk_size=CHUNK_SIZE,\n            download_kwargs=DOWNLOAD_KWARGS,\n            worker_type=transfer_manager.THREAD,\n            crc32c_checksum=True,\n        )\n\n\ndef test_download_chunks_concurrently_with_crc32c_failure():\n    blob_mock = mock.Mock(spec=Blob)\n    FILENAME = \"file_a.txt\"\n    MULTIPLE = 4\n    BLOB_CHUNK = b\"abcdefgh\"\n    BLOB_CONTENTS = BLOB_CHUNK * MULTIPLE\n    blob_mock.size = len(BLOB_CONTENTS)\n    blob_mock.crc32c = \"invalid\"\n\n    expected_download_kwargs = EXPECTED_DOWNLOAD_KWARGS.copy()\n    expected_download_kwargs[\"command\"] = \"tm.download_sharded\"\n\n    def write_to_file(f, *args, **kwargs):\n        f.write(BLOB_CHUNK)\n\n    blob_mock._prep_and_do_download.side_effect = write_to_file\n\n    with mock.patch(\"google.cloud.storage.transfer_manager.open\", mock.mock_open()):\n        with pytest.raises(DataCorruption):\n            transfer_manager.download_chunks_concurrently(\n                blob_mock,\n                FILENAME,\n                chunk_size=CHUNK_SIZE,\n                download_kwargs=DOWNLOAD_KWARGS,\n                worker_type=transfer_manager.THREAD,\n                crc32c_checksum=True,\n            )\n\n\ndef test_download_chunks_concurrently_raises_on_invalid_kwargs():\n    blob_mock = mock.Mock(spec=Blob)\n    FILENAME = \"file_a.txt\"\n    MULTIPLE = 4\n    blob_mock.size = CHUNK_SIZE * MULTIPLE\n\n    with mock.patch(\"google.cloud.storage.transfer_manager.open\", mock.mock_open()):\n        with pytest.raises(ValueError):\n            transfer_manager.download_chunks_concurrently(\n                blob_mock,\n                FILENAME,\n                chunk_size=CHUNK_SIZE,\n                worker_type=transfer_manager.THREAD,\n                download_kwargs={\n                    \"start\": CHUNK_SIZE,\n                },\n            )\n        with pytest.raises(ValueError):\n            transfer_manager.download_chunks_concurrently(\n                blob_mock,\n                FILENAME,\n                chunk_size=CHUNK_SIZE,\n                worker_type=transfer_manager.THREAD,\n                download_kwargs={\n                    \"end\": (CHUNK_SIZE * (MULTIPLE - 1)) - 1,\n                },\n            )\n        with pytest.raises(ValueError):\n            transfer_manager.download_chunks_concurrently(\n                blob_mock,\n                FILENAME,\n                chunk_size=CHUNK_SIZE,\n                worker_type=transfer_manager.THREAD,\n                download_kwargs={\n                    \"checksum\": \"crc32c\",\n                },\n            )\n\n\ndef test_download_chunks_concurrently_passes_concurrency_options():\n    blob_mock = mock.Mock(spec=Blob)\n    FILENAME = \"file_a.txt\"\n    MAX_WORKERS = 7\n    DEADLINE = 10\n    MULTIPLE = 4\n    blob_mock.size = CHUNK_SIZE * MULTIPLE\n\n    with mock.patch(\"concurrent.futures.ThreadPoolExecutor\") as pool_patch, mock.patch(\n        \"concurrent.futures.wait\"\n    ) as wait_patch, mock.patch(\n        \"google.cloud.storage.transfer_manager.open\", mock.mock_open()\n    ):\n        transfer_manager.download_chunks_concurrently(\n            blob_mock,\n            FILENAME,\n            chunk_size=CHUNK_SIZE,\n            deadline=DEADLINE,\n            worker_type=transfer_manager.THREAD,\n            max_workers=MAX_WORKERS,\n            crc32c_checksum=False,\n        )\n        pool_patch.assert_called_with(max_workers=MAX_WORKERS)\n        wait_patch.assert_called_with(mock.ANY, timeout=DEADLINE, return_when=mock.ANY)\n\n\ndef test_upload_chunks_concurrently():\n    bucket = mock.Mock()\n    bucket.name = \"bucket\"\n    bucket.client = _PickleableMockClient(identify_as_client=True)\n    transport = bucket.client._http\n    bucket.user_project = None\n\n    blob = Blob(\"blob\", bucket)\n    blob.content_type = FAKE_CONTENT_TYPE\n\n    FILENAME = \"file_a.txt\"\n    SIZE = 2048\n\n    container_mock = mock.Mock()\n    container_mock.upload_id = \"abcd\"\n    part_mock = mock.Mock()\n    ETAG = \"efgh\"\n    part_mock.etag = ETAG\n\n    with mock.patch(\"os.path.getsize\", return_value=SIZE), mock.patch(\n        \"google.cloud.storage.transfer_manager.XMLMPUContainer\",\n        return_value=container_mock,\n    ), mock.patch(\n        \"google.cloud.storage.transfer_manager.XMLMPUPart\", return_value=part_mock\n    ):\n        transfer_manager.upload_chunks_concurrently(\n            FILENAME,\n            blob,\n            chunk_size=SIZE // 2,\n            worker_type=transfer_manager.THREAD,\n        )\n        container_mock.initiate.assert_called_once_with(\n            transport=transport, content_type=blob.content_type\n        )\n        container_mock.register_part.assert_any_call(1, ETAG)\n        container_mock.register_part.assert_any_call(2, ETAG)\n        container_mock.finalize.assert_called_once_with(bucket.client._http)\n\n        assert container_mock._retry_strategy.max_sleep == 60.0\n        assert container_mock._retry_strategy.max_cumulative_retry == 120.0\n        assert container_mock._retry_strategy.max_retries is None\n\n        part_mock.upload.assert_called_with(transport)\n\n\ndef test_upload_chunks_concurrently_passes_concurrency_options():\n    bucket = mock.Mock()\n    bucket.name = \"bucket\"\n    bucket.client = _PickleableMockClient(identify_as_client=True)\n    transport = bucket.client._http\n    bucket.user_project = None\n\n    blob = Blob(\"blob\", bucket)\n\n    FILENAME = \"file_a.txt\"\n    SIZE = 2048\n\n    container_mock = mock.Mock()\n    container_mock.upload_id = \"abcd\"\n\n    MAX_WORKERS = 7\n    DEADLINE = 10\n\n    with mock.patch(\"os.path.getsize\", return_value=SIZE), mock.patch(\n        \"google.cloud.storage.transfer_manager.XMLMPUContainer\",\n        return_value=container_mock,\n    ), mock.patch(\"concurrent.futures.ThreadPoolExecutor\") as pool_patch, mock.patch(\n        \"concurrent.futures.wait\"\n    ) as wait_patch:\n        try:\n            transfer_manager.upload_chunks_concurrently(\n                FILENAME,\n                blob,\n                chunk_size=SIZE // 2,\n                worker_type=transfer_manager.THREAD,\n                max_workers=MAX_WORKERS,\n                deadline=DEADLINE,\n                retry=None,\n            )\n        except ValueError:\n            pass  # The futures don't actually work, so we expect this to abort.\n            # Conveniently, that gives us a chance to test the auto-delete\n            # exception handling feature.\n        container_mock.cancel.assert_called_once_with(transport)\n        assert container_mock._retry_strategy.max_retries == 0\n\n        pool_patch.assert_called_with(max_workers=MAX_WORKERS)\n        wait_patch.assert_called_with(mock.ANY, timeout=DEADLINE, return_when=mock.ANY)\n\n\ndef test_upload_chunks_concurrently_with_metadata_and_encryption():\n    import datetime\n    from google.cloud.storage._helpers import _UTC\n    from google.cloud._helpers import _RFC3339_MICROS\n\n    now = datetime.datetime.now(_UTC)\n    now_str = now.strftime(_RFC3339_MICROS)\n\n    custom_metadata = {\"key_a\": \"value_a\", \"key_b\": \"value_b\"}\n    encryption_key = \"b23ff11bba187db8c37077e6af3b25b8\"\n    kms_key_name = \"sample_key_name\"\n    custom_headers = {\n        \"x-goog-custom-audit-foo\": \"bar\",\n    }\n\n    METADATA = {\n        \"cache_control\": \"private\",\n        \"content_disposition\": \"inline\",\n        \"content_language\": \"en-US\",\n        \"custom_time\": now,\n        \"metadata\": custom_metadata,\n        \"storage_class\": \"NEARLINE\",\n    }\n\n    bucket = mock.Mock()\n    bucket.name = \"bucket\"\n    bucket.client = _PickleableMockClient(\n        identify_as_client=True, extra_headers=custom_headers\n    )\n    transport = bucket.client._http\n    user_project = \"my_project\"\n    bucket.user_project = user_project\n\n    blob = Blob(\"blob\", bucket, kms_key_name=kms_key_name)\n    blob.content_type = FAKE_CONTENT_TYPE\n\n    for key, value in METADATA.items():\n        setattr(blob, key, value)\n    blob.metadata = {**custom_metadata}\n    blob.encryption_key = encryption_key\n\n    FILENAME = \"file_a.txt\"\n    SIZE = 2048\n\n    container_mock = mock.Mock()\n    container_mock.upload_id = \"abcd\"\n    part_mock = mock.Mock()\n    ETAG = \"efgh\"\n    part_mock.etag = ETAG\n    container_cls_mock = mock.Mock(return_value=container_mock)\n\n    invocation_id = \"b9f8cbb0-6456-420c-819d-3f4ee3c0c455\"\n\n    with mock.patch(\"os.path.getsize\", return_value=SIZE), mock.patch(\n        \"google.cloud.storage.transfer_manager.XMLMPUContainer\", new=container_cls_mock\n    ), mock.patch(\n        \"google.cloud.storage.transfer_manager.XMLMPUPart\", return_value=part_mock\n    ), mock.patch(\n        \"google.cloud.storage._helpers._get_invocation_id\",\n        return_value=\"gccl-invocation-id/\" + invocation_id,\n    ):\n        transfer_manager.upload_chunks_concurrently(\n            FILENAME,\n            blob,\n            chunk_size=SIZE // 2,\n            worker_type=transfer_manager.THREAD,\n        )\n        expected_headers = {\n            \"Accept\": \"application/json\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"User-Agent\": \"agent\",\n            \"X-Goog-API-Client\": f\"agent gccl-invocation-id/{invocation_id} gccl-gcs-cmd/tm.upload_sharded\",\n            \"content-type\": FAKE_CONTENT_TYPE,\n            \"x-upload-content-type\": FAKE_CONTENT_TYPE,\n            \"X-Goog-Encryption-Algorithm\": \"AES256\",\n            \"X-Goog-Encryption-Key\": \"YjIzZmYxMWJiYTE4N2RiOGMzNzA3N2U2YWYzYjI1Yjg=\",\n            \"X-Goog-Encryption-Key-Sha256\": \"B25Y4hgVlNXDliAklsNz9ykLk7qvgqDrSbdds5iu8r4=\",\n            \"Cache-Control\": \"private\",\n            \"Content-Disposition\": \"inline\",\n            \"Content-Language\": \"en-US\",\n            \"x-goog-storage-class\": \"NEARLINE\",\n            \"x-goog-custom-time\": now_str,\n            \"x-goog-meta-key_a\": \"value_a\",\n            \"x-goog-meta-key_b\": \"value_b\",\n            \"x-goog-user-project\": \"my_project\",\n            \"x-goog-encryption-kms-key-name\": \"sample_key_name\",\n            **custom_headers,\n        }\n        container_cls_mock.assert_called_once_with(\n            URL, FILENAME, headers=expected_headers\n        )\n        container_mock.initiate.assert_called_once_with(\n            transport=transport, content_type=blob.content_type\n        )\n        container_mock.register_part.assert_any_call(1, ETAG)\n        container_mock.register_part.assert_any_call(2, ETAG)\n        container_mock.finalize.assert_called_once_with(transport)\n        part_mock.upload.assert_called_with(blob.client._http)\n\n\nclass _PickleableMockBlob:\n    def __init__(\n        self,\n        name=\"\",\n        size=None,\n        generation=None,\n        size_after_reload=None,\n        generation_after_reload=None,\n    ):\n        self.name = name\n        self.size = size\n        self.generation = generation\n        self._size_after_reload = size_after_reload\n        self._generation_after_reload = generation_after_reload\n        self.client = _PickleableMockClient()\n\n    def reload(self):\n        self.size = self._size_after_reload\n        self.generation = self._generation_after_reload\n\n    def _prep_and_do_download(self, *args, **kwargs):\n        return \"SUCCESS\"\n\n\nclass _PickleableMockConnection:\n    @staticmethod\n    def get_api_base_url_for_mtls():\n        return HOSTNAME\n\n    user_agent = USER_AGENT\n\n\nclass _PickleableMockClient:\n    def __init__(self, identify_as_client=False, extra_headers={}):\n        self._http = \"my_transport\"  # used as an identifier for \"called_with\"\n        self._connection = _PickleableMockConnection()\n        self.identify_as_client = identify_as_client\n        self._extra_headers = extra_headers\n\n    @property\n    def __class__(self):\n        if self.identify_as_client:\n            return Client\n        else:\n            return _PickleableMockClient\n\n\n# Used in subprocesses only, so excluded from coverage\ndef _validate_blob_token_in_subprocess_for_chunk(\n    maybe_pickled_blob, filename, **kwargs\n):  # pragma: NO COVER\n    blob = pickle.loads(maybe_pickled_blob)\n    assert isinstance(blob, _PickleableMockBlob)\n    assert filename.startswith(\"file\")\n    return FAKE_RESULT\n\n\ndef test_download_chunks_concurrently_with_processes():\n    blob = _PickleableMockBlob(\n        \"file_a_blob\", size_after_reload=24, generation_after_reload=100\n    )\n    FILENAME = \"file_a.txt\"\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._download_and_write_chunk_in_place\",\n        new=_validate_blob_token_in_subprocess_for_chunk,\n    ), mock.patch(\"google.cloud.storage.transfer_manager.open\", mock.mock_open()):\n        result = transfer_manager.download_chunks_concurrently(\n            blob,\n            FILENAME,\n            chunk_size=CHUNK_SIZE,\n            download_kwargs=DOWNLOAD_KWARGS,\n            worker_type=transfer_manager.PROCESS,\n            crc32c_checksum=False,\n        )\n    assert result is None\n\n\ndef test__LazyClient():\n    fake_cache = {}\n    MOCK_ID = 9999\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._cached_clients\", new=fake_cache\n    ), mock.patch(\"google.cloud.storage.transfer_manager.Client\"):\n        lazyclient = transfer_manager._LazyClient(MOCK_ID)\n        lazyclient_cached = transfer_manager._LazyClient(MOCK_ID)\n        assert lazyclient is lazyclient_cached\n        assert len(fake_cache) == 1\n\n\ndef test__pickle_client():\n    # This test nominally has coverage, but doesn't assert that the essential\n    # copyreg behavior in _pickle_client works. Unfortunately there doesn't seem\n    # to be a good way to check that without actually creating a Client, which\n    # will spin up HTTP connections undesirably. This is more fully checked in\n    # the system tests.\n    pkl = transfer_manager._pickle_client(FAKE_RESULT)\n    assert pickle.loads(pkl) == FAKE_RESULT\n\n\ndef test__download_and_write_chunk_in_place():\n    pickled_mock = pickle.dumps(_PickleableMockBlob())\n    FILENAME = \"file_a.txt\"\n    with mock.patch(\"google.cloud.storage.transfer_manager.open\", mock.mock_open()):\n        result = transfer_manager._download_and_write_chunk_in_place(\n            pickled_mock, FILENAME, 0, 8, {}, False\n        )\n    assert result is not None\n\n\ndef test__upload_part():\n    from google.cloud.storage.retry import DEFAULT_RETRY\n\n    pickled_mock = pickle.dumps(_PickleableMockClient())\n    FILENAME = \"file_a.txt\"\n    UPLOAD_ID = \"abcd\"\n    ETAG = \"efgh\"\n\n    part = mock.Mock()\n    part.etag = ETAG\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.XMLMPUPart\", return_value=part\n    ):\n        result = transfer_manager._upload_part(\n            pickled_mock,\n            URL,\n            UPLOAD_ID,\n            FILENAME,\n            0,\n            256,\n            1,\n            None,\n            {\"key\", \"value\"},\n            retry=DEFAULT_RETRY,\n        )\n        part.upload.assert_called_once()\n        assert part._retry_strategy.max_sleep == 60.0\n        assert part._retry_strategy.max_cumulative_retry == 120.0\n        assert part._retry_strategy.max_retries is None\n\n        assert result == (1, ETAG)\n\n\ndef test__get_pool_class_and_requirements_error():\n    with pytest.raises(ValueError):\n        transfer_manager._get_pool_class_and_requirements(\"garbage\")\n\n\ndef test__reduce_client():\n    fake_cache = {}\n    client = mock.Mock()\n    custom_headers = {\n        \"x-goog-custom-audit-foo\": \"bar\",\n    }\n    client._extra_headers = custom_headers\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager._cached_clients\", new=fake_cache\n    ), mock.patch(\"google.cloud.storage.transfer_manager.Client\"):\n        replicated_client, kwargs = transfer_manager._reduce_client(client)\n        assert replicated_client is not None\n        assert custom_headers in kwargs\n\n\ndef test__call_method_on_maybe_pickled_blob():\n    blob = mock.Mock(spec=Blob)\n    blob._prep_and_do_download.return_value = \"SUCCESS\"\n    result = transfer_manager._call_method_on_maybe_pickled_blob(\n        blob, \"_prep_and_do_download\"\n    )\n    assert result == \"SUCCESS\"\n\n    pickled_blob = pickle.dumps(_PickleableMockBlob())\n    result = transfer_manager._call_method_on_maybe_pickled_blob(\n        pickled_blob, \"_prep_and_do_download\"\n    )\n    assert result == \"SUCCESS\"\n\n\ndef test__ChecksummingSparseFileWrapper():\n    FILENAME = \"file_a.txt\"\n    import google_crc32c\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.open\", mock.mock_open()\n    ) as open_mock:\n        # test no checksumming\n        wrapper = transfer_manager._ChecksummingSparseFileWrapper(FILENAME, 0, False)\n        wrapper.write(b\"abcdefgh\")\n        handle = open_mock()\n        handle.write.assert_called_with(b\"abcdefgh\")\n        wrapper.write(b\"ijklmnop\")\n        assert wrapper.crc is None\n        handle.write.assert_called_with(b\"ijklmnop\")\n\n    with mock.patch(\n        \"google.cloud.storage.transfer_manager.open\", mock.mock_open()\n    ) as open_mock:\n        wrapper = transfer_manager._ChecksummingSparseFileWrapper(FILENAME, 0, True)\n        wrapper.write(b\"abcdefgh\")\n        handle = open_mock()\n        handle.write.assert_called_with(b\"abcdefgh\")\n        wrapper.write(b\"ijklmnop\")\n        assert wrapper.crc == google_crc32c.value(b\"abcdefghijklmnop\")\n        handle.write.assert_called_with(b\"ijklmnop\")\n", "tests/unit/test_batch.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport http.client\nfrom http.client import SERVICE_UNAVAILABLE\nfrom http.client import NO_CONTENT\nimport unittest\n\nimport mock\nimport requests\n\nfrom google.cloud.storage._helpers import _DEFAULT_UNIVERSE_DOMAIN\n\n\ndef _make_credentials():\n    import google.auth.credentials\n\n    return mock.Mock(\n        spec=google.auth.credentials.Credentials,\n        universe_domain=_DEFAULT_UNIVERSE_DOMAIN,\n    )\n\n\ndef _make_response(status=http.client.OK, content=b\"\", headers={}):\n    response = requests.Response()\n    response.status_code = status\n    response._content = content\n    response.headers = headers\n    response.request = requests.Request()\n    return response\n\n\ndef _make_requests_session(responses):\n    session = mock.create_autospec(requests.Session, instance=True)\n    session.request.side_effect = responses\n    return session\n\n\nclass TestMIMEApplicationHTTP(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.batch import MIMEApplicationHTTP\n\n        return MIMEApplicationHTTP\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor_body_None(self):\n        METHOD = \"DELETE\"\n        PATH = \"/path/to/api\"\n        LINES = [\"DELETE /path/to/api HTTP/1.1\", \"\"]\n        mah = self._make_one(METHOD, PATH, {}, None)\n        self.assertEqual(mah.get_content_type(), \"application/http\")\n        self.assertEqual(mah.get_payload().splitlines(), LINES)\n\n    def test_ctor_body_str(self):\n        METHOD = \"GET\"\n        PATH = \"/path/to/api\"\n        BODY = \"ABC\"\n        HEADERS = {\"Content-Length\": len(BODY), \"Content-Type\": \"text/plain\"}\n        LINES = [\n            \"GET /path/to/api HTTP/1.1\",\n            \"Content-Length: 3\",\n            \"Content-Type: text/plain\",\n            \"\",\n            \"ABC\",\n        ]\n        mah = self._make_one(METHOD, PATH, HEADERS, BODY)\n        self.assertEqual(mah.get_payload().splitlines(), LINES)\n\n    def test_ctor_body_dict(self):\n        METHOD = \"GET\"\n        PATH = \"/path/to/api\"\n        BODY = {\"foo\": \"bar\"}\n        HEADERS = {}\n        LINES = [\n            \"GET /path/to/api HTTP/1.1\",\n            \"Content-Length: 14\",\n            \"Content-Type: application/json\",\n            \"\",\n            '{\"foo\": \"bar\"}',\n        ]\n        mah = self._make_one(METHOD, PATH, HEADERS, BODY)\n        self.assertEqual(mah.get_payload().splitlines(), LINES)\n\n\nclass TestBatch(unittest.TestCase):\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.batch import Batch\n\n        return Batch\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor(self):\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        self.assertIs(batch._client, client)\n        self.assertEqual(len(batch._requests), 0)\n        self.assertEqual(len(batch._target_objects), 0)\n\n    def test_current(self):\n        from google.cloud.storage.client import Client\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = Client(project=project, credentials=credentials)\n        batch1 = self._make_one(client)\n        self.assertIsNone(batch1.current())\n\n        client._push_batch(batch1)\n        self.assertIs(batch1.current(), batch1)\n\n        batch2 = self._make_one(client)\n        client._push_batch(batch2)\n        self.assertIs(batch1.current(), batch2)\n\n    def test__make_request_GET_normal(self):\n        from google.cloud.storage.batch import _FutureDict\n\n        url = \"http://example.com/api\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        target = _MockObject()\n\n        response = batch._make_request(\"GET\", url, target_object=target)\n\n        # Check the respone\n        self.assertEqual(response.status_code, 204)\n        self.assertIsInstance(response.json(), _FutureDict)\n        self.assertIsInstance(response.content, _FutureDict)\n        self.assertIs(target._properties, response.content)\n\n        # The real http request should not have been called yet.\n        http.request.assert_not_called()\n\n        # Check the queued request\n        self.assertEqual(len(batch._requests), 1)\n        request = batch._requests[0]\n        request_method, request_url, _, request_data, _ = request\n        self.assertEqual(request_method, \"GET\")\n        self.assertEqual(request_url, url)\n        self.assertIsNone(request_data)\n\n    def test__make_request_POST_normal(self):\n        from google.cloud.storage.batch import _FutureDict\n\n        url = \"http://example.com/api\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        data = {\"foo\": 1}\n        target = _MockObject()\n\n        response = batch._make_request(\n            \"POST\", url, data={\"foo\": 1}, target_object=target\n        )\n\n        self.assertEqual(response.status_code, 204)\n        self.assertIsInstance(response.content, _FutureDict)\n        self.assertIs(target._properties, response.content)\n\n        # The real http request should not have been called yet.\n        http.request.assert_not_called()\n\n        request = batch._requests[0]\n        request_method, request_url, _, request_data, _ = request\n        self.assertEqual(request_method, \"POST\")\n        self.assertEqual(request_url, url)\n        self.assertEqual(request_data, data)\n\n    def test__make_request_PATCH_normal(self):\n        from google.cloud.storage.batch import _FutureDict\n\n        url = \"http://example.com/api\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        data = {\"foo\": 1}\n        target = _MockObject()\n\n        response = batch._make_request(\n            \"PATCH\", url, data={\"foo\": 1}, target_object=target\n        )\n\n        self.assertEqual(response.status_code, 204)\n        self.assertIsInstance(response.content, _FutureDict)\n        self.assertIs(target._properties, response.content)\n\n        # The real http request should not have been called yet.\n        http.request.assert_not_called()\n\n        request = batch._requests[0]\n        request_method, request_url, _, request_data, _ = request\n        self.assertEqual(request_method, \"PATCH\")\n        self.assertEqual(request_url, url)\n        self.assertEqual(request_data, data)\n\n    def test__make_request_DELETE_normal(self):\n        from google.cloud.storage.batch import _FutureDict\n\n        url = \"http://example.com/api\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        target = _MockObject()\n\n        response = batch._make_request(\"DELETE\", url, target_object=target)\n\n        # Check the respone\n        self.assertEqual(response.status_code, 204)\n        self.assertIsInstance(response.content, _FutureDict)\n        self.assertIs(target._properties, response.content)\n\n        # The real http request should not have been called yet.\n        http.request.assert_not_called()\n\n        # Check the queued request\n        self.assertEqual(len(batch._requests), 1)\n        request = batch._requests[0]\n        request_method, request_url, _, request_data, _ = request\n        self.assertEqual(request_method, \"DELETE\")\n        self.assertEqual(request_url, url)\n        self.assertIsNone(request_data)\n\n    def test__make_request_POST_too_many_requests(self):\n        url = \"http://example.com/api\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n\n        batch._MAX_BATCH_SIZE = 1\n        batch._requests.append((\"POST\", url, {}, {\"bar\": 2}))\n\n        with self.assertRaises(ValueError):\n            batch._make_request(\"POST\", url, data={\"foo\": 1})\n\n    def test_finish_empty(self):\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n\n        with self.assertRaises(ValueError):\n            batch.finish()\n\n    def _get_payload_chunks(self, boundary, payload):\n        divider = \"--\" + boundary[len('boundary=\"') : -1]\n        chunks = payload.split(divider)[1:-1]  # discard prolog / epilog\n        return chunks\n\n    def _check_subrequest_no_payload(self, chunk, method, url):\n        lines = chunk.splitlines()\n        # blank + 2 headers + blank + request + blank + blank\n        self.assertEqual(len(lines), 7)\n        self.assertEqual(lines[0], \"\")\n        self.assertEqual(lines[1], \"Content-Type: application/http\")\n        self.assertEqual(lines[2], \"MIME-Version: 1.0\")\n        self.assertEqual(lines[3], \"\")\n        self.assertEqual(lines[4], f\"{method} {url} HTTP/1.1\")\n        self.assertEqual(lines[5], \"\")\n        self.assertEqual(lines[6], \"\")\n\n    def _check_subrequest_payload(self, chunk, method, url, payload):\n        import json\n\n        lines = chunk.splitlines()\n        # blank + 2 headers + blank + request + 2 headers + blank + body\n        payload_str = json.dumps(payload)\n        self.assertEqual(lines[0], \"\")\n        self.assertEqual(lines[1], \"Content-Type: application/http\")\n        self.assertEqual(lines[2], \"MIME-Version: 1.0\")\n        self.assertEqual(lines[3], \"\")\n        self.assertEqual(lines[4], f\"{method} {url} HTTP/1.1\")\n        if method == \"GET\":\n            self.assertEqual(len(lines), 7)\n            self.assertEqual(lines[5], \"\")\n            self.assertEqual(lines[6], \"\")\n        else:\n            self.assertEqual(len(lines), 9)\n            self.assertEqual(lines[5], f\"Content-Length: {len(payload_str)}\")\n            self.assertEqual(lines[6], \"Content-Type: application/json\")\n            self.assertEqual(lines[7], \"\")\n            self.assertEqual(json.loads(lines[8]), payload)\n\n    def _get_mutlipart_request(self, http):\n        request_call = http.request.mock_calls[0][2]\n        request_headers = request_call[\"headers\"]\n        request_body = request_call[\"data\"]\n        content_type, boundary = [\n            value.strip() for value in request_headers[\"Content-Type\"].split(\";\")\n        ]\n\n        return request_headers, request_body, content_type, boundary\n\n    def test_finish_nonempty(self):\n        url = \"http://api.example.com/other_api\"\n        expected_response = _make_response(\n            content=_THREE_PART_MIME_RESPONSE,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        batch.API_BASE_URL = \"http://api.example.com\"\n\n        batch._do_request(\"POST\", url, {}, {\"foo\": 1, \"bar\": 2}, None)\n        batch._do_request(\"PATCH\", url, {}, {\"bar\": 3}, None)\n        batch._do_request(\"DELETE\", url, {}, None, None)\n        result = batch.finish()\n\n        self.assertEqual(len(result), len(batch._requests))\n        self.assertEqual(len(result), len(batch._responses))\n\n        response1, response2, response3 = result\n\n        self.assertEqual(\n            response1.headers,\n            {\"Content-Length\": \"20\", \"Content-Type\": \"application/json; charset=UTF-8\"},\n        )\n        self.assertEqual(response1.json(), {\"foo\": 1, \"bar\": 2})\n\n        self.assertEqual(\n            response2.headers,\n            {\"Content-Length\": \"20\", \"Content-Type\": \"application/json; charset=UTF-8\"},\n        )\n        self.assertEqual(response2.json(), {\"foo\": 1, \"bar\": 3})\n\n        self.assertEqual(response3.headers, {\"Content-Length\": \"0\"})\n        self.assertEqual(response3.status_code, NO_CONTENT)\n\n        expected_url = f\"{batch.API_BASE_URL}/batch/storage/v1\"\n        http.request.assert_called_once_with(\n            method=\"POST\",\n            url=expected_url,\n            headers=mock.ANY,\n            data=mock.ANY,\n            timeout=self._get_default_timeout(),\n        )\n\n        request_info = self._get_mutlipart_request(http)\n        request_headers, request_body, content_type, boundary = request_info\n\n        self.assertEqual(content_type, \"multipart/mixed\")\n        self.assertTrue(boundary.startswith('boundary=\"=='))\n        self.assertTrue(boundary.endswith('==\"'))\n        self.assertEqual(request_headers[\"MIME-Version\"], \"1.0\")\n\n        chunks = self._get_payload_chunks(boundary, request_body)\n        self.assertEqual(len(chunks), 3)\n        self._check_subrequest_payload(chunks[0], \"POST\", url, {\"foo\": 1, \"bar\": 2})\n        self._check_subrequest_payload(chunks[1], \"PATCH\", url, {\"bar\": 3})\n        self._check_subrequest_no_payload(chunks[2], \"DELETE\", url)\n\n    def test_finish_responses_mismatch(self):\n        url = \"http://api.example.com/other_api\"\n        expected_response = _make_response(\n            content=_TWO_PART_MIME_RESPONSE_WITH_FAIL,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        batch.API_BASE_URL = \"http://api.example.com\"\n\n        batch._requests.append((\"GET\", url, {}, None))\n        with self.assertRaises(ValueError):\n            batch.finish()\n\n    def test_finish_nonempty_with_status_failure(self):\n        from google.cloud.exceptions import NotFound\n\n        url = \"http://api.example.com/other_api\"\n        expected_response = _make_response(\n            content=_TWO_PART_MIME_RESPONSE_WITH_FAIL,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        batch.API_BASE_URL = \"http://api.example.com\"\n        target1 = _MockObject()\n        target2 = _MockObject()\n\n        batch._do_request(\"GET\", url, {}, None, target1, timeout=42)\n        batch._do_request(\"GET\", url, {}, None, target2, timeout=420)\n\n        # Make sure futures are not populated.\n        self.assertEqual(\n            [future for future in batch._target_objects], [target1, target2]\n        )\n        target2_future_before = target2._properties\n\n        with self.assertRaises(NotFound):\n            batch.finish()\n\n        self.assertEqual(target1._properties, {\"foo\": 1, \"bar\": 2})\n        self.assertIs(target2._properties, target2_future_before)\n\n        expected_url = f\"{batch.API_BASE_URL}/batch/storage/v1\"\n        http.request.assert_called_once_with(\n            method=\"POST\",\n            url=expected_url,\n            headers=mock.ANY,\n            data=mock.ANY,\n            timeout=420,  # the last request timeout prevails\n        )\n\n        _, request_body, _, boundary = self._get_mutlipart_request(http)\n\n        chunks = self._get_payload_chunks(boundary, request_body)\n        self.assertEqual(len(chunks), 2)\n        self._check_subrequest_payload(chunks[0], \"GET\", url, {})\n        self._check_subrequest_payload(chunks[1], \"GET\", url, {})\n\n    def test_finish_no_raise_exception(self):\n        url = \"http://api.example.com/other_api\"\n        expected_response = _make_response(\n            content=_TWO_PART_MIME_RESPONSE_WITH_FAIL,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        batch.API_BASE_URL = \"http://api.example.com\"\n        target1 = _MockObject()\n        target2 = _MockObject()\n\n        batch._do_request(\"GET\", url, {}, None, target1, timeout=42)\n        batch._do_request(\"GET\", url, {}, None, target2, timeout=420)\n\n        # Make sure futures are not populated.\n        self.assertEqual(\n            [future for future in batch._target_objects], [target1, target2]\n        )\n\n        batch.finish(raise_exception=False)\n\n        self.assertEqual(len(batch._requests), 2)\n        self.assertEqual(len(batch._responses), 2)\n\n        # Make sure NotFound exception is added to responses and target2\n        self.assertEqual(target1._properties, {\"foo\": 1, \"bar\": 2})\n        self.assertEqual(target2._properties, {\"error\": {\"message\": \"Not Found\"}})\n\n        expected_url = f\"{batch.API_BASE_URL}/batch/storage/v1\"\n        http.request.assert_called_once_with(\n            method=\"POST\",\n            url=expected_url,\n            headers=mock.ANY,\n            data=mock.ANY,\n            timeout=420,  # the last request timeout prevails\n        )\n\n        _, request_body, _, boundary = self._get_mutlipart_request(http)\n\n        chunks = self._get_payload_chunks(boundary, request_body)\n        self.assertEqual(len(chunks), 2)\n        self._check_subrequest_payload(chunks[0], \"GET\", url, {})\n        self._check_subrequest_payload(chunks[1], \"GET\", url, {})\n        self.assertEqual(batch._responses[0].status_code, 200)\n        self.assertEqual(batch._responses[1].status_code, 404)\n\n    def test_finish_nonempty_non_multipart_response(self):\n        url = \"http://api.example.com/other_api\"\n        http = _make_requests_session([_make_response()])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        batch._requests.append((\"POST\", url, {}, {\"foo\": 1, \"bar\": 2}))\n\n        with self.assertRaises(ValueError):\n            batch.finish()\n\n    def test_finish_multipart_response_with_status_failure(self):\n        from google.cloud.exceptions import ServiceUnavailable\n\n        url = \"http://api.example.com/other_api\"\n        expected_response = _make_response(\n            status=SERVICE_UNAVAILABLE,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        batch.API_BASE_URL = \"http://api.example.com\"\n        batch._requests.append((\"POST\", url, {}, {\"foo\": 1, \"bar\": 2}, None))\n\n        with self.assertRaises(ServiceUnavailable):\n            batch.finish()\n\n    def test_as_context_mgr_wo_error(self):\n        from google.cloud.storage.client import Client\n\n        url = \"http://example.com/api\"\n        expected_response = _make_response(\n            content=_THREE_PART_MIME_RESPONSE,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = Client(project=project, credentials=credentials)\n        client._http_internal = http\n\n        self.assertEqual(list(client._batch_stack), [])\n\n        target1 = _MockObject()\n        target2 = _MockObject()\n        target3 = _MockObject()\n\n        with self._make_one(client) as batch:\n            self.assertEqual(list(client._batch_stack), [batch])\n            batch._make_request(\n                \"POST\", url, {\"foo\": 1, \"bar\": 2}, target_object=target1\n            )\n            batch._make_request(\"PATCH\", url, {\"bar\": 3}, target_object=target2)\n            batch._make_request(\"DELETE\", url, target_object=target3)\n\n        self.assertEqual(list(client._batch_stack), [])\n        self.assertEqual(len(batch._requests), 3)\n        self.assertEqual(len(batch._responses), 3)\n        self.assertEqual(batch._requests[0][0], \"POST\")\n        self.assertEqual(batch._requests[1][0], \"PATCH\")\n        self.assertEqual(batch._requests[2][0], \"DELETE\")\n        self.assertEqual(batch._target_objects, [target1, target2, target3])\n        self.assertEqual(target1._properties, {\"foo\": 1, \"bar\": 2})\n        self.assertEqual(target2._properties, {\"foo\": 1, \"bar\": 3})\n        self.assertEqual(target3._properties, b\"\")\n\n    def test_as_context_mgr_no_raise_exception(self):\n        from google.cloud.storage.client import Client\n\n        url = \"http://api.example.com/other_api\"\n        expected_response = _make_response(\n            content=_TWO_PART_MIME_RESPONSE_WITH_FAIL,\n            headers={\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'},\n        )\n        http = _make_requests_session([expected_response])\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = Client(project=project, credentials=credentials)\n        client._http_internal = http\n\n        self.assertEqual(list(client._batch_stack), [])\n\n        target1 = _MockObject()\n        target2 = _MockObject()\n\n        with self._make_one(client, raise_exception=False) as batch:\n            self.assertEqual(list(client._batch_stack), [batch])\n            batch._make_request(\"GET\", url, {}, target_object=target1)\n            batch._make_request(\"GET\", url, {}, target_object=target2)\n\n        self.assertEqual(list(client._batch_stack), [])\n        self.assertEqual(len(batch._requests), 2)\n        self.assertEqual(len(batch._responses), 2)\n        self.assertEqual(batch._requests[0][0], \"GET\")\n        self.assertEqual(batch._requests[1][0], \"GET\")\n        self.assertEqual(batch._target_objects, [target1, target2])\n\n        # Make sure NotFound exception is added to responses and target2\n        self.assertEqual(batch._responses[0].status_code, 200)\n        self.assertEqual(batch._responses[1].status_code, 404)\n        self.assertEqual(target1._properties, {\"foo\": 1, \"bar\": 2})\n        self.assertEqual(target2._properties, {\"error\": {\"message\": \"Not Found\"}})\n\n    def test_as_context_mgr_w_error(self):\n        from google.cloud.storage.batch import _FutureDict\n        from google.cloud.storage.client import Client\n\n        URL = \"http://example.com/api\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = Client(project=project, credentials=credentials)\n        client._base_connection = connection\n\n        self.assertEqual(list(client._batch_stack), [])\n\n        target1 = _MockObject()\n        target2 = _MockObject()\n        target3 = _MockObject()\n        try:\n            with self._make_one(client) as batch:\n                self.assertEqual(list(client._batch_stack), [batch])\n                batch._make_request(\n                    \"POST\", URL, {\"foo\": 1, \"bar\": 2}, target_object=target1\n                )\n                batch._make_request(\"PATCH\", URL, {\"bar\": 3}, target_object=target2)\n                batch._make_request(\"DELETE\", URL, target_object=target3)\n                raise ValueError()\n        except ValueError:\n            pass\n\n        http.request.assert_not_called()\n        self.assertEqual(list(client._batch_stack), [])\n        self.assertEqual(len(batch._requests), 3)\n        self.assertEqual(batch._target_objects, [target1, target2, target3])\n        # Since the context manager fails, finish will not get called and\n        # the _properties will still be futures.\n        self.assertIsInstance(target1._properties, _FutureDict)\n        self.assertIsInstance(target2._properties, _FutureDict)\n        self.assertIsInstance(target3._properties, _FutureDict)\n\n    def test_respect_client_existing_connection(self):\n        client_endpoint = \"http://localhost:9023\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http, api_endpoint=client_endpoint)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        self.assertEqual(batch.API_BASE_URL, client_endpoint)\n        self.assertEqual(batch._client._connection.API_BASE_URL, client_endpoint)\n\n    def test_use_default_api_without_existing_connection(self):\n        default_api_endpoint = \"https://storage.googleapis.com\"\n        http = _make_requests_session([])\n        connection = _Connection(http=http)\n        client = _Client(connection)\n        batch = self._make_one(client)\n        self.assertEqual(batch.API_BASE_URL, default_api_endpoint)\n        self.assertIsNone(batch._client._connection.API_BASE_URL)\n        self.assertIsNone(batch._client._connection._client_info)\n\n\nclass Test__unpack_batch_response(unittest.TestCase):\n    def _call_fut(self, headers, content):\n        from google.cloud.storage.batch import _unpack_batch_response\n\n        response = _make_response(content=content, headers=headers)\n\n        return _unpack_batch_response(response)\n\n    def _unpack_helper(self, response, content):\n        result = list(self._call_fut(response, content))\n        self.assertEqual(len(result), 3)\n\n        self.assertEqual(result[0].status_code, http.client.OK)\n        self.assertEqual(result[0].json(), {\"bar\": 2, \"foo\": 1})\n        self.assertEqual(result[1].status_code, http.client.OK)\n        self.assertEqual(result[1].json(), {\"foo\": 1, \"bar\": 3})\n        self.assertEqual(result[2].status_code, http.client.NO_CONTENT)\n\n    def test_bytes_headers(self):\n        RESPONSE = {\"content-type\": b'multipart/mixed; boundary=\"DEADBEEF=\"'}\n        CONTENT = _THREE_PART_MIME_RESPONSE\n        self._unpack_helper(RESPONSE, CONTENT)\n\n    def test_unicode_headers(self):\n        RESPONSE = {\"content-type\": 'multipart/mixed; boundary=\"DEADBEEF=\"'}\n        CONTENT = _THREE_PART_MIME_RESPONSE\n        self._unpack_helper(RESPONSE, CONTENT)\n\n\n_TWO_PART_MIME_RESPONSE_WITH_FAIL = b\"\"\"\\\n--DEADBEEF=\nContent-Type: application/json\nContent-ID: <response-8a09ca85-8d1d-4f45-9eb0-da8e8b07ec83+1>\n\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=UTF-8\nContent-Length: 20\n\n{\"foo\": 1, \"bar\": 2}\n\n--DEADBEEF=\nContent-Type: application/json\nContent-ID: <response-8a09ca85-8d1d-4f45-9eb0-da8e8b07ec83+2>\n\nHTTP/1.1 404 Not Found\nContent-Type: application/json; charset=UTF-8\nContent-Length: 35\n\n{\"error\": {\"message\": \"Not Found\"}}\n\n--DEADBEEF=--\n\"\"\"\n\n_THREE_PART_MIME_RESPONSE = b\"\"\"\\\n--DEADBEEF=\nContent-Type: application/json\nContent-ID: <response-8a09ca85-8d1d-4f45-9eb0-da8e8b07ec83+1>\n\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=UTF-8\nContent-Length: 20\n\n{\"foo\": 1, \"bar\": 2}\n\n--DEADBEEF=\nContent-Type: application/json\nContent-ID: <response-8a09ca85-8d1d-4f45-9eb0-da8e8b07ec83+2>\n\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=UTF-8\nContent-Length: 20\n\n{\"foo\": 1, \"bar\": 3}\n\n--DEADBEEF=\nContent-Type: text/plain\nContent-ID: <response-8a09ca85-8d1d-4f45-9eb0-da8e8b07ec83+3>\n\nHTTP/1.1 204 No Content\nContent-Length: 0\n\n--DEADBEEF=--\n\"\"\"\n\n\nclass Test__FutureDict(unittest.TestCase):\n    def _make_one(self, *args, **kw):\n        from google.cloud.storage.batch import _FutureDict\n\n        return _FutureDict(*args, **kw)\n\n    def test_get(self):\n        future = self._make_one()\n        self.assertRaises(KeyError, future.get, None)\n\n    def test___getitem__(self):\n        future = self._make_one()\n        value = orig_value = object()\n        with self.assertRaises(KeyError):\n            value = future[None]\n        self.assertIs(value, orig_value)\n\n    def test___setitem__(self):\n        future = self._make_one()\n        with self.assertRaises(KeyError):\n            future[None] = None\n\n\nclass _Connection(object):\n    project = \"TESTING\"\n\n    def __init__(self, **kw):\n        self.__dict__.update(kw)\n        self._client_info = kw.get(\"client_info\", None)\n        self.API_BASE_URL = kw.get(\"api_endpoint\", None)\n\n    def _make_request(self, method, url, data=None, headers=None, timeout=None):\n        return self.http.request(\n            url=url, method=method, headers=headers, data=data, timeout=timeout\n        )\n\n\nclass _MockObject(object):\n    pass\n\n\nclass _Client(object):\n    def __init__(self, connection):\n        self._base_connection = connection\n        self._connection = connection\n", "tests/unit/test_client.py": "# Copyright 2015 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport http.client\nimport io\nimport json\nfrom unittest.mock import patch\nimport mock\nimport pytest\nimport re\nimport requests\nimport unittest\nimport urllib\n\nfrom google.api_core import exceptions\nfrom google.auth.credentials import AnonymousCredentials\nfrom google.oauth2.service_account import Credentials\n\nfrom google.cloud.storage import _helpers\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\nfrom google.cloud.storage._helpers import STORAGE_EMULATOR_ENV_VAR\nfrom google.cloud.storage._helpers import _API_ENDPOINT_OVERRIDE_ENV_VAR\nfrom google.cloud.storage._helpers import _get_default_headers\nfrom google.cloud.storage._helpers import _DEFAULT_UNIVERSE_DOMAIN\nfrom google.cloud.storage._http import Connection\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom tests.unit.test__helpers import GCCL_INVOCATION_TEST_CONST\nfrom . import _read_local_json\n\n_SERVICE_ACCOUNT_JSON = _read_local_json(\"url_signer_v4_test_account.json\")\n_CONFORMANCE_TESTS = _read_local_json(\"url_signer_v4_test_data.json\")[\n    \"postPolicyV4Tests\"\n]\n_POST_POLICY_TESTS = [test for test in _CONFORMANCE_TESTS if \"policyInput\" in test]\n_FAKE_CREDENTIALS = Credentials.from_service_account_info(_SERVICE_ACCOUNT_JSON)\n\n\ndef _make_credentials(project=None, universe_domain=_DEFAULT_UNIVERSE_DOMAIN):\n    import google.auth.credentials\n\n    if project is not None:\n        return mock.Mock(\n            spec=google.auth.credentials.Credentials,\n            project_id=project,\n            universe_domain=universe_domain,\n        )\n\n    return mock.Mock(\n        spec=google.auth.credentials.Credentials, universe_domain=universe_domain\n    )\n\n\ndef _create_signing_credentials():\n    import google.auth.credentials\n\n    class _SigningCredentials(\n        google.auth.credentials.Credentials, google.auth.credentials.Signing\n    ):\n        pass\n\n    credentials = mock.Mock(\n        spec=_SigningCredentials, universe_domain=_DEFAULT_UNIVERSE_DOMAIN\n    )\n    credentials.sign_bytes = mock.Mock(return_value=b\"Signature_bytes\")\n    credentials.signer_email = \"test@mail.com\"\n    return credentials\n\n\ndef _make_connection(*responses):\n    import google.cloud.storage._http\n    from google.cloud.exceptions import NotFound\n\n    mock_conn = mock.create_autospec(google.cloud.storage._http.Connection)\n    mock_conn.user_agent = \"testing 1.2.3\"\n    mock_conn.api_request.side_effect = list(responses) + [NotFound(\"miss\")]\n    return mock_conn\n\n\ndef _make_response(status=http.client.OK, content=b\"\", headers={}):\n    response = requests.Response()\n    response.status_code = status\n    response._content = content\n    response.headers = headers\n    response.request = requests.Request()\n    return response\n\n\ndef _make_json_response(data, status=http.client.OK, headers=None):\n    headers = headers or {}\n    headers[\"Content-Type\"] = \"application/json\"\n    return _make_response(\n        status=status, content=json.dumps(data).encode(\"utf-8\"), headers=headers\n    )\n\n\ndef _make_requests_session(responses):\n    session = mock.create_autospec(requests.Session, instance=True)\n    session.request.side_effect = responses\n    session.is_mtls = False\n    return session\n\n\nclass TestClient(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.client import Client\n\n        return Client\n\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor_connection_type(self):\n        from google.cloud._http import ClientInfo\n\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials()\n\n        client = self._make_one(project=PROJECT, credentials=credentials)\n\n        self.assertEqual(client.project, PROJECT)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIs(client._connection.credentials, credentials)\n        self.assertIsNone(client.current_batch)\n        self.assertEqual(list(client._batch_stack), [])\n        self.assertIsInstance(client._connection._client_info, ClientInfo)\n        self.assertEqual(\n            client._connection.API_BASE_URL, Connection.DEFAULT_API_ENDPOINT\n        )\n\n    def test_ctor_w_empty_client_options(self):\n        from google.api_core.client_options import ClientOptions\n\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials()\n        client_options = ClientOptions()\n\n        client = self._make_one(\n            project=PROJECT, credentials=credentials, client_options=client_options\n        )\n\n        self.assertEqual(\n            client._connection.API_BASE_URL, client._connection.DEFAULT_API_ENDPOINT\n        )\n\n    def test_ctor_w_client_options_dict(self):\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials()\n        api_endpoint = \"https://www.foo-googleapis.com\"\n        client_options = {\"api_endpoint\": api_endpoint}\n\n        client = self._make_one(\n            project=PROJECT, credentials=credentials, client_options=client_options\n        )\n\n        self.assertEqual(client._connection.API_BASE_URL, api_endpoint)\n        self.assertEqual(client.api_endpoint, api_endpoint)\n\n    def test_ctor_w_client_options_object(self):\n        from google.api_core.client_options import ClientOptions\n\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials()\n        api_endpoint = \"https://www.foo-googleapis.com\"\n        client_options = ClientOptions(api_endpoint=api_endpoint)\n\n        client = self._make_one(\n            project=PROJECT, credentials=credentials, client_options=client_options\n        )\n\n        self.assertEqual(client._connection.API_BASE_URL, api_endpoint)\n        self.assertEqual(client.api_endpoint, api_endpoint)\n\n    def test_ctor_w_universe_domain_and_matched_credentials(self):\n        PROJECT = \"PROJECT\"\n        universe_domain = \"example.com\"\n        expected_api_endpoint = f\"https://storage.{universe_domain}\"\n        credentials = _make_credentials(universe_domain=universe_domain)\n        client_options = {\"universe_domain\": universe_domain}\n\n        client = self._make_one(\n            project=PROJECT, credentials=credentials, client_options=client_options\n        )\n\n        self.assertEqual(client._connection.API_BASE_URL, expected_api_endpoint)\n        self.assertEqual(client.api_endpoint, expected_api_endpoint)\n        self.assertEqual(client.universe_domain, universe_domain)\n\n    def test_ctor_w_universe_domain_and_mismatched_credentials(self):\n        PROJECT = \"PROJECT\"\n        universe_domain = \"example.com\"\n        credentials = _make_credentials()  # default universe domain\n        client_options = {\"universe_domain\": universe_domain}\n\n        with self.assertRaises(ValueError):\n            self._make_one(\n                project=PROJECT, credentials=credentials, client_options=client_options\n            )\n\n    def test_ctor_w_universe_domain_and_mtls(self):\n        PROJECT = \"PROJECT\"\n        universe_domain = \"example.com\"\n        client_options = {\"universe_domain\": universe_domain}\n\n        credentials = _make_credentials(\n            project=PROJECT, universe_domain=universe_domain\n        )\n\n        environ = {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": \"true\"}\n        with mock.patch(\"os.environ\", environ):\n            with self.assertRaises(ValueError):\n                self._make_one(credentials=credentials, client_options=client_options)\n\n    def test_ctor_w_custom_headers(self):\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials()\n        custom_headers = {\"x-goog-custom-audit-foo\": \"bar\"}\n        client = self._make_one(\n            project=PROJECT, credentials=credentials, extra_headers=custom_headers\n        )\n        self.assertEqual(\n            client._connection.API_BASE_URL, client._connection.DEFAULT_API_ENDPOINT\n        )\n        self.assertEqual(client._connection.extra_headers, custom_headers)\n\n    def test_ctor_wo_project(self):\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials(project=PROJECT)\n\n        client = self._make_one(credentials=credentials)\n\n        self.assertEqual(client.project, PROJECT)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIs(client._connection.credentials, credentials)\n        self.assertIsNone(client.current_batch)\n        self.assertEqual(list(client._batch_stack), [])\n\n    def test_ctor_w_project_explicit_none(self):\n        credentials = _make_credentials()\n\n        client = self._make_one(project=None, credentials=credentials)\n\n        self.assertIsNone(client.project)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIs(client._connection.credentials, credentials)\n        self.assertIsNone(client.current_batch)\n        self.assertEqual(list(client._batch_stack), [])\n\n    def test_ctor_w_client_info(self):\n        from google.cloud._http import ClientInfo\n\n        credentials = _make_credentials()\n        client_info = ClientInfo()\n\n        client = self._make_one(\n            project=None, credentials=credentials, client_info=client_info\n        )\n\n        self.assertIsNone(client.project)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIs(client._connection.credentials, credentials)\n        self.assertIsNone(client.current_batch)\n        self.assertEqual(list(client._batch_stack), [])\n        self.assertIs(client._connection._client_info, client_info)\n\n    def test_ctor_mtls(self):\n        PROJECT = \"PROJECT\"\n        credentials = _make_credentials(project=PROJECT)\n\n        client = self._make_one(credentials=credentials)\n        self.assertEqual(client._connection.ALLOW_AUTO_SWITCH_TO_MTLS_URL, True)\n        self.assertEqual(\n            client._connection.API_BASE_URL, \"https://storage.googleapis.com\"\n        )\n\n        client = self._make_one(\n            credentials=credentials, client_options={\"api_endpoint\": \"http://foo\"}\n        )\n        self.assertEqual(client._connection.ALLOW_AUTO_SWITCH_TO_MTLS_URL, False)\n        self.assertEqual(client._connection.API_BASE_URL, \"http://foo\")\n\n    def test_ctor_w_custom_endpoint_use_auth(self):\n        custom_endpoint = \"storage-example.p.googleapis.com\"\n        client = self._make_one(client_options={\"api_endpoint\": custom_endpoint})\n        self.assertEqual(client._connection.API_BASE_URL, custom_endpoint)\n        self.assertIsNotNone(client.project)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIsNotNone(client._connection.credentials)\n        self.assertNotIsInstance(client._connection.credentials, AnonymousCredentials)\n\n    def test_ctor_w_custom_endpoint_bypass_auth(self):\n        custom_endpoint = \"storage-example.p.googleapis.com\"\n        client = self._make_one(\n            client_options={\"api_endpoint\": custom_endpoint},\n            use_auth_w_custom_endpoint=False,\n        )\n        self.assertEqual(client._connection.API_BASE_URL, custom_endpoint)\n        self.assertEqual(client.project, None)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIsInstance(client._connection.credentials, AnonymousCredentials)\n\n    def test_ctor_w_custom_endpoint_w_credentials(self):\n        PROJECT = \"PROJECT\"\n        custom_endpoint = \"storage-example.p.googleapis.com\"\n        credentials = _make_credentials(project=PROJECT)\n        client = self._make_one(\n            credentials=credentials, client_options={\"api_endpoint\": custom_endpoint}\n        )\n        self.assertEqual(client._connection.API_BASE_URL, custom_endpoint)\n        self.assertEqual(client.project, PROJECT)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIs(client._connection.credentials, credentials)\n\n    def test_ctor_w_emulator_wo_project(self):\n        # bypasses authentication if STORAGE_EMULATOR_ENV_VAR is set\n        host = \"http://localhost:8080\"\n        environ = {STORAGE_EMULATOR_ENV_VAR: host}\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one()\n\n        self.assertIsNone(client.project)\n        self.assertEqual(client._connection.API_BASE_URL, host)\n        self.assertIsInstance(client._connection.credentials, AnonymousCredentials)\n\n    def test_ctor_w_emulator_w_environ_project(self):\n        # bypasses authentication and infers the project from the environment\n        host = \"http://localhost:8080\"\n        environ_project = \"environ-project\"\n        environ = {\n            STORAGE_EMULATOR_ENV_VAR: host,\n            \"GOOGLE_CLOUD_PROJECT\": environ_project,\n        }\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one()\n\n        self.assertEqual(client.project, environ_project)\n        self.assertEqual(client._connection.API_BASE_URL, host)\n        self.assertIsInstance(client._connection.credentials, AnonymousCredentials)\n\n    def test_ctor_w_emulator_w_project_arg(self):\n        # project argument overrides project set in the enviroment\n        host = \"http://localhost:8080\"\n        environ_project = \"environ-project\"\n        project = \"my-test-project\"\n        environ = {\n            STORAGE_EMULATOR_ENV_VAR: host,\n            \"GOOGLE_CLOUD_PROJECT\": environ_project,\n        }\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one(project=project)\n\n        self.assertEqual(client.project, project)\n        self.assertEqual(client._connection.API_BASE_URL, host)\n        self.assertIsInstance(client._connection.credentials, AnonymousCredentials)\n\n    def test_ctor_w_emulator_w_credentials(self):\n        host = \"http://localhost:8080\"\n        environ = {STORAGE_EMULATOR_ENV_VAR: host}\n        credentials = _make_credentials()\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one(credentials=credentials)\n\n        self.assertEqual(client._connection.API_BASE_URL, host)\n        self.assertIs(client._connection.credentials, credentials)\n\n    def test_ctor_w_api_endpoint_override(self):\n        host = \"http://localhost:8080\"\n        environ = {_API_ENDPOINT_OVERRIDE_ENV_VAR: host}\n        project = \"my-test-project\"\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one(project=project)\n\n        self.assertEqual(client.project, project)\n        self.assertEqual(client._connection.API_BASE_URL, host)\n\n    def test_create_anonymous_client(self):\n        klass = self._get_target_class()\n        client = klass.create_anonymous_client()\n\n        self.assertIsNone(client.project)\n        self.assertIsInstance(client._connection, Connection)\n        self.assertIsInstance(client._connection.credentials, AnonymousCredentials)\n\n    def test__push_batch_and__pop_batch(self):\n        from google.cloud.storage.batch import Batch\n\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        batch1 = Batch(client)\n        batch2 = Batch(client)\n        client._push_batch(batch1)\n        self.assertEqual(list(client._batch_stack), [batch1])\n        self.assertIs(client.current_batch, batch1)\n        client._push_batch(batch2)\n        self.assertIs(client.current_batch, batch2)\n        # list(_LocalStack) returns in reverse order.\n        self.assertEqual(list(client._batch_stack), [batch2, batch1])\n        self.assertIs(client._pop_batch(), batch2)\n        self.assertEqual(list(client._batch_stack), [batch1])\n        self.assertIs(client._pop_batch(), batch1)\n        self.assertEqual(list(client._batch_stack), [])\n\n    def test__connection_setter(self):\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        client._base_connection = None  # Unset the value from the constructor\n        client._connection = connection = object()\n        self.assertIs(client._base_connection, connection)\n\n    def test__connection_setter_when_set(self):\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        self.assertRaises(ValueError, setattr, client, \"_connection\", None)\n\n    def test__connection_getter_no_batch(self):\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        self.assertIs(client._connection, client._base_connection)\n        self.assertIsNone(client.current_batch)\n\n    def test__connection_getter_with_batch(self):\n        from google.cloud.storage.batch import Batch\n\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        batch = Batch(client)\n        client._push_batch(batch)\n        self.assertIsNot(client._connection, client._base_connection)\n        self.assertIs(client._connection, batch)\n        self.assertIs(client.current_batch, batch)\n\n    def test_get_service_account_email_wo_project(self):\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n        EMAIL = \"storage-user-123@example.com\"\n        RESOURCE = {\"kind\": \"storage#serviceAccount\", \"email_address\": EMAIL}\n\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        http = _make_requests_session([_make_json_response(RESOURCE)])\n        client._http_internal = http\n\n        service_account_email = client.get_service_account_email(timeout=42)\n\n        self.assertEqual(service_account_email, EMAIL)\n        http.request.assert_called_once_with(\n            method=\"GET\", url=mock.ANY, data=None, headers=mock.ANY, timeout=42\n        )\n        _, kwargs = http.request.call_args\n        scheme, netloc, path, qs, _ = urllib.parse.urlsplit(kwargs.get(\"url\"))\n        self.assertEqual(f\"{scheme}://{netloc}\", client._connection.API_BASE_URL)\n        self.assertEqual(\n            path,\n            \"/\".join(\n                [\n                    \"\",\n                    \"storage\",\n                    client._connection.API_VERSION,\n                    \"projects\",\n                    PROJECT,\n                    \"serviceAccount\",\n                ]\n            ),\n        )\n\n    def test_get_service_account_email_w_project(self):\n        PROJECT = \"PROJECT\"\n        OTHER_PROJECT = \"OTHER_PROJECT\"\n        CREDENTIALS = _make_credentials()\n        EMAIL = \"storage-user-123@example.com\"\n        RESOURCE = {\"kind\": \"storage#serviceAccount\", \"email_address\": EMAIL}\n\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        http = _make_requests_session([_make_json_response(RESOURCE)])\n        client._http_internal = http\n\n        service_account_email = client.get_service_account_email(project=OTHER_PROJECT)\n\n        self.assertEqual(service_account_email, EMAIL)\n        http.request.assert_called_once_with(\n            method=\"GET\",\n            url=mock.ANY,\n            data=None,\n            headers=mock.ANY,\n            timeout=self._get_default_timeout(),\n        )\n        _, kwargs = http.request.call_args\n        scheme, netloc, path, qs, _ = urllib.parse.urlsplit(kwargs.get(\"url\"))\n        self.assertEqual(f\"{scheme}://{netloc}\", client._connection.API_BASE_URL)\n        self.assertEqual(\n            path,\n            \"/\".join(\n                [\n                    \"\",\n                    \"storage\",\n                    client._connection.API_VERSION,\n                    \"projects\",\n                    OTHER_PROJECT,\n                    \"serviceAccount\",\n                ]\n            ),\n        )\n\n    def test_bucket(self):\n        from google.cloud.storage.bucket import Bucket\n\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n        BUCKET_NAME = \"BUCKET_NAME\"\n\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        bucket = client.bucket(BUCKET_NAME)\n        self.assertIsInstance(bucket, Bucket)\n        self.assertIs(bucket.client, client)\n        self.assertEqual(bucket.name, BUCKET_NAME)\n        self.assertIsNone(bucket.user_project)\n\n    def test_bucket_w_user_project(self):\n        from google.cloud.storage.bucket import Bucket\n\n        PROJECT = \"PROJECT\"\n        USER_PROJECT = \"USER_PROJECT\"\n        CREDENTIALS = _make_credentials()\n        BUCKET_NAME = \"BUCKET_NAME\"\n\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        bucket = client.bucket(BUCKET_NAME, user_project=USER_PROJECT)\n        self.assertIsInstance(bucket, Bucket)\n        self.assertIs(bucket.client, client)\n        self.assertEqual(bucket.name, BUCKET_NAME)\n        self.assertEqual(bucket.user_project, USER_PROJECT)\n\n    def test_batch(self):\n        from google.cloud.storage.batch import Batch\n\n        PROJECT = \"PROJECT\"\n        CREDENTIALS = _make_credentials()\n\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n        batch = client.batch()\n        self.assertIsInstance(batch, Batch)\n        self.assertIs(batch._client, client)\n\n    def test__get_resource_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        with self.assertRaises(NotFound):\n            client._get_resource(path)\n\n        connection.api_request.assert_called_once_with(\n            method=\"GET\",\n            path=path,\n            query_params=None,\n            headers=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test__get_resource_hit_w_explicit(self):\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        query_params = {\"foo\": \"Foo\"}\n        headers = {\"bar\": \"Bar\"}\n        timeout = 100\n        retry = mock.Mock(spec=[])\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        expected = mock.Mock(spec={})\n        connection = client._base_connection = _make_connection(expected)\n        target = mock.Mock(spec={})\n\n        found = client._get_resource(\n            path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n        self.assertIs(found, expected)\n\n        connection.api_request.assert_called_once_with(\n            method=\"GET\",\n            path=path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n    def test__list_resource_w_defaults(self):\n        import functools\n        from google.api_core.page_iterator import HTTPIterator\n        from google.api_core.page_iterator import _do_nothing_page_start\n\n        project = \"PROJECT\"\n        path = \"/path/to/list/resource\"\n        item_to_value = mock.Mock(spec=[])\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        iterator = client._list_resource(\n            path=path,\n            item_to_value=item_to_value,\n        )\n\n        self.assertIsInstance(iterator, HTTPIterator)\n        self.assertIs(iterator.client, client)\n        self.assertIsInstance(iterator.api_request, functools.partial)\n        self.assertIs(iterator.api_request.func, connection.api_request)\n        self.assertEqual(iterator.api_request.args, ())\n        expected_keywords = {\n            \"timeout\": self._get_default_timeout(),\n            \"retry\": DEFAULT_RETRY,\n        }\n        self.assertEqual(iterator.api_request.keywords, expected_keywords)\n        self.assertEqual(iterator.path, path)\n        self.assertEqual(iterator.next_page_token, None)\n        self.assertEqual(iterator.max_results, None)\n        self.assertIs(iterator._page_start, _do_nothing_page_start)\n\n    def test__list_resource_w_explicit(self):\n        import functools\n        from google.api_core.page_iterator import HTTPIterator\n\n        project = \"PROJECT\"\n        path = \"/path/to/list/resource\"\n        item_to_value = mock.Mock(spec=[])\n        page_token = \"PAGE-TOKEN\"\n        max_results = 47\n        extra_params = {\"foo\": \"Foo\"}\n        page_start = mock.Mock(spec=[])\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        iterator = client._list_resource(\n            path=path,\n            item_to_value=item_to_value,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_start=page_start,\n        )\n\n        self.assertIsInstance(iterator, HTTPIterator)\n        self.assertIs(iterator.client, client)\n        self.assertIsInstance(iterator.api_request, functools.partial)\n        self.assertIs(iterator.api_request.func, connection.api_request)\n        self.assertEqual(iterator.api_request.args, ())\n        expected_keywords = {\n            \"timeout\": self._get_default_timeout(),\n            \"retry\": DEFAULT_RETRY,\n        }\n        self.assertEqual(iterator.api_request.keywords, expected_keywords)\n        self.assertEqual(iterator.path, path)\n        self.assertEqual(iterator.next_page_token, page_token)\n        self.assertEqual(iterator.max_results, max_results)\n        self.assertIs(iterator._page_start, page_start)\n\n    def test__patch_resource_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        credentials = _make_credentials()\n        data = {\"baz\": \"Baz\"}\n\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        with self.assertRaises(NotFound):\n            client._patch_resource(path, data)\n\n        connection.api_request.assert_called_once_with(\n            method=\"PATCH\",\n            path=path,\n            data=data,\n            query_params=None,\n            headers=None,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=None,\n        )\n\n    def test__patch_resource_hit_w_explicit(self):\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        data = {\"baz\": \"Baz\"}\n        query_params = {\"foo\": \"Foo\"}\n        headers = {\"bar\": \"Bar\"}\n        timeout = 100\n        retry = mock.Mock(spec=[])\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        expected = mock.Mock(spec={})\n        connection = client._base_connection = _make_connection(expected)\n        target = mock.Mock(spec={})\n\n        found = client._patch_resource(\n            path,\n            data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n        self.assertIs(found, expected)\n\n        connection.api_request.assert_called_once_with(\n            method=\"PATCH\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n    def test__put_resource_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        credentials = _make_credentials()\n        data = {\"baz\": \"Baz\"}\n\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        with self.assertRaises(NotFound):\n            client._put_resource(path, data)\n\n        connection.api_request.assert_called_once_with(\n            method=\"PUT\",\n            path=path,\n            data=data,\n            query_params=None,\n            headers=None,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=None,\n        )\n\n    def test__put_resource_hit_w_explicit(self):\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        data = {\"baz\": \"Baz\"}\n        query_params = {\"foo\": \"Foo\"}\n        headers = {\"bar\": \"Bar\"}\n        timeout = 100\n        retry = mock.Mock(spec=[])\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        expected = mock.Mock(spec={})\n        connection = client._base_connection = _make_connection(expected)\n        target = mock.Mock(spec={})\n\n        found = client._put_resource(\n            path,\n            data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n        self.assertIs(found, expected)\n\n        connection.api_request.assert_called_once_with(\n            method=\"PUT\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n    def test__post_resource_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        credentials = _make_credentials()\n        data = {\"baz\": \"Baz\"}\n\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        with self.assertRaises(NotFound):\n            client._post_resource(path, data)\n\n        connection.api_request.assert_called_once_with(\n            method=\"POST\",\n            path=path,\n            data=data,\n            query_params=None,\n            headers=None,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=None,\n        )\n\n    def test__post_resource_hit_w_explicit(self):\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        data = {\"baz\": \"Baz\"}\n        query_params = {\"foo\": \"Foo\"}\n        headers = {\"bar\": \"Bar\"}\n        timeout = 100\n        retry = mock.Mock(spec=[])\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        expected = mock.Mock(spec={})\n        connection = client._base_connection = _make_connection(expected)\n        target = mock.Mock(spec={})\n\n        found = client._post_resource(\n            path,\n            data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n        self.assertIs(found, expected)\n\n        connection.api_request.assert_called_once_with(\n            method=\"POST\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n    def test__delete_resource_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        connection = client._base_connection = _make_connection()\n\n        with self.assertRaises(NotFound):\n            client._delete_resource(path)\n\n        connection.api_request.assert_called_once_with(\n            method=\"DELETE\",\n            path=path,\n            query_params=None,\n            headers=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test__delete_resource_hit_w_explicit(self):\n        project = \"PROJECT\"\n        path = \"/path/to/something\"\n        query_params = {\"foo\": \"Foo\"}\n        headers = {\"bar\": \"Bar\"}\n        timeout = 100\n        retry = mock.Mock(spec=[])\n        credentials = _make_credentials()\n\n        client = self._make_one(project=project, credentials=credentials)\n        expected = mock.Mock(spec={})\n        connection = client._base_connection = _make_connection(expected)\n        target = mock.Mock(spec={})\n\n        found = client._delete_resource(\n            path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n        self.assertIs(found, expected)\n\n        connection.api_request.assert_called_once_with(\n            method=\"DELETE\",\n            path=path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=target,\n        )\n\n    def test__bucket_arg_to_bucket_w_bucket_w_client(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        other_client = mock.Mock(spec=[])\n        bucket_name = \"w_client\"\n\n        bucket = Bucket(other_client, name=bucket_name)\n\n        found = client._bucket_arg_to_bucket(bucket)\n\n        self.assertIs(found, bucket)\n        self.assertIs(found.client, other_client)\n\n    def test__bucket_arg_to_bucket_w_bucket_wo_client(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        bucket_name = \"wo_client\"\n\n        bucket = Bucket(client=None, name=bucket_name)\n\n        found = client._bucket_arg_to_bucket(bucket)\n\n        self.assertIs(found, bucket)\n        self.assertIs(found.client, client)\n\n    def test__bucket_arg_to_bucket_w_bucket_name(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        bucket_name = \"string-name\"\n\n        found = client._bucket_arg_to_bucket(bucket_name)\n\n        self.assertIsInstance(found, Bucket)\n        self.assertEqual(found.name, bucket_name)\n        self.assertIs(found.client, client)\n\n    def test_get_bucket_miss_w_string_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock()\n        client._get_resource.side_effect = NotFound(\"testing\")\n        bucket_name = \"nonesuch\"\n\n        with self.assertRaises(NotFound):\n            client.get_bucket(bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=mock.ANY,\n        )\n\n        target = client._get_resource.call_args[1][\"_target_object\"]\n        self.assertIsInstance(target, Bucket)\n        self.assertEqual(target.name, bucket_name)\n\n    def test_get_bucket_hit_w_string_w_timeout(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        timeout = 42\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n\n        bucket = client.get_bucket(bucket_name, timeout=timeout)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_get_bucket_hit_w_string_w_metageneration_match(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        metageneration_number = 6\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n\n        bucket = client.get_bucket(\n            bucket_name, if_metageneration_match=metageneration_number\n        )\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_get_bucket_miss_w_object_w_retry(self):\n        from google.cloud.exceptions import NotFound\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"nonesuch\"\n        retry = mock.Mock(spec=[])\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(side_effect=NotFound(\"testing\"))\n        bucket_obj = Bucket(client, bucket_name)\n\n        with self.assertRaises(NotFound):\n            client.get_bucket(bucket_obj, retry=retry)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=mock.ANY,\n        )\n\n        target = client._get_resource.call_args[1][\"_target_object\"]\n        self.assertIsInstance(target, Bucket)\n        self.assertEqual(target.name, bucket_name)\n\n    def test_get_bucket_hit_w_object_defaults(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n        bucket_obj = Bucket(client, bucket_name)\n\n        bucket = client.get_bucket(bucket_obj)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_get_bucket_hit_w_object_w_retry_none(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n        bucket_obj = Bucket(client, bucket_name)\n\n        bucket = client.get_bucket(bucket_obj, retry=None)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=bucket,\n        )\n\n    def test_lookup_bucket_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"nonesuch\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(side_effect=NotFound(\"testing\"))\n\n        bucket = client.lookup_bucket(bucket_name)\n\n        self.assertIsNone(bucket)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=mock.ANY,\n        )\n\n        target = client._get_resource.call_args[1][\"_target_object\"]\n        self.assertIsInstance(target, Bucket)\n        self.assertEqual(target.name, bucket_name)\n\n    def test_lookup_bucket_hit_w_timeout(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        timeout = 42\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n\n        bucket = client.lookup_bucket(bucket_name, timeout=timeout)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_lookup_bucket_hit_w_metageneration_match(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        metageneration_number = 6\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n\n        bucket = client.lookup_bucket(\n            bucket_name, if_metageneration_match=metageneration_number\n        )\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_lookup_bucket_hit_w_retry(self):\n        from google.cloud.storage.bucket import Bucket\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._get_resource = mock.Mock(return_value=api_response)\n        bucket_obj = Bucket(client, bucket_name)\n\n        bucket = client.lookup_bucket(bucket_obj, retry=None)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n\n        expected_path = f\"/b/{bucket_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=None,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_missing_client_project(self):\n        from google.cloud.exceptions import BadRequest\n\n        credentials = _make_credentials()\n        client = self._make_one(project=None, credentials=credentials)\n\n        client._post_resource = mock.Mock()\n        client._post_resource.side_effect = BadRequest(\"Required parameter: project\")\n\n        bucket_name = \"bucket-name\"\n\n        with self.assertRaises(BadRequest):\n            client.create_bucket(bucket_name)\n\n        expected_path = \"/b\"\n        expected_data = {\"name\": bucket_name}\n        # no required parameter: project\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=mock.ANY,\n        )\n\n    def test_create_bucket_w_missing_client_project_w_emulator(self):\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        host = \"http://localhost:8080\"\n        environ = {STORAGE_EMULATOR_ENV_VAR: host}\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one()\n\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        with mock.patch(\"os.environ\", environ):\n            bucket = client.create_bucket(bucket_name)\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\"project\": \"<none>\"}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_environ_project_w_emulator(self):\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        host = \"http://localhost:8080\"\n        environ_project = \"environ-project\"\n        environ = {\n            STORAGE_EMULATOR_ENV_VAR: host,\n            \"GOOGLE_CLOUD_PROJECT\": environ_project,\n        }\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one()\n\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        with mock.patch(\"os.environ\", environ):\n            bucket = client.create_bucket(bucket_name)\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\"project\": environ_project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_custom_endpoint(self):\n        custom_endpoint = \"storage-example.p.googleapis.com\"\n        client = self._make_one(client_options={\"api_endpoint\": custom_endpoint})\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        bucket = client.create_bucket(bucket_name)\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\"project\": client.project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_conflict_w_user_project(self):\n        from google.cloud.exceptions import Conflict\n\n        project = \"PROJECT\"\n        user_project = \"USER_PROJECT\"\n        other_project = \"OTHER_PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.side_effect = Conflict(\"testing\")\n\n        bucket_name = \"bucket-name\"\n\n        with self.assertRaises(Conflict):\n            client.create_bucket(\n                bucket_name, project=other_project, user_project=user_project\n            )\n\n        expected_path = \"/b\"\n        expected_data = {\"name\": bucket_name}\n        expected_query_params = {\n            \"project\": other_project,\n            \"userProject\": user_project,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=mock.ANY,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_create_bucket_w_requester_pays_deprecated(self, mock_warn):\n        from google.cloud.storage.bucket import Bucket\n\n        bucket_name = \"bucket-name\"\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        api_respone = {\"name\": bucket_name, \"billing\": {\"requesterPays\": True}}\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_respone\n\n        bucket = client.create_bucket(bucket_name, requester_pays=True)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertEqual(bucket.name, bucket_name)\n        self.assertTrue(bucket.requester_pays)\n\n        expected_path = \"/b\"\n        expected_data = api_respone\n        expected_query_params = {\"project\": project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=mock.ANY,\n        )\n\n        mock_warn.assert_called_with(\n            \"requester_pays arg is deprecated. Use Bucket().requester_pays instead.\",\n            PendingDeprecationWarning,\n            stacklevel=1,\n        )\n\n    def test_create_bucket_w_predefined_acl_invalid(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n\n        with self.assertRaises(ValueError):\n            client.create_bucket(bucket_name, predefined_acl=\"bogus\")\n\n        client._post_resource.assert_not_called()\n\n    def test_create_bucket_w_predefined_acl_valid_w_timeout(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n        timeout = 42\n\n        bucket = client.create_bucket(\n            bucket_name,\n            predefined_acl=\"publicRead\",\n            timeout=timeout,\n        )\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\n            \"project\": project,\n            \"predefinedAcl\": \"publicRead\",\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_predefined_default_object_acl_invalid(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n\n        with self.assertRaises(ValueError):\n            client.create_bucket(bucket_name, predefined_default_object_acl=\"bogus\")\n\n        client._post_resource.assert_not_called()\n\n    def test_create_bucket_w_predefined_default_object_acl_valid_w_retry(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n        retry = mock.Mock(spec=[])\n\n        bucket = client.create_bucket(\n            bucket_name,\n            predefined_default_object_acl=\"publicRead\",\n            retry=retry,\n        )\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\n            \"project\": project,\n            \"predefinedDefaultObjectAcl\": \"publicRead\",\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_explicit_location(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        location = \"us-central1\"\n        api_response = {\"location\": location, \"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        bucket = client.create_bucket(bucket_name, location=location)\n\n        self.assertEqual(bucket.location, location)\n\n        expected_path = \"/b\"\n        expected_data = {\"location\": location, \"name\": bucket_name}\n        expected_query_params = {\"project\": project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_custom_dual_region(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        location = \"US\"\n        data_locations = [\"US-EAST1\", \"US-WEST1\"]\n        api_response = {\n            \"location\": location,\n            \"customPlacementConfig\": {\"dataLocations\": data_locations},\n            \"name\": bucket_name,\n        }\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        bucket = client.create_bucket(\n            bucket_name, location=location, data_locations=data_locations\n        )\n\n        self.assertEqual(bucket.location, location)\n        self.assertEqual(bucket.data_locations, data_locations)\n\n        expected_path = \"/b\"\n        expected_data = {\n            \"location\": location,\n            \"customPlacementConfig\": {\"dataLocations\": data_locations},\n            \"name\": bucket_name,\n        }\n        expected_query_params = {\"project\": project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_explicit_project(self):\n        project = \"PROJECT\"\n        other_project = \"other-project-123\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        bucket = client.create_bucket(bucket_name, project=other_project)\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\"project\": other_project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_extra_properties(self):\n        from google.cloud.storage.bucket import Bucket\n\n        bucket_name = \"bucket-name\"\n        project = \"PROJECT\"\n        cors = [\n            {\n                \"maxAgeSeconds\": 60,\n                \"methods\": [\"*\"],\n                \"origin\": [\"https://example.com/frontend\"],\n                \"responseHeader\": [\"X-Custom-Header\"],\n            }\n        ]\n        lifecycle_rules = [{\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 365}}]\n        location = \"eu\"\n        labels = {\"color\": \"red\", \"flavor\": \"cherry\"}\n        storage_class = \"NEARLINE\"\n        api_response = {\n            \"name\": bucket_name,\n            \"cors\": cors,\n            \"lifecycle\": {\"rule\": lifecycle_rules},\n            \"location\": location,\n            \"storageClass\": storage_class,\n            \"versioning\": {\"enabled\": True},\n            \"billing\": {\"requesterPays\": True},\n            \"labels\": labels,\n        }\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        bucket = Bucket(client=client, name=bucket_name)\n        bucket.cors = cors\n        bucket.lifecycle_rules = lifecycle_rules\n        bucket.storage_class = storage_class\n        bucket.versioning_enabled = True\n        bucket.requester_pays = True\n        bucket.labels = labels\n\n        client.create_bucket(bucket, location=location, enable_object_retention=True)\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\n            \"project\": project,\n            \"enableObjectRetention\": True,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_create_bucket_w_name_only(self):\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        bucket = client.create_bucket(bucket_name)\n\n        expected_path = \"/b\"\n        expected_data = api_response\n        expected_query_params = {\"project\": project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    @staticmethod\n    def _make_blob(*args, **kw):\n        from google.cloud.storage.blob import Blob\n\n        blob = Blob(*args, **kw)\n\n        return blob\n\n    def test_download_blob_to_file_with_failure(self):\n        from google.resumable_media import InvalidResponse\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        project = \"PROJECT\"\n        raw_response = requests.Response()\n        raw_response.status_code = http.client.NOT_FOUND\n        raw_request = requests.Request(\"GET\", \"http://example.com\")\n        raw_response.request = raw_request.prepare()\n        grmp_response = InvalidResponse(raw_response)\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n        blob = self._make_blob(name=\"blob_name\", bucket=None)\n        blob._encryption_key = None\n        blob._get_download_url = mock.Mock()\n        blob._do_download = mock.Mock()\n        blob._do_download.side_effect = grmp_response\n\n        file_obj = io.BytesIO()\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            with self.assertRaises(exceptions.NotFound):\n                client.download_blob_to_file(blob, file_obj)\n\n            self.assertEqual(file_obj.tell(), 0)\n            headers = {\n                **_get_default_headers(client._connection.user_agent),\n                \"accept-encoding\": \"gzip\",\n            }\n        blob._do_download.assert_called_once_with(\n            client._http,\n            file_obj,\n            blob._get_download_url(),\n            headers,\n            None,\n            None,\n            False,\n            checksum=\"md5\",\n            timeout=_DEFAULT_TIMEOUT,\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_download_blob_to_file_with_uri(self):\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(project=project, credentials=credentials)\n        blob = self._make_blob(name=\"blob_name\", bucket=None)\n        file_obj = io.BytesIO()\n        blob._encryption_key = None\n        blob._get_download_url = mock.Mock()\n        blob._do_download = mock.Mock()\n\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            with mock.patch(\n                \"google.cloud.storage.client.Blob.from_string\", return_value=blob\n            ):\n                client.download_blob_to_file(\n                    \"gs://bucket_name/path/to/object\", file_obj\n                )\n\n            headers = {\n                **_get_default_headers(client._connection.user_agent),\n                \"accept-encoding\": \"gzip\",\n            }\n        blob._do_download.assert_called_once_with(\n            client._http,\n            file_obj,\n            blob._get_download_url(),\n            headers,\n            None,\n            None,\n            False,\n            checksum=\"md5\",\n            timeout=_DEFAULT_TIMEOUT,\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_download_blob_to_file_with_invalid_uri(self):\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        file_obj = io.BytesIO()\n\n        with pytest.raises(ValueError):\n            client.download_blob_to_file(\"http://bucket_name/path/to/object\", file_obj)\n\n    def test_download_blob_to_file_w_no_retry(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True, raw_download=True, retry=None\n        )\n\n    def test_download_blob_to_file_w_conditional_etag_match_string(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True,\n            raw_download=True,\n            retry=None,\n            if_etag_match=\"kittens\",\n        )\n\n    def test_download_blob_to_file_w_conditional_etag_match_list(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True,\n            raw_download=True,\n            retry=None,\n            if_etag_match=[\"kittens\", \"fluffy\"],\n        )\n\n    def test_download_blob_to_file_w_conditional_etag_not_match_string(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True,\n            raw_download=True,\n            retry=None,\n            if_etag_not_match=\"kittens\",\n        )\n\n    def test_download_blob_to_file_w_conditional_etag_not_match_list(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True,\n            raw_download=True,\n            retry=None,\n            if_etag_not_match=[\"kittens\", \"fluffy\"],\n        )\n\n    def test_download_blob_to_file_w_conditional_retry_pass(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True,\n            raw_download=True,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            if_generation_match=1,\n        )\n\n    def test_download_blob_to_file_w_conditional_retry_fail(self):\n        self._download_blob_to_file_helper(\n            use_chunks=True,\n            raw_download=True,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            expect_condition_fail=True,\n        )\n\n    def _download_blob_to_file_helper(\n        self, use_chunks, raw_download, expect_condition_fail=False, **extra_kwargs\n    ):\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n        blob = self._make_blob(name=\"blob_name\", bucket=None)\n        blob._encryption_key = None\n        blob._get_download_url = mock.Mock()\n        if use_chunks:\n            blob._CHUNK_SIZE_MULTIPLE = 1\n            blob.chunk_size = 3\n        blob._do_download = mock.Mock()\n        file_obj = io.BytesIO()\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            if raw_download:\n                client.download_blob_to_file(\n                    blob, file_obj, raw_download=True, **extra_kwargs\n                )\n            else:\n                client.download_blob_to_file(blob, file_obj, **extra_kwargs)\n\n        expected_retry = extra_kwargs.get(\"retry\", DEFAULT_RETRY)\n        if (\n            expected_retry is DEFAULT_RETRY_IF_GENERATION_SPECIFIED\n            and not expect_condition_fail\n        ):\n            expected_retry = DEFAULT_RETRY\n        elif expect_condition_fail:\n            expected_retry = None\n\n        headers = {\"accept-encoding\": \"gzip\"}\n        if_etag_match = extra_kwargs.get(\"if_etag_match\")\n        if if_etag_match is not None:\n            if isinstance(if_etag_match, str):\n                if_etag_match = [if_etag_match]\n            headers[\"If-Match\"] = \", \".join(if_etag_match)\n        if_etag_not_match = extra_kwargs.get(\"if_etag_not_match\")\n        if if_etag_not_match is not None:\n            if isinstance(if_etag_not_match, str):\n                if_etag_not_match = [if_etag_not_match]\n            headers[\"If-None-Match\"] = \", \".join(if_etag_not_match)\n\n        with patch.object(\n            _helpers, \"_get_invocation_id\", return_value=GCCL_INVOCATION_TEST_CONST\n        ):\n            headers = {**_get_default_headers(client._connection.user_agent), **headers}\n\n        blob._do_download.assert_called_once_with(\n            client._http,\n            file_obj,\n            blob._get_download_url(),\n            headers,\n            None,\n            None,\n            raw_download,\n            checksum=\"md5\",\n            timeout=_DEFAULT_TIMEOUT,\n            retry=expected_retry,\n        )\n\n    def test_download_blob_to_file_wo_chunks_wo_raw(self):\n        self._download_blob_to_file_helper(use_chunks=False, raw_download=False)\n\n    def test_download_blob_to_file_w_chunks_wo_raw(self):\n        self._download_blob_to_file_helper(use_chunks=True, raw_download=False)\n\n    def test_download_blob_to_file_wo_chunks_w_raw(self):\n        self._download_blob_to_file_helper(use_chunks=False, raw_download=True)\n\n    def test_download_blob_to_file_w_chunks_w_raw(self):\n        self._download_blob_to_file_helper(use_chunks=True, raw_download=True)\n\n    def test_download_blob_have_different_uuid(self):\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n        blob = self._make_blob(name=\"blob_name\", bucket=None)\n        blob._encryption_key = None\n        blob._do_download = mock.Mock()\n        blob._get_download_url = mock.Mock()\n        file_obj = io.BytesIO()\n        client.download_blob_to_file(blob, file_obj)\n        client.download_blob_to_file(blob, file_obj)\n\n        self.assertNotEqual(\n            blob._do_download.call_args_list[0][0][3][\"X-Goog-API-Client\"],\n            blob._do_download.call_args_list[1][0][3][\"X-Goog-API-Client\"],\n        )\n\n    def test_list_blobs_w_defaults_w_bucket_obj(self):\n        from google.cloud.storage.bucket import Bucket\n        from google.cloud.storage.bucket import _blobs_page_start\n        from google.cloud.storage.bucket import _item_to_blob\n\n        project = \"PROJECT\"\n        bucket_name = \"bucket-name\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._list_resource = mock.Mock(spec=[])\n        bucket = Bucket(client, bucket_name)\n\n        iterator = client.list_blobs(bucket)\n\n        self.assertIs(iterator, client._list_resource.return_value)\n        self.assertIs(iterator.bucket, bucket)\n        self.assertEqual(iterator.prefixes, set())\n\n        expected_path = f\"/b/{bucket_name}/o\"\n        expected_item_to_value = _item_to_blob\n        expected_page_token = None\n        expected_max_results = None\n        expected_extra_params = {\"projection\": \"noAcl\"}\n        expected_page_start = _blobs_page_start\n        expected_page_size = None\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_start=expected_page_start,\n            page_size=expected_page_size,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_blobs_w_explicit_w_user_project(self):\n        from google.cloud.storage.bucket import _blobs_page_start\n        from google.cloud.storage.bucket import _item_to_blob\n\n        project = \"PROJECT\"\n        user_project = \"user-project-123\"\n        bucket_name = \"name\"\n        max_results = 10\n        page_token = \"ABCD\"\n        prefix = \"subfolder\"\n        delimiter = \"/\"\n        match_glob = \"**txt\"\n        start_offset = \"c\"\n        end_offset = \"g\"\n        include_trailing_delimiter = True\n        include_folders_as_prefixes = True\n        soft_deleted = False\n        versions = True\n        projection = \"full\"\n        page_size = 2\n        fields = \"items/contentLanguage,nextPageToken\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._list_resource = mock.Mock(spec=[])\n        client._bucket_arg_to_bucket = mock.Mock(spec=[])\n        bucket = client._bucket_arg_to_bucket.return_value = mock.Mock(\n            spec=[\"path\", \"user_project\"],\n        )\n        bucket.path = f\"/b/{bucket_name}\"\n        bucket.user_project = user_project\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        iterator = client.list_blobs(\n            bucket_or_name=bucket_name,\n            max_results=max_results,\n            page_token=page_token,\n            prefix=prefix,\n            delimiter=delimiter,\n            start_offset=start_offset,\n            end_offset=end_offset,\n            include_trailing_delimiter=include_trailing_delimiter,\n            versions=versions,\n            projection=projection,\n            fields=fields,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n            match_glob=match_glob,\n            include_folders_as_prefixes=include_folders_as_prefixes,\n            soft_deleted=soft_deleted,\n        )\n\n        self.assertIs(iterator, client._list_resource.return_value)\n        self.assertIs(iterator.bucket, bucket)\n        self.assertEqual(iterator.prefixes, set())\n\n        expected_path = f\"/b/{bucket_name}/o\"\n        expected_item_to_value = _item_to_blob\n        expected_page_token = page_token\n        expected_max_results = max_results\n        expected_extra_params = {\n            \"projection\": projection,\n            \"prefix\": prefix,\n            \"delimiter\": delimiter,\n            \"matchGlob\": match_glob,\n            \"startOffset\": start_offset,\n            \"endOffset\": end_offset,\n            \"includeTrailingDelimiter\": include_trailing_delimiter,\n            \"versions\": versions,\n            \"fields\": fields,\n            \"userProject\": user_project,\n            \"includeFoldersAsPrefixes\": include_folders_as_prefixes,\n            \"softDeleted\": soft_deleted,\n        }\n        expected_page_start = _blobs_page_start\n        expected_page_size = 2\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_start=expected_page_start,\n            page_size=expected_page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_list_buckets_wo_project(self):\n        from google.cloud.exceptions import BadRequest\n        from google.cloud.storage.client import _item_to_bucket\n\n        credentials = _make_credentials()\n        client = self._make_one(project=None, credentials=credentials)\n\n        client._list_resource = mock.Mock()\n        client._list_resource.side_effect = BadRequest(\"Required parameter: project\")\n\n        with self.assertRaises(BadRequest):\n            client.list_buckets()\n\n        expected_path = \"/b\"\n        expected_item_to_value = _item_to_bucket\n        expected_page_token = None\n        expected_max_results = None\n        expected_page_size = None\n        # no required parameter: project\n        expected_extra_params = {\n            \"projection\": \"noAcl\",\n        }\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_size=expected_page_size,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_buckets_wo_project_w_emulator(self):\n        from google.cloud.storage.client import _item_to_bucket\n\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        host = \"http://localhost:8080\"\n        environ = {STORAGE_EMULATOR_ENV_VAR: host}\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one()\n\n        client._list_resource = mock.Mock(spec=[])\n\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        with mock.patch(\"os.environ\", environ):\n            client.list_buckets()\n\n        expected_path = \"/b\"\n        expected_item_to_value = _item_to_bucket\n        expected_page_token = None\n        expected_max_results = None\n        expected_page_size = None\n        expected_extra_params = {\n            \"project\": \"<none>\",\n            \"projection\": \"noAcl\",\n        }\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_size=expected_page_size,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_buckets_w_environ_project_w_emulator(self):\n        from google.cloud.storage.client import _item_to_bucket\n\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        host = \"http://localhost:8080\"\n        environ_project = \"environ-project\"\n        environ = {\n            STORAGE_EMULATOR_ENV_VAR: host,\n            \"GOOGLE_CLOUD_PROJECT\": environ_project,\n        }\n        with mock.patch(\"os.environ\", environ):\n            client = self._make_one()\n\n        client._list_resource = mock.Mock(spec=[])\n\n        # mock STORAGE_EMULATOR_ENV_VAR is set\n        with mock.patch(\"os.environ\", environ):\n            client.list_buckets()\n\n        expected_path = \"/b\"\n        expected_item_to_value = _item_to_bucket\n        expected_page_token = None\n        expected_max_results = None\n        expected_page_size = None\n        expected_extra_params = {\n            \"project\": environ_project,\n            \"projection\": \"noAcl\",\n        }\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_size=expected_page_size,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_buckets_w_custom_endpoint(self):\n        from google.cloud.storage.client import _item_to_bucket\n\n        custom_endpoint = \"storage-example.p.googleapis.com\"\n        client = self._make_one(client_options={\"api_endpoint\": custom_endpoint})\n        client._list_resource = mock.Mock(spec=[])\n\n        iterator = client.list_buckets()\n\n        self.assertIs(iterator, client._list_resource.return_value)\n\n        expected_path = \"/b\"\n        expected_item_to_value = _item_to_bucket\n        expected_page_token = None\n        expected_max_results = None\n        expected_page_size = None\n        expected_extra_params = {\n            \"project\": client.project,\n            \"projection\": \"noAcl\",\n        }\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_size=expected_page_size,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_buckets_w_defaults(self):\n        from google.cloud.storage.client import _item_to_bucket\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._list_resource = mock.Mock(spec=[])\n\n        iterator = client.list_buckets()\n\n        self.assertIs(iterator, client._list_resource.return_value)\n\n        expected_path = \"/b\"\n        expected_item_to_value = _item_to_bucket\n        expected_page_token = None\n        expected_max_results = None\n        expected_page_size = None\n        expected_extra_params = {\n            \"project\": project,\n            \"projection\": \"noAcl\",\n        }\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_size=expected_page_size,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_buckets_w_explicit(self):\n        from google.cloud.storage.client import _item_to_bucket\n\n        project = \"foo-bar\"\n        other_project = \"OTHER_PROJECT\"\n        max_results = 10\n        page_token = \"ABCD\"\n        prefix = \"subfolder\"\n        projection = \"full\"\n        fields = \"items/id,nextPageToken\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._list_resource = mock.Mock(spec=[])\n        page_size = 2\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        iterator = client.list_buckets(\n            project=other_project,\n            max_results=max_results,\n            page_token=page_token,\n            prefix=prefix,\n            projection=projection,\n            fields=fields,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.assertIs(iterator, client._list_resource.return_value)\n\n        expected_path = \"/b\"\n        expected_item_to_value = _item_to_bucket\n        expected_page_token = page_token\n        expected_max_results = max_results\n        expected_extra_params = {\n            \"project\": other_project,\n            \"prefix\": prefix,\n            \"projection\": projection,\n            \"fields\": fields,\n        }\n        expected_page_size = 2\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            page_token=expected_page_token,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            page_size=expected_page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def _create_hmac_key_helper(\n        self,\n        explicit_project=None,\n        user_project=None,\n        timeout=None,\n        retry=None,\n    ):\n        from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n        project = \"PROJECT\"\n        access_id = \"ACCESS-ID\"\n        credentials = _make_credentials()\n        email = \"storage-user-123@example.com\"\n        secret = \"a\" * 40\n        now = _NOW(_UTC)\n        now_stamp = f\"{now.isoformat()}Z\"\n\n        if explicit_project is not None:\n            expected_project = explicit_project\n        else:\n            expected_project = project\n\n        api_response = {\n            \"kind\": \"storage#hmacKey\",\n            \"metadata\": {\n                \"accessId\": access_id,\n                \"etag\": \"ETAG\",\n                \"id\": f\"projects/{project}/hmacKeys/{access_id}\",\n                \"project\": expected_project,\n                \"state\": \"ACTIVE\",\n                \"serviceAccountEmail\": email,\n                \"timeCreated\": now_stamp,\n                \"updated\": now_stamp,\n            },\n            \"secret\": secret,\n        }\n\n        client = self._make_one(project=project, credentials=credentials)\n        client._post_resource = mock.Mock()\n        client._post_resource.return_value = api_response\n\n        kwargs = {}\n        if explicit_project is not None:\n            kwargs[\"project_id\"] = explicit_project\n\n        if user_project is not None:\n            kwargs[\"user_project\"] = user_project\n\n        if timeout is None:\n            expected_timeout = self._get_default_timeout()\n        else:\n            expected_timeout = kwargs[\"timeout\"] = timeout\n\n        if retry is None:\n            expected_retry = None\n        else:\n            expected_retry = kwargs[\"retry\"] = retry\n\n        metadata, secret = client.create_hmac_key(service_account_email=email, **kwargs)\n\n        self.assertIsInstance(metadata, HMACKeyMetadata)\n\n        self.assertIs(metadata._client, client)\n        self.assertEqual(metadata._properties, api_response[\"metadata\"])\n        self.assertEqual(secret, api_response[\"secret\"])\n\n        expected_path = f\"/projects/{expected_project}/hmacKeys\"\n        expected_data = None\n        expected_query_params = {\"serviceAccountEmail\": email}\n\n        if user_project is not None:\n            expected_query_params[\"userProject\"] = user_project\n\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=expected_timeout,\n            retry=expected_retry,\n        )\n\n    def test_create_hmac_key_defaults(self):\n        self._create_hmac_key_helper()\n\n    def test_create_hmac_key_explicit_project(self):\n        self._create_hmac_key_helper(explicit_project=\"other-project-456\")\n\n    def test_create_hmac_key_w_user_project(self):\n        self._create_hmac_key_helper(user_project=\"billed-project\")\n\n    def test_create_hmac_key_w_timeout(self):\n        self._create_hmac_key_helper(timeout=42)\n\n    def test_create_hmac_key_w_retry(self):\n        self._create_hmac_key_helper(retry=mock.Mock(spec=[]))\n\n    def test_list_hmac_keys_w_defaults(self):\n        from google.cloud.storage.client import _item_to_hmac_key_metadata\n\n        project = \"PROJECT\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._list_resource = mock.Mock(spec=[])\n\n        iterator = client.list_hmac_keys()\n\n        self.assertIs(iterator, client._list_resource.return_value)\n\n        expected_path = f\"/projects/{project}/hmacKeys\"\n        expected_item_to_value = _item_to_hmac_key_metadata\n        expected_max_results = None\n        expected_extra_params = {}\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_hmac_keys_w_explicit(self):\n        from google.cloud.storage.client import _item_to_hmac_key_metadata\n\n        project = \"PROJECT\"\n        other_project = \"other-project-456\"\n        max_results = 3\n        show_deleted_keys = True\n        service_account_email = \"storage-user-123@example.com\"\n        user_project = \"billed-project\"\n        credentials = _make_credentials()\n        client = self._make_one(project=project, credentials=credentials)\n        client._list_resource = mock.Mock(spec=[])\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        iterator = client.list_hmac_keys(\n            max_results=max_results,\n            service_account_email=service_account_email,\n            show_deleted_keys=show_deleted_keys,\n            project_id=other_project,\n            user_project=user_project,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.assertIs(iterator, client._list_resource.return_value)\n\n        expected_path = f\"/projects/{other_project}/hmacKeys\"\n        expected_item_to_value = _item_to_hmac_key_metadata\n        expected_max_results = max_results\n        expected_extra_params = {\n            \"serviceAccountEmail\": service_account_email,\n            \"showDeletedKeys\": show_deleted_keys,\n            \"userProject\": user_project,\n        }\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            max_results=expected_max_results,\n            extra_params=expected_extra_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_get_hmac_key_metadata_wo_project(self):\n        from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n        PROJECT = \"PROJECT\"\n        EMAIL = \"storage-user-123@example.com\"\n        ACCESS_ID = \"ACCESS-ID\"\n        CREDENTIALS = _make_credentials()\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n\n        resource = {\n            \"kind\": \"storage#hmacKeyMetadata\",\n            \"accessId\": ACCESS_ID,\n            \"projectId\": PROJECT,\n            \"serviceAccountEmail\": EMAIL,\n        }\n\n        http = _make_requests_session([_make_json_response(resource)])\n        client._http_internal = http\n\n        metadata = client.get_hmac_key_metadata(ACCESS_ID, timeout=42)\n\n        self.assertIsInstance(metadata, HMACKeyMetadata)\n        self.assertIs(metadata._client, client)\n        self.assertEqual(metadata.access_id, ACCESS_ID)\n        self.assertEqual(metadata.project, PROJECT)\n\n        http.request.assert_called_once_with(\n            method=\"GET\", url=mock.ANY, data=None, headers=mock.ANY, timeout=42\n        )\n        _, kwargs = http.request.call_args\n        scheme, netloc, path, qs, _ = urllib.parse.urlsplit(kwargs.get(\"url\"))\n        self.assertEqual(f\"{scheme}://{netloc}\", client._connection.API_BASE_URL)\n        self.assertEqual(\n            path,\n            \"/\".join(\n                [\n                    \"\",\n                    \"storage\",\n                    client._connection.API_VERSION,\n                    \"projects\",\n                    PROJECT,\n                    \"hmacKeys\",\n                    ACCESS_ID,\n                ]\n            ),\n        )\n\n    def test_get_hmac_key_metadata_w_project(self):\n        from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n        PROJECT = \"PROJECT\"\n        OTHER_PROJECT = \"other-project-456\"\n        EMAIL = \"storage-user-123@example.com\"\n        ACCESS_ID = \"ACCESS-ID\"\n        USER_PROJECT = \"billed-project\"\n        CREDENTIALS = _make_credentials()\n        client = self._make_one(project=PROJECT, credentials=CREDENTIALS)\n\n        resource = {\n            \"kind\": \"storage#hmacKeyMetadata\",\n            \"accessId\": ACCESS_ID,\n            \"projectId\": OTHER_PROJECT,\n            \"serviceAccountEmail\": EMAIL,\n        }\n\n        http = _make_requests_session([_make_json_response(resource)])\n        client._http_internal = http\n\n        metadata = client.get_hmac_key_metadata(\n            ACCESS_ID, project_id=OTHER_PROJECT, user_project=USER_PROJECT\n        )\n\n        self.assertIsInstance(metadata, HMACKeyMetadata)\n        self.assertIs(metadata._client, client)\n        self.assertEqual(metadata.access_id, ACCESS_ID)\n        self.assertEqual(metadata.project, OTHER_PROJECT)\n\n        http.request.assert_called_once_with(\n            method=\"GET\",\n            url=mock.ANY,\n            data=None,\n            headers=mock.ANY,\n            timeout=self._get_default_timeout(),\n        )\n        _, kwargs = http.request.call_args\n        scheme, netloc, path, qs, _ = urllib.parse.urlsplit(kwargs.get(\"url\"))\n        self.assertEqual(f\"{scheme}://{netloc}\", client._connection.API_BASE_URL)\n        self.assertEqual(\n            path,\n            \"/\".join(\n                [\n                    \"\",\n                    \"storage\",\n                    client._connection.API_VERSION,\n                    \"projects\",\n                    OTHER_PROJECT,\n                    \"hmacKeys\",\n                    ACCESS_ID,\n                ]\n            ),\n        )\n        parms = dict(urllib.parse.parse_qsl(qs))\n        self.assertEqual(parms[\"userProject\"], USER_PROJECT)\n\n    def test_get_signed_policy_v4(self):\n        import datetime\n\n        BUCKET_NAME = \"bucket-name\"\n        BLOB_NAME = \"object-name\"\n        EXPECTED_SIGN = \"5369676e61747572655f6279746573\"\n        EXPECTED_POLICY = \"eyJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJidWNrZXQtbmFtZSJ9LHsiYWNsIjoicHJpdmF0ZSJ9LFsic3RhcnRzLXdpdGgiLCIkQ29udGVudC1UeXBlIiwidGV4dC9wbGFpbiJdLHsiYnVja2V0IjoiYnVja2V0LW5hbWUifSx7ImtleSI6Im9iamVjdC1uYW1lIn0seyJ4LWdvb2ctZGF0ZSI6IjIwMjAwMzEyVDExNDcxNloifSx7IngtZ29vZy1jcmVkZW50aWFsIjoidGVzdEBtYWlsLmNvbS8yMDIwMDMxMi9hdXRvL3N0b3JhZ2UvZ29vZzRfcmVxdWVzdCJ9LHsieC1nb29nLWFsZ29yaXRobSI6IkdPT0c0LVJTQS1TSEEyNTYifV0sImV4cGlyYXRpb24iOiIyMDIwLTAzLTI2VDAwOjAwOjEwWiJ9\"\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, now_patch, expire_secs_patch = _time_functions_patches()\n        with dtstamps_patch, now_patch, expire_secs_patch:\n            policy = client.generate_signed_post_policy_v4(\n                BUCKET_NAME,\n                BLOB_NAME,\n                expiration=datetime.datetime(2020, 3, 12),\n                conditions=[\n                    {\"bucket\": BUCKET_NAME},\n                    {\"acl\": \"private\"},\n                    [\"starts-with\", \"$Content-Type\", \"text/plain\"],\n                ],\n                credentials=_create_signing_credentials(),\n            )\n        self.assertEqual(\n            policy[\"url\"], \"https://storage.googleapis.com/\" + BUCKET_NAME + \"/\"\n        )\n        fields = policy[\"fields\"]\n\n        self.assertEqual(fields[\"key\"], BLOB_NAME)\n        self.assertEqual(fields[\"x-goog-algorithm\"], \"GOOG4-RSA-SHA256\")\n        self.assertEqual(fields[\"x-goog-date\"], \"20200312T114716Z\")\n        self.assertEqual(\n            fields[\"x-goog-credential\"],\n            \"test@mail.com/20200312/auto/storage/goog4_request\",\n        )\n        self.assertEqual(fields[\"x-goog-signature\"], EXPECTED_SIGN)\n        self.assertEqual(fields[\"policy\"], EXPECTED_POLICY)\n\n    def test_get_signed_policy_v4_without_credentials(self):\n        import datetime\n\n        BUCKET_NAME = \"bucket-name\"\n        BLOB_NAME = \"object-name\"\n        EXPECTED_SIGN = \"5369676e61747572655f6279746573\"\n        EXPECTED_POLICY = \"eyJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJidWNrZXQtbmFtZSJ9LHsiYWNsIjoicHJpdmF0ZSJ9LFsic3RhcnRzLXdpdGgiLCIkQ29udGVudC1UeXBlIiwidGV4dC9wbGFpbiJdLHsiYnVja2V0IjoiYnVja2V0LW5hbWUifSx7ImtleSI6Im9iamVjdC1uYW1lIn0seyJ4LWdvb2ctZGF0ZSI6IjIwMjAwMzEyVDExNDcxNloifSx7IngtZ29vZy1jcmVkZW50aWFsIjoidGVzdEBtYWlsLmNvbS8yMDIwMDMxMi9hdXRvL3N0b3JhZ2UvZ29vZzRfcmVxdWVzdCJ9LHsieC1nb29nLWFsZ29yaXRobSI6IkdPT0c0LVJTQS1TSEEyNTYifV0sImV4cGlyYXRpb24iOiIyMDIwLTAzLTI2VDAwOjAwOjEwWiJ9\"\n\n        client = self._make_one(\n            project=\"PROJECT\", credentials=_create_signing_credentials()\n        )\n\n        dtstamps_patch, now_patch, expire_secs_patch = _time_functions_patches()\n        with dtstamps_patch, now_patch, expire_secs_patch:\n            policy = client.generate_signed_post_policy_v4(\n                BUCKET_NAME,\n                BLOB_NAME,\n                expiration=datetime.datetime(2020, 3, 12),\n                conditions=[\n                    {\"bucket\": BUCKET_NAME},\n                    {\"acl\": \"private\"},\n                    [\"starts-with\", \"$Content-Type\", \"text/plain\"],\n                ],\n            )\n        self.assertEqual(\n            policy[\"url\"], \"https://storage.googleapis.com/\" + BUCKET_NAME + \"/\"\n        )\n        fields = policy[\"fields\"]\n\n        self.assertEqual(fields[\"key\"], BLOB_NAME)\n        self.assertEqual(fields[\"x-goog-algorithm\"], \"GOOG4-RSA-SHA256\")\n        self.assertEqual(fields[\"x-goog-date\"], \"20200312T114716Z\")\n        self.assertEqual(\n            fields[\"x-goog-credential\"],\n            \"test@mail.com/20200312/auto/storage/goog4_request\",\n        )\n        self.assertEqual(fields[\"x-goog-signature\"], EXPECTED_SIGN)\n        self.assertEqual(fields[\"policy\"], EXPECTED_POLICY)\n\n    def test_get_signed_policy_v4_with_fields(self):\n        import datetime\n\n        BUCKET_NAME = \"bucket-name\"\n        BLOB_NAME = \"object-name\"\n        FIELD1_VALUE = \"Value1\"\n        EXPECTED_SIGN = \"5369676e61747572655f6279746573\"\n        EXPECTED_POLICY = \"eyJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJidWNrZXQtbmFtZSJ9LHsiYWNsIjoicHJpdmF0ZSJ9LFsic3RhcnRzLXdpdGgiLCIkQ29udGVudC1UeXBlIiwidGV4dC9wbGFpbiJdLHsiZmllbGQxIjoiVmFsdWUxIn0seyJidWNrZXQiOiJidWNrZXQtbmFtZSJ9LHsia2V5Ijoib2JqZWN0LW5hbWUifSx7IngtZ29vZy1kYXRlIjoiMjAyMDAzMTJUMTE0NzE2WiJ9LHsieC1nb29nLWNyZWRlbnRpYWwiOiJ0ZXN0QG1haWwuY29tLzIwMjAwMzEyL2F1dG8vc3RvcmFnZS9nb29nNF9yZXF1ZXN0In0seyJ4LWdvb2ctYWxnb3JpdGhtIjoiR09PRzQtUlNBLVNIQTI1NiJ9XSwiZXhwaXJhdGlvbiI6IjIwMjAtMDMtMjZUMDA6MDA6MTBaIn0=\"\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, now_patch, expire_secs_patch = _time_functions_patches()\n        with dtstamps_patch, now_patch, expire_secs_patch:\n            policy = client.generate_signed_post_policy_v4(\n                BUCKET_NAME,\n                BLOB_NAME,\n                expiration=datetime.datetime(2020, 3, 12),\n                conditions=[\n                    {\"bucket\": BUCKET_NAME},\n                    {\"acl\": \"private\"},\n                    [\"starts-with\", \"$Content-Type\", \"text/plain\"],\n                ],\n                fields={\"field1\": FIELD1_VALUE, \"x-ignore-field\": \"Ignored_value\"},\n                credentials=_create_signing_credentials(),\n            )\n        self.assertEqual(\n            policy[\"url\"], \"https://storage.googleapis.com/\" + BUCKET_NAME + \"/\"\n        )\n        fields = policy[\"fields\"]\n\n        self.assertEqual(fields[\"key\"], BLOB_NAME)\n        self.assertEqual(fields[\"x-goog-algorithm\"], \"GOOG4-RSA-SHA256\")\n        self.assertEqual(fields[\"x-goog-date\"], \"20200312T114716Z\")\n        self.assertEqual(fields[\"field1\"], FIELD1_VALUE)\n        self.assertNotIn(\"x-ignore-field\", fields.keys())\n        self.assertEqual(\n            fields[\"x-goog-credential\"],\n            \"test@mail.com/20200312/auto/storage/goog4_request\",\n        )\n        self.assertEqual(fields[\"x-goog-signature\"], EXPECTED_SIGN)\n        self.assertEqual(fields[\"policy\"], EXPECTED_POLICY)\n\n    def test_get_signed_policy_v4_virtual_hosted_style(self):\n        import datetime\n\n        BUCKET_NAME = \"bucket-name\"\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, _, _ = _time_functions_patches()\n        with dtstamps_patch:\n            policy = client.generate_signed_post_policy_v4(\n                BUCKET_NAME,\n                \"object-name\",\n                expiration=datetime.datetime(2020, 3, 12),\n                virtual_hosted_style=True,\n                credentials=_create_signing_credentials(),\n            )\n        self.assertEqual(\n            policy[\"url\"], f\"https://{BUCKET_NAME}.storage.googleapis.com/\"\n        )\n\n    def test_get_signed_policy_v4_bucket_bound_hostname(self):\n        import datetime\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, _, _ = _time_functions_patches()\n        with dtstamps_patch:\n            policy = client.generate_signed_post_policy_v4(\n                \"bucket-name\",\n                \"object-name\",\n                expiration=datetime.datetime(2020, 3, 12),\n                bucket_bound_hostname=\"https://bucket.bound_hostname\",\n                credentials=_create_signing_credentials(),\n            )\n        self.assertEqual(policy[\"url\"], \"https://bucket.bound_hostname/\")\n\n    def test_get_signed_policy_v4_with_conflicting_arguments(self):\n        import datetime\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, _, _ = _time_functions_patches()\n        with dtstamps_patch:\n            with self.assertRaises(ValueError):\n                client.generate_signed_post_policy_v4(\n                    \"bucket-name\",\n                    \"object-name\",\n                    expiration=datetime.datetime(2020, 3, 12),\n                    bucket_bound_hostname=\"https://bucket.bound_hostname\",\n                    virtual_hosted_style=True,\n                    credentials=_create_signing_credentials(),\n                )\n\n    def test_get_signed_policy_v4_bucket_bound_hostname_with_scheme(self):\n        import datetime\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, _, _ = _time_functions_patches()\n        with dtstamps_patch:\n            policy = client.generate_signed_post_policy_v4(\n                \"bucket-name\",\n                \"object-name\",\n                expiration=datetime.datetime(2020, 3, 12),\n                bucket_bound_hostname=\"bucket.bound_hostname\",\n                scheme=\"http\",\n                credentials=_create_signing_credentials(),\n            )\n        self.assertEqual(policy[\"url\"], \"http://bucket.bound_hostname/\")\n\n    def test_get_signed_policy_v4_no_expiration(self):\n        BUCKET_NAME = \"bucket-name\"\n        EXPECTED_POLICY = \"eyJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJidWNrZXQtbmFtZSJ9LHsia2V5Ijoib2JqZWN0LW5hbWUifSx7IngtZ29vZy1kYXRlIjoiMjAyMDAzMTJUMTE0NzE2WiJ9LHsieC1nb29nLWNyZWRlbnRpYWwiOiJ0ZXN0QG1haWwuY29tLzIwMjAwMzEyL2F1dG8vc3RvcmFnZS9nb29nNF9yZXF1ZXN0In0seyJ4LWdvb2ctYWxnb3JpdGhtIjoiR09PRzQtUlNBLVNIQTI1NiJ9XSwiZXhwaXJhdGlvbiI6IjIwMjAtMDMtMjZUMDA6MDA6MTBaIn0=\"\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, now_patch, expire_secs_patch = _time_functions_patches()\n        with dtstamps_patch, now_patch, expire_secs_patch:\n            policy = client.generate_signed_post_policy_v4(\n                BUCKET_NAME,\n                \"object-name\",\n                expiration=None,\n                credentials=_create_signing_credentials(),\n            )\n\n        self.assertEqual(\n            policy[\"url\"], \"https://storage.googleapis.com/\" + BUCKET_NAME + \"/\"\n        )\n        self.assertEqual(policy[\"fields\"][\"policy\"], EXPECTED_POLICY)\n\n    def test_get_signed_policy_v4_with_access_token(self):\n        import datetime\n\n        BUCKET_NAME = \"bucket-name\"\n        BLOB_NAME = \"object-name\"\n        EXPECTED_SIGN = \"0c4003044105\"\n        EXPECTED_POLICY = \"eyJjb25kaXRpb25zIjpbeyJidWNrZXQiOiJidWNrZXQtbmFtZSJ9LHsiYWNsIjoicHJpdmF0ZSJ9LFsic3RhcnRzLXdpdGgiLCIkQ29udGVudC1UeXBlIiwidGV4dC9wbGFpbiJdLHsiYnVja2V0IjoiYnVja2V0LW5hbWUifSx7ImtleSI6Im9iamVjdC1uYW1lIn0seyJ4LWdvb2ctZGF0ZSI6IjIwMjAwMzEyVDExNDcxNloifSx7IngtZ29vZy1jcmVkZW50aWFsIjoidGVzdEBtYWlsLmNvbS8yMDIwMDMxMi9hdXRvL3N0b3JhZ2UvZ29vZzRfcmVxdWVzdCJ9LHsieC1nb29nLWFsZ29yaXRobSI6IkdPT0c0LVJTQS1TSEEyNTYifV0sImV4cGlyYXRpb24iOiIyMDIwLTAzLTI2VDAwOjAwOjEwWiJ9\"\n\n        project = \"PROJECT\"\n        credentials = _make_credentials(project=project)\n        client = self._make_one(credentials=credentials)\n\n        dtstamps_patch, now_patch, expire_secs_patch = _time_functions_patches()\n        with dtstamps_patch, now_patch, expire_secs_patch:\n            with mock.patch(\n                \"google.cloud.storage.client._sign_message\", return_value=b\"DEADBEEF\"\n            ):\n                policy = client.generate_signed_post_policy_v4(\n                    BUCKET_NAME,\n                    BLOB_NAME,\n                    expiration=datetime.datetime(2020, 3, 12),\n                    conditions=[\n                        {\"bucket\": BUCKET_NAME},\n                        {\"acl\": \"private\"},\n                        [\"starts-with\", \"$Content-Type\", \"text/plain\"],\n                    ],\n                    credentials=_create_signing_credentials(),\n                    service_account_email=\"test@mail.com\",\n                    access_token=\"token\",\n                )\n        self.assertEqual(\n            policy[\"url\"], \"https://storage.googleapis.com/\" + BUCKET_NAME + \"/\"\n        )\n        fields = policy[\"fields\"]\n\n        self.assertEqual(fields[\"key\"], BLOB_NAME)\n        self.assertEqual(fields[\"x-goog-algorithm\"], \"GOOG4-RSA-SHA256\")\n        self.assertEqual(fields[\"x-goog-date\"], \"20200312T114716Z\")\n        self.assertEqual(\n            fields[\"x-goog-credential\"],\n            \"test@mail.com/20200312/auto/storage/goog4_request\",\n        )\n        self.assertEqual(fields[\"x-goog-signature\"], EXPECTED_SIGN)\n        self.assertEqual(fields[\"policy\"], EXPECTED_POLICY)\n\n\nclass Test__item_to_bucket(unittest.TestCase):\n    def _call_fut(self, iterator, item):\n        from google.cloud.storage.client import _item_to_bucket\n\n        return _item_to_bucket(iterator, item)\n\n    def test_w_empty_item(self):\n        from google.cloud.storage.bucket import Bucket\n\n        iterator = mock.Mock(spec=[\"client\"])\n        item = {}\n\n        bucket = self._call_fut(iterator, item)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertIs(bucket.client, iterator.client)\n        self.assertIsNone(bucket.name)\n\n    def test_w_name(self):\n        from google.cloud.storage.bucket import Bucket\n\n        name = \"name\"\n        iterator = mock.Mock(spec=[\"client\"])\n        item = {\"name\": name}\n\n        bucket = self._call_fut(iterator, item)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertIs(bucket.client, iterator.client)\n        self.assertEqual(bucket.name, name)\n\n\nclass Test__item_to_hmac_key_metadata(unittest.TestCase):\n    def _call_fut(self, iterator, item):\n        from google.cloud.storage.client import _item_to_hmac_key_metadata\n\n        return _item_to_hmac_key_metadata(iterator, item)\n\n    def test_it(self):\n        from google.cloud.storage.hmac_key import HMACKeyMetadata\n\n        access_id = \"ABCDE\"\n        iterator = mock.Mock(spec=[\"client\"])\n        item = {\"id\": access_id}\n\n        metadata = self._call_fut(iterator, item)\n\n        self.assertIsInstance(metadata, HMACKeyMetadata)\n        self.assertIs(metadata._client, iterator.client)\n        self.assertEqual(metadata._properties, item)\n\n\n@pytest.mark.parametrize(\"test_data\", _POST_POLICY_TESTS)\ndef test_conformance_post_policy(test_data):\n    import datetime\n    from google.cloud.storage.client import Client\n\n    in_data = test_data[\"policyInput\"]\n    timestamp = datetime.datetime.strptime(in_data[\"timestamp\"], \"%Y-%m-%dT%H:%M:%SZ\")\n\n    client = Client(credentials=_FAKE_CREDENTIALS, project=\"PROJECT\")\n\n    # mocking time functions\n    with mock.patch(\"google.cloud.storage._signing._NOW\", return_value=timestamp):\n        with mock.patch(\n            \"google.cloud.storage.client.get_expiration_seconds_v4\",\n            return_value=in_data[\"expiration\"],\n        ):\n            with mock.patch(\"google.cloud.storage.client._NOW\", return_value=timestamp):\n                policy = client.generate_signed_post_policy_v4(\n                    bucket_name=in_data[\"bucket\"],\n                    blob_name=in_data[\"object\"],\n                    conditions=_prepare_conditions(in_data),\n                    fields=in_data.get(\"fields\"),\n                    credentials=_FAKE_CREDENTIALS,\n                    expiration=in_data[\"expiration\"],\n                    virtual_hosted_style=in_data.get(\"urlStyle\")\n                    == \"VIRTUAL_HOSTED_STYLE\",\n                    bucket_bound_hostname=in_data.get(\"bucketBoundHostname\"),\n                    scheme=in_data.get(\"scheme\"),\n                )\n    fields = policy[\"fields\"]\n    out_data = test_data[\"policyOutput\"]\n\n    decoded_policy = base64.b64decode(fields[\"policy\"]).decode(\"unicode_escape\")\n    assert decoded_policy == out_data[\"expectedDecodedPolicy\"]\n\n    for field in (\n        \"x-goog-algorithm\",\n        \"x-goog-credential\",\n        \"x-goog-date\",\n        \"x-goog-signature\",\n    ):\n        assert fields[field] == test_data[\"policyOutput\"][\"fields\"][field]\n\n    assert policy[\"url\"] == out_data[\"url\"]\n\n\ndef _prepare_conditions(in_data):\n    \"\"\"Helper for V4 POST policy generation conformance tests.\n\n    Convert conformance test data conditions dict into list.\n\n    Args:\n        in_data (dict): conditions arg from conformance test data.\n\n    Returns:\n        list: conditions arg to pass into generate_signed_post_policy_v4().\n    \"\"\"\n    if \"conditions\" in in_data:\n        conditions = []\n        for key, value in in_data[\"conditions\"].items():\n            # camel case to snake case with \"-\" separator\n            field = re.sub(r\"(?<!^)(?=[A-Z])\", \"-\", key).lower()\n            conditions.append([field] + value)\n\n        return conditions\n\n\ndef _time_functions_patches():\n    \"\"\"Helper for POST policy generation - returns all necessary time functions patches.\"\"\"\n    import datetime\n\n    dtstamps_patch = mock.patch(\n        \"google.cloud.storage.client.get_v4_now_dtstamps\",\n        return_value=(\"20200312T114716Z\", \"20200312\"),\n    )\n    now_patch = mock.patch(\n        \"google.cloud.storage.client._NOW\", return_value=datetime.datetime(2020, 3, 26)\n    )\n    expire_secs_patch = mock.patch(\n        \"google.cloud.storage.client.get_expiration_seconds_v4\", return_value=10\n    )\n    return dtstamps_patch, now_patch, expire_secs_patch\n", "tests/unit/test__helpers.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport mock\n\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\nGCCL_INVOCATION_TEST_CONST = \"gccl-invocation-id/test-invocation-123\"\n\n\nclass Test__get_storage_emulator_override(unittest.TestCase):\n    @staticmethod\n    def _call_fut():\n        from google.cloud.storage._helpers import _get_storage_emulator_override\n\n        return _get_storage_emulator_override()\n\n    def test_wo_env_var(self):\n        with mock.patch(\"os.environ\", {}):\n            override = self._call_fut()\n\n        self.assertIsNone(override)\n\n    def test_w_env_var(self):\n        from google.cloud.storage._helpers import STORAGE_EMULATOR_ENV_VAR\n\n        HOST = \"https://api.example.com\"\n\n        with mock.patch(\"os.environ\", {STORAGE_EMULATOR_ENV_VAR: HOST}):\n            emu = self._call_fut()\n\n        self.assertEqual(emu, HOST)\n\n\nclass Test__get_api_endpoint_override(unittest.TestCase):\n    @staticmethod\n    def _call_fut():\n        from google.cloud.storage._helpers import _get_api_endpoint_override\n\n        return _get_api_endpoint_override()\n\n    def test_wo_env_var(self):\n        from google.cloud.storage._helpers import _TRUE_DEFAULT_STORAGE_HOST\n        from google.cloud.storage._helpers import _DEFAULT_SCHEME\n\n        with mock.patch(\"os.environ\", {}):\n            override = self._call_fut()\n\n        self.assertIsNone(override, _DEFAULT_SCHEME + _TRUE_DEFAULT_STORAGE_HOST)\n\n    def test_w_env_var(self):\n        from google.cloud.storage._helpers import _API_ENDPOINT_OVERRIDE_ENV_VAR\n\n        BASE_URL = \"https://api.example.com\"\n\n        with mock.patch(\"os.environ\", {_API_ENDPOINT_OVERRIDE_ENV_VAR: BASE_URL}):\n            override = self._call_fut()\n\n        self.assertEqual(override, BASE_URL)\n\n\nclass Test__get_environ_project(unittest.TestCase):\n    @staticmethod\n    def _call_fut():\n        from google.cloud.storage._helpers import _get_environ_project\n\n        return _get_environ_project()\n\n    def test_wo_env_var(self):\n        with mock.patch(\"os.environ\", {}):\n            project = self._call_fut()\n\n        self.assertEqual(project, None)\n\n    def test_w_env_var(self):\n        from google.auth import environment_vars\n\n        PROJECT = \"environ-project\"\n\n        with mock.patch(\"os.environ\", {environment_vars.PROJECT: PROJECT}):\n            project = self._call_fut()\n        self.assertEqual(project, PROJECT)\n\n        with mock.patch(\"os.environ\", {environment_vars.LEGACY_PROJECT: PROJECT}):\n            project = self._call_fut()\n\n        self.assertEqual(project, PROJECT)\n\n\nclass Test_PropertyMixin(unittest.TestCase):\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage._helpers import _PropertyMixin\n\n        return _PropertyMixin\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def _derivedClass(self, path=None, user_project=None):\n        class Derived(self._get_target_class()):\n            client = None\n            _actual_encryption_headers = None\n\n            @property\n            def path(self):\n                return path\n\n            @property\n            def user_project(self):\n                return user_project\n\n            def _encryption_headers(self):\n                return self._actual_encryption_headers or {}\n\n        return Derived\n\n    def test_path_is_abstract(self):\n        mixin = self._make_one()\n        with self.assertRaises(NotImplementedError):\n            mixin.path\n\n    def test_client_is_abstract(self):\n        mixin = self._make_one()\n        with self.assertRaises(NotImplementedError):\n            mixin.client\n\n    def test_user_project_is_abstract(self):\n        mixin = self._make_one()\n        with self.assertRaises(NotImplementedError):\n            mixin.user_project\n\n    def test__encryption_headers(self):\n        mixin = self._make_one()\n        self.assertEqual(mixin._encryption_headers(), {})\n\n    def test__query_params_wo_user_project(self):\n        derived = self._derivedClass(\"/path\", None)()\n        self.assertEqual(derived._query_params, {})\n\n    def test__query_params_w_user_project(self):\n        user_project = \"user-project-123\"\n        derived = self._derivedClass(\"/path\", user_project)()\n        self.assertEqual(derived._query_params, {\"userProject\": user_project})\n\n    def test_reload_w_defaults(self):\n        path = \"/path\"\n        response = {\"foo\": \"Foo\"}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = response\n        derived = self._derivedClass(path)()\n        # Make sure changes is not a set instance before calling reload\n        # (which will clear / replace it with an empty set), checked below.\n        derived._changes = object()\n        derived.client = client\n\n        derived.reload()\n\n        self.assertEqual(derived._properties, response)\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}  # no encryption headers by default\n        client._get_resource.assert_called_once_with(\n            path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=derived,\n        )\n\n    def test_reload_w_etag_match(self):\n        etag = \"kittens\"\n        path = \"/path\"\n        response = {\"foo\": \"Foo\"}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = response\n        derived = self._derivedClass(path)()\n        # Make sure changes is not a set instance before calling reload\n        # (which will clear / replace it with an empty set), checked below.\n        derived._changes = object()\n        derived.client = client\n\n        derived.reload(\n            if_etag_match=etag,\n        )\n\n        self.assertEqual(derived._properties, response)\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n        }\n        # no encryption headers by default\n        expected_headers = {\n            \"If-Match\": etag,\n        }\n        client._get_resource.assert_called_once_with(\n            path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=derived,\n        )\n\n    def test_reload_w_generation_match_w_timeout(self):\n        generation_number = 9\n        metageneration_number = 6\n        path = \"/path\"\n        timeout = 42\n        response = {\"foo\": \"Foo\"}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = response\n        derived = self._derivedClass(path)()\n        # Make sure changes is not a set instance before calling reload\n        # (which will clear / replace it with an empty set), checked below.\n        derived._changes = object()\n        derived.client = client\n\n        derived.reload(\n            if_generation_match=generation_number,\n            if_metageneration_match=metageneration_number,\n            timeout=timeout,\n        )\n\n        self.assertEqual(derived._properties, response)\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n            \"ifGenerationMatch\": generation_number,\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        expected_headers = {}  # no encryption headers by default\n        client._get_resource.assert_called_once_with(\n            path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=derived,\n        )\n\n    def test_reload_w_user_project_w_retry(self):\n        user_project = \"user-project-123\"\n        path = \"/path\"\n        retry = mock.Mock(spec=[])\n        response = {\"foo\": \"Foo\"}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = response\n        derived = self._derivedClass(path, user_project)()\n        # Make sure changes is not a set instance before calling reload\n        # (which will clear / replace it with an empty set), checked below.\n        derived._changes = object()\n        derived.client = client\n\n        derived.reload(retry=retry)\n\n        self.assertEqual(derived._properties, response)\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n            \"userProject\": user_project,\n        }\n        expected_headers = {}  # no encryption headers by default\n        client._get_resource.assert_called_once_with(\n            path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=derived,\n        )\n\n    def test_reload_w_projection_w_explicit_client_w_enc_header(self):\n        path = \"/path\"\n        response = {\"foo\": \"Foo\"}\n        encryption_headers = {\"bar\": \"Bar\"}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = response\n        derived = self._derivedClass(path)()\n        # Make sure changes is not a set instance before calling reload\n        # (which will clear / replace it with an empty set), checked below.\n        derived._changes = object()\n        derived._actual_encryption_headers = encryption_headers\n\n        derived.reload(projection=\"full\", client=client)\n\n        self.assertEqual(derived._properties, response)\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\"projection\": \"full\"}\n        client._get_resource.assert_called_once_with(\n            path,\n            query_params=expected_query_params,\n            headers=encryption_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=derived,\n        )\n\n    def test__set_properties(self):\n        mixin = self._make_one()\n        self.assertEqual(mixin._properties, {})\n        VALUE = object()\n        mixin._set_properties(VALUE)\n        self.assertEqual(mixin._properties, VALUE)\n\n    def test__patch_property(self):\n        derived = self._derivedClass()()\n        derived._patch_property(\"foo\", \"Foo\")\n        self.assertEqual(derived._properties, {\"foo\": \"Foo\"})\n\n    def test_patch_w_defaults(self):\n        path = \"/path\"\n        api_response = {\"foo\": \"Foo\"}\n        derived = self._derivedClass(path)()\n        # Make sure changes is non-empty, so we can observe a change.\n        bar = object()\n        baz = object()\n        derived._properties = {\"bar\": bar, \"baz\": baz}\n        derived._changes = set([\"bar\"])  # Ignore baz.\n        client = derived.client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n\n        derived.patch()\n\n        self.assertEqual(derived._properties, api_response)\n        # Make sure changes get reset by patch().\n        self.assertEqual(derived._changes, set())\n\n        expected_data = {\"bar\": bar}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=derived,\n        )\n\n    def test_patch_w_metageneration_match_w_timeout_w_retry(self):\n        path = \"/path\"\n        api_response = {\"foo\": \"Foo\"}\n        derived = self._derivedClass(path)()\n        # Make sure changes is non-empty, so we can observe a change.\n        bar = object()\n        baz = object()\n        derived._properties = {\"bar\": bar, \"baz\": baz}\n        derived._changes = set([\"bar\"])  # Ignore baz.\n        client = derived.client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        generation_number = 9\n        metageneration_number = 6\n        override_unlocked_retention = True\n\n        derived.patch(\n            if_generation_match=generation_number,\n            if_metageneration_match=metageneration_number,\n            timeout=timeout,\n            retry=retry,\n            override_unlocked_retention=override_unlocked_retention,\n        )\n\n        self.assertEqual(derived._properties, {\"foo\": \"Foo\"})\n        # Make sure changes get reset by patch().\n        self.assertEqual(derived._changes, set())\n\n        expected_data = {\"bar\": bar}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifGenerationMatch\": generation_number,\n            \"ifMetagenerationMatch\": metageneration_number,\n            \"overrideUnlockedRetention\": override_unlocked_retention,\n        }\n        client._patch_resource.assert_called_once_with(\n            path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=derived,\n        )\n\n    def test_patch_w_user_project_w_explicit_client(self):\n        path = \"/path\"\n        user_project = \"user-project-123\"\n        api_response = {\"foo\": \"Foo\"}\n        derived = self._derivedClass(path, user_project)()\n        # Make sure changes is non-empty, so we can observe a change.\n        bar = object()\n        baz = object()\n        derived._properties = {\"bar\": bar, \"baz\": baz}\n        derived._changes = set([\"bar\"])  # Ignore baz.\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n\n        derived.patch(client=client)\n\n        self.assertEqual(derived._properties, {\"foo\": \"Foo\"})\n        # Make sure changes get reset by patch().\n        self.assertEqual(derived._changes, set())\n\n        expected_data = {\"bar\": bar}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"userProject\": user_project,\n        }\n        client._patch_resource.assert_called_once_with(\n            path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=derived,\n        )\n\n    def test_update_w_defaults(self):\n        path = \"/path\"\n        api_response = {\"foo\": \"Foo\"}\n        derived = self._derivedClass(path)()\n        # Make sure changes is non-empty, so we can observe a change.\n        bar = object()\n        baz = object()\n        expected_data = derived._properties = {\"bar\": bar, \"baz\": baz}\n        derived._changes = set([\"bar\"])  # Update sends 'baz' anyway.\n        client = derived.client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n\n        derived.update()\n\n        self.assertEqual(derived._properties, api_response)\n        # Make sure changes get reset by update().\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\"projection\": \"full\"}\n        client._put_resource.assert_called_once_with(\n            path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=derived,\n        )\n\n    def test_update_with_metageneration_not_match_w_timeout_w_retry(self):\n        path = \"/path\"\n        generation_number = 6\n        api_response = {\"foo\": \"Foo\"}\n        derived = self._derivedClass(path)()\n        # Make sure changes is non-empty, so we can observe a change.\n        bar = object()\n        baz = object()\n        expected_data = derived._properties = {\"bar\": bar, \"baz\": baz}\n        derived._changes = set([\"bar\"])  # Update sends 'baz' anyway.\n        client = derived.client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n        timeout = 42\n        override_unlocked_retention = True\n\n        derived.update(\n            if_metageneration_not_match=generation_number,\n            timeout=timeout,\n            override_unlocked_retention=override_unlocked_retention,\n        )\n\n        self.assertEqual(derived._properties, {\"foo\": \"Foo\"})\n        # Make sure changes get reset by patch().\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationNotMatch\": generation_number,\n            \"overrideUnlockedRetention\": override_unlocked_retention,\n        }\n        client._put_resource.assert_called_once_with(\n            path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=derived,\n        )\n\n    def test_update_w_user_project_w_retry_w_explicit_client(self):\n        user_project = \"user-project-123\"\n        path = \"/path\"\n        api_response = {\"foo\": \"Foo\"}\n        derived = self._derivedClass(path, user_project)()\n        # Make sure changes is non-empty, so we can observe a change.\n        bar = object()\n        baz = object()\n        expected_data = derived._properties = {\"bar\": bar, \"baz\": baz}\n        derived._changes = set([\"bar\"])  # Update sends 'baz' anyway.\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n        retry = mock.Mock(spec=[])\n\n        derived.update(client=client, retry=retry)\n        # Make sure changes get reset by patch().\n        self.assertEqual(derived._changes, set())\n\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"userProject\": user_project,\n        }\n        client._put_resource.assert_called_once_with(\n            path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=derived,\n        )\n\n\nclass Test__scalar_property(unittest.TestCase):\n    def _call_fut(self, fieldName):\n        from google.cloud.storage._helpers import _scalar_property\n\n        return _scalar_property(fieldName)\n\n    def test_getter(self):\n        class Test(object):\n            def __init__(self, **kw):\n                self._properties = kw.copy()\n\n            do_re_mi = self._call_fut(\"solfege\")\n\n        test = Test(solfege=\"Latido\")\n        self.assertEqual(test.do_re_mi, \"Latido\")\n\n    def test_setter(self):\n        class Test(object):\n            def _patch_property(self, name, value):\n                self._patched = (name, value)\n\n            do_re_mi = self._call_fut(\"solfege\")\n\n        test = Test()\n        test.do_re_mi = \"Latido\"\n        self.assertEqual(test._patched, (\"solfege\", \"Latido\"))\n\n\nclass Test__base64_md5hash(unittest.TestCase):\n    def _call_fut(self, bytes_to_sign):\n        from google.cloud.storage._helpers import _base64_md5hash\n\n        return _base64_md5hash(bytes_to_sign)\n\n    def test_it(self):\n        from io import BytesIO\n\n        BYTES_TO_SIGN = b\"FOO\"\n        BUFFER = BytesIO()\n        BUFFER.write(BYTES_TO_SIGN)\n        BUFFER.seek(0)\n\n        SIGNED_CONTENT = self._call_fut(BUFFER)\n        self.assertEqual(SIGNED_CONTENT, b\"kBiQqOnIz21aGlQrIp/r/w==\")\n\n    def test_it_with_stubs(self):\n        import mock\n\n        class _Buffer(object):\n            def __init__(self, return_vals):\n                self.return_vals = return_vals\n                self._block_sizes = []\n\n            def read(self, block_size):\n                self._block_sizes.append(block_size)\n                return self.return_vals.pop()\n\n        BASE64 = _Base64()\n        DIGEST_VAL = object()\n        BYTES_TO_SIGN = b\"BYTES_TO_SIGN\"\n        BUFFER = _Buffer([b\"\", BYTES_TO_SIGN])\n        MD5 = _MD5(DIGEST_VAL)\n\n        patch = mock.patch.multiple(\n            \"google.cloud.storage._helpers\", base64=BASE64, md5=MD5\n        )\n        with patch:\n            SIGNED_CONTENT = self._call_fut(BUFFER)\n\n        self.assertEqual(BUFFER._block_sizes, [8192, 8192])\n        self.assertIs(SIGNED_CONTENT, DIGEST_VAL)\n        self.assertEqual(BASE64._called_b64encode, [DIGEST_VAL])\n        self.assertEqual(MD5._called, [None])\n        self.assertEqual(MD5.hash_obj.num_digest_calls, 1)\n        self.assertEqual(MD5.hash_obj._blocks, [BYTES_TO_SIGN])\n\n\nclass Test__add_etag_match_headers(unittest.TestCase):\n    def _call_fut(self, headers, **match_params):\n        from google.cloud.storage._helpers import _add_etag_match_headers\n\n        return _add_etag_match_headers(headers, **match_params)\n\n    def test_add_etag_match_parameters_str(self):\n        ETAG = \"kittens\"\n        headers = {\"foo\": \"bar\"}\n        EXPECTED_HEADERS = {\n            \"foo\": \"bar\",\n            \"If-Match\": ETAG,\n        }\n        self._call_fut(headers, if_etag_match=ETAG)\n        self.assertEqual(headers, EXPECTED_HEADERS)\n\n    def test_add_generation_match_parameters_list(self):\n        ETAGS = [\"kittens\", \"fluffy\"]\n        EXPECTED_HEADERS = {\n            \"foo\": \"bar\",\n            \"If-Match\": \", \".join(ETAGS),\n        }\n        headers = {\"foo\": \"bar\"}\n        self._call_fut(headers, if_etag_match=ETAGS)\n        self.assertEqual(headers, EXPECTED_HEADERS)\n\n\nclass Test__add_generation_match_parameters(unittest.TestCase):\n    def _call_fut(self, params, **match_params):\n        from google.cloud.storage._helpers import _add_generation_match_parameters\n\n        return _add_generation_match_parameters(params, **match_params)\n\n    def test_add_generation_match_parameters_list(self):\n        GENERATION_NUMBER = 9\n        METAGENERATION_NUMBER = 6\n        EXPECTED_PARAMS = [\n            (\"param1\", \"value1\"),\n            (\"param2\", \"value2\"),\n            (\"ifGenerationMatch\", GENERATION_NUMBER),\n            (\"ifMetagenerationMatch\", METAGENERATION_NUMBER),\n        ]\n        params = [(\"param1\", \"value1\"), (\"param2\", \"value2\")]\n        self._call_fut(\n            params,\n            if_generation_match=GENERATION_NUMBER,\n            if_metageneration_match=METAGENERATION_NUMBER,\n        )\n        self.assertEqual(params, EXPECTED_PARAMS)\n\n    def test_add_generation_match_parameters_dict(self):\n        GENERATION_NUMBER = 9\n        METAGENERATION_NUMBER = 6\n        EXPECTED_PARAMS = {\n            \"param1\": \"value1\",\n            \"param2\": \"value2\",\n            \"ifGenerationMatch\": GENERATION_NUMBER,\n            \"ifMetagenerationMatch\": METAGENERATION_NUMBER,\n        }\n\n        params = {\"param1\": \"value1\", \"param2\": \"value2\"}\n        self._call_fut(\n            params,\n            if_generation_match=GENERATION_NUMBER,\n            if_metageneration_match=METAGENERATION_NUMBER,\n        )\n        self.assertEqual(params, EXPECTED_PARAMS)\n\n    def test_add_generation_match_parameters_tuple(self):\n        GENERATION_NUMBER = 9\n        METAGENERATION_NUMBER = 6\n\n        params = ((\"param1\", \"value1\"), (\"param2\", \"value2\"))\n        with self.assertRaises(ValueError):\n            self._call_fut(\n                params,\n                if_generation_match=GENERATION_NUMBER,\n                if_metageneration_match=METAGENERATION_NUMBER,\n            )\n\n\nclass Test__bucket_bound_hostname_url(unittest.TestCase):\n    def _call_fut(self, **args):\n        from google.cloud.storage._helpers import _bucket_bound_hostname_url\n\n        return _bucket_bound_hostname_url(**args)\n\n    def test_full_hostname(self):\n        HOST = \"scheme://domain.tcl\"\n        self.assertEqual(self._call_fut(host=HOST), HOST)\n\n    def test_hostname_and_scheme(self):\n        HOST = \"domain.tcl\"\n        SCHEME = \"scheme\"\n        EXPECTED_URL = SCHEME + \"://\" + HOST\n\n        self.assertEqual(self._call_fut(host=HOST, scheme=SCHEME), EXPECTED_URL)\n\n\nclass Test__api_core_retry_to_resumable_media_retry(unittest.TestCase):\n    def test_conflict(self):\n        from google.cloud.storage._helpers import (\n            _api_core_retry_to_resumable_media_retry,\n        )\n\n        with self.assertRaises(ValueError):\n            _api_core_retry_to_resumable_media_retry(retry=DEFAULT_RETRY, num_retries=2)\n\n    def test_retry(self):\n        from google.cloud.storage._helpers import (\n            _api_core_retry_to_resumable_media_retry,\n        )\n\n        retry_strategy = _api_core_retry_to_resumable_media_retry(retry=DEFAULT_RETRY)\n        self.assertEqual(retry_strategy.max_sleep, DEFAULT_RETRY._maximum)\n        self.assertEqual(retry_strategy.max_cumulative_retry, DEFAULT_RETRY._deadline)\n        self.assertEqual(retry_strategy.initial_delay, DEFAULT_RETRY._initial)\n        self.assertEqual(retry_strategy.multiplier, DEFAULT_RETRY._multiplier)\n\n    def test_num_retries(self):\n        from google.cloud.storage._helpers import (\n            _api_core_retry_to_resumable_media_retry,\n        )\n\n        retry_strategy = _api_core_retry_to_resumable_media_retry(\n            retry=None, num_retries=2\n        )\n        self.assertEqual(retry_strategy.max_retries, 2)\n\n    def test_none(self):\n        from google.cloud.storage._helpers import (\n            _api_core_retry_to_resumable_media_retry,\n        )\n\n        retry_strategy = _api_core_retry_to_resumable_media_retry(retry=None)\n        self.assertEqual(retry_strategy.max_retries, 0)\n\n\nclass _MD5Hash(object):\n    def __init__(self, digest_val):\n        self.digest_val = digest_val\n        self.num_digest_calls = 0\n        self._blocks = []\n\n    def update(self, block):\n        self._blocks.append(block)\n\n    def digest(self):\n        self.num_digest_calls += 1\n        return self.digest_val\n\n\nclass _MD5(object):\n    def __init__(self, digest_val):\n        self.hash_obj = _MD5Hash(digest_val)\n        self._called = []\n\n    def __call__(self, data=None):\n        self._called.append(data)\n        return self.hash_obj\n\n\nclass _Base64(object):\n    def __init__(self):\n        self._called_b64encode = []\n\n    def b64encode(self, value):\n        self._called_b64encode.append(value)\n        return value\n", "tests/unit/test_fileio.py": "# coding=utf-8\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport io\nimport string\n\nimport mock\n\nfrom google.api_core.exceptions import RequestRangeNotSatisfiable\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\nTEST_TEXT_DATA = string.ascii_lowercase + \"\\n\" + string.ascii_uppercase + \"\\n\"\nTEST_BINARY_DATA = TEST_TEXT_DATA.encode(\"utf-8\")\nTEST_MULTIBYTE_TEXT_DATA = \"\u3042\u3044\u3046\u3048\u304a\u304b\u304d\u304f\u3051\u3053\u3055\u3057\u3059\u305b\u305d\u305f\u3061\u3064\u3066\u3068\"\nPLAIN_CONTENT_TYPE = \"text/plain\"\nNUM_RETRIES = 2\n\n\nclass _BlobReaderBase:\n    @staticmethod\n    def _make_blob_reader(*args, **kwargs):\n        from google.cloud.storage.fileio import BlobReader\n\n        return BlobReader(*args, **kwargs)\n\n\nclass _BlobWriterBase:\n    @staticmethod\n    def _make_blob_writer(*args, **kwargs):\n        from google.cloud.storage.fileio import BlobWriter\n\n        return BlobWriter(*args, **kwargs)\n\n\nclass TestBlobReaderBinary(unittest.TestCase, _BlobReaderBase):\n    def test_attributes(self):\n        blob = mock.Mock()\n        blob.chunk_size = 256\n        reader = self._make_blob_reader(blob)\n        self.assertTrue(reader.seekable())\n        self.assertTrue(reader.readable())\n        self.assertFalse(reader.writable())\n        self.assertEqual(reader._chunk_size, 256)\n        self.assertEqual(reader._retry, DEFAULT_RETRY)\n\n    def test_attributes_explict(self):\n        blob = mock.Mock()\n        blob.chunk_size = 256\n        reader = self._make_blob_reader(blob, chunk_size=1024, retry=None)\n        self.assertEqual(reader._chunk_size, 1024)\n        self.assertIsNone(reader._retry)\n\n    def test_read(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_BINARY_DATA[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = self._make_blob_reader(blob, chunk_size=8, **download_kwargs)\n\n        # Read and trigger the first download of chunk_size.\n        self.assertEqual(reader.read(1), TEST_BINARY_DATA[0:1])\n        blob.download_as_bytes.assert_called_once_with(\n            start=0, end=8, checksum=None, retry=DEFAULT_RETRY, **download_kwargs\n        )\n\n        # Read from buffered data only.\n        self.assertEqual(reader.read(3), TEST_BINARY_DATA[1:4])\n        blob.download_as_bytes.assert_called_once()\n\n        # Read remaining buffer plus an additional chunk read.\n        self.assertEqual(reader.read(8), TEST_BINARY_DATA[4:12])\n        self.assertEqual(reader._pos, 12)\n        self.assertEqual(blob.download_as_bytes.call_count, 2)\n        blob.download_as_bytes.assert_called_with(\n            start=8, end=16, checksum=None, retry=DEFAULT_RETRY, **download_kwargs\n        )\n\n        # Read a larger amount, requiring a download larger than chunk_size.\n        self.assertEqual(reader.read(16), TEST_BINARY_DATA[12:28])\n        self.assertEqual(reader._pos, 28)\n        self.assertEqual(blob.download_as_bytes.call_count, 3)\n        blob.download_as_bytes.assert_called_with(\n            start=16, end=28, checksum=None, retry=DEFAULT_RETRY, **download_kwargs\n        )\n\n        # Read all remaining data.\n        self.assertEqual(reader.read(), TEST_BINARY_DATA[28:])\n        self.assertEqual(blob.download_as_bytes.call_count, 4)\n        blob.download_as_bytes.assert_called_with(\n            start=28, end=None, checksum=None, retry=DEFAULT_RETRY, **download_kwargs\n        )\n\n        reader.close()\n\n    def test_read_with_raw_download(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_BINARY_DATA[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        download_kwargs = {\"raw_download\": True}\n        reader = self._make_blob_reader(blob, chunk_size=8, **download_kwargs)\n\n        # Read and trigger the first download of chunk_size.\n        self.assertEqual(reader.read(1), TEST_BINARY_DATA[0:1])\n        blob.download_as_bytes.assert_called_once_with(\n            start=0, end=8, checksum=None, retry=DEFAULT_RETRY, raw_download=True\n        )\n\n        reader.close()\n\n    def test_retry_passed_through(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_BINARY_DATA[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = self._make_blob_reader(\n            blob, chunk_size=8, retry=None, **download_kwargs\n        )\n\n        # Read and trigger the first download of chunk_size.\n        self.assertEqual(reader.read(1), TEST_BINARY_DATA[0:1])\n        blob.download_as_bytes.assert_called_once_with(\n            start=0, end=8, checksum=None, retry=None, **download_kwargs\n        )\n\n        reader.close()\n\n    def test_416_error_handled(self):\n        blob = mock.Mock()\n        blob.download_as_bytes = mock.Mock(\n            side_effect=RequestRangeNotSatisfiable(\"message\")\n        )\n\n        reader = self._make_blob_reader(blob)\n        self.assertEqual(reader.read(), b\"\")\n\n    def test_readline(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_BINARY_DATA[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        reader = self._make_blob_reader(blob, chunk_size=10)\n\n        # Read a line. With chunk_size=10, expect three chunks downloaded.\n        self.assertEqual(reader.readline(), TEST_BINARY_DATA[:27])\n        blob.download_as_bytes.assert_called_with(\n            start=20, end=30, checksum=None, retry=DEFAULT_RETRY\n        )\n        self.assertEqual(blob.download_as_bytes.call_count, 3)\n\n        # Read another line.\n        self.assertEqual(reader.readline(), TEST_BINARY_DATA[27:])\n        blob.download_as_bytes.assert_called_with(\n            start=50, end=60, checksum=None, retry=DEFAULT_RETRY\n        )\n        self.assertEqual(blob.download_as_bytes.call_count, 6)\n\n        blob.size = len(TEST_BINARY_DATA)\n        reader.seek(0)\n\n        # Read all lines. The readlines algorithm will attempt to read past the end of the last line once to verify there is no more to read.\n        self.assertEqual(b\"\".join(reader.readlines()), TEST_BINARY_DATA)\n        blob.download_as_bytes.assert_called_with(\n            start=len(TEST_BINARY_DATA),\n            end=len(TEST_BINARY_DATA) + 10,\n            checksum=None,\n            retry=DEFAULT_RETRY,\n        )\n        self.assertEqual(blob.download_as_bytes.call_count, 13)\n\n        reader.close()\n\n    def test_seek(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_BINARY_DATA[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        blob.size = None\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = self._make_blob_reader(blob, chunk_size=8, **download_kwargs)\n\n        # Seek needs the blob size to work and should call reload() if the size\n        # is not known. Set a mock to initialize the size if reload() is called.\n        def initialize_size(**_):\n            blob.size = len(TEST_BINARY_DATA)\n\n        blob.reload = mock.Mock(side_effect=initialize_size)\n\n        # Seek, forcing a blob reload in order to validate the seek doesn't\n        # exceed the end of the blob.\n        self.assertEqual(reader.seek(4), 4)\n        blob.reload.assert_called_once_with(**download_kwargs)\n        self.assertEqual(reader.read(4), TEST_BINARY_DATA[4:8])\n        self.assertEqual(blob.download_as_bytes.call_count, 1)\n\n        # Seek forward 2 bytes with whence=1. Position is still in buffer.\n        self.assertEqual(reader.seek(2, 1), 10)\n        self.assertEqual(reader.read(2), TEST_BINARY_DATA[10:12])\n        self.assertEqual(blob.download_as_bytes.call_count, 1)\n\n        # Attempt seek past end of file. Position should be at end of file.\n        self.assertEqual(\n            reader.seek(len(TEST_BINARY_DATA) + 100), len(TEST_BINARY_DATA)\n        )\n\n        # Seek to beginning. The next read will need to download data again.\n        self.assertEqual(reader.seek(0), 0)\n        self.assertEqual(reader.read(4), TEST_BINARY_DATA[0:4])\n        self.assertEqual(blob.download_as_bytes.call_count, 2)\n\n        # Seek relative to end with whence=2.\n        self.assertEqual(reader.seek(-1, 2), len(TEST_BINARY_DATA) - 1)\n        self.assertEqual(reader.read(), TEST_BINARY_DATA[-1:])\n        self.assertEqual(blob.download_as_bytes.call_count, 3)\n\n        with self.assertRaises(ValueError):\n            reader.seek(1, 4)\n\n        # tell() is an inherited method that uses seek().\n        self.assertEqual(reader.tell(), reader._pos)\n\n        reader.close()\n\n    def test_advanced_seek(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_BINARY_DATA[start:end] * 1024\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        blob.size = None\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = self._make_blob_reader(blob, chunk_size=1024, **download_kwargs)\n\n        # Seek needs the blob size to work and should call reload() if the size\n        # is not known. Set a mock to initialize the size if reload() is called.\n        def initialize_size(**_):\n            blob.size = len(TEST_BINARY_DATA) * 1024\n\n        blob.reload = mock.Mock(side_effect=initialize_size)\n\n        self.assertEqual(reader.tell(), 0)\n        # Mimic tarfile access pattern. Read tarinfo block.\n        reader.read(512)\n        self.assertEqual(reader.tell(), 512)\n        self.assertEqual(reader.seek(512), 512)\n        # Mimic read actual tar content.\n        reader.read(400)\n        self.assertEqual(reader.tell(), 912)\n        # Tarfile offsets are rounded up by block size\n        # A sanity seek/read is used to check for unexpected ends.\n        reader.seek(1023)\n        reader.read(1)\n        self.assertEqual(reader.tell(), 1024)\n        reader.read(512)\n        self.assertEqual(reader.tell(), 1536)\n        reader.close()\n\n    def test_close(self):\n        blob = mock.Mock()\n        reader = self._make_blob_reader(blob)\n\n        reader.close()\n        self.assertTrue(reader.closed)\n\n        with self.assertRaises(ValueError):\n            reader.read()\n\n        with self.assertRaises(ValueError):\n            reader.seek(0)\n\n    def test_context_mgr(self):\n        # Just very that the context manager form doesn't crash.\n        blob = mock.Mock()\n        with self._make_blob_reader(blob) as reader:\n            reader.close()\n\n    def test_rejects_invalid_kwargs(self):\n        blob = mock.Mock()\n        with self.assertRaises(ValueError):\n            self._make_blob_reader(blob, invalid_kwarg=1)\n\n\nclass TestBlobWriterBinary(unittest.TestCase, _BlobWriterBase):\n    def test_attributes(self):\n        blob = mock.Mock()\n        blob.chunk_size = 256 * 1024\n        writer = self._make_blob_writer(blob)\n        self.assertFalse(writer.seekable())\n        self.assertFalse(writer.readable())\n        self.assertTrue(writer.writable())\n        self.assertEqual(writer._chunk_size, 256 * 1024)\n\n    def test_attributes_explicit(self):\n        blob = mock.Mock()\n        blob.chunk_size = 256 * 1024\n        writer = self._make_blob_writer(\n            blob, chunk_size=512 * 1024, retry=DEFAULT_RETRY\n        )\n        self.assertEqual(writer._chunk_size, 512 * 1024)\n        self.assertEqual(writer._retry, DEFAULT_RETRY)\n\n    def test_deprecated_text_mode_attribute(self):\n        blob = mock.Mock()\n        blob.chunk_size = 256 * 1024\n        writer = self._make_blob_writer(blob, text_mode=True)\n        self.assertTrue(writer._ignore_flush)\n        writer.flush()  # This should do nothing and not raise an error.\n\n    def test_reject_wrong_chunk_size(self):\n        blob = mock.Mock()\n        blob.chunk_size = 123\n        with self.assertRaises(ValueError):\n            _ = self._make_blob_writer(blob)\n\n    @mock.patch(\"warnings.warn\")\n    def test_write(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        blob = mock.Mock()\n        upload = mock.Mock()\n        transport = mock.Mock()\n        timeout = 600\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer with (arbitrary) arguments so we can validate the\n            # arguments are used.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            upload_kwargs = {\n                \"if_metageneration_match\": 1,\n                \"timeout\": timeout,\n            }\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                num_retries=NUM_RETRIES,\n                content_type=PLAIN_CONTENT_TYPE,\n                **upload_kwargs\n            )\n\n        # The transmit_next_chunk method must actually consume bytes from the\n        # sliding buffer for the flush() feature to work properly.\n        upload.transmit_next_chunk.side_effect = lambda _, timeout: writer._buffer.read(\n            chunk_size\n        )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_BINARY_DATA[0:4])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write over chunk_size. This should result in upload initialization\n        # and multiple chunks uploaded.\n        writer.write(TEST_BINARY_DATA[4:32])\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,\n            NUM_RETRIES,\n            chunk_size=chunk_size,\n            retry=None,\n            **upload_kwargs\n        )\n        upload.transmit_next_chunk.assert_called_with(transport, timeout=timeout)\n        self.assertEqual(upload.transmit_next_chunk.call_count, 4)\n\n        # Write another byte, finalize and close.\n        writer.write(TEST_BINARY_DATA[32:33])\n        self.assertEqual(writer.tell(), 33)\n        writer.close()\n        self.assertEqual(upload.transmit_next_chunk.call_count, 5)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def test_close_errors(self):\n        blob = mock.Mock(chunk_size=None)\n\n        upload = mock.Mock()\n        transport = mock.Mock()\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        writer = self._make_blob_writer(blob)\n\n        writer.close()\n        # Close a second time to verify it successfully does nothing.\n        writer.close()\n\n        self.assertTrue(writer.closed)\n        # Try to write to closed file.\n        with self.assertRaises(ValueError):\n            writer.write(TEST_BINARY_DATA)\n\n    def test_flush_fails(self):\n        blob = mock.Mock(chunk_size=None)\n        writer = self._make_blob_writer(blob)\n\n        with self.assertRaises(io.UnsupportedOperation):\n            writer.flush()\n\n    def test_seek_fails(self):\n        blob = mock.Mock(chunk_size=None)\n        writer = self._make_blob_writer(blob)\n\n        with self.assertRaises(io.UnsupportedOperation):\n            writer.seek(0)\n\n    def test_conditional_retry_failure(self):\n        blob = mock.Mock()\n\n        upload = mock.Mock()\n        transport = mock.Mock()\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                content_type=PLAIN_CONTENT_TYPE,\n            )\n\n        # The transmit_next_chunk method must actually consume bytes from the\n        # sliding buffer for the flush() feature to work properly.\n        upload.transmit_next_chunk.side_effect = lambda _: writer._buffer.read(\n            chunk_size\n        )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_BINARY_DATA[0:4])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write over chunk_size. This should result in upload initialization\n        # and multiple chunks uploaded.\n        # Due to the condition not being fulfilled, retry should be None.\n        writer.write(TEST_BINARY_DATA[4:32])\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,  # size\n            None,  # num_retries\n            chunk_size=chunk_size,\n            retry=None,\n        )\n        upload.transmit_next_chunk.assert_called_with(transport)\n        self.assertEqual(upload.transmit_next_chunk.call_count, 4)\n\n        # Write another byte, finalize and close.\n        writer.write(TEST_BINARY_DATA[32:33])\n        writer.close()\n        self.assertEqual(upload.transmit_next_chunk.call_count, 5)\n\n    def test_conditional_retry_pass(self):\n        blob = mock.Mock()\n\n        upload = mock.Mock()\n        transport = mock.Mock()\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                content_type=PLAIN_CONTENT_TYPE,\n                if_generation_match=123456,\n            )\n\n        # The transmit_next_chunk method must actually consume bytes from the\n        # sliding buffer for the flush() feature to work properly.\n        upload.transmit_next_chunk.side_effect = lambda _: writer._buffer.read(\n            chunk_size\n        )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_BINARY_DATA[0:4])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write over chunk_size. This should result in upload initialization\n        # and multiple chunks uploaded.\n        # Due to the condition being fulfilled, retry should be DEFAULT_RETRY.\n        writer.write(TEST_BINARY_DATA[4:32])\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,  # size\n            None,  # num_retries\n            chunk_size=chunk_size,\n            retry=DEFAULT_RETRY,\n            if_generation_match=123456,\n        )\n        upload.transmit_next_chunk.assert_called_with(transport)\n        self.assertEqual(upload.transmit_next_chunk.call_count, 4)\n\n        # Write another byte, finalize and close.\n        writer.write(TEST_BINARY_DATA[32:33])\n        writer.close()\n        self.assertEqual(upload.transmit_next_chunk.call_count, 5)\n\n    def test_forced_default_retry(self):\n        blob = mock.Mock()\n\n        upload = mock.Mock()\n        transport = mock.Mock()\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                content_type=PLAIN_CONTENT_TYPE,\n                retry=DEFAULT_RETRY,\n            )\n\n        # The transmit_next_chunk method must actually consume bytes from the\n        # sliding buffer for the flush() feature to work properly.\n        upload.transmit_next_chunk.side_effect = lambda _: writer._buffer.read(\n            chunk_size\n        )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_BINARY_DATA[0:4])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write over chunk_size. This should result in upload initialization\n        # and multiple chunks uploaded.\n        writer.write(TEST_BINARY_DATA[4:32])\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,  # size\n            None,  # num_retries\n            chunk_size=chunk_size,\n            retry=DEFAULT_RETRY,\n        )\n        upload.transmit_next_chunk.assert_called_with(transport)\n        self.assertEqual(upload.transmit_next_chunk.call_count, 4)\n\n        # Write another byte, finalize and close.\n        writer.write(TEST_BINARY_DATA[32:33])\n        writer.close()\n        self.assertEqual(upload.transmit_next_chunk.call_count, 5)\n\n    @mock.patch(\"warnings.warn\")\n    def test_num_retries_and_retry_conflict(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        blob = mock.Mock()\n\n        blob._initiate_resumable_upload.side_effect = ValueError\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                content_type=PLAIN_CONTENT_TYPE,\n                num_retries=2,\n                retry=DEFAULT_RETRY,\n            )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_BINARY_DATA[0:4])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write over chunk_size. The mock will raise a ValueError, simulating\n        # actual behavior when num_retries and retry are both specified.\n        with self.assertRaises(ValueError):\n            writer.write(TEST_BINARY_DATA[4:32])\n\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,  # size\n            2,  # num_retries\n            chunk_size=chunk_size,\n            retry=DEFAULT_RETRY,\n        )\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    @mock.patch(\"warnings.warn\")\n    def test_num_retries_only(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        blob = mock.Mock()\n        upload = mock.Mock()\n        transport = mock.Mock()\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                content_type=PLAIN_CONTENT_TYPE,\n                num_retries=2,\n            )\n\n        # The transmit_next_chunk method must actually consume bytes from the\n        # sliding buffer for the flush() feature to work properly.\n        upload.transmit_next_chunk.side_effect = lambda _: writer._buffer.read(\n            chunk_size\n        )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_BINARY_DATA[0:4])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write over chunk_size. This should result in upload initialization\n        # and multiple chunks uploaded.\n        writer.write(TEST_BINARY_DATA[4:32])\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,  # size\n            2,  # num_retries\n            chunk_size=chunk_size,\n            retry=None,\n        )\n        upload.transmit_next_chunk.assert_called_with(transport)\n        self.assertEqual(upload.transmit_next_chunk.call_count, 4)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE, DeprecationWarning, stacklevel=2\n        )\n\n        # Write another byte, finalize and close.\n        writer.write(TEST_BINARY_DATA[32:33])\n        writer.close()\n        self.assertEqual(upload.transmit_next_chunk.call_count, 5)\n\n    def test_rejects_invalid_kwargs(self):\n        blob = mock.Mock()\n        with self.assertRaises(ValueError):\n            self._make_blob_writer(blob, invalid_kwarg=1)\n\n\nclass Test_SlidingBuffer(unittest.TestCase):\n    @staticmethod\n    def _make_sliding_buffer(*args, **kwargs):\n        from google.cloud.storage.fileio import SlidingBuffer\n\n        return SlidingBuffer(*args, **kwargs)\n\n    def test_write_and_read(self):\n        buff = self._make_sliding_buffer()\n\n        # Write and verify tell() still reports 0 and len is correct.\n        buff.write(TEST_BINARY_DATA)\n        self.assertEqual(buff.tell(), 0)\n        self.assertEqual(len(buff), len(TEST_BINARY_DATA))\n\n        # Read and verify tell() reports end.\n        self.assertEqual(buff.read(), TEST_BINARY_DATA)\n        self.assertEqual(buff.tell(), len(TEST_BINARY_DATA))\n        self.assertEqual(len(buff), len(TEST_BINARY_DATA))\n\n    def test_flush(self):\n        buff = self._make_sliding_buffer()\n\n        # Write and verify tell() still reports 0 and len is correct.\n        buff.write(TEST_BINARY_DATA)\n        self.assertEqual(buff.tell(), 0)\n        self.assertEqual(len(buff), len(TEST_BINARY_DATA))\n\n        # Read 8 bytes and verify tell reports correctly.\n        self.assertEqual(buff.read(8), TEST_BINARY_DATA[:8])\n        self.assertEqual(buff.tell(), 8)\n        self.assertEqual(len(buff), len(TEST_BINARY_DATA))\n\n        # Flush buffer and verify tell doesn't change but len does.\n        buff.flush()\n        self.assertEqual(buff.tell(), 8)\n        self.assertEqual(len(buff), len(TEST_BINARY_DATA) - 8)\n\n        # Read remainder.\n        self.assertEqual(buff.read(), TEST_BINARY_DATA[8:])\n        self.assertEqual(buff.tell(), len(TEST_BINARY_DATA))\n        self.assertEqual(len(buff), len(TEST_BINARY_DATA[8:]))\n\n    def test_seek(self):\n        buff = self._make_sliding_buffer()\n        buff.write(TEST_BINARY_DATA)\n\n        # Try to seek forward. Verify the tell() doesn't change.\n        with self.assertRaises(ValueError):\n            pos = buff.tell()\n            buff.seek(len(TEST_BINARY_DATA) + 1)\n        self.assertEqual(pos, buff.tell())\n\n        # Read 8 bytes, test seek backwards, read again, and flush.\n        self.assertEqual(buff.read(8), TEST_BINARY_DATA[:8])\n        buff.seek(0)\n        self.assertEqual(buff.read(8), TEST_BINARY_DATA[:8])\n        buff.flush()\n        self.assertEqual(buff.tell(), 8)\n\n        # Try to seek to a byte that has already been flushed.\n        with self.assertRaises(ValueError):\n            pos = buff.tell()\n            buff.seek(0)\n        self.assertEqual(pos, buff.tell())\n\n    def test_close(self):\n        buff = self._make_sliding_buffer()\n        buff.close()\n        self.assertTrue(buff.closed)\n        with self.assertRaises(ValueError):\n            buff.read()\n\n\nclass TestBlobReaderText(unittest.TestCase, _BlobReaderBase):\n    def test_attributes(self):\n        blob = mock.Mock()\n        reader = io.TextIOWrapper(self._make_blob_reader(blob))\n        self.assertTrue(reader.seekable())\n        self.assertTrue(reader.readable())\n        self.assertFalse(reader.writable())\n\n    def test_read(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_TEXT_DATA.encode(\"utf-8\")[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        blob.chunk_size = None\n        blob.size = len(TEST_TEXT_DATA.encode(\"utf-8\"))\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = io.TextIOWrapper(self._make_blob_reader(blob, **download_kwargs))\n\n        # The TextIOWrapper class has an internally defined chunk size which\n        # will override ours. The wrapper class is not under test.\n        # Read and trigger the first download of chunk_size.\n        self.assertEqual(reader.read(1), TEST_TEXT_DATA[0:1])\n        blob.download_as_bytes.assert_called_once()\n\n        # Read from buffered data only.\n        self.assertEqual(reader.read(3), TEST_TEXT_DATA[1:4])\n        blob.download_as_bytes.assert_called_once()\n\n        # Read all remaining data.\n        self.assertEqual(reader.read(), TEST_TEXT_DATA[4:])\n\n        # Seek to 0 and read all remaining data again.\n        reader.seek(0)\n        self.assertEqual(reader.read(), TEST_TEXT_DATA)\n\n        reader.close()\n\n    def test_multibyte_read(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_MULTIBYTE_TEXT_DATA.encode(\"utf-8\")[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        blob.chunk_size = None\n        blob.size = len(TEST_MULTIBYTE_TEXT_DATA.encode(\"utf-8\"))\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = io.TextIOWrapper(self._make_blob_reader(blob, **download_kwargs))\n\n        # The TextIOWrapper class has an internally defined chunk size which\n        # will override ours. The wrapper class is not under test.\n        # Read and trigger the first download of chunk_size.\n        self.assertEqual(reader.read(1), TEST_MULTIBYTE_TEXT_DATA[0:1])\n        blob.download_as_bytes.assert_called_once()\n\n        # Read from buffered data only.\n        self.assertEqual(reader.read(3), TEST_MULTIBYTE_TEXT_DATA[1:4])\n        blob.download_as_bytes.assert_called_once()\n\n        # Read all remaining data.\n        self.assertEqual(reader.read(), TEST_MULTIBYTE_TEXT_DATA[4:])\n\n        # Seek to 0 and read all remaining data again.\n        reader.seek(0)\n        self.assertEqual(reader.read(), TEST_MULTIBYTE_TEXT_DATA)\n\n        reader.close()\n\n    def test_seek(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_TEXT_DATA.encode(\"utf-8\")[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        blob.size = None\n        blob.chunk_size = None\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = io.TextIOWrapper(self._make_blob_reader(blob, **download_kwargs))\n\n        # Seek needs the blob size to work and should call reload() if the size\n        # is not known. Set a mock to initialize the size if reload() is called.\n        def initialize_size(**_):\n            blob.size = len(TEST_TEXT_DATA.encode(\"utf-8\"))\n\n        blob.reload = mock.Mock(side_effect=initialize_size)\n\n        # Seek, forcing a blob reload in order to validate the seek doesn't\n        # exceed the end of the blob.\n        self.assertEqual(reader.seek(4), 4)\n        blob.reload.assert_called_once_with(**download_kwargs)\n        self.assertEqual(reader.read(4), TEST_TEXT_DATA[4:8])\n        self.assertEqual(blob.download_as_bytes.call_count, 1)\n\n        # Seek to beginning. The next read will need to download data again.\n        self.assertEqual(reader.seek(0), 0)\n        self.assertEqual(reader.read(), TEST_TEXT_DATA)\n        self.assertEqual(blob.download_as_bytes.call_count, 2)\n\n        reader.close()\n\n    def test_multibyte_seek(self):\n        blob = mock.Mock()\n\n        def read_from_fake_data(start=0, end=None, **_):\n            return TEST_MULTIBYTE_TEXT_DATA.encode(\"utf-8\")[start:end]\n\n        blob.download_as_bytes = mock.Mock(side_effect=read_from_fake_data)\n        blob.size = None\n        blob.chunk_size = None\n        download_kwargs = {\"if_metageneration_match\": 1}\n        reader = io.TextIOWrapper(self._make_blob_reader(blob, **download_kwargs))\n\n        # Seek needs the blob size to work and should call reload() if the size\n        # is not known. Set a mock to initialize the size if reload() is called.\n        def initialize_size(**_):\n            blob.size = len(TEST_MULTIBYTE_TEXT_DATA.encode(\"utf-8\"))\n\n        blob.reload = mock.Mock(side_effect=initialize_size)\n\n        # Seek, forcing a blob reload in order to validate the seek doesn't\n        # exceed the end of the blob.\n        self.assertEqual(reader.seek(4), 4)\n        blob.reload.assert_called_once_with(**download_kwargs)\n\n        # Seek to beginning.\n        self.assertEqual(reader.seek(0), 0)\n        self.assertEqual(reader.read(), TEST_MULTIBYTE_TEXT_DATA)\n        self.assertEqual(blob.download_as_bytes.call_count, 1)\n\n        # tell() is an inherited method that uses seek().\n        self.assertEqual(reader.tell(), len(TEST_MULTIBYTE_TEXT_DATA.encode(\"utf-8\")))\n\n        reader.close()\n\n    def test_close(self):\n        blob = mock.Mock()\n        reader = self._make_blob_reader(blob)\n\n        reader.close()\n        self.assertTrue(reader.closed)\n\n        with self.assertRaises(ValueError):\n            reader.read()\n\n        with self.assertRaises(ValueError):\n            reader.seek(0)\n\n\nclass TestBlobWriterText(unittest.TestCase, _BlobWriterBase):\n    @mock.patch(\"warnings.warn\")\n    def test_write(self, mock_warn):\n        from google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\n\n        blob = mock.Mock()\n        upload = mock.Mock()\n        transport = mock.Mock()\n\n        blob._initiate_resumable_upload.return_value = (upload, transport)\n\n        with mock.patch(\"google.cloud.storage.fileio.CHUNK_SIZE_MULTIPLE\", 1):\n            # Create a writer in text mode.\n            # It would be normal to use a context manager here, but not doing so\n            # gives us more control over close() for test purposes.\n            chunk_size = 8  # Note: Real upload requires a multiple of 256KiB.\n            unwrapped_writer = self._make_blob_writer(\n                blob,\n                chunk_size=chunk_size,\n                ignore_flush=True,\n                num_retries=NUM_RETRIES,\n                content_type=PLAIN_CONTENT_TYPE,\n            )\n\n        writer = io.TextIOWrapper(unwrapped_writer)\n\n        # The transmit_next_chunk method must actually consume bytes from the\n        # sliding buffer for the flush() feature to work properly.\n        upload.transmit_next_chunk.side_effect = (\n            lambda _: unwrapped_writer._buffer.read(chunk_size)\n        )\n\n        # Write under chunk_size. This should be buffered and the upload not\n        # initiated.\n        writer.write(TEST_MULTIBYTE_TEXT_DATA[0:2])\n        blob.initiate_resumable_upload.assert_not_called()\n\n        # Write all data and close.\n        writer.write(TEST_MULTIBYTE_TEXT_DATA[2:])\n        writer.close()\n\n        blob._initiate_resumable_upload.assert_called_once_with(\n            blob.bucket.client,\n            unwrapped_writer._buffer,\n            PLAIN_CONTENT_TYPE,\n            None,\n            NUM_RETRIES,\n            chunk_size=chunk_size,\n            retry=None,\n        )\n        upload.transmit_next_chunk.assert_called_with(transport)\n\n        mock_warn.assert_called_once_with(\n            _NUM_RETRIES_MESSAGE,\n            DeprecationWarning,\n            stacklevel=2,\n        )\n", "tests/unit/test_acl.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport mock\n\nfrom google.cloud.storage.retry import (\n    DEFAULT_RETRY,\n    DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n)\n\n\nclass Test_ACLEntity(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.acl import _ACLEntity\n\n        return _ACLEntity\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor_default_identifier(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        self.assertEqual(entity.type, TYPE)\n        self.assertIsNone(entity.identifier)\n        self.assertEqual(entity.get_roles(), set())\n\n    def test_ctor_w_identifier(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        entity = self._make_one(TYPE, ID)\n        self.assertEqual(entity.type, TYPE)\n        self.assertEqual(entity.identifier, ID)\n        self.assertEqual(entity.get_roles(), set())\n\n    def test___str__no_identifier(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        self.assertEqual(str(entity), TYPE)\n\n    def test___str__w_identifier(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        entity = self._make_one(TYPE, ID)\n        self.assertEqual(str(entity), f\"{TYPE}-{ID}\")\n\n    def test_grant_simple(self):\n        TYPE = \"type\"\n        ROLE = \"role\"\n        entity = self._make_one(TYPE)\n        entity.grant(ROLE)\n        self.assertEqual(entity.get_roles(), set([ROLE]))\n\n    def test_grant_duplicate(self):\n        TYPE = \"type\"\n        ROLE1 = \"role1\"\n        ROLE2 = \"role2\"\n        entity = self._make_one(TYPE)\n        entity.grant(ROLE1)\n        entity.grant(ROLE2)\n        entity.grant(ROLE1)\n        self.assertEqual(entity.get_roles(), set([ROLE1, ROLE2]))\n\n    def test_revoke_miss(self):\n        TYPE = \"type\"\n        ROLE = \"nonesuch\"\n        entity = self._make_one(TYPE)\n        entity.revoke(ROLE)\n        self.assertEqual(entity.get_roles(), set())\n\n    def test_revoke_hit(self):\n        TYPE = \"type\"\n        ROLE1 = \"role1\"\n        ROLE2 = \"role2\"\n        entity = self._make_one(TYPE)\n        entity.grant(ROLE1)\n        entity.grant(ROLE2)\n        entity.revoke(ROLE1)\n        self.assertEqual(entity.get_roles(), set([ROLE2]))\n\n    def test_grant_read(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        entity.grant_read()\n        self.assertEqual(entity.get_roles(), set([entity.READER_ROLE]))\n\n    def test_grant_write(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        entity.grant_write()\n        self.assertEqual(entity.get_roles(), set([entity.WRITER_ROLE]))\n\n    def test_grant_owner(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        entity.grant_owner()\n        self.assertEqual(entity.get_roles(), set([entity.OWNER_ROLE]))\n\n    def test_revoke_read(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        entity.grant(entity.READER_ROLE)\n        entity.revoke_read()\n        self.assertEqual(entity.get_roles(), set())\n\n    def test_revoke_write(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        entity.grant(entity.WRITER_ROLE)\n        entity.revoke_write()\n        self.assertEqual(entity.get_roles(), set())\n\n    def test_revoke_owner(self):\n        TYPE = \"type\"\n        entity = self._make_one(TYPE)\n        entity.grant(entity.OWNER_ROLE)\n        entity.revoke_owner()\n        self.assertEqual(entity.get_roles(), set())\n\n\nclass FakeReload(object):\n    \"\"\"A callable used for faking the reload() method of an ACL instance.\"\"\"\n\n    def __init__(self, acl):\n        self.acl = acl\n        self.timeouts_used = []\n\n    def __call__(self, timeout=None):\n        self.acl.loaded = True\n        self.timeouts_used.append(timeout)\n\n\nclass Test_ACL(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.acl import ACL\n\n        return ACL\n\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_validate_predefined(self):\n        ACL = self._get_target_class()\n        self.assertIsNone(ACL.validate_predefined(None))\n        self.assertEqual(ACL.validate_predefined(\"public-read\"), \"publicRead\")\n        self.assertEqual(ACL.validate_predefined(\"publicRead\"), \"publicRead\")\n        with self.assertRaises(ValueError):\n            ACL.validate_predefined(\"publicread\")\n\n    def test_ctor(self):\n        acl = self._make_one()\n        self.assertEqual(acl.entities, {})\n        self.assertFalse(acl.loaded)\n\n    def test__ensure_loaded(self):\n        acl = self._make_one()\n        _reload = FakeReload(acl)\n        acl.reload = _reload\n        acl.loaded = False\n\n        acl._ensure_loaded(timeout=42)\n\n        self.assertTrue(acl.loaded)\n        self.assertEqual(_reload.timeouts_used[0], 42)\n\n    def test_client_is_abstract(self):\n        acl = self._make_one()\n        self.assertRaises(NotImplementedError, lambda: acl.client)\n\n    def test_reset(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        acl.entity(TYPE, ID)\n        acl.reset()\n        self.assertEqual(acl.entities, {})\n        self.assertFalse(acl.loaded)\n\n    def test___iter___empty_eager(self):\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertEqual(list(acl), [])\n\n    def test___iter___empty_lazy(self):\n        acl = self._make_one()\n        _reload = FakeReload(acl)\n        acl.loaded = False\n\n        acl.reload = _reload\n        self.assertEqual(list(acl), [])\n        self.assertTrue(acl.loaded)\n        self.assertEqual(_reload.timeouts_used[0], self._get_default_timeout())\n\n    def test___iter___non_empty_no_roles(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        acl.entity(TYPE, ID)\n        self.assertEqual(list(acl), [])\n\n    def test___iter___non_empty_w_roles(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity(TYPE, ID)\n        entity.grant(ROLE)\n        self.assertEqual(list(acl), [{\"entity\": f\"{TYPE}-{ID}\", \"role\": ROLE}])\n\n    def test___iter___non_empty_w_empty_role(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity(TYPE, ID)\n        entity.grant(\"\")\n        self.assertEqual(list(acl), [])\n\n    def test_entity_from_dict_allUsers_eager(self):\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity_from_dict({\"entity\": \"allUsers\", \"role\": ROLE})\n        self.assertEqual(entity.type, \"allUsers\")\n        self.assertIsNone(entity.identifier)\n        self.assertEqual(entity.get_roles(), set([ROLE]))\n        self.assertEqual(list(acl), [{\"entity\": \"allUsers\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_entity_from_dict_allAuthenticatedUsers(self):\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity_from_dict({\"entity\": \"allAuthenticatedUsers\", \"role\": ROLE})\n        self.assertEqual(entity.type, \"allAuthenticatedUsers\")\n        self.assertIsNone(entity.identifier)\n        self.assertEqual(entity.get_roles(), set([ROLE]))\n        self.assertEqual(list(acl), [{\"entity\": \"allAuthenticatedUsers\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_entity_from_dict_string_w_hyphen(self):\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity_from_dict({\"entity\": \"type-id\", \"role\": ROLE})\n        self.assertEqual(entity.type, \"type\")\n        self.assertEqual(entity.identifier, \"id\")\n        self.assertEqual(entity.get_roles(), set([ROLE]))\n        self.assertEqual(list(acl), [{\"entity\": \"type-id\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_entity_from_dict_string_wo_hyphen(self):\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertRaises(\n            ValueError, acl.entity_from_dict, {\"entity\": \"bogus\", \"role\": ROLE}\n        )\n        self.assertEqual(list(acl.get_entities()), [])\n\n    def test_has_entity_miss_str_eager(self):\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertFalse(acl.has_entity(\"nonesuch\"))\n\n    def test_has_entity_miss_str_lazy(self):\n        acl = self._make_one()\n        _reload = FakeReload(acl)\n        acl.reload = _reload\n        acl.loaded = False\n\n        self.assertFalse(acl.has_entity(\"nonesuch\"))\n        self.assertTrue(acl.loaded)\n        self.assertEqual(_reload.timeouts_used[0], self._get_default_timeout())\n\n    def test_has_entity_miss_entity(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        TYPE = \"type\"\n        ID = \"id\"\n        entity = _ACLEntity(TYPE, ID)\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertFalse(acl.has_entity(entity))\n\n    def test_has_entity_hit_str(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        acl.entity(TYPE, ID)\n        self.assertTrue(acl.has_entity(f\"{TYPE}-{ID}\"))\n\n    def test_has_entity_hit_entity(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity(TYPE, ID)\n        self.assertTrue(acl.has_entity(entity))\n\n    def test_get_entity_miss_str_no_default_eager(self):\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertIsNone(acl.get_entity(\"nonesuch\"))\n\n    def test_get_entity_miss_str_no_default_lazy(self):\n        acl = self._make_one()\n        _reload = FakeReload(acl)\n        acl.reload = _reload\n        acl.loaded = False\n\n        self.assertIsNone(acl.get_entity(\"nonesuch\"))\n        self.assertTrue(acl.loaded)\n        self.assertEqual(_reload.timeouts_used[0], self._get_default_timeout())\n\n    def test_get_entity_miss_entity_no_default(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        TYPE = \"type\"\n        ID = \"id\"\n        entity = _ACLEntity(TYPE, ID)\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertIsNone(acl.get_entity(entity))\n\n    def test_get_entity_miss_str_w_default(self):\n        DEFAULT = object()\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertIs(acl.get_entity(\"nonesuch\", DEFAULT), DEFAULT)\n\n    def test_get_entity_miss_entity_w_default(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        DEFAULT = object()\n        TYPE = \"type\"\n        ID = \"id\"\n        entity = _ACLEntity(TYPE, ID)\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertIs(acl.get_entity(entity, DEFAULT), DEFAULT)\n\n    def test_get_entity_hit_str(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        acl.entity(TYPE, ID)\n        self.assertTrue(acl.has_entity(f\"{TYPE}-{ID}\"))\n\n    def test_get_entity_hit_entity(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity(TYPE, ID)\n        self.assertTrue(acl.has_entity(entity))\n\n    def test_add_entity_miss_eager(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        TYPE = \"type\"\n        ID = \"id\"\n        ROLE = \"role\"\n        entity = _ACLEntity(TYPE, ID)\n        entity.grant(ROLE)\n        acl = self._make_one()\n        acl.loaded = True\n        acl.add_entity(entity)\n        self.assertTrue(acl.loaded)\n        self.assertEqual(list(acl), [{\"entity\": \"type-id\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_add_entity_miss_lazy(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        TYPE = \"type\"\n        ID = \"id\"\n        ROLE = \"role\"\n        entity = _ACLEntity(TYPE, ID)\n        entity.grant(ROLE)\n        acl = self._make_one()\n\n        _reload = FakeReload(acl)\n        acl.reload = _reload\n        acl.loaded = False\n\n        acl.add_entity(entity)\n        self.assertTrue(acl.loaded)\n        self.assertEqual(list(acl), [{\"entity\": \"type-id\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n        self.assertTrue(acl.loaded)\n        self.assertEqual(_reload.timeouts_used[0], self._get_default_timeout())\n\n    def test_add_entity_hit(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        TYPE = \"type\"\n        ID = \"id\"\n        ENTITY_VAL = f\"{TYPE}-{ID}\"\n        ROLE = \"role\"\n        entity = _ACLEntity(TYPE, ID)\n        entity.grant(ROLE)\n        acl = self._make_one()\n        acl.loaded = True\n        before = acl.entity(TYPE, ID)\n        acl.add_entity(entity)\n        self.assertTrue(acl.loaded)\n        self.assertIsNot(acl.get_entity(ENTITY_VAL), before)\n        self.assertIs(acl.get_entity(ENTITY_VAL), entity)\n        self.assertEqual(list(acl), [{\"entity\": \"type-id\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_entity_miss(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity(TYPE, ID)\n        self.assertTrue(acl.loaded)\n        entity.grant(ROLE)\n        self.assertEqual(list(acl), [{\"entity\": \"type-id\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_entity_hit(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        before = acl.entity(TYPE, ID)\n        before.grant(ROLE)\n        entity = acl.entity(TYPE, ID)\n        self.assertIs(entity, before)\n        self.assertEqual(list(acl), [{\"entity\": \"type-id\", \"role\": ROLE}])\n        self.assertEqual(list(acl.get_entities()), [entity])\n\n    def test_user(self):\n        ID = \"id\"\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.user(ID)\n        entity.grant(ROLE)\n        self.assertEqual(entity.type, \"user\")\n        self.assertEqual(entity.identifier, ID)\n        self.assertEqual(list(acl), [{\"entity\": f\"user-{ID}\", \"role\": ROLE}])\n\n    def test_group(self):\n        ID = \"id\"\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.group(ID)\n        entity.grant(ROLE)\n        self.assertEqual(entity.type, \"group\")\n        self.assertEqual(entity.identifier, ID)\n        self.assertEqual(list(acl), [{\"entity\": f\"group-{ID}\", \"role\": ROLE}])\n\n    def test_domain(self):\n        ID = \"id\"\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.domain(ID)\n        entity.grant(ROLE)\n        self.assertEqual(entity.type, \"domain\")\n        self.assertEqual(entity.identifier, ID)\n        self.assertEqual(list(acl), [{\"entity\": f\"domain-{ID}\", \"role\": ROLE}])\n\n    def test_all(self):\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.all()\n        entity.grant(ROLE)\n        self.assertEqual(entity.type, \"allUsers\")\n        self.assertIsNone(entity.identifier)\n        self.assertEqual(list(acl), [{\"entity\": \"allUsers\", \"role\": ROLE}])\n\n    def test_all_authenticated(self):\n        ROLE = \"role\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.all_authenticated()\n        entity.grant(ROLE)\n        self.assertEqual(entity.type, \"allAuthenticatedUsers\")\n        self.assertIsNone(entity.identifier)\n        self.assertEqual(list(acl), [{\"entity\": \"allAuthenticatedUsers\", \"role\": ROLE}])\n\n    def test_get_entities_empty_eager(self):\n        acl = self._make_one()\n        acl.loaded = True\n        self.assertEqual(acl.get_entities(), [])\n\n    def test_get_entities_empty_lazy(self):\n        acl = self._make_one()\n        _reload = FakeReload(acl)\n        acl.reload = _reload\n        acl.loaded = False\n\n        self.assertEqual(acl.get_entities(), [])\n        self.assertTrue(acl.loaded)\n        self.assertEqual(_reload.timeouts_used[0], self._get_default_timeout())\n\n    def test_get_entities_nonempty(self):\n        TYPE = \"type\"\n        ID = \"id\"\n        acl = self._make_one()\n        acl.loaded = True\n        entity = acl.entity(TYPE, ID)\n        self.assertEqual(acl.get_entities(), [entity])\n\n    def test_reload_missing_w_defaults(self):\n        # https://github.com/GoogleCloudPlatform/google-cloud-python/issues/652\n        class Derived(self._get_target_class()):\n            client = None\n\n        role = \"role\"\n        reload_path = \"/testing/acl\"\n        api_response = {}\n        acl = Derived()\n        acl.reload_path = reload_path\n        acl.loaded = True\n        acl.entity(\"allUsers\", role)\n        client = acl.client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n\n        acl.reload()\n\n        self.assertEqual(list(acl), [])\n\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            reload_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_reload_w_empty_result_w_timeout_w_retry_w_explicit_client(self):\n        role = \"role\"\n        reload_path = \"/testing/acl\"\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        api_response = {\"items\": []}\n        acl = self._make_one()\n        acl.reload_path = reload_path\n        acl.loaded = True\n        acl.entity(\"allUsers\", role)\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n\n        acl.reload(client=client, timeout=timeout, retry=retry)\n\n        self.assertTrue(acl.loaded)\n        self.assertEqual(list(acl), [])\n\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            reload_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_reload_w_nonempty_result_w_user_project(self):\n        role = \"role\"\n        reload_path = \"/testing/acl\"\n        user_project = \"user-project-123\"\n        api_response = {\"items\": [{\"entity\": \"allUsers\", \"role\": role}]}\n        acl = self._make_one()\n        acl.reload_path = reload_path\n        acl.loaded = True\n        acl.user_project = user_project\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n\n        acl.reload(client=client)\n\n        self.assertTrue(acl.loaded)\n        self.assertEqual(list(acl), [{\"entity\": \"allUsers\", \"role\": role}])\n\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            reload_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_save_none_set_none_passed(self):\n        save_path = \"/testing\"\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        acl = self._make_one()\n        acl.save_path = save_path\n\n        acl.save(client=client)\n\n        client._patch_resource.assert_not_called()\n\n    def test_save_w_empty_response_w_defaults(self):\n        class Derived(self._get_target_class()):\n            client = None\n\n        save_path = \"/testing\"\n        api_response = {}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = Derived()\n        acl.client = client\n        acl.save_path = save_path\n        acl.loaded = True\n\n        acl.save()\n\n        self.assertEqual(list(acl), [])\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_no_acl_w_timeout(self):\n        save_path = \"/testing\"\n        role = \"role\"\n        expected_acl = [{\"entity\": \"allUsers\", \"role\": role}]\n        api_response = {\"acl\": expected_acl}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n        acl.entity(\"allUsers\").grant(role)\n        timeout = 42\n\n        acl.save(client=client, timeout=timeout)\n\n        self.assertEqual(list(acl), expected_acl)\n\n        expected_data = api_response\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_w_acl_w_user_project(self):\n        save_path = \"/testing\"\n        user_project = \"user-project-123\"\n        role1 = \"role1\"\n        role2 = \"role2\"\n        sticky = {\"entity\": \"allUsers\", \"role\": role2}\n        new_acl = [{\"entity\": \"allUsers\", \"role\": role1}]\n        api_response = {\"acl\": [sticky] + new_acl}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n        acl.user_project = user_project\n\n        acl.save(new_acl, client=client)\n\n        entries = list(acl)\n        self.assertEqual(len(entries), 2)\n        self.assertTrue(sticky in entries)\n        self.assertTrue(new_acl[0] in entries)\n\n        expected_data = {\"acl\": new_acl}\n        expected_query_params = {\"projection\": \"full\", \"userProject\": user_project}\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_w_acl_w_preconditions(self):\n        save_path = \"/testing\"\n        role1 = \"role1\"\n        role2 = \"role2\"\n        sticky = {\"entity\": \"allUsers\", \"role\": role2}\n        new_acl = [{\"entity\": \"allUsers\", \"role\": role1}]\n        api_response = {\"acl\": [sticky] + new_acl}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n\n        acl.save(\n            new_acl,\n            client=client,\n            if_metageneration_match=2,\n            if_metageneration_not_match=1,\n        )\n\n        entries = list(acl)\n        self.assertEqual(len(entries), 2)\n        self.assertTrue(sticky in entries)\n        self.assertTrue(new_acl[0] in entries)\n\n        expected_data = {\"acl\": new_acl}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_prefefined_invalid(self):\n        save_path = \"/testing\"\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n\n        with self.assertRaises(ValueError):\n            acl.save_predefined(\"bogus\", client=client)\n\n        client._patch_resource.assert_not_called()\n\n    def test_save_predefined_w_defaults(self):\n        class Derived(self._get_target_class()):\n            client = None\n\n        save_path = \"/testing\"\n        predefined = \"private\"\n        api_response = {\"acl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = Derived()\n        acl.save_path = save_path\n        acl.loaded = True\n        acl.client = client\n\n        acl.save_predefined(predefined)\n\n        entries = list(acl)\n        self.assertEqual(len(entries), 0)\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"predefinedAcl\": predefined,\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_predefined_w_XML_alias_w_timeout(self):\n        save_path = \"/testing\"\n        predefined_xml = \"project-private\"\n        predefined_json = \"projectPrivate\"\n        api_response = {\"acl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n        timeout = 42\n\n        acl.save_predefined(predefined_xml, client=client, timeout=timeout)\n\n        entries = list(acl)\n        self.assertEqual(len(entries), 0)\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"predefinedAcl\": predefined_json,\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_predefined_w_alternate_query_param(self):\n        # Cover case where subclass overrides _PREDEFINED_QUERY_PARAM\n        save_path = \"/testing\"\n        predefined = \"publicRead\"\n        api_response = {\"acl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n        acl._PREDEFINED_QUERY_PARAM = \"alternate\"\n\n        acl.save_predefined(predefined, client=client)\n\n        entries = list(acl)\n        self.assertEqual(len(entries), 0)\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"alternate\": predefined,\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_save_predefined_w_preconditions(self):\n        save_path = \"/testing\"\n        predefined = \"private\"\n        api_response = {\"acl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n\n        acl.save_predefined(\n            predefined,\n            client=client,\n            if_metageneration_match=2,\n            if_metageneration_not_match=1,\n        )\n\n        entries = list(acl)\n        self.assertEqual(len(entries), 0)\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"predefinedAcl\": predefined,\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_clear_w_defaults(self):\n        class Derived(self._get_target_class()):\n            client = None\n\n        save_path = \"/testing\"\n        role1 = \"role1\"\n        role2 = \"role2\"\n        sticky = {\"entity\": \"allUsers\", \"role\": role2}\n        api_response = {\"acl\": [sticky]}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = Derived()\n        acl.client = client\n        acl.save_path = save_path\n        acl.loaded = True\n        acl.entity(\"allUsers\", role1)\n\n        acl.clear()\n\n        self.assertEqual(list(acl), [sticky])\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_clear_w_explicit_client_w_timeout(self):\n        save_path = \"/testing\"\n        role1 = \"role1\"\n        role2 = \"role2\"\n        sticky = {\"entity\": \"allUsers\", \"role\": role2}\n        api_response = {\"acl\": [sticky]}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n        acl.entity(\"allUsers\", role1)\n        timeout = 42\n\n        acl.clear(client=client, timeout=timeout)\n\n        self.assertEqual(list(acl), [sticky])\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_clear_w_explicit_client_w_preconditions(self):\n        save_path = \"/testing\"\n        role1 = \"role1\"\n        role2 = \"role2\"\n        sticky = {\"entity\": \"allUsers\", \"role\": role2}\n        api_response = {\"acl\": [sticky]}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        acl = self._make_one()\n        acl.save_path = save_path\n        acl.loaded = True\n        acl.entity(\"allUsers\", role1)\n\n        acl.clear(\n            client=client, if_metageneration_match=2, if_metageneration_not_match=1\n        )\n\n        self.assertEqual(list(acl), [sticky])\n\n        expected_data = {\"acl\": []}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            save_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n\nclass Test_BucketACL(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.acl import BucketACL\n\n        return BucketACL\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor(self):\n        NAME = \"name\"\n        bucket = _Bucket(NAME)\n        acl = self._make_one(bucket)\n        self.assertEqual(acl.entities, {})\n        self.assertFalse(acl.loaded)\n        self.assertIs(acl.bucket, bucket)\n        self.assertEqual(acl.reload_path, f\"/b/{NAME}/acl\")\n        self.assertEqual(acl.save_path, f\"/b/{NAME}\")\n\n    def test_user_project(self):\n        NAME = \"name\"\n        USER_PROJECT = \"user-project-123\"\n        bucket = _Bucket(NAME)\n        acl = self._make_one(bucket)\n        self.assertIsNone(acl.user_project)\n        bucket.user_project = USER_PROJECT\n        self.assertEqual(acl.user_project, USER_PROJECT)\n\n\nclass Test_DefaultObjectACL(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.acl import DefaultObjectACL\n\n        return DefaultObjectACL\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor(self):\n        NAME = \"name\"\n        bucket = _Bucket(NAME)\n        acl = self._make_one(bucket)\n        self.assertEqual(acl.entities, {})\n        self.assertFalse(acl.loaded)\n        self.assertIs(acl.bucket, bucket)\n        self.assertEqual(acl.reload_path, f\"/b/{NAME}/defaultObjectAcl\")\n        self.assertEqual(acl.save_path, f\"/b/{NAME}\")\n\n\nclass Test_ObjectACL(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.acl import ObjectACL\n\n        return ObjectACL\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def test_ctor(self):\n        NAME = \"name\"\n        BLOB_NAME = \"blob-name\"\n        bucket = _Bucket(NAME)\n        blob = _Blob(bucket, BLOB_NAME)\n        acl = self._make_one(blob)\n        self.assertEqual(acl.entities, {})\n        self.assertFalse(acl.loaded)\n        self.assertIs(acl.blob, blob)\n        self.assertEqual(acl.reload_path, f\"/b/{NAME}/o/{BLOB_NAME}/acl\")\n        self.assertEqual(acl.save_path, f\"/b/{NAME}/o/{BLOB_NAME}\")\n\n    def test_user_project(self):\n        NAME = \"name\"\n        BLOB_NAME = \"blob-name\"\n        USER_PROJECT = \"user-project-123\"\n        bucket = _Bucket(NAME)\n        blob = _Blob(bucket, BLOB_NAME)\n        acl = self._make_one(blob)\n        self.assertIsNone(acl.user_project)\n        blob.user_project = USER_PROJECT\n        self.assertEqual(acl.user_project, USER_PROJECT)\n\n\nclass _Blob(object):\n    user_project = None\n\n    def __init__(self, bucket, blob):\n        self.bucket = bucket\n        self.blob = blob\n\n    @property\n    def path(self):\n        return f\"{self.bucket.path}/o/{self.blob}\"\n\n\nclass _Bucket(object):\n    user_project = None\n\n    def __init__(self, name):\n        self.name = name\n\n    @property\n    def path(self):\n        return f\"/b/{self.name}\"\n", "tests/unit/test_notification.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport mock\n\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\nclass TestBucketNotification(unittest.TestCase):\n    BUCKET_NAME = \"test-bucket\"\n    BUCKET_PROJECT = \"bucket-project-123\"\n    TOPIC_NAME = \"test-topic\"\n    TOPIC_ALT_PROJECT = \"topic-project-456\"\n    TOPIC_REF_FMT = \"//pubsub.googleapis.com/projects/{}/topics/{}\"\n    TOPIC_REF = TOPIC_REF_FMT.format(BUCKET_PROJECT, TOPIC_NAME)\n    TOPIC_ALT_REF = TOPIC_REF_FMT.format(TOPIC_ALT_PROJECT, TOPIC_NAME)\n    CUSTOM_ATTRIBUTES = {\"attr1\": \"value1\", \"attr2\": \"value2\"}\n    BLOB_NAME_PREFIX = \"blob-name-prefix/\"\n    NOTIFICATION_ID = \"123\"\n    SELF_LINK = \"https://example.com/notification/123\"\n    ETAG = \"DEADBEEF\"\n    CREATE_PATH = f\"/b/{BUCKET_NAME}/notificationConfigs\"\n    NOTIFICATION_PATH = f\"/b/{BUCKET_NAME}/notificationConfigs/{NOTIFICATION_ID}\"\n\n    @staticmethod\n    def event_types():\n        from google.cloud.storage.notification import (\n            OBJECT_FINALIZE_EVENT_TYPE,\n            OBJECT_DELETE_EVENT_TYPE,\n        )\n\n        return [OBJECT_FINALIZE_EVENT_TYPE, OBJECT_DELETE_EVENT_TYPE]\n\n    @staticmethod\n    def payload_format():\n        from google.cloud.storage.notification import JSON_API_V1_PAYLOAD_FORMAT\n\n        return JSON_API_V1_PAYLOAD_FORMAT\n\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.notification import BucketNotification\n\n        return BucketNotification\n\n    def _make_one(self, *args, **kw):\n        return self._get_target_class()(*args, **kw)\n\n    def _make_client(self, project=BUCKET_PROJECT):\n        from google.cloud.storage.client import Client\n\n        return mock.Mock(project=project, spec=Client)\n\n    def _make_bucket(self, client, name=BUCKET_NAME, user_project=None):\n        bucket = mock.Mock(spec=[\"client\", \"name\", \"user_project\"])\n        bucket.client = client\n        bucket.name = name\n        bucket.user_project = user_project\n        return bucket\n\n    def test_ctor_w_missing_project(self):\n        client = self._make_client(project=None)\n        bucket = self._make_bucket(client)\n\n        with self.assertRaises(ValueError):\n            self._make_one(bucket, self.TOPIC_NAME)\n\n    def test_ctor_defaults(self):\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        self.assertIs(notification.bucket, bucket)\n        self.assertEqual(notification.topic_name, self.TOPIC_NAME)\n        self.assertEqual(notification.topic_project, self.BUCKET_PROJECT)\n        self.assertIsNone(notification.custom_attributes)\n        self.assertIsNone(notification.event_types)\n        self.assertIsNone(notification.blob_name_prefix)\n        self.assertEqual(notification.payload_format, NONE_PAYLOAD_FORMAT)\n\n    def test_ctor_explicit(self):\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n\n        notification = self._make_one(\n            bucket,\n            self.TOPIC_NAME,\n            topic_project=self.TOPIC_ALT_PROJECT,\n            custom_attributes=self.CUSTOM_ATTRIBUTES,\n            event_types=self.event_types(),\n            blob_name_prefix=self.BLOB_NAME_PREFIX,\n            payload_format=self.payload_format(),\n        )\n\n        self.assertIs(notification.bucket, bucket)\n        self.assertEqual(notification.topic_name, self.TOPIC_NAME)\n        self.assertEqual(notification.topic_project, self.TOPIC_ALT_PROJECT)\n        self.assertEqual(notification.custom_attributes, self.CUSTOM_ATTRIBUTES)\n        self.assertEqual(notification.event_types, self.event_types())\n        self.assertEqual(notification.blob_name_prefix, self.BLOB_NAME_PREFIX)\n        self.assertEqual(notification.payload_format, self.payload_format())\n\n    def test_from_api_repr_no_topic(self):\n        klass = self._get_target_class()\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n        resource = {}\n\n        with self.assertRaises(ValueError):\n            klass.from_api_repr(resource, bucket=bucket)\n\n    def test_from_api_repr_invalid_topic(self):\n        klass = self._get_target_class()\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n        resource = {\"topic\": \"@#$%\"}\n\n        with self.assertRaises(ValueError):\n            klass.from_api_repr(resource, bucket=bucket)\n\n    def test_from_api_repr_minimal(self):\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        klass = self._get_target_class()\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n        resource = {\n            \"topic\": self.TOPIC_REF,\n            \"id\": self.NOTIFICATION_ID,\n            \"etag\": self.ETAG,\n            \"selfLink\": self.SELF_LINK,\n            \"payload_format\": NONE_PAYLOAD_FORMAT,\n        }\n\n        notification = klass.from_api_repr(resource, bucket=bucket)\n\n        self.assertIs(notification.bucket, bucket)\n        self.assertEqual(notification.topic_name, self.TOPIC_NAME)\n        self.assertEqual(notification.topic_project, self.BUCKET_PROJECT)\n        self.assertIsNone(notification.custom_attributes)\n        self.assertIsNone(notification.event_types)\n        self.assertIsNone(notification.blob_name_prefix)\n        self.assertEqual(notification.payload_format, NONE_PAYLOAD_FORMAT)\n        self.assertEqual(notification.etag, self.ETAG)\n        self.assertEqual(notification.self_link, self.SELF_LINK)\n\n    def test_from_api_repr_explicit(self):\n        klass = self._get_target_class()\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n        resource = {\n            \"topic\": self.TOPIC_ALT_REF,\n            \"custom_attributes\": self.CUSTOM_ATTRIBUTES,\n            \"event_types\": self.event_types(),\n            \"object_name_prefix\": self.BLOB_NAME_PREFIX,\n            \"payload_format\": self.payload_format(),\n            \"id\": self.NOTIFICATION_ID,\n            \"etag\": self.ETAG,\n            \"selfLink\": self.SELF_LINK,\n        }\n\n        notification = klass.from_api_repr(resource, bucket=bucket)\n\n        self.assertIs(notification.bucket, bucket)\n        self.assertEqual(notification.topic_name, self.TOPIC_NAME)\n        self.assertEqual(notification.topic_project, self.TOPIC_ALT_PROJECT)\n        self.assertEqual(notification.custom_attributes, self.CUSTOM_ATTRIBUTES)\n        self.assertEqual(notification.event_types, self.event_types())\n        self.assertEqual(notification.blob_name_prefix, self.BLOB_NAME_PREFIX)\n        self.assertEqual(notification.payload_format, self.payload_format())\n        self.assertEqual(notification.notification_id, self.NOTIFICATION_ID)\n        self.assertEqual(notification.etag, self.ETAG)\n        self.assertEqual(notification.self_link, self.SELF_LINK)\n\n    def test_notification_id(self):\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        self.assertIsNone(notification.notification_id)\n\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n        self.assertEqual(notification.notification_id, self.NOTIFICATION_ID)\n\n    def test_etag(self):\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        self.assertIsNone(notification.etag)\n\n        notification._properties[\"etag\"] = self.ETAG\n        self.assertEqual(notification.etag, self.ETAG)\n\n    def test_self_link(self):\n        client = self._make_client()\n        bucket = self._make_bucket(client)\n\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        self.assertIsNone(notification.self_link)\n\n        notification._properties[\"selfLink\"] = self.SELF_LINK\n        self.assertEqual(notification.self_link, self.SELF_LINK)\n\n    def test_create_w_existing_notification_id(self):\n        client = mock.Mock(spec=[\"_post_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n\n        with self.assertRaises(ValueError):\n            notification.create()\n\n        client._post_resource.assert_not_called()\n\n    def test_create_wo_topic_name(self):\n        from google.cloud.exceptions import BadRequest\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        client = mock.Mock(spec=[\"_post_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        client._post_resource.side_effect = BadRequest(\n            \"Invalid Google Cloud Pub/Sub topic.\"\n        )\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, None)\n\n        with self.assertRaises(BadRequest):\n            notification.create()\n\n        expected_topic = self.TOPIC_REF_FMT.format(self.BUCKET_PROJECT, \"\")\n        expected_data = {\n            \"topic\": expected_topic,\n            \"payload_format\": NONE_PAYLOAD_FORMAT,\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            self.CREATE_PATH,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=None,\n        )\n\n    def test_create_w_defaults(self):\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        api_response = {\n            \"topic\": self.TOPIC_REF,\n            \"id\": self.NOTIFICATION_ID,\n            \"etag\": self.ETAG,\n            \"selfLink\": self.SELF_LINK,\n            \"payload_format\": NONE_PAYLOAD_FORMAT,\n        }\n        client = mock.Mock(spec=[\"_post_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        client._post_resource.return_value = api_response\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        notification.create()\n\n        self.assertEqual(notification.notification_id, self.NOTIFICATION_ID)\n        self.assertEqual(notification.etag, self.ETAG)\n        self.assertEqual(notification.self_link, self.SELF_LINK)\n        self.assertIsNone(notification.custom_attributes)\n        self.assertIsNone(notification.event_types)\n        self.assertIsNone(notification.blob_name_prefix)\n        self.assertEqual(notification.payload_format, NONE_PAYLOAD_FORMAT)\n\n        expected_data = {\n            \"topic\": self.TOPIC_REF,\n            \"payload_format\": NONE_PAYLOAD_FORMAT,\n        }\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            self.CREATE_PATH,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=None,\n        )\n\n    def test_create_w_explicit_client_w_timeout_w_retry(self):\n        user_project = \"user-project-123\"\n        api_response = {\n            \"topic\": self.TOPIC_ALT_REF,\n            \"custom_attributes\": self.CUSTOM_ATTRIBUTES,\n            \"event_types\": self.event_types(),\n            \"object_name_prefix\": self.BLOB_NAME_PREFIX,\n            \"payload_format\": self.payload_format(),\n            \"id\": self.NOTIFICATION_ID,\n            \"etag\": self.ETAG,\n            \"selfLink\": self.SELF_LINK,\n        }\n        bucket = self._make_bucket(client=None, user_project=user_project)\n        notification = self._make_one(\n            bucket,\n            self.TOPIC_NAME,\n            topic_project=self.TOPIC_ALT_PROJECT,\n            custom_attributes=self.CUSTOM_ATTRIBUTES,\n            event_types=self.event_types(),\n            blob_name_prefix=self.BLOB_NAME_PREFIX,\n            payload_format=self.payload_format(),\n        )\n        client = mock.Mock(spec=[\"_post_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        client._post_resource.return_value = api_response\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        notification.create(client=client, timeout=timeout, retry=retry)\n\n        self.assertEqual(notification.custom_attributes, self.CUSTOM_ATTRIBUTES)\n        self.assertEqual(notification.event_types, self.event_types())\n        self.assertEqual(notification.blob_name_prefix, self.BLOB_NAME_PREFIX)\n        self.assertEqual(notification.payload_format, self.payload_format())\n        self.assertEqual(notification.notification_id, self.NOTIFICATION_ID)\n        self.assertEqual(notification.etag, self.ETAG)\n        self.assertEqual(notification.self_link, self.SELF_LINK)\n\n        expected_data = {\n            \"topic\": self.TOPIC_ALT_REF,\n            \"custom_attributes\": self.CUSTOM_ATTRIBUTES,\n            \"event_types\": self.event_types(),\n            \"object_name_prefix\": self.BLOB_NAME_PREFIX,\n            \"payload_format\": self.payload_format(),\n        }\n        expected_query_params = {\"userProject\": user_project}\n        client._post_resource.assert_called_once_with(\n            self.CREATE_PATH,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_exists_wo_notification_id(self):\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        with self.assertRaises(ValueError):\n            notification.exists()\n\n        client._get_resource.assert_not_called()\n\n    def test_exists_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n\n        self.assertFalse(notification.exists())\n\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            self.NOTIFICATION_PATH,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_exists_hit_w_explicit_w_user_project(self):\n        user_project = \"user-project-123\"\n        api_response = {\n            \"topic\": self.TOPIC_REF,\n            \"id\": self.NOTIFICATION_ID,\n            \"etag\": self.ETAG,\n            \"selfLink\": self.SELF_LINK,\n        }\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.return_vale = api_response\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client, user_project=user_project)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        self.assertTrue(\n            notification.exists(client=client, timeout=timeout, retry=retry)\n        )\n\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            self.NOTIFICATION_PATH,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_reload_wo_notification_id(self):\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        with self.assertRaises(ValueError):\n            notification.reload()\n\n        client._get_resource.assert_not_called()\n\n    def test_reload_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n\n        with self.assertRaises(NotFound):\n            notification.reload()\n\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            self.NOTIFICATION_PATH,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_reload_hit_w_explicit_w_user_project(self):\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        user_project = \"user-project-123\"\n        api_response = {\n            \"topic\": self.TOPIC_REF,\n            \"id\": self.NOTIFICATION_ID,\n            \"etag\": self.ETAG,\n            \"selfLink\": self.SELF_LINK,\n            \"payload_format\": NONE_PAYLOAD_FORMAT,\n        }\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.return_value = api_response\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client, user_project=user_project)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        notification.reload(client=client, timeout=timeout, retry=retry)\n\n        self.assertEqual(notification.etag, self.ETAG)\n        self.assertEqual(notification.self_link, self.SELF_LINK)\n        self.assertIsNone(notification.custom_attributes)\n        self.assertIsNone(notification.event_types)\n        self.assertIsNone(notification.blob_name_prefix)\n        self.assertEqual(notification.payload_format, NONE_PAYLOAD_FORMAT)\n\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            self.NOTIFICATION_PATH,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_delete_wo_notification_id(self):\n        client = mock.Mock(spec=[\"_delete_resource\", \"project\"])\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n\n        with self.assertRaises(ValueError):\n            notification.delete()\n\n        client._delete_resource.assert_not_called()\n\n    def test_delete_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        client = mock.Mock(spec=[\"_delete_resource\", \"project\"])\n        client._delete_resource.side_effect = NotFound(\"testing\")\n        client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(client)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n\n        with self.assertRaises(NotFound):\n            notification.delete()\n\n        client._delete_resource.assert_called_once_with(\n            self.NOTIFICATION_PATH,\n            query_params={},\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_delete_hit_w_explicit_client_timeout_retry(self):\n        user_project = \"user-project-123\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket_client = mock.Mock(spec=[\"project\"])\n        bucket_client.project = self.BUCKET_PROJECT\n        bucket = self._make_bucket(bucket_client, user_project=user_project)\n        notification = self._make_one(bucket, self.TOPIC_NAME)\n        notification._properties[\"id\"] = self.NOTIFICATION_ID\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        notification.delete(client=client, timeout=timeout, retry=retry)\n\n        client._delete_resource.assert_called_once_with(\n            self.NOTIFICATION_PATH,\n            query_params={\"userProject\": user_project},\n            timeout=timeout,\n            retry=retry,\n        )\n\n\nclass Test__parse_topic_path(unittest.TestCase):\n    @staticmethod\n    def _call_fut(*args, **kwargs):\n        from google.cloud.storage import notification\n\n        return notification._parse_topic_path(*args, **kwargs)\n\n    @staticmethod\n    def _make_topic_path(project, topic_name):\n        from google.cloud.storage import notification\n\n        return notification._TOPIC_REF_FMT.format(project, topic_name)\n\n    def test_project_name_too_long(self):\n        project = \"a\" * 31\n        topic_path = self._make_topic_path(project, \"topic-name\")\n        with self.assertRaises(ValueError):\n            self._call_fut(topic_path)\n\n    def test_project_name_uppercase(self):\n        project = \"aaaAaa\"\n        topic_path = self._make_topic_path(project, \"topic-name\")\n        with self.assertRaises(ValueError):\n            self._call_fut(topic_path)\n\n    def test_leading_digit(self):\n        project = \"1aaaaa\"\n        topic_path = self._make_topic_path(project, \"topic-name\")\n        with self.assertRaises(ValueError):\n            self._call_fut(topic_path)\n\n    def test_leading_hyphen(self):\n        project = \"-aaaaa\"\n        topic_path = self._make_topic_path(project, \"topic-name\")\n        with self.assertRaises(ValueError):\n            self._call_fut(topic_path)\n\n    def test_trailing_hyphen(self):\n        project = \"aaaaa-\"\n        topic_path = self._make_topic_path(project, \"topic-name\")\n        with self.assertRaises(ValueError):\n            self._call_fut(topic_path)\n\n    def test_invalid_format(self):\n        topic_path = \"@#$%\"\n        with self.assertRaises(ValueError):\n            self._call_fut(topic_path)\n\n    def test_success(self):\n        topic_name = \"tah-pic-nehm\"\n        project_choices = (\n            \"a\" * 30,  # Max length.\n            \"a-b--c---d\",  # Valid hyphen usage.\n            \"abcdefghijklmnopqrstuvwxyz\",  # Valid letters.\n            \"z0123456789\",  # Valid digits (non-leading).\n            \"a-bcdefghijklmn12opqrstuv0wxyz\",\n        )\n        for project in project_choices:\n            topic_path = self._make_topic_path(project, topic_name)\n            result = self._call_fut(topic_path)\n            self.assertEqual(result, (topic_name, project))\n", "tests/unit/__init__.py": "# Copyright 2016 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport json\nimport os\n\n\ndef _read_local_json(json_file):\n    here = os.path.dirname(__file__)\n    json_path = os.path.abspath(os.path.join(here, json_file))\n    with io.open(json_path, \"r\", encoding=\"utf-8-sig\") as fileobj:\n        return json.load(fileobj)\n", "tests/unit/test_bucket.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport unittest\n\nimport mock\nimport pytest\n\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_ENFORCED\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_INHERITED\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_UNSPECIFIED\nfrom google.cloud.storage.constants import RPO_DEFAULT\nfrom google.cloud.storage.constants import RPO_ASYNC_TURBO\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\nfrom google.cloud.storage._helpers import _get_default_storage_base_url\n\n\ndef _create_signing_credentials():\n    import google.auth.credentials\n\n    class _SigningCredentials(\n        google.auth.credentials.Credentials, google.auth.credentials.Signing\n    ):\n        pass\n\n    credentials = mock.Mock(spec=_SigningCredentials)\n\n    return credentials\n\n\nclass Test__blobs_page_start(unittest.TestCase):\n    @staticmethod\n    def _call_fut(iterator, page, response):\n        from google.cloud.storage.bucket import _blobs_page_start\n\n        return _blobs_page_start(iterator, page, response)\n\n    def test_wo_any_prefixes(self):\n        iterator = mock.Mock(spec=[\"prefixes\"], prefixes=set())\n        page = mock.Mock(spec=[\"prefixes\"])\n        response = {}\n\n        self._call_fut(iterator, page, response)\n\n        self.assertEqual(page.prefixes, ())\n        self.assertEqual(iterator.prefixes, set())\n\n    def test_w_prefixes(self):\n        iterator_prefixes = set([\"foo/\", \"qux/\"])\n        iterator = mock.Mock(spec=[\"prefixes\"], prefixes=iterator_prefixes)\n        page = mock.Mock(spec=[\"prefixes\"])\n        page_prefixes = [\"foo/\", \"bar/\", \"baz/\"]\n        response = {\"prefixes\": page_prefixes}\n\n        self._call_fut(iterator, page, response)\n\n        self.assertEqual(page.prefixes, tuple(page_prefixes))\n        self.assertEqual(iterator.prefixes, iterator_prefixes.union(page_prefixes))\n\n\nclass Test__item_to_blob(unittest.TestCase):\n    @staticmethod\n    def _call_fut(iterator, item):\n        from google.cloud.storage.bucket import _item_to_blob\n\n        return _item_to_blob(iterator, item)\n\n    def test_wo_extra_properties(self):\n        from google.cloud.storage.blob import Blob\n\n        blob_name = \"blob-name\"\n        bucket = mock.Mock(spec=[])\n        iterator = mock.Mock(spec=[\"bucket\"], bucket=bucket)\n        item = {\"name\": blob_name}\n\n        blob = self._call_fut(iterator, item)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob._properties, item)\n\n    def test_w_extra_properties(self):\n        from google.cloud.storage.blob import Blob\n\n        blob_name = \"blob-name\"\n        bucket = mock.Mock(spec=[])\n        iterator = mock.Mock(spec=[\"bucket\"], bucket=bucket)\n        item = {\n            \"name\": blob_name,\n            \"generation\": 123,\n            \"contentType\": \"text/plain\",\n            \"contentLanguage\": \"en-US\",\n        }\n\n        blob = self._call_fut(iterator, item)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob._properties, item)\n\n\nclass Test_LifecycleRuleConditions(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.bucket import LifecycleRuleConditions\n\n        return LifecycleRuleConditions\n\n    def _make_one(self, **kw):\n        return self._get_target_class()(**kw)\n\n    def test_ctor_wo_conditions(self):\n        with self.assertRaises(ValueError):\n            self._make_one()\n\n    def test_ctor_w_age_and_matches_storage_class(self):\n        conditions = self._make_one(age=10, matches_storage_class=[\"COLDLINE\"])\n        expected = {\"age\": 10, \"matchesStorageClass\": [\"COLDLINE\"]}\n        self.assertEqual(dict(conditions), expected)\n        self.assertEqual(conditions.age, 10)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertEqual(conditions.matches_storage_class, [\"COLDLINE\"])\n        self.assertIsNone(conditions.number_of_newer_versions)\n\n    def test_ctor_w_created_before_and_is_live(self):\n        import datetime\n\n        before = datetime.date(2018, 8, 1)\n        conditions = self._make_one(created_before=before, is_live=False)\n        expected = {\"createdBefore\": \"2018-08-01\", \"isLive\": False}\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertEqual(conditions.created_before, before)\n        self.assertEqual(conditions.is_live, False)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertIsNone(conditions.number_of_newer_versions)\n        self.assertIsNone(conditions.days_since_custom_time)\n        self.assertIsNone(conditions.custom_time_before)\n        self.assertIsNone(conditions.noncurrent_time_before)\n\n    def test_ctor_w_number_of_newer_versions(self):\n        conditions = self._make_one(number_of_newer_versions=3)\n        expected = {\"numNewerVersions\": 3}\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertEqual(conditions.number_of_newer_versions, 3)\n\n    def test_ctor_w_days_since_custom_time(self):\n        conditions = self._make_one(\n            number_of_newer_versions=3, days_since_custom_time=2\n        )\n        expected = {\"numNewerVersions\": 3, \"daysSinceCustomTime\": 2}\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertEqual(conditions.number_of_newer_versions, 3)\n        self.assertEqual(conditions.days_since_custom_time, 2)\n\n    def test_ctor_w_days_since_noncurrent_time(self):\n        conditions = self._make_one(\n            number_of_newer_versions=3, days_since_noncurrent_time=2\n        )\n        expected = {\"numNewerVersions\": 3, \"daysSinceNoncurrentTime\": 2}\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertEqual(conditions.number_of_newer_versions, 3)\n        self.assertEqual(conditions.days_since_noncurrent_time, 2)\n\n    def test_ctor_w_custom_time_before(self):\n        import datetime\n\n        custom_time_before = datetime.date(2018, 8, 1)\n        conditions = self._make_one(\n            number_of_newer_versions=3, custom_time_before=custom_time_before\n        )\n        expected = {\n            \"numNewerVersions\": 3,\n            \"customTimeBefore\": custom_time_before.isoformat(),\n        }\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertEqual(conditions.number_of_newer_versions, 3)\n        self.assertEqual(conditions.custom_time_before, custom_time_before)\n\n    def test_ctor_w_noncurrent_time_before(self):\n        import datetime\n\n        noncurrent_before = datetime.date(2018, 8, 1)\n        conditions = self._make_one(\n            number_of_newer_versions=3, noncurrent_time_before=noncurrent_before\n        )\n\n        expected = {\n            \"numNewerVersions\": 3,\n            \"noncurrentTimeBefore\": noncurrent_before.isoformat(),\n        }\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertEqual(conditions.number_of_newer_versions, 3)\n        self.assertEqual(conditions.noncurrent_time_before, noncurrent_before)\n\n    def test_ctor_w_matches_prefix(self):\n        conditions = self._make_one(matches_prefix=[\"test-prefix\"])\n        expected = {\"matchesPrefix\": [\"test-prefix\"]}\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertIsNone(conditions.matches_suffix)\n        self.assertEqual(conditions.matches_prefix, [\"test-prefix\"])\n\n    def test_ctor_w_matches_suffix(self):\n        conditions = self._make_one(matches_suffix=[\"test-suffix\"])\n        expected = {\"matchesSuffix\": [\"test-suffix\"]}\n        self.assertEqual(dict(conditions), expected)\n        self.assertIsNone(conditions.age)\n        self.assertIsNone(conditions.created_before)\n        self.assertIsNone(conditions.is_live)\n        self.assertIsNone(conditions.matches_storage_class)\n        self.assertIsNone(conditions.matches_prefix)\n        self.assertEqual(conditions.matches_suffix, [\"test-suffix\"])\n\n    def test_from_api_repr(self):\n        import datetime\n\n        custom_time_before = datetime.date(2018, 8, 1)\n        noncurrent_before = datetime.date(2018, 8, 1)\n        before = datetime.date(2018, 8, 1)\n        klass = self._get_target_class()\n        resource = {\n            \"age\": 10,\n            \"createdBefore\": \"2018-08-01\",\n            \"isLive\": True,\n            \"matchesStorageClass\": [\"COLDLINE\"],\n            \"numNewerVersions\": 3,\n            \"daysSinceCustomTime\": 2,\n            \"customTimeBefore\": custom_time_before.isoformat(),\n            \"daysSinceNoncurrentTime\": 2,\n            \"noncurrentTimeBefore\": noncurrent_before.isoformat(),\n        }\n        conditions = klass.from_api_repr(resource)\n        self.assertEqual(conditions.age, 10)\n        self.assertEqual(conditions.created_before, before)\n        self.assertEqual(conditions.is_live, True)\n        self.assertEqual(conditions.matches_storage_class, [\"COLDLINE\"])\n        self.assertEqual(conditions.number_of_newer_versions, 3)\n        self.assertEqual(conditions.days_since_custom_time, 2)\n        self.assertEqual(conditions.custom_time_before, custom_time_before)\n        self.assertEqual(conditions.days_since_noncurrent_time, 2)\n        self.assertEqual(conditions.noncurrent_time_before, noncurrent_before)\n\n\nclass Test_LifecycleRuleDelete(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.bucket import LifecycleRuleDelete\n\n        return LifecycleRuleDelete\n\n    def _make_one(self, **kw):\n        return self._get_target_class()(**kw)\n\n    def test_ctor_wo_conditions(self):\n        with self.assertRaises(ValueError):\n            self._make_one()\n\n    def test_ctor_w_condition(self):\n        rule = self._make_one(age=10, matches_storage_class=[\"COLDLINE\"])\n        expected = {\n            \"action\": {\"type\": \"Delete\"},\n            \"condition\": {\"age\": 10, \"matchesStorageClass\": [\"COLDLINE\"]},\n        }\n        self.assertEqual(dict(rule), expected)\n\n    def test_from_api_repr(self):\n        klass = self._get_target_class()\n        conditions = {\n            \"age\": 10,\n            \"createdBefore\": \"2018-08-01\",\n            \"isLive\": True,\n            \"matchesStorageClass\": [\"COLDLINE\"],\n            \"numNewerVersions\": 3,\n        }\n        resource = {\"action\": {\"type\": \"Delete\"}, \"condition\": conditions}\n        rule = klass.from_api_repr(resource)\n        self.assertEqual(dict(rule), resource)\n\n\nclass Test_LifecycleRuleSetStorageClass(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.bucket import LifecycleRuleSetStorageClass\n\n        return LifecycleRuleSetStorageClass\n\n    def _make_one(self, **kw):\n        return self._get_target_class()(**kw)\n\n    def test_ctor_wo_conditions(self):\n        with self.assertRaises(ValueError):\n            self._make_one(storage_class=\"COLDLINE\")\n\n    def test_ctor_w_condition(self):\n        rule = self._make_one(\n            storage_class=\"COLDLINE\", age=10, matches_storage_class=[\"NEARLINE\"]\n        )\n        expected = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"COLDLINE\"},\n            \"condition\": {\"age\": 10, \"matchesStorageClass\": [\"NEARLINE\"]},\n        }\n        self.assertEqual(dict(rule), expected)\n\n    def test_from_api_repr(self):\n        klass = self._get_target_class()\n        conditions = {\n            \"age\": 10,\n            \"createdBefore\": \"2018-08-01\",\n            \"isLive\": True,\n            \"matchesStorageClass\": [\"NEARLINE\"],\n            \"numNewerVersions\": 3,\n        }\n        resource = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"COLDLINE\"},\n            \"condition\": conditions,\n        }\n        rule = klass.from_api_repr(resource)\n        self.assertEqual(dict(rule), resource)\n\n\nclass Test_LifecycleRuleAbortIncompleteMultipartUpload(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.bucket import (\n            LifecycleRuleAbortIncompleteMultipartUpload,\n        )\n\n        return LifecycleRuleAbortIncompleteMultipartUpload\n\n    def _make_one(self, **kw):\n        return self._get_target_class()(**kw)\n\n    def test_ctor_wo_conditions(self):\n        with self.assertRaises(ValueError):\n            self._make_one()\n\n    def test_ctor_w_condition(self):\n        rule = self._make_one(age=10)\n        expected = {\n            \"action\": {\"type\": \"AbortIncompleteMultipartUpload\"},\n            \"condition\": {\"age\": 10},\n        }\n        self.assertEqual(dict(rule), expected)\n\n    def test_from_api_repr(self):\n        klass = self._get_target_class()\n        conditions = {\n            \"age\": 10,\n        }\n        resource = {\n            \"action\": {\"type\": \"AbortIncompleteMultipartUpload\"},\n            \"condition\": conditions,\n        }\n        rule = klass.from_api_repr(resource)\n        self.assertEqual(dict(rule), resource)\n\n\nclass Test_IAMConfiguration(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.bucket import IAMConfiguration\n\n        return IAMConfiguration\n\n    def _make_one(self, bucket, **kw):\n        return self._get_target_class()(bucket, **kw)\n\n    @staticmethod\n    def _make_bucket():\n        from google.cloud.storage.bucket import Bucket\n\n        return mock.create_autospec(Bucket, instance=True)\n\n    def test_ctor_defaults(self):\n        bucket = self._make_bucket()\n\n        config = self._make_one(bucket)\n\n        self.assertIs(config.bucket, bucket)\n        self.assertFalse(config.uniform_bucket_level_access_enabled)\n        self.assertIsNone(config.uniform_bucket_level_access_locked_time)\n        # TODO: Remove unspecified after changeover is complete\n        self.assertIn(\n            config.public_access_prevention,\n            [PUBLIC_ACCESS_PREVENTION_UNSPECIFIED, PUBLIC_ACCESS_PREVENTION_INHERITED],\n        )\n        self.assertFalse(config.bucket_policy_only_enabled)\n        self.assertIsNone(config.bucket_policy_only_locked_time)\n\n    def test_ctor_explicit_ubla(self):\n        bucket = self._make_bucket()\n        now = _NOW(_UTC)\n\n        config = self._make_one(\n            bucket,\n            uniform_bucket_level_access_enabled=True,\n            uniform_bucket_level_access_locked_time=now,\n        )\n\n        self.assertIs(config.bucket, bucket)\n        self.assertTrue(config.uniform_bucket_level_access_enabled)\n        self.assertEqual(config.uniform_bucket_level_access_locked_time, now)\n        self.assertTrue(config.bucket_policy_only_enabled)\n        self.assertEqual(config.bucket_policy_only_locked_time, now)\n\n    def test_ctor_explicit_pap(self):\n        bucket = self._make_bucket()\n\n        config = self._make_one(\n            bucket,\n            public_access_prevention=PUBLIC_ACCESS_PREVENTION_ENFORCED,\n        )\n\n        self.assertIs(config.bucket, bucket)\n        self.assertFalse(config.uniform_bucket_level_access_enabled)\n        self.assertEqual(\n            config.public_access_prevention, PUBLIC_ACCESS_PREVENTION_ENFORCED\n        )\n\n        config.public_access_prevention = PUBLIC_ACCESS_PREVENTION_INHERITED\n        # TODO: Remove unspecified after changeover is complete\n        self.assertIn(\n            config.public_access_prevention,\n            [PUBLIC_ACCESS_PREVENTION_UNSPECIFIED, PUBLIC_ACCESS_PREVENTION_INHERITED],\n        )\n\n    def test_ctor_explicit_bpo(self):\n        bucket = self._make_bucket()\n        now = _NOW(_UTC)\n\n        config = pytest.deprecated_call(\n            self._make_one,\n            bucket,\n            bucket_policy_only_enabled=True,\n            bucket_policy_only_locked_time=now,\n        )\n\n        self.assertIs(config.bucket, bucket)\n        self.assertTrue(config.uniform_bucket_level_access_enabled)\n        self.assertEqual(config.uniform_bucket_level_access_locked_time, now)\n        self.assertTrue(config.bucket_policy_only_enabled)\n        self.assertEqual(config.bucket_policy_only_locked_time, now)\n\n    def test_ctor_ubla_and_bpo_enabled(self):\n        bucket = self._make_bucket()\n\n        with self.assertRaises(ValueError):\n            self._make_one(\n                bucket,\n                uniform_bucket_level_access_enabled=True,\n                bucket_policy_only_enabled=True,\n            )\n\n    def test_ctor_ubla_and_bpo_time(self):\n        bucket = self._make_bucket()\n        now = _NOW(_UTC)\n\n        with self.assertRaises(ValueError):\n            self._make_one(\n                bucket,\n                uniform_bucket_level_access_enabled=True,\n                uniform_bucket_level_access_locked_time=now,\n                bucket_policy_only_locked_time=now,\n            )\n\n    def test_from_api_repr_w_empty_resource(self):\n        klass = self._get_target_class()\n        bucket = self._make_bucket()\n        resource = {}\n\n        config = klass.from_api_repr(resource, bucket)\n\n        self.assertIs(config.bucket, bucket)\n        self.assertFalse(config.bucket_policy_only_enabled)\n        self.assertIsNone(config.bucket_policy_only_locked_time)\n\n    def test_from_api_repr_w_empty_bpo(self):\n        klass = self._get_target_class()\n        bucket = self._make_bucket()\n        resource = {\"uniformBucketLevelAccess\": {}}\n\n        config = klass.from_api_repr(resource, bucket)\n\n        self.assertIs(config.bucket, bucket)\n        self.assertFalse(config.bucket_policy_only_enabled)\n        self.assertIsNone(config.bucket_policy_only_locked_time)\n\n    def test_from_api_repr_w_disabled(self):\n        klass = self._get_target_class()\n        bucket = self._make_bucket()\n        resource = {\"uniformBucketLevelAccess\": {\"enabled\": False}}\n\n        config = klass.from_api_repr(resource, bucket)\n\n        self.assertIs(config.bucket, bucket)\n        self.assertFalse(config.bucket_policy_only_enabled)\n        self.assertIsNone(config.bucket_policy_only_locked_time)\n\n    def test_from_api_repr_w_enabled(self):\n        from google.cloud._helpers import _datetime_to_rfc3339\n\n        klass = self._get_target_class()\n        bucket = self._make_bucket()\n        now = _NOW(_UTC)\n        resource = {\n            \"uniformBucketLevelAccess\": {\n                \"enabled\": True,\n                \"lockedTime\": _datetime_to_rfc3339(now),\n            }\n        }\n\n        config = klass.from_api_repr(resource, bucket)\n\n        self.assertIs(config.bucket, bucket)\n        self.assertTrue(config.uniform_bucket_level_access_enabled)\n        self.assertEqual(config.uniform_bucket_level_access_locked_time, now)\n        self.assertTrue(config.bucket_policy_only_enabled)\n        self.assertEqual(config.bucket_policy_only_locked_time, now)\n\n    def test_uniform_bucket_level_access_enabled_setter(self):\n        bucket = self._make_bucket()\n        config = self._make_one(bucket)\n\n        config.uniform_bucket_level_access_enabled = True\n        self.assertTrue(config.bucket_policy_only_enabled)\n\n        self.assertTrue(config[\"uniformBucketLevelAccess\"][\"enabled\"])\n        bucket._patch_property.assert_called_once_with(\"iamConfiguration\", config)\n\n    def test_bucket_policy_only_enabled_setter(self):\n        bucket = self._make_bucket()\n        config = self._make_one(bucket)\n\n        with pytest.deprecated_call():\n            config.bucket_policy_only_enabled = True\n\n        self.assertTrue(config.uniform_bucket_level_access_enabled)\n        self.assertTrue(config[\"uniformBucketLevelAccess\"][\"enabled\"])\n        bucket._patch_property.assert_called_once_with(\"iamConfiguration\", config)\n\n\nclass Test_Bucket(unittest.TestCase):\n    @staticmethod\n    def _get_target_class():\n        from google.cloud.storage.bucket import Bucket\n\n        return Bucket\n\n    @staticmethod\n    def _get_default_timeout():\n        from google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n        return _DEFAULT_TIMEOUT\n\n    @staticmethod\n    def _make_client(**kw):\n        from google.cloud.storage.client import Client\n\n        kw[\"api_endpoint\"] = kw.get(\"api_endpoint\") or _get_default_storage_base_url()\n        return mock.create_autospec(Client, instance=True, **kw)\n\n    def _make_one(self, client=None, name=None, properties=None, user_project=None):\n        if client is None:\n            client = self._make_client()\n        if user_project is None:\n            bucket = self._get_target_class()(client, name=name)\n        else:\n            bucket = self._get_target_class()(\n                client, name=name, user_project=user_project\n            )\n        bucket._properties = properties or {}\n        return bucket\n\n    def test_ctor_w_invalid_name(self):\n        NAME = \"#invalid\"\n        with self.assertRaises(ValueError):\n            self._make_one(name=NAME)\n\n    def test_ctor(self):\n        NAME = \"name\"\n        properties = {\"key\": \"value\"}\n        bucket = self._make_one(name=NAME, properties=properties)\n        self.assertEqual(bucket.name, NAME)\n        self.assertEqual(bucket._properties, properties)\n        self.assertEqual(list(bucket._changes), [])\n        self.assertFalse(bucket._acl.loaded)\n        self.assertIs(bucket._acl.bucket, bucket)\n        self.assertFalse(bucket._default_object_acl.loaded)\n        self.assertIs(bucket._default_object_acl.bucket, bucket)\n        self.assertEqual(list(bucket._label_removals), [])\n        self.assertIsNone(bucket.user_project)\n\n    def test_ctor_w_user_project(self):\n        NAME = \"name\"\n        USER_PROJECT = \"user-project-123\"\n        client = self._make_client()\n        bucket = self._make_one(client, name=NAME, user_project=USER_PROJECT)\n        self.assertEqual(bucket.name, NAME)\n        self.assertEqual(bucket._properties, {})\n        self.assertEqual(list(bucket._changes), [])\n        self.assertFalse(bucket._acl.loaded)\n        self.assertIs(bucket._acl.bucket, bucket)\n        self.assertFalse(bucket._default_object_acl.loaded)\n        self.assertIs(bucket._default_object_acl.bucket, bucket)\n        self.assertEqual(list(bucket._label_removals), [])\n        self.assertEqual(bucket.user_project, USER_PROJECT)\n\n    def test_blob_wo_keys(self):\n        from google.cloud.storage.blob import Blob\n\n        BUCKET_NAME = \"BUCKET_NAME\"\n        BLOB_NAME = \"BLOB_NAME\"\n        CHUNK_SIZE = 1024 * 1024\n\n        bucket = self._make_one(name=BUCKET_NAME)\n        blob = bucket.blob(BLOB_NAME, chunk_size=CHUNK_SIZE)\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertIs(blob.client, bucket.client)\n        self.assertEqual(blob.name, BLOB_NAME)\n        self.assertEqual(blob.chunk_size, CHUNK_SIZE)\n        self.assertIsNone(blob._encryption_key)\n        self.assertIsNone(blob.kms_key_name)\n\n    def test_blob_w_encryption_key(self):\n        from google.cloud.storage.blob import Blob\n\n        BUCKET_NAME = \"BUCKET_NAME\"\n        BLOB_NAME = \"BLOB_NAME\"\n        CHUNK_SIZE = 1024 * 1024\n        KEY = b\"01234567890123456789012345678901\"  # 32 bytes\n\n        bucket = self._make_one(name=BUCKET_NAME)\n        blob = bucket.blob(BLOB_NAME, chunk_size=CHUNK_SIZE, encryption_key=KEY)\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertIs(blob.client, bucket.client)\n        self.assertEqual(blob.name, BLOB_NAME)\n        self.assertEqual(blob.chunk_size, CHUNK_SIZE)\n        self.assertEqual(blob._encryption_key, KEY)\n        self.assertIsNone(blob.kms_key_name)\n\n    def test_blob_w_generation(self):\n        from google.cloud.storage.blob import Blob\n\n        BUCKET_NAME = \"BUCKET_NAME\"\n        BLOB_NAME = \"BLOB_NAME\"\n        GENERATION = 123\n\n        bucket = self._make_one(name=BUCKET_NAME)\n        blob = bucket.blob(BLOB_NAME, generation=GENERATION)\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertIs(blob.client, bucket.client)\n        self.assertEqual(blob.name, BLOB_NAME)\n        self.assertEqual(blob.generation, GENERATION)\n\n    def test_blob_w_kms_key_name(self):\n        from google.cloud.storage.blob import Blob\n\n        BUCKET_NAME = \"BUCKET_NAME\"\n        BLOB_NAME = \"BLOB_NAME\"\n        CHUNK_SIZE = 1024 * 1024\n        KMS_RESOURCE = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n\n        bucket = self._make_one(name=BUCKET_NAME)\n        blob = bucket.blob(BLOB_NAME, chunk_size=CHUNK_SIZE, kms_key_name=KMS_RESOURCE)\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertIs(blob.client, bucket.client)\n        self.assertEqual(blob.name, BLOB_NAME)\n        self.assertEqual(blob.chunk_size, CHUNK_SIZE)\n        self.assertIsNone(blob._encryption_key)\n        self.assertEqual(blob.kms_key_name, KMS_RESOURCE)\n\n    def test_notification_defaults(self):\n        from google.cloud.storage.notification import BucketNotification\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        PROJECT = \"PROJECT\"\n        BUCKET_NAME = \"BUCKET_NAME\"\n        TOPIC_NAME = \"TOPIC_NAME\"\n        client = self._make_client(project=PROJECT)\n        bucket = self._make_one(client, name=BUCKET_NAME)\n\n        notification = bucket.notification(TOPIC_NAME)\n\n        self.assertIsInstance(notification, BucketNotification)\n        self.assertIs(notification.bucket, bucket)\n        self.assertEqual(notification.topic_project, PROJECT)\n        self.assertIsNone(notification.custom_attributes)\n        self.assertIsNone(notification.event_types)\n        self.assertIsNone(notification.blob_name_prefix)\n        self.assertEqual(notification.payload_format, NONE_PAYLOAD_FORMAT)\n\n    def test_notification_explicit(self):\n        from google.cloud.storage.notification import (\n            BucketNotification,\n            OBJECT_FINALIZE_EVENT_TYPE,\n            OBJECT_DELETE_EVENT_TYPE,\n            JSON_API_V1_PAYLOAD_FORMAT,\n        )\n\n        PROJECT = \"PROJECT\"\n        BUCKET_NAME = \"BUCKET_NAME\"\n        TOPIC_NAME = \"TOPIC_NAME\"\n        TOPIC_ALT_PROJECT = \"topic-project-456\"\n        CUSTOM_ATTRIBUTES = {\"attr1\": \"value1\", \"attr2\": \"value2\"}\n        EVENT_TYPES = [OBJECT_FINALIZE_EVENT_TYPE, OBJECT_DELETE_EVENT_TYPE]\n        BLOB_NAME_PREFIX = \"blob-name-prefix/\"\n        client = self._make_client(project=PROJECT)\n        bucket = self._make_one(client, name=BUCKET_NAME)\n\n        notification = bucket.notification(\n            TOPIC_NAME,\n            topic_project=TOPIC_ALT_PROJECT,\n            custom_attributes=CUSTOM_ATTRIBUTES,\n            event_types=EVENT_TYPES,\n            blob_name_prefix=BLOB_NAME_PREFIX,\n            payload_format=JSON_API_V1_PAYLOAD_FORMAT,\n        )\n\n        self.assertIsInstance(notification, BucketNotification)\n        self.assertIs(notification.bucket, bucket)\n        self.assertEqual(notification.topic_project, TOPIC_ALT_PROJECT)\n        self.assertEqual(notification.custom_attributes, CUSTOM_ATTRIBUTES)\n        self.assertEqual(notification.event_types, EVENT_TYPES)\n        self.assertEqual(notification.blob_name_prefix, BLOB_NAME_PREFIX)\n        self.assertEqual(notification.payload_format, JSON_API_V1_PAYLOAD_FORMAT)\n\n    def test_bucket_name_value(self):\n        BUCKET_NAME = \"bucket-name\"\n        self._make_one(name=BUCKET_NAME)\n\n        bad_start_bucket_name = \"/testing123\"\n        with self.assertRaises(ValueError):\n            self._make_one(name=bad_start_bucket_name)\n\n        bad_end_bucket_name = \"testing123/\"\n        with self.assertRaises(ValueError):\n            self._make_one(name=bad_end_bucket_name)\n\n    def test_user_project(self):\n        BUCKET_NAME = \"name\"\n        USER_PROJECT = \"user-project-123\"\n        bucket = self._make_one(name=BUCKET_NAME)\n        bucket._user_project = USER_PROJECT\n        self.assertEqual(bucket.user_project, USER_PROJECT)\n\n    def test_exists_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        bucket_name = \"bucket-name\"\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        bucket = self._make_one(client, name=bucket_name)\n\n        self.assertFalse(bucket.exists())\n\n        expected_query_params = {\"fields\": \"name\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_exists_w_etag_match(self):\n        bucket_name = \"bucket-name\"\n        etag = \"kittens\"\n        api_response = {\"name\": bucket_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=bucket_name)\n\n        self.assertTrue(bucket.exists(if_etag_match=etag))\n\n        expected_query_params = {\n            \"fields\": \"name\",\n        }\n        expected_headers = {\n            \"If-Match\": etag,\n        }\n        client._get_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_exists_w_metageneration_match_w_timeout(self):\n        bucket_name = \"bucket-name\"\n        metageneration_number = 6\n        timeout = 42\n        api_response = {\"name\": bucket_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=bucket_name)\n\n        self.assertTrue(\n            bucket.exists(timeout=42, if_metageneration_match=metageneration_number)\n        )\n\n        expected_query_params = {\n            \"fields\": \"name\",\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_exists_hit_w_user_project_w_retry_w_explicit_client(self):\n        bucket_name = \"bucket-name\"\n        user_project = \"user-project-123\"\n        retry = mock.Mock(spec=[])\n        api_response = {\"name\": bucket_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(name=bucket_name, user_project=user_project)\n\n        self.assertTrue(bucket.exists(client=client, retry=retry))\n\n        expected_query_params = {\n            \"fields\": \"name\",\n            \"userProject\": user_project,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_acl_property(self):\n        from google.cloud.storage.acl import BucketACL\n\n        bucket = self._make_one()\n        acl = bucket.acl\n        self.assertIsInstance(acl, BucketACL)\n        self.assertIs(acl, bucket._acl)\n\n    def test_default_object_acl_property(self):\n        from google.cloud.storage.acl import DefaultObjectACL\n\n        bucket = self._make_one()\n        acl = bucket.default_object_acl\n        self.assertIsInstance(acl, DefaultObjectACL)\n        self.assertIs(acl, bucket._default_object_acl)\n\n    def test_path_no_name(self):\n        bucket = self._make_one()\n        self.assertRaises(ValueError, getattr, bucket, \"path\")\n\n    def test_path_w_name(self):\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(bucket.path, f\"/b/{NAME}\")\n\n    def test_get_blob_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n        from google.cloud.storage.blob import Blob\n\n        name = \"name\"\n        blob_name = \"nonesuch\"\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        bucket = self._make_one(client, name=name)\n\n        result = bucket.get_blob(blob_name)\n\n        self.assertIsNone(result)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\"projection\": \"noAcl\"}\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=mock.ANY,\n        )\n\n        target = client._get_resource.call_args[1][\"_target_object\"]\n        self.assertIsInstance(target, Blob)\n        self.assertIs(target.bucket, bucket)\n        self.assertEqual(target.name, blob_name)\n\n    def test_get_blob_hit_w_user_project(self):\n        from google.cloud.storage.blob import Blob\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        user_project = \"user-project-123\"\n        api_response = {\"name\": blob_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name, user_project=user_project)\n\n        blob = bucket.get_blob(blob_name, client=client)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"userProject\": user_project,\n            \"projection\": \"noAcl\",\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=blob,\n        )\n\n    def test_get_blob_hit_w_generation_w_soft_deleted(self):\n        from google.cloud.storage.blob import Blob\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        generation = 1512565576797178\n        api_response = {\"name\": blob_name, \"generation\": generation}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name)\n\n        blob = bucket.get_blob(blob_name, generation=generation, soft_deleted=True)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob.generation, generation)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"generation\": generation,\n            \"projection\": \"noAcl\",\n            \"softDeleted\": True,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=blob,\n        )\n\n    def test_get_blob_hit_w_generation_w_timeout(self):\n        from google.cloud.storage.blob import Blob\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        generation = 1512565576797178\n        timeout = 42\n        api_response = {\"name\": blob_name, \"generation\": generation}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name)\n\n        blob = bucket.get_blob(blob_name, generation=generation, timeout=timeout)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob.generation, generation)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"generation\": generation,\n            \"projection\": \"noAcl\",\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=blob,\n        )\n\n    def test_get_blob_w_etag_match_w_retry(self):\n        from google.cloud.storage.blob import Blob\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        etag = \"kittens\"\n        retry = mock.Mock(spec=[])\n        api_response = {\"name\": blob_name, \"etag\": etag}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name)\n\n        blob = bucket.get_blob(blob_name, if_etag_match=etag, retry=retry)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob.etag, etag)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n        }\n        expected_headers = {\n            \"If-Match\": etag,\n        }\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=blob,\n        )\n\n    def test_get_blob_w_generation_match_w_retry(self):\n        from google.cloud.storage.blob import Blob\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        generation = 1512565576797178\n        retry = mock.Mock(spec=[])\n        api_response = {\"name\": blob_name, \"generation\": generation}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name)\n\n        blob = bucket.get_blob(blob_name, if_generation_match=generation, retry=retry)\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob.generation, generation)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"ifGenerationMatch\": generation,\n            \"projection\": \"noAcl\",\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=blob,\n        )\n\n    def test_get_blob_hit_with_kwargs_w_explicit_client(self):\n        from google.cloud.storage.blob import Blob\n        from google.cloud.storage.blob import _get_encryption_headers\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        chunk_size = 1024 * 1024\n        key = b\"01234567890123456789012345678901\"  # 32 bytes\n        api_response = {\"name\": blob_name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(name=name)\n\n        blob = bucket.get_blob(\n            blob_name, client=client, encryption_key=key, chunk_size=chunk_size\n        )\n\n        self.assertIsInstance(blob, Blob)\n        self.assertIs(blob.bucket, bucket)\n        self.assertEqual(blob.name, blob_name)\n        self.assertEqual(blob.chunk_size, chunk_size)\n        self.assertEqual(blob._encryption_key, key)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n        }\n        expected_headers = _get_encryption_headers(key)\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=blob,\n        )\n\n    def test_list_blobs_w_defaults(self):\n        name = \"name\"\n        client = self._make_client()\n        client.list_blobs = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=name)\n\n        iterator = bucket.list_blobs()\n\n        self.assertIs(iterator, client.list_blobs.return_value)\n\n        expected_page_token = None\n        expected_max_results = None\n        expected_prefix = None\n        expected_delimiter = None\n        expected_match_glob = None\n        expected_start_offset = None\n        expected_end_offset = None\n        expected_include_trailing_delimiter = None\n        expected_versions = None\n        expected_projection = \"noAcl\"\n        expected_fields = None\n        expected_include_folders_as_prefixes = None\n        soft_deleted = None\n        page_size = None\n        client.list_blobs.assert_called_once_with(\n            bucket,\n            max_results=expected_max_results,\n            page_token=expected_page_token,\n            prefix=expected_prefix,\n            delimiter=expected_delimiter,\n            start_offset=expected_start_offset,\n            end_offset=expected_end_offset,\n            include_trailing_delimiter=expected_include_trailing_delimiter,\n            versions=expected_versions,\n            projection=expected_projection,\n            fields=expected_fields,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            match_glob=expected_match_glob,\n            include_folders_as_prefixes=expected_include_folders_as_prefixes,\n            soft_deleted=soft_deleted,\n            page_size=page_size,\n        )\n\n    def test_list_blobs_w_explicit(self):\n        name = \"name\"\n        max_results = 10\n        page_token = \"ABCD\"\n        prefix = \"subfolder\"\n        delimiter = \"/\"\n        match_glob = \"**txt\"\n        start_offset = \"c\"\n        end_offset = \"g\"\n        include_trailing_delimiter = True\n        include_folders_as_prefixes = True\n        versions = True\n        soft_deleted = True\n        page_size = 2\n        projection = \"full\"\n        fields = \"items/contentLanguage,nextPageToken\"\n        bucket = self._make_one(client=None, name=name)\n        other_client = self._make_client()\n        other_client.list_blobs = mock.Mock(spec=[])\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        iterator = bucket.list_blobs(\n            max_results=max_results,\n            page_token=page_token,\n            prefix=prefix,\n            delimiter=delimiter,\n            start_offset=start_offset,\n            end_offset=end_offset,\n            include_trailing_delimiter=include_trailing_delimiter,\n            versions=versions,\n            projection=projection,\n            fields=fields,\n            client=other_client,\n            timeout=timeout,\n            retry=retry,\n            match_glob=match_glob,\n            include_folders_as_prefixes=include_folders_as_prefixes,\n            soft_deleted=soft_deleted,\n            page_size=page_size,\n        )\n\n        self.assertIs(iterator, other_client.list_blobs.return_value)\n\n        expected_page_token = page_token\n        expected_max_results = max_results\n        expected_prefix = prefix\n        expected_delimiter = delimiter\n        expected_match_glob = match_glob\n        expected_start_offset = start_offset\n        expected_end_offset = end_offset\n        expected_include_trailing_delimiter = include_trailing_delimiter\n        expected_versions = versions\n        expected_projection = projection\n        expected_fields = fields\n        expected_include_folders_as_prefixes = include_folders_as_prefixes\n        expected_soft_deleted = soft_deleted\n        expected_page_size = page_size\n        other_client.list_blobs.assert_called_once_with(\n            bucket,\n            max_results=expected_max_results,\n            page_token=expected_page_token,\n            prefix=expected_prefix,\n            delimiter=expected_delimiter,\n            start_offset=expected_start_offset,\n            end_offset=expected_end_offset,\n            include_trailing_delimiter=expected_include_trailing_delimiter,\n            versions=expected_versions,\n            projection=expected_projection,\n            fields=expected_fields,\n            timeout=timeout,\n            retry=retry,\n            match_glob=expected_match_glob,\n            include_folders_as_prefixes=expected_include_folders_as_prefixes,\n            soft_deleted=expected_soft_deleted,\n            page_size=expected_page_size,\n        )\n\n    def test_list_notifications_w_defaults(self):\n        from google.cloud.storage.bucket import _item_to_notification\n\n        bucket_name = \"name\"\n        client = self._make_client()\n        client._list_resource = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=bucket_name)\n\n        iterator = bucket.list_notifications()\n\n        self.assertIs(iterator, client._list_resource.return_value)\n        self.assertIs(iterator.bucket, bucket)\n\n        expected_path = f\"/b/{bucket_name}/notificationConfigs\"\n        expected_item_to_value = _item_to_notification\n        client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_list_notifications_w_explicit(self):\n        from google.cloud.storage.bucket import _item_to_notification\n\n        bucket_name = \"name\"\n        other_client = self._make_client()\n        other_client._list_resource = mock.Mock(spec=[])\n        bucket = self._make_one(client=None, name=bucket_name)\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        iterator = bucket.list_notifications(\n            client=other_client,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.assertIs(iterator, other_client._list_resource.return_value)\n        self.assertIs(iterator.bucket, bucket)\n\n        expected_path = f\"/b/{bucket_name}/notificationConfigs\"\n        expected_item_to_value = _item_to_notification\n        other_client._list_resource.assert_called_once_with(\n            expected_path,\n            expected_item_to_value,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_get_notification_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        project = \"my-project-123\"\n        name = \"name\"\n        notification_id = \"1\"\n\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.side_effect = NotFound(\"testing\")\n        client.project = project\n        bucket = self._make_one(client=client, name=name)\n\n        with self.assertRaises(NotFound):\n            bucket.get_notification(notification_id=notification_id)\n\n        expected_path = f\"/b/{name}/notificationConfigs/{notification_id}\"\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_get_notification_hit_w_explicit_w_user_project(self):\n        from google.cloud.storage.notification import BucketNotification\n        from google.cloud.storage.notification import _TOPIC_REF_FMT\n        from google.cloud.storage.notification import JSON_API_V1_PAYLOAD_FORMAT\n\n        project = \"my-project-123\"\n        user_project = \"user-project-456\"\n        name = \"name\"\n        etag = \"FACECABB\"\n        notification_id = \"1\"\n        self_link = \"https://example.com/notification/1\"\n        api_response = {\n            \"topic\": _TOPIC_REF_FMT.format(\"my-project-123\", \"topic-1\"),\n            \"id\": notification_id,\n            \"etag\": etag,\n            \"selfLink\": self_link,\n            \"payload_format\": JSON_API_V1_PAYLOAD_FORMAT,\n        }\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        client = mock.Mock(spec=[\"_get_resource\", \"project\"])\n        client._get_resource.return_value = api_response\n        client.project = project\n        bucket = self._make_one(client=client, name=name, user_project=user_project)\n\n        notification = bucket.get_notification(\n            notification_id=notification_id,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.assertIsInstance(notification, BucketNotification)\n        self.assertEqual(notification.notification_id, notification_id)\n        self.assertEqual(notification.etag, etag)\n        self.assertEqual(notification.self_link, self_link)\n        self.assertIsNone(notification.custom_attributes)\n        self.assertIsNone(notification.event_types)\n        self.assertIsNone(notification.blob_name_prefix)\n        self.assertEqual(notification.payload_format, JSON_API_V1_PAYLOAD_FORMAT)\n\n        expected_path = f\"/b/{name}/notificationConfigs/{notification_id}\"\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_delete_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        name = \"name\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.side_effect = NotFound(\"testing\")\n        bucket = self._make_one(client=client, name=name)\n\n        with self.assertRaises(NotFound):\n            bucket.delete()\n\n        expected_query_params = {}\n        client._delete_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_delete_hit_w_metageneration_match_w_explicit_client(self):\n        name = \"name\"\n        metageneration_number = 6\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=None, name=name)\n\n        result = bucket.delete(\n            client=client,\n            if_metageneration_match=metageneration_number,\n        )\n\n        self.assertIsNone(result)\n\n        expected_query_params = {\"ifMetagenerationMatch\": metageneration_number}\n        client._delete_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_delete_hit_w_force_w_user_project_w_explicit_timeout_retry(self):\n        name = \"name\"\n        user_project = \"user-project-123\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name, user_project=user_project)\n        bucket.list_blobs = mock.Mock(return_value=iter([]))\n        bucket.delete_blobs = mock.Mock(return_value=None)\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        result = bucket.delete(force=True, timeout=timeout, retry=retry)\n\n        self.assertIsNone(result)\n\n        bucket.list_blobs.assert_called_once_with(\n            max_results=bucket._MAX_OBJECTS_FOR_ITERATION + 1,\n            client=client,\n            timeout=timeout,\n            retry=retry,\n            versions=True,\n        )\n\n        bucket.delete_blobs.assert_called_once_with(\n            [],\n            on_error=mock.ANY,\n            client=client,\n            timeout=timeout,\n            retry=retry,\n            preserve_generation=True,\n        )\n\n        expected_query_params = {\"userProject\": user_project}\n        client._delete_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_delete_hit_w_force_delete_blobs(self):\n        name = \"name\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name)\n        blobs = [mock.Mock(spec=[]), mock.Mock(spec=[])]\n        bucket.list_blobs = mock.Mock(return_value=iter(blobs))\n        bucket.delete_blobs = mock.Mock(return_value=None)\n\n        result = bucket.delete(force=True)\n\n        self.assertIsNone(result)\n\n        bucket.list_blobs.assert_called_once_with(\n            max_results=bucket._MAX_OBJECTS_FOR_ITERATION + 1,\n            client=client,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            versions=True,\n        )\n\n        bucket.delete_blobs.assert_called_once_with(\n            blobs,\n            on_error=mock.ANY,\n            client=client,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            preserve_generation=True,\n        )\n\n        expected_query_params = {}\n        client._delete_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_delete_w_force_w_user_project_w_miss_on_blob(self):\n        from google.cloud.exceptions import NotFound\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name)\n        blob = mock.Mock(spec=[\"name\", \"generation\"])\n        blob.name = blob_name\n        GEN = 1234\n        blob.generation = GEN\n        blobs = [blob]\n        bucket.list_blobs = mock.Mock(return_value=iter(blobs))\n        bucket.delete_blob = mock.Mock(side_effect=NotFound(\"testing\"))\n\n        result = bucket.delete(force=True)\n\n        self.assertIsNone(result)\n\n        bucket.delete_blob.assert_called_once_with(\n            blob_name,\n            client=client,\n            generation=GEN,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n        expected_query_params = {}\n        client._delete_resource.assert_called_once_with(\n            bucket.path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_delete_w_too_many(self):\n        name = \"name\"\n        blob_name1 = \"blob-name1\"\n        blob_name2 = \"blob-name2\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name)\n        blob1 = mock.Mock(spec=[\"name\"])\n        blob1.name = blob_name1\n        blob2 = mock.Mock(spec=[\"name\"])\n        blob2.name = blob_name2\n        blobs = [blob1, blob2]\n        bucket.list_blobs = mock.Mock(return_value=iter(blobs))\n        bucket.delete_blobs = mock.Mock()\n        # Make the Bucket refuse to delete with 2 objects.\n        bucket._MAX_OBJECTS_FOR_ITERATION = 1\n\n        with self.assertRaises(ValueError):\n            bucket.delete(force=True)\n\n        bucket.delete_blobs.assert_not_called()\n\n    def test_delete_blob_miss_w_defaults(self):\n        from google.cloud.exceptions import NotFound\n\n        name = \"name\"\n        blob_name = \"nonesuch\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.side_effect = NotFound(\"testing\")\n        bucket = self._make_one(client=client, name=name)\n\n        with self.assertRaises(NotFound):\n            bucket.delete_blob(blob_name)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {}\n        client._delete_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=None,\n        )\n\n    def test_delete_blob_hit_w_user_project_w_timeout(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        user_project = \"user-project-123\"\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name, user_project=user_project)\n        timeout = 42\n\n        result = bucket.delete_blob(blob_name, timeout=timeout)\n\n        self.assertIsNone(result)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\"userProject\": user_project}\n        client._delete_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=None,\n        )\n\n    def test_delete_blob_hit_w_generation_w_retry(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        generation = 1512565576797178\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name)\n        retry = mock.Mock(spec=[])\n\n        result = bucket.delete_blob(blob_name, generation=generation, retry=retry)\n\n        self.assertIsNone(result)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\"generation\": generation}\n        client._delete_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_delete_blob_hit_w_generation_match(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        generation = 6\n        metageneration = 9\n        client = mock.Mock(spec=[\"_delete_resource\"])\n        client._delete_resource.return_value = None\n        bucket = self._make_one(client=client, name=name)\n\n        result = bucket.delete_blob(\n            blob_name,\n            if_generation_match=generation,\n            if_metageneration_match=metageneration,\n        )\n\n        self.assertIsNone(result)\n\n        expected_path = f\"/b/{name}/o/{blob_name}\"\n        expected_query_params = {\n            \"ifGenerationMatch\": generation,\n            \"ifMetagenerationMatch\": metageneration,\n        }\n        client._delete_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=None,\n        )\n\n    def test_delete_blobs_empty(self):\n        name = \"name\"\n        bucket = self._make_one(client=None, name=name)\n        bucket.delete_blob = mock.Mock()\n\n        bucket.delete_blobs([])\n\n        bucket.delete_blob.assert_not_called()\n\n    def test_delete_blobs_hit_w_explicit_client_w_timeout(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        client = mock.Mock(spec=[])\n        bucket = self._make_one(client=None, name=name)\n        bucket.delete_blob = mock.Mock()\n        timeout = 42\n\n        bucket.delete_blobs([blob_name], client=client, timeout=timeout)\n\n        bucket.delete_blob.assert_called_once_with(\n            blob_name,\n            client=client,\n            generation=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n\n    def test_delete_blobs_w_generation_match_wrong_len(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        blob_name2 = \"blob-name2\"\n        generation_number = 6\n        bucket = self._make_one(client=None, name=name)\n        bucket.delete_blob = mock.Mock()\n\n        with self.assertRaises(ValueError):\n            bucket.delete_blobs(\n                [blob_name, blob_name2],\n                if_generation_not_match=[generation_number],\n            )\n\n        bucket.delete_blob.assert_not_called()\n\n    def test_delete_blobs_w_generation_match_w_retry(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        blob_name2 = \"blob-name2\"\n        generation_number = 6\n        generation_number2 = 9\n        client = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=name)\n        bucket.delete_blob = mock.Mock()\n        retry = mock.Mock(spec=[])\n\n        bucket.delete_blobs(\n            [blob_name, blob_name2],\n            if_generation_match=[generation_number, generation_number2],\n            retry=retry,\n        )\n\n        call_1 = mock.call(\n            blob_name,\n            client=None,\n            generation=None,\n            if_generation_match=generation_number,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n        )\n        call_2 = mock.call(\n            blob_name2,\n            client=None,\n            generation=None,\n            if_generation_match=generation_number2,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n        )\n        bucket.delete_blob.assert_has_calls([call_1, call_2])\n\n    def test_delete_blobs_w_generation_match_none(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        blob_name2 = \"blob-name2\"\n        generation_number = 6\n        generation_number2 = None\n        client = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=name)\n        bucket.delete_blob = mock.Mock()\n\n        bucket.delete_blobs(\n            [blob_name, blob_name2],\n            if_generation_match=[generation_number, generation_number2],\n        )\n\n        call_1 = mock.call(\n            blob_name,\n            client=None,\n            generation=None,\n            if_generation_match=generation_number,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n        call_2 = mock.call(\n            blob_name2,\n            client=None,\n            generation=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n        bucket.delete_blob.assert_has_calls([call_1, call_2])\n\n    def test_delete_blobs_w_preserve_generation(self):\n        name = \"name\"\n        blob_name = \"blob-name\"\n        blob_name2 = \"blob-name2\"\n        generation_number = 1234567890\n        generation_number2 = 7890123456\n        client = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=name)\n        blob = self._make_blob(bucket.name, blob_name)\n        blob.generation = generation_number\n        blob2 = self._make_blob(bucket.name, blob_name2)\n        blob2.generation = generation_number2\n        bucket.delete_blob = mock.Mock()\n        retry = mock.Mock(spec=[])\n\n        # Test generation is propagated from list of blob instances\n        bucket.delete_blobs(\n            [blob, blob2],\n            preserve_generation=True,\n            retry=retry,\n        )\n\n        call_1 = mock.call(\n            blob_name,\n            client=None,\n            generation=generation_number,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n        )\n        call_2 = mock.call(\n            blob_name2,\n            client=None,\n            generation=generation_number2,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n        )\n        bucket.delete_blob.assert_has_calls([call_1, call_2])\n\n    def test_delete_blobs_miss_wo_on_error(self):\n        from google.cloud.exceptions import NotFound\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        blob_name2 = \"nonesuch\"\n        client = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=name)\n        bucket.delete_blob = mock.Mock()\n        bucket.delete_blob.side_effect = [None, NotFound(\"testing\")]\n\n        with self.assertRaises(NotFound):\n            bucket.delete_blobs([blob_name, blob_name2])\n\n        call_1 = mock.call(\n            blob_name,\n            client=None,\n            generation=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n        call_2 = mock.call(\n            blob_name2,\n            client=None,\n            generation=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n        bucket.delete_blob.assert_has_calls([call_1, call_2])\n\n    def test_delete_blobs_miss_w_on_error(self):\n        from google.cloud.exceptions import NotFound\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        blob_name2 = \"nonesuch\"\n        client = mock.Mock(spec=[])\n        bucket = self._make_one(client=client, name=name)\n        bucket.delete_blob = mock.Mock()\n        bucket.delete_blob.side_effect = [None, NotFound(\"testing\")]\n\n        errors = []\n        bucket.delete_blobs([blob_name, blob_name2], on_error=errors.append)\n\n        self.assertEqual(errors, [blob_name2])\n\n        call_1 = mock.call(\n            blob_name,\n            client=None,\n            generation=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n        call_2 = mock.call(\n            blob_name2,\n            client=None,\n            generation=None,\n            if_generation_match=None,\n            if_generation_not_match=None,\n            if_metageneration_match=None,\n            if_metageneration_not_match=None,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n        bucket.delete_blob.assert_has_calls([call_1, call_2])\n\n    def test_reload_w_etag_match(self):\n        name = \"name\"\n        etag = \"kittens\"\n        api_response = {\"name\": name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name)\n\n        bucket.reload(if_etag_match=etag)\n\n        expected_path = f\"/b/{name}\"\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n        }\n        expected_headers = {\n            \"If-Match\": etag,\n        }\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_reload_w_metageneration_match(self):\n        name = \"name\"\n        metageneration_number = 9\n        api_response = {\"name\": name}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client, name=name)\n\n        bucket.reload(if_metageneration_match=metageneration_number)\n\n        expected_path = f\"/b/{name}\"\n        expected_query_params = {\n            \"projection\": \"noAcl\",\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        expected_headers = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            headers=expected_headers,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_reload_w_generation_match(self):\n        client = self._make_client()\n        bucket = self._make_one(client=client, name=\"name\")\n\n        with self.assertRaises(TypeError):\n            bucket.reload(if_generation_match=6)\n\n    def test_update_w_metageneration_match(self):\n        name = \"name\"\n        metageneration_number = 9\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = {}\n        bucket = self._make_one(client=client, name=name)\n\n        bucket.update(if_metageneration_match=metageneration_number)\n\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": metageneration_number,\n        }\n        client._put_resource.assert_called_once_with(\n            bucket.path,\n            bucket._properties,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=bucket,\n        )\n\n    def test_update_w_generation_match(self):\n        name = \"name\"\n        generation_number = 6\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = {}\n        bucket = self._make_one(client=client, name=name)\n\n        with self.assertRaises(TypeError):\n            bucket.update(if_generation_match=generation_number)\n\n        client._put_resource.assert_not_called()\n\n    @staticmethod\n    def _make_blob(bucket_name, blob_name):\n        from google.cloud.storage.blob import Blob\n\n        blob = mock.create_autospec(Blob)\n        blob.name = blob_name\n        blob.path = f\"/b/{bucket_name}/o/{blob_name}\"\n        return blob\n\n    def test_copy_blobs_wo_name(self):\n        source_name = \"source\"\n        dest_name = \"dest\"\n        blob_name = \"blob-name\"\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source = self._make_one(client=client, name=source_name)\n        dest = self._make_one(client=client, name=dest_name)\n        blob = self._make_blob(source_name, blob_name)\n\n        new_blob = source.copy_blob(blob, dest)\n\n        self.assertIs(new_blob.bucket, dest)\n        self.assertEqual(new_blob.name, blob_name)\n\n        expected_path = \"/b/{}/o/{}/copyTo/b/{}/o/{}\".format(\n            source_name, blob_name, dest_name, blob_name\n        )\n        expected_data = None\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=new_blob,\n        )\n\n    def test_copy_blob_w_source_generation_w_timeout(self):\n        source_name = \"source\"\n        dest_name = \"dest\"\n        blob_name = \"blob-name\"\n        generation = 1512565576797178\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source = self._make_one(client=client, name=source_name)\n        dest = self._make_one(client=client, name=dest_name)\n        blob = self._make_blob(source_name, blob_name)\n        timeout = 42\n\n        new_blob = source.copy_blob(\n            blob,\n            dest,\n            source_generation=generation,\n            timeout=timeout,\n        )\n\n        self.assertIs(new_blob.bucket, dest)\n        self.assertEqual(new_blob.name, blob_name)\n\n        expected_path = \"/b/{}/o/{}/copyTo/b/{}/o/{}\".format(\n            source_name, blob_name, dest_name, blob_name\n        )\n        expected_data = None\n        expected_query_params = {\"sourceGeneration\": generation}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=new_blob,\n        )\n\n    def test_copy_blob_w_generation_match_w_retry(self):\n        source_name = \"source\"\n        dest_name = \"dest\"\n        blob_name = \"blob-name\"\n        generation_number = 6\n        source_generation_number = 9\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source = self._make_one(client=client, name=source_name)\n        dest = self._make_one(client=client, name=dest_name)\n        blob = self._make_blob(source_name, blob_name)\n        retry = mock.Mock(spec=[])\n\n        new_blob = source.copy_blob(\n            blob,\n            dest,\n            if_generation_match=generation_number,\n            if_source_generation_match=source_generation_number,\n            retry=retry,\n        )\n        self.assertIs(new_blob.bucket, dest)\n        self.assertEqual(new_blob.name, blob_name)\n\n        expected_path = \"/b/{}/o/{}/copyTo/b/{}/o/{}\".format(\n            source_name, blob_name, dest_name, blob_name\n        )\n        expected_data = None\n        expected_query_params = {\n            \"ifGenerationMatch\": generation_number,\n            \"ifSourceGenerationMatch\": source_generation_number,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=new_blob,\n        )\n\n    def test_copy_blob_w_preserve_acl_false_w_explicit_client(self):\n        from google.cloud.storage.acl import ObjectACL\n\n        source_name = \"source\"\n        dest_name = \"dest\"\n        blob_name = \"blob-name\"\n        new_name = \"new_name\"\n        post_api_response = {}\n        patch_api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\", \"_patch_resource\"])\n        client._post_resource.return_value = post_api_response\n        client._patch_resource.return_value = patch_api_response\n        source = self._make_one(client=None, name=source_name)\n        dest = self._make_one(client=None, name=dest_name)\n        blob = self._make_blob(source_name, blob_name)\n\n        new_blob = source.copy_blob(\n            blob, dest, new_name, client=client, preserve_acl=False\n        )\n\n        self.assertIs(new_blob.bucket, dest)\n        self.assertEqual(new_blob.name, new_name)\n        self.assertIsInstance(new_blob.acl, ObjectACL)\n\n        expected_copy_path = \"/b/{}/o/{}/copyTo/b/{}/o/{}\".format(\n            source_name, blob_name, dest_name, new_name\n        )\n        expected_copy_data = None\n        expected_copy_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_copy_path,\n            expected_copy_data,\n            query_params=expected_copy_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=new_blob,\n        )\n\n        expected_patch_path = f\"/b/{dest_name}/o/{new_name}\"\n        expected_patch_data = {\"acl\": []}\n        expected_patch_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            expected_patch_path,\n            expected_patch_data,\n            query_params=expected_patch_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_copy_blob_w_name_and_user_project(self):\n        source_name = \"source\"\n        dest_name = \"dest\"\n        blob_name = \"blob-name\"\n        new_name = \"new_name\"\n        user_project = \"user-project-123\"\n        api_response = {}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        source = self._make_one(\n            client=client, name=source_name, user_project=user_project\n        )\n        dest = self._make_one(client=client, name=dest_name)\n        blob = self._make_blob(source_name, blob_name)\n\n        new_blob = source.copy_blob(blob, dest, new_name)\n\n        self.assertIs(new_blob.bucket, dest)\n        self.assertEqual(new_blob.name, new_name)\n\n        expected_path = \"/b/{}/o/{}/copyTo/b/{}/o/{}\".format(\n            source_name, blob_name, dest_name, new_name\n        )\n        expected_data = None\n        expected_query_params = {\"userProject\": user_project}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n            _target_object=new_blob,\n        )\n\n    def _rename_blob_helper(self, explicit_client=False, same_name=False, **kw):\n        bucket_name = \"BUCKET_NAME\"\n        blob_name = \"blob-name\"\n\n        if same_name:\n            new_blob_name = blob_name\n        else:\n            new_blob_name = \"new-blob-name\"\n\n        client = mock.Mock(spec=[])\n        kw = kw.copy()\n\n        if explicit_client:\n            bucket = self._make_one(client=None, name=bucket_name)\n            expected_client = kw[\"client\"] = client\n        else:\n            bucket = self._make_one(client=client, name=bucket_name)\n            expected_client = None\n\n        expected_i_g_m = kw.get(\"if_generation_match\")\n        expected_i_g_n_m = kw.get(\"if_generation_not_match\")\n        expected_i_m_m = kw.get(\"if_metageneration_match\")\n        expected_i_m_n_m = kw.get(\"if_metageneration_not_match\")\n        expected_i_s_g_m = kw.get(\"if_source_generation_match\")\n        expected_i_s_g_n_m = kw.get(\"if_source_generation_not_match\")\n        expected_i_s_m_m = kw.get(\"if_source_metageneration_match\")\n        expected_i_s_m_n_m = kw.get(\"if_source_metageneration_not_match\")\n        expected_timeout = kw.get(\"timeout\", self._get_default_timeout())\n        expected_retry = kw.get(\"retry\", DEFAULT_RETRY_IF_GENERATION_SPECIFIED)\n\n        bucket.copy_blob = mock.Mock(spec=[])\n        blob = self._make_blob(bucket_name, blob_name)\n\n        renamed_blob = bucket.rename_blob(blob, new_blob_name, **kw)\n\n        self.assertIs(renamed_blob, bucket.copy_blob.return_value)\n\n        bucket.copy_blob.assert_called_once_with(\n            blob,\n            bucket,\n            new_blob_name,\n            client=expected_client,\n            if_generation_match=expected_i_g_m,\n            if_generation_not_match=expected_i_g_n_m,\n            if_metageneration_match=expected_i_m_m,\n            if_metageneration_not_match=expected_i_m_n_m,\n            if_source_generation_match=expected_i_s_g_m,\n            if_source_generation_not_match=expected_i_s_g_n_m,\n            if_source_metageneration_match=expected_i_s_m_m,\n            if_source_metageneration_not_match=expected_i_s_m_n_m,\n            timeout=expected_timeout,\n            retry=expected_retry,\n        )\n\n        if same_name:\n            blob.delete.assert_not_called()\n        else:\n            blob.delete.assert_called_once_with(\n                client=expected_client,\n                if_generation_match=expected_i_s_g_m,\n                if_generation_not_match=expected_i_s_g_n_m,\n                if_metageneration_match=expected_i_s_m_m,\n                if_metageneration_not_match=expected_i_s_m_n_m,\n                timeout=expected_timeout,\n                retry=expected_retry,\n            )\n\n    def test_rename_blob_w_defaults(self):\n        self._rename_blob_helper()\n\n    def test_rename_blob_w_explicit_client(self):\n        self._rename_blob_helper(explicit_client=True)\n\n    def test_rename_blob_w_generation_match(self):\n        generation_number = 6\n        source_generation_number = 7\n        source_metageneration_number = 9\n\n        self._rename_blob_helper(\n            if_generation_match=generation_number,\n            if_source_generation_match=source_generation_number,\n            if_source_metageneration_not_match=source_metageneration_number,\n        )\n\n    def test_rename_blob_w_timeout(self):\n        timeout = 42\n        self._rename_blob_helper(timeout=timeout)\n\n    def test_rename_blob_w_retry(self):\n        retry = mock.Mock(spec={})\n        self._rename_blob_helper(retry=retry)\n\n    def test_rename_blob_to_itself(self):\n        self._rename_blob_helper(same_name=True)\n\n    def test_etag(self):\n        ETAG = \"ETAG\"\n        properties = {\"etag\": ETAG}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.etag, ETAG)\n\n    def test_id(self):\n        ID = \"ID\"\n        properties = {\"id\": ID}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.id, ID)\n\n    def test_location_getter(self):\n        NAME = \"name\"\n        before = {\"location\": \"AS\"}\n        bucket = self._make_one(name=NAME, properties=before)\n        self.assertEqual(bucket.location, \"AS\")\n\n    @mock.patch(\"warnings.warn\")\n    def test_location_setter(self, mock_warn):\n        from google.cloud.storage import bucket as bucket_module\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        self.assertIsNone(bucket.location)\n        bucket.location = \"AS\"\n        self.assertEqual(bucket.location, \"AS\")\n        self.assertTrue(\"location\" in bucket._changes)\n        mock_warn.assert_called_once_with(\n            bucket_module._LOCATION_SETTER_MESSAGE, DeprecationWarning, stacklevel=2\n        )\n\n    def test_iam_configuration_policy_missing(self):\n        from google.cloud.storage.bucket import IAMConfiguration\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n\n        config = bucket.iam_configuration\n\n        self.assertIsInstance(config, IAMConfiguration)\n        self.assertIs(config.bucket, bucket)\n        self.assertFalse(config.bucket_policy_only_enabled)\n        self.assertIsNone(config.bucket_policy_only_locked_time)\n\n    def test_iam_configuration_policy_w_entry(self):\n        from google.cloud._helpers import _datetime_to_rfc3339\n        from google.cloud.storage.bucket import IAMConfiguration\n\n        now = _NOW(_UTC)\n        NAME = \"name\"\n        properties = {\n            \"iamConfiguration\": {\n                \"uniformBucketLevelAccess\": {\n                    \"enabled\": True,\n                    \"lockedTime\": _datetime_to_rfc3339(now),\n                }\n            }\n        }\n        bucket = self._make_one(name=NAME, properties=properties)\n\n        config = bucket.iam_configuration\n\n        self.assertIsInstance(config, IAMConfiguration)\n        self.assertIs(config.bucket, bucket)\n        self.assertTrue(config.uniform_bucket_level_access_enabled)\n        self.assertEqual(config.uniform_bucket_level_access_locked_time, now)\n\n    @mock.patch(\"warnings.warn\")\n    def test_lifecycle_rules_getter_unknown_action_type(self, mock_warn):\n        NAME = \"name\"\n        BOGUS_RULE = {\"action\": {\"type\": \"Bogus\"}, \"condition\": {\"age\": 42}}\n        rules = [BOGUS_RULE]\n        properties = {\"lifecycle\": {\"rule\": rules}}\n        bucket = self._make_one(name=NAME, properties=properties)\n\n        list(bucket.lifecycle_rules)\n        mock_warn.assert_called_with(\n            \"Unknown lifecycle rule type received: {}. Please upgrade to the latest version of google-cloud-storage.\".format(\n                BOGUS_RULE\n            ),\n            UserWarning,\n            stacklevel=1,\n        )\n\n    def test_lifecycle_rules_getter(self):\n        from google.cloud.storage.bucket import (\n            LifecycleRuleDelete,\n            LifecycleRuleSetStorageClass,\n            LifecycleRuleAbortIncompleteMultipartUpload,\n        )\n\n        NAME = \"name\"\n        DELETE_RULE = {\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 42}}\n        SSC_RULE = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n            \"condition\": {\"isLive\": False},\n        }\n        MULTIPART_RULE = {\n            \"action\": {\"type\": \"AbortIncompleteMultipartUpload\"},\n            \"condition\": {\"age\": 42},\n        }\n        rules = [DELETE_RULE, SSC_RULE, MULTIPART_RULE]\n        properties = {\"lifecycle\": {\"rule\": rules}}\n        bucket = self._make_one(name=NAME, properties=properties)\n\n        found = list(bucket.lifecycle_rules)\n\n        delete_rule = found[0]\n        self.assertIsInstance(delete_rule, LifecycleRuleDelete)\n        self.assertEqual(dict(delete_rule), DELETE_RULE)\n\n        ssc_rule = found[1]\n        self.assertIsInstance(ssc_rule, LifecycleRuleSetStorageClass)\n        self.assertEqual(dict(ssc_rule), SSC_RULE)\n\n        multipart_rule = found[2]\n        self.assertIsInstance(\n            multipart_rule, LifecycleRuleAbortIncompleteMultipartUpload\n        )\n        self.assertEqual(dict(multipart_rule), MULTIPART_RULE)\n\n    def test_lifecycle_rules_setter_w_dicts(self):\n        NAME = \"name\"\n        DELETE_RULE = {\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 42}}\n        SSC_RULE = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n            \"condition\": {\"isLive\": False},\n        }\n        rules = [DELETE_RULE, SSC_RULE]\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(list(bucket.lifecycle_rules), [])\n\n        bucket.lifecycle_rules = rules\n\n        self.assertEqual([dict(rule) for rule in bucket.lifecycle_rules], rules)\n        self.assertTrue(\"lifecycle\" in bucket._changes)\n\n    def test_lifecycle_rules_setter_w_helpers(self):\n        from google.cloud.storage.bucket import (\n            LifecycleRuleDelete,\n            LifecycleRuleSetStorageClass,\n        )\n\n        NAME = \"name\"\n        DELETE_RULE = {\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 42}}\n        SSC_RULE = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n            \"condition\": {\"isLive\": False},\n        }\n        rules = [DELETE_RULE, SSC_RULE]\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(list(bucket.lifecycle_rules), [])\n\n        bucket.lifecycle_rules = [\n            LifecycleRuleDelete(age=42),\n            LifecycleRuleSetStorageClass(\"NEARLINE\", is_live=False),\n        ]\n\n        self.assertEqual([dict(rule) for rule in bucket.lifecycle_rules], rules)\n        self.assertTrue(\"lifecycle\" in bucket._changes)\n\n    def test_clear_lifecycle_rules(self):\n        NAME = \"name\"\n        DELETE_RULE = {\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 42}}\n        SSC_RULE = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n            \"condition\": {\"isLive\": False},\n        }\n        rules = [DELETE_RULE, SSC_RULE]\n        bucket = self._make_one(name=NAME)\n        bucket._properties[\"lifecycle\"] = {\"rule\": rules}\n        self.assertEqual(list(bucket.lifecycle_rules), rules)\n\n        # This is a deprecated alias and will test both methods\n        bucket.clear_lifecyle_rules()\n\n        self.assertEqual(list(bucket.lifecycle_rules), [])\n        self.assertTrue(\"lifecycle\" in bucket._changes)\n\n    def test_add_lifecycle_delete_rule(self):\n        NAME = \"name\"\n        DELETE_RULE = {\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 42}}\n        rules = [DELETE_RULE]\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(list(bucket.lifecycle_rules), [])\n\n        bucket.add_lifecycle_delete_rule(age=42)\n\n        self.assertEqual([dict(rule) for rule in bucket.lifecycle_rules], rules)\n        self.assertTrue(\"lifecycle\" in bucket._changes)\n\n    def test_add_lifecycle_set_storage_class_rule(self):\n        NAME = \"name\"\n        SSC_RULE = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": \"NEARLINE\"},\n            \"condition\": {\"isLive\": False},\n        }\n        rules = [SSC_RULE]\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(list(bucket.lifecycle_rules), [])\n\n        bucket.add_lifecycle_set_storage_class_rule(\"NEARLINE\", is_live=False)\n\n        self.assertEqual([dict(rule) for rule in bucket.lifecycle_rules], rules)\n        self.assertTrue(\"lifecycle\" in bucket._changes)\n\n    def test_add_lifecycle_abort_incomplete_multipart_upload_rule(self):\n        NAME = \"name\"\n        AIMPU_RULE = {\n            \"action\": {\"type\": \"AbortIncompleteMultipartUpload\"},\n            \"condition\": {\"age\": 42},\n        }\n        rules = [AIMPU_RULE]\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(list(bucket.lifecycle_rules), [])\n\n        bucket.add_lifecycle_abort_incomplete_multipart_upload_rule(age=42)\n\n        self.assertEqual([dict(rule) for rule in bucket.lifecycle_rules], rules)\n        self.assertTrue(\"lifecycle\" in bucket._changes)\n\n    def test_cors_getter(self):\n        NAME = \"name\"\n        CORS_ENTRY = {\n            \"maxAgeSeconds\": 1234,\n            \"method\": [\"OPTIONS\", \"GET\"],\n            \"origin\": [\"127.0.0.1\"],\n            \"responseHeader\": [\"Content-Type\"],\n        }\n        properties = {\"cors\": [CORS_ENTRY, {}]}\n        bucket = self._make_one(name=NAME, properties=properties)\n        entries = bucket.cors\n        self.assertEqual(len(entries), 2)\n        self.assertEqual(entries[0], CORS_ENTRY)\n        self.assertEqual(entries[1], {})\n        # Make sure it was a copy, not the same object.\n        self.assertIsNot(entries[0], CORS_ENTRY)\n\n    def test_cors_setter(self):\n        NAME = \"name\"\n        CORS_ENTRY = {\n            \"maxAgeSeconds\": 1234,\n            \"method\": [\"OPTIONS\", \"GET\"],\n            \"origin\": [\"127.0.0.1\"],\n            \"responseHeader\": [\"Content-Type\"],\n        }\n        bucket = self._make_one(name=NAME)\n\n        self.assertEqual(bucket.cors, [])\n        bucket.cors = [CORS_ENTRY]\n        self.assertEqual(bucket.cors, [CORS_ENTRY])\n        self.assertTrue(\"cors\" in bucket._changes)\n\n    def test_default_kms_key_name_getter(self):\n        NAME = \"name\"\n        KMS_RESOURCE = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        ENCRYPTION_CONFIG = {\"defaultKmsKeyName\": KMS_RESOURCE}\n        bucket = self._make_one(name=NAME)\n        self.assertIsNone(bucket.default_kms_key_name)\n        bucket._properties[\"encryption\"] = ENCRYPTION_CONFIG\n        self.assertEqual(bucket.default_kms_key_name, KMS_RESOURCE)\n\n    def test_default_kms_key_name_setter(self):\n        NAME = \"name\"\n        KMS_RESOURCE = (\n            \"projects/test-project-123/\"\n            \"locations/us/\"\n            \"keyRings/test-ring/\"\n            \"cryptoKeys/test-key\"\n        )\n        ENCRYPTION_CONFIG = {\"defaultKmsKeyName\": KMS_RESOURCE}\n        bucket = self._make_one(name=NAME)\n        bucket.default_kms_key_name = KMS_RESOURCE\n        self.assertEqual(bucket._properties[\"encryption\"], ENCRYPTION_CONFIG)\n        self.assertTrue(\"encryption\" in bucket._changes)\n\n    def test_labels_getter(self):\n        NAME = \"name\"\n        LABELS = {\"color\": \"red\", \"flavor\": \"cherry\"}\n        properties = {\"labels\": LABELS}\n        bucket = self._make_one(name=NAME, properties=properties)\n        labels = bucket.labels\n        self.assertEqual(labels, LABELS)\n        # Make sure it was a copy, not the same object.\n        self.assertIsNot(labels, LABELS)\n\n    def test_labels_setter(self):\n        NAME = \"name\"\n        LABELS = {\"color\": \"red\", \"flavor\": \"cherry\"}\n        bucket = self._make_one(name=NAME)\n\n        self.assertEqual(bucket.labels, {})\n        bucket.labels = LABELS\n        self.assertEqual(bucket.labels, LABELS)\n        self.assertIsNot(bucket._properties[\"labels\"], LABELS)\n        self.assertIn(\"labels\", bucket._changes)\n\n    def test_labels_setter_with_nan(self):\n        NAME = \"name\"\n        LABELS = {\"color\": \"red\", \"foo\": float(\"nan\")}\n        bucket = self._make_one(name=NAME)\n\n        self.assertEqual(bucket.labels, {})\n        bucket.labels = LABELS\n        value = bucket.labels[\"foo\"]\n        self.assertIsInstance(value, str)\n\n    def test_labels_setter_with_removal(self):\n        # Make sure the bucket labels look correct and follow the expected\n        # public structure.\n        bucket = self._make_one(name=\"name\")\n        self.assertEqual(bucket.labels, {})\n        bucket.labels = {\"color\": \"red\", \"flavor\": \"cherry\"}\n        self.assertEqual(bucket.labels, {\"color\": \"red\", \"flavor\": \"cherry\"})\n        bucket.labels = {\"color\": \"red\"}\n        self.assertEqual(bucket.labels, {\"color\": \"red\"})\n\n        # Make sure that a patch call correctly removes the flavor label.\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = {}\n\n        bucket.patch(client=client)\n\n        expected_patch_data = {\n            \"labels\": {\"color\": \"red\", \"flavor\": None},\n        }\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            bucket.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=bucket,\n        )\n\n        # A second patch call should be a no-op for labels.\n        client._patch_resource.reset_mock()\n\n        bucket.patch(client=client, timeout=42)\n\n        expected_patch_data = {}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            bucket.path,\n            expected_patch_data,\n            query_params=expected_query_params,\n            timeout=42,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n            _target_object=bucket,\n        )\n\n    def test_location_type_getter_unset(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.location_type)\n\n    def test_location_type_getter_set(self):\n        from google.cloud.storage.constants import REGION_LOCATION_TYPE\n\n        properties = {\"locationType\": REGION_LOCATION_TYPE}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.location_type, REGION_LOCATION_TYPE)\n\n    def test_rpo_getter_and_setter(self):\n        bucket = self._make_one()\n        bucket.rpo = RPO_ASYNC_TURBO\n        self.assertEqual(bucket.rpo, RPO_ASYNC_TURBO)\n        bucket.rpo = RPO_DEFAULT\n        self.assertIn(\"rpo\", bucket._changes)\n        self.assertEqual(bucket.rpo, RPO_DEFAULT)\n\n    def test_autoclass_enabled_getter_and_setter(self):\n        properties = {\"autoclass\": {\"enabled\": True}}\n        bucket = self._make_one(properties=properties)\n        self.assertTrue(bucket.autoclass_enabled)\n        bucket.autoclass_enabled = False\n        self.assertIn(\"autoclass\", bucket._changes)\n        self.assertFalse(bucket.autoclass_enabled)\n\n    def test_autoclass_config_unset(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.autoclass_toggle_time)\n        self.assertIsNone(bucket.autoclass_terminal_storage_class)\n        self.assertIsNone(bucket.autoclass_terminal_storage_class_update_time)\n\n        properties = {\"autoclass\": {}}\n        bucket = self._make_one(properties=properties)\n        self.assertIsNone(bucket.autoclass_toggle_time)\n        self.assertIsNone(bucket.autoclass_terminal_storage_class)\n        self.assertIsNone(bucket.autoclass_terminal_storage_class_update_time)\n\n    def test_autoclass_toggle_and_tsc_update_time(self):\n        from google.cloud._helpers import _datetime_to_rfc3339\n\n        effective_time = _NOW(_UTC)\n        properties = {\n            \"autoclass\": {\n                \"enabled\": True,\n                \"toggleTime\": _datetime_to_rfc3339(effective_time),\n                \"terminalStorageClass\": \"NEARLINE\",\n                \"terminalStorageClassUpdateTime\": _datetime_to_rfc3339(effective_time),\n            }\n        }\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.autoclass_toggle_time, effective_time)\n        self.assertEqual(\n            bucket.autoclass_terminal_storage_class_update_time, effective_time\n        )\n\n    def test_autoclass_tsc_getter_and_setter(self):\n        from google.cloud.storage import constants\n\n        properties = {\n            \"autoclass\": {\"terminalStorageClass\": constants.ARCHIVE_STORAGE_CLASS}\n        }\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(\n            bucket.autoclass_terminal_storage_class, constants.ARCHIVE_STORAGE_CLASS\n        )\n        bucket.autoclass_terminal_storage_class = constants.NEARLINE_STORAGE_CLASS\n        self.assertIn(\"autoclass\", bucket._changes)\n        self.assertEqual(\n            bucket.autoclass_terminal_storage_class, constants.NEARLINE_STORAGE_CLASS\n        )\n\n    def test_get_logging_w_prefix(self):\n        NAME = \"name\"\n        LOG_BUCKET = \"logs\"\n        LOG_PREFIX = \"pfx\"\n        before = {\"logging\": {\"logBucket\": LOG_BUCKET, \"logObjectPrefix\": LOG_PREFIX}}\n        bucket = self._make_one(name=NAME, properties=before)\n        info = bucket.get_logging()\n        self.assertEqual(info[\"logBucket\"], LOG_BUCKET)\n        self.assertEqual(info[\"logObjectPrefix\"], LOG_PREFIX)\n\n    def test_enable_logging_defaults(self):\n        NAME = \"name\"\n        LOG_BUCKET = \"logs\"\n        before = {\"logging\": None}\n        bucket = self._make_one(name=NAME, properties=before)\n        self.assertIsNone(bucket.get_logging())\n        bucket.enable_logging(LOG_BUCKET)\n        info = bucket.get_logging()\n        self.assertEqual(info[\"logBucket\"], LOG_BUCKET)\n        self.assertEqual(info[\"logObjectPrefix\"], \"\")\n\n    def test_enable_logging(self):\n        NAME = \"name\"\n        LOG_BUCKET = \"logs\"\n        LOG_PFX = \"pfx\"\n        before = {\"logging\": None}\n        bucket = self._make_one(name=NAME, properties=before)\n        self.assertIsNone(bucket.get_logging())\n        bucket.enable_logging(LOG_BUCKET, LOG_PFX)\n        info = bucket.get_logging()\n        self.assertEqual(info[\"logBucket\"], LOG_BUCKET)\n        self.assertEqual(info[\"logObjectPrefix\"], LOG_PFX)\n\n    def test_disable_logging(self):\n        NAME = \"name\"\n        before = {\"logging\": {\"logBucket\": \"logs\", \"logObjectPrefix\": \"pfx\"}}\n        bucket = self._make_one(name=NAME, properties=before)\n        self.assertIsNotNone(bucket.get_logging())\n        bucket.disable_logging()\n        self.assertIsNone(bucket.get_logging())\n\n    def test_metageneration(self):\n        METAGENERATION = 42\n        properties = {\"metageneration\": METAGENERATION}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.metageneration, METAGENERATION)\n\n    def test_metageneration_unset(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.metageneration)\n\n    def test_metageneration_string_val(self):\n        METAGENERATION = 42\n        properties = {\"metageneration\": str(METAGENERATION)}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.metageneration, METAGENERATION)\n\n    def test_owner(self):\n        OWNER = {\"entity\": \"project-owner-12345\", \"entityId\": \"23456\"}\n        properties = {\"owner\": OWNER}\n        bucket = self._make_one(properties=properties)\n        owner = bucket.owner\n        self.assertEqual(owner[\"entity\"], \"project-owner-12345\")\n        self.assertEqual(owner[\"entityId\"], \"23456\")\n\n    def test_project_number(self):\n        PROJECT_NUMBER = 12345\n        properties = {\"projectNumber\": PROJECT_NUMBER}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.project_number, PROJECT_NUMBER)\n\n    def test_project_number_unset(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.project_number)\n\n    def test_project_number_string_val(self):\n        PROJECT_NUMBER = 12345\n        properties = {\"projectNumber\": str(PROJECT_NUMBER)}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.project_number, PROJECT_NUMBER)\n\n    def test_retention_policy_effective_time_policy_missing(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.retention_policy_effective_time)\n\n    def test_retention_policy_effective_time_et_missing(self):\n        properties = {\"retentionPolicy\": {}}\n        bucket = self._make_one(properties=properties)\n\n        self.assertIsNone(bucket.retention_policy_effective_time)\n\n    def test_retention_policy_effective_time(self):\n        from google.cloud._helpers import _datetime_to_rfc3339\n\n        effective_time = _NOW(_UTC)\n        properties = {\n            \"retentionPolicy\": {\"effectiveTime\": _datetime_to_rfc3339(effective_time)}\n        }\n        bucket = self._make_one(properties=properties)\n\n        self.assertEqual(bucket.retention_policy_effective_time, effective_time)\n\n    def test_retention_policy_locked_missing(self):\n        bucket = self._make_one()\n        self.assertFalse(bucket.retention_policy_locked)\n\n    def test_retention_policy_locked_false(self):\n        properties = {\"retentionPolicy\": {\"isLocked\": False}}\n        bucket = self._make_one(properties=properties)\n        self.assertFalse(bucket.retention_policy_locked)\n\n    def test_retention_policy_locked_true(self):\n        properties = {\"retentionPolicy\": {\"isLocked\": True}}\n        bucket = self._make_one(properties=properties)\n        self.assertTrue(bucket.retention_policy_locked)\n\n    def test_retention_period_getter_policymissing(self):\n        bucket = self._make_one()\n\n        self.assertIsNone(bucket.retention_period)\n\n    def test_retention_period_getter_pr_missing(self):\n        properties = {\"retentionPolicy\": {}}\n        bucket = self._make_one(properties=properties)\n\n        self.assertIsNone(bucket.retention_period)\n\n    def test_retention_period_getter(self):\n        period = 86400 * 100  # 100 days\n        properties = {\"retentionPolicy\": {\"retentionPeriod\": str(period)}}\n        bucket = self._make_one(properties=properties)\n\n        self.assertEqual(bucket.retention_period, period)\n\n    def test_retention_period_setter_w_none(self):\n        period = 86400 * 100  # 100 days\n        bucket = self._make_one()\n        bucket._properties[\"retentionPolicy\"] = {\"retentionPeriod\": period}\n\n        bucket.retention_period = None\n\n        self.assertIsNone(bucket._properties[\"retentionPolicy\"])\n\n    def test_retention_period_setter_w_int(self):\n        period = 86400 * 100  # 100 days\n        bucket = self._make_one()\n\n        bucket.retention_period = period\n\n        self.assertEqual(\n            bucket._properties[\"retentionPolicy\"][\"retentionPeriod\"], str(period)\n        )\n\n    def test_self_link(self):\n        SELF_LINK = \"http://example.com/self/\"\n        properties = {\"selfLink\": SELF_LINK}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.self_link, SELF_LINK)\n\n    def test_storage_class_getter(self):\n        from google.cloud.storage.constants import NEARLINE_STORAGE_CLASS\n\n        properties = {\"storageClass\": NEARLINE_STORAGE_CLASS}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.storage_class, NEARLINE_STORAGE_CLASS)\n\n    def test_storage_class_setter_invalid(self):\n        invalid_class = \"BOGUS\"\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = invalid_class\n\n        # Test that invalid classes are allowed without client side validation.\n        # Fall back to server side validation and errors.\n        self.assertEqual(bucket.storage_class, invalid_class)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_STANDARD(self):\n        from google.cloud.storage.constants import STANDARD_STORAGE_CLASS\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = STANDARD_STORAGE_CLASS\n        self.assertEqual(bucket.storage_class, STANDARD_STORAGE_CLASS)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_NEARLINE(self):\n        from google.cloud.storage.constants import NEARLINE_STORAGE_CLASS\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = NEARLINE_STORAGE_CLASS\n        self.assertEqual(bucket.storage_class, NEARLINE_STORAGE_CLASS)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_COLDLINE(self):\n        from google.cloud.storage.constants import COLDLINE_STORAGE_CLASS\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = COLDLINE_STORAGE_CLASS\n        self.assertEqual(bucket.storage_class, COLDLINE_STORAGE_CLASS)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_ARCHIVE(self):\n        from google.cloud.storage.constants import ARCHIVE_STORAGE_CLASS\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = ARCHIVE_STORAGE_CLASS\n        self.assertEqual(bucket.storage_class, ARCHIVE_STORAGE_CLASS)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_MULTI_REGIONAL(self):\n        from google.cloud.storage.constants import MULTI_REGIONAL_LEGACY_STORAGE_CLASS\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = MULTI_REGIONAL_LEGACY_STORAGE_CLASS\n        self.assertEqual(bucket.storage_class, MULTI_REGIONAL_LEGACY_STORAGE_CLASS)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_REGIONAL(self):\n        from google.cloud.storage.constants import REGIONAL_LEGACY_STORAGE_CLASS\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = REGIONAL_LEGACY_STORAGE_CLASS\n        self.assertEqual(bucket.storage_class, REGIONAL_LEGACY_STORAGE_CLASS)\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_storage_class_setter_DURABLE_REDUCED_AVAILABILITY(self):\n        from google.cloud.storage.constants import (\n            DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS,\n        )\n\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        bucket.storage_class = DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS\n        self.assertEqual(\n            bucket.storage_class, DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS\n        )\n        self.assertTrue(\"storageClass\" in bucket._changes)\n\n    def test_time_created(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        TIMESTAMP = datetime.datetime(2014, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        TIME_CREATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"timeCreated\": TIME_CREATED}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.time_created, TIMESTAMP)\n\n    def test_time_created_unset(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.time_created)\n\n    def test_updated(self):\n        from google.cloud._helpers import _RFC3339_MICROS\n\n        TIMESTAMP = datetime.datetime(2023, 11, 5, 20, 34, 37, tzinfo=_UTC)\n        UPDATED = TIMESTAMP.strftime(_RFC3339_MICROS)\n        properties = {\"updated\": UPDATED}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.updated, TIMESTAMP)\n\n    def test_updated_unset(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.updated)\n\n    def test_versioning_enabled_getter_missing(self):\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(bucket.versioning_enabled, False)\n\n    def test_versioning_enabled_getter(self):\n        NAME = \"name\"\n        before = {\"versioning\": {\"enabled\": True}}\n        bucket = self._make_one(name=NAME, properties=before)\n        self.assertEqual(bucket.versioning_enabled, True)\n\n    def test_create_w_defaults(self):\n        bucket_name = \"bucket-name\"\n        api_response = {\"name\": bucket_name}\n        client = mock.Mock(spec=[\"create_bucket\"])\n        client.create_bucket.return_value = api_response\n        bucket = self._make_one(client=client, name=bucket_name)\n\n        bucket.create()\n\n        client.create_bucket.assert_called_once_with(\n            bucket_or_name=bucket,\n            project=None,\n            user_project=None,\n            location=None,\n            predefined_acl=None,\n            predefined_default_object_acl=None,\n            enable_object_retention=False,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n        )\n\n    def test_create_w_explicit(self):\n        project = \"PROJECT\"\n        location = \"eu\"\n        user_project = \"USER_PROJECT\"\n        bucket_name = \"bucket-name\"\n        predefined_acl = \"authenticatedRead\"\n        predefined_default_object_acl = \"bucketOwnerFullControl\"\n        enable_object_retention = True\n        api_response = {\"name\": bucket_name}\n        client = mock.Mock(spec=[\"create_bucket\"])\n        client.create_bucket.return_value = api_response\n        bucket = self._make_one(client=None, name=bucket_name)\n        bucket._user_project = user_project\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        bucket.create(\n            client=client,\n            project=project,\n            location=location,\n            predefined_acl=predefined_acl,\n            predefined_default_object_acl=predefined_default_object_acl,\n            enable_object_retention=enable_object_retention,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        client.create_bucket.assert_called_once_with(\n            bucket_or_name=bucket,\n            project=project,\n            user_project=user_project,\n            location=location,\n            predefined_acl=predefined_acl,\n            predefined_default_object_acl=predefined_default_object_acl,\n            enable_object_retention=enable_object_retention,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def test_versioning_enabled_setter(self):\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        self.assertFalse(bucket.versioning_enabled)\n        bucket.versioning_enabled = True\n        self.assertTrue(bucket.versioning_enabled)\n\n    def test_requester_pays_getter_missing(self):\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        self.assertEqual(bucket.requester_pays, False)\n\n    def test_requester_pays_getter(self):\n        NAME = \"name\"\n        before = {\"billing\": {\"requesterPays\": True}}\n        bucket = self._make_one(name=NAME, properties=before)\n        self.assertEqual(bucket.requester_pays, True)\n\n    def test_requester_pays_setter(self):\n        NAME = \"name\"\n        bucket = self._make_one(name=NAME)\n        self.assertFalse(bucket.requester_pays)\n        bucket.requester_pays = True\n        self.assertTrue(bucket.requester_pays)\n\n    def test_object_retention_mode_getter(self):\n        bucket = self._make_one()\n        self.assertIsNone(bucket.object_retention_mode)\n        mode = \"Enabled\"\n        properties = {\"objectRetention\": {\"mode\": mode}}\n        bucket = self._make_one(properties=properties)\n        self.assertEqual(bucket.object_retention_mode, mode)\n\n    def test_soft_delete_policy_getter_w_entry(self):\n        from google.cloud.storage.bucket import SoftDeletePolicy\n        from google.cloud._helpers import _datetime_to_rfc3339\n\n        seconds = 86400 * 10  # 10 days\n        effective_time = _NOW(_UTC)\n        properties = {\n            \"softDeletePolicy\": {\n                \"retentionDurationSeconds\": seconds,\n                \"effectiveTime\": _datetime_to_rfc3339(effective_time),\n            }\n        }\n        bucket = self._make_one(properties=properties)\n\n        policy = SoftDeletePolicy(\n            bucket=bucket,\n            retention_duration_seconds=seconds,\n            effective_time=effective_time,\n        )\n        self.assertIsInstance(bucket.soft_delete_policy, SoftDeletePolicy)\n        self.assertEqual(bucket.soft_delete_policy, policy)\n        self.assertEqual(bucket.soft_delete_policy.retention_duration_seconds, seconds)\n        self.assertEqual(bucket.soft_delete_policy.effective_time, effective_time)\n\n    def test_soft_delete_policy_setter(self):\n        bucket = self._make_one()\n        policy = bucket.soft_delete_policy\n        self.assertIsNone(policy.retention_duration_seconds)\n        self.assertIsNone(policy.effective_time)\n\n        seconds = 86400 * 10  # 10 days\n        bucket.soft_delete_policy.retention_duration_seconds = seconds\n        self.assertTrue(\"softDeletePolicy\" in bucket._changes)\n        self.assertEqual(bucket.soft_delete_policy.retention_duration_seconds, seconds)\n\n    def test_hierarchical_namespace_enabled_getter_and_setter(self):\n        # Test hierarchical_namespace configuration unset\n        bucket = self._make_one()\n        self.assertIsNone(bucket.hierarchical_namespace_enabled)\n\n        # Test hierarchical_namespace configuration explicitly set\n        properties = {\"hierarchicalNamespace\": {\"enabled\": True}}\n        bucket = self._make_one(properties=properties)\n        self.assertTrue(bucket.hierarchical_namespace_enabled)\n        bucket.hierarchical_namespace_enabled = False\n        self.assertIn(\"hierarchicalNamespace\", bucket._changes)\n        self.assertFalse(bucket.hierarchical_namespace_enabled)\n\n    def test_configure_website_defaults(self):\n        NAME = \"name\"\n        UNSET = {\"website\": {\"mainPageSuffix\": None, \"notFoundPage\": None}}\n        bucket = self._make_one(name=NAME)\n        bucket.configure_website()\n        self.assertEqual(bucket._properties, UNSET)\n\n    def test_configure_website(self):\n        NAME = \"name\"\n        WEBSITE_VAL = {\n            \"website\": {\"mainPageSuffix\": \"html\", \"notFoundPage\": \"404.html\"}\n        }\n        bucket = self._make_one(name=NAME)\n        bucket.configure_website(\"html\", \"404.html\")\n        self.assertEqual(bucket._properties, WEBSITE_VAL)\n\n    def test_disable_website(self):\n        NAME = \"name\"\n        UNSET = {\"website\": {\"mainPageSuffix\": None, \"notFoundPage\": None}}\n        bucket = self._make_one(name=NAME)\n        bucket.disable_website()\n        self.assertEqual(bucket._properties, UNSET)\n\n    def test_get_iam_policy_defaults(self):\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n        from google.cloud.storage.iam import STORAGE_EDITOR_ROLE\n        from google.cloud.storage.iam import STORAGE_VIEWER_ROLE\n        from google.api_core.iam import Policy\n\n        bucket_name = \"name\"\n        path = f\"/b/{bucket_name}\"\n        etag = \"DEADBEEF\"\n        version = 1\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        editor1 = \"domain:google.com\"\n        editor2 = \"user:phred@example.com\"\n        viewer1 = \"serviceAccount:1234-abcdef@service.example.com\"\n        viewer2 = \"user:phred@example.com\"\n        api_response = {\n            \"resourceId\": path,\n            \"etag\": etag,\n            \"version\": version,\n            \"bindings\": [\n                {\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]},\n                {\"role\": STORAGE_EDITOR_ROLE, \"members\": [editor1, editor2]},\n                {\"role\": STORAGE_VIEWER_ROLE, \"members\": [viewer1, viewer2]},\n            ],\n        }\n        expected_policy = {\n            binding[\"role\"]: set(binding[\"members\"])\n            for binding in api_response[\"bindings\"]\n        }\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=bucket_name)\n\n        policy = bucket.get_iam_policy()\n\n        self.assertIsInstance(policy, Policy)\n        self.assertEqual(policy.etag, api_response[\"etag\"])\n        self.assertEqual(policy.version, api_response[\"version\"])\n        self.assertEqual(dict(policy), expected_policy)\n\n        expected_path = f\"/b/{bucket_name}/iam\"\n        expected_query_params = {}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_get_iam_policy_w_user_project_w_timeout(self):\n        from google.api_core.iam import Policy\n\n        bucket_name = \"name\"\n        timeout = 42\n        user_project = \"user-project-123\"\n        path = f\"/b/{bucket_name}\"\n        etag = \"DEADBEEF\"\n        version = 1\n        api_response = {\n            \"resourceId\": path,\n            \"etag\": etag,\n            \"version\": version,\n            \"bindings\": [],\n        }\n        expected_policy = {}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(\n            client=client, name=bucket_name, user_project=user_project\n        )\n\n        policy = bucket.get_iam_policy(timeout=timeout)\n\n        self.assertIsInstance(policy, Policy)\n        self.assertEqual(policy.etag, api_response[\"etag\"])\n        self.assertEqual(policy.version, api_response[\"version\"])\n        self.assertEqual(dict(policy), expected_policy)\n\n        expected_path = f\"/b/{bucket_name}/iam\"\n        expected_query_params = {\"userProject\": user_project}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_get_iam_policy_w_requested_policy_version_w_retry(self):\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n\n        bucket_name = \"name\"\n        path = f\"/b/{bucket_name}\"\n        etag = \"DEADBEEF\"\n        version = 3\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        api_response = {\n            \"resourceId\": path,\n            \"etag\": etag,\n            \"version\": version,\n            \"bindings\": [{\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]}],\n        }\n        retry = mock.Mock(spec=[])\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=bucket_name)\n\n        policy = bucket.get_iam_policy(requested_policy_version=3, retry=retry)\n\n        self.assertEqual(policy.version, version)\n\n        expected_path = f\"/b/{bucket_name}/iam\"\n        expected_query_params = {\"optionsRequestedPolicyVersion\": version}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_set_iam_policy_w_defaults(self):\n        import operator\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n        from google.cloud.storage.iam import STORAGE_EDITOR_ROLE\n        from google.cloud.storage.iam import STORAGE_VIEWER_ROLE\n        from google.api_core.iam import Policy\n\n        name = \"name\"\n        etag = \"DEADBEEF\"\n        version = 1\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        editor1 = \"domain:google.com\"\n        editor2 = \"user:phred@example.com\"\n        viewer1 = \"serviceAccount:1234-abcdef@service.example.com\"\n        viewer2 = \"user:phred@example.com\"\n        bindings = [\n            {\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]},\n            {\"role\": STORAGE_EDITOR_ROLE, \"members\": [editor1, editor2]},\n            {\"role\": STORAGE_VIEWER_ROLE, \"members\": [viewer1, viewer2]},\n        ]\n        policy = Policy()\n        for binding in bindings:\n            policy[binding[\"role\"]] = binding[\"members\"]\n\n        api_response = {\"etag\": etag, \"version\": version, \"bindings\": bindings}\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n\n        returned = bucket.set_iam_policy(policy)\n\n        self.assertEqual(returned.etag, etag)\n        self.assertEqual(returned.version, version)\n        self.assertEqual(dict(returned), dict(policy))\n\n        expected_path = f\"{bucket.path}/iam\"\n        expected_data = {\n            \"resourceId\": bucket.path,\n            \"bindings\": mock.ANY,\n        }\n        expected_query_params = {}\n        client._put_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n            _target_object=None,\n        )\n\n        sent_bindings = client._put_resource.call_args.args[1][\"bindings\"]\n        key = operator.itemgetter(\"role\")\n        for found, expected in zip(\n            sorted(sent_bindings, key=key), sorted(bindings, key=key)\n        ):\n            self.assertEqual(found[\"role\"], expected[\"role\"])\n            self.assertEqual(sorted(found[\"members\"]), sorted(expected[\"members\"]))\n\n    def test_set_iam_policy_w_user_project_w_expl_client_w_timeout_retry(self):\n        import operator\n        from google.cloud.storage.iam import STORAGE_OWNER_ROLE\n        from google.cloud.storage.iam import STORAGE_EDITOR_ROLE\n        from google.cloud.storage.iam import STORAGE_VIEWER_ROLE\n        from google.api_core.iam import Policy\n\n        name = \"name\"\n        user_project = \"user-project-123\"\n        etag = \"DEADBEEF\"\n        version = 1\n        owner1 = \"user:phred@example.com\"\n        owner2 = \"group:cloud-logs@google.com\"\n        editor1 = \"domain:google.com\"\n        editor2 = \"user:phred@example.com\"\n        viewer1 = \"serviceAccount:1234-abcdef@service.example.com\"\n        viewer2 = \"user:phred@example.com\"\n        bindings = [\n            {\"role\": STORAGE_OWNER_ROLE, \"members\": [owner1, owner2]},\n            {\"role\": STORAGE_EDITOR_ROLE, \"members\": [editor1, editor2]},\n            {\"role\": STORAGE_VIEWER_ROLE, \"members\": [viewer1, viewer2]},\n        ]\n        policy = Policy()\n        for binding in bindings:\n            policy[binding[\"role\"]] = binding[\"members\"]\n\n        api_response = {\"etag\": etag, \"version\": version, \"bindings\": bindings}\n        client = mock.Mock(spec=[\"_put_resource\"])\n        client._put_resource.return_value = api_response\n        bucket = self._make_one(client=None, name=name, user_project=user_project)\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        returned = bucket.set_iam_policy(\n            policy, client=client, timeout=timeout, retry=retry\n        )\n\n        self.assertEqual(returned.etag, etag)\n        self.assertEqual(returned.version, version)\n        self.assertEqual(dict(returned), dict(policy))\n\n        expected_path = f\"{bucket.path}/iam\"\n        expected_data = {\n            \"resourceId\": bucket.path,\n            \"bindings\": mock.ANY,\n        }\n        expected_query_params = {\"userProject\": user_project}\n        client._put_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n        sent_bindings = client._put_resource.call_args.args[1][\"bindings\"]\n        key = operator.itemgetter(\"role\")\n        for found, expected in zip(\n            sorted(sent_bindings, key=key), sorted(bindings, key=key)\n        ):\n            self.assertEqual(found[\"role\"], expected[\"role\"])\n            self.assertEqual(sorted(found[\"members\"]), sorted(expected[\"members\"]))\n\n    def test_test_iam_permissions_defaults(self):\n        from google.cloud.storage.iam import STORAGE_OBJECTS_LIST\n        from google.cloud.storage.iam import STORAGE_BUCKETS_GET\n        from google.cloud.storage.iam import STORAGE_BUCKETS_UPDATE\n\n        name = \"name\"\n        permissions = [\n            STORAGE_OBJECTS_LIST,\n            STORAGE_BUCKETS_GET,\n            STORAGE_BUCKETS_UPDATE,\n        ]\n        expected = permissions[1:]\n        api_response = {\"permissions\": expected}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n\n        found = bucket.test_iam_permissions(permissions)\n\n        self.assertEqual(found, expected)\n\n        expected_path = f\"/b/{name}/iam/testPermissions\"\n        expected_query_params = {}\n        expected_query_params = {\"permissions\": permissions}\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=None,\n        )\n\n    def test_test_iam_permissions_w_user_project_w_timeout_w_retry(self):\n        from google.cloud.storage.iam import STORAGE_OBJECTS_LIST\n        from google.cloud.storage.iam import STORAGE_BUCKETS_GET\n        from google.cloud.storage.iam import STORAGE_BUCKETS_UPDATE\n\n        name = \"name\"\n        user_project = \"user-project-123\"\n        timeout = 42\n        retry = mock.Mock(spec=[])\n        permissions = [\n            STORAGE_OBJECTS_LIST,\n            STORAGE_BUCKETS_GET,\n            STORAGE_BUCKETS_UPDATE,\n        ]\n        expected = permissions[1:]\n        api_response = {\"permissions\": expected}\n        client = mock.Mock(spec=[\"_get_resource\"])\n        client._get_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name, user_project=user_project)\n\n        found = bucket.test_iam_permissions(permissions, timeout=timeout, retry=retry)\n\n        self.assertEqual(found, expected)\n\n        expected_path = f\"/b/{name}/iam/testPermissions\"\n        expected_query_params = {\n            \"permissions\": permissions,\n            \"userProject\": user_project,\n        }\n        client._get_resource.assert_called_once_with(\n            expected_path,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def test_make_public_defaults(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        name = \"name\"\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n        api_response = {\"acl\": permissive, \"defaultObjectAcl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        bucket.make_public()\n\n        self.assertEqual(list(bucket.acl), permissive)\n        self.assertEqual(list(bucket.default_object_acl), [])\n\n        expected_path = bucket.path\n        expected_data = {\"acl\": permissive}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_public_w_preconditions(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        name = \"name\"\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n        api_response = {\"acl\": permissive, \"defaultObjectAcl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        bucket.make_public(if_metageneration_match=2, if_metageneration_not_match=1)\n\n        self.assertEqual(list(bucket.acl), permissive)\n        self.assertEqual(list(bucket.default_object_acl), [])\n\n        expected_path = bucket.path\n        expected_data = {\"acl\": permissive}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def _make_public_w_future_helper(self, default_object_acl_loaded=True):\n        from google.cloud.storage.acl import _ACLEntity\n\n        name = \"name\"\n        get_api_response = {\"items\": []}\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n        acl_patched_response = {\"acl\": permissive, \"defaultObjectAcl\": []}\n        dac_patched_response = {\"acl\": permissive, \"defaultObjectAcl\": permissive}\n        client = mock.Mock(spec=[\"_get_resource\", \"_patch_resource\"])\n        client._get_resource.return_value = get_api_response\n        client._patch_resource.side_effect = [\n            acl_patched_response,\n            dac_patched_response,\n        ]\n\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = default_object_acl_loaded\n\n        bucket.make_public(future=True)\n\n        self.assertEqual(list(bucket.acl), permissive)\n        self.assertEqual(list(bucket.default_object_acl), permissive)\n\n        self.assertEqual(len(client._patch_resource.call_args_list), 2)\n        expected_acl_data = {\"acl\": permissive}\n        expected_dac_data = {\"defaultObjectAcl\": permissive}\n        expected_kw = {\n            \"query_params\": {\"projection\": \"full\"},\n            \"timeout\": self._get_default_timeout(),\n            \"retry\": DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        }\n        client._patch_resource.assert_has_calls(\n            [\n                ((bucket.path, expected_acl_data), expected_kw),\n                ((bucket.path, expected_dac_data), expected_kw),\n            ]\n        )\n\n        if not default_object_acl_loaded:\n            expected_path = f\"/b/{name}/defaultObjectAcl\"\n            expected_query_params = {}\n            client._get_resource.assert_called_once_with(\n                expected_path,\n                query_params=expected_query_params,\n                timeout=self._get_default_timeout(),\n                retry=DEFAULT_RETRY,\n            )\n        else:\n            client._get_resource.assert_not_called()\n\n    def test_make_public_w_future(self):\n        self._make_public_w_future_helper(default_object_acl_loaded=True)\n\n    def test_make_public_w_future_reload_default(self):\n        self._make_public_w_future_helper(default_object_acl_loaded=False)\n\n    def test_make_public_recursive(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        _saved = []\n\n        class _Blob(object):\n            _granted = False\n\n            def __init__(self, bucket, name):\n                self._bucket = bucket\n                self._name = name\n\n            @property\n            def acl(self):\n                return self\n\n            # Faux ACL methods\n            def all(self):\n                return self\n\n            def grant_read(self):\n                self._granted = True\n\n            def save(self, client=None, timeout=None):\n                _saved.append(\n                    (self._bucket, self._name, self._granted, client, timeout)\n                )\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n\n        patch_acl_response = {\"acl\": permissive, \"defaultObjectAcl\": []}\n        client = mock.Mock(spec=[\"list_blobs\", \"_patch_resource\"])\n        client._patch_resource.return_value = patch_acl_response\n\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        list_blobs_response = iter([_Blob(bucket, blob_name)])\n        client.list_blobs.return_value = list_blobs_response\n\n        timeout = 42\n\n        bucket.make_public(recursive=True, timeout=timeout)\n\n        self.assertEqual(list(bucket.acl), permissive)\n        self.assertEqual(list(bucket.default_object_acl), [])\n        self.assertEqual(_saved, [(bucket, blob_name, True, None, timeout)])\n\n        expected_patch_data = {\"acl\": permissive}\n        expected_patch_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            bucket.path,\n            expected_patch_data,\n            query_params=expected_patch_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n        client.list_blobs.assert_called_once()\n\n    def test_make_public_recursive_too_many(self):\n        from google.cloud.storage.acl import _ACLEntity\n\n        permissive = [{\"entity\": \"allUsers\", \"role\": _ACLEntity.READER_ROLE}]\n\n        name = \"name\"\n        blob1 = mock.Mock(spec=[])\n        blob2 = mock.Mock(spec=[])\n        patch_acl_response = {\"acl\": permissive, \"defaultObjectAcl\": []}\n        list_blobs_response = iter([blob1, blob2])\n        client = mock.Mock(spec=[\"list_blobs\", \"_patch_resource\"])\n        client.list_blobs.return_value = list_blobs_response\n        client._patch_resource.return_value = patch_acl_response\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        # Make the Bucket refuse to make_public with 2 objects.\n        bucket._MAX_OBJECTS_FOR_ITERATION = 1\n\n        with self.assertRaises(ValueError):\n            bucket.make_public(recursive=True)\n\n        expected_path = bucket.path\n        expected_data = {\"acl\": permissive}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n        client.list_blobs.assert_called_once()\n\n    def test_make_private_defaults(self):\n        name = \"name\"\n        no_permissions = []\n        api_response = {\"acl\": no_permissions, \"defaultObjectAcl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        bucket.make_private()\n\n        self.assertEqual(list(bucket.acl), no_permissions)\n        self.assertEqual(list(bucket.default_object_acl), [])\n\n        expected_path = bucket.path\n        expected_data = {\"acl\": no_permissions}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def test_make_private_w_preconditions(self):\n        name = \"name\"\n        no_permissions = []\n        api_response = {\"acl\": no_permissions, \"defaultObjectAcl\": []}\n        client = mock.Mock(spec=[\"_patch_resource\"])\n        client._patch_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        bucket.make_private(if_metageneration_match=2, if_metageneration_not_match=1)\n\n        self.assertEqual(list(bucket.acl), no_permissions)\n        self.assertEqual(list(bucket.default_object_acl), [])\n\n        expected_path = bucket.path\n        expected_data = {\"acl\": no_permissions}\n        expected_query_params = {\n            \"projection\": \"full\",\n            \"ifMetagenerationMatch\": 2,\n            \"ifMetagenerationNotMatch\": 1,\n        }\n        client._patch_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n    def _make_private_w_future_helper(self, default_object_acl_loaded=True):\n        name = \"name\"\n        no_permissions = []\n        get_api_response = {\"items\": []}\n        acl_patched_response = {\"acl\": no_permissions, \"defaultObjectAcl\": []}\n        dac_patched_response = {\n            \"acl\": no_permissions,\n            \"defaultObjectAcl\": no_permissions,\n        }\n        client = mock.Mock(spec=[\"_get_resource\", \"_patch_resource\"])\n        client._get_resource.return_value = get_api_response\n        client._patch_resource.side_effect = [\n            acl_patched_response,\n            dac_patched_response,\n        ]\n\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = default_object_acl_loaded\n\n        bucket.make_private(future=True)\n\n        self.assertEqual(list(bucket.acl), no_permissions)\n        self.assertEqual(list(bucket.default_object_acl), no_permissions)\n\n        self.assertEqual(len(client._patch_resource.call_args_list), 2)\n        expected_acl_data = {\"acl\": no_permissions}\n        expected_dac_data = {\"defaultObjectAcl\": no_permissions}\n        expected_kw = {\n            \"query_params\": {\"projection\": \"full\"},\n            \"timeout\": self._get_default_timeout(),\n            \"retry\": DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        }\n        client._patch_resource.assert_has_calls(\n            [\n                ((bucket.path, expected_acl_data), expected_kw),\n                ((bucket.path, expected_dac_data), expected_kw),\n            ]\n        )\n\n        if not default_object_acl_loaded:\n            expected_path = f\"/b/{name}/defaultObjectAcl\"\n            expected_query_params = {}\n            client._get_resource.assert_called_once_with(\n                expected_path,\n                query_params=expected_query_params,\n                timeout=self._get_default_timeout(),\n                retry=DEFAULT_RETRY,\n            )\n        else:\n            client._get_resource.assert_not_called()\n\n    def test_make_private_w_future(self):\n        self._make_private_w_future_helper(default_object_acl_loaded=True)\n\n    def test_make_private_w_future_reload_default(self):\n        self._make_private_w_future_helper(default_object_acl_loaded=False)\n\n    def test_make_private_recursive(self):\n        _saved = []\n\n        class _Blob(object):\n            _granted = True\n\n            def __init__(self, bucket, name):\n                self._bucket = bucket\n                self._name = name\n\n            @property\n            def acl(self):\n                return self\n\n            # Faux ACL methods\n            def all(self):\n                return self\n\n            def revoke_read(self):\n                self._granted = False\n\n            def save(self, client=None, timeout=None):\n                _saved.append(\n                    (self._bucket, self._name, self._granted, client, timeout)\n                )\n\n        name = \"name\"\n        blob_name = \"blob-name\"\n        no_permissions = []\n\n        patch_acl_response = {\"acl\": no_permissions, \"defaultObjectAcl\": []}\n        client = mock.Mock(spec=[\"list_blobs\", \"_patch_resource\"])\n        client._patch_resource.return_value = patch_acl_response\n\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        list_blobs_response = iter([_Blob(bucket, blob_name)])\n        client.list_blobs.return_value = list_blobs_response\n\n        timeout = 42\n\n        bucket.make_private(recursive=True, timeout=42)\n\n        self.assertEqual(list(bucket.acl), no_permissions)\n        self.assertEqual(list(bucket.default_object_acl), [])\n        self.assertEqual(_saved, [(bucket, blob_name, False, None, timeout)])\n\n        expected_patch_data = {\"acl\": no_permissions}\n        expected_patch_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            bucket.path,\n            expected_patch_data,\n            query_params=expected_patch_query_params,\n            timeout=timeout,\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n        client.list_blobs.assert_called_once()\n\n    def test_make_private_recursive_too_many(self):\n        no_permissions = []\n\n        name = \"name\"\n        blob1 = mock.Mock(spec=[])\n        blob2 = mock.Mock(spec=[])\n        patch_acl_response = {\"acl\": no_permissions, \"defaultObjectAcl\": []}\n        list_blobs_response = iter([blob1, blob2])\n        client = mock.Mock(spec=[\"list_blobs\", \"_patch_resource\"])\n        client.list_blobs.return_value = list_blobs_response\n        client._patch_resource.return_value = patch_acl_response\n        bucket = self._make_one(client=client, name=name)\n        bucket.acl.loaded = True\n        bucket.default_object_acl.loaded = True\n\n        # Make the Bucket refuse to make_private with 2 objects.\n        bucket._MAX_OBJECTS_FOR_ITERATION = 1\n\n        with self.assertRaises(ValueError):\n            bucket.make_private(recursive=True)\n\n        expected_path = bucket.path\n        expected_data = {\"acl\": no_permissions}\n        expected_query_params = {\"projection\": \"full\"}\n        client._patch_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        )\n\n        client.list_blobs.assert_called_once()\n\n    def _generate_upload_policy_helper(self, **kwargs):\n        import base64\n        import json\n\n        credentials = _create_signing_credentials()\n        credentials.signer_email = mock.sentinel.signer_email\n        credentials.sign_bytes.return_value = b\"DEADBEEF\"\n        client = self._make_client(_credentials=credentials)\n        name = \"name\"\n        bucket = self._make_one(client=client, name=name)\n\n        conditions = [[\"starts-with\", \"$key\", \"\"]]\n\n        policy_fields = bucket.generate_upload_policy(conditions, **kwargs)\n\n        self.assertEqual(policy_fields[\"bucket\"], bucket.name)\n        self.assertEqual(policy_fields[\"GoogleAccessId\"], mock.sentinel.signer_email)\n        self.assertEqual(\n            policy_fields[\"signature\"], base64.b64encode(b\"DEADBEEF\").decode(\"utf-8\")\n        )\n\n        policy = json.loads(base64.b64decode(policy_fields[\"policy\"]).decode(\"utf-8\"))\n\n        policy_conditions = policy[\"conditions\"]\n        expected_conditions = [{\"bucket\": bucket.name}] + conditions\n        for expected_condition in expected_conditions:\n            for condition in policy_conditions:\n                if condition == expected_condition:\n                    break\n            else:  # pragma: NO COVER\n                self.fail(\n                    f\"Condition {expected_condition} not found in {policy_conditions}\"\n                )\n\n        return policy_fields, policy\n\n    @mock.patch(\n        \"google.cloud.storage.bucket._NOW\", return_value=datetime.datetime(1990, 1, 1)\n    )\n    def test_generate_upload_policy(self, now):\n        from google.cloud._helpers import _datetime_to_rfc3339\n\n        _, policy = self._generate_upload_policy_helper()\n\n        self.assertEqual(\n            policy[\"expiration\"],\n            _datetime_to_rfc3339(now() + datetime.timedelta(hours=1)),\n        )\n\n    def test_generate_upload_policy_args(self):\n        from google.cloud._helpers import _datetime_to_rfc3339\n\n        expiration = datetime.datetime(1990, 5, 29)\n\n        _, policy = self._generate_upload_policy_helper(expiration=expiration)\n\n        self.assertEqual(policy[\"expiration\"], _datetime_to_rfc3339(expiration))\n\n    def test_generate_upload_policy_bad_credentials(self):\n        credentials = object()\n        client = self._make_client(_credentials=credentials)\n        name = \"name\"\n        bucket = self._make_one(client=client, name=name)\n\n        with self.assertRaises(AttributeError):\n            bucket.generate_upload_policy([])\n\n    def test_lock_retention_policy_no_policy_set(self):\n        client = mock.Mock(spec=[\"_post_resource\"])\n        name = \"name\"\n        bucket = self._make_one(client=client, name=name)\n        bucket._properties[\"metageneration\"] = 1234\n\n        with self.assertRaises(ValueError):\n            bucket.lock_retention_policy()\n\n        client._post_resource.assert_not_called()\n\n    def test_lock_retention_policy_no_metageneration(self):\n        client = mock.Mock(spec=[\"_post_resource\"])\n        name = \"name\"\n        bucket = self._make_one(client=client, name=name)\n        bucket._properties[\"retentionPolicy\"] = {\n            \"effectiveTime\": \"2018-03-01T16:46:27.123456Z\",\n            \"retentionPeriod\": 86400 * 100,  # 100 days\n        }\n\n        with self.assertRaises(ValueError):\n            bucket.lock_retention_policy()\n\n        client._post_resource.assert_not_called()\n\n    def test_lock_retention_policy_already_locked(self):\n        client = mock.Mock(spec=[\"_post_resource\"])\n        name = \"name\"\n        bucket = self._make_one(client=client, name=name)\n        bucket._properties[\"metageneration\"] = 1234\n        bucket._properties[\"retentionPolicy\"] = {\n            \"effectiveTime\": \"2018-03-01T16:46:27.123456Z\",\n            \"isLocked\": True,\n            \"retentionPeriod\": 86400 * 100,  # 100 days\n        }\n\n        with self.assertRaises(ValueError):\n            bucket.lock_retention_policy()\n\n        client._post_resource.assert_not_called()\n\n    def test_lock_retention_policy_ok_w_timeout_w_retry(self):\n        name = \"name\"\n        effective_time = \"2018-03-01T16:46:27.123456Z\"\n        one_hundred_days = 86400 * 100  # seconds in 100 days\n        metageneration = 1234\n        api_response = {\n            \"name\": name,\n            \"metageneration\": metageneration + 1,\n            \"retentionPolicy\": {\n                \"effectiveTime\": effective_time,\n                \"isLocked\": True,\n                \"retentionPeriod\": one_hundred_days,\n            },\n        }\n        metageneration = 1234\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name)\n        bucket._properties[\"metageneration\"] = metageneration\n        bucket._properties[\"retentionPolicy\"] = {\n            \"effectiveTime\": effective_time,\n            \"retentionPeriod\": one_hundred_days,\n        }\n        timeout = 42\n        retry = mock.Mock(spec=[])\n\n        bucket.lock_retention_policy(timeout=timeout, retry=retry)\n\n        expected_path = f\"/b/{name}/lockRetentionPolicy\"\n        expected_data = None\n        expected_query_params = {\"ifMetagenerationMatch\": metageneration}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=bucket,\n        )\n\n    def test_lock_retention_policy_w_user_project(self):\n        name = \"name\"\n        user_project = \"user-project-123\"\n        metageneration = 1234\n        effective_time = \"2018-03-01T16:46:27.123456Z\"\n        one_hundred_days = 86400 * 100  # seconds in 100 days\n        api_response = {\n            \"name\": name,\n            \"metageneration\": metageneration + 1,\n            \"retentionPolicy\": {\n                \"effectiveTime\": effective_time,\n                \"isLocked\": True,\n                \"retentionPeriod\": one_hundred_days,\n            },\n        }\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=name, user_project=user_project)\n        bucket._properties[\"metageneration\"] = 1234\n        bucket._properties[\"retentionPolicy\"] = {\n            \"effectiveTime\": effective_time,\n            \"retentionPeriod\": one_hundred_days,\n        }\n\n        bucket.lock_retention_policy()\n\n        expected_path = f\"/b/{name}/lockRetentionPolicy\"\n        expected_data = None\n        expected_query_params = {\n            \"ifMetagenerationMatch\": metageneration,\n            \"userProject\": user_project,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY,\n            _target_object=bucket,\n        )\n\n    def test_restore_blob_w_defaults(self):\n        bucket_name = \"restore_bucket\"\n        blob_name = \"restore_blob\"\n        generation = 123456\n        api_response = {\"name\": blob_name, \"generation\": generation}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=bucket_name)\n\n        restored_blob = bucket.restore_blob(blob_name)\n\n        self.assertIs(restored_blob.bucket, bucket)\n        self.assertEqual(restored_blob.name, blob_name)\n        expected_path = f\"/b/{bucket_name}/o/{blob_name}/restore\"\n        expected_data = None\n        expected_query_params = {}\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n\n    def test_restore_blob_w_explicit(self):\n        user_project = \"user-project-123\"\n        bucket_name = \"restore_bucket\"\n        blob_name = \"restore_blob\"\n        generation = 123456\n        api_response = {\"name\": blob_name, \"generation\": generation}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = self._make_one(\n            client=client, name=bucket_name, user_project=user_project\n        )\n        if_generation_match = 123456\n        if_generation_not_match = 654321\n        if_metageneration_match = 1\n        if_metageneration_not_match = 2\n        projection = \"noAcl\"\n\n        restored_blob = bucket.restore_blob(\n            blob_name,\n            client=client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            projection=projection,\n        )\n\n        self.assertEqual(restored_blob.name, blob_name)\n        self.assertEqual(restored_blob.bucket, bucket)\n        expected_path = f\"/b/{bucket_name}/o/{blob_name}/restore\"\n        expected_data = None\n        expected_query_params = {\n            \"userProject\": user_project,\n            \"projection\": projection,\n            \"ifGenerationMatch\": if_generation_match,\n            \"ifGenerationNotMatch\": if_generation_not_match,\n            \"ifMetagenerationMatch\": if_metageneration_match,\n            \"ifMetagenerationNotMatch\": if_metageneration_not_match,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n\n    def test_restore_blob_explicit_copy_source_acl(self):\n        bucket_name = \"restore_bucket\"\n        blob_name = \"restore\"\n        generation = 123456\n        api_response = {\"name\": blob_name, \"generation\": generation}\n        client = mock.Mock(spec=[\"_post_resource\"])\n        client._post_resource.return_value = api_response\n        bucket = self._make_one(client=client, name=bucket_name)\n        copy_source_acl = False\n\n        restored_blob = bucket.restore_blob(\n            blob_name,\n            copy_source_acl=copy_source_acl,\n            generation=generation,\n        )\n\n        self.assertEqual(restored_blob.name, blob_name)\n        self.assertEqual(restored_blob.bucket, bucket)\n        expected_path = f\"/b/{bucket_name}/o/{blob_name}/restore\"\n        expected_data = None\n        expected_query_params = {\n            \"copySourceAcl\": False,\n            \"generation\": generation,\n        }\n        client._post_resource.assert_called_once_with(\n            expected_path,\n            expected_data,\n            query_params=expected_query_params,\n            timeout=self._get_default_timeout(),\n            retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        )\n\n    def test_generate_signed_url_w_invalid_version(self):\n        expiration = \"2014-10-16T20:34:37.000Z\"\n        client = self._make_client()\n        bucket = self._make_one(name=\"bucket_name\", client=client)\n        with self.assertRaises(ValueError):\n            bucket.generate_signed_url(expiration, version=\"nonesuch\")\n\n    def _generate_signed_url_helper(\n        self,\n        version=None,\n        bucket_name=\"bucket-name\",\n        api_access_endpoint=None,\n        method=\"GET\",\n        content_md5=None,\n        content_type=None,\n        response_type=None,\n        response_disposition=None,\n        generation=None,\n        headers=None,\n        query_parameters=None,\n        credentials=None,\n        expiration=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n    ):\n        from urllib import parse\n        from google.cloud.storage._helpers import _bucket_bound_hostname_url\n        from google.cloud.storage._helpers import _get_default_storage_base_url\n\n        delta = datetime.timedelta(hours=1)\n\n        if expiration is None:\n            expiration = _NOW(_UTC) + delta\n\n        client = self._make_client(_credentials=credentials)\n        bucket = self._make_one(name=bucket_name, client=client)\n\n        if version is None:\n            effective_version = \"v2\"\n        else:\n            effective_version = version\n\n        to_patch = \"google.cloud.storage.bucket.generate_signed_url_{}\".format(\n            effective_version\n        )\n\n        with mock.patch(to_patch) as signer:\n            signed_uri = bucket.generate_signed_url(\n                expiration=expiration,\n                api_access_endpoint=api_access_endpoint,\n                method=method,\n                credentials=credentials,\n                headers=headers,\n                query_parameters=query_parameters,\n                version=version,\n                virtual_hosted_style=virtual_hosted_style,\n                bucket_bound_hostname=bucket_bound_hostname,\n            )\n\n        self.assertEqual(signed_uri, signer.return_value)\n\n        if credentials is None:\n            expected_creds = client._credentials\n        else:\n            expected_creds = credentials\n\n        if virtual_hosted_style:\n            expected_api_access_endpoint = \"https://{}.storage.googleapis.com\".format(\n                bucket_name\n            )\n        elif bucket_bound_hostname:\n            expected_api_access_endpoint = _bucket_bound_hostname_url(\n                bucket_bound_hostname, scheme\n            )\n        else:\n            expected_api_access_endpoint = (\n                api_access_endpoint or _get_default_storage_base_url()\n            )\n            expected_resource = f\"/{parse.quote(bucket_name)}\"\n\n        if virtual_hosted_style or bucket_bound_hostname:\n            expected_resource = \"/\"\n\n        expected_kwargs = {\n            \"resource\": expected_resource,\n            \"expiration\": expiration,\n            \"api_access_endpoint\": expected_api_access_endpoint,\n            \"method\": method.upper(),\n            \"headers\": headers,\n            \"query_parameters\": query_parameters,\n        }\n        signer.assert_called_once_with(expected_creds, **expected_kwargs)\n\n    def test_get_bucket_from_string_w_valid_uri(self):\n        from google.cloud.storage.bucket import Bucket\n\n        client = self._make_client()\n        BUCKET_NAME = \"BUCKET_NAME\"\n        uri = \"gs://\" + BUCKET_NAME\n\n        bucket = Bucket.from_string(uri, client)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertIs(bucket.client, client)\n        self.assertEqual(bucket.name, BUCKET_NAME)\n\n    def test_get_bucket_from_string_w_invalid_uri(self):\n        from google.cloud.storage.bucket import Bucket\n\n        client = self._make_client()\n\n        with pytest.raises(ValueError, match=\"URI scheme must be gs\"):\n            Bucket.from_string(\"http://bucket_name\", client)\n\n    def test_get_bucket_from_string_w_domain_name_bucket(self):\n        from google.cloud.storage.bucket import Bucket\n\n        client = self._make_client()\n        BUCKET_NAME = \"buckets.example.com\"\n        uri = \"gs://\" + BUCKET_NAME\n\n        bucket = Bucket.from_string(uri, client)\n\n        self.assertIsInstance(bucket, Bucket)\n        self.assertIs(bucket.client, client)\n        self.assertEqual(bucket.name, BUCKET_NAME)\n\n    def test_generate_signed_url_no_version_passed_warning(self):\n        self._generate_signed_url_helper()\n\n    def _generate_signed_url_v2_helper(self, **kw):\n        version = \"v2\"\n        self._generate_signed_url_helper(version, **kw)\n\n    def test_generate_signed_url_v2_w_defaults(self):\n        self._generate_signed_url_v2_helper()\n\n    def test_generate_signed_url_v2_w_expiration(self):\n        expiration = _NOW(_UTC)\n        self._generate_signed_url_v2_helper(expiration=expiration)\n\n    def test_generate_signed_url_v2_w_endpoint(self):\n        self._generate_signed_url_v2_helper(\n            api_access_endpoint=\"https://api.example.com/v1\"\n        )\n\n    def test_generate_signed_url_v2_w_method(self):\n        self._generate_signed_url_v2_helper(method=\"POST\")\n\n    def test_generate_signed_url_v2_w_lowercase_method(self):\n        self._generate_signed_url_v2_helper(method=\"get\")\n\n    def test_generate_signed_url_v2_w_content_md5(self):\n        self._generate_signed_url_v2_helper(content_md5=\"FACEDACE\")\n\n    def test_generate_signed_url_v2_w_content_type(self):\n        self._generate_signed_url_v2_helper(content_type=\"text.html\")\n\n    def test_generate_signed_url_v2_w_response_type(self):\n        self._generate_signed_url_v2_helper(response_type=\"text.html\")\n\n    def test_generate_signed_url_v2_w_response_disposition(self):\n        self._generate_signed_url_v2_helper(response_disposition=\"inline\")\n\n    def test_generate_signed_url_v2_w_generation(self):\n        self._generate_signed_url_v2_helper(generation=12345)\n\n    def test_generate_signed_url_v2_w_headers(self):\n        self._generate_signed_url_v2_helper(headers={\"x-goog-foo\": \"bar\"})\n\n    def test_generate_signed_url_v2_w_credentials(self):\n        credentials = object()\n        self._generate_signed_url_v2_helper(credentials=credentials)\n\n    def _generate_signed_url_v4_helper(self, **kw):\n        version = \"v4\"\n        self._generate_signed_url_helper(version, **kw)\n\n    def test_generate_signed_url_v4_w_defaults(self):\n        self._generate_signed_url_v2_helper()\n\n    def test_generate_signed_url_v4_w_endpoint(self):\n        self._generate_signed_url_v4_helper(\n            api_access_endpoint=\"https://api.example.com/v1\"\n        )\n\n    def test_generate_signed_url_v4_w_method(self):\n        self._generate_signed_url_v4_helper(method=\"POST\")\n\n    def test_generate_signed_url_v4_w_lowercase_method(self):\n        self._generate_signed_url_v4_helper(method=\"get\")\n\n    def test_generate_signed_url_v4_w_content_md5(self):\n        self._generate_signed_url_v4_helper(content_md5=\"FACEDACE\")\n\n    def test_generate_signed_url_v4_w_content_type(self):\n        self._generate_signed_url_v4_helper(content_type=\"text.html\")\n\n    def test_generate_signed_url_v4_w_response_type(self):\n        self._generate_signed_url_v4_helper(response_type=\"text.html\")\n\n    def test_generate_signed_url_v4_w_response_disposition(self):\n        self._generate_signed_url_v4_helper(response_disposition=\"inline\")\n\n    def test_generate_signed_url_v4_w_generation(self):\n        self._generate_signed_url_v4_helper(generation=12345)\n\n    def test_generate_signed_url_v4_w_headers(self):\n        self._generate_signed_url_v4_helper(headers={\"x-goog-foo\": \"bar\"})\n\n    def test_generate_signed_url_v4_w_credentials(self):\n        credentials = object()\n        self._generate_signed_url_v4_helper(credentials=credentials)\n\n    def test_generate_signed_url_v4_w_virtual_hostname(self):\n        self._generate_signed_url_v4_helper(virtual_hosted_style=True)\n\n    def test_generate_signed_url_v4_w_bucket_bound_hostname_w_scheme(self):\n        self._generate_signed_url_v4_helper(\n            bucket_bound_hostname=\"http://cdn.example.com\"\n        )\n\n    def test_generate_signed_url_v4_w_bucket_bound_hostname_w_bare_hostname(self):\n        self._generate_signed_url_v4_helper(bucket_bound_hostname=\"cdn.example.com\")\n\n    def test_generate_signed_url_v4_w_incompatible_params(self):\n        with self.assertRaises(ValueError):\n            self._generate_signed_url_v4_helper(\n                api_access_endpoint=\"example.com\",\n                bucket_bound_hostname=\"cdn.example.com\",\n            )\n        with self.assertRaises(ValueError):\n            self._generate_signed_url_v4_helper(\n                virtual_hosted_style=True, bucket_bound_hostname=\"cdn.example.com\"\n            )\n\n\nclass Test__item_to_notification(unittest.TestCase):\n    def _call_fut(self, iterator, item):\n        from google.cloud.storage.bucket import _item_to_notification\n\n        return _item_to_notification(iterator, item)\n\n    def test_it(self):\n        from google.cloud.storage.notification import BucketNotification\n        from google.cloud.storage.notification import _TOPIC_REF_FMT\n        from google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\n\n        iterator = mock.Mock(spec=[\"bucket\"])\n        project = \"my-project-123\"\n        topic = \"topic-1\"\n        item = {\n            \"topic\": _TOPIC_REF_FMT.format(project, topic),\n            \"id\": \"1\",\n            \"etag\": \"DEADBEEF\",\n            \"selfLink\": \"https://example.com/notification/1\",\n            \"payload_format\": NONE_PAYLOAD_FORMAT,\n        }\n\n        notification = self._call_fut(iterator, item)\n\n        self.assertIsInstance(notification, BucketNotification)\n        self.assertIs(notification._bucket, iterator.bucket)\n        self.assertEqual(notification._topic_name, topic)\n        self.assertEqual(notification._topic_project, project)\n        self.assertEqual(notification._properties, item)\n", "tests/conformance/test_conformance.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Conformance tests for retry. Verifies correct behavior around retryable errors, idempotency and preconditions.\"\"\"\n\nimport functools\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport time\nimport uuid\n\nimport pytest\nimport requests\nimport urllib\n\nfrom google.auth.credentials import AnonymousCredentials\nfrom google.cloud import storage\nfrom google.cloud.storage.hmac_key import HMACKeyMetadata\n\nfrom . import _read_local_json\n\n\n_CONFORMANCE_TESTS = _read_local_json(\"retry_strategy_test_data.json\")[\"retryTests\"]\n\n\"\"\"Environment variable or default host for Storage testbench emulator.\"\"\"\n_HOST = os.environ.get(\"STORAGE_EMULATOR_HOST\", \"http://localhost:9000\")\n_PORT = urllib.parse.urlsplit(_HOST).port\n\n\"\"\"The storage testbench docker image info and commands.\"\"\"\n_DEFAULT_IMAGE_NAME = \"gcr.io/cloud-devrel-public-resources/storage-testbench\"\n_DEFAULT_IMAGE_TAG = \"latest\"\n_DOCKER_IMAGE = f\"{_DEFAULT_IMAGE_NAME}:{_DEFAULT_IMAGE_TAG}\"\n_PULL_CMD = [\"docker\", \"pull\", _DOCKER_IMAGE]\n_RUN_CMD = [\"docker\", \"run\", \"--rm\", \"-d\", \"-p\", f\"{_PORT}:9000\", _DOCKER_IMAGE]\n\n_CONF_TEST_PROJECT_ID = \"my-project-id\"\n_CONF_TEST_SERVICE_ACCOUNT_EMAIL = (\n    \"my-service-account@my-project-id.iam.gserviceaccount.com\"\n)\n_CONF_TEST_PUBSUB_TOPIC_NAME = \"my-topic-name\"\n\n_STRING_CONTENT = \"hello world\"\n_BYTE_CONTENT = b\"12345678\"\n_RESUMABLE_UPLOAD_CHUNK_SIZE = 2 * 1024 * 1024\n\n\n########################################################################################################################################\n### Library methods for mapping ########################################################################################################\n########################################################################################################################################\n\n\ndef bucket_get_blob(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    bucket = client.bucket(bucket.name)\n    bucket.get_blob(object.name)\n\n\ndef blob_exists(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    blob.exists()\n\n\ndef blob_download_as_bytes(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    # download the file and assert data integrity\n    blob = client.bucket(bucket.name).blob(file.name)\n    stored_contents = blob.download_as_bytes()\n    assert stored_contents == data.encode(\"utf-8\")\n\n\ndef blob_download_as_text(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(file.name)\n    stored_contents = blob.download_as_text()\n    assert stored_contents == data\n\n\ndef blob_download_to_filename(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(file.name)\n    with tempfile.NamedTemporaryFile() as temp_f:\n        blob.download_to_filename(temp_f.name)\n        with open(temp_f.name, \"r\") as file_obj:\n            stored_contents = file_obj.read()\n    assert stored_contents == data\n\n\ndef blob_download_to_filename_chunked(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(file.name, chunk_size=40 * 1024 * 1024)\n    with tempfile.NamedTemporaryFile() as temp_f:\n        blob.download_to_filename(temp_f.name)\n        with open(temp_f.name, \"r\") as file_obj:\n            stored_contents = file_obj.read()\n    assert stored_contents == data\n\n\ndef client_download_blob_to_file(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(file.name)\n    with tempfile.NamedTemporaryFile() as temp_f:\n        with open(temp_f.name, \"wb\") as file_obj:\n            client.download_blob_to_file(blob, file_obj)\n        with open(temp_f.name, \"r\") as to_read:\n            stored_contents = to_read.read()\n    assert stored_contents == data\n\n\ndef blobreader_read(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(file.name)\n    with blob.open(mode=\"r\") as reader:\n        stored_contents = reader.read()\n    assert stored_contents == data\n\n\ndef client_list_blobs(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    blobs = client.list_blobs(bucket.name)\n    for b in blobs:\n        pass\n\n\ndef bucket_list_blobs(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    blobs = client.bucket(bucket.name).list_blobs()\n    for b in blobs:\n        pass\n\n\ndef bucket_delete(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.delete(force=True)\n\n\ndef bucket_reload(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.reload()\n\n\ndef client_get_bucket(client, _preconditions, **resources):\n    client.get_bucket(resources.get(\"bucket\").name)\n\n\ndef client_lookup_bucket(client, _preconditions, **resources):\n    client.lookup_bucket(resources.get(\"bucket\").name)\n\n\ndef bucket_exists(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.exists()\n\n\ndef client_create_bucket(client, _preconditions, **_):\n    bucket = client.bucket(uuid.uuid4().hex)\n    client.create_bucket(bucket)\n\n\ndef bucket_create(client, _preconditions, **_):\n    bucket = client.bucket(uuid.uuid4().hex)\n    bucket.create()\n\n\ndef client_list_buckets(client, _preconditions, **_):\n    buckets = client.list_buckets()\n    for b in buckets:\n        pass\n\n\ndef bucket_get_iam_policy(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.get_iam_policy()\n\n\ndef bucket_test_iam_permissions(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    permissions = [\"storage.buckets.get\", \"storage.buckets.create\"]\n    bucket.test_iam_permissions(permissions)\n\n\ndef bucket_lock_retention_policy(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.retention_period = 60\n    bucket.patch()\n    bucket.lock_retention_policy()\n\n\ndef client_get_service_account_email(client, _preconditions, **_):\n    client.get_service_account_email()\n\n\ndef notification_create(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    notification = bucket.notification(topic_name=_CONF_TEST_PUBSUB_TOPIC_NAME)\n    notification.create()\n\n\ndef bucket_list_notifications(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    notifications = client.bucket(bucket.name).list_notifications()\n    for n in notifications:\n        pass\n\n\ndef bucket_get_notification(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    notification = resources.get(\"notification\")\n    client.bucket(bucket.name).get_notification(notification.notification_id)\n\n\ndef notification_reload(client, _preconditions, **resources):\n    notification = client.bucket(resources.get(\"bucket\").name).notification(\n        notification_id=resources.get(\"notification\").notification_id\n    )\n    notification.reload()\n\n\ndef notification_exists(client, _preconditions, **resources):\n    notification = client.bucket(resources.get(\"bucket\").name).notification(\n        notification_id=resources.get(\"notification\").notification_id\n    )\n    notification.exists()\n\n\ndef notification_delete(client, _preconditions, **resources):\n    notification = client.bucket(resources.get(\"bucket\").name).notification(\n        notification_id=resources.get(\"notification\").notification_id\n    )\n    notification.delete()\n\n\ndef client_list_hmac_keys(client, _preconditions, **_):\n    hmac_keys = client.list_hmac_keys()\n    for k in hmac_keys:\n        pass\n\n\ndef client_get_hmac_key_metadata(client, _preconditions, **resources):\n    access_id = resources.get(\"hmac_key\").access_id\n    client.get_hmac_key_metadata(access_id=access_id)\n\n\ndef hmac_key_exists(client, _preconditions, **resources):\n    access_id = resources.get(\"hmac_key\").access_id\n    hmac_key = HMACKeyMetadata(client, access_id=access_id)\n    hmac_key.exists()\n\n\ndef hmac_key_reload(client, _preconditions, **resources):\n    access_id = resources.get(\"hmac_key\").access_id\n    hmac_key = HMACKeyMetadata(client, access_id=access_id)\n    hmac_key.reload()\n\n\ndef hmac_key_delete(client, _preconditions, **resources):\n    access_id = resources.get(\"hmac_key\").access_id\n    hmac_key = HMACKeyMetadata(client, access_id=access_id)\n    hmac_key.state = \"INACTIVE\"\n    hmac_key.update()\n    hmac_key.delete()\n\n\ndef client_create_hmac_key(client, _preconditions, **_):\n    client.create_hmac_key(service_account_email=_CONF_TEST_SERVICE_ACCOUNT_EMAIL)\n\n\ndef hmac_key_update(client, _preconditions, **resources):\n    access_id = resources.get(\"hmac_key\").access_id\n    etag = resources.get(\"hmac_key\").etag\n    hmac_key = HMACKeyMetadata(client, access_id=access_id)\n    if _preconditions:\n        pytest.skip(\"Etag is not yet supported\")\n        hmac_key.etag = etag\n    hmac_key.state = \"INACTIVE\"\n    hmac_key.update()\n\n\ndef bucket_patch(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    metageneration = resources.get(\"bucket\").metageneration\n    bucket.storage_class = \"COLDLINE\"\n    if _preconditions:\n        bucket.patch(if_metageneration_match=metageneration)\n    else:\n        bucket.patch()\n\n\ndef bucket_update(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    metageneration = resources.get(\"bucket\").metageneration\n    bucket._properties = {\"storageClass\": \"STANDARD\"}\n    if _preconditions:\n        bucket.update(if_metageneration_match=metageneration)\n    else:\n        bucket.update()\n\n\ndef bucket_set_iam_policy(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    role = \"roles/storage.objectViewer\"\n    member = _CONF_TEST_SERVICE_ACCOUNT_EMAIL\n\n    policy = bucket.get_iam_policy(requested_policy_version=3)\n    policy.bindings.append({\"role\": role, \"members\": {member}})\n    if _preconditions:\n        bucket.set_iam_policy(policy)\n    else:\n        # IAM policies have no metageneration:  clear ETag to avoid checking that it matches.\n        policy.etag = None\n        bucket.set_iam_policy(policy)\n\n\ndef bucket_delete_blob(client, _preconditions, **resources):\n    object = resources.get(\"object\")\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        generation = object.generation\n        bucket.delete_blob(object.name, if_generation_match=generation)\n    else:\n        bucket.delete_blob(object.name)\n\n\ndef bucket_delete_blobs(client, _preconditions, **resources):\n    object = resources.get(\"object\")\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    sources = [object]\n    source_generations = [object.generation]\n    if _preconditions:\n        bucket.delete_blobs(sources, if_generation_match=source_generations)\n    else:\n        bucket.delete_blobs(sources)\n\n\ndef blob_delete(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    if _preconditions:\n        blob.delete(if_generation_match=object.generation)\n    else:\n        blob.delete()\n\n\ndef blob_patch(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    blob.metadata = {\"foo\": \"bar\"}\n    if _preconditions:\n        blob.patch(if_metageneration_match=object.metageneration)\n    else:\n        blob.patch()\n\n\ndef blob_update(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    blob.metadata = {\"foo\": \"bar\"}\n    if _preconditions:\n        blob.update(if_metageneration_match=object.metageneration)\n    else:\n        blob.update()\n\n\ndef bucket_copy_blob(client, _preconditions, **resources):\n    object = resources.get(\"object\")\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    destination = client.create_bucket(uuid.uuid4().hex)\n    if _preconditions:\n        bucket.copy_blob(\n            object, destination, new_name=uuid.uuid4().hex, if_generation_match=0\n        )\n    else:\n        bucket.copy_blob(object, destination)\n\n\ndef bucket_rename_blob(client, _preconditions, **resources):\n    object = resources.get(\"object\")\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    blob = bucket.blob(resources.get(\"object\").name)\n    new_name = uuid.uuid4().hex\n    if _preconditions:\n        bucket.rename_blob(\n            blob,\n            new_name,\n            if_generation_match=0,\n            if_source_generation_match=object.generation,\n        )\n    else:\n        bucket.rename_blob(blob, new_name)\n\n\ndef blob_rewrite(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    new_blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    new_blob.metadata = {\"foo\": \"bar\"}\n    if _preconditions:\n        new_blob.rewrite(object, if_generation_match=0)\n    else:\n        new_blob.rewrite(object)\n\n\ndef blob_update_storage_class(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    storage_class = \"STANDARD\"\n    if _preconditions:\n        blob.update_storage_class(storage_class, if_generation_match=object.generation)\n    else:\n        blob.update_storage_class(storage_class)\n\n\ndef blob_compose(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    blob_2 = bucket.blob(uuid.uuid4().hex)\n    blob_2.upload_from_string(_STRING_CONTENT)\n    sources = [blob_2]\n    if _preconditions:\n        blob.compose(sources, if_generation_match=object.generation)\n    else:\n        blob.compose(sources)\n\n\ndef blob_upload_from_string(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    _, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    blob.chunk_size = _RESUMABLE_UPLOAD_CHUNK_SIZE\n    if _preconditions:\n        blob.upload_from_string(data, if_generation_match=0)\n    else:\n        blob.upload_from_string(data)\n    assert blob.size == len(data)\n\n\ndef blob_upload_from_file(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    file_blob = client.bucket(bucket.name).blob(file.name)\n    upload_blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    upload_blob.chunk_size = _RESUMABLE_UPLOAD_CHUNK_SIZE\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        # Create a named temporary file with payload.\n        with open(temp_f.name, \"wb\") as file_obj:\n            client.download_blob_to_file(file_blob, file_obj)\n        # Upload the temporary file and assert data integrity.\n        if _preconditions:\n            upload_blob.upload_from_file(temp_f, if_generation_match=0)\n        else:\n            upload_blob.upload_from_file(temp_f)\n\n    upload_blob.reload()\n    assert upload_blob.size == len(data)\n\n\ndef blob_upload_from_filename(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    blob.chunk_size = _RESUMABLE_UPLOAD_CHUNK_SIZE\n\n    bucket = resources.get(\"bucket\")\n    file, data = resources.get(\"file_data\")\n    file_blob = client.bucket(bucket.name).blob(file.name)\n    upload_blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    upload_blob.chunk_size = _RESUMABLE_UPLOAD_CHUNK_SIZE\n\n    with tempfile.NamedTemporaryFile() as temp_f:\n        # Create a named temporary file with payload.\n        with open(temp_f.name, \"wb\") as file_obj:\n            client.download_blob_to_file(file_blob, file_obj)\n        # Upload the temporary file and assert data integrity.\n        if _preconditions:\n            upload_blob.upload_from_filename(temp_f.name, if_generation_match=0)\n        else:\n            upload_blob.upload_from_filename(temp_f.name)\n\n    upload_blob.reload()\n    assert upload_blob.size == len(data)\n\n\ndef blobwriter_write(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    _, data = resources.get(\"file_data\")\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    if _preconditions:\n        with blob.open(\n            \"w\", chunk_size=_RESUMABLE_UPLOAD_CHUNK_SIZE, if_generation_match=0\n        ) as writer:\n            writer.write(data)\n    else:\n        with blob.open(\"w\", chunk_size=_RESUMABLE_UPLOAD_CHUNK_SIZE) as writer:\n            writer.write(data)\n    blob.reload()\n    assert blob.size == len(data)\n\n\ndef blobwriter_write_multipart(client, _preconditions, **resources):\n    chunk_size = 256 * 1024\n    bucket = resources.get(\"bucket\")\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    if _preconditions:\n        with blob.open(\"wb\", chunk_size=chunk_size, if_generation_match=0) as writer:\n            writer.write(_BYTE_CONTENT)\n    else:\n        with blob.open(\"wb\", chunk_size=chunk_size) as writer:\n            writer.write(_BYTE_CONTENT)\n\n\ndef blob_upload_from_string_multipart(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    if _preconditions:\n        blob.upload_from_string(_STRING_CONTENT, if_generation_match=0)\n    else:\n        blob.upload_from_string(_STRING_CONTENT)\n\n\ndef blob_create_resumable_upload_session(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    if _preconditions:\n        blob.create_resumable_upload_session(if_generation_match=0)\n    else:\n        blob.create_resumable_upload_session()\n\n\ndef blob_make_private(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    if _preconditions:\n        blob.make_private(if_metageneration_match=object.metageneration)\n    else:\n        blob.make_private()\n\n\ndef blob_make_public(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    if _preconditions:\n        blob.make_public(if_metageneration_match=object.metageneration)\n    else:\n        blob.make_public()\n\n\ndef bucket_make_private(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        bucket.make_private(\n            if_metageneration_match=resources.get(\"bucket\").metageneration\n        )\n    else:\n        bucket.make_private()\n\n\ndef bucket_make_public(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        bucket.make_public(\n            if_metageneration_match=resources.get(\"bucket\").metageneration\n        )\n    else:\n        bucket.make_public()\n\n\ndef bucket_acl_reload(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.acl.reload()\n\n\ndef bucket_acl_save(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.acl.user(_CONF_TEST_SERVICE_ACCOUNT_EMAIL).grant_owner()\n    if _preconditions:\n        bucket.acl.save(if_metageneration_match=resources.get(\"bucket\").metageneration)\n    else:\n        bucket.acl.save()\n\n\ndef bucket_acl_save_predefined(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        bucket.acl.save_predefined(\n            \"bucketOwnerFullControl\",\n            if_metageneration_match=resources.get(\"bucket\").metageneration,\n        )\n    else:\n        bucket.acl.save_predefined(\"bucketOwnerFullControl\")\n\n\ndef bucket_acl_clear(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        bucket.acl.clear(if_metageneration_match=resources.get(\"bucket\").metageneration)\n    else:\n        bucket.acl.clear()\n\n\ndef default_object_acl_reload(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.default_object_acl.reload()\n\n\ndef default_object_acl_save(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    bucket.default_object_acl.user(_CONF_TEST_SERVICE_ACCOUNT_EMAIL).grant_owner()\n    if _preconditions:\n        bucket.default_object_acl.save(\n            if_metageneration_match=resources.get(\"bucket\").metageneration\n        )\n    else:\n        bucket.default_object_acl.save()\n\n\ndef default_object_acl_save_predefined(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        bucket.default_object_acl.save_predefined(\n            \"bucketOwnerFullControl\",\n            if_metageneration_match=resources.get(\"bucket\").metageneration,\n        )\n    else:\n        bucket.default_object_acl.save_predefined(\"bucketOwnerFullControl\")\n\n\ndef default_object_acl_clear(client, _preconditions, **resources):\n    bucket = client.bucket(resources.get(\"bucket\").name)\n    if _preconditions:\n        bucket.default_object_acl.clear(\n            if_metageneration_match=resources.get(\"bucket\").metageneration\n        )\n    else:\n        bucket.default_object_acl.clear()\n\n\ndef object_acl_reload(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    blob.acl.reload()\n\n\ndef object_acl_save(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    blob.acl.user(_CONF_TEST_SERVICE_ACCOUNT_EMAIL).grant_owner()\n    if _preconditions:\n        blob.acl.save(if_metageneration_match=object.metageneration)\n    else:\n        blob.acl.save()\n\n\ndef object_acl_save_predefined(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    if _preconditions:\n        blob.acl.save_predefined(\n            \"bucketOwnerFullControl\", if_metageneration_match=object.metageneration\n        )\n    else:\n        blob.acl.save_predefined(\"bucketOwnerFullControl\")\n\n\ndef object_acl_clear(client, _preconditions, **resources):\n    bucket = resources.get(\"bucket\")\n    object = resources.get(\"object\")\n    blob = client.bucket(bucket.name).blob(object.name)\n    if _preconditions:\n        blob.acl.clear(if_metageneration_match=object.metageneration)\n    else:\n        blob.acl.clear()\n\n\n########################################################################################################################################\n### Method Invocation Mapping ##########################################################################################################\n########################################################################################################################################\n\n# Method invocation mapping is a map whose keys are a string describing a standard\n# API call (e.g. storage.objects.get) and values are a list of functions which\n# wrap library methods that implement these calls. There may be multiple values\n# because multiple library methods may use the same call (e.g. get could be a\n# read or just a metadata get).\n\nmethod_mapping = {\n    \"storage.bucket_acl.list\": [bucket_acl_reload],  # S1 start\n    \"storage.buckets.delete\": [bucket_delete],\n    \"storage.buckets.get\": [\n        client_get_bucket,\n        bucket_reload,\n        client_lookup_bucket,\n        bucket_exists,\n    ],\n    \"storage.buckets.getIamPolicy\": [bucket_get_iam_policy],\n    \"storage.buckets.insert\": [client_create_bucket, bucket_create],\n    \"storage.buckets.list\": [client_list_buckets],\n    \"storage.buckets.lockRetentionPolicy\": [bucket_lock_retention_policy],\n    \"storage.buckets.testIamPermissions\": [bucket_test_iam_permissions],\n    \"storage.default_object_acl.list\": [default_object_acl_reload],\n    \"storage.hmacKey.delete\": [hmac_key_delete],\n    \"storage.hmacKey.get\": [\n        client_get_hmac_key_metadata,\n        hmac_key_exists,\n        hmac_key_reload,\n    ],\n    \"storage.hmacKey.list\": [client_list_hmac_keys],\n    \"storage.notifications.delete\": [notification_delete],\n    \"storage.notifications.get\": [\n        bucket_get_notification,\n        notification_exists,\n        notification_reload,\n    ],\n    \"storage.notifications.list\": [bucket_list_notifications],\n    \"storage.object_acl.list\": [object_acl_reload],\n    \"storage.objects.get\": [\n        bucket_get_blob,\n        blob_exists,\n        client_download_blob_to_file,\n        blob_download_to_filename,\n        blob_download_to_filename_chunked,\n        blob_download_as_bytes,\n        blob_download_as_text,\n        blobreader_read,\n    ],\n    \"storage.objects.download\": [\n        client_download_blob_to_file,\n        blob_download_to_filename,\n        blob_download_to_filename_chunked,\n        blob_download_as_bytes,\n        blob_download_as_text,\n        blobreader_read,\n    ],\n    \"storage.objects.list\": [client_list_blobs, bucket_list_blobs, bucket_delete],\n    \"storage.serviceaccount.get\": [client_get_service_account_email],  # S1 end\n    \"storage.buckets.patch\": [\n        bucket_patch,\n        bucket_make_public,\n        bucket_make_private,\n        bucket_acl_save,\n        bucket_acl_save_predefined,\n        bucket_acl_clear,\n        default_object_acl_save,\n        default_object_acl_save_predefined,\n        default_object_acl_clear,\n    ],  # S2/S3 start\n    \"storage.buckets.setIamPolicy\": [bucket_set_iam_policy],\n    \"storage.buckets.update\": [bucket_update],\n    \"storage.hmacKey.update\": [hmac_key_update],\n    \"storage.objects.compose\": [blob_compose],\n    \"storage.objects.copy\": [bucket_copy_blob, bucket_rename_blob],\n    \"storage.objects.delete\": [\n        bucket_delete_blob,\n        bucket_delete_blobs,\n        blob_delete,\n        bucket_rename_blob,\n    ],\n    \"storage.objects.insert\": [\n        blob_upload_from_string_multipart,\n        blobwriter_write_multipart,\n        blob_create_resumable_upload_session,\n    ],\n    \"storage.resumable.upload\": [\n        blob_upload_from_string,\n        blob_upload_from_file,\n        blob_upload_from_filename,\n        blobwriter_write,\n    ],\n    \"storage.objects.patch\": [\n        blob_patch,\n        object_acl_save,\n        object_acl_save_predefined,\n        object_acl_clear,\n        blob_make_private,\n        blob_make_public,\n    ],\n    \"storage.objects.rewrite\": [blob_rewrite, blob_update_storage_class],\n    \"storage.objects.update\": [blob_update],  # S2/S3 end\n    \"storage.hmacKey.create\": [client_create_hmac_key],  # S4 start\n    \"storage.notifications.insert\": [notification_create],\n}\n\n\n########################################################################################################################################\n### Helper Methods for Testbench Retry Test API ########################################################################################\n########################################################################################################################################\n\n\n\"\"\"\nThe Retry Test API in the testbench is used to run the retry conformance tests. It offers a mechanism to describe more complex\nretry scenarios while sending a single, constant header through all the HTTP requests from a test program. The Retry Test API\ncan be accessed by adding the path \"/retry-test\" to the host. See also: https://github.com/googleapis/storage-testbench\n\"\"\"\n\n\ndef _create_retry_test(host, method_name, instructions):\n    \"\"\"\n    For each test case, initialize a Retry Test resource by loading a set of\n    instructions to the testbench host. The instructions include an API method\n    and a list of errors. An unique id is created for each Retry Test resource.\n    \"\"\"\n    import json\n\n    retry_test_uri = host + \"/retry_test\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n    }\n    data_dict = {\"instructions\": {method_name: instructions}}\n    data = json.dumps(data_dict)\n    r = requests.post(retry_test_uri, headers=headers, data=data)\n    return r.json()\n\n\ndef _get_retry_test(host, id):\n    \"\"\"\n    Retrieve the state of the Retry Test resource, including the unique id,\n    instructions, and a boolean status \"completed\". This can be used to verify\n    if all instructions were used as expected.\n    \"\"\"\n    get_retry_test_uri = f\"{host}/retry_test/{id}\"\n    r = requests.get(get_retry_test_uri)\n    return r.json()\n\n\ndef _run_retry_test(\n    host,\n    id,\n    lib_func,\n    _preconditions,\n    bucket,\n    object,\n    notification,\n    hmac_key,\n    file_data,\n):\n    \"\"\"\n    To execute tests against the list of instrucions sent to the Retry Test API,\n    create a client to send the retry test ID using the x-retry-test-id header\n    in each request. For incoming requests that match the test ID and API method,\n    the testbench will pop off the next instruction from the list and force the\n    listed failure case.\n    \"\"\"\n    client = storage.Client(\n        project=_CONF_TEST_PROJECT_ID,\n        credentials=AnonymousCredentials(),\n        client_options={\"api_endpoint\": host},\n    )\n    client._http.headers.update({\"x-retry-test-id\": id})\n    lib_func(\n        client,\n        _preconditions,\n        bucket=bucket,\n        object=object,\n        notification=notification,\n        hmac_key=hmac_key,\n        file_data=file_data,\n    )\n\n\ndef _delete_retry_test(host, id):\n    \"\"\"\n    Delete the Retry Test resource by id.\n    \"\"\"\n    get_retry_test_uri = f\"{host}/retry_test/{id}\"\n    requests.delete(get_retry_test_uri)\n\n\n########################################################################################################################################\n### Run Test Case for Retry Strategy ###################################################################################################\n########################################################################################################################################\n\n\ndef run_test_case(\n    scenario_id,\n    method,\n    case,\n    lib_func,\n    host,\n    bucket,\n    object,\n    notification,\n    hmac_key,\n    file_data,\n):\n    scenario = _CONFORMANCE_TESTS[scenario_id - 1]\n    expect_success = scenario[\"expectSuccess\"]\n    precondition_provided = scenario[\"preconditionProvided\"]\n    method_name = method[\"name\"]\n    instructions = case[\"instructions\"]\n\n    try:\n        r = _create_retry_test(host, method_name, instructions)\n        id = r[\"id\"]\n    except Exception as e:\n        raise Exception(\n            f\"Error creating retry test for {method_name}: {e}\"\n        ).with_traceback(e.__traceback__)\n\n    # Run retry tests on library methods.\n    try:\n        _run_retry_test(\n            host,\n            id,\n            lib_func,\n            precondition_provided,\n            bucket,\n            object,\n            notification,\n            hmac_key,\n            file_data,\n        )\n    except Exception as e:\n        logging.exception(f\"Caught an exception while running retry instructions\\n {e}\")\n        success_results = False\n    else:\n        success_results = True\n\n    # Assert expected success for each scenario.\n    assert (\n        expect_success == success_results\n    ), \"Retry API call expected_success was {}, should be {}\".format(\n        success_results, expect_success\n    )\n\n    # Verify that all instructions were used up during the test\n    # (indicates that the client sent the correct requests).\n    status_response = _get_retry_test(host, id)\n    assert (\n        status_response[\"completed\"] is True\n    ), \"Retry test not completed; unused instructions:{}\".format(\n        status_response[\"instructions\"]\n    )\n\n    # Clean up and close out test in testbench.\n    _delete_retry_test(host, id)\n\n\n########################################################################################################################################\n### Run Conformance Tests for Retry Strategy ###########################################################################################\n########################################################################################################################################\n\n# Pull storage-testbench docker image\nsubprocess.run(_PULL_CMD)\ntime.sleep(5)\n\n# Run docker image to start storage-testbench\nwith subprocess.Popen(_RUN_CMD) as proc:\n    # Run retry conformance tests\n    for scenario in _CONFORMANCE_TESTS:\n        id = scenario[\"id\"]\n        methods = scenario[\"methods\"]\n        cases = scenario[\"cases\"]\n        for i, c in enumerate(cases):\n            for m in methods:\n                method_name = m[\"name\"]\n                method_group = m[\"group\"] if m.get(\"group\", None) else m[\"name\"]\n                if method_group not in method_mapping:\n                    logging.info(f\"No tests for operation {method_name}\")\n                    continue\n\n                for lib_func in method_mapping[method_group]:\n                    test_name = f\"test-S{id}-{method_name}-{lib_func.__name__}-{i}\"\n                    globals()[test_name] = functools.partial(\n                        run_test_case, id, m, c, lib_func, _HOST\n                    )\n    time.sleep(5)\n    proc.kill()\n", "tests/conformance/conftest.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport random\nimport uuid\n\nimport pytest\n\nfrom google.auth.credentials import AnonymousCredentials\nfrom google.cloud import storage\nfrom google.cloud.exceptions import NotFound\n\n\n\"\"\"Environment variable or default host for Storage testbench emulator.\"\"\"\n_HOST = os.environ.get(\"STORAGE_EMULATOR_HOST\", \"http://localhost:9000\")\n\n\n\"\"\"Emulated project information for the storage testbench.\"\"\"\n_CONF_TEST_PROJECT_ID = \"my-project-id\"\n_CONF_TEST_SERVICE_ACCOUNT_EMAIL = (\n    \"my-service-account@my-project-id.iam.gserviceaccount.com\"\n)\n_CONF_TEST_PUBSUB_TOPIC_NAME = \"my-topic-name\"\n\n\n\"\"\"Create content payload in different sizes.\"\"\"\n\n\ndef _create_block(desired_kib):\n    line = \"abcdefXYZ123456789ADDINGrandom#\"  # len(line) = 31\n    multiplier = int(desired_kib / (len(line) + 1))\n    lines = \"\".join(\n        line + str(random.randint(0, 9)) for _ in range(multiplier)\n    )  # add random single digit integers\n    return 1024 * lines\n\n\n_STRING_CONTENT = \"hello world\"\n_SIZE_9MB = 9216  # 9*1024 KiB\n\n\n########################################################################################################################################\n### Pytest Fixtures to Populate Retry Conformance Test Resources #######################################################################\n########################################################################################################################################\n\n\n@pytest.fixture\ndef client():\n    client = storage.Client(\n        project=_CONF_TEST_PROJECT_ID,\n        credentials=AnonymousCredentials(),\n        client_options={\"api_endpoint\": _HOST},\n    )\n    return client\n\n\n@pytest.fixture\ndef bucket(client):\n    bucket = client.bucket(uuid.uuid4().hex)\n    client.create_bucket(bucket)\n    yield bucket\n    try:\n        bucket.delete(force=True)\n    except NotFound:  # in cases where bucket is deleted within the test\n        pass\n\n\n@pytest.fixture\ndef object(client, bucket):\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    blob.upload_from_string(_STRING_CONTENT)\n    blob.reload()\n    yield blob\n    try:\n        blob.delete()\n    except NotFound:  # in cases where object is deleted within the test\n        pass\n\n\n@pytest.fixture\ndef notification(client, bucket):\n    notification = client.bucket(bucket.name).notification(\n        topic_name=_CONF_TEST_PUBSUB_TOPIC_NAME\n    )\n    notification.create()\n    notification.reload()\n    yield notification\n    try:\n        notification.delete()\n    except NotFound:  # in cases where notification is deleted within the test\n        pass\n\n\n@pytest.fixture\ndef hmac_key(client):\n    hmac_key, _secret = client.create_hmac_key(\n        service_account_email=_CONF_TEST_SERVICE_ACCOUNT_EMAIL,\n        project_id=_CONF_TEST_PROJECT_ID,\n    )\n    yield hmac_key\n    try:\n        hmac_key.state = \"INACTIVE\"\n        hmac_key.update()\n        hmac_key.delete()\n    except NotFound:  # in cases where hmac_key is deleted within the test\n        pass\n\n\n@pytest.fixture\ndef file_data(client, bucket):\n    blob = client.bucket(bucket.name).blob(uuid.uuid4().hex)\n    payload = _create_block(_SIZE_9MB)\n    blob.upload_from_string(payload)\n    yield blob, payload\n    try:\n        blob.delete()\n    except NotFound:  # in cases where object is deleted within the test\n        pass\n", "tests/conformance/__init__.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport json\nimport os\n\n\ndef _read_local_json(json_file):\n    here = os.path.dirname(__file__)\n    json_path = os.path.abspath(os.path.join(here, json_file))\n    with io.open(json_path, \"r\", encoding=\"utf-8-sig\") as fileobj:\n        return json.load(fileobj)\n", "google/cloud/storage/_signing.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport base64\nimport binascii\nimport collections\nimport datetime\nimport hashlib\nimport json\n\nimport http\nimport urllib\n\nimport google.auth.credentials\n\nfrom google.auth import exceptions\nfrom google.auth.transport import requests\nfrom google.cloud import _helpers\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\n\n\n# `google.cloud.storage._signing.NOW` is deprecated.\n# Use `_NOW(_UTC)` instead.\nNOW = datetime.datetime.utcnow\n\nSERVICE_ACCOUNT_URL = (\n    \"https://googleapis.dev/python/google-api-core/latest/\"\n    \"auth.html#setting-up-a-service-account\"\n)\n\n\ndef ensure_signed_credentials(credentials):\n    \"\"\"Raise AttributeError if the credentials are unsigned.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: The credentials used to create a private key\n                        for signing text.\n\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n    \"\"\"\n    if not isinstance(credentials, google.auth.credentials.Signing):\n        raise AttributeError(\n            \"you need a private key to sign credentials.\"\n            \"the credentials you are currently using {} \"\n            \"just contains a token. see {} for more \"\n            \"details.\".format(type(credentials), SERVICE_ACCOUNT_URL)\n        )\n\n\ndef get_signed_query_params_v2(credentials, expiration, string_to_sign):\n    \"\"\"Gets query parameters for creating a signed URL.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: The credentials used to create a private key\n                        for signing text.\n\n    :type expiration: int or long\n    :param expiration: When the signed URL should expire.\n\n    :type string_to_sign: str\n    :param string_to_sign: The string to be signed by the credentials.\n\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: dict\n    :returns: Query parameters matching the signing credentials with a\n              signed payload.\n    \"\"\"\n    ensure_signed_credentials(credentials)\n    signature_bytes = credentials.sign_bytes(string_to_sign.encode(\"ascii\"))\n    signature = base64.b64encode(signature_bytes)\n    service_account_name = credentials.signer_email\n    return {\n        \"GoogleAccessId\": service_account_name,\n        \"Expires\": expiration,\n        \"Signature\": signature,\n    }\n\n\ndef get_expiration_seconds_v2(expiration):\n    \"\"\"Convert 'expiration' to a number of seconds in the future.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n\n    :rtype: int\n    :returns: a timestamp as an absolute number of seconds since epoch.\n    \"\"\"\n    # If it's a timedelta, add it to `now` in UTC.\n    if isinstance(expiration, datetime.timedelta):\n        now = _NOW(_UTC)\n        expiration = now + expiration\n\n    # If it's a datetime, convert to a timestamp.\n    if isinstance(expiration, datetime.datetime):\n        micros = _helpers._microseconds_from_datetime(expiration)\n        expiration = micros // 10**6\n\n    if not isinstance(expiration, int):\n        raise TypeError(\n            \"Expected an integer timestamp, datetime, or \"\n            \"timedelta. Got %s\" % type(expiration)\n        )\n    return expiration\n\n\n_EXPIRATION_TYPES = (int, datetime.datetime, datetime.timedelta)\n\n\ndef get_expiration_seconds_v4(expiration):\n    \"\"\"Convert 'expiration' to a number of seconds offset from the current time.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`ValueError` when expiration is too large.\n    :rtype: Integer\n    :returns: seconds in the future when the signed URL will expire\n    \"\"\"\n    if not isinstance(expiration, _EXPIRATION_TYPES):\n        raise TypeError(\n            \"Expected an integer timestamp, datetime, or \"\n            \"timedelta. Got %s\" % type(expiration)\n        )\n\n    now = _NOW(_UTC)\n\n    if isinstance(expiration, int):\n        seconds = expiration\n\n    if isinstance(expiration, datetime.datetime):\n        if expiration.tzinfo is None:\n            expiration = expiration.replace(tzinfo=_helpers.UTC)\n        expiration = expiration - now\n\n    if isinstance(expiration, datetime.timedelta):\n        seconds = int(expiration.total_seconds())\n\n    if seconds > SEVEN_DAYS:\n        raise ValueError(f\"Max allowed expiration interval is seven days {SEVEN_DAYS}\")\n\n    return seconds\n\n\ndef get_canonical_headers(headers):\n    \"\"\"Canonicalize headers for signing.\n\n    See:\n    https://cloud.google.com/storage/docs/access-control/signed-urls#about-canonical-extension-headers\n\n    :type headers: Union[dict|List(Tuple(str,str))]\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :rtype: str\n    :returns: List of headers, normalized / sortted per the URL refernced above.\n    \"\"\"\n    if headers is None:\n        headers = []\n    elif isinstance(headers, dict):\n        headers = list(headers.items())\n\n    if not headers:\n        return [], []\n\n    normalized = collections.defaultdict(list)\n    for key, val in headers:\n        key = key.lower().strip()\n        val = \" \".join(val.split())\n        normalized[key].append(val)\n\n    ordered_headers = sorted((key, \",\".join(val)) for key, val in normalized.items())\n\n    canonical_headers = [\"{}:{}\".format(*item) for item in ordered_headers]\n    return canonical_headers, ordered_headers\n\n\n_Canonical = collections.namedtuple(\n    \"_Canonical\", [\"method\", \"resource\", \"query_parameters\", \"headers\"]\n)\n\n\ndef canonicalize_v2(method, resource, query_parameters, headers):\n    \"\"\"Canonicalize method, resource per the V2 spec.\n\n    :type method: str\n    :param method: The HTTP verb that will be used when requesting the URL.\n                   Defaults to ``'GET'``. If method is ``'RESUMABLE'`` then the\n                   signature will additionally contain the `x-goog-resumable`\n                   header, and the method changed to POST. See the signed URL\n                   docs regarding this flow:\n                   https://cloud.google.com/storage/docs/access-control/signed-urls\n\n    :type resource: str\n    :param resource: A pointer to a specific resource\n                     (typically, ``/bucket-name/path/to/blob.txt``).\n\n    :type query_parameters: dict\n    :param query_parameters:\n        (Optional) Additional query parameters to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n    :type headers: Union[dict|List(Tuple(str,str))]\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :rtype: :class:_Canonical\n    :returns: Canonical method, resource, query_parameters, and headers.\n    \"\"\"\n    headers, _ = get_canonical_headers(headers)\n\n    if method == \"RESUMABLE\":\n        method = \"POST\"\n        headers.append(\"x-goog-resumable:start\")\n\n    if query_parameters is None:\n        return _Canonical(method, resource, [], headers)\n\n    normalized_qp = sorted(\n        (key.lower(), value and value.strip() or \"\")\n        for key, value in query_parameters.items()\n    )\n    encoded_qp = urllib.parse.urlencode(normalized_qp)\n    canonical_resource = f\"{resource}?{encoded_qp}\"\n    return _Canonical(method, canonical_resource, normalized_qp, headers)\n\n\ndef generate_signed_url_v2(\n    credentials,\n    resource,\n    expiration,\n    api_access_endpoint=\"\",\n    method=\"GET\",\n    content_md5=None,\n    content_type=None,\n    response_type=None,\n    response_disposition=None,\n    generation=None,\n    headers=None,\n    query_parameters=None,\n    service_account_email=None,\n    access_token=None,\n):\n    \"\"\"Generate a V2 signed URL to provide query-string auth'n to a resource.\n\n    .. note::\n\n        Assumes ``credentials`` implements the\n        :class:`google.auth.credentials.Signing` interface. Also assumes\n        ``credentials`` has a ``signer_email`` property which\n        identifies the credentials.\n\n    .. note::\n\n        If you are on Google Compute Engine, you can't generate a signed URL.\n        If you'd like to be able to generate a signed URL from GCE, you can use a\n        standard service account from a JSON file rather than a GCE service account.\n\n    See headers [reference](https://cloud.google.com/storage/docs/reference-headers)\n    for more details on optional arguments.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: Credentials object with an associated private key to\n                        sign text.\n\n    :type resource: str\n    :param resource: A pointer to a specific resource\n                     (typically, ``/bucket-name/path/to/blob.txt``).\n                     Caller should have already URL-encoded the value.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :type api_access_endpoint: str\n    :param api_access_endpoint: (Optional) URI base. Defaults to empty string.\n\n    :type method: str\n    :param method: The HTTP verb that will be used when requesting the URL.\n                   Defaults to ``'GET'``. If method is ``'RESUMABLE'`` then the\n                   signature will additionally contain the `x-goog-resumable`\n                   header, and the method changed to POST. See the signed URL\n                   docs regarding this flow:\n                   https://cloud.google.com/storage/docs/access-control/signed-urls\n\n\n    :type content_md5: str\n    :param content_md5: (Optional) The MD5 hash of the object referenced by\n                        ``resource``.\n\n    :type content_type: str\n    :param content_type: (Optional) The content type of the object referenced\n                         by ``resource``.\n\n    :type response_type: str\n    :param response_type: (Optional) Content type of responses to requests for\n                          the signed URL. Ignored if content_type is set on\n                          object/blob metadata.\n\n    :type response_disposition: str\n    :param response_disposition: (Optional) Content disposition of responses to\n                                 requests for the signed URL.\n\n    :type generation: str\n    :param generation: (Optional) A value that indicates which generation of\n                       the resource to fetch.\n\n    :type headers: Union[dict|List(Tuple(str,str))]\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :type service_account_email: str\n    :param service_account_email: (Optional) E-mail address of the service account.\n\n    :type access_token: str\n    :param access_token: (Optional) Access token for a service account.\n\n    :type query_parameters: dict\n    :param query_parameters:\n        (Optional) Additional query parameters to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: str\n    :returns: A signed URL you can use to access the resource\n              until expiration.\n    \"\"\"\n    expiration_stamp = get_expiration_seconds_v2(expiration)\n\n    canonical = canonicalize_v2(method, resource, query_parameters, headers)\n\n    # Generate the string to sign.\n    elements_to_sign = [\n        canonical.method,\n        content_md5 or \"\",\n        content_type or \"\",\n        str(expiration_stamp),\n    ]\n    elements_to_sign.extend(canonical.headers)\n    elements_to_sign.append(canonical.resource)\n    string_to_sign = \"\\n\".join(elements_to_sign)\n\n    # If you are on Google Compute Engine, you can't generate a signed URL.\n    # See https://github.com/googleapis/google-cloud-python/issues/922\n    # Set the right query parameters.\n    if access_token and service_account_email:\n        signature = _sign_message(string_to_sign, access_token, service_account_email)\n        signed_query_params = {\n            \"GoogleAccessId\": service_account_email,\n            \"Expires\": expiration_stamp,\n            \"Signature\": signature,\n        }\n    else:\n        signed_query_params = get_signed_query_params_v2(\n            credentials, expiration_stamp, string_to_sign\n        )\n\n    if response_type is not None:\n        signed_query_params[\"response-content-type\"] = response_type\n    if response_disposition is not None:\n        signed_query_params[\"response-content-disposition\"] = response_disposition\n    if generation is not None:\n        signed_query_params[\"generation\"] = generation\n\n    signed_query_params.update(canonical.query_parameters)\n    sorted_signed_query_params = sorted(signed_query_params.items())\n\n    # Return the built URL.\n    return \"{endpoint}{resource}?{querystring}\".format(\n        endpoint=api_access_endpoint,\n        resource=resource,\n        querystring=urllib.parse.urlencode(sorted_signed_query_params),\n    )\n\n\nSEVEN_DAYS = 7 * 24 * 60 * 60  # max age for V4 signed URLs.\nDEFAULT_ENDPOINT = \"https://storage.googleapis.com\"\n\n\ndef generate_signed_url_v4(\n    credentials,\n    resource,\n    expiration,\n    api_access_endpoint=DEFAULT_ENDPOINT,\n    method=\"GET\",\n    content_md5=None,\n    content_type=None,\n    response_type=None,\n    response_disposition=None,\n    generation=None,\n    headers=None,\n    query_parameters=None,\n    service_account_email=None,\n    access_token=None,\n    _request_timestamp=None,  # for testing only\n):\n    \"\"\"Generate a V4 signed URL to provide query-string auth'n to a resource.\n\n    .. note::\n\n        Assumes ``credentials`` implements the\n        :class:`google.auth.credentials.Signing` interface. Also assumes\n        ``credentials`` has a ``signer_email`` property which\n        identifies the credentials.\n\n    .. note::\n\n        If you are on Google Compute Engine, you can't generate a signed URL.\n        If you'd like to be able to generate a signed URL from GCE,you can use a\n        standard service account from a JSON file rather than a GCE service account.\n\n    See headers [reference](https://cloud.google.com/storage/docs/reference-headers)\n    for more details on optional arguments.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: Credentials object with an associated private key to\n                        sign text. That credentials must provide signer_email\n                        only if service_account_email and access_token are not\n                        passed.\n\n    :type resource: str\n    :param resource: A pointer to a specific resource\n                     (typically, ``/bucket-name/path/to/blob.txt``).\n                     Caller should have already URL-encoded the value.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire. If\n                       a ``datetime`` instance is passed without an explicit\n                       ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n    :type api_access_endpoint: str\n    :param api_access_endpoint: URI base. Defaults to\n                                \"https://storage.googleapis.com/\"\n\n    :type method: str\n    :param method: The HTTP verb that will be used when requesting the URL.\n                   Defaults to ``'GET'``. If method is ``'RESUMABLE'`` then the\n                   signature will additionally contain the `x-goog-resumable`\n                   header, and the method changed to POST. See the signed URL\n                   docs regarding this flow:\n                   https://cloud.google.com/storage/docs/access-control/signed-urls\n\n\n    :type content_md5: str\n    :param content_md5: (Optional) The MD5 hash of the object referenced by\n                        ``resource``.\n\n    :type content_type: str\n    :param content_type: (Optional) The content type of the object referenced\n                         by ``resource``.\n\n    :type response_type: str\n    :param response_type: (Optional) Content type of responses to requests for\n                          the signed URL. Ignored if content_type is set on\n                          object/blob metadata.\n\n    :type response_disposition: str\n    :param response_disposition: (Optional) Content disposition of responses to\n                                 requests for the signed URL.\n\n    :type generation: str\n    :param generation: (Optional) A value that indicates which generation of\n                       the resource to fetch.\n\n    :type headers: dict\n    :param headers:\n        (Optional) Additional HTTP headers to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers\n        Requests using the signed URL *must* pass the specified header\n        (name and value) with each request for the URL.\n\n    :type query_parameters: dict\n    :param query_parameters:\n        (Optional) Additional query parameters to be included as part of the\n        signed URLs.  See:\n        https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n    :type service_account_email: str\n    :param service_account_email: (Optional) E-mail address of the service account.\n\n    :type access_token: str\n    :param access_token: (Optional) Access token for a service account.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: str\n    :returns: A signed URL you can use to access the resource\n              until expiration.\n    \"\"\"\n    expiration_seconds = get_expiration_seconds_v4(expiration)\n\n    if _request_timestamp is None:\n        request_timestamp, datestamp = get_v4_now_dtstamps()\n    else:\n        request_timestamp = _request_timestamp\n        datestamp = _request_timestamp[:8]\n\n    # If you are on Google Compute Engine, you can't generate a signed URL.\n    # See https://github.com/googleapis/google-cloud-python/issues/922\n    client_email = service_account_email\n    if not access_token or not service_account_email:\n        ensure_signed_credentials(credentials)\n        client_email = credentials.signer_email\n\n    credential_scope = f\"{datestamp}/auto/storage/goog4_request\"\n    credential = f\"{client_email}/{credential_scope}\"\n\n    if headers is None:\n        headers = {}\n\n    if content_type is not None:\n        headers[\"Content-Type\"] = content_type\n\n    if content_md5 is not None:\n        headers[\"Content-MD5\"] = content_md5\n\n    header_names = [key.lower() for key in headers]\n    if \"host\" not in header_names:\n        headers[\"Host\"] = urllib.parse.urlparse(api_access_endpoint).netloc\n\n    if method.upper() == \"RESUMABLE\":\n        method = \"POST\"\n        headers[\"x-goog-resumable\"] = \"start\"\n\n    canonical_headers, ordered_headers = get_canonical_headers(headers)\n    canonical_header_string = (\n        \"\\n\".join(canonical_headers) + \"\\n\"\n    )  # Yes, Virginia, the extra newline is part of the spec.\n    signed_headers = \";\".join([key for key, _ in ordered_headers])\n\n    if query_parameters is None:\n        query_parameters = {}\n    else:\n        query_parameters = {key: value or \"\" for key, value in query_parameters.items()}\n\n    query_parameters[\"X-Goog-Algorithm\"] = \"GOOG4-RSA-SHA256\"\n    query_parameters[\"X-Goog-Credential\"] = credential\n    query_parameters[\"X-Goog-Date\"] = request_timestamp\n    query_parameters[\"X-Goog-Expires\"] = expiration_seconds\n    query_parameters[\"X-Goog-SignedHeaders\"] = signed_headers\n\n    if response_type is not None:\n        query_parameters[\"response-content-type\"] = response_type\n\n    if response_disposition is not None:\n        query_parameters[\"response-content-disposition\"] = response_disposition\n\n    if generation is not None:\n        query_parameters[\"generation\"] = generation\n\n    canonical_query_string = _url_encode(query_parameters)\n\n    lowercased_headers = dict(ordered_headers)\n\n    if \"x-goog-content-sha256\" in lowercased_headers:\n        payload = lowercased_headers[\"x-goog-content-sha256\"]\n    else:\n        payload = \"UNSIGNED-PAYLOAD\"\n\n    canonical_elements = [\n        method,\n        resource,\n        canonical_query_string,\n        canonical_header_string,\n        signed_headers,\n        payload,\n    ]\n    canonical_request = \"\\n\".join(canonical_elements)\n\n    canonical_request_hash = hashlib.sha256(\n        canonical_request.encode(\"ascii\")\n    ).hexdigest()\n\n    string_elements = [\n        \"GOOG4-RSA-SHA256\",\n        request_timestamp,\n        credential_scope,\n        canonical_request_hash,\n    ]\n    string_to_sign = \"\\n\".join(string_elements)\n\n    if access_token and service_account_email:\n        signature = _sign_message(string_to_sign, access_token, service_account_email)\n        signature_bytes = base64.b64decode(signature)\n        signature = binascii.hexlify(signature_bytes).decode(\"ascii\")\n    else:\n        signature_bytes = credentials.sign_bytes(string_to_sign.encode(\"ascii\"))\n        signature = binascii.hexlify(signature_bytes).decode(\"ascii\")\n\n    return \"{}{}?{}&X-Goog-Signature={}\".format(\n        api_access_endpoint, resource, canonical_query_string, signature\n    )\n\n\ndef get_v4_now_dtstamps():\n    \"\"\"Get current timestamp and datestamp in V4 valid format.\n\n    :rtype: str, str\n    :returns: Current timestamp, datestamp.\n    \"\"\"\n    now = _NOW(_UTC).replace(tzinfo=None)\n    timestamp = now.strftime(\"%Y%m%dT%H%M%SZ\")\n    datestamp = now.date().strftime(\"%Y%m%d\")\n    return timestamp, datestamp\n\n\ndef _sign_message(message, access_token, service_account_email):\n    \"\"\"Signs a message.\n\n    :type message: str\n    :param message: The message to be signed.\n\n    :type access_token: str\n    :param access_token: Access token for a service account.\n\n\n    :type service_account_email: str\n    :param service_account_email: E-mail address of the service account.\n\n    :raises: :exc:`TransportError` if an `access_token` is unauthorized.\n\n    :rtype: str\n    :returns: The signature of the message.\n\n    \"\"\"\n    message = _helpers._to_bytes(message)\n\n    method = \"POST\"\n    url = \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/{}:signBlob?alt=json\".format(\n        service_account_email\n    )\n    headers = {\n        \"Authorization\": \"Bearer \" + access_token,\n        \"Content-type\": \"application/json\",\n    }\n    body = json.dumps({\"payload\": base64.b64encode(message).decode(\"utf-8\")})\n\n    request = requests.Request()\n    response = request(url=url, method=method, body=body, headers=headers)\n\n    if response.status != http.client.OK:\n        raise exceptions.TransportError(\n            f\"Error calling the IAM signBytes API: {response.data}\"\n        )\n\n    data = json.loads(response.data.decode(\"utf-8\"))\n    return data[\"signedBlob\"]\n\n\ndef _url_encode(query_params):\n    \"\"\"Encode query params into URL.\n\n    :type query_params: dict\n    :param query_params: Query params to be encoded.\n\n    :rtype: str\n    :returns: URL encoded query params.\n    \"\"\"\n    params = [\n        f\"{_quote_param(name)}={_quote_param(value)}\"\n        for name, value in query_params.items()\n    ]\n\n    return \"&\".join(sorted(params))\n\n\ndef _quote_param(param):\n    \"\"\"Quote query param.\n\n    :type param: Any\n    :param param: Query param to be encoded.\n\n    :rtype: str\n    :returns: URL encoded query param.\n    \"\"\"\n    if not isinstance(param, bytes):\n        param = str(param)\n    return urllib.parse.quote(param, safe=\"~\")\n", "google/cloud/storage/_http.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Create / interact with Google Cloud Storage connections.\"\"\"\n\nimport functools\nfrom google.cloud import _http\nfrom google.cloud.storage import __version__\nfrom google.cloud.storage import _helpers\n\n\nclass Connection(_http.JSONConnection):\n    \"\"\"A connection to Google Cloud Storage via the JSON REST API.\n\n    Mutual TLS will be enabled if the \"GOOGLE_API_USE_CLIENT_CERTIFICATE\"\n    environment variable is set to the exact string \"true\" (case-sensitive).\n\n    Mutual TLS is not compatible with any API endpoint or universe domain\n    override at this time. If such settings are enabled along with\n    \"GOOGLE_API_USE_CLIENT_CERTIFICATE\", a ValueError will be raised.\n\n    :type client: :class:`~google.cloud.storage.client.Client`\n    :param client: The client that owns the current connection.\n\n    :type client_info: :class:`~google.api_core.client_info.ClientInfo`\n    :param client_info: (Optional) instance used to generate user agent.\n\n    :type api_endpoint: str\n    :param api_endpoint: (Optional) api endpoint to use.\n    \"\"\"\n\n    DEFAULT_API_ENDPOINT = _helpers._get_default_storage_base_url()\n    DEFAULT_API_MTLS_ENDPOINT = \"https://storage.mtls.googleapis.com\"\n\n    def __init__(self, client, client_info=None, api_endpoint=None):\n        super(Connection, self).__init__(client, client_info)\n        self.API_BASE_URL = api_endpoint or self.DEFAULT_API_ENDPOINT\n        self.API_BASE_MTLS_URL = self.DEFAULT_API_MTLS_ENDPOINT\n        self.ALLOW_AUTO_SWITCH_TO_MTLS_URL = api_endpoint is None\n        self._client_info.client_library_version = __version__\n\n        # TODO: When metrics all use gccl, this should be removed #9552\n        if self._client_info.user_agent is None:  # pragma: no branch\n            self._client_info.user_agent = \"\"\n        agent_version = f\"gcloud-python/{__version__}\"\n        if agent_version not in self._client_info.user_agent:\n            self._client_info.user_agent += f\" {agent_version} \"\n\n    API_VERSION = _helpers._API_VERSION\n    \"\"\"The version of the API, used in building the API call's URL.\"\"\"\n\n    API_URL_TEMPLATE = \"{api_base_url}/storage/{api_version}{path}\"\n    \"\"\"A template for the URL of a particular API call.\"\"\"\n\n    def api_request(self, *args, **kwargs):\n        retry = kwargs.pop(\"retry\", None)\n        kwargs[\"extra_api_info\"] = _helpers._get_invocation_id()\n        call = functools.partial(super(Connection, self).api_request, *args, **kwargs)\n        if retry:\n            # If this is a ConditionalRetryPolicy, check conditions.\n            try:\n                retry = retry.get_retry_policy_if_conditions_met(**kwargs)\n            except AttributeError:  # This is not a ConditionalRetryPolicy.\n                pass\n            if retry:\n                call = retry(call)\n        return call()\n", "google/cloud/storage/hmac_key.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configure HMAC keys that can be used to authenticate requests to Google Cloud Storage.\n\nSee [HMAC keys documentation](https://cloud.google.com/storage/docs/authentication/hmackeys)\n\"\"\"\n\nfrom google.cloud.exceptions import NotFound\nfrom google.cloud._helpers import _rfc3339_nanos_to_datetime\n\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\n\n\nclass HMACKeyMetadata(object):\n    \"\"\"Metadata about an HMAC service account key withn Cloud Storage.\n\n    :type client: :class:`~google.cloud.stoage.client.Client`\n    :param client: client associated with the key metadata.\n\n    :type access_id: str\n    :param access_id: (Optional) Unique ID of an existing key.\n\n    :type project_id: str\n    :param project_id: (Optional) Project ID of an existing key.\n        Defaults to client's project.\n\n    :type user_project: str\n    :param user_project: (Optional) This parameter is currently ignored.\n    \"\"\"\n\n    ACTIVE_STATE = \"ACTIVE\"\n    \"\"\"Key is active, and may be used to sign requests.\"\"\"\n    INACTIVE_STATE = \"INACTIVE\"\n    \"\"\"Key is inactive, and may not be used to sign requests.\n\n    It can be re-activated via :meth:`update`.\n    \"\"\"\n    DELETED_STATE = \"DELETED\"\n    \"\"\"Key is deleted.  It cannot be re-activated.\"\"\"\n\n    _SETTABLE_STATES = (ACTIVE_STATE, INACTIVE_STATE)\n\n    def __init__(self, client, access_id=None, project_id=None, user_project=None):\n        self._client = client\n        self._properties = {}\n\n        if access_id is not None:\n            self._properties[\"accessId\"] = access_id\n\n        if project_id is not None:\n            self._properties[\"projectId\"] = project_id\n\n        self._user_project = user_project\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._client == other._client and self.access_id == other.access_id\n\n    def __hash__(self):\n        return hash(self._client) + hash(self.access_id)\n\n    @property\n    def access_id(self):\n        \"\"\"Access ID of the key.\n\n        :rtype: str or None\n        :returns: unique identifier of the key within a project.\n        \"\"\"\n        return self._properties.get(\"accessId\")\n\n    @property\n    def etag(self):\n        \"\"\"ETag identifying the version of the key metadata.\n\n        :rtype: str or None\n        :returns: ETag for the version of the key's metadata.\n        \"\"\"\n        return self._properties.get(\"etag\")\n\n    @property\n    def id(self):\n        \"\"\"ID of the key, including the Project ID and the Access ID.\n\n        :rtype: str or None\n        :returns: ID of the key.\n        \"\"\"\n        return self._properties.get(\"id\")\n\n    @property\n    def project(self):\n        \"\"\"Project ID associated with the key.\n\n        :rtype: str or None\n        :returns: project identfier for the key.\n        \"\"\"\n        return self._properties.get(\"projectId\")\n\n    @property\n    def service_account_email(self):\n        \"\"\"Service account e-mail address associated with the key.\n\n        :rtype: str or None\n        :returns: e-mail address for the service account which created the key.\n        \"\"\"\n        return self._properties.get(\"serviceAccountEmail\")\n\n    @property\n    def state(self):\n        \"\"\"Get / set key's state.\n\n        One of:\n            - ``ACTIVE``\n            - ``INACTIVE``\n            - ``DELETED``\n\n        :rtype: str or None\n        :returns: key's current state.\n        \"\"\"\n        return self._properties.get(\"state\")\n\n    @state.setter\n    def state(self, value):\n        self._properties[\"state\"] = value\n\n    @property\n    def time_created(self):\n        \"\"\"Retrieve the timestamp at which the HMAC key was created.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"timeCreated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def updated(self):\n        \"\"\"Retrieve the timestamp at which the HMAC key was created.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"updated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def path(self):\n        \"\"\"Resource path for the metadata's key.\"\"\"\n\n        if self.access_id is None:\n            raise ValueError(\"No 'access_id' set.\")\n\n        project = self.project\n        if project is None:\n            project = self._client.project\n\n        return f\"/projects/{project}/hmacKeys/{self.access_id}\"\n\n    @property\n    def user_project(self):\n        \"\"\"Project ID to be billed for API requests made via this bucket.\n\n        This property is currently ignored by the server.\n\n        :rtype: str\n        \"\"\"\n        return self._user_project\n\n    def exists(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Determine whether or not the key for this metadata exists.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: bool\n        :returns: True if the key exists in Cloud Storage.\n        \"\"\"\n        try:\n            qs_params = {}\n\n            if self.user_project is not None:\n                qs_params[\"userProject\"] = self.user_project\n\n            self._client._get_resource(\n                self.path,\n                query_params=qs_params,\n                timeout=timeout,\n                retry=retry,\n            )\n        except NotFound:\n            return False\n        else:\n            return True\n\n    def reload(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Reload properties from Cloud Storage.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises :class:`~google.api_core.exceptions.NotFound`:\n            if the key does not exist on the back-end.\n        \"\"\"\n        qs_params = {}\n\n        if self.user_project is not None:\n            qs_params[\"userProject\"] = self.user_project\n\n        self._properties = self._client._get_resource(\n            self.path,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def update(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY_IF_ETAG_IN_JSON):\n        \"\"\"Save writable properties to Cloud Storage.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises :class:`~google.api_core.exceptions.NotFound`:\n            if the key does not exist on the back-end.\n        \"\"\"\n        qs_params = {}\n        if self.user_project is not None:\n            qs_params[\"userProject\"] = self.user_project\n\n        payload = {\"state\": self.state}\n        self._properties = self._client._put_resource(\n            self.path,\n            payload,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def delete(self, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Delete the key from Cloud Storage.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises :class:`~google.api_core.exceptions.NotFound`:\n            if the key does not exist on the back-end.\n        \"\"\"\n        qs_params = {}\n        if self.user_project is not None:\n            qs_params[\"userProject\"] = self.user_project\n\n        self._client._delete_resource(\n            self.path,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n", "google/cloud/storage/retry.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helpers for configuring retries with exponential back-off.\n\nSee [Retry Strategy for Google Cloud Storage](https://cloud.google.com/storage/docs/retry-strategy#client-libraries)\n\"\"\"\n\nimport requests\nimport requests.exceptions as requests_exceptions\n\nfrom google.api_core import exceptions as api_exceptions\nfrom google.api_core import retry\nfrom google.auth import exceptions as auth_exceptions\n\n\n_RETRYABLE_TYPES = (\n    api_exceptions.TooManyRequests,  # 429\n    api_exceptions.InternalServerError,  # 500\n    api_exceptions.BadGateway,  # 502\n    api_exceptions.ServiceUnavailable,  # 503\n    api_exceptions.GatewayTimeout,  # 504\n    ConnectionError,\n    requests.ConnectionError,\n    requests_exceptions.ChunkedEncodingError,\n    requests_exceptions.Timeout,\n)\n\n\n# Some retriable errors don't have their own custom exception in api_core.\n_ADDITIONAL_RETRYABLE_STATUS_CODES = (408,)\n\n\ndef _should_retry(exc):\n    \"\"\"Predicate for determining when to retry.\"\"\"\n    if isinstance(exc, _RETRYABLE_TYPES):\n        return True\n    elif isinstance(exc, api_exceptions.GoogleAPICallError):\n        return exc.code in _ADDITIONAL_RETRYABLE_STATUS_CODES\n    elif isinstance(exc, auth_exceptions.TransportError):\n        return _should_retry(exc.args[0])\n    else:\n        return False\n\n\nDEFAULT_RETRY = retry.Retry(predicate=_should_retry)\n\"\"\"The default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES.\n\nTo modify the default retry behavior, create a new retry object modeled after\nthis one by calling it a ``with_XXX`` method. For example, to create a copy of\nDEFAULT_RETRY with a deadline of 30 seconds, pass\n``retry=DEFAULT_RETRY.with_deadline(30)``. See google-api-core reference\n(https://googleapis.dev/python/google-api-core/latest/retry.html) for details.\n\"\"\"\n\n\nclass ConditionalRetryPolicy(object):\n    \"\"\"A class for use when an API call is only conditionally safe to retry.\n\n    This class is intended for use in inspecting the API call parameters of an\n    API call to verify that any flags necessary to make the API call idempotent\n    (such as specifying an ``if_generation_match`` or related flag) are present.\n\n    It can be used in place of a ``retry.Retry`` object, in which case\n    ``_http.Connection.api_request`` will pass the requested api call keyword\n    arguments into the ``conditional_predicate`` and return the ``retry_policy``\n    if the conditions are met.\n\n    :type retry_policy: class:`google.api_core.retry.Retry`\n    :param retry_policy: A retry object defining timeouts, persistence and which\n        exceptions to retry.\n\n    :type conditional_predicate: callable\n    :param conditional_predicate: A callable that accepts exactly the number of\n        arguments in ``required_kwargs``, in order, and returns True if the\n        arguments have sufficient data to determine that the call is safe to\n        retry (idempotent).\n\n    :type required_kwargs: list(str)\n    :param required_kwargs:\n        A list of keyword argument keys that will be extracted from the API call\n        and passed into the ``conditional predicate`` in order. For example,\n        ``[\"query_params\"]`` is commmonly used for preconditions in query_params.\n    \"\"\"\n\n    def __init__(self, retry_policy, conditional_predicate, required_kwargs):\n        self.retry_policy = retry_policy\n        self.conditional_predicate = conditional_predicate\n        self.required_kwargs = required_kwargs\n\n    def get_retry_policy_if_conditions_met(self, **kwargs):\n        if self.conditional_predicate(*[kwargs[key] for key in self.required_kwargs]):\n            return self.retry_policy\n        return None\n\n\ndef is_generation_specified(query_params):\n    \"\"\"Return True if generation or if_generation_match is specified.\"\"\"\n    generation = query_params.get(\"generation\") is not None\n    if_generation_match = query_params.get(\"ifGenerationMatch\") is not None\n    return generation or if_generation_match\n\n\ndef is_metageneration_specified(query_params):\n    \"\"\"Return True if if_metageneration_match is specified.\"\"\"\n    if_metageneration_match = query_params.get(\"ifMetagenerationMatch\") is not None\n    return if_metageneration_match\n\n\ndef is_etag_in_data(data):\n    \"\"\"Return True if an etag is contained in the request body.\n\n    :type data: dict or None\n    :param data: A dict representing the request JSON body. If not passed, returns False.\n    \"\"\"\n    return data is not None and \"etag\" in data\n\n\ndef is_etag_in_json(data):\n    \"\"\"\n    ``is_etag_in_json`` is supported for backwards-compatibility reasons only;\n    please use ``is_etag_in_data`` instead.\n    \"\"\"\n    return is_etag_in_data(data)\n\n\nDEFAULT_RETRY_IF_GENERATION_SPECIFIED = ConditionalRetryPolicy(\n    DEFAULT_RETRY, is_generation_specified, [\"query_params\"]\n)\n\"\"\"Conditional wrapper for the default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES, but only if the request included an\n``ifGenerationMatch`` header.\n\"\"\"\n\nDEFAULT_RETRY_IF_METAGENERATION_SPECIFIED = ConditionalRetryPolicy(\n    DEFAULT_RETRY, is_metageneration_specified, [\"query_params\"]\n)\n\"\"\"Conditional wrapper for the default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES, but only if the request included an\n``ifMetagenerationMatch`` header.\n\"\"\"\n\nDEFAULT_RETRY_IF_ETAG_IN_JSON = ConditionalRetryPolicy(\n    DEFAULT_RETRY, is_etag_in_json, [\"data\"]\n)\n\"\"\"Conditional wrapper for the default retry object.\n\nThis retry setting will retry all _RETRYABLE_TYPES and any status codes from\n_ADDITIONAL_RETRYABLE_STATUS_CODES, but only if the request included an\n``ETAG`` entry in its payload.\n\"\"\"\n", "google/cloud/storage/iam.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Storage API IAM policy definitions\n\nFor allowed roles / permissions, see:\nhttps://cloud.google.com/storage/docs/access-control/iam\n\"\"\"\n\n# Storage-specific IAM roles\n\nSTORAGE_OBJECT_CREATOR_ROLE = \"roles/storage.objectCreator\"\n\"\"\"Role implying rights to create objects, but not delete or overwrite them.\"\"\"\n\nSTORAGE_OBJECT_VIEWER_ROLE = \"roles/storage.objectViewer\"\n\"\"\"Role implying rights to view object properties, excluding ACLs.\"\"\"\n\nSTORAGE_OBJECT_ADMIN_ROLE = \"roles/storage.objectAdmin\"\n\"\"\"Role implying full control of objects.\"\"\"\n\nSTORAGE_ADMIN_ROLE = \"roles/storage.admin\"\n\"\"\"Role implying full control of objects and buckets.\"\"\"\n\nSTORAGE_VIEWER_ROLE = \"Viewer\"\n\"\"\"Can list buckets.\"\"\"\n\nSTORAGE_EDITOR_ROLE = \"Editor\"\n\"\"\"Can create, list, and delete buckets.\"\"\"\n\nSTORAGE_OWNER_ROLE = \"Owners\"\n\"\"\"Can create, list, and delete buckets.\"\"\"\n\n\n# Storage-specific permissions\n\nSTORAGE_BUCKETS_CREATE = \"storage.buckets.create\"\n\"\"\"Permission: create buckets.\"\"\"\n\nSTORAGE_BUCKETS_DELETE = \"storage.buckets.delete\"\n\"\"\"Permission: delete buckets.\"\"\"\n\nSTORAGE_BUCKETS_GET = \"storage.buckets.get\"\n\"\"\"Permission: read bucket metadata, excluding ACLs.\"\"\"\n\nSTORAGE_BUCKETS_GET_IAM_POLICY = \"storage.buckets.getIamPolicy\"\n\"\"\"Permission: read bucket ACLs.\"\"\"\n\nSTORAGE_BUCKETS_LIST = \"storage.buckets.list\"\n\"\"\"Permission: list buckets.\"\"\"\n\nSTORAGE_BUCKETS_SET_IAM_POLICY = \"storage.buckets.setIamPolicy\"\n\"\"\"Permission: update bucket ACLs.\"\"\"\n\nSTORAGE_BUCKETS_UPDATE = \"storage.buckets.list\"\n\"\"\"Permission: update buckets, excluding ACLS.\"\"\"\n\nSTORAGE_OBJECTS_CREATE = \"storage.objects.create\"\n\"\"\"Permission: add new objects to a bucket.\"\"\"\n\nSTORAGE_OBJECTS_DELETE = \"storage.objects.delete\"\n\"\"\"Permission: delete objects.\"\"\"\n\nSTORAGE_OBJECTS_GET = \"storage.objects.get\"\n\"\"\"Permission: read object data / metadata, excluding ACLs.\"\"\"\n\nSTORAGE_OBJECTS_GET_IAM_POLICY = \"storage.objects.getIamPolicy\"\n\"\"\"Permission: read object ACLs.\"\"\"\n\nSTORAGE_OBJECTS_LIST = \"storage.objects.list\"\n\"\"\"Permission: list objects in a bucket.\"\"\"\n\nSTORAGE_OBJECTS_SET_IAM_POLICY = \"storage.objects.setIamPolicy\"\n\"\"\"Permission: update object ACLs.\"\"\"\n\nSTORAGE_OBJECTS_UPDATE = \"storage.objects.update\"\n\"\"\"Permission: update object metadat, excluding ACLs.\"\"\"\n", "google/cloud/storage/version.py": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__version__ = \"2.17.0\"\n", "google/cloud/storage/transfer_manager.py": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Concurrent media operations.\"\"\"\n\nimport concurrent.futures\n\nimport io\nimport inspect\nimport os\nimport warnings\nimport pickle\nimport copyreg\nimport struct\nimport base64\nimport functools\n\nfrom google.api_core import exceptions\nfrom google.cloud.storage import Client\nfrom google.cloud.storage import Blob\nfrom google.cloud.storage.blob import _get_host_name\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage._helpers import _api_core_retry_to_resumable_media_retry\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\nimport google_crc32c\n\nfrom google.resumable_media.requests.upload import XMLMPUContainer\nfrom google.resumable_media.requests.upload import XMLMPUPart\nfrom google.resumable_media.common import DataCorruption\n\nTM_DEFAULT_CHUNK_SIZE = 32 * 1024 * 1024\nDEFAULT_MAX_WORKERS = 8\nMAX_CRC32C_ZERO_ARRAY_SIZE = 4 * 1024 * 1024\nMETADATA_HEADER_TRANSLATION = {\n    \"cacheControl\": \"Cache-Control\",\n    \"contentDisposition\": \"Content-Disposition\",\n    \"contentEncoding\": \"Content-Encoding\",\n    \"contentLanguage\": \"Content-Language\",\n    \"customTime\": \"x-goog-custom-time\",\n    \"storageClass\": \"x-goog-storage-class\",\n}\n\n# Constants to be passed in as `worker_type`.\nPROCESS = \"process\"\nTHREAD = \"thread\"\n\nDOWNLOAD_CRC32C_MISMATCH_TEMPLATE = \"\"\"\\\nChecksum mismatch while downloading:\n\n  {}\n\nThe object metadata indicated a crc32c checksum of:\n\n  {}\n\nbut the actual crc32c checksum of the downloaded contents was:\n\n  {}\n\"\"\"\n\n\n_cached_clients = {}\n\n\ndef _deprecate_threads_param(func):\n    @functools.wraps(func)\n    def convert_threads_or_raise(*args, **kwargs):\n        binding = inspect.signature(func).bind(*args, **kwargs)\n        threads = binding.arguments.get(\"threads\")\n        if threads:\n            worker_type = binding.arguments.get(\"worker_type\")\n            max_workers = binding.arguments.get(\"max_workers\")\n            if worker_type or max_workers:  # Parameter conflict\n                raise ValueError(\n                    \"The `threads` parameter is deprecated and conflicts with its replacement parameters, `worker_type` and `max_workers`.\"\n                )\n            # No conflict, so issue a warning and set worker_type and max_workers.\n            warnings.warn(\n                \"The `threads` parameter is deprecated. Please use `worker_type` and `max_workers` parameters instead.\"\n            )\n            args = binding.args\n            kwargs = binding.kwargs\n            kwargs[\"worker_type\"] = THREAD\n            kwargs[\"max_workers\"] = threads\n            return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n\n    return convert_threads_or_raise\n\n\n@_deprecate_threads_param\ndef upload_many(\n    file_blob_pairs,\n    skip_if_exists=False,\n    upload_kwargs=None,\n    threads=None,\n    deadline=None,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n):\n    \"\"\"Upload many files concurrently via a worker pool.\n\n    :type file_blob_pairs: List(Tuple(IOBase or str, 'google.cloud.storage.blob.Blob'))\n    :param file_blob_pairs:\n        A list of tuples of a file or filename and a blob. Each file will be\n        uploaded to the corresponding blob by using APIs identical to\n        `blob.upload_from_file()` or `blob.upload_from_filename()` as\n        appropriate.\n\n        File handlers are only supported if worker_type is set to THREAD.\n        If worker_type is set to PROCESS, please use filenames only.\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        If True, blobs that already have a live version will not be overwritten.\n        This is accomplished by setting `if_generation_match = 0` on uploads.\n        Uploads so skipped will result in a 412 Precondition Failed response\n        code, which will be included in the return value but not raised\n        as an exception regardless of the value of raise_exception.\n\n    :type upload_kwargs: dict\n    :param upload_kwargs:\n        A dictionary of keyword arguments to pass to the upload method. Refer\n        to the documentation for `blob.upload_from_file()` or\n        `blob.upload_from_filename()` for more information. The dict is directly\n        passed into the upload methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure.\n\n        If skip_if_exists is True, 412 Precondition Failed responses are\n        considered part of normal operation and are not raised as an exception.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n        PROCESS workers do not support writing to file handlers. Please refer\n        to files by filename only when using PROCESS workers.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        upload method is used (which will be None).\n    \"\"\"\n    if upload_kwargs is None:\n        upload_kwargs = {}\n\n    if skip_if_exists:\n        upload_kwargs = upload_kwargs.copy()\n        upload_kwargs[\"if_generation_match\"] = 0\n\n    upload_kwargs[\"command\"] = \"tm.upload_many\"\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n\n    with pool_class(max_workers=max_workers) as executor:\n        futures = []\n        for path_or_file, blob in file_blob_pairs:\n            # File objects are only supported by the THREAD worker because they can't\n            # be pickled.\n            if needs_pickling and not isinstance(path_or_file, str):\n                raise ValueError(\n                    \"Passing in a file object is only supported by the THREAD worker type. Please either select THREAD workers, or pass in filenames only.\"\n                )\n\n            futures.append(\n                executor.submit(\n                    _call_method_on_maybe_pickled_blob,\n                    _pickle_client(blob) if needs_pickling else blob,\n                    \"_handle_filename_and_upload\"\n                    if isinstance(path_or_file, str)\n                    else \"_prep_and_do_upload\",\n                    path_or_file,\n                    **upload_kwargs,\n                )\n            )\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    results = []\n    for future in futures:\n        exp = future.exception()\n\n        # If raise_exception is False, don't call future.result()\n        if exp and not raise_exception:\n            results.append(exp)\n        # If skip_if_exists and the exception is PreconditionFailed, do same.\n        elif exp and skip_if_exists and isinstance(exp, exceptions.PreconditionFailed):\n            results.append(exp)\n        # Get the real result. If there was an exception not handled above,\n        # this will raise it.\n        else:\n            results.append(future.result())\n    return results\n\n\n@_deprecate_threads_param\ndef download_many(\n    blob_file_pairs,\n    download_kwargs=None,\n    threads=None,\n    deadline=None,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    skip_if_exists=False,\n):\n    \"\"\"Download many blobs concurrently via a worker pool.\n\n    :type blob_file_pairs: List(Tuple('google.cloud.storage.blob.Blob', IOBase or str))\n    :param blob_file_pairs:\n        A list of tuples of blob and a file or filename. Each blob will be downloaded to the corresponding blob by using APIs identical to blob.download_to_file() or blob.download_to_filename() as appropriate.\n\n        Note that blob.download_to_filename() does not delete the destination file if the download fails.\n\n        File handlers are only supported if worker_type is set to THREAD.\n        If worker_type is set to PROCESS, please use filenames only.\n\n    :type download_kwargs: dict\n    :param download_kwargs:\n        A dictionary of keyword arguments to pass to the download method. Refer\n        to the documentation for `blob.download_to_file()` or\n        `blob.download_to_filename()` for more information. The dict is directly\n        passed into the download methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n        PROCESS workers do not support writing to file handlers. Please refer\n        to files by filename only when using PROCESS workers.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        Before downloading each blob, check if the file for the filename exists;\n        if it does, skip that blob.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        download method is used (which will be None).\n    \"\"\"\n\n    if download_kwargs is None:\n        download_kwargs = {}\n\n    download_kwargs[\"command\"] = \"tm.download_many\"\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n\n    with pool_class(max_workers=max_workers) as executor:\n        futures = []\n        for blob, path_or_file in blob_file_pairs:\n            # File objects are only supported by the THREAD worker because they can't\n            # be pickled.\n            if needs_pickling and not isinstance(path_or_file, str):\n                raise ValueError(\n                    \"Passing in a file object is only supported by the THREAD worker type. Please either select THREAD workers, or pass in filenames only.\"\n                )\n\n            if skip_if_exists and isinstance(path_or_file, str):\n                if os.path.isfile(path_or_file):\n                    continue\n\n            futures.append(\n                executor.submit(\n                    _call_method_on_maybe_pickled_blob,\n                    _pickle_client(blob) if needs_pickling else blob,\n                    \"_handle_filename_and_download\"\n                    if isinstance(path_or_file, str)\n                    else \"_prep_and_do_download\",\n                    path_or_file,\n                    **download_kwargs,\n                )\n            )\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    results = []\n    for future in futures:\n        # If raise_exception is False, don't call future.result()\n        if not raise_exception:\n            exp = future.exception()\n            if exp:\n                results.append(exp)\n                continue\n        # Get the real result. If there was an exception, this will raise it.\n        results.append(future.result())\n    return results\n\n\n@_deprecate_threads_param\ndef upload_many_from_filenames(\n    bucket,\n    filenames,\n    source_directory=\"\",\n    blob_name_prefix=\"\",\n    skip_if_exists=False,\n    blob_constructor_kwargs=None,\n    upload_kwargs=None,\n    threads=None,\n    deadline=None,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    additional_blob_attributes=None,\n):\n    \"\"\"Upload many files concurrently by their filenames.\n\n    The destination blobs are automatically created, with blob names based on\n    the source filenames and the blob_name_prefix.\n\n    For example, if the `filenames` include \"images/icon.jpg\",\n    `source_directory` is \"/home/myuser/\", and `blob_name_prefix` is \"myfiles/\",\n    then the file at \"/home/myuser/images/icon.jpg\" will be uploaded to a blob\n    named \"myfiles/images/icon.jpg\".\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket:\n        The bucket which will contain the uploaded blobs.\n\n    :type filenames: list(str)\n    :param filenames:\n        A list of filenames to be uploaded. This may include part of the path.\n        The file will be accessed at the full path of `source_directory` +\n        `filename`.\n\n    :type source_directory: str\n    :param source_directory:\n        A string that will be prepended (with `os.path.join()`) to each filename\n        in the input list, in order to find the source file for each blob.\n        Unlike the filename itself, the source_directory does not affect the\n        name of the uploaded blob.\n\n        For instance, if the source_directory is \"/tmp/img/\" and a filename is\n        \"0001.jpg\", with an empty blob_name_prefix, then the file uploaded will\n        be \"/tmp/img/0001.jpg\" and the destination blob will be \"0001.jpg\".\n\n        This parameter can be an empty string.\n\n        Note that this parameter allows directory traversal (e.g. \"/\", \"../\")\n        and is not intended for unsanitized end user input.\n\n    :type blob_name_prefix: str\n    :param blob_name_prefix:\n        A string that will be prepended to each filename in the input list, in\n        order to determine the name of the destination blob. Unlike the filename\n        itself, the prefix string does not affect the location the library will\n        look for the source data on the local filesystem.\n\n        For instance, if the source_directory is \"/tmp/img/\", the\n        blob_name_prefix is \"myuser/mystuff-\" and a filename is \"0001.jpg\" then\n        the file uploaded will be \"/tmp/img/0001.jpg\" and the destination blob\n        will be \"myuser/mystuff-0001.jpg\".\n\n        The blob_name_prefix can be blank (an empty string).\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        If True, blobs that already have a live version will not be overwritten.\n        This is accomplished by setting `if_generation_match = 0` on uploads.\n        Uploads so skipped will result in a 412 Precondition Failed response\n        code, which will be included in the return value, but not raised\n        as an exception regardless of the value of raise_exception.\n\n    :type blob_constructor_kwargs: dict\n    :param blob_constructor_kwargs:\n        A dictionary of keyword arguments to pass to the blob constructor. Refer\n        to the documentation for `blob.Blob()` for more information. The dict is\n        directly passed into the constructor and is not validated by this\n        function. `name` and `bucket` keyword arguments are reserved by this\n        function and will result in an error if passed in here.\n\n    :type upload_kwargs: dict\n    :param upload_kwargs:\n        A dictionary of keyword arguments to pass to the upload method. Refer\n        to the documentation for `blob.upload_from_file()` or\n        `blob.upload_from_filename()` for more information. The dict is directly\n        passed into the upload methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure.\n\n        If skip_if_exists is True, 412 Precondition Failed responses are\n        considered part of normal operation and are not raised as an exception.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type additional_blob_attributes: dict\n    :param additional_blob_attributes:\n        A dictionary of blob attribute names and values. This allows the\n        configuration of blobs beyond what is possible with\n        blob_constructor_kwargs. For instance, {\"cache_control\": \"no-cache\"}\n        would set the cache_control attribute of each blob to \"no-cache\".\n\n        As with blob_constructor_kwargs, this affects the creation of every\n        blob identically. To fine-tune each blob individually, use `upload_many`\n        and create the blobs as desired before passing them in.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        upload method is used (which will be None).\n    \"\"\"\n    if blob_constructor_kwargs is None:\n        blob_constructor_kwargs = {}\n    if additional_blob_attributes is None:\n        additional_blob_attributes = {}\n\n    file_blob_pairs = []\n\n    for filename in filenames:\n        path = os.path.join(source_directory, filename)\n        blob_name = blob_name_prefix + filename\n        blob = bucket.blob(blob_name, **blob_constructor_kwargs)\n        for prop, value in additional_blob_attributes.items():\n            setattr(blob, prop, value)\n        file_blob_pairs.append((path, blob))\n\n    return upload_many(\n        file_blob_pairs,\n        skip_if_exists=skip_if_exists,\n        upload_kwargs=upload_kwargs,\n        deadline=deadline,\n        raise_exception=raise_exception,\n        worker_type=worker_type,\n        max_workers=max_workers,\n    )\n\n\n@_deprecate_threads_param\ndef download_many_to_path(\n    bucket,\n    blob_names,\n    destination_directory=\"\",\n    blob_name_prefix=\"\",\n    download_kwargs=None,\n    threads=None,\n    deadline=None,\n    create_directories=True,\n    raise_exception=False,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    skip_if_exists=False,\n):\n    \"\"\"Download many files concurrently by their blob names.\n\n    The destination files are automatically created, with paths based on the\n    source blob_names and the destination_directory.\n\n    The destination files are not automatically deleted if their downloads fail,\n    so please check the return value of this function for any exceptions, or\n    enable `raise_exception=True`, and process the files accordingly.\n\n    For example, if the `blob_names` include \"icon.jpg\", `destination_directory`\n    is \"/home/myuser/\", and `blob_name_prefix` is \"images/\", then the blob named\n    \"images/icon.jpg\" will be downloaded to a file named\n    \"/home/myuser/icon.jpg\".\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket:\n        The bucket which contains the blobs to be downloaded\n\n    :type blob_names: list(str)\n    :param blob_names:\n        A list of blobs to be downloaded. The blob name in this string will be\n        used to determine the destination file path as well.\n\n        The full name to the blob must be blob_name_prefix + blob_name. The\n        blob_name is separate from the blob_name_prefix because the blob_name\n        will also determine the name of the destination blob. Any shared part of\n        the blob names that need not be part of the destination path should be\n        included in the blob_name_prefix.\n\n    :type destination_directory: str\n    :param destination_directory:\n        A string that will be prepended (with os.path.join()) to each blob_name\n        in the input list, in order to determine the destination path for that\n        blob.\n\n        For instance, if the destination_directory string is \"/tmp/img\" and a\n        blob_name is \"0001.jpg\", with an empty blob_name_prefix, then the source\n        blob \"0001.jpg\" will be downloaded to destination \"/tmp/img/0001.jpg\" .\n\n        This parameter can be an empty string.\n\n        Note that this parameter allows directory traversal (e.g. \"/\", \"../\")\n        and is not intended for unsanitized end user input.\n\n    :type blob_name_prefix: str\n    :param blob_name_prefix:\n        A string that will be prepended to each blob_name in the input list, in\n        order to determine the name of the source blob. Unlike the blob_name\n        itself, the prefix string does not affect the destination path on the\n        local filesystem. For instance, if the destination_directory is\n        \"/tmp/img/\", the blob_name_prefix is \"myuser/mystuff-\" and a blob_name\n        is \"0001.jpg\" then the source blob \"myuser/mystuff-0001.jpg\" will be\n        downloaded to \"/tmp/img/0001.jpg\". The blob_name_prefix can be blank\n        (an empty string).\n\n    :type download_kwargs: dict\n    :param download_kwargs:\n        A dictionary of keyword arguments to pass to the download method. Refer\n        to the documentation for `blob.download_to_file()` or\n        `blob.download_to_filename()` for more information. The dict is directly\n        passed into the download methods and is not validated by this function.\n\n    :type threads: int\n    :param threads:\n        ***DEPRECATED*** Sets `worker_type` to THREAD and `max_workers` to the\n        number specified. If `worker_type` or `max_workers` are set explicitly,\n        this parameter should be set to None. Please use `worker_type` and\n        `max_workers` instead of this parameter.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type create_directories: bool\n    :param create_directories:\n        If True, recursively create any directories that do not exist. For\n        instance, if downloading object \"images/img001.png\", create the\n        directory \"images\" before downloading.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        If True, instead of adding exceptions to the list of return values,\n        instead they will be raised. Note that encountering an exception on one\n        operation will not prevent other operations from starting. Exceptions\n        are only processed and potentially raised after all operations are\n        complete in success or failure. If skip_if_exists is True, 412\n        Precondition Failed responses are considered part of normal operation\n        and are not raised as an exception.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type skip_if_exists: bool\n    :param skip_if_exists:\n        Before downloading each blob, check if the file for the filename exists;\n        if it does, skip that blob. This only works for filenames.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n\n    :rtype: list\n    :returns: A list of results corresponding to, in order, each item in the\n        input list. If an exception was received, it will be the result\n        for that operation. Otherwise, the return value from the successful\n        download method is used (which will be None).\n    \"\"\"\n    blob_file_pairs = []\n\n    for blob_name in blob_names:\n        full_blob_name = blob_name_prefix + blob_name\n        path = os.path.join(destination_directory, blob_name)\n        if create_directories:\n            directory, _ = os.path.split(path)\n            os.makedirs(directory, exist_ok=True)\n        blob_file_pairs.append((bucket.blob(full_blob_name), path))\n\n    return download_many(\n        blob_file_pairs,\n        download_kwargs=download_kwargs,\n        deadline=deadline,\n        raise_exception=raise_exception,\n        worker_type=worker_type,\n        max_workers=max_workers,\n        skip_if_exists=skip_if_exists,\n    )\n\n\ndef download_chunks_concurrently(\n    blob,\n    filename,\n    chunk_size=TM_DEFAULT_CHUNK_SIZE,\n    download_kwargs=None,\n    deadline=None,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    crc32c_checksum=True,\n):\n    \"\"\"Download a single file in chunks, concurrently.\n\n    In some environments, using this feature with mutiple processes will result\n    in faster downloads of large files.\n\n    Using this feature with multiple threads is unlikely to improve download\n    performance under normal circumstances due to Python interpreter threading\n    behavior. The default is therefore to use processes instead of threads.\n\n    :type blob: :class:`google.cloud.storage.blob.Blob`\n    :param blob:\n        The blob to be downloaded.\n\n    :type filename: str\n    :param filename:\n        The destination filename or path.\n\n    :type chunk_size: int\n    :param chunk_size:\n        The size in bytes of each chunk to send. The optimal chunk size for\n        maximum throughput may vary depending on the exact network environment\n        and size of the blob.\n\n    :type download_kwargs: dict\n    :param download_kwargs:\n        A dictionary of keyword arguments to pass to the download method. Refer\n        to the documentation for `blob.download_to_file()` or\n        `blob.download_to_filename()` for more information. The dict is directly\n        passed into the download methods and is not validated by this function.\n\n        Keyword arguments \"start\" and \"end\" which are not supported and will\n        cause a ValueError if present. The key \"checksum\" is also not supported\n        in `download_kwargs`, but see the argument `crc32c_checksum` (which does\n        not go in `download_kwargs`) below.\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type crc32c_checksum: bool\n    :param crc32c_checksum:\n        Whether to compute a checksum for the resulting object, using the crc32c\n        algorithm. As the checksums for each chunk must be combined using a\n        feature of crc32c that is not available for md5, md5 is not supported.\n\n    :raises:\n        :exc:`concurrent.futures.TimeoutError`\n            if deadline is exceeded.\n        :exc:`google.resumable_media.common.DataCorruption`\n            if the download's checksum doesn't agree with server-computed\n            checksum. The `google.resumable_media` exception is used here for\n            consistency with other download methods despite the exception\n            originating elsewhere.\n    \"\"\"\n    client = blob.client\n\n    if download_kwargs is None:\n        download_kwargs = {}\n    if \"start\" in download_kwargs or \"end\" in download_kwargs:\n        raise ValueError(\n            \"Download arguments 'start' and 'end' are not supported by download_chunks_concurrently.\"\n        )\n    if \"checksum\" in download_kwargs:\n        raise ValueError(\n            \"'checksum' is in download_kwargs, but is not supported because sliced downloads have a different checksum mechanism from regular downloads. Use the 'crc32c_checksum' argument on download_chunks_concurrently instead.\"\n        )\n\n    download_kwargs[\"command\"] = \"tm.download_sharded\"\n\n    # We must know the size and the generation of the blob.\n    if not blob.size or not blob.generation:\n        blob.reload()\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n    # Pickle the blob ahead of time (just once, not once per chunk) if needed.\n    maybe_pickled_blob = _pickle_client(blob) if needs_pickling else blob\n\n    futures = []\n\n    # Create and/or truncate the destination file to prepare for sparse writing.\n    with open(filename, \"wb\") as _:\n        pass\n\n    with pool_class(max_workers=max_workers) as executor:\n        cursor = 0\n        end = blob.size\n        while cursor < end:\n            start = cursor\n            cursor = min(cursor + chunk_size, end)\n            futures.append(\n                executor.submit(\n                    _download_and_write_chunk_in_place,\n                    maybe_pickled_blob,\n                    filename,\n                    start=start,\n                    end=cursor - 1,\n                    download_kwargs=download_kwargs,\n                    crc32c_checksum=crc32c_checksum,\n                )\n            )\n\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    # Raise any exceptions; combine checksums.\n    results = []\n    for future in futures:\n        results.append(future.result())\n\n    if crc32c_checksum and results:\n        crc_digest = _digest_ordered_checksum_and_size_pairs(results)\n        actual_checksum = base64.b64encode(crc_digest).decode(\"utf-8\")\n        expected_checksum = blob.crc32c\n        if actual_checksum != expected_checksum:\n            # For consistency with other download methods we will use\n            # \"google.resumable_media.common.DataCorruption\" despite the error\n            # not originating inside google.resumable_media.\n            download_url = blob._get_download_url(\n                client,\n                if_generation_match=download_kwargs.get(\"if_generation_match\"),\n                if_generation_not_match=download_kwargs.get(\"if_generation_not_match\"),\n                if_metageneration_match=download_kwargs.get(\"if_metageneration_match\"),\n                if_metageneration_not_match=download_kwargs.get(\n                    \"if_metageneration_not_match\"\n                ),\n            )\n            raise DataCorruption(\n                None,\n                DOWNLOAD_CRC32C_MISMATCH_TEMPLATE.format(\n                    download_url, expected_checksum, actual_checksum\n                ),\n            )\n    return None\n\n\ndef upload_chunks_concurrently(\n    filename,\n    blob,\n    content_type=None,\n    chunk_size=TM_DEFAULT_CHUNK_SIZE,\n    deadline=None,\n    worker_type=PROCESS,\n    max_workers=DEFAULT_MAX_WORKERS,\n    *,\n    checksum=\"md5\",\n    timeout=_DEFAULT_TIMEOUT,\n    retry=DEFAULT_RETRY,\n):\n    \"\"\"Upload a single file in chunks, concurrently.\n\n    This function uses the XML MPU API to initialize an upload and upload a\n    file in chunks, concurrently with a worker pool.\n\n    The XML MPU API is significantly different from other uploads; please review\n    the documentation at `https://cloud.google.com/storage/docs/multipart-uploads`\n    before using this feature.\n\n    The library will attempt to cancel uploads that fail due to an exception.\n    If the upload fails in a way that precludes cancellation, such as a\n    hardware failure, process termination, or power outage, then the incomplete\n    upload may persist indefinitely. To mitigate this, set the\n    `AbortIncompleteMultipartUpload` with a nonzero `Age` in bucket lifecycle\n    rules, or refer to the XML API documentation linked above to learn more\n    about how to list and delete individual downloads.\n\n    Using this feature with multiple threads is unlikely to improve upload\n    performance under normal circumstances due to Python interpreter threading\n    behavior. The default is therefore to use processes instead of threads.\n\n    ACL information cannot be sent with this function and should be set\n    separately with :class:`ObjectACL` methods.\n\n    :type filename: str\n    :param filename:\n        The path to the file to upload. File-like objects are not supported.\n\n    :type blob: :class:`google.cloud.storage.blob.Blob`\n    :param blob:\n        The blob to which to upload.\n\n    :type content_type: str\n    :param content_type: (Optional) Type of content being uploaded.\n\n    :type chunk_size: int\n    :param chunk_size:\n        The size in bytes of each chunk to send. The optimal chunk size for\n        maximum throughput may vary depending on the exact network environment\n        and size of the blob. The remote API has restrictions on the minimum\n        and maximum size allowable, see: `https://cloud.google.com/storage/quotas#requests`\n\n    :type deadline: int\n    :param deadline:\n        The number of seconds to wait for all threads to resolve. If the\n        deadline is reached, all threads will be terminated regardless of their\n        progress and `concurrent.futures.TimeoutError` will be raised. This can\n        be left as the default of `None` (no deadline) for most use cases.\n\n    :type worker_type: str\n    :param worker_type:\n        The worker type to use; one of `google.cloud.storage.transfer_manager.PROCESS`\n        or `google.cloud.storage.transfer_manager.THREAD`.\n\n        Although the exact performance impact depends on the use case, in most\n        situations the PROCESS worker type will use more system resources (both\n        memory and CPU) and result in faster operations than THREAD workers.\n\n        Because the subprocesses of the PROCESS worker type can't access memory\n        from the main process, Client objects have to be serialized and then\n        recreated in each subprocess. The serialization of the Client object\n        for use in subprocesses is an approximation and may not capture every\n        detail of the Client object, especially if the Client was modified after\n        its initial creation or if `Client._http` was modified in any way.\n\n        THREAD worker types are observed to be relatively efficient for\n        operations with many small files, but not for operations with large\n        files. PROCESS workers are recommended for large file operations.\n\n    :type max_workers: int\n    :param max_workers:\n        The maximum number of workers to create to handle the workload.\n\n        With PROCESS workers, a larger number of workers will consume more\n        system resources (memory and CPU) at once.\n\n        How many workers is optimal depends heavily on the specific use case,\n        and the default is a conservative number that should work okay in most\n        cases without consuming excessive resources.\n\n    :type checksum: str\n    :param checksum:\n        (Optional) The checksum scheme to use: either \"md5\", \"crc32c\" or None.\n        Each individual part is checksummed. At present, the selected checksum\n        rule is only applied to parts and a separate checksum of the entire\n        resulting blob is not computed. Please compute and compare the checksum\n        of the file to the resulting blob separately if needed, using the\n        \"crc32c\" algorithm as per the XML MPU documentation.\n\n    :type timeout: float or tuple\n    :param timeout:\n        (Optional) The amount of time, in seconds, to wait\n        for the server response.  See: :ref:`configuring_timeouts`\n\n    :type retry: google.api_core.retry.Retry\n    :param retry: (Optional) How to retry the RPC. A None value will disable\n        retries. A `google.api_core.retry.Retry` value will enable retries,\n        and the object will configure backoff and timeout options. Custom\n        predicates (customizable error codes) are not supported for media\n        operations such as this one.\n\n        This function does not accept `ConditionalRetryPolicy` values because\n        preconditions are not supported by the underlying API call.\n\n        See the retry.py source code and docstrings in this package\n        (`google.cloud.storage.retry`) for information on retry types and how\n        to configure them.\n\n    :raises: :exc:`concurrent.futures.TimeoutError` if deadline is exceeded.\n    \"\"\"\n\n    bucket = blob.bucket\n    client = blob.client\n    transport = blob._get_transport(client)\n\n    hostname = _get_host_name(client._connection)\n    url = \"{hostname}/{bucket}/{blob}\".format(\n        hostname=hostname, bucket=bucket.name, blob=blob.name\n    )\n\n    base_headers, object_metadata, content_type = blob._get_upload_arguments(\n        client, content_type, filename=filename, command=\"tm.upload_sharded\"\n    )\n    headers = {**base_headers, **_headers_from_metadata(object_metadata)}\n\n    if blob.user_project is not None:\n        headers[\"x-goog-user-project\"] = blob.user_project\n\n    # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n    # at rest, object resource metadata will store the version of the Key Management\n    # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n    # used to upload a new version of the object then the existing kmsKeyName version\n    # value can't be used in the upload request and the client instead ignores it.\n    if blob.kms_key_name is not None and \"cryptoKeyVersions\" not in blob.kms_key_name:\n        headers[\"x-goog-encryption-kms-key-name\"] = blob.kms_key_name\n\n    container = XMLMPUContainer(url, filename, headers=headers)\n    container._retry_strategy = _api_core_retry_to_resumable_media_retry(retry)\n\n    container.initiate(transport=transport, content_type=content_type)\n    upload_id = container.upload_id\n\n    size = os.path.getsize(filename)\n    num_of_parts = -(size // -chunk_size)  # Ceiling division\n\n    pool_class, needs_pickling = _get_pool_class_and_requirements(worker_type)\n    # Pickle the blob ahead of time (just once, not once per chunk) if needed.\n    maybe_pickled_client = _pickle_client(client) if needs_pickling else client\n\n    futures = []\n\n    with pool_class(max_workers=max_workers) as executor:\n        for part_number in range(1, num_of_parts + 1):\n            start = (part_number - 1) * chunk_size\n            end = min(part_number * chunk_size, size)\n\n            futures.append(\n                executor.submit(\n                    _upload_part,\n                    maybe_pickled_client,\n                    url,\n                    upload_id,\n                    filename,\n                    start=start,\n                    end=end,\n                    part_number=part_number,\n                    checksum=checksum,\n                    headers=headers,\n                    retry=retry,\n                )\n            )\n\n        concurrent.futures.wait(\n            futures, timeout=deadline, return_when=concurrent.futures.ALL_COMPLETED\n        )\n\n    try:\n        # Harvest results and raise exceptions.\n        for future in futures:\n            part_number, etag = future.result()\n            container.register_part(part_number, etag)\n\n        container.finalize(blob._get_transport(client))\n    except Exception:\n        container.cancel(blob._get_transport(client))\n        raise\n\n\ndef _upload_part(\n    maybe_pickled_client,\n    url,\n    upload_id,\n    filename,\n    start,\n    end,\n    part_number,\n    checksum,\n    headers,\n    retry,\n):\n    \"\"\"Helper function that runs inside a thread or subprocess to upload a part.\n\n    `maybe_pickled_client` is either a Client (for threads) or a specially\n    pickled Client (for processes) because the default pickling mangles Client\n    objects.\"\"\"\n\n    if isinstance(maybe_pickled_client, Client):\n        client = maybe_pickled_client\n    else:\n        client = pickle.loads(maybe_pickled_client)\n    part = XMLMPUPart(\n        url,\n        upload_id,\n        filename,\n        start=start,\n        end=end,\n        part_number=part_number,\n        checksum=checksum,\n        headers=headers,\n    )\n    part._retry_strategy = _api_core_retry_to_resumable_media_retry(retry)\n    part.upload(client._http)\n    return (part_number, part.etag)\n\n\ndef _headers_from_metadata(metadata):\n    \"\"\"Helper function to translate object metadata into a header dictionary.\"\"\"\n\n    headers = {}\n    # Handle standard writable metadata\n    for key, value in metadata.items():\n        if key in METADATA_HEADER_TRANSLATION:\n            headers[METADATA_HEADER_TRANSLATION[key]] = value\n    # Handle custom metadata\n    if \"metadata\" in metadata:\n        for key, value in metadata[\"metadata\"].items():\n            headers[\"x-goog-meta-\" + key] = value\n    return headers\n\n\ndef _download_and_write_chunk_in_place(\n    maybe_pickled_blob, filename, start, end, download_kwargs, crc32c_checksum\n):\n    \"\"\"Helper function that runs inside a thread or subprocess.\n\n    `maybe_pickled_blob` is either a Blob (for threads) or a specially pickled\n    Blob (for processes) because the default pickling mangles Client objects\n    which are attached to Blobs.\n\n    Returns a crc if configured (or None) and the size written.\n    \"\"\"\n\n    if isinstance(maybe_pickled_blob, Blob):\n        blob = maybe_pickled_blob\n    else:\n        blob = pickle.loads(maybe_pickled_blob)\n\n    with _ChecksummingSparseFileWrapper(filename, start, crc32c_checksum) as f:\n        blob._prep_and_do_download(f, start=start, end=end, **download_kwargs)\n        return (f.crc, (end - start) + 1)\n\n\nclass _ChecksummingSparseFileWrapper:\n    \"\"\"A file wrapper that writes to a sparse file and optionally checksums.\n\n    This wrapper only implements write() and does not inherit from `io` module\n    base classes.\n    \"\"\"\n\n    def __init__(self, filename, start_position, crc32c_enabled):\n        # Open in mixed read/write mode to avoid truncating or appending\n        self.f = open(filename, \"rb+\")\n        self.f.seek(start_position)\n        self._crc = None\n        self._crc32c_enabled = crc32c_enabled\n\n    def write(self, chunk):\n        if self._crc32c_enabled:\n            if self._crc is None:\n                self._crc = google_crc32c.value(chunk)\n            else:\n                self._crc = google_crc32c.extend(self._crc, chunk)\n        self.f.write(chunk)\n\n    @property\n    def crc(self):\n        return self._crc\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        self.f.close()\n\n\ndef _call_method_on_maybe_pickled_blob(\n    maybe_pickled_blob, method_name, *args, **kwargs\n):\n    \"\"\"Helper function that runs inside a thread or subprocess.\n\n    `maybe_pickled_blob` is either a Blob (for threads) or a specially pickled\n    Blob (for processes) because the default pickling mangles Client objects\n    which are attached to Blobs.\"\"\"\n\n    if isinstance(maybe_pickled_blob, Blob):\n        blob = maybe_pickled_blob\n    else:\n        blob = pickle.loads(maybe_pickled_blob)\n    return getattr(blob, method_name)(*args, **kwargs)\n\n\ndef _reduce_client(cl):\n    \"\"\"Replicate a Client by constructing a new one with the same params.\n\n    LazyClient performs transparent caching for when the same client is needed\n    on the same process multiple times.\"\"\"\n\n    client_object_id = id(cl)\n    project = cl.project\n    credentials = cl._credentials\n    _http = None  # Can't carry this over\n    client_info = cl._initial_client_info\n    client_options = cl._initial_client_options\n    extra_headers = cl._extra_headers\n\n    return _LazyClient, (\n        client_object_id,\n        project,\n        credentials,\n        _http,\n        client_info,\n        client_options,\n        extra_headers,\n    )\n\n\ndef _pickle_client(obj):\n    \"\"\"Pickle a Client or an object that owns a Client (like a Blob)\"\"\"\n\n    # We need a custom pickler to process Client objects, which are attached to\n    # Buckets (and therefore to Blobs in turn). Unfortunately, the Python\n    # multiprocessing library doesn't seem to have a good way to use a custom\n    # pickler, and using copyreg will mutate global state and affect code\n    # outside of the client library. Instead, we'll pre-pickle the object and\n    # pass the bytestring in.\n    f = io.BytesIO()\n    p = pickle.Pickler(f)\n    p.dispatch_table = copyreg.dispatch_table.copy()\n    p.dispatch_table[Client] = _reduce_client\n    p.dump(obj)\n    return f.getvalue()\n\n\ndef _get_pool_class_and_requirements(worker_type):\n    \"\"\"Returns the pool class, and whether the pool requires pickled Blobs.\"\"\"\n\n    if worker_type == PROCESS:\n        # Use processes. Pickle blobs with custom logic to handle the client.\n        return (concurrent.futures.ProcessPoolExecutor, True)\n    elif worker_type == THREAD:\n        # Use threads. Pass blobs through unpickled.\n        return (concurrent.futures.ThreadPoolExecutor, False)\n    else:\n        raise ValueError(\n            \"The worker_type must be google.cloud.storage.transfer_manager.PROCESS or google.cloud.storage.transfer_manager.THREAD\"\n        )\n\n\ndef _digest_ordered_checksum_and_size_pairs(checksum_and_size_pairs):\n    base_crc = None\n    zeroes = bytes(MAX_CRC32C_ZERO_ARRAY_SIZE)\n    for part_crc, size in checksum_and_size_pairs:\n        if not base_crc:\n            base_crc = part_crc\n        else:\n            base_crc ^= 0xFFFFFFFF  # precondition\n\n            # Zero pad base_crc32c. To conserve memory, do so with only\n            # MAX_CRC32C_ZERO_ARRAY_SIZE at a time. Reuse the zeroes array where\n            # possible.\n            padded = 0\n            while padded < size:\n                desired_zeroes_size = min((size - padded), MAX_CRC32C_ZERO_ARRAY_SIZE)\n                base_crc = google_crc32c.extend(base_crc, zeroes[:desired_zeroes_size])\n                padded += desired_zeroes_size\n\n            base_crc ^= 0xFFFFFFFF  # postcondition\n            base_crc ^= part_crc\n    crc_digest = struct.pack(\n        \">L\", base_crc\n    )  # https://cloud.google.com/storage/docs/json_api/v1/objects#crc32c\n    return crc_digest\n\n\nclass _LazyClient:\n    \"\"\"An object that will transform into either a cached or a new Client\"\"\"\n\n    def __new__(cls, id, *args, **kwargs):\n        cached_client = _cached_clients.get(id)\n        if cached_client:\n            return cached_client\n        else:\n            cached_client = Client(*args, **kwargs)\n            _cached_clients[id] = cached_client\n            return cached_client\n", "google/cloud/storage/acl.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Manage access to objects and buckets.\"\"\"\n\nfrom google.cloud.storage._helpers import _add_generation_match_parameters\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\n\nclass _ACLEntity(object):\n    \"\"\"Class representing a set of roles for an entity.\n\n    This is a helper class that you likely won't ever construct\n    outside of using the factor methods on the :class:`ACL` object.\n\n    :type entity_type: str\n    :param entity_type: The type of entity (ie, 'group' or 'user').\n\n    :type identifier: str\n    :param identifier: (Optional) The ID or e-mail of the entity. For the special\n                       entity types (like 'allUsers').\n    \"\"\"\n\n    READER_ROLE = \"READER\"\n    WRITER_ROLE = \"WRITER\"\n    OWNER_ROLE = \"OWNER\"\n\n    def __init__(self, entity_type, identifier=None):\n        self.identifier = identifier\n        self.roles = set([])\n        self.type = entity_type\n\n    def __str__(self):\n        if not self.identifier:\n            return str(self.type)\n        else:\n            return \"{acl.type}-{acl.identifier}\".format(acl=self)\n\n    def __repr__(self):\n        return f\"<ACL Entity: {self} ({', '.join(self.roles)})>\"\n\n    def get_roles(self):\n        \"\"\"Get the list of roles permitted by this entity.\n\n        :rtype: list of strings\n        :returns: The list of roles associated with this entity.\n        \"\"\"\n        return self.roles\n\n    def grant(self, role):\n        \"\"\"Add a role to the entity.\n\n        :type role: str\n        :param role: The role to add to the entity.\n        \"\"\"\n        self.roles.add(role)\n\n    def revoke(self, role):\n        \"\"\"Remove a role from the entity.\n\n        :type role: str\n        :param role: The role to remove from the entity.\n        \"\"\"\n        if role in self.roles:\n            self.roles.remove(role)\n\n    def grant_read(self):\n        \"\"\"Grant read access to the current entity.\"\"\"\n        self.grant(_ACLEntity.READER_ROLE)\n\n    def grant_write(self):\n        \"\"\"Grant write access to the current entity.\"\"\"\n        self.grant(_ACLEntity.WRITER_ROLE)\n\n    def grant_owner(self):\n        \"\"\"Grant owner access to the current entity.\"\"\"\n        self.grant(_ACLEntity.OWNER_ROLE)\n\n    def revoke_read(self):\n        \"\"\"Revoke read access from the current entity.\"\"\"\n        self.revoke(_ACLEntity.READER_ROLE)\n\n    def revoke_write(self):\n        \"\"\"Revoke write access from the current entity.\"\"\"\n        self.revoke(_ACLEntity.WRITER_ROLE)\n\n    def revoke_owner(self):\n        \"\"\"Revoke owner access from the current entity.\"\"\"\n        self.revoke(_ACLEntity.OWNER_ROLE)\n\n\nclass ACL(object):\n    \"\"\"Container class representing a list of access controls.\"\"\"\n\n    _URL_PATH_ELEM = \"acl\"\n    _PREDEFINED_QUERY_PARAM = \"predefinedAcl\"\n\n    PREDEFINED_XML_ACLS = {\n        # XML API name -> JSON API name\n        \"project-private\": \"projectPrivate\",\n        \"public-read\": \"publicRead\",\n        \"public-read-write\": \"publicReadWrite\",\n        \"authenticated-read\": \"authenticatedRead\",\n        \"bucket-owner-read\": \"bucketOwnerRead\",\n        \"bucket-owner-full-control\": \"bucketOwnerFullControl\",\n    }\n\n    PREDEFINED_JSON_ACLS = frozenset(\n        [\n            \"private\",\n            \"projectPrivate\",\n            \"publicRead\",\n            \"publicReadWrite\",\n            \"authenticatedRead\",\n            \"bucketOwnerRead\",\n            \"bucketOwnerFullControl\",\n        ]\n    )\n    \"\"\"See\n    https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n    \"\"\"\n\n    loaded = False\n\n    # Subclasses must override to provide these attributes (typically,\n    # as properties).\n    reload_path = None\n    save_path = None\n    user_project = None\n\n    def __init__(self):\n        self.entities = {}\n\n    def _ensure_loaded(self, timeout=_DEFAULT_TIMEOUT):\n        \"\"\"Load if not already loaded.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n        \"\"\"\n        if not self.loaded:\n            self.reload(timeout=timeout)\n\n    @classmethod\n    def validate_predefined(cls, predefined):\n        \"\"\"Ensures predefined is in list of predefined json values\n\n        :type predefined: str\n        :param predefined: name of a predefined acl\n\n        :type predefined: str\n        :param predefined: validated JSON name of predefined acl\n\n        :raises: :exc: `ValueError`: If predefined is not a valid acl\n        \"\"\"\n        predefined = cls.PREDEFINED_XML_ACLS.get(predefined, predefined)\n        if predefined and predefined not in cls.PREDEFINED_JSON_ACLS:\n            raise ValueError(f\"Invalid predefined ACL: {predefined}\")\n        return predefined\n\n    def reset(self):\n        \"\"\"Remove all entities from the ACL, and clear the ``loaded`` flag.\"\"\"\n        self.entities.clear()\n        self.loaded = False\n\n    def __iter__(self):\n        self._ensure_loaded()\n\n        for entity in self.entities.values():\n            for role in entity.get_roles():\n                if role:\n                    yield {\"entity\": str(entity), \"role\": role}\n\n    def entity_from_dict(self, entity_dict):\n        \"\"\"Build an _ACLEntity object from a dictionary of data.\n\n        An entity is a mutable object that represents a list of roles\n        belonging to either a user or group or the special types for all\n        users and all authenticated users.\n\n        :type entity_dict: dict\n        :param entity_dict: Dictionary full of data from an ACL lookup.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity constructed from the dictionary.\n        \"\"\"\n        entity = entity_dict[\"entity\"]\n        role = entity_dict[\"role\"]\n\n        if entity == \"allUsers\":\n            entity = self.all()\n\n        elif entity == \"allAuthenticatedUsers\":\n            entity = self.all_authenticated()\n\n        elif \"-\" in entity:\n            entity_type, identifier = entity.split(\"-\", 1)\n            entity = self.entity(entity_type=entity_type, identifier=identifier)\n\n        if not isinstance(entity, _ACLEntity):\n            raise ValueError(f\"Invalid dictionary: {entity_dict}\")\n\n        entity.grant(role)\n        return entity\n\n    def has_entity(self, entity):\n        \"\"\"Returns whether or not this ACL has any entries for an entity.\n\n        :type entity: :class:`_ACLEntity`\n        :param entity: The entity to check for existence in this ACL.\n\n        :rtype: bool\n        :returns: True of the entity exists in the ACL.\n        \"\"\"\n        self._ensure_loaded()\n        return str(entity) in self.entities\n\n    def get_entity(self, entity, default=None):\n        \"\"\"Gets an entity object from the ACL.\n\n        :type entity: :class:`_ACLEntity` or string\n        :param entity: The entity to get lookup in the ACL.\n\n        :type default: anything\n        :param default: This value will be returned if the entity\n                        doesn't exist.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: The corresponding entity or the value provided\n                  to ``default``.\n        \"\"\"\n        self._ensure_loaded()\n        return self.entities.get(str(entity), default)\n\n    def add_entity(self, entity):\n        \"\"\"Add an entity to the ACL.\n\n        :type entity: :class:`_ACLEntity`\n        :param entity: The entity to add to this ACL.\n        \"\"\"\n        self._ensure_loaded()\n        self.entities[str(entity)] = entity\n\n    def entity(self, entity_type, identifier=None):\n        \"\"\"Factory method for creating an Entity.\n\n        If an entity with the same type and identifier already exists,\n        this will return a reference to that entity.  If not, it will\n        create a new one and add it to the list of known entities for\n        this ACL.\n\n        :type entity_type: str\n        :param entity_type: The type of entity to create\n                            (ie, ``user``, ``group``, etc)\n\n        :type identifier: str\n        :param identifier: The ID of the entity (if applicable).\n                           This can be either an ID or an e-mail address.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: A new Entity or a reference to an existing identical entity.\n        \"\"\"\n        entity = _ACLEntity(entity_type=entity_type, identifier=identifier)\n        if self.has_entity(entity):\n            entity = self.get_entity(entity)\n        else:\n            self.add_entity(entity)\n        return entity\n\n    def user(self, identifier):\n        \"\"\"Factory method for a user Entity.\n\n        :type identifier: str\n        :param identifier: An id or e-mail for this particular user.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity corresponding to this user.\n        \"\"\"\n        return self.entity(\"user\", identifier=identifier)\n\n    def group(self, identifier):\n        \"\"\"Factory method for a group Entity.\n\n        :type identifier: str\n        :param identifier: An id or e-mail for this particular group.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity corresponding to this group.\n        \"\"\"\n        return self.entity(\"group\", identifier=identifier)\n\n    def domain(self, domain):\n        \"\"\"Factory method for a domain Entity.\n\n        :type domain: str\n        :param domain: The domain for this entity.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An entity corresponding to this domain.\n        \"\"\"\n        return self.entity(\"domain\", identifier=domain)\n\n    def all(self):\n        \"\"\"Factory method for an Entity representing all users.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An entity representing all users.\n        \"\"\"\n        return self.entity(\"allUsers\")\n\n    def all_authenticated(self):\n        \"\"\"Factory method for an Entity representing all authenticated users.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An entity representing all authenticated users.\n        \"\"\"\n        return self.entity(\"allAuthenticatedUsers\")\n\n    def get_entities(self):\n        \"\"\"Get a list of all Entity objects.\n\n        :rtype: list of :class:`_ACLEntity` objects\n        :returns: A list of all Entity objects.\n        \"\"\"\n        self._ensure_loaded()\n        return list(self.entities.values())\n\n    @property\n    def client(self):\n        \"\"\"Abstract getter for the object client.\"\"\"\n        raise NotImplementedError\n\n    def _require_client(self, client):\n        \"\"\"Check client or verify over-ride.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current ACL.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: The client passed in or the currently bound client.\n        \"\"\"\n        if client is None:\n            client = self.client\n        return client\n\n    def reload(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Reload the ACL data from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: :class:`~google.api_core.retry.Retry`\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        path = self.reload_path\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        self.entities.clear()\n\n        found = client._get_resource(\n            path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        self.loaded = True\n\n        for entry in found.get(\"items\", ()):\n            self.add_entity(self.entity_from_dict(entry))\n\n    def _save(\n        self,\n        acl,\n        predefined,\n        client,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Helper for :meth:`save` and :meth:`save_predefined`.\n\n        :type acl: :class:`google.cloud.storage.acl.ACL`, or a compatible list.\n        :param acl: The ACL object to save.  If left blank, this will save\n                    current entries.\n\n        :type predefined: str\n        :param predefined: An identifier for a predefined ACL.  Must be one of the\n            keys in :attr:`PREDEFINED_JSON_ACLS` If passed, `acl` must be None.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {\"projection\": \"full\"}\n\n        if predefined is not None:\n            acl = []\n            query_params[self._PREDEFINED_QUERY_PARAM] = predefined\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        path = self.save_path\n\n        result = client._patch_resource(\n            path,\n            {self._URL_PATH_ELEM: list(acl)},\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        self.entities.clear()\n\n        for entry in result.get(self._URL_PATH_ELEM, ()):\n            self.add_entity(self.entity_from_dict(entry))\n\n        self.loaded = True\n\n    def save(\n        self,\n        acl=None,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Save this ACL for the current bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type acl: :class:`google.cloud.storage.acl.ACL`, or a compatible list.\n        :param acl: The ACL object to save.  If left blank, this will save\n                    current entries.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        if acl is None:\n            acl = self\n            save_to_backend = acl.loaded\n        else:\n            save_to_backend = True\n\n        if save_to_backend:\n            self._save(\n                acl,\n                None,\n                client,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                timeout=timeout,\n                retry=retry,\n            )\n\n    def save_predefined(\n        self,\n        predefined,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Save this ACL for the current bucket using a predefined ACL.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type predefined: str\n        :param predefined: An identifier for a predefined ACL.  Must be one\n                           of the keys in :attr:`PREDEFINED_JSON_ACLS`\n                           or :attr:`PREDEFINED_XML_ACLS` (which will be\n                           aliased to the corresponding JSON name).\n                           If passed, `acl` must be None.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        predefined = self.validate_predefined(predefined)\n        self._save(\n            None,\n            predefined,\n            client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def clear(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Remove all ACL entries.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        Note that this won't actually remove *ALL* the rules, but it\n        will remove all the non-default rules.  In short, you'll still\n        have access to a bucket that you created even after you clear\n        ACL rules with this method.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the ACL's parent.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        self.save(\n            [],\n            client=client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n\nclass BucketACL(ACL):\n    \"\"\"An ACL specifically for a bucket.\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket: The bucket to which this ACL relates.\n    \"\"\"\n\n    def __init__(self, bucket):\n        super(BucketACL, self).__init__()\n        self.bucket = bucket\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this ACL's bucket.\"\"\"\n        return self.bucket.client\n\n    @property\n    def reload_path(self):\n        \"\"\"Compute the path for GET API requests for this ACL.\"\"\"\n        return f\"{self.bucket.path}/{self._URL_PATH_ELEM}\"\n\n    @property\n    def save_path(self):\n        \"\"\"Compute the path for PATCH API requests for this ACL.\"\"\"\n        return self.bucket.path\n\n    @property\n    def user_project(self):\n        \"\"\"Compute the user project charged for API requests for this ACL.\"\"\"\n        return self.bucket.user_project\n\n\nclass DefaultObjectACL(BucketACL):\n    \"\"\"A class representing the default object ACL for a bucket.\"\"\"\n\n    _URL_PATH_ELEM = \"defaultObjectAcl\"\n    _PREDEFINED_QUERY_PARAM = \"predefinedDefaultObjectAcl\"\n\n\nclass ObjectACL(ACL):\n    \"\"\"An ACL specifically for a Cloud Storage object / blob.\n\n    :type blob: :class:`google.cloud.storage.blob.Blob`\n    :param blob: The blob that this ACL corresponds to.\n    \"\"\"\n\n    def __init__(self, blob):\n        super(ObjectACL, self).__init__()\n        self.blob = blob\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this ACL's blob.\"\"\"\n        return self.blob.client\n\n    @property\n    def reload_path(self):\n        \"\"\"Compute the path for GET API requests for this ACL.\"\"\"\n        return f\"{self.blob.path}/acl\"\n\n    @property\n    def save_path(self):\n        \"\"\"Compute the path for PATCH API requests for this ACL.\"\"\"\n        return self.blob.path\n\n    @property\n    def user_project(self):\n        \"\"\"Compute the user project charged for API requests for this ACL.\"\"\"\n        return self.blob.user_project\n", "google/cloud/storage/notification.py": "# Copyright 2017 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configure bucket notification resources to interact with Google Cloud Pub/Sub.\n\nSee [Cloud Pub/Sub Notifications for Google Cloud Storage](https://cloud.google.com/storage/docs/pubsub-notifications)\n\"\"\"\n\nimport re\n\nfrom google.api_core.exceptions import NotFound\n\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\nOBJECT_FINALIZE_EVENT_TYPE = \"OBJECT_FINALIZE\"\nOBJECT_METADATA_UPDATE_EVENT_TYPE = \"OBJECT_METADATA_UPDATE\"\nOBJECT_DELETE_EVENT_TYPE = \"OBJECT_DELETE\"\nOBJECT_ARCHIVE_EVENT_TYPE = \"OBJECT_ARCHIVE\"\n\nJSON_API_V1_PAYLOAD_FORMAT = \"JSON_API_V1\"\nNONE_PAYLOAD_FORMAT = \"NONE\"\n\n_TOPIC_REF_FMT = \"//pubsub.googleapis.com/projects/{}/topics/{}\"\n_PROJECT_PATTERN = r\"(?P<project>[a-z][a-z0-9-]{4,28}[a-z0-9])\"\n_TOPIC_NAME_PATTERN = r\"(?P<name>[A-Za-z](\\w|[-_.~+%])+)\"\n_TOPIC_REF_PATTERN = _TOPIC_REF_FMT.format(_PROJECT_PATTERN, _TOPIC_NAME_PATTERN)\n_TOPIC_REF_RE = re.compile(_TOPIC_REF_PATTERN)\n_BAD_TOPIC = (\n    \"Resource has invalid topic: {}; see \"\n    \"https://cloud.google.com/storage/docs/json_api/v1/\"\n    \"notifications/insert#topic\"\n)\n\n\nclass BucketNotification(object):\n    \"\"\"Represent a single notification resource for a bucket.\n\n    See: https://cloud.google.com/storage/docs/json_api/v1/notifications\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket: Bucket to which the notification is bound.\n\n    :type topic_name: str\n    :param topic_name:\n        (Optional) Topic name to which notifications are published.\n\n    :type topic_project: str\n    :param topic_project:\n        (Optional) Project ID of topic to which notifications are published.\n        If not passed, uses the project ID of the bucket's client.\n\n    :type custom_attributes: dict\n    :param custom_attributes:\n        (Optional) Additional attributes passed with notification events.\n\n    :type event_types: list(str)\n    :param event_types:\n        (Optional) Event types for which notification events are published.\n\n    :type blob_name_prefix: str\n    :param blob_name_prefix:\n        (Optional) Prefix of blob names for which notification events are\n        published.\n\n    :type payload_format: str\n    :param payload_format:\n        (Optional) Format of payload for notification events.\n\n    :type notification_id: str\n    :param notification_id:\n        (Optional) The ID of the notification.\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket,\n        topic_name=None,\n        topic_project=None,\n        custom_attributes=None,\n        event_types=None,\n        blob_name_prefix=None,\n        payload_format=NONE_PAYLOAD_FORMAT,\n        notification_id=None,\n    ):\n        self._bucket = bucket\n        self._topic_name = topic_name\n\n        if topic_project is None:\n            topic_project = bucket.client.project\n\n        if topic_project is None:\n            raise ValueError(\"Client project not set:  pass an explicit topic_project.\")\n\n        self._topic_project = topic_project\n\n        self._properties = {}\n\n        if custom_attributes is not None:\n            self._properties[\"custom_attributes\"] = custom_attributes\n\n        if event_types is not None:\n            self._properties[\"event_types\"] = event_types\n\n        if blob_name_prefix is not None:\n            self._properties[\"object_name_prefix\"] = blob_name_prefix\n\n        if notification_id is not None:\n            self._properties[\"id\"] = notification_id\n\n        self._properties[\"payload_format\"] = payload_format\n\n    @classmethod\n    def from_api_repr(cls, resource, bucket):\n        \"\"\"Construct an instance from the JSON repr returned by the server.\n\n        See: https://cloud.google.com/storage/docs/json_api/v1/notifications\n\n        :type resource: dict\n        :param resource: JSON repr of the notification\n\n        :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n        :param bucket: Bucket to which the notification is bound.\n\n        :rtype: :class:`BucketNotification`\n        :returns: the new notification instance\n        \"\"\"\n        topic_path = resource.get(\"topic\")\n        if topic_path is None:\n            raise ValueError(\"Resource has no topic\")\n\n        name, project = _parse_topic_path(topic_path)\n        instance = cls(bucket, name, topic_project=project)\n        instance._properties = resource\n\n        return instance\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket to which the notification is bound.\"\"\"\n        return self._bucket\n\n    @property\n    def topic_name(self):\n        \"\"\"Topic name to which notifications are published.\"\"\"\n        return self._topic_name\n\n    @property\n    def topic_project(self):\n        \"\"\"Project ID of topic to which notifications are published.\"\"\"\n        return self._topic_project\n\n    @property\n    def custom_attributes(self):\n        \"\"\"Custom attributes passed with notification events.\"\"\"\n        return self._properties.get(\"custom_attributes\")\n\n    @property\n    def event_types(self):\n        \"\"\"Event types for which notification events are published.\"\"\"\n        return self._properties.get(\"event_types\")\n\n    @property\n    def blob_name_prefix(self):\n        \"\"\"Prefix of blob names for which notification events are published.\"\"\"\n        return self._properties.get(\"object_name_prefix\")\n\n    @property\n    def payload_format(self):\n        \"\"\"Format of payload of notification events.\"\"\"\n        return self._properties.get(\"payload_format\")\n\n    @property\n    def notification_id(self):\n        \"\"\"Server-set ID of notification resource.\"\"\"\n        return self._properties.get(\"id\")\n\n    @property\n    def etag(self):\n        \"\"\"Server-set ETag of notification resource.\"\"\"\n        return self._properties.get(\"etag\")\n\n    @property\n    def self_link(self):\n        \"\"\"Server-set ETag of notification resource.\"\"\"\n        return self._properties.get(\"selfLink\")\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this notfication.\"\"\"\n        return self.bucket.client\n\n    @property\n    def path(self):\n        \"\"\"The URL path for this notification.\"\"\"\n        return f\"/b/{self.bucket.name}/notificationConfigs/{self.notification_id}\"\n\n    def _require_client(self, client):\n        \"\"\"Check client or verify over-ride.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: The client passed in or the bucket's client.\n        \"\"\"\n        if client is None:\n            client = self.client\n        return client\n\n    def _set_properties(self, response):\n        \"\"\"Helper for :meth:`reload`.\n\n        :type response: dict\n        :param response: resource mapping from server\n        \"\"\"\n        self._properties.clear()\n        self._properties.update(response)\n\n    def create(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=None):\n        \"\"\"API wrapper: create the notification.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/insert\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the notification's bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError: if the notification already exists.\n        \"\"\"\n        if self.notification_id is not None:\n            raise ValueError(\n                f\"notification_id already set to {self.notification_id}; must be None to create a Notification.\"\n            )\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        path = f\"/b/{self.bucket.name}/notificationConfigs\"\n        properties = self._properties.copy()\n\n        if self.topic_name is None:\n            properties[\"topic\"] = _TOPIC_REF_FMT.format(self.topic_project, \"\")\n        else:\n            properties[\"topic\"] = _TOPIC_REF_FMT.format(\n                self.topic_project, self.topic_name\n            )\n\n        self._properties = client._post_resource(\n            path,\n            properties,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def exists(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Test whether this notification exists.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/get\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: bool\n        :returns: True, if the notification exists, else False.\n        :raises ValueError: if the notification has no ID.\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification ID not set: set an explicit notification_id\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        try:\n            client._get_resource(\n                self.path,\n                query_params=query_params,\n                timeout=timeout,\n                retry=retry,\n            )\n        except NotFound:\n            return False\n        else:\n            return True\n\n    def reload(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Update this notification from the server configuration.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/get\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n\n        :raises ValueError: if the notification has no ID.\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification ID not set: set an explicit notification_id\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        response = client._get_resource(\n            self.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        self._set_properties(response)\n\n    def delete(self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY):\n        \"\"\"Delete this notification.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/delete\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises: :class:`google.api_core.exceptions.NotFound`:\n            if the notification does not exist.\n        :raises ValueError: if the notification has no ID.\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification ID not set: set an explicit notification_id\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        client._delete_resource(\n            self.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n\ndef _parse_topic_path(topic_path):\n    \"\"\"Verify that a topic path is in the correct format.\n\n    Expected to be of the form:\n\n        //pubsub.googleapis.com/projects/{project}/topics/{topic}\n\n    where the ``project`` value must be \"6 to 30 lowercase letters, digits,\n    or hyphens. It must start with a letter. Trailing hyphens are prohibited.\"\n    (see [`resource manager docs`](https://cloud.google.com/resource-manager/reference/rest/v1beta1/projects#Project.FIELDS.project_id))\n    and ``topic`` must have length at least two,\n    must start with a letter and may only contain alphanumeric characters or\n    ``-``, ``_``, ``.``, ``~``, ``+`` or ``%`` (i.e characters used for URL\n    encoding, see [`topic spec`](https://cloud.google.com/storage/docs/json_api/v1/notifications/insert#topic)).\n\n    Args:\n        topic_path (str): The topic path to be verified.\n\n    Returns:\n        Tuple[str, str]: The ``project`` and ``topic`` parsed from the\n        ``topic_path``.\n\n    Raises:\n        ValueError: If the topic path is invalid.\n    \"\"\"\n    match = _TOPIC_REF_RE.match(topic_path)\n    if match is None:\n        raise ValueError(_BAD_TOPIC.format(topic_path))\n\n    return match.group(\"name\"), match.group(\"project\")\n", "google/cloud/storage/fileio.py": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for file-like access of blobs, usually invoked via Blob.open().\"\"\"\n\nimport io\nimport warnings\n\nfrom google.api_core.exceptions import RequestRangeNotSatisfiable\nfrom google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import ConditionalRetryPolicy\n\n\n# Resumable uploads require a chunk size of precisely a multiple of 256 KiB.\nCHUNK_SIZE_MULTIPLE = 256 * 1024  # 256 KiB\nDEFAULT_CHUNK_SIZE = 40 * 1024 * 1024  # 40 MiB\n\n# Valid keyword arguments for download methods, and blob.reload() if needed.\n# Note: Changes here need to be reflected in the blob.open() docstring.\nVALID_DOWNLOAD_KWARGS = {\n    \"if_generation_match\",\n    \"if_generation_not_match\",\n    \"if_metageneration_match\",\n    \"if_metageneration_not_match\",\n    \"timeout\",\n    \"retry\",\n    \"raw_download\",\n}\n\n# Valid keyword arguments for upload methods.\n# Note: Changes here need to be reflected in the blob.open() docstring.\nVALID_UPLOAD_KWARGS = {\n    \"content_type\",\n    \"predefined_acl\",\n    \"num_retries\",\n    \"if_generation_match\",\n    \"if_generation_not_match\",\n    \"if_metageneration_match\",\n    \"if_metageneration_not_match\",\n    \"timeout\",\n    \"checksum\",\n    \"retry\",\n}\n\n\nclass BlobReader(io.BufferedIOBase):\n    \"\"\"A file-like object that reads from a blob.\n\n    :type blob: 'google.cloud.storage.blob.Blob'\n    :param blob:\n        The blob to download.\n\n    :type chunk_size: long\n    :param chunk_size:\n        (Optional) The minimum number of bytes to read at a time. If fewer\n        bytes than the chunk_size are requested, the remainder is buffered.\n        The default is the chunk_size of the blob, or 40MiB.\n\n    :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n    :param retry:\n        (Optional) How to retry the RPC. A None value will disable\n        retries. A google.api_core.retry.Retry value will enable retries,\n        and the object will define retriable response codes and errors and\n        configure backoff and timeout options.\n\n        A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n        Retry object and activates it only if certain conditions are met.\n        This class exists to provide safe defaults for RPC calls that are\n        not technically safe to retry normally (due to potential data\n        duplication or other side-effects) but become safe to retry if a\n        condition such as if_metageneration_match is set.\n\n        See the retry.py source code and docstrings in this package\n        (google.cloud.storage.retry) for information on retry types and how\n        to configure them.\n\n        Media operations (downloads and uploads) do not support non-default\n        predicates in a Retry object. The default will always be used. Other\n        configuration changes for Retry objects such as delays and deadlines\n        are respected.\n\n    :param download_kwargs:\n        Keyword arguments to pass to the underlying API calls.\n        The following arguments are supported:\n\n        - ``if_generation_match``\n        - ``if_generation_not_match``\n        - ``if_metageneration_match``\n        - ``if_metageneration_not_match``\n        - ``timeout``\n\n        Note that download_kwargs are also applied to blob.reload(), if a reload\n        is needed during seek().\n    \"\"\"\n\n    def __init__(self, blob, chunk_size=None, retry=DEFAULT_RETRY, **download_kwargs):\n        for kwarg in download_kwargs:\n            if kwarg not in VALID_DOWNLOAD_KWARGS:\n                raise ValueError(\n                    f\"BlobReader does not support keyword argument {kwarg}.\"\n                )\n\n        self._blob = blob\n        self._pos = 0\n        self._buffer = io.BytesIO()\n        self._chunk_size = chunk_size or blob.chunk_size or DEFAULT_CHUNK_SIZE\n        self._retry = retry\n        self._download_kwargs = download_kwargs\n\n    def read(self, size=-1):\n        self._checkClosed()  # Raises ValueError if closed.\n\n        result = self._buffer.read(size)\n        # If the read request demands more bytes than are buffered, fetch more.\n        remaining_size = size - len(result)\n        if remaining_size > 0 or size < 0:\n            self._pos += self._buffer.tell()\n            read_size = len(result)\n\n            self._buffer.seek(0)\n            self._buffer.truncate(0)  # Clear the buffer to make way for new data.\n            fetch_start = self._pos\n            if size > 0:\n                # Fetch the larger of self._chunk_size or the remaining_size.\n                fetch_end = fetch_start + max(remaining_size, self._chunk_size)\n            else:\n                fetch_end = None\n\n            # Download the blob. Checksumming must be disabled as we are using\n            # chunked downloads, and the server only knows the checksum of the\n            # entire file.\n            try:\n                result += self._blob.download_as_bytes(\n                    start=fetch_start,\n                    end=fetch_end,\n                    checksum=None,\n                    retry=self._retry,\n                    **self._download_kwargs,\n                )\n            except RequestRangeNotSatisfiable:\n                # We've reached the end of the file. Python file objects should\n                # return an empty response in this case, not raise an error.\n                pass\n\n            # If more bytes were read than is immediately needed, buffer the\n            # remainder and then trim the result.\n            if size > 0 and len(result) > size:\n                self._buffer.write(result[size:])\n                self._buffer.seek(0)\n                result = result[:size]\n            # Increment relative offset by true amount read.\n            self._pos += len(result) - read_size\n        return result\n\n    def read1(self, size=-1):\n        return self.read(size)\n\n    def seek(self, pos, whence=0):\n        \"\"\"Seek within the blob.\n\n        This implementation of seek() uses knowledge of the blob size to\n        validate that the reported position does not exceed the blob last byte.\n        If the blob size is not already known it will call blob.reload().\n        \"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        if self._blob.size is None:\n            self._blob.reload(**self._download_kwargs)\n\n        initial_offset = self._pos + self._buffer.tell()\n\n        if whence == 0:\n            target_pos = pos\n        elif whence == 1:\n            target_pos = initial_offset + pos\n        elif whence == 2:\n            target_pos = self._blob.size + pos\n        if whence not in {0, 1, 2}:\n            raise ValueError(\"invalid whence value\")\n\n        if target_pos > self._blob.size:\n            target_pos = self._blob.size\n\n        # Seek or invalidate buffer as needed.\n        if target_pos < self._pos:\n            # Target position < relative offset <= true offset.\n            # As data is not in buffer, invalidate buffer.\n            self._buffer.seek(0)\n            self._buffer.truncate(0)\n            new_pos = target_pos\n            self._pos = target_pos\n        else:\n            # relative offset <= target position <= size of file.\n            difference = target_pos - initial_offset\n            new_pos = self._pos + self._buffer.seek(difference, 1)\n        return new_pos\n\n    def close(self):\n        self._buffer.close()\n\n    @property\n    def closed(self):\n        return self._buffer.closed\n\n    def readable(self):\n        return True\n\n    def writable(self):\n        return False\n\n    def seekable(self):\n        return True\n\n\nclass BlobWriter(io.BufferedIOBase):\n    \"\"\"A file-like object that writes to a blob.\n\n    :type blob: 'google.cloud.storage.blob.Blob'\n    :param blob:\n        The blob to which to write.\n\n    :type chunk_size: long\n    :param chunk_size:\n        (Optional) The maximum number of bytes to buffer before sending data\n        to the server, and the size of each request when data is sent.\n        Writes are implemented as a \"resumable upload\", so chunk_size for\n        writes must be exactly a multiple of 256KiB as with other resumable\n        uploads. The default is the chunk_size of the blob, or 40 MiB.\n\n    :type text_mode: bool\n    :param text_mode:\n        (Deprecated) A synonym for ignore_flush. For backwards-compatibility,\n        if True, sets ignore_flush to True. Use ignore_flush instead. This\n        parameter will be removed in a future release.\n\n    :type ignore_flush: bool\n    :param ignore_flush:\n        Makes flush() do nothing instead of raise an error. flush() without\n        closing is not supported by the remote service and therefore calling it\n        on this class normally results in io.UnsupportedOperation. However, that\n        behavior is incompatible with some consumers and wrappers of file\n        objects in Python, such as zipfile.ZipFile or io.TextIOWrapper. Setting\n        ignore_flush will cause flush() to successfully do nothing, for\n        compatibility with those contexts. The correct way to actually flush\n        data to the remote server is to close() (using this object as a context\n        manager is recommended).\n\n    :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n    :param retry:\n        (Optional) How to retry the RPC. A None value will disable\n        retries. A google.api_core.retry.Retry value will enable retries,\n        and the object will define retriable response codes and errors and\n        configure backoff and timeout options.\n\n        A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n        Retry object and activates it only if certain conditions are met.\n        This class exists to provide safe defaults for RPC calls that are\n        not technically safe to retry normally (due to potential data\n        duplication or other side-effects) but become safe to retry if a\n        condition such as if_metageneration_match is set.\n\n        See the retry.py source code and docstrings in this package\n        (google.cloud.storage.retry) for information on retry types and how\n        to configure them.\n\n        Media operations (downloads and uploads) do not support non-default\n        predicates in a Retry object. The default will always be used. Other\n        configuration changes for Retry objects such as delays and deadlines\n        are respected.\n\n    :param upload_kwargs:\n        Keyword arguments to pass to the underlying API\n        calls. The following arguments are supported:\n\n        - ``if_generation_match``\n        - ``if_generation_not_match``\n        - ``if_metageneration_match``\n        - ``if_metageneration_not_match``\n        - ``timeout``\n        - ``content_type``\n        - ``num_retries``\n        - ``predefined_acl``\n        - ``checksum``\n    \"\"\"\n\n    def __init__(\n        self,\n        blob,\n        chunk_size=None,\n        text_mode=False,\n        ignore_flush=False,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        **upload_kwargs,\n    ):\n        for kwarg in upload_kwargs:\n            if kwarg not in VALID_UPLOAD_KWARGS:\n                raise ValueError(\n                    f\"BlobWriter does not support keyword argument {kwarg}.\"\n                )\n        self._blob = blob\n        self._buffer = SlidingBuffer()\n        self._upload_and_transport = None\n        # Resumable uploads require a chunk size of a multiple of 256KiB.\n        # self._chunk_size must not be changed after the upload is initiated.\n        self._chunk_size = chunk_size or blob.chunk_size or DEFAULT_CHUNK_SIZE\n        # text_mode is a deprecated synonym for ignore_flush\n        self._ignore_flush = ignore_flush or text_mode\n        self._retry = retry\n        self._upload_kwargs = upload_kwargs\n\n    @property\n    def _chunk_size(self):\n        \"\"\"Get the blob's default chunk size.\n\n        :rtype: int or ``NoneType``\n        :returns: The current blob's chunk size, if it is set.\n        \"\"\"\n        return self.__chunk_size\n\n    @_chunk_size.setter\n    def _chunk_size(self, value):\n        \"\"\"Set the blob's default chunk size.\n\n        :type value: int\n        :param value: (Optional) The current blob's chunk size, if it is set.\n\n        :raises: :class:`ValueError` if ``value`` is not ``None`` and is not a\n                 multiple of 256 KiB.\n        \"\"\"\n        if value is not None and value > 0 and value % CHUNK_SIZE_MULTIPLE != 0:\n            raise ValueError(\n                \"Chunk size must be a multiple of %d.\" % CHUNK_SIZE_MULTIPLE\n            )\n        self.__chunk_size = value\n\n    def write(self, b):\n        self._checkClosed()  # Raises ValueError if closed.\n\n        pos = self._buffer.write(b)\n\n        # If there is enough content, upload chunks.\n        num_chunks = len(self._buffer) // self._chunk_size\n        if num_chunks:\n            self._upload_chunks_from_buffer(num_chunks)\n\n        return pos\n\n    def _initiate_upload(self):\n        # num_retries is only supported for backwards-compatibility reasons.\n        num_retries = self._upload_kwargs.pop(\"num_retries\", None)\n        retry = self._retry\n        content_type = self._upload_kwargs.pop(\"content_type\", None)\n\n        if num_retries is not None:\n            warnings.warn(_NUM_RETRIES_MESSAGE, DeprecationWarning, stacklevel=2)\n            # num_retries and retry are mutually exclusive. If num_retries is\n            # set and retry is exactly the default, then nullify retry for\n            # backwards compatibility.\n            if retry is DEFAULT_RETRY_IF_GENERATION_SPECIFIED:\n                retry = None\n\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": self._upload_kwargs.get(\"if_generation_match\"),\n                \"ifMetagenerationMatch\": self._upload_kwargs.get(\n                    \"if_metageneration_match\"\n                ),\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        self._upload_and_transport = self._blob._initiate_resumable_upload(\n            self._blob.bucket.client,\n            self._buffer,\n            content_type,\n            None,\n            num_retries,\n            chunk_size=self._chunk_size,\n            retry=retry,\n            **self._upload_kwargs,\n        )\n\n    def _upload_chunks_from_buffer(self, num_chunks):\n        \"\"\"Upload a specified number of chunks.\"\"\"\n\n        # Initialize the upload if necessary.\n        if not self._upload_and_transport:\n            self._initiate_upload()\n\n        upload, transport = self._upload_and_transport\n\n        # Attach timeout if specified in the keyword arguments.\n        # Otherwise, the default timeout will be used from the media library.\n        kwargs = {}\n        if \"timeout\" in self._upload_kwargs:\n            kwargs = {\"timeout\": self._upload_kwargs.get(\"timeout\")}\n\n        # Upload chunks. The SlidingBuffer class will manage seek position.\n        for _ in range(num_chunks):\n            upload.transmit_next_chunk(transport, **kwargs)\n\n        # Wipe the buffer of chunks uploaded, preserving any remaining data.\n        self._buffer.flush()\n\n    def tell(self):\n        return self._buffer.tell() + len(self._buffer)\n\n    def flush(self):\n        # flush() is not fully supported by the remote service, so raise an\n        # error here, unless self._ignore_flush is set.\n        if not self._ignore_flush:\n            raise io.UnsupportedOperation(\n                \"Cannot flush without finalizing upload. Use close() instead, \"\n                \"or set ignore_flush=True when constructing this class (see \"\n                \"docstring).\"\n            )\n\n    def close(self):\n        if not self._buffer.closed:\n            self._upload_chunks_from_buffer(1)\n        self._buffer.close()\n\n    @property\n    def closed(self):\n        return self._buffer.closed\n\n    def readable(self):\n        return False\n\n    def writable(self):\n        return True\n\n    def seekable(self):\n        return False\n\n\nclass SlidingBuffer(object):\n    \"\"\"A non-rewindable buffer that frees memory of chunks already consumed.\n\n    This class is necessary because `google-resumable-media-python` expects\n    `tell()` to work relative to the start of the file, not relative to a place\n    in an intermediate buffer. Using this class, we present an external\n    interface with consistent seek and tell behavior without having to actually\n    store bytes already sent.\n\n    Behavior of this class differs from an ordinary BytesIO buffer. `write()`\n    will always append to the end of the file only and not change the seek\n    position otherwise. `flush()` will delete all data already read (data to the\n    left of the seek position). `tell()` will report the seek position of the\n    buffer including all deleted data. Additionally the class implements\n    __len__() which will report the size of the actual underlying buffer.\n\n    This class does not attempt to implement the entire Python I/O interface.\n    \"\"\"\n\n    def __init__(self):\n        self._buffer = io.BytesIO()\n        self._cursor = 0\n\n    def write(self, b):\n        \"\"\"Append to the end of the buffer without changing the position.\"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        bookmark = self._buffer.tell()\n        self._buffer.seek(0, io.SEEK_END)\n        pos = self._buffer.write(b)\n        self._buffer.seek(bookmark)\n        return self._cursor + pos\n\n    def read(self, size=-1):\n        \"\"\"Read and move the cursor.\"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        data = self._buffer.read(size)\n        self._cursor += len(data)\n        return data\n\n    def flush(self):\n        \"\"\"Delete already-read data (all data to the left of the position).\"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        # BytesIO can't be deleted from the left, so save any leftover, unread\n        # data and truncate at 0, then readd leftover data.\n        leftover = self._buffer.read()\n        self._buffer.seek(0)\n        self._buffer.truncate(0)\n        self._buffer.write(leftover)\n        self._buffer.seek(0)\n\n    def tell(self):\n        \"\"\"Report how many bytes have been read from the buffer in total.\"\"\"\n        return self._cursor\n\n    def seek(self, pos):\n        \"\"\"Seek to a position (backwards only) within the internal buffer.\n\n        This implementation of seek() verifies that the seek destination is\n        contained in _buffer. It will raise ValueError if the destination byte\n        has already been purged from the buffer.\n\n        The \"whence\" argument is not supported in this implementation.\n        \"\"\"\n        self._checkClosed()  # Raises ValueError if closed.\n\n        buffer_initial_pos = self._buffer.tell()\n        difference = pos - self._cursor\n        buffer_seek_result = self._buffer.seek(difference, io.SEEK_CUR)\n        if (\n            not buffer_seek_result - buffer_initial_pos == difference\n            or pos > self._cursor\n        ):\n            # The seek did not arrive at the expected byte because the internal\n            # buffer does not (or no longer) contains the byte. Reset and raise.\n            self._buffer.seek(buffer_initial_pos)\n            raise ValueError(\"Cannot seek() to that value.\")\n\n        self._cursor = pos\n        return self._cursor\n\n    def __len__(self):\n        \"\"\"Determine the size of the buffer by seeking to the end.\"\"\"\n        bookmark = self._buffer.tell()\n        length = self._buffer.seek(0, io.SEEK_END)\n        self._buffer.seek(bookmark)\n        return length\n\n    def close(self):\n        return self._buffer.close()\n\n    def _checkClosed(self):\n        return self._buffer._checkClosed()\n\n    @property\n    def closed(self):\n        return self._buffer.closed\n", "google/cloud/storage/constants.py": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Constants used across google.cloud.storage modules.\n\nSee [Python Storage Client Constants Page](https://github.com/googleapis/python-storage/blob/main/google/cloud/storage/constants.py)\nfor constants used across storage classes, location types, public access prevention, etc.\n\n\"\"\"\n\n# Storage classes\n\nSTANDARD_STORAGE_CLASS = \"STANDARD\"\n\"\"\"Storage class for objects accessed more than once per month.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nNEARLINE_STORAGE_CLASS = \"NEARLINE\"\n\"\"\"Storage class for objects accessed at most once per month.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nCOLDLINE_STORAGE_CLASS = \"COLDLINE\"\n\"\"\"Storage class for objects accessed at most once per year.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nARCHIVE_STORAGE_CLASS = \"ARCHIVE\"\n\"\"\"Storage class for objects accessed less frequently than once per year.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nMULTI_REGIONAL_LEGACY_STORAGE_CLASS = \"MULTI_REGIONAL\"\n\"\"\"Legacy storage class.\n\nAlias for :attr:`STANDARD_STORAGE_CLASS`.\n\nCan only be used for objects in buckets whose\n:attr:`~google.cloud.storage.bucket.Bucket.location_type` is\n:attr:`~google.cloud.storage.bucket.Bucket.MULTI_REGION_LOCATION_TYPE`.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nREGIONAL_LEGACY_STORAGE_CLASS = \"REGIONAL\"\n\"\"\"Legacy storage class.\n\nAlias for :attr:`STANDARD_STORAGE_CLASS`.\n\nCan only be used for objects in buckets whose\n:attr:`~google.cloud.storage.bucket.Bucket.location_type` is\n:attr:`~google.cloud.storage.bucket.Bucket.REGION_LOCATION_TYPE`.\n\nSee: https://cloud.google.com/storage/docs/storage-classes\n\"\"\"\n\nDURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS = \"DURABLE_REDUCED_AVAILABILITY\"\n\"\"\"Legacy storage class.\n\nSimilar to :attr:`NEARLINE_STORAGE_CLASS`.\n\"\"\"\n\n\n# Location types\n\nMULTI_REGION_LOCATION_TYPE = \"multi-region\"\n\"\"\"Location type: data will be replicated across regions in a multi-region.\n\nProvides highest availability across largest area.\n\"\"\"\n\nREGION_LOCATION_TYPE = \"region\"\n\"\"\"Location type: data will be stored within a single region.\n\nProvides lowest latency within a single region.\n\"\"\"\n\nDUAL_REGION_LOCATION_TYPE = \"dual-region\"\n\"\"\"Location type: data will be stored within two primary regions.\n\nProvides high availability and low latency across two regions.\n\"\"\"\n\n\n# Internal constants\n\n_DEFAULT_TIMEOUT = 60  # in seconds\n\"\"\"The default request timeout in seconds if a timeout is not explicitly given.\n\"\"\"\n\n# Public Access Prevention\nPUBLIC_ACCESS_PREVENTION_ENFORCED = \"enforced\"\n\"\"\"Enforced public access prevention value.\n\nSee: https://cloud.google.com/storage/docs/public-access-prevention\n\"\"\"\n\nPUBLIC_ACCESS_PREVENTION_UNSPECIFIED = \"unspecified\"\n\"\"\"Unspecified public access prevention value.\n\nDEPRECATED: Use 'PUBLIC_ACCESS_PREVENTION_INHERITED' instead.\n\nSee: https://cloud.google.com/storage/docs/public-access-prevention\n\"\"\"\n\nPUBLIC_ACCESS_PREVENTION_INHERITED = \"inherited\"\n\"\"\"Inherited public access prevention value.\n\nSee: https://cloud.google.com/storage/docs/public-access-prevention\n\"\"\"\n\nRPO_ASYNC_TURBO = \"ASYNC_TURBO\"\n\"\"\"The recovery point objective (RPO) indicates how quickly newly written objects are asynchronously replicated to a separate geographic location.\nWhen the RPO value is set to ASYNC_TURBO, the turbo replication feature is enabled.\n\nSee: https://cloud.google.com/storage/docs/managing-turbo-replication\n\"\"\"\n\nRPO_DEFAULT = \"DEFAULT\"\n\"\"\"The recovery point objective (RPO) indicates how quickly newly written objects are asynchronously replicated to a separate geographic location.\nWhen the RPO value is set to DEFAULT, the default replication behavior is enabled.\n\nSee: https://cloud.google.com/storage/docs/managing-turbo-replication\n\"\"\"\n", "google/cloud/storage/blob.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: disable=too-many-lines\n\n\"\"\"Create / interact with Google Cloud Storage blobs.\n\"\"\"\n\nimport base64\nimport copy\nimport hashlib\nfrom io import BytesIO\nfrom io import TextIOWrapper\nimport logging\nimport mimetypes\nimport os\nimport re\nfrom email.parser import HeaderParser\nfrom urllib.parse import parse_qsl\nfrom urllib.parse import quote\nfrom urllib.parse import urlencode\nfrom urllib.parse import urlsplit\nfrom urllib.parse import urlunsplit\nimport warnings\n\nfrom google import resumable_media\nfrom google.resumable_media.requests import ChunkedDownload\nfrom google.resumable_media.requests import Download\nfrom google.resumable_media.requests import RawDownload\nfrom google.resumable_media.requests import RawChunkedDownload\nfrom google.resumable_media.requests import MultipartUpload\nfrom google.resumable_media.requests import ResumableUpload\n\nfrom google.api_core.iam import Policy\nfrom google.cloud import exceptions\nfrom google.cloud._helpers import _bytes_to_unicode\nfrom google.cloud._helpers import _datetime_to_rfc3339\nfrom google.cloud._helpers import _rfc3339_nanos_to_datetime\nfrom google.cloud._helpers import _to_bytes\nfrom google.cloud.exceptions import NotFound\nfrom google.cloud.storage._helpers import _add_etag_match_headers\nfrom google.cloud.storage._helpers import _add_generation_match_parameters\nfrom google.cloud.storage._helpers import _PropertyMixin\nfrom google.cloud.storage._helpers import _scalar_property\nfrom google.cloud.storage._helpers import _bucket_bound_hostname_url\nfrom google.cloud.storage._helpers import _raise_if_more_than_one_set\nfrom google.cloud.storage._helpers import _api_core_retry_to_resumable_media_retry\nfrom google.cloud.storage._helpers import _get_default_headers\nfrom google.cloud.storage._helpers import _get_default_storage_base_url\nfrom google.cloud.storage._signing import generate_signed_url_v2\nfrom google.cloud.storage._signing import generate_signed_url_v4\nfrom google.cloud.storage._helpers import _NUM_RETRIES_MESSAGE\nfrom google.cloud.storage._helpers import _API_VERSION\nfrom google.cloud.storage._helpers import _virtual_hosted_style_base_url\nfrom google.cloud.storage.acl import ACL\nfrom google.cloud.storage.acl import ObjectACL\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.constants import ARCHIVE_STORAGE_CLASS\nfrom google.cloud.storage.constants import COLDLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import MULTI_REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import NEARLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import STANDARD_STORAGE_CLASS\nfrom google.cloud.storage.retry import ConditionalRetryPolicy\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\nfrom google.cloud.storage.fileio import BlobReader\nfrom google.cloud.storage.fileio import BlobWriter\n\n\n_DEFAULT_CONTENT_TYPE = \"application/octet-stream\"\n_DOWNLOAD_URL_TEMPLATE = \"{hostname}/download/storage/{api_version}{path}?alt=media\"\n_BASE_UPLOAD_TEMPLATE = (\n    \"{hostname}/upload/storage/{api_version}{bucket_path}/o?uploadType=\"\n)\n_MULTIPART_URL_TEMPLATE = _BASE_UPLOAD_TEMPLATE + \"multipart\"\n_RESUMABLE_URL_TEMPLATE = _BASE_UPLOAD_TEMPLATE + \"resumable\"\n# NOTE: \"acl\" is also writeable but we defer ACL management to\n#       the classes in the google.cloud.storage.acl module.\n_CONTENT_TYPE_FIELD = \"contentType\"\n_WRITABLE_FIELDS = (\n    \"cacheControl\",\n    \"contentDisposition\",\n    \"contentEncoding\",\n    \"contentLanguage\",\n    _CONTENT_TYPE_FIELD,\n    \"crc32c\",\n    \"customTime\",\n    \"md5Hash\",\n    \"metadata\",\n    \"name\",\n    \"retention\",\n    \"storageClass\",\n)\n_READ_LESS_THAN_SIZE = (\n    \"Size {:d} was specified but the file-like object only had \" \"{:d} bytes remaining.\"\n)\n_CHUNKED_DOWNLOAD_CHECKSUM_MESSAGE = (\n    \"A checksum of type `{}` was requested, but checksumming is not available \"\n    \"for downloads when chunk_size is set.\"\n)\n_COMPOSE_IF_GENERATION_LIST_DEPRECATED = (\n    \"'if_generation_match: type list' is deprecated and supported for \"\n    \"backwards-compatability reasons only.  Use 'if_source_generation_match' \"\n    \"instead' to match source objects' generations.\"\n)\n_COMPOSE_IF_GENERATION_LIST_AND_IF_SOURCE_GENERATION_ERROR = (\n    \"Use 'if_generation_match' to match the generation of the destination \"\n    \"object by passing in a generation number, instead of a list. \"\n    \"Use 'if_source_generation_match' to match source objects generations.\"\n)\n_COMPOSE_IF_METAGENERATION_LIST_DEPRECATED = (\n    \"'if_metageneration_match: type list' is deprecated and supported for \"\n    \"backwards-compatability reasons only. Note that the metageneration to \"\n    \"be matched is that of the destination blob. Please pass in a single \"\n    \"value (type long).\"\n)\n_COMPOSE_IF_SOURCE_GENERATION_MISMATCH_ERROR = (\n    \"'if_source_generation_match' length must be the same as 'sources' length\"\n)\n_DOWNLOAD_AS_STRING_DEPRECATED = (\n    \"Blob.download_as_string() is deprecated and will be removed in future. \"\n    \"Use Blob.download_as_bytes() instead.\"\n)\n_GS_URL_REGEX_PATTERN = re.compile(\n    r\"(?P<scheme>gs)://(?P<bucket_name>[a-z0-9_.-]+)/(?P<object_name>.+)\"\n)\n\n_DEFAULT_CHUNKSIZE = 104857600  # 1024 * 1024 B * 100 = 100 MB\n_MAX_MULTIPART_SIZE = 8388608  # 8 MB\n\n_logger = logging.getLogger(__name__)\n\n\nclass Blob(_PropertyMixin):\n    \"\"\"A wrapper around Cloud Storage's concept of an ``Object``.\n\n    :type name: str\n    :param name: The name of the blob.  This corresponds to the unique path of\n                 the object in the bucket. If bytes, will be converted to a\n                 unicode object. Blob / object names can contain any sequence\n                 of valid unicode characters, of length 1-1024 bytes when\n                 UTF-8 encoded.\n\n    :type bucket: :class:`google.cloud.storage.bucket.Bucket`\n    :param bucket: The bucket to which this blob belongs.\n\n    :type chunk_size: int\n    :param chunk_size:\n        (Optional) The size of a chunk of data whenever iterating (in bytes).\n        This must be a multiple of 256 KB per the API specification. If not\n        specified, the chunk_size of the blob itself is used. If that is not\n        specified, a default value of 40 MB is used.\n\n    :type encryption_key: bytes\n    :param encryption_key:\n        (Optional) 32 byte encryption key for customer-supplied encryption.\n        See https://cloud.google.com/storage/docs/encryption#customer-supplied.\n\n    :type kms_key_name: str\n    :param kms_key_name:\n        (Optional) Resource name of Cloud KMS key used to encrypt the blob's\n        contents.\n\n    :type generation: long\n    :param generation:\n        (Optional) If present, selects a specific revision of this object.\n    \"\"\"\n\n    _chunk_size = None  # Default value for each instance.\n    _CHUNK_SIZE_MULTIPLE = 256 * 1024\n    \"\"\"Number (256 KB, in bytes) that must divide the chunk size.\"\"\"\n\n    STORAGE_CLASSES = (\n        STANDARD_STORAGE_CLASS,\n        NEARLINE_STORAGE_CLASS,\n        COLDLINE_STORAGE_CLASS,\n        ARCHIVE_STORAGE_CLASS,\n        MULTI_REGIONAL_LEGACY_STORAGE_CLASS,\n        REGIONAL_LEGACY_STORAGE_CLASS,\n    )\n    \"\"\"Allowed values for :attr:`storage_class`.\n\n    See\n    https://cloud.google.com/storage/docs/json_api/v1/objects#storageClass\n    https://cloud.google.com/storage/docs/per-object-storage-class\n\n    .. note::\n       This list does not include 'DURABLE_REDUCED_AVAILABILITY', which\n       is only documented for buckets (and deprecated).\n    \"\"\"\n\n    def __init__(\n        self,\n        name,\n        bucket,\n        chunk_size=None,\n        encryption_key=None,\n        kms_key_name=None,\n        generation=None,\n    ):\n        \"\"\"\n        property :attr:`name`\n            Get the blob's name.\n        \"\"\"\n        name = _bytes_to_unicode(name)\n        super(Blob, self).__init__(name=name)\n\n        self.chunk_size = chunk_size  # Check that setter accepts value.\n        self._bucket = bucket\n        self._acl = ObjectACL(self)\n        _raise_if_more_than_one_set(\n            encryption_key=encryption_key, kms_key_name=kms_key_name\n        )\n\n        self._encryption_key = encryption_key\n\n        if kms_key_name is not None:\n            self._properties[\"kmsKeyName\"] = kms_key_name\n\n        if generation is not None:\n            self._properties[\"generation\"] = generation\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket which contains the object.\n\n        :rtype: :class:`~google.cloud.storage.bucket.Bucket`\n        :returns: The object's bucket.\n        \"\"\"\n        return self._bucket\n\n    @property\n    def chunk_size(self):\n        \"\"\"Get the blob's default chunk size.\n\n        :rtype: int or ``NoneType``\n        :returns: The current blob's chunk size, if it is set.\n        \"\"\"\n        return self._chunk_size\n\n    @chunk_size.setter\n    def chunk_size(self, value):\n        \"\"\"Set the blob's default chunk size.\n\n        :type value: int\n        :param value: (Optional) The current blob's chunk size, if it is set.\n\n        :raises: :class:`ValueError` if ``value`` is not ``None`` and is not a\n                 multiple of 256 KB.\n        \"\"\"\n        if value is not None and value > 0 and value % self._CHUNK_SIZE_MULTIPLE != 0:\n            raise ValueError(\n                \"Chunk size must be a multiple of %d.\" % (self._CHUNK_SIZE_MULTIPLE,)\n            )\n        self._chunk_size = value\n\n    @property\n    def encryption_key(self):\n        \"\"\"Retrieve the customer-supplied encryption key for the object.\n\n        :rtype: bytes or ``NoneType``\n        :returns:\n            The encryption key or ``None`` if no customer-supplied encryption key was used,\n            or the blob's resource has not been loaded from the server.\n        \"\"\"\n        return self._encryption_key\n\n    @encryption_key.setter\n    def encryption_key(self, value):\n        \"\"\"Set the blob's encryption key.\n\n        See https://cloud.google.com/storage/docs/encryption#customer-supplied\n\n        To perform a key rotation for an encrypted blob, use :meth:`rewrite`.\n        See https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys?hl=ca#rotating\n\n        :type value: bytes\n        :param value: 32 byte encryption key for customer-supplied encryption.\n        \"\"\"\n        self._encryption_key = value\n\n    @staticmethod\n    def path_helper(bucket_path, blob_name):\n        \"\"\"Relative URL path for a blob.\n\n        :type bucket_path: str\n        :param bucket_path: The URL path for a bucket.\n\n        :type blob_name: str\n        :param blob_name: The name of the blob.\n\n        :rtype: str\n        :returns: The relative URL path for ``blob_name``.\n        \"\"\"\n        return bucket_path + \"/o/\" + _quote(blob_name)\n\n    @property\n    def acl(self):\n        \"\"\"Create our ACL on demand.\"\"\"\n        return self._acl\n\n    def __repr__(self):\n        if self.bucket:\n            bucket_name = self.bucket.name\n        else:\n            bucket_name = None\n\n        return f\"<Blob: {bucket_name}, {self.name}, {self.generation}>\"\n\n    @property\n    def path(self):\n        \"\"\"Getter property for the URL path to this Blob.\n\n        :rtype: str\n        :returns: The URL path to this Blob.\n        \"\"\"\n        if not self.name:\n            raise ValueError(\"Cannot determine path without a blob name.\")\n\n        return self.path_helper(self.bucket.path, self.name)\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this blob.\"\"\"\n        return self.bucket.client\n\n    @property\n    def user_project(self):\n        \"\"\"Project ID billed for API requests made via this blob.\n\n        Derived from bucket's value.\n\n        :rtype: str\n        \"\"\"\n        return self.bucket.user_project\n\n    def _encryption_headers(self):\n        \"\"\"Return any encryption headers needed to fetch the object.\n\n        :rtype: List(Tuple(str, str))\n        :returns: a list of tuples to be passed as headers.\n        \"\"\"\n        return _get_encryption_headers(self._encryption_key)\n\n    @property\n    def _query_params(self):\n        \"\"\"Default query parameters.\"\"\"\n        params = {}\n        if self.generation is not None:\n            params[\"generation\"] = self.generation\n        if self.user_project is not None:\n            params[\"userProject\"] = self.user_project\n        return params\n\n    @property\n    def public_url(self):\n        \"\"\"The public URL for this blob.\n\n        Use :meth:`make_public` to enable anonymous access via the returned\n        URL.\n\n        :rtype: `string`\n        :returns: The public URL for this blob.\n        \"\"\"\n        if self.client:\n            endpoint = self.client.api_endpoint\n        else:\n            endpoint = _get_default_storage_base_url()\n        return \"{storage_base_url}/{bucket_name}/{quoted_name}\".format(\n            storage_base_url=endpoint,\n            bucket_name=self.bucket.name,\n            quoted_name=_quote(self.name, safe=b\"/~\"),\n        )\n\n    @classmethod\n    def from_string(cls, uri, client=None):\n        \"\"\"Get a constructor for blob object by URI.\n\n        .. code-block:: python\n\n            from google.cloud import storage\n            from google.cloud.storage.blob import Blob\n            client = storage.Client()\n            blob = Blob.from_string(\"gs://bucket/object\", client=client)\n\n        :type uri: str\n        :param uri: The blob uri following a gs://bucket/object pattern.\n          Both a bucket and object name is required to construct a blob object.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  Application code should\n            *always* pass ``client``.\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The blob object created.\n        \"\"\"\n        from google.cloud.storage.bucket import Bucket\n\n        match = _GS_URL_REGEX_PATTERN.match(uri)\n        if not match:\n            raise ValueError(\"URI pattern must be gs://bucket/object\")\n        bucket = Bucket(client, name=match.group(\"bucket_name\"))\n        return cls(match.group(\"object_name\"), bucket)\n\n    def generate_signed_url(\n        self,\n        expiration=None,\n        api_access_endpoint=None,\n        method=\"GET\",\n        content_md5=None,\n        content_type=None,\n        response_disposition=None,\n        response_type=None,\n        generation=None,\n        headers=None,\n        query_parameters=None,\n        client=None,\n        credentials=None,\n        version=None,\n        service_account_email=None,\n        access_token=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n    ):\n        \"\"\"Generates a signed URL for this blob.\n\n        .. note::\n\n            If you are on Google Compute Engine, you can't generate a signed\n            URL using GCE service account.\n            If you'd like to be able to generate a signed URL from GCE,\n            you can use a standard service account from a JSON file rather\n            than a GCE service account.\n\n        If you have a blob that you want to allow access to for a set\n        amount of time, you can use this method to generate a URL that\n        is only valid within a certain time period.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-generate-signed-url-v4#storage_generate_signed_url_v4-python).\n\n        This is particularly useful if you don't want publicly\n        accessible blobs, but don't want to require users to explicitly\n        log in.\n\n        If ``bucket_bound_hostname`` is set as an argument of :attr:`api_access_endpoint`,\n        ``https`` works only if using a ``CDN``.\n\n        :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n        :param expiration:\n            Point in time when the signed URL should expire. If a ``datetime``\n            instance is passed without an explicit ``tzinfo`` set,  it will be\n            assumed to be ``UTC``.\n\n        :type api_access_endpoint: str\n        :param api_access_endpoint: (Optional) URI base, for instance\n            \"https://storage.googleapis.com\". If not specified, the client's\n            api_endpoint will be used. Incompatible with bucket_bound_hostname.\n\n        :type method: str\n        :param method: The HTTP verb that will be used when requesting the URL.\n\n        :type content_md5: str\n        :param content_md5:\n            (Optional) The MD5 hash of the object referenced by ``resource``.\n\n        :type content_type: str\n        :param content_type:\n            (Optional) The content type of the object referenced by\n            ``resource``.\n\n        :type response_disposition: str\n        :param response_disposition:\n            (Optional) Content disposition of responses to requests for the\n            signed URL.  For example, to enable the signed URL to initiate a\n            file of ``blog.png``, use the value ``'attachment;\n            filename=blob.png'``.\n\n        :type response_type: str\n        :param response_type:\n            (Optional) Content type of responses to requests for the signed\n            URL. Ignored if content_type is set on object/blob metadata.\n\n        :type generation: str\n        :param generation:\n            (Optional) A value that indicates which generation of the resource\n            to fetch.\n\n        :type headers: dict\n        :param headers:\n            (Optional) Additional HTTP headers to be included as part of the\n            signed URLs. See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers\n            Requests using the signed URL *must* pass the specified header\n            (name and value) with each request for the URL.\n\n        :type query_parameters: dict\n        :param query_parameters:\n            (Optional) Additional query parameters to be included as part of the\n            signed URLs. See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type credentials: :class:`google.auth.credentials.Credentials`\n        :param credentials:\n            (Optional) The authorization credentials to attach to requests.\n            These credentials identify this application to the service.  If\n            none are specified, the client will attempt to ascertain the\n            credentials from the environment.\n\n        :type version: str\n        :param version:\n            (Optional) The version of signed credential to create.  Must be one\n            of 'v2' | 'v4'.\n\n        :type service_account_email: str\n        :param service_account_email:\n            (Optional) E-mail address of the service account.\n\n        :type access_token: str\n        :param access_token: (Optional) Access token for a service account.\n\n        :type virtual_hosted_style: bool\n        :param virtual_hosted_style:\n            (Optional) If true, then construct the URL relative the bucket's\n            virtual hostname, e.g., '<bucket-name>.storage.googleapis.com'.\n            Incompatible with bucket_bound_hostname.\n\n        :type bucket_bound_hostname: str\n        :param bucket_bound_hostname:\n            (Optional) If passed, then construct the URL relative to the bucket-bound hostname.\n            Value can be a bare or with scheme, e.g., 'example.com' or 'http://example.com'.\n            Incompatible with api_access_endpoint and virtual_hosted_style.\n            See: https://cloud.google.com/storage/docs/request-endpoints#cname\n\n        :type scheme: str\n        :param scheme:\n            (Optional) If ``bucket_bound_hostname`` is passed as a bare\n            hostname, use this value as the scheme.  ``https`` will work only\n            when using a CDN.  Defaults to ``\"http\"``.\n\n        :raises: :exc:`ValueError` when version is invalid or mutually exclusive arguments are used.\n        :raises: :exc:`TypeError` when expiration is not a valid type.\n        :raises: :exc:`AttributeError` if credentials is not an instance\n                of :class:`google.auth.credentials.Signing`.\n\n        :rtype: str\n        :returns: A signed URL you can use to access the resource\n                  until expiration.\n        \"\"\"\n        if version is None:\n            version = \"v2\"\n        elif version not in (\"v2\", \"v4\"):\n            raise ValueError(\"'version' must be either 'v2' or 'v4'\")\n\n        if (\n            api_access_endpoint is not None or virtual_hosted_style\n        ) and bucket_bound_hostname:\n            raise ValueError(\n                \"The bucket_bound_hostname argument is not compatible with \"\n                \"either api_access_endpoint or virtual_hosted_style.\"\n            )\n\n        if api_access_endpoint is None:\n            client = self._require_client(client)\n            api_access_endpoint = client.api_endpoint\n\n        quoted_name = _quote(self.name, safe=b\"/~\")\n\n        # If you are on Google Compute Engine, you can't generate a signed URL\n        # using GCE service account.\n        # See https://github.com/googleapis/google-auth-library-python/issues/50\n        if virtual_hosted_style:\n            api_access_endpoint = _virtual_hosted_style_base_url(\n                api_access_endpoint, self.bucket.name\n            )\n            resource = f\"/{quoted_name}\"\n        elif bucket_bound_hostname:\n            api_access_endpoint = _bucket_bound_hostname_url(\n                bucket_bound_hostname, scheme\n            )\n            resource = f\"/{quoted_name}\"\n        else:\n            resource = f\"/{self.bucket.name}/{quoted_name}\"\n\n        if credentials is None:\n            client = self._require_client(client)  # May be redundant, but that's ok.\n            credentials = client._credentials\n\n        if version == \"v2\":\n            helper = generate_signed_url_v2\n        else:\n            helper = generate_signed_url_v4\n\n        if self._encryption_key is not None:\n            encryption_headers = _get_encryption_headers(self._encryption_key)\n            if headers is None:\n                headers = {}\n            if version == \"v2\":\n                # See: https://cloud.google.com/storage/docs/access-control/signed-urls-v2#about-canonical-extension-headers\n                v2_copy_only = \"X-Goog-Encryption-Algorithm\"\n                headers[v2_copy_only] = encryption_headers[v2_copy_only]\n            else:\n                headers.update(encryption_headers)\n\n        return helper(\n            credentials,\n            resource=resource,\n            expiration=expiration,\n            api_access_endpoint=api_access_endpoint,\n            method=method.upper(),\n            content_md5=content_md5,\n            content_type=content_type,\n            response_type=response_type,\n            response_disposition=response_disposition,\n            generation=generation,\n            headers=headers,\n            query_parameters=query_parameters,\n            service_account_email=service_account_email,\n            access_token=access_token,\n        )\n\n    def exists(\n        self,\n        client=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        soft_deleted=None,\n    ):\n        \"\"\"Determines whether or not this blob exists.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If True, looks for a soft-deleted object. Will only return True\n            if the object exists and is in a soft-deleted state.\n            :attr:`generation` is required to be set on the blob if ``soft_deleted`` is set to True.\n            See: https://cloud.google.com/storage/docs/soft-delete\n\n        :rtype: bool\n        :returns: True if the blob exists in Cloud Storage.\n        \"\"\"\n        client = self._require_client(client)\n        # We only need the status code (200 or not) so we seek to\n        # minimize the returned payload.\n        query_params = self._query_params\n        query_params[\"fields\"] = \"name\"\n        if soft_deleted is not None:\n            query_params[\"softDeleted\"] = soft_deleted\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        headers = {}\n        _add_etag_match_headers(\n            headers, if_etag_match=if_etag_match, if_etag_not_match=if_etag_not_match\n        )\n\n        try:\n            # We intentionally pass `_target_object=None` since fields=name\n            # would limit the local properties.\n            client._get_resource(\n                self.path,\n                query_params=query_params,\n                headers=headers,\n                timeout=timeout,\n                retry=retry,\n                _target_object=None,\n            )\n        except NotFound:\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            return False\n        return True\n\n    def delete(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Deletes a blob from Cloud Storage.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n                 (propagated from\n                 :meth:`google.cloud.storage.bucket.Bucket.delete_blob`).\n        \"\"\"\n        self.bucket.delete_blob(\n            self.name,\n            client=client,\n            generation=self.generation,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def _get_transport(self, client):\n        \"\"\"Return the client's transport.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :rtype transport:\n            :class:`~google.auth.transport.requests.AuthorizedSession`\n        :returns: The transport (with credentials) that will\n                  make authenticated requests.\n        \"\"\"\n        client = self._require_client(client)\n        return client._http\n\n    def _get_download_url(\n        self,\n        client,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n    ):\n        \"\"\"Get the download URL for the current blob.\n\n        If the ``media_link`` has been loaded, it will be used, otherwise\n        the URL will be constructed from the current blob's path (and possibly\n        generation) to avoid a round trip.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: The client to use.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :rtype: str\n        :returns: The download URL for the current blob.\n        \"\"\"\n        name_value_pairs = []\n        if self.media_link is None:\n            hostname = _get_host_name(client._connection)\n            base_url = _DOWNLOAD_URL_TEMPLATE.format(\n                hostname=hostname, path=self.path, api_version=_API_VERSION\n            )\n            if self.generation is not None:\n                name_value_pairs.append((\"generation\", f\"{self.generation:d}\"))\n        else:\n            base_url = self.media_link\n\n        if self.user_project is not None:\n            name_value_pairs.append((\"userProject\", self.user_project))\n\n        _add_generation_match_parameters(\n            name_value_pairs,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        return _add_query_parameters(base_url, name_value_pairs)\n\n    def _extract_headers_from_download(self, response):\n        \"\"\"Extract headers from a non-chunked request's http object.\n\n        This avoids the need to make a second request for commonly used\n        headers.\n\n        :type response:\n            :class requests.models.Response\n        :param response: The server response from downloading a non-chunked file\n        \"\"\"\n        self._properties[\"contentEncoding\"] = response.headers.get(\n            \"Content-Encoding\", None\n        )\n        self._properties[_CONTENT_TYPE_FIELD] = response.headers.get(\n            \"Content-Type\", None\n        )\n        self._properties[\"cacheControl\"] = response.headers.get(\"Cache-Control\", None)\n        self._properties[\"storageClass\"] = response.headers.get(\n            \"X-Goog-Storage-Class\", None\n        )\n        self._properties[\"contentLanguage\"] = response.headers.get(\n            \"Content-Language\", None\n        )\n        self._properties[\"etag\"] = response.headers.get(\"ETag\", None)\n        self._properties[\"generation\"] = response.headers.get(\"X-goog-generation\", None)\n        self._properties[\"metageneration\"] = response.headers.get(\n            \"X-goog-metageneration\", None\n        )\n        #  'X-Goog-Hash': 'crc32c=4gcgLQ==,md5=CS9tHYTtyFntzj7B9nkkJQ==',\n        x_goog_hash = response.headers.get(\"X-Goog-Hash\", \"\")\n\n        if x_goog_hash:\n            digests = {}\n            for encoded_digest in x_goog_hash.split(\",\"):\n                match = re.match(r\"(crc32c|md5)=([\\w\\d/\\+/]+={0,3})\", encoded_digest)\n                if match:\n                    method, digest = match.groups()\n                    digests[method] = digest\n\n            self._properties[\"crc32c\"] = digests.get(\"crc32c\", None)\n            self._properties[\"md5Hash\"] = digests.get(\"md5\", None)\n\n    def _do_download(\n        self,\n        transport,\n        file_obj,\n        download_url,\n        headers,\n        start=None,\n        end=None,\n        raw_download=False,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=None,\n    ):\n        \"\"\"Perform a download without any error handling.\n\n        This is intended to be called by :meth:`_prep_and_do_download` so it can\n        be wrapped with error handling / remapping.\n\n        :type transport:\n            :class:`~google.auth.transport.requests.AuthorizedSession`\n        :param transport:\n            The transport (with credentials) that will make authenticated\n            requests.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type download_url: str\n        :param download_url: The URL where the media can be accessed.\n\n        :type headers: dict\n        :param headers: Headers to be sent with the request(s).\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._prep_and_do_download().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n        \"\"\"\n\n        retry_strategy = _api_core_retry_to_resumable_media_retry(retry)\n\n        if self.chunk_size is None:\n            if raw_download:\n                klass = RawDownload\n            else:\n                klass = Download\n\n            download = klass(\n                download_url,\n                stream=file_obj,\n                headers=headers,\n                start=start,\n                end=end,\n                checksum=checksum,\n            )\n            download._retry_strategy = retry_strategy\n            response = download.consume(transport, timeout=timeout)\n            self._extract_headers_from_download(response)\n        else:\n            if checksum:\n                msg = _CHUNKED_DOWNLOAD_CHECKSUM_MESSAGE.format(checksum)\n                _logger.info(msg)\n\n            if raw_download:\n                klass = RawChunkedDownload\n            else:\n                klass = ChunkedDownload\n\n            download = klass(\n                download_url,\n                self.chunk_size,\n                file_obj,\n                headers=headers,\n                start=start if start else 0,\n                end=end,\n            )\n\n            download._retry_strategy = retry_strategy\n            while not download.finished:\n                download.consume_next_chunk(transport, timeout=timeout)\n\n    def download_to_file(\n        self,\n        file_obj,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob into a file-like object.\n\n        .. note::\n\n           If the server-set property, :attr:`media_link`, is not yet\n           initialized, makes an additional API request to load it.\n\n        If the :attr:`chunk_size` of a current blob is `None`, will download data\n        in single download request otherwise it will download the :attr:`chunk_size`\n        of data in each request.\n\n        For more fine-grained control over the download process, check out\n        [`google-resumable-media`](https://googleapis.dev/python/google-resumable-media/latest/index.html).\n        For example, this library allows downloading **parts** of a blob rather than the whole thing.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n\n        self._prep_and_do_download(\n            file_obj,\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def _handle_filename_and_download(self, filename, *args, **kwargs):\n        \"\"\"Download the contents of this blob into a named file.\n\n        :type filename: str\n        :param filename: A filename to be passed to ``open``.\n\n        For *args and **kwargs, refer to the documentation for download_to_filename() for more information.\n        \"\"\"\n\n        try:\n            with open(filename, \"wb\") as file_obj:\n                self._prep_and_do_download(\n                    file_obj,\n                    *args,\n                    **kwargs,\n                )\n\n        except resumable_media.DataCorruption:\n            # Delete the corrupt downloaded file.\n            os.remove(filename)\n            raise\n\n        updated = self.updated\n        if updated is not None:\n            mtime = updated.timestamp()\n            os.utime(file_obj.name, (mtime, mtime))\n\n    def download_to_filename(\n        self,\n        filename,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob into a named file.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-download-encrypted-file#storage_download_encrypted_file-python)\n        to download a file with a [`customer-supplied encryption key`](https://cloud.google.com/storage/docs/encryption#customer-supplied).\n\n        :type filename: str\n        :param filename: A filename to be passed to ``open``.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n\n        self._handle_filename_and_download(\n            filename,\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def download_as_bytes(\n        self,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob as a bytes object.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :rtype: bytes\n        :returns: The data stored in this blob.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n\n        string_buffer = BytesIO()\n\n        self._prep_and_do_download(\n            string_buffer,\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n        return string_buffer.getvalue()\n\n    def download_as_string(\n        self,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"(Deprecated) Download the contents of this blob as a bytes object.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        .. note::\n           Deprecated alias for :meth:`download_as_bytes`.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :rtype: bytes\n        :returns: The data stored in this blob.\n\n        :raises: :class:`google.cloud.exceptions.NotFound`\n        \"\"\"\n        warnings.warn(\n            _DOWNLOAD_AS_STRING_DEPRECATED, PendingDeprecationWarning, stacklevel=2\n        )\n        return self.download_as_bytes(\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def download_as_text(\n        self,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        encoding=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of this blob as text (*not* bytes).\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type encoding: str\n        :param encoding: (Optional) encoding to be used to decode the\n            downloaded bytes.  Defaults to the ``charset`` param of\n            attr:`content_type`, or else to \"utf-8\".\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :rtype: text\n        :returns: The data stored in this blob, decoded to text.\n        \"\"\"\n        data = self.download_as_bytes(\n            client=client,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n        if encoding is not None:\n            return data.decode(encoding)\n\n        if self.content_type is not None:\n            msg = HeaderParser().parsestr(\"Content-Type: \" + self.content_type)\n            params = dict(msg.get_params()[1:])\n            if \"charset\" in params:\n                return data.decode(params[\"charset\"])\n\n        return data.decode(\"utf-8\")\n\n    def _get_content_type(self, content_type, filename=None):\n        \"\"\"Determine the content type from the current object.\n\n        The return value will be determined in order of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content.\n\n        :type filename: str\n        :param filename:\n            (Optional) The name of the file where the content is stored.\n\n        :rtype: str\n        :returns: Type of content gathered from the object.\n        \"\"\"\n        if content_type is None:\n            content_type = self.content_type\n\n        if content_type is None and filename is not None:\n            content_type, _ = mimetypes.guess_type(filename)\n\n        if content_type is None:\n            content_type = _DEFAULT_CONTENT_TYPE\n\n        return content_type\n\n    def _get_writable_metadata(self):\n        \"\"\"Get the object / blob metadata which is writable.\n\n        This is intended to be used when creating a new object / blob.\n\n        See the [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects)\n        for more information, the fields marked as writable are:\n\n        * ``acl``\n        * ``cacheControl``\n        * ``contentDisposition``\n        * ``contentEncoding``\n        * ``contentLanguage``\n        * ``contentType``\n        * ``crc32c``\n        * ``customTime``\n        * ``md5Hash``\n        * ``metadata``\n        * ``name``\n        * ``retention``\n        * ``storageClass``\n\n        For now, we don't support ``acl``, access control lists should be\n        managed directly through :class:`ObjectACL` methods.\n        \"\"\"\n        # NOTE: This assumes `self.name` is unicode.\n        object_metadata = {\"name\": self.name}\n        for key in self._changes:\n            if key in _WRITABLE_FIELDS:\n                object_metadata[key] = self._properties[key]\n\n        return object_metadata\n\n    def _get_upload_arguments(self, client, content_type, filename=None, command=None):\n        \"\"\"Get required arguments for performing an upload.\n\n        The content type returned will be determined in order of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: tuple\n        :returns: A triple of\n\n                  * A header dictionary\n                  * An object metadata dictionary\n                  * The ``content_type`` as a string (according to precedence)\n        \"\"\"\n        content_type = self._get_content_type(content_type, filename=filename)\n        # Add any client attached custom headers to the upload headers.\n        headers = {\n            **_get_default_headers(\n                client._connection.user_agent, content_type, command=command\n            ),\n            **_get_encryption_headers(self._encryption_key),\n            **client._extra_headers,\n        }\n        object_metadata = self._get_writable_metadata()\n        return headers, object_metadata, content_type\n\n    def _do_multipart_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl,\n        if_generation_match,\n        if_generation_not_match,\n        if_metageneration_match,\n        if_metageneration_not_match,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Perform a multipart upload.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. The request metadata will be amended\n            to include the computed value. Using this option will override a\n            manually-set checksum value. Supported values are \"md5\",\n            \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._do_upload().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: :class:`~requests.Response`\n        :returns: The \"200 OK\" response object returned after the multipart\n                  upload request.\n        :raises: :exc:`ValueError` if ``size`` is not :data:`None` but the\n                 ``stream`` has fewer than ``size`` bytes remaining.\n        \"\"\"\n        if size is None:\n            data = stream.read()\n        else:\n            data = stream.read(size)\n            if len(data) < size:\n                msg = _READ_LESS_THAN_SIZE.format(size, len(data))\n                raise ValueError(msg)\n\n        client = self._require_client(client)\n        transport = self._get_transport(client)\n        if \"metadata\" in self._properties and \"metadata\" not in self._changes:\n            self._changes.add(\"metadata\")\n        info = self._get_upload_arguments(client, content_type, command=command)\n        headers, object_metadata, content_type = info\n\n        hostname = _get_host_name(client._connection)\n        base_url = _MULTIPART_URL_TEMPLATE.format(\n            hostname=hostname, bucket_path=self.bucket.path, api_version=_API_VERSION\n        )\n        name_value_pairs = []\n\n        if self.user_project is not None:\n            name_value_pairs.append((\"userProject\", self.user_project))\n\n        # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n        # at rest, object resource metadata will store the version of the Key Management\n        # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n        # used to upload a new version of the object then the existing kmsKeyName version\n        # value can't be used in the upload request and the client instead ignores it.\n        if (\n            self.kms_key_name is not None\n            and \"cryptoKeyVersions\" not in self.kms_key_name\n        ):\n            name_value_pairs.append((\"kmsKeyName\", self.kms_key_name))\n\n        if predefined_acl is not None:\n            name_value_pairs.append((\"predefinedAcl\", predefined_acl))\n\n        if if_generation_match is not None:\n            name_value_pairs.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            name_value_pairs.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            name_value_pairs.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            name_value_pairs.append(\n                (\"ifMetaGenerationNotMatch\", if_metageneration_not_match)\n            )\n\n        upload_url = _add_query_parameters(base_url, name_value_pairs)\n        upload = MultipartUpload(upload_url, headers=headers, checksum=checksum)\n\n        upload._retry_strategy = _api_core_retry_to_resumable_media_retry(\n            retry, num_retries\n        )\n\n        response = upload.transmit(\n            transport, data, object_metadata, content_type, timeout=timeout\n        )\n\n        return response\n\n    def _initiate_resumable_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl=None,\n        extra_headers=None,\n        chunk_size=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Initiate a resumable upload.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type extra_headers: dict\n        :param extra_headers:\n            (Optional) Extra headers to add to standard headers.\n\n        :type chunk_size: int\n        :param chunk_size:\n            (Optional) Chunk size to use when creating a\n            :class:`~google.resumable_media.requests.ResumableUpload`.\n            If not passed, will fall back to the chunk size on the\n            current blob, if the chunk size of a current blob is also\n            `None`, will set the default value.\n            The default value of ``chunk_size`` is 100 MB.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. After the upload is complete, the\n            server-computed checksum of the resulting object will be checked\n            and google.resumable_media.common.DataCorruption will be raised on\n            a mismatch. On a validation failure, the client will attempt to\n            delete the uploaded object automatically. Supported values\n            are \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._do_upload().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: tuple\n        :returns:\n            Pair of\n\n            * The :class:`~google.resumable_media.requests.ResumableUpload`\n              that was created\n            * The ``transport`` used to initiate the upload.\n        \"\"\"\n        client = self._require_client(client)\n        if chunk_size is None:\n            chunk_size = self.chunk_size\n            if chunk_size is None:\n                chunk_size = _DEFAULT_CHUNKSIZE\n\n        transport = self._get_transport(client)\n        if \"metadata\" in self._properties and \"metadata\" not in self._changes:\n            self._changes.add(\"metadata\")\n        info = self._get_upload_arguments(client, content_type, command=command)\n        headers, object_metadata, content_type = info\n        if extra_headers is not None:\n            headers.update(extra_headers)\n\n        hostname = _get_host_name(client._connection)\n        base_url = _RESUMABLE_URL_TEMPLATE.format(\n            hostname=hostname, bucket_path=self.bucket.path, api_version=_API_VERSION\n        )\n        name_value_pairs = []\n\n        if self.user_project is not None:\n            name_value_pairs.append((\"userProject\", self.user_project))\n\n        # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n        # at rest, object resource metadata will store the version of the Key Management\n        # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n        # used to upload a new version of the object then the existing kmsKeyName version\n        # value can't be used in the upload request and the client instead ignores it.\n        if (\n            self.kms_key_name is not None\n            and \"cryptoKeyVersions\" not in self.kms_key_name\n        ):\n            name_value_pairs.append((\"kmsKeyName\", self.kms_key_name))\n\n        if predefined_acl is not None:\n            name_value_pairs.append((\"predefinedAcl\", predefined_acl))\n\n        if if_generation_match is not None:\n            name_value_pairs.append((\"ifGenerationMatch\", if_generation_match))\n\n        if if_generation_not_match is not None:\n            name_value_pairs.append((\"ifGenerationNotMatch\", if_generation_not_match))\n\n        if if_metageneration_match is not None:\n            name_value_pairs.append((\"ifMetagenerationMatch\", if_metageneration_match))\n\n        if if_metageneration_not_match is not None:\n            name_value_pairs.append(\n                (\"ifMetaGenerationNotMatch\", if_metageneration_not_match)\n            )\n\n        upload_url = _add_query_parameters(base_url, name_value_pairs)\n        upload = ResumableUpload(\n            upload_url, chunk_size, headers=headers, checksum=checksum\n        )\n\n        upload._retry_strategy = _api_core_retry_to_resumable_media_retry(\n            retry, num_retries\n        )\n\n        upload.initiate(\n            transport,\n            stream,\n            object_metadata,\n            content_type,\n            total_bytes=size,\n            stream_final=False,\n            timeout=timeout,\n        )\n\n        return upload, transport\n\n    def _do_resumable_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl,\n        if_generation_match,\n        if_generation_not_match,\n        if_metageneration_match,\n        if_metageneration_not_match,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Perform a resumable upload.\n\n        Assumes ``chunk_size`` is not :data:`None` on the current blob.\n        The default value of ``chunk_size`` is 100 MB.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. After the upload is complete, the\n            server-computed checksum of the resulting object will be checked\n            and google.resumable_media.common.DataCorruption will be raised on\n            a mismatch. On a validation failure, the client will attempt to\n            delete the uploaded object automatically. Supported values\n            are \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will configure backoff and timeout options. Custom\n            predicates (customizable error codes) are not supported for media\n            operations such as this one.\n\n            This private method does not accept ConditionalRetryPolicy values\n            because the information necessary to evaluate the policy is instead\n            evaluated in blob._do_upload().\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: :class:`~requests.Response`\n        :returns: The \"200 OK\" response object returned after the final chunk\n                  is uploaded.\n        \"\"\"\n        upload, transport = self._initiate_resumable_upload(\n            client,\n            stream,\n            content_type,\n            size,\n            num_retries,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n            command=command,\n        )\n        while not upload.finished:\n            try:\n                response = upload.transmit_next_chunk(transport, timeout=timeout)\n            except resumable_media.DataCorruption:\n                # Attempt to delete the corrupted object.\n                self.delete()\n                raise\n        return response\n\n    def _do_upload(\n        self,\n        client,\n        stream,\n        content_type,\n        size,\n        num_retries,\n        predefined_acl,\n        if_generation_match,\n        if_generation_not_match,\n        if_metageneration_match,\n        if_metageneration_not_match,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=None,\n        command=None,\n    ):\n        \"\"\"Determine an upload strategy and then perform the upload.\n\n        If the size of the data to be uploaded exceeds 8 MB a resumable media\n        request will be used, otherwise the content and the metadata will be\n        uploaded in a single multipart upload request.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type stream: IO[bytes]\n        :param stream: A bytes IO object open for reading.\n\n        :type content_type: str\n        :param content_type: Type of content being uploaded (or :data:`None`).\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``stream``). If not provided, the upload will be concluded once\n            ``stream`` is exhausted (or :data:`None`).\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_generation_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :rtype: dict\n        :returns: The parsed JSON from the \"200 OK\" response. This will be the\n                  **only** response in the multipart case and it will be the\n                  **final** response in the resumable case.\n        \"\"\"\n\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": if_generation_match,\n                \"ifMetagenerationMatch\": if_metageneration_match,\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        if size is not None and size <= _MAX_MULTIPART_SIZE:\n            response = self._do_multipart_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n                command=command,\n            )\n        else:\n            response = self._do_resumable_upload(\n                client,\n                stream,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n                command=command,\n            )\n\n        return response.json()\n\n    def _prep_and_do_upload(\n        self,\n        file_obj,\n        rewind=False,\n        size=None,\n        content_type=None,\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n        command=None,\n    ):\n        \"\"\"Upload the contents of this blob from a file-like object.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If the size of the data to be uploaded exceeds 8 MB a resumable media\n        request will be used, otherwise the content and the metadata will be\n        uploaded in a single multipart upload request.\n\n        For more fine-grained over the upload process, check out\n        [`google-resumable-media`](https://googleapis.dev/python/google-resumable-media/latest/index.html).\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle opened in binary mode for reading.\n\n        :type rewind: bool\n        :param rewind:\n            If True, seek to the beginning of the file handle before writing\n            the file to Cloud Storage.\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``file_obj``). If not provided, the upload will be concluded once\n            ``file_obj`` is exhausted.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_generation_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for upload was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n\n        :raises: :class:`~google.cloud.exceptions.GoogleCloudError`\n                 if the upload response returns an error status.\n        \"\"\"\n        if num_retries is not None:\n            warnings.warn(_NUM_RETRIES_MESSAGE, DeprecationWarning, stacklevel=2)\n            # num_retries and retry are mutually exclusive. If num_retries is\n            # set and retry is exactly the default, then nullify retry for\n            # backwards compatibility.\n            if retry is DEFAULT_RETRY_IF_GENERATION_SPECIFIED:\n                retry = None\n\n        _maybe_rewind(file_obj, rewind=rewind)\n        predefined_acl = ACL.validate_predefined(predefined_acl)\n\n        try:\n            created_json = self._do_upload(\n                client,\n                file_obj,\n                content_type,\n                size,\n                num_retries,\n                predefined_acl,\n                if_generation_match,\n                if_generation_not_match,\n                if_metageneration_match,\n                if_metageneration_not_match,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n                command=command,\n            )\n            self._set_properties(created_json)\n        except resumable_media.InvalidResponse as exc:\n            _raise_from_invalid_response(exc)\n\n    def upload_from_file(\n        self,\n        file_obj,\n        rewind=False,\n        size=None,\n        content_type=None,\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Upload the contents of this blob from a file-like object.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If the size of the data to be uploaded exceeds 8 MB a resumable media\n        request will be used, otherwise the content and the metadata will be\n        uploaded in a single multipart upload request.\n\n        For more fine-grained over the upload process, check out\n        [`google-resumable-media`](https://googleapis.dev/python/google-resumable-media/latest/index.html).\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle opened in binary mode for reading.\n\n        :type rewind: bool\n        :param rewind:\n            If True, seek to the beginning of the file handle before writing\n            the file to Cloud Storage.\n\n        :type size: int\n        :param size:\n            The number of bytes to be uploaded (which will be read from\n            ``file_obj``). If not provided, the upload will be concluded once\n            ``file_obj`` is exhausted.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n\n        :raises: :class:`~google.cloud.exceptions.GoogleCloudError`\n                 if the upload response returns an error status.\n        \"\"\"\n        self._prep_and_do_upload(\n            file_obj,\n            rewind=rewind,\n            size=size,\n            content_type=content_type,\n            num_retries=num_retries,\n            client=client,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def _handle_filename_and_upload(self, filename, content_type=None, *args, **kwargs):\n        \"\"\"Upload this blob's contents from the content of a named file.\n\n        :type filename: str\n        :param filename: The path to the file.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        For *args and **kwargs, refer to the documentation for upload_from_filename() for more information.\n        \"\"\"\n\n        content_type = self._get_content_type(content_type, filename=filename)\n\n        with open(filename, \"rb\") as file_obj:\n            total_bytes = os.fstat(file_obj.fileno()).st_size\n            self._prep_and_do_upload(\n                file_obj,\n                content_type=content_type,\n                size=total_bytes,\n                *args,\n                **kwargs,\n            )\n\n    def upload_from_filename(\n        self,\n        filename,\n        content_type=None,\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Upload this blob's contents from the content of a named file.\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The value given by ``mimetypes.guess_type``\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-upload-encrypted-file#storage_upload_encrypted_file-python)\n        to upload a file with a\n        [`customer-supplied encryption key`](https://cloud.google.com/storage/docs/encryption#customer-supplied).\n\n        :type filename: str\n        :param filename: The path to the file.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n        \"\"\"\n\n        self._handle_filename_and_upload(\n            filename,\n            content_type=content_type,\n            num_retries=num_retries,\n            client=client,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def upload_from_string(\n        self,\n        data,\n        content_type=\"text/plain\",\n        num_retries=None,\n        client=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Upload contents of this blob from the provided string.\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type data: bytes or str\n        :param data:\n            The data to store in this blob.  If the value is text, it will be\n            encoded as UTF-8.\n\n        :type content_type: str\n        :param content_type:\n            (Optional) Type of content being uploaded. Defaults to\n            ``'text/plain'``.\n\n        :type num_retries: int\n        :param num_retries:\n            Number of upload retries. By default, only uploads with\n            if_generation_match set will be retried, as uploads without the\n            argument are not guaranteed to be idempotent. Setting num_retries\n            will override this default behavior and guarantee retries even when\n            if_generation_match is not set.  (Deprecated: This argument\n            will be removed in a future release.)\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. If the upload is completed in a single\n            request, the checksum will be entirely precomputed and the remote\n            server will handle verification and error handling. If the upload\n            is too large and must be transmitted in multiple requests, the\n            checksum will be incrementally computed and the client will handle\n            verification and error handling, raising\n            google.resumable_media.common.DataCorruption on a mismatch and\n            attempting to delete the corrupted file. Supported values are\n            \"md5\", \"crc32c\" and None. The default is None.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n        \"\"\"\n        data = _to_bytes(data, encoding=\"utf-8\")\n        string_buffer = BytesIO(data)\n        self.upload_from_file(\n            file_obj=string_buffer,\n            size=len(data),\n            content_type=content_type,\n            num_retries=num_retries,\n            client=client,\n            predefined_acl=predefined_acl,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def create_resumable_upload_session(\n        self,\n        content_type=None,\n        size=None,\n        origin=None,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=None,\n        predefined_acl=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Create a resumable upload session.\n\n        Resumable upload sessions allow you to start an upload session from\n        one client and complete the session in another. This method is called\n        by the initiator to set the metadata and limits. The initiator then\n        passes the session URL to the client that will upload the binary data.\n        The client performs a PUT request on the session URL to complete the\n        upload. This process allows untrusted clients to upload to an\n        access-controlled bucket.\n\n        For more details, see the\n        documentation on [`signed URLs`](https://cloud.google.com/storage/docs/access-control/signed-urls#signing-resumable).\n\n        The content type of the upload will be determined in order\n        of precedence:\n\n        - The value passed in to this method (if not :data:`None`)\n        - The value stored on the current blob\n        - The default value ('application/octet-stream')\n\n        .. note::\n           The effect of uploading to an existing blob depends on the\n           \"versioning\" and \"lifecycle\" policies defined on the blob's\n           bucket.  In the absence of those policies, upload will\n           overwrite any existing contents.\n\n           See the [`object versioning`](https://cloud.google.com/storage/docs/object-versioning)\n           and [`lifecycle`](https://cloud.google.com/storage/docs/lifecycle)\n           API documents for details.\n\n        If :attr:`encryption_key` is set, the blob will be encrypted with\n        a [`customer-supplied`](https://cloud.google.com/storage/docs/encryption#customer-supplied)\n        encryption key.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type size: int\n        :param size:\n            (Optional) The maximum number of bytes that can be uploaded using\n            this session. If the size is not known when creating the session,\n            this should be left blank.\n\n        :type content_type: str\n        :param content_type: (Optional) Type of content being uploaded.\n\n        :type origin: str\n        :param origin:\n            (Optional) If set, the upload can only be completed by a user-agent\n            that uploads from the given origin. This can be useful when passing\n            the session to a web client.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify\n            the integrity of the object. After the upload is complete, the\n            server-computed checksum of the resulting object will be checked\n            and google.resumable_media.common.DataCorruption will be raised on\n            a mismatch. On a validation failure, the client will attempt to\n            delete the uploaded object automatically. Supported values\n            are \"md5\", \"crc32c\" and None. The default is None.\n\n        :type predefined_acl: str\n        :param predefined_acl: (Optional) Predefined access control list\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. Other configuration changes for Retry objects\n            such as delays and deadlines are respected.\n\n        :rtype: str\n        :returns: The resumable upload session URL. The upload can be\n                  completed by making an HTTP PUT request with the\n                  file's contents.\n\n        :raises: :class:`google.cloud.exceptions.GoogleCloudError`\n                 if the session creation response returns an error status.\n        \"\"\"\n\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": if_generation_match,\n                \"ifMetagenerationMatch\": if_metageneration_match,\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        extra_headers = {}\n        if origin is not None:\n            # This header is specifically for client-side uploads, it\n            # determines the origins allowed for CORS.\n            extra_headers[\"Origin\"] = origin\n\n        try:\n            fake_stream = BytesIO(b\"\")\n            # Send a fake the chunk size which we **know** will be acceptable\n            # to the `ResumableUpload` constructor. The chunk size only\n            # matters when **sending** bytes to an upload.\n            upload, _ = self._initiate_resumable_upload(\n                client,\n                fake_stream,\n                content_type,\n                size,\n                None,\n                predefined_acl=predefined_acl,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                extra_headers=extra_headers,\n                chunk_size=self._CHUNK_SIZE_MULTIPLE,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n            )\n\n            return upload.resumable_url\n        except resumable_media.InvalidResponse as exc:\n            _raise_from_invalid_response(exc)\n\n    def get_iam_policy(\n        self,\n        client=None,\n        requested_policy_version=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Retrieve the IAM policy for the object.\n\n        .. note::\n\n           Blob- / object-level IAM support does not yet exist and methods\n           currently call an internal ACL backend not providing any utility\n           beyond the blob's :attr:`acl` at this time. The API may be enhanced\n           in the future and is currently undocumented. Use :attr:`acl` for\n           managing object access control.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current object's bucket.\n\n        :type requested_policy_version: int or ``NoneType``\n        :param requested_policy_version:\n            (Optional) The version of IAM policies to request.  If a policy\n            with a condition is requested without setting this, the server will\n            return an error.  This must be set to a value of 3 to retrieve IAM\n            policies containing conditions. This is to prevent client code that\n            isn't aware of IAM conditions from interpreting and modifying\n            policies incorrectly.  The service might return a policy with\n            version lower than the one that was requested, based on the feature\n            syntax in the policy fetched.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``getIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        if requested_policy_version is not None:\n            query_params[\"optionsRequestedPolicyVersion\"] = requested_policy_version\n\n        info = client._get_resource(\n            f\"{self.path}/iam\",\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return Policy.from_api_repr(info)\n\n    def set_iam_policy(\n        self,\n        policy,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n    ):\n        \"\"\"Update the IAM policy for the bucket.\n\n        .. note::\n\n           Blob- / object-level IAM support does not yet exist and methods\n           currently call an internal ACL backend not providing any utility\n           beyond the blob's :attr:`acl` at this time. The API may be enhanced\n           in the future and is currently undocumented. Use :attr:`acl` for\n           managing object access control.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type policy: :class:`google.api_core.iam.Policy`\n        :param policy: policy instance used to update bucket's IAM policy.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``setIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam\"\n        resource = policy.to_api_repr()\n        resource[\"resourceId\"] = self.path\n        info = client._put_resource(\n            path,\n            resource,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return Policy.from_api_repr(info)\n\n    def test_iam_permissions(\n        self, permissions, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"API call:  test permissions\n\n        .. note::\n\n           Blob- / object-level IAM support does not yet exist and methods\n           currently call an internal ACL backend not providing any utility\n           beyond the blob's :attr:`acl` at this time. The API may be enhanced\n           in the future and is currently undocumented. Use :attr:`acl` for\n           managing object access control.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type permissions: list of string\n        :param permissions: the permissions to check\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: list of string\n        :returns: the permissions returned by the ``testIamPermissions`` API\n                  request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {\"permissions\": permissions}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam/testPermissions\"\n        resp = client._get_resource(\n            path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n        return resp.get(\"permissions\", [])\n\n    def make_public(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update blob's ACL, granting read access to anonymous users.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        self.acl.all().grant_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def make_private(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update blob's ACL, revoking read access for anonymous users.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        self.acl.all().revoke_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def compose(\n        self,\n        sources,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_metageneration_match=None,\n        if_source_generation_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Concatenate source blobs into this one.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/objects/compose)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-compose-file#storage_compose_file-python).\n\n        :type sources: list of :class:`Blob`\n        :param sources: Blobs whose contents will be composed into this blob.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) Makes the operation conditional on whether the\n            destination object's current generation matches the given value.\n            Setting to 0 makes the operation succeed only if there are no live\n            versions of the object.\n            Note: In a previous version, this argument worked identically to the\n            ``if_source_generation_match`` argument. For\n            backwards-compatibility reasons, if a list is passed in,\n            this argument will behave like ``if_source_generation_match``\n            and also issue a DeprecationWarning.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) Makes the operation conditional on whether the\n            destination object's current metageneration matches the given\n            value.\n\n            If a list of long is passed in, no match operation will be\n            performed.  (Deprecated: type(list of long) is supported for\n            backwards-compatability reasons only.)\n\n        :type if_source_generation_match: list of long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the current\n            generation of each source blob matches the corresponding generation.\n            The list must match ``sources`` item-to-item.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n        \"\"\"\n        sources_len = len(sources)\n        client = self._require_client(client)\n        query_params = {}\n\n        if isinstance(if_generation_match, list):\n            warnings.warn(\n                _COMPOSE_IF_GENERATION_LIST_DEPRECATED,\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n            if if_source_generation_match is not None:\n                raise ValueError(\n                    _COMPOSE_IF_GENERATION_LIST_AND_IF_SOURCE_GENERATION_ERROR\n                )\n\n            if_source_generation_match = if_generation_match\n            if_generation_match = None\n\n        if isinstance(if_metageneration_match, list):\n            warnings.warn(\n                _COMPOSE_IF_METAGENERATION_LIST_DEPRECATED,\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n            if_metageneration_match = None\n\n        if if_source_generation_match is None:\n            if_source_generation_match = [None] * sources_len\n        if len(if_source_generation_match) != sources_len:\n            raise ValueError(_COMPOSE_IF_SOURCE_GENERATION_MISMATCH_ERROR)\n\n        source_objects = []\n        for source, source_generation in zip(sources, if_source_generation_match):\n            source_object = {\"name\": source.name, \"generation\": source.generation}\n\n            preconditions = {}\n            if source_generation is not None:\n                preconditions[\"ifGenerationMatch\"] = source_generation\n\n            if preconditions:\n                source_object[\"objectPreconditions\"] = preconditions\n\n            source_objects.append(source_object)\n\n        request = {\n            \"sourceObjects\": source_objects,\n            \"destination\": self._properties.copy(),\n        }\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_metageneration_match=if_metageneration_match,\n        )\n\n        api_response = client._post_resource(\n            f\"{self.path}/compose\",\n            request,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n    def rewrite(\n        self,\n        source,\n        token=None,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Rewrite source blob into this one.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        .. note::\n\n           ``rewrite`` is not supported in a ``Batch`` context.\n\n        :type source: :class:`Blob`\n        :param source: blob whose contents will be rewritten into this blob.\n\n        :type token: str\n        :param token:\n            (Optional) Token returned from an earlier, not-completed call to\n            rewrite the same source blob.  If passed, result will include\n            updated status, total bytes written.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: tuple\n        :returns: ``(token, bytes_rewritten, total_bytes)``, where ``token``\n                  is a rewrite token (``None`` if the rewrite is complete),\n                  ``bytes_rewritten`` is the number of bytes rewritten so far,\n                  and ``total_bytes`` is the total number of bytes to be\n                  rewritten.\n        \"\"\"\n        client = self._require_client(client)\n        headers = _get_encryption_headers(self._encryption_key)\n        headers.update(_get_encryption_headers(source._encryption_key, source=True))\n\n        query_params = self._query_params\n        if \"generation\" in query_params:\n            del query_params[\"generation\"]\n\n        if token:\n            query_params[\"rewriteToken\"] = token\n\n        if source.generation:\n            query_params[\"sourceGeneration\"] = source.generation\n\n        # When a Customer Managed Encryption Key is used to encrypt Cloud Storage object\n        # at rest, object resource metadata will store the version of the Key Management\n        # Service cryptographic material. If a Blob instance with KMS Key metadata set is\n        # used to rewrite the object, then the existing kmsKeyName version\n        # value can't be used in the rewrite request and the client instead ignores it.\n        if (\n            self.kms_key_name is not None\n            and \"cryptoKeyVersions\" not in self.kms_key_name\n        ):\n            query_params[\"destinationKmsKeyName\"] = self.kms_key_name\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n        )\n\n        path = f\"{source.path}/rewriteTo{self.path}\"\n        api_response = client._post_resource(\n            path,\n            self._properties,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        rewritten = int(api_response[\"totalBytesRewritten\"])\n        size = int(api_response[\"objectSize\"])\n\n        # The resource key is set if and only if the API response is\n        # completely done. Additionally, there is no rewrite token to return\n        # in this case.\n        if api_response[\"done\"]:\n            self._set_properties(api_response[\"resource\"])\n            return None, rewritten, size\n\n        return api_response[\"rewriteToken\"], rewritten, size\n\n    def update_storage_class(\n        self,\n        new_class,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Update blob's storage class via a rewrite-in-place. This helper will\n        wait for the rewrite to complete before returning, so it may take some\n        time for large files.\n\n        See\n        https://cloud.google.com/storage/docs/per-object-storage-class\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type new_class: str\n        :param new_class:\n            new storage class for the object.   One of:\n            :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n            or\n            :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n        \"\"\"\n        # Update current blob's storage class prior to rewrite\n        self._patch_property(\"storageClass\", new_class)\n\n        # Execute consecutive rewrite operations until operation is done\n        token, _, _ = self.rewrite(\n            self,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n        while token is not None:\n            token, _, _ = self.rewrite(\n                self,\n                token=token,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                if_source_generation_match=if_source_generation_match,\n                if_source_generation_not_match=if_source_generation_not_match,\n                if_source_metageneration_match=if_source_metageneration_match,\n                if_source_metageneration_not_match=if_source_metageneration_not_match,\n                timeout=timeout,\n                retry=retry,\n            )\n\n    def open(\n        self,\n        mode=\"r\",\n        chunk_size=None,\n        ignore_flush=None,\n        encoding=None,\n        errors=None,\n        newline=None,\n        **kwargs,\n    ):\n        r\"\"\"Create a file handler for file-like I/O to or from this blob.\n\n        This method can be used as a context manager, just like Python's\n        built-in 'open()' function.\n\n        While reading, as with other read methods, if blob.generation is not set\n        the most recent blob generation will be used. Because the file-like IO\n        reader downloads progressively in chunks, this could result in data from\n        multiple versions being mixed together. If this is a concern, use\n        either bucket.get_blob(), or blob.reload(), which will download the\n        latest generation number and set it; or, if the generation is known, set\n        it manually, for instance with bucket.blob(generation=123456).\n\n        Checksumming (hashing) to verify data integrity is disabled for reads\n        using this feature because reads are implemented using request ranges,\n        which do not provide checksums to validate. See\n        https://cloud.google.com/storage/docs/hashes-etags for details.\n\n        See a [code sample](https://github.com/googleapis/python-storage/blob/main/samples/snippets/storage_fileio_write_read.py).\n\n        Keyword arguments to pass to the underlying API calls.\n        For both uploads and downloads, the following arguments are\n        supported:\n\n        - ``if_generation_match``\n        - ``if_generation_not_match``\n        - ``if_metageneration_match``\n        - ``if_metageneration_not_match``\n        - ``timeout``\n        - ``retry``\n\n        For downloads only, the following additional arguments are supported:\n\n        - ``raw_download``\n\n        For uploads only, the following additional arguments are supported:\n\n        - ``content_type``\n        - ``num_retries``\n        - ``predefined_acl``\n        - ``checksum``\n\n        .. note::\n\n           ``num_retries`` is supported for backwards-compatibility\n           reasons only; please use ``retry`` with a Retry object or\n           ConditionalRetryPolicy instead.\n\n        :type mode: str\n        :param mode:\n            (Optional) A mode string, as per standard Python `open()` semantics.The first\n            character must be 'r', to open the blob for reading, or 'w' to open\n            it for writing. The second character, if present, must be 't' for\n            (unicode) text mode, or 'b' for bytes mode. If the second character\n            is omitted, text mode is the default.\n\n        :type chunk_size: long\n        :param chunk_size:\n            (Optional) For reads, the minimum number of bytes to read at a time.\n            If fewer bytes than the chunk_size are requested, the remainder is\n            buffered. For writes, the maximum number of bytes to buffer before\n            sending data to the server, and the size of each request when data\n            is sent. Writes are implemented as a \"resumable upload\", so\n            chunk_size for writes must be exactly a multiple of 256KiB as with\n            other resumable uploads. The default is 40 MiB.\n\n        :type ignore_flush: bool\n        :param ignore_flush:\n            (Optional) For non text-mode writes, makes flush() do nothing\n            instead of raising an error. flush() without closing is not\n            supported by the remote service and therefore calling it normally\n            results in io.UnsupportedOperation. However, that behavior is\n            incompatible with some consumers and wrappers of file objects in\n            Python, such as zipfile.ZipFile or io.TextIOWrapper. Setting\n            ignore_flush will cause flush() to successfully do nothing, for\n            compatibility with those contexts. The correct way to actually flush\n            data to the remote server is to close() (using a context manager,\n            such as in the example, will cause this to happen automatically).\n\n        :type encoding: str\n        :param encoding:\n            (Optional) For text mode only, the name of the encoding that the stream will\n            be decoded or encoded with. If omitted, it defaults to\n            locale.getpreferredencoding(False).\n\n        :type errors: str\n        :param errors:\n            (Optional) For text mode only, an optional string that specifies how encoding\n            and decoding errors are to be handled. Pass 'strict' to raise a\n            ValueError exception if there is an encoding error (the default of\n            None has the same effect), or pass 'ignore' to ignore errors. (Note\n            that ignoring encoding errors can lead to data loss.) Other more\n            rarely-used options are also available; see the Python 'io' module\n            documentation for 'io.TextIOWrapper' for a complete list.\n\n        :type newline: str\n        :param newline:\n            (Optional) For text mode only, controls how line endings are handled. It can\n            be None, '', '\\n', '\\r', and '\\r\\n'. If None, reads use \"universal\n            newline mode\" and writes use the system default. See the Python\n            'io' module documentation for 'io.TextIOWrapper' for details.\n\n        :returns: A 'BlobReader' or 'BlobWriter' from\n            'google.cloud.storage.fileio', or an 'io.TextIOWrapper' around one\n            of those classes, depending on the 'mode' argument.\n        \"\"\"\n        if mode == \"rb\":\n            if encoding or errors or newline:\n                raise ValueError(\n                    \"encoding, errors and newline arguments are for text mode only\"\n                )\n            if ignore_flush:\n                raise ValueError(\n                    \"ignore_flush argument is for non-text write mode only\"\n                )\n            return BlobReader(self, chunk_size=chunk_size, **kwargs)\n        elif mode == \"wb\":\n            if encoding or errors or newline:\n                raise ValueError(\n                    \"encoding, errors and newline arguments are for text mode only\"\n                )\n            return BlobWriter(\n                self, chunk_size=chunk_size, ignore_flush=ignore_flush, **kwargs\n            )\n        elif mode in (\"r\", \"rt\"):\n            if ignore_flush:\n                raise ValueError(\n                    \"ignore_flush argument is for non-text write mode only\"\n                )\n            return TextIOWrapper(\n                BlobReader(self, chunk_size=chunk_size, **kwargs),\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n        elif mode in (\"w\", \"wt\"):\n            if ignore_flush is False:\n                raise ValueError(\n                    \"ignore_flush is required for text mode writing and \"\n                    \"cannot be set to False\"\n                )\n            return TextIOWrapper(\n                BlobWriter(self, chunk_size=chunk_size, ignore_flush=True, **kwargs),\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n        else:\n            raise NotImplementedError(\n                \"Supported modes strings are 'r', 'rb', 'rt', 'w', 'wb', and 'wt' only.\"\n            )\n\n    cache_control = _scalar_property(\"cacheControl\")\n    \"\"\"HTTP 'Cache-Control' header for this object.\n\n    See [`RFC 7234`](https://tools.ietf.org/html/rfc7234#section-5.2)\n    and [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n\n    \"\"\"\n\n    content_disposition = _scalar_property(\"contentDisposition\")\n    \"\"\"HTTP 'Content-Disposition' header for this object.\n\n    See [`RFC 6266`](https://tools.ietf.org/html/rfc7234#section-5.2) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    content_encoding = _scalar_property(\"contentEncoding\")\n    \"\"\"HTTP 'Content-Encoding' header for this object.\n\n    See [`RFC 7231`](https://tools.ietf.org/html/rfc7231#section-3.1.2.2) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    content_language = _scalar_property(\"contentLanguage\")\n    \"\"\"HTTP 'Content-Language' header for this object.\n\n    See [`BCP47`](https://tools.ietf.org/html/bcp47) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    content_type = _scalar_property(_CONTENT_TYPE_FIELD)\n    \"\"\"HTTP 'Content-Type' header for this object.\n\n    See [`RFC 2616`](https://tools.ietf.org/html/rfc2616#section-14.17) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    crc32c = _scalar_property(\"crc32c\")\n    \"\"\"CRC32C checksum for this object.\n\n    This returns the blob's CRC32C checksum. To retrieve the value, first use a\n    reload method of the Blob class which loads the blob's properties from the server.\n\n    See [`RFC 4960`](https://tools.ietf.org/html/rfc4960#appendix-B) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If not set before upload, the server will compute the hash.\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    def _prep_and_do_download(\n        self,\n        file_obj,\n        client=None,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n        command=None,\n    ):\n        \"\"\"Download the contents of a blob object into a file-like object.\n\n        See https://cloud.google.com/storage/docs/downloading-objects\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client:\n            (Optional) The client to use. If not passed, falls back to the\n            ``client`` stored on the blob's bucket.\n\n        :type start: int\n        :param start: (Optional) The first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: (Optional) The last byte in a range to be downloaded.\n\n        :type raw_download: bool\n        :param raw_download:\n            (Optional) If true, download the object without any expansion.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type checksum: str\n        :param checksum:\n            (Optional) The type of checksum to compute to verify the integrity\n            of the object. The response headers must contain a checksum of the\n            requested type. If the headers lack an appropriate checksum (for\n            instance in the case of transcoded or ranged downloads where the\n            remote service does not know the correct checksum, including\n            downloads where chunk_size is set) an INFO-level log will be\n            emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n            is \"md5\".\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable\n            retries. A google.api_core.retry.Retry value will enable retries,\n            and the object will define retriable response codes and errors and\n            configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n            Retry object and activates it only if certain conditions are met.\n            This class exists to provide safe defaults for RPC calls that are\n            not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a\n            condition such as if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package\n            (google.cloud.storage.retry) for information on retry types and how\n            to configure them.\n\n            Media operations (downloads and uploads) do not support non-default\n            predicates in a Retry object. The default will always be used. Other\n            configuration changes for Retry objects such as delays and deadlines\n            are respected.\n\n        :type command: str\n        :param command:\n            (Optional) Information about which interface for download was used,\n            to be included in the X-Goog-API-Client header. Please leave as None\n            unless otherwise directed.\n        \"\"\"\n        # Handle ConditionalRetryPolicy.\n        if isinstance(retry, ConditionalRetryPolicy):\n            # Conditional retries are designed for non-media calls, which change\n            # arguments into query_params dictionaries. Media operations work\n            # differently, so here we make a \"fake\" query_params to feed to the\n            # ConditionalRetryPolicy.\n            query_params = {\n                \"ifGenerationMatch\": if_generation_match,\n                \"ifMetagenerationMatch\": if_metageneration_match,\n            }\n            retry = retry.get_retry_policy_if_conditions_met(query_params=query_params)\n\n        client = self._require_client(client)\n\n        download_url = self._get_download_url(\n            client,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        headers = _get_encryption_headers(self._encryption_key)\n        headers[\"accept-encoding\"] = \"gzip\"\n        _add_etag_match_headers(\n            headers,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n        )\n        # Add any client attached custom headers to be sent with the request.\n        headers = {\n            **_get_default_headers(client._connection.user_agent, command=command),\n            **headers,\n            **client._extra_headers,\n        }\n\n        transport = client._http\n\n        try:\n            self._do_download(\n                transport,\n                file_obj,\n                download_url,\n                headers,\n                start,\n                end,\n                raw_download,\n                timeout=timeout,\n                checksum=checksum,\n                retry=retry,\n            )\n        except resumable_media.InvalidResponse as exc:\n            _raise_from_invalid_response(exc)\n\n    @property\n    def component_count(self):\n        \"\"\"Number of underlying components that make up this object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The component count (in case of a composed object) or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server.  This property will not be set on objects\n                  not created via ``compose``.\n        \"\"\"\n        component_count = self._properties.get(\"componentCount\")\n        if component_count is not None:\n            return int(component_count)\n\n    @property\n    def etag(self):\n        \"\"\"Retrieve the ETag for the object.\n\n        See [`RFC 2616 (etags)`](https://tools.ietf.org/html/rfc2616#section-3.11) and\n        [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n        :rtype: str or ``NoneType``\n        :returns: The blob etag or ``None`` if the blob's resource has not\n                  been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"etag\")\n\n    event_based_hold = _scalar_property(\"eventBasedHold\")\n    \"\"\"Is an event-based hold active on the object?\n\n    See [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If the property is not set locally, returns :data:`None`.\n\n    :rtype: bool or ``NoneType``\n    \"\"\"\n\n    @property\n    def generation(self):\n        \"\"\"Retrieve the generation for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The generation of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        generation = self._properties.get(\"generation\")\n        if generation is not None:\n            return int(generation)\n\n    @property\n    def id(self):\n        \"\"\"Retrieve the ID for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        The ID consists of the bucket name, object name, and generation number.\n\n        :rtype: str or ``NoneType``\n        :returns: The ID of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"id\")\n\n    md5_hash = _scalar_property(\"md5Hash\")\n    \"\"\"MD5 hash for this object.\n\n    This returns the blob's MD5 hash. To retrieve the value, first use a\n    reload method of the Blob class which loads the blob's properties from the server.\n\n    See [`RFC 1321`](https://tools.ietf.org/html/rfc1321) and\n    [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If not set before upload, the server will compute the hash.\n\n    :rtype: str or ``NoneType``\n    \"\"\"\n\n    @property\n    def media_link(self):\n        \"\"\"Retrieve the media download URI for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: str or ``NoneType``\n        :returns: The media link for the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"mediaLink\")\n\n    @property\n    def metadata(self):\n        \"\"\"Retrieve arbitrary/application specific metadata for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :setter: Update arbitrary/application specific metadata for the\n                 object.\n        :getter: Retrieve arbitrary/application specific metadata for\n                 the object.\n\n        :rtype: dict or ``NoneType``\n        :returns: The metadata associated with the blob or ``None`` if the\n                  property is not set.\n        \"\"\"\n        return copy.deepcopy(self._properties.get(\"metadata\"))\n\n    @metadata.setter\n    def metadata(self, value):\n        \"\"\"Update arbitrary/application specific metadata for the object.\n\n        Values are stored to GCS as strings. To delete a key, set its value to\n        None and call blob.patch().\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :type value: dict\n        :param value: The blob metadata to set.\n        \"\"\"\n        if value is not None:\n            value = {k: str(v) if v is not None else None for k, v in value.items()}\n        self._patch_property(\"metadata\", value)\n\n    @property\n    def metageneration(self):\n        \"\"\"Retrieve the metageneration for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The metageneration of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        metageneration = self._properties.get(\"metageneration\")\n        if metageneration is not None:\n            return int(metageneration)\n\n    @property\n    def owner(self):\n        \"\"\"Retrieve info about the owner of the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: dict or ``NoneType``\n        :returns: Mapping of owner's role/ID, or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return copy.deepcopy(self._properties.get(\"owner\"))\n\n    @property\n    def retention_expiration_time(self):\n        \"\"\"Retrieve timestamp at which the object's retention period expires.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the property is not set locally.\n        \"\"\"\n        value = self._properties.get(\"retentionExpirationTime\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def self_link(self):\n        \"\"\"Retrieve the URI for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: str or ``NoneType``\n        :returns: The self link for the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"selfLink\")\n\n    @property\n    def size(self):\n        \"\"\"Size of the object, in bytes.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: int or ``NoneType``\n        :returns: The size of the blob or ``None`` if the blob's\n                  resource has not been loaded from the server.\n        \"\"\"\n        size = self._properties.get(\"size\")\n        if size is not None:\n            return int(size)\n\n    @property\n    def kms_key_name(self):\n        \"\"\"Resource name of Cloud KMS key used to encrypt the blob's contents.\n\n        :rtype: str or ``NoneType``\n        :returns:\n            The resource name or ``None`` if no Cloud KMS key was used,\n            or the blob's resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"kmsKeyName\")\n\n    @kms_key_name.setter\n    def kms_key_name(self, value):\n        \"\"\"Set KMS encryption key for object.\n\n        :type value: str or ``NoneType``\n        :param value: new KMS key name (None to clear any existing key).\n        \"\"\"\n        self._patch_property(\"kmsKeyName\", value)\n\n    storage_class = _scalar_property(\"storageClass\")\n    \"\"\"Retrieve the storage class for the object.\n\n    This can only be set at blob / object **creation** time. If you'd\n    like to change the storage class **after** the blob / object already\n    exists in a bucket, call :meth:`update_storage_class` (which uses\n    :meth:`rewrite`).\n\n    See https://cloud.google.com/storage/docs/storage-classes\n\n    :rtype: str or ``NoneType``\n    :returns:\n        If set, one of\n        :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`,\n        :attr:`~google.cloud.storage.constants.DURABLE_REDUCED_AVAILABILITY_STORAGE_CLASS`,\n        else ``None``.\n    \"\"\"\n\n    temporary_hold = _scalar_property(\"temporaryHold\")\n    \"\"\"Is a temporary hold active on the object?\n\n    See [`API reference docs`](https://cloud.google.com/storage/docs/json_api/v1/objects).\n\n    If the property is not set locally, returns :data:`None`.\n\n    :rtype: bool or ``NoneType``\n    \"\"\"\n\n    @property\n    def time_deleted(self):\n        \"\"\"Retrieve the timestamp at which the object was deleted.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`). If the blob has\n                  not been deleted, this will never be set.\n        \"\"\"\n        value = self._properties.get(\"timeDeleted\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def time_created(self):\n        \"\"\"Retrieve the timestamp at which the object was created.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self._properties.get(\"timeCreated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def updated(self):\n        \"\"\"Retrieve the timestamp at which the object was updated.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self._properties.get(\"updated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def custom_time(self):\n        \"\"\"Retrieve the custom time for the object.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self._properties.get(\"customTime\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @custom_time.setter\n    def custom_time(self, value):\n        \"\"\"Set the custom time for the object.\n\n        Once set on the server side object, this value can't be unset, but may\n        only changed to a custom datetime in the future.\n\n        If :attr:`custom_time` must be unset, either perform a rewrite\n        operation or upload the data again.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/objects\n\n        :type value: :class:`datetime.datetime`\n        :param value: new value\n        \"\"\"\n        if value is not None:\n            value = _datetime_to_rfc3339(value)\n\n        self._patch_property(\"customTime\", value)\n\n    @property\n    def retention(self):\n        \"\"\"Retrieve the retention configuration for this object.\n\n        :rtype: :class:`Retention`\n        :returns: an instance for managing the object's retention configuration.\n        \"\"\"\n        info = self._properties.get(\"retention\", {})\n        return Retention.from_api_repr(info, self)\n\n    @property\n    def soft_delete_time(self):\n        \"\"\"If this object has been soft-deleted, returns the time at which it became soft-deleted.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns:\n            (readonly) The time that the object became soft-deleted.\n             Note this property is only set for soft-deleted objects.\n        \"\"\"\n        soft_delete_time = self._properties.get(\"softDeleteTime\")\n        if soft_delete_time is not None:\n            return _rfc3339_nanos_to_datetime(soft_delete_time)\n\n    @property\n    def hard_delete_time(self):\n        \"\"\"If this object has been soft-deleted, returns the time at which it will be permanently deleted.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns:\n            (readonly) The time that the object will be permanently deleted.\n            Note this property is only set for soft-deleted objects.\n        \"\"\"\n        hard_delete_time = self._properties.get(\"hardDeleteTime\")\n        if hard_delete_time is not None:\n            return _rfc3339_nanos_to_datetime(hard_delete_time)\n\n\ndef _get_host_name(connection):\n    \"\"\"Returns the host name from the given connection.\n\n    :type connection: :class:`~google.cloud.storage._http.Connection`\n    :param connection: The connection object.\n\n    :rtype: str\n    :returns: The host name.\n    \"\"\"\n    # TODO: After google-cloud-core 1.6.0 is stable and we upgrade it\n    # to 1.6.0 in setup.py, we no longer need to check the attribute\n    # existence. We can simply return connection.get_api_base_url_for_mtls().\n    return (\n        connection.API_BASE_URL\n        if not hasattr(connection, \"get_api_base_url_for_mtls\")\n        else connection.get_api_base_url_for_mtls()\n    )\n\n\ndef _get_encryption_headers(key, source=False):\n    \"\"\"Builds customer encryption key headers\n\n    :type key: bytes\n    :param key: 32 byte key to build request key and hash.\n\n    :type source: bool\n    :param source: If true, return headers for the \"source\" blob; otherwise,\n                   return headers for the \"destination\" blob.\n\n    :rtype: dict\n    :returns: dict of HTTP headers being sent in request.\n    \"\"\"\n    if key is None:\n        return {}\n\n    key = _to_bytes(key)\n    key_hash = hashlib.sha256(key).digest()\n    key_hash = base64.b64encode(key_hash)\n    key = base64.b64encode(key)\n\n    if source:\n        prefix = \"X-Goog-Copy-Source-Encryption-\"\n    else:\n        prefix = \"X-Goog-Encryption-\"\n\n    return {\n        prefix + \"Algorithm\": \"AES256\",\n        prefix + \"Key\": _bytes_to_unicode(key),\n        prefix + \"Key-Sha256\": _bytes_to_unicode(key_hash),\n    }\n\n\ndef _quote(value, safe=b\"~\"):\n    \"\"\"URL-quote a string.\n\n    If the value is unicode, this method first UTF-8 encodes it as bytes and\n    then quotes the bytes. (In Python 3, ``urllib.parse.quote`` does this\n    encoding automatically, but in Python 2, non-ASCII characters cannot be\n    quoted.)\n\n    :type value: str or bytes\n    :param value: The value to be URL-quoted.\n\n    :type safe: bytes\n    :param safe: Bytes *not* to be quoted.  By default, includes only ``b'~'``.\n\n    :rtype: str\n    :returns: The encoded value (bytes in Python 2, unicode in Python 3).\n    \"\"\"\n    value = _to_bytes(value, encoding=\"utf-8\")\n    return quote(value, safe=safe)\n\n\ndef _maybe_rewind(stream, rewind=False):\n    \"\"\"Rewind the stream if desired.\n\n    :type stream: IO[bytes]\n    :param stream: A bytes IO object open for reading.\n\n    :type rewind: bool\n    :param rewind: Indicates if we should seek to the beginning of the stream.\n    \"\"\"\n    if rewind:\n        stream.seek(0, os.SEEK_SET)\n\n\ndef _raise_from_invalid_response(error):\n    \"\"\"Re-wrap and raise an ``InvalidResponse`` exception.\n\n    :type error: :exc:`google.resumable_media.InvalidResponse`\n    :param error: A caught exception from the ``google-resumable-media``\n                  library.\n\n    :raises: :class:`~google.cloud.exceptions.GoogleCloudError` corresponding\n             to the failed status code\n    \"\"\"\n    response = error.response\n\n    # The 'response.text' gives the actual reason of error, where 'error' gives\n    # the message of expected status code.\n    if response.text:\n        error_message = response.text + \": \" + str(error)\n    else:\n        error_message = str(error)\n\n    message = f\"{response.request.method} {response.request.url}: {error_message}\"\n\n    raise exceptions.from_http_status(response.status_code, message, response=response)\n\n\ndef _add_query_parameters(base_url, name_value_pairs):\n    \"\"\"Add one query parameter to a base URL.\n\n    :type base_url: string\n    :param base_url: Base URL (may already contain query parameters)\n\n    :type name_value_pairs: list of (string, string) tuples.\n    :param name_value_pairs: Names and values of the query parameters to add\n\n    :rtype: string\n    :returns: URL with additional query strings appended.\n    \"\"\"\n    if len(name_value_pairs) == 0:\n        return base_url\n\n    scheme, netloc, path, query, frag = urlsplit(base_url)\n    query = parse_qsl(query)\n    query.extend(name_value_pairs)\n    return urlunsplit((scheme, netloc, path, urlencode(query), frag))\n\n\nclass Retention(dict):\n    \"\"\"Map an object's retention configuration.\n\n    :type blob: :class:`Blob`\n    :params blob: blob for which this retention configuration applies to.\n\n    :type mode: str or ``NoneType``\n    :params mode:\n        (Optional) The mode of the retention configuration, which can be either Unlocked or Locked.\n        See: https://cloud.google.com/storage/docs/object-lock\n\n    :type retain_until_time: :class:`datetime.datetime` or ``NoneType``\n    :params retain_until_time:\n        (Optional) The earliest time that the object can be deleted or replaced, which is the\n        retention configuration set for this object.\n\n    :type retention_expiration_time: :class:`datetime.datetime` or ``NoneType``\n    :params retention_expiration_time:\n        (Optional) The earliest time that the object can be deleted, which depends on any\n        retention configuration set for the object and any retention policy set for the bucket\n        that contains the object. This value should normally only be set by the back-end API.\n    \"\"\"\n\n    def __init__(\n        self,\n        blob,\n        mode=None,\n        retain_until_time=None,\n        retention_expiration_time=None,\n    ):\n        data = {\"mode\": mode}\n        if retain_until_time is not None:\n            retain_until_time = _datetime_to_rfc3339(retain_until_time)\n        data[\"retainUntilTime\"] = retain_until_time\n\n        if retention_expiration_time is not None:\n            retention_expiration_time = _datetime_to_rfc3339(retention_expiration_time)\n        data[\"retentionExpirationTime\"] = retention_expiration_time\n\n        super(Retention, self).__init__(data)\n        self._blob = blob\n\n    @classmethod\n    def from_api_repr(cls, resource, blob):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type blob: :class:`Blob`\n        :params blob: Blob for which this retention configuration applies to.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`Retention`\n        :returns: Retention configuration created from resource.\n        \"\"\"\n        instance = cls(blob)\n        instance.update(resource)\n        return instance\n\n    @property\n    def blob(self):\n        \"\"\"Blob for which this retention configuration applies to.\n\n        :rtype: :class:`Blob`\n        :returns: the instance's blob.\n        \"\"\"\n        return self._blob\n\n    @property\n    def mode(self):\n        \"\"\"The mode of the retention configuration. Options are 'Unlocked' or 'Locked'.\n\n        :rtype: string\n        :returns: The mode of the retention configuration, which can be either set to 'Unlocked' or 'Locked'.\n        \"\"\"\n        return self.get(\"mode\")\n\n    @mode.setter\n    def mode(self, value):\n        self[\"mode\"] = value\n        self.blob._patch_property(\"retention\", self)\n\n    @property\n    def retain_until_time(self):\n        \"\"\"The earliest time that the object can be deleted or replaced, which is the\n        retention configuration set for this object.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the blob's resource has not been loaded from\n                  the server (see :meth:`reload`).\n        \"\"\"\n        value = self.get(\"retainUntilTime\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @retain_until_time.setter\n    def retain_until_time(self, value):\n        \"\"\"Set the retain_until_time for the object retention configuration.\n\n        :type value: :class:`datetime.datetime`\n        :param value: The earliest time that the object can be deleted or replaced.\n        \"\"\"\n        if value is not None:\n            value = _datetime_to_rfc3339(value)\n        self[\"retainUntilTime\"] = value\n        self.blob._patch_property(\"retention\", self)\n\n    @property\n    def retention_expiration_time(self):\n        \"\"\"The earliest time that the object can be deleted, which depends on any\n        retention configuration set for the object and any retention policy set for\n        the bucket that contains the object.\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns:\n            (readonly) The earliest time that the object can be deleted.\n        \"\"\"\n        retention_expiration_time = self.get(\"retentionExpirationTime\")\n        if retention_expiration_time is not None:\n            return _rfc3339_nanos_to_datetime(retention_expiration_time)\n", "google/cloud/storage/bucket.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Create / interact with Google Cloud Storage buckets.\"\"\"\n\nimport base64\nimport copy\nimport datetime\nimport json\nfrom urllib.parse import urlsplit\nimport warnings\n\nfrom google.api_core import datetime_helpers\nfrom google.cloud._helpers import _datetime_to_rfc3339\nfrom google.cloud._helpers import _rfc3339_nanos_to_datetime\nfrom google.cloud.exceptions import NotFound\nfrom google.api_core.iam import Policy\nfrom google.cloud.storage import _signing\nfrom google.cloud.storage._helpers import _add_etag_match_headers\nfrom google.cloud.storage._helpers import _add_generation_match_parameters\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _PropertyMixin\nfrom google.cloud.storage._helpers import _UTC\nfrom google.cloud.storage._helpers import _scalar_property\nfrom google.cloud.storage._helpers import _validate_name\nfrom google.cloud.storage._signing import generate_signed_url_v2\nfrom google.cloud.storage._signing import generate_signed_url_v4\nfrom google.cloud.storage._helpers import _bucket_bound_hostname_url\nfrom google.cloud.storage._helpers import _virtual_hosted_style_base_url\nfrom google.cloud.storage.acl import BucketACL\nfrom google.cloud.storage.acl import DefaultObjectACL\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.constants import ARCHIVE_STORAGE_CLASS\nfrom google.cloud.storage.constants import COLDLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import DUAL_REGION_LOCATION_TYPE\nfrom google.cloud.storage.constants import (\n    DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS,\n)\nfrom google.cloud.storage.constants import MULTI_REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import MULTI_REGION_LOCATION_TYPE\nfrom google.cloud.storage.constants import NEARLINE_STORAGE_CLASS\nfrom google.cloud.storage.constants import PUBLIC_ACCESS_PREVENTION_INHERITED\nfrom google.cloud.storage.constants import REGIONAL_LEGACY_STORAGE_CLASS\nfrom google.cloud.storage.constants import REGION_LOCATION_TYPE\nfrom google.cloud.storage.constants import STANDARD_STORAGE_CLASS\nfrom google.cloud.storage.notification import BucketNotification\nfrom google.cloud.storage.notification import NONE_PAYLOAD_FORMAT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_GENERATION_SPECIFIED\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_ETAG_IN_JSON\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\n\n_UBLA_BPO_ENABLED_MESSAGE = (\n    \"Pass only one of 'uniform_bucket_level_access_enabled' / \"\n    \"'bucket_policy_only_enabled' to 'IAMConfiguration'.\"\n)\n_BPO_ENABLED_MESSAGE = (\n    \"'IAMConfiguration.bucket_policy_only_enabled' is deprecated.  \"\n    \"Instead, use 'IAMConfiguration.uniform_bucket_level_access_enabled'.\"\n)\n_UBLA_BPO_LOCK_TIME_MESSAGE = (\n    \"Pass only one of 'uniform_bucket_level_access_lock_time' / \"\n    \"'bucket_policy_only_lock_time' to 'IAMConfiguration'.\"\n)\n_BPO_LOCK_TIME_MESSAGE = (\n    \"'IAMConfiguration.bucket_policy_only_lock_time' is deprecated.  \"\n    \"Instead, use 'IAMConfiguration.uniform_bucket_level_access_lock_time'.\"\n)\n_LOCATION_SETTER_MESSAGE = (\n    \"Assignment to 'Bucket.location' is deprecated, as it is only \"\n    \"valid before the bucket is created. Instead, pass the location \"\n    \"to `Bucket.create`.\"\n)\n\n\ndef _blobs_page_start(iterator, page, response):\n    \"\"\"Grab prefixes after a :class:`~google.cloud.iterator.Page` started.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that is currently in use.\n\n    :type page: :class:`~google.cloud.api.core.page_iterator.Page`\n    :param page: The page that was just created.\n\n    :type response: dict\n    :param response: The JSON API response for a page of blobs.\n    \"\"\"\n    page.prefixes = tuple(response.get(\"prefixes\", ()))\n    iterator.prefixes.update(page.prefixes)\n\n\ndef _item_to_blob(iterator, item):\n    \"\"\"Convert a JSON blob to the native object.\n\n    .. note::\n\n        This assumes that the ``bucket`` attribute has been\n        added to the iterator after being created.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a blob.\n\n    :rtype: :class:`.Blob`\n    :returns: The next blob in the page.\n    \"\"\"\n    name = item.get(\"name\")\n    blob = Blob(name, bucket=iterator.bucket)\n    blob._set_properties(item)\n    return blob\n\n\ndef _item_to_notification(iterator, item):\n    \"\"\"Convert a JSON blob to the native object.\n\n    .. note::\n\n        This assumes that the ``bucket`` attribute has been\n        added to the iterator after being created.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a blob.\n\n    :rtype: :class:`.BucketNotification`\n    :returns: The next notification being iterated.\n    \"\"\"\n    return BucketNotification.from_api_repr(item, bucket=iterator.bucket)\n\n\nclass LifecycleRuleConditions(dict):\n    \"\"\"Map a single lifecycle rule for a bucket.\n\n    See: https://cloud.google.com/storage/docs/lifecycle\n\n    :type age: int\n    :param age: (Optional) Apply rule action to items whose age, in days,\n                exceeds this value.\n\n    :type created_before: datetime.date\n    :param created_before: (Optional) Apply rule action to items created\n                           before this date.\n\n    :type is_live: bool\n    :param is_live: (Optional) If true, apply rule action to non-versioned\n                    items, or to items with no newer versions. If false, apply\n                    rule action to versioned items with at least one newer\n                    version.\n\n    :type matches_prefix: list(str)\n    :param matches_prefix: (Optional) Apply rule action to items which\n                                  any prefix matches the beginning of the item name.\n\n    :type matches_storage_class: list(str), one or more of\n                                 :attr:`Bucket.STORAGE_CLASSES`.\n    :param matches_storage_class: (Optional) Apply rule action to items\n                                  whose storage class matches this value.\n\n    :type matches_suffix: list(str)\n    :param matches_suffix: (Optional) Apply rule action to items which\n                                  any suffix matches the end of the item name.\n\n    :type number_of_newer_versions: int\n    :param number_of_newer_versions: (Optional) Apply rule action to versioned\n                                     items having N newer versions.\n\n    :type days_since_custom_time: int\n    :param days_since_custom_time: (Optional) Apply rule action to items whose number of days\n                                   elapsed since the custom timestamp. This condition is relevant\n                                   only for versioned objects. The value of the field must be a non\n                                   negative integer. If it's zero, the object version will become\n                                   eligible for lifecycle action as soon as it becomes custom.\n\n    :type custom_time_before: :class:`datetime.date`\n    :param custom_time_before: (Optional)  Date object parsed from RFC3339 valid date, apply rule action\n                               to items whose custom time is before this date. This condition is relevant\n                               only for versioned objects, e.g., 2019-03-16.\n\n    :type days_since_noncurrent_time: int\n    :param days_since_noncurrent_time: (Optional) Apply rule action to items whose number of days\n                                        elapsed since the non current timestamp. This condition\n                                        is relevant only for versioned objects. The value of the field\n                                        must be a non negative integer. If it's zero, the object version\n                                        will become eligible for lifecycle action as soon as it becomes\n                                        non current.\n\n    :type noncurrent_time_before: :class:`datetime.date`\n    :param noncurrent_time_before: (Optional) Date object parsed from RFC3339 valid date, apply\n                                   rule action to items whose non current time is before this date.\n                                   This condition is relevant only for versioned objects, e.g, 2019-03-16.\n\n    :raises ValueError: if no arguments are passed.\n    \"\"\"\n\n    def __init__(\n        self,\n        age=None,\n        created_before=None,\n        is_live=None,\n        matches_storage_class=None,\n        number_of_newer_versions=None,\n        days_since_custom_time=None,\n        custom_time_before=None,\n        days_since_noncurrent_time=None,\n        noncurrent_time_before=None,\n        matches_prefix=None,\n        matches_suffix=None,\n        _factory=False,\n    ):\n        conditions = {}\n\n        if age is not None:\n            conditions[\"age\"] = age\n\n        if created_before is not None:\n            conditions[\"createdBefore\"] = created_before.isoformat()\n\n        if is_live is not None:\n            conditions[\"isLive\"] = is_live\n\n        if matches_storage_class is not None:\n            conditions[\"matchesStorageClass\"] = matches_storage_class\n\n        if number_of_newer_versions is not None:\n            conditions[\"numNewerVersions\"] = number_of_newer_versions\n\n        if days_since_custom_time is not None:\n            conditions[\"daysSinceCustomTime\"] = days_since_custom_time\n\n        if custom_time_before is not None:\n            conditions[\"customTimeBefore\"] = custom_time_before.isoformat()\n\n        if days_since_noncurrent_time is not None:\n            conditions[\"daysSinceNoncurrentTime\"] = days_since_noncurrent_time\n\n        if noncurrent_time_before is not None:\n            conditions[\"noncurrentTimeBefore\"] = noncurrent_time_before.isoformat()\n\n        if matches_prefix is not None:\n            conditions[\"matchesPrefix\"] = matches_prefix\n\n        if matches_suffix is not None:\n            conditions[\"matchesSuffix\"] = matches_suffix\n\n        if not _factory and not conditions:\n            raise ValueError(\"Supply at least one condition\")\n\n        super(LifecycleRuleConditions, self).__init__(conditions)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleConditions`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(_factory=True)\n        instance.update(resource)\n        return instance\n\n    @property\n    def age(self):\n        \"\"\"Conditon's age value.\"\"\"\n        return self.get(\"age\")\n\n    @property\n    def created_before(self):\n        \"\"\"Conditon's created_before value.\"\"\"\n        before = self.get(\"createdBefore\")\n        if before is not None:\n            return datetime_helpers.from_iso8601_date(before)\n\n    @property\n    def is_live(self):\n        \"\"\"Conditon's 'is_live' value.\"\"\"\n        return self.get(\"isLive\")\n\n    @property\n    def matches_prefix(self):\n        \"\"\"Conditon's 'matches_prefix' value.\"\"\"\n        return self.get(\"matchesPrefix\")\n\n    @property\n    def matches_storage_class(self):\n        \"\"\"Conditon's 'matches_storage_class' value.\"\"\"\n        return self.get(\"matchesStorageClass\")\n\n    @property\n    def matches_suffix(self):\n        \"\"\"Conditon's 'matches_suffix' value.\"\"\"\n        return self.get(\"matchesSuffix\")\n\n    @property\n    def number_of_newer_versions(self):\n        \"\"\"Conditon's 'number_of_newer_versions' value.\"\"\"\n        return self.get(\"numNewerVersions\")\n\n    @property\n    def days_since_custom_time(self):\n        \"\"\"Conditon's 'days_since_custom_time' value.\"\"\"\n        return self.get(\"daysSinceCustomTime\")\n\n    @property\n    def custom_time_before(self):\n        \"\"\"Conditon's 'custom_time_before' value.\"\"\"\n        before = self.get(\"customTimeBefore\")\n        if before is not None:\n            return datetime_helpers.from_iso8601_date(before)\n\n    @property\n    def days_since_noncurrent_time(self):\n        \"\"\"Conditon's 'days_since_noncurrent_time' value.\"\"\"\n        return self.get(\"daysSinceNoncurrentTime\")\n\n    @property\n    def noncurrent_time_before(self):\n        \"\"\"Conditon's 'noncurrent_time_before' value.\"\"\"\n        before = self.get(\"noncurrentTimeBefore\")\n        if before is not None:\n            return datetime_helpers.from_iso8601_date(before)\n\n\nclass LifecycleRuleDelete(dict):\n    \"\"\"Map a lifecycle rule deleting matching items.\n\n    :type kw: dict\n    :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n    \"\"\"\n\n    def __init__(self, **kw):\n        conditions = LifecycleRuleConditions(**kw)\n        rule = {\"action\": {\"type\": \"Delete\"}, \"condition\": dict(conditions)}\n        super().__init__(rule)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleDelete`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(_factory=True)\n        instance.update(resource)\n        return instance\n\n\nclass LifecycleRuleSetStorageClass(dict):\n    \"\"\"Map a lifecycle rule updating storage class of matching items.\n\n    :type storage_class: str, one of :attr:`Bucket.STORAGE_CLASSES`.\n    :param storage_class: new storage class to assign to matching items.\n\n    :type kw: dict\n    :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n    \"\"\"\n\n    def __init__(self, storage_class, **kw):\n        conditions = LifecycleRuleConditions(**kw)\n        rule = {\n            \"action\": {\"type\": \"SetStorageClass\", \"storageClass\": storage_class},\n            \"condition\": dict(conditions),\n        }\n        super().__init__(rule)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleSetStorageClass`\n        :returns: Instance created from resource.\n        \"\"\"\n        action = resource[\"action\"]\n        instance = cls(action[\"storageClass\"], _factory=True)\n        instance.update(resource)\n        return instance\n\n\nclass LifecycleRuleAbortIncompleteMultipartUpload(dict):\n    \"\"\"Map a rule aborting incomplete multipart uploads of matching items.\n\n    The \"age\" lifecycle condition is the only supported condition for this rule.\n\n    :type kw: dict\n    :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n    \"\"\"\n\n    def __init__(self, **kw):\n        conditions = LifecycleRuleConditions(**kw)\n        rule = {\n            \"action\": {\"type\": \"AbortIncompleteMultipartUpload\"},\n            \"condition\": dict(conditions),\n        }\n        super().__init__(rule)\n\n    @classmethod\n    def from_api_repr(cls, resource):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleAbortIncompleteMultipartUpload`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(_factory=True)\n        instance.update(resource)\n        return instance\n\n\n_default = object()\n\n\nclass IAMConfiguration(dict):\n    \"\"\"Map a bucket's IAM configuration.\n\n    :type bucket: :class:`Bucket`\n    :params bucket: Bucket for which this instance is the policy.\n\n    :type public_access_prevention: str\n    :params public_access_prevention:\n        (Optional) Whether the public access prevention policy is 'inherited' (default) or 'enforced'\n        See: https://cloud.google.com/storage/docs/public-access-prevention\n\n    :type uniform_bucket_level_access_enabled: bool\n    :params bucket_policy_only_enabled:\n        (Optional) Whether the IAM-only policy is enabled for the bucket.\n\n    :type uniform_bucket_level_access_locked_time: :class:`datetime.datetime`\n    :params uniform_bucket_level_locked_time:\n        (Optional) When the bucket's IAM-only policy was enabled.\n        This value should normally only be set by the back-end API.\n\n    :type bucket_policy_only_enabled: bool\n    :params bucket_policy_only_enabled:\n        Deprecated alias for :data:`uniform_bucket_level_access_enabled`.\n\n    :type bucket_policy_only_locked_time: :class:`datetime.datetime`\n    :params bucket_policy_only_locked_time:\n        Deprecated alias for :data:`uniform_bucket_level_access_locked_time`.\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket,\n        public_access_prevention=_default,\n        uniform_bucket_level_access_enabled=_default,\n        uniform_bucket_level_access_locked_time=_default,\n        bucket_policy_only_enabled=_default,\n        bucket_policy_only_locked_time=_default,\n    ):\n        if bucket_policy_only_enabled is not _default:\n            if uniform_bucket_level_access_enabled is not _default:\n                raise ValueError(_UBLA_BPO_ENABLED_MESSAGE)\n\n            warnings.warn(_BPO_ENABLED_MESSAGE, DeprecationWarning, stacklevel=2)\n            uniform_bucket_level_access_enabled = bucket_policy_only_enabled\n\n        if bucket_policy_only_locked_time is not _default:\n            if uniform_bucket_level_access_locked_time is not _default:\n                raise ValueError(_UBLA_BPO_LOCK_TIME_MESSAGE)\n\n            warnings.warn(_BPO_LOCK_TIME_MESSAGE, DeprecationWarning, stacklevel=2)\n            uniform_bucket_level_access_locked_time = bucket_policy_only_locked_time\n\n        if uniform_bucket_level_access_enabled is _default:\n            uniform_bucket_level_access_enabled = False\n\n        if public_access_prevention is _default:\n            public_access_prevention = PUBLIC_ACCESS_PREVENTION_INHERITED\n\n        data = {\n            \"uniformBucketLevelAccess\": {\n                \"enabled\": uniform_bucket_level_access_enabled\n            },\n            \"publicAccessPrevention\": public_access_prevention,\n        }\n        if uniform_bucket_level_access_locked_time is not _default:\n            data[\"uniformBucketLevelAccess\"][\"lockedTime\"] = _datetime_to_rfc3339(\n                uniform_bucket_level_access_locked_time\n            )\n        super(IAMConfiguration, self).__init__(data)\n        self._bucket = bucket\n\n    @classmethod\n    def from_api_repr(cls, resource, bucket):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type bucket: :class:`Bucket`\n        :params bucket: Bucket for which this instance is the policy.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`IAMConfiguration`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(bucket)\n        instance.update(resource)\n        return instance\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket for which this instance is the policy.\n\n        :rtype: :class:`Bucket`\n        :returns: the instance's bucket.\n        \"\"\"\n        return self._bucket\n\n    @property\n    def public_access_prevention(self):\n        \"\"\"Setting for public access prevention policy. Options are 'inherited' (default) or 'enforced'.\n\n            See: https://cloud.google.com/storage/docs/public-access-prevention\n\n        :rtype: string\n        :returns: the public access prevention status, either 'enforced' or 'inherited'.\n        \"\"\"\n        return self[\"publicAccessPrevention\"]\n\n    @public_access_prevention.setter\n    def public_access_prevention(self, value):\n        self[\"publicAccessPrevention\"] = value\n        self.bucket._patch_property(\"iamConfiguration\", self)\n\n    @property\n    def uniform_bucket_level_access_enabled(self):\n        \"\"\"If set, access checks only use bucket-level IAM policies or above.\n\n        :rtype: bool\n        :returns: whether the bucket is configured to allow only IAM.\n        \"\"\"\n        ubla = self.get(\"uniformBucketLevelAccess\", {})\n        return ubla.get(\"enabled\", False)\n\n    @uniform_bucket_level_access_enabled.setter\n    def uniform_bucket_level_access_enabled(self, value):\n        ubla = self.setdefault(\"uniformBucketLevelAccess\", {})\n        ubla[\"enabled\"] = bool(value)\n        self.bucket._patch_property(\"iamConfiguration\", self)\n\n    @property\n    def uniform_bucket_level_access_locked_time(self):\n        \"\"\"Deadline for changing :attr:`uniform_bucket_level_access_enabled` from true to false.\n\n        If the bucket's :attr:`uniform_bucket_level_access_enabled` is true, this property\n        is time time after which that setting becomes immutable.\n\n        If the bucket's :attr:`uniform_bucket_level_access_enabled` is false, this property\n        is ``None``.\n\n        :rtype: Union[:class:`datetime.datetime`, None]\n        :returns:  (readonly) Time after which :attr:`uniform_bucket_level_access_enabled` will\n                   be frozen as true.\n        \"\"\"\n        ubla = self.get(\"uniformBucketLevelAccess\", {})\n        stamp = ubla.get(\"lockedTime\")\n        if stamp is not None:\n            stamp = _rfc3339_nanos_to_datetime(stamp)\n        return stamp\n\n    @property\n    def bucket_policy_only_enabled(self):\n        \"\"\"Deprecated alias for :attr:`uniform_bucket_level_access_enabled`.\n\n        :rtype: bool\n        :returns: whether the bucket is configured to allow only IAM.\n        \"\"\"\n        return self.uniform_bucket_level_access_enabled\n\n    @bucket_policy_only_enabled.setter\n    def bucket_policy_only_enabled(self, value):\n        warnings.warn(_BPO_ENABLED_MESSAGE, DeprecationWarning, stacklevel=2)\n        self.uniform_bucket_level_access_enabled = value\n\n    @property\n    def bucket_policy_only_locked_time(self):\n        \"\"\"Deprecated alias for :attr:`uniform_bucket_level_access_locked_time`.\n\n        :rtype: Union[:class:`datetime.datetime`, None]\n        :returns:\n            (readonly) Time after which :attr:`bucket_policy_only_enabled` will\n            be frozen as true.\n        \"\"\"\n        return self.uniform_bucket_level_access_locked_time\n\n\nclass Bucket(_PropertyMixin):\n    \"\"\"A class representing a Bucket on Cloud Storage.\n\n    :type client: :class:`google.cloud.storage.client.Client`\n    :param client: A client which holds credentials and project configuration\n                   for the bucket (which requires a project).\n\n    :type name: str\n    :param name: The name of the bucket. Bucket names must start and end with a\n                 number or letter.\n\n    :type user_project: str\n    :param user_project: (Optional) the project ID to be billed for API\n                         requests made via this instance.\n    \"\"\"\n\n    _MAX_OBJECTS_FOR_ITERATION = 256\n    \"\"\"Maximum number of existing objects allowed in iteration.\n\n    This is used in Bucket.delete() and Bucket.make_public().\n    \"\"\"\n\n    STORAGE_CLASSES = (\n        STANDARD_STORAGE_CLASS,\n        NEARLINE_STORAGE_CLASS,\n        COLDLINE_STORAGE_CLASS,\n        ARCHIVE_STORAGE_CLASS,\n        MULTI_REGIONAL_LEGACY_STORAGE_CLASS,  # legacy\n        REGIONAL_LEGACY_STORAGE_CLASS,  # legacy\n        DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS,  # legacy\n    )\n    \"\"\"Allowed values for :attr:`storage_class`.\n\n    Default value is :attr:`STANDARD_STORAGE_CLASS`.\n\n    See\n    https://cloud.google.com/storage/docs/json_api/v1/buckets#storageClass\n    https://cloud.google.com/storage/docs/storage-classes\n    \"\"\"\n\n    _LOCATION_TYPES = (\n        MULTI_REGION_LOCATION_TYPE,\n        REGION_LOCATION_TYPE,\n        DUAL_REGION_LOCATION_TYPE,\n    )\n    \"\"\"Allowed values for :attr:`location_type`.\"\"\"\n\n    def __init__(self, client, name=None, user_project=None):\n        \"\"\"\n        property :attr:`name`\n            Get the bucket's name.\n        \"\"\"\n        name = _validate_name(name)\n        super(Bucket, self).__init__(name=name)\n        self._client = client\n        self._acl = BucketACL(self)\n        self._default_object_acl = DefaultObjectACL(self)\n        self._label_removals = set()\n        self._user_project = user_project\n\n    def __repr__(self):\n        return f\"<Bucket: {self.name}>\"\n\n    @property\n    def client(self):\n        \"\"\"The client bound to this bucket.\"\"\"\n        return self._client\n\n    def _set_properties(self, value):\n        \"\"\"Set the properties for the current object.\n\n        :type value: dict or :class:`google.cloud.storage.batch._FutureDict`\n        :param value: The properties to be set.\n        \"\"\"\n        self._label_removals.clear()\n        return super(Bucket, self)._set_properties(value)\n\n    @property\n    def rpo(self):\n        \"\"\"Get the RPO (Recovery Point Objective) of this bucket\n\n        See: https://cloud.google.com/storage/docs/managing-turbo-replication\n\n        \"ASYNC_TURBO\" or \"DEFAULT\"\n        :rtype: str\n        \"\"\"\n        return self._properties.get(\"rpo\")\n\n    @rpo.setter\n    def rpo(self, value):\n        \"\"\"\n        Set the RPO (Recovery Point Objective) of this bucket.\n\n        See: https://cloud.google.com/storage/docs/managing-turbo-replication\n\n        :type value: str\n        :param value: \"ASYNC_TURBO\" or \"DEFAULT\"\n        \"\"\"\n        self._patch_property(\"rpo\", value)\n\n    @property\n    def user_project(self):\n        \"\"\"Project ID to be billed for API requests made via this bucket.\n\n        If unset, API requests are billed to the bucket owner.\n\n        A user project is required for all operations on Requester Pays buckets.\n\n        See https://cloud.google.com/storage/docs/requester-pays#requirements for details.\n\n        :rtype: str\n        \"\"\"\n        return self._user_project\n\n    @classmethod\n    def from_string(cls, uri, client=None):\n        \"\"\"Get a constructor for bucket object by URI.\n\n        .. code-block:: python\n\n            from google.cloud import storage\n            from google.cloud.storage.bucket import Bucket\n            client = storage.Client()\n            bucket = Bucket.from_string(\"gs://bucket\", client=client)\n\n        :type uri: str\n        :param uri: The bucket uri pass to get bucket object.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  Application code should\n            *always* pass ``client``.\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket`\n        :returns: The bucket object created.\n        \"\"\"\n        scheme, netloc, path, query, frag = urlsplit(uri)\n\n        if scheme != \"gs\":\n            raise ValueError(\"URI scheme must be gs\")\n\n        return cls(client, name=netloc)\n\n    def blob(\n        self,\n        blob_name,\n        chunk_size=None,\n        encryption_key=None,\n        kms_key_name=None,\n        generation=None,\n    ):\n        \"\"\"Factory constructor for blob object.\n\n        .. note::\n          This will not make an HTTP request; it simply instantiates\n          a blob object owned by this bucket.\n\n        :type blob_name: str\n        :param blob_name: The name of the blob to be instantiated.\n\n        :type chunk_size: int\n        :param chunk_size: The size of a chunk of data whenever iterating\n                           (in bytes). This must be a multiple of 256 KB per\n                           the API specification.\n\n        :type encryption_key: bytes\n        :param encryption_key:\n            (Optional) 32 byte encryption key for customer-supplied encryption.\n\n        :type kms_key_name: str\n        :param kms_key_name:\n            (Optional) Resource name of KMS key used to encrypt blob's content.\n\n        :type generation: long\n        :param generation: (Optional) If present, selects a specific revision of\n                           this object.\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The blob object created.\n        \"\"\"\n        return Blob(\n            name=blob_name,\n            bucket=self,\n            chunk_size=chunk_size,\n            encryption_key=encryption_key,\n            kms_key_name=kms_key_name,\n            generation=generation,\n        )\n\n    def notification(\n        self,\n        topic_name=None,\n        topic_project=None,\n        custom_attributes=None,\n        event_types=None,\n        blob_name_prefix=None,\n        payload_format=NONE_PAYLOAD_FORMAT,\n        notification_id=None,\n    ):\n        \"\"\"Factory:  create a notification resource for the bucket.\n\n        See: :class:`.BucketNotification` for parameters.\n\n        :rtype: :class:`.BucketNotification`\n        \"\"\"\n        return BucketNotification(\n            self,\n            topic_name=topic_name,\n            topic_project=topic_project,\n            custom_attributes=custom_attributes,\n            event_types=event_types,\n            blob_name_prefix=blob_name_prefix,\n            payload_format=payload_format,\n            notification_id=notification_id,\n        )\n\n    def exists(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Determines whether or not this bucket exists.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match: (Optional) Make the operation conditional on whether the\n                              bucket's current ETag matches the given value.\n\n        :type if_etag_not_match: Union[str, Set[str]])\n        :param if_etag_not_match: (Optional) Make the operation conditional on whether the\n                                  bucket's current ETag does not match the given value.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        bucket's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            bucket's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: bool\n        :returns: True if the bucket exists in Cloud Storage.\n        \"\"\"\n        client = self._require_client(client)\n        # We only need the status code (200 or not) so we seek to\n        # minimize the returned payload.\n        query_params = {\"fields\": \"name\"}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        headers = {}\n        _add_etag_match_headers(\n            headers, if_etag_match=if_etag_match, if_etag_not_match=if_etag_not_match\n        )\n\n        try:\n            # We intentionally pass `_target_object=None` since fields=name\n            # would limit the local properties.\n            client._get_resource(\n                self.path,\n                query_params=query_params,\n                headers=headers,\n                timeout=timeout,\n                retry=retry,\n                _target_object=None,\n            )\n        except NotFound:\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            return False\n        return True\n\n    def create(\n        self,\n        client=None,\n        project=None,\n        location=None,\n        predefined_acl=None,\n        predefined_default_object_acl=None,\n        enable_object_retention=False,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Creates current bucket.\n\n        If the bucket already exists, will raise\n        :class:`google.cloud.exceptions.Conflict`.\n\n        This implements \"storage.buckets.insert\".\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type project: str\n        :param project: (Optional) The project under which the bucket is to\n                        be created. If not passed, uses the project set on\n                        the client.\n        :raises ValueError: if ``project`` is None and client's\n                            :attr:`project` is also None.\n\n        :type location: str\n        :param location: (Optional) The location of the bucket. If not passed,\n                         the default location, US, will be used. See\n                         https://cloud.google.com/storage/docs/bucket-locations\n\n        :type predefined_acl: str\n        :param predefined_acl:\n            (Optional) Name of predefined ACL to apply to bucket. See:\n            https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n\n        :type predefined_default_object_acl: str\n        :param predefined_default_object_acl:\n            (Optional) Name of predefined ACL to apply to bucket's objects. See:\n            https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n\n        :type enable_object_retention: bool\n        :param enable_object_retention:\n            (Optional) Whether object retention should be enabled on this bucket. See:\n            https://cloud.google.com/storage/docs/object-lock\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n\n        client = self._require_client(client)\n        client.create_bucket(\n            bucket_or_name=self,\n            project=project,\n            user_project=self.user_project,\n            location=location,\n            predefined_acl=predefined_acl,\n            predefined_default_object_acl=predefined_default_object_acl,\n            enable_object_retention=enable_object_retention,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def update(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Sends all properties in a PUT request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        super(Bucket, self).update(\n            client=client,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def reload(\n        self,\n        client=None,\n        projection=\"noAcl\",\n        timeout=_DEFAULT_TIMEOUT,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Reload properties from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type projection: str\n        :param projection: (Optional) If used, must be 'full' or 'noAcl'.\n                           Defaults to ``'noAcl'``. Specifies the set of\n                           properties to return.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match: (Optional) Make the operation conditional on whether the\n                              bucket's current ETag matches the given value.\n\n        :type if_etag_not_match: Union[str, Set[str]])\n        :param if_etag_not_match: (Optional) Make the operation conditional on whether the\n                                  bucket's current ETag does not match the given value.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        bucket's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            bucket's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        super(Bucket, self).reload(\n            client=client,\n            projection=projection,\n            timeout=timeout,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n    def patch(\n        self,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Sends all changed properties in a PATCH request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n        \"\"\"\n        # Special case: For buckets, it is possible that labels are being\n        # removed; this requires special handling.\n        if self._label_removals:\n            self._changes.add(\"labels\")\n            self._properties.setdefault(\"labels\", {})\n            for removed_label in self._label_removals:\n                self._properties[\"labels\"][removed_label] = None\n\n        # Call the superclass method.\n        super(Bucket, self).patch(\n            client=client,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    @property\n    def acl(self):\n        \"\"\"Create our ACL on demand.\"\"\"\n        return self._acl\n\n    @property\n    def default_object_acl(self):\n        \"\"\"Create our defaultObjectACL on demand.\"\"\"\n        return self._default_object_acl\n\n    @staticmethod\n    def path_helper(bucket_name):\n        \"\"\"Relative URL path for a bucket.\n\n        :type bucket_name: str\n        :param bucket_name: The bucket name in the path.\n\n        :rtype: str\n        :returns: The relative URL path for ``bucket_name``.\n        \"\"\"\n        return \"/b/\" + bucket_name\n\n    @property\n    def path(self):\n        \"\"\"The URL path to this bucket.\"\"\"\n        if not self.name:\n            raise ValueError(\"Cannot determine path without bucket name.\")\n\n        return self.path_helper(self.name)\n\n    def get_blob(\n        self,\n        blob_name,\n        client=None,\n        encryption_key=None,\n        generation=None,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        soft_deleted=None,\n        **kwargs,\n    ):\n        \"\"\"Get a blob object by name.\n\n        See a [code sample](https://cloud.google.com/storage/docs/samples/storage-get-metadata#storage_get_metadata-python)\n        on how to retrieve metadata of an object.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blob_name: str\n        :param blob_name: The name of the blob to retrieve.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type encryption_key: bytes\n        :param encryption_key:\n            (Optional) 32 byte encryption key for customer-supplied encryption.\n            See\n            https://cloud.google.com/storage/docs/encryption#customer-supplied.\n\n        :type generation: long\n        :param generation:\n            (Optional) If present, selects a specific revision of this object.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match:\n            (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]]\n        :param if_etag_not_match:\n            (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If True, looks for a soft-deleted object. Will only return\n            the object metadata if the object exists and is in a soft-deleted state.\n            Object ``generation`` is required if ``soft_deleted`` is set to True.\n            See: https://cloud.google.com/storage/docs/soft-delete\n\n        :param kwargs: Keyword arguments to pass to the\n                       :class:`~google.cloud.storage.blob.Blob` constructor.\n\n        :rtype: :class:`google.cloud.storage.blob.Blob` or None\n        :returns: The blob object if it exists, otherwise None.\n        \"\"\"\n        blob = Blob(\n            bucket=self,\n            name=blob_name,\n            encryption_key=encryption_key,\n            generation=generation,\n            **kwargs,\n        )\n        try:\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            blob.reload(\n                client=client,\n                timeout=timeout,\n                if_etag_match=if_etag_match,\n                if_etag_not_match=if_etag_not_match,\n                if_generation_match=if_generation_match,\n                if_generation_not_match=if_generation_not_match,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n                soft_deleted=soft_deleted,\n            )\n        except NotFound:\n            return None\n        else:\n            return blob\n\n    def list_blobs(\n        self,\n        max_results=None,\n        page_token=None,\n        prefix=None,\n        delimiter=None,\n        start_offset=None,\n        end_offset=None,\n        include_trailing_delimiter=None,\n        versions=None,\n        projection=\"noAcl\",\n        fields=None,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        match_glob=None,\n        include_folders_as_prefixes=None,\n        soft_deleted=None,\n        page_size=None,\n    ):\n        \"\"\"Return an iterator used to find blobs in the bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type max_results: int\n        :param max_results:\n            (Optional) The maximum number of blobs to return.\n\n        :type page_token: str\n        :param page_token:\n            (Optional) If present, return the next batch of blobs, using the\n            value, which must correspond to the ``nextPageToken`` value\n            returned in the previous response.  Deprecated: use the ``pages``\n            property of the returned iterator instead of manually passing the\n            token.\n\n        :type prefix: str\n        :param prefix: (Optional) Prefix used to filter blobs.\n\n        :type delimiter: str\n        :param delimiter: (Optional) Delimiter, used with ``prefix`` to\n                          emulate hierarchy.\n\n        :type start_offset: str\n        :param start_offset:\n            (Optional) Filter results to objects whose names are\n            lexicographically equal to or after ``startOffset``. If\n            ``endOffset`` is also set, the objects listed will have names\n            between ``startOffset`` (inclusive) and ``endOffset`` (exclusive).\n\n        :type end_offset: str\n        :param end_offset:\n            (Optional) Filter results to objects whose names are\n            lexicographically before ``endOffset``. If ``startOffset`` is also\n            set, the objects listed will have names between ``startOffset``\n            (inclusive) and ``endOffset`` (exclusive).\n\n        :type include_trailing_delimiter: boolean\n        :param include_trailing_delimiter:\n            (Optional) If true, objects that end in exactly one instance of\n            ``delimiter`` will have their metadata included in ``items`` in\n            addition to ``prefixes``.\n\n        :type versions: bool\n        :param versions: (Optional) Whether object versions should be returned\n                         as separate blobs.\n\n        :type projection: str\n        :param projection: (Optional) If used, must be 'full' or 'noAcl'.\n                           Defaults to ``'noAcl'``. Specifies the set of\n                           properties to return.\n\n        :type fields: str\n        :param fields:\n            (Optional) Selector specifying which fields to include\n            in a partial response. Must be a list of fields. For\n            example to get a partial response with just the next\n            page token and the name and language of each blob returned:\n            ``'items(name,contentLanguage),nextPageToken'``.\n            See: https://cloud.google.com/storage/docs/json_api/v1/parameters#fields\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type match_glob: str\n        :param match_glob:\n            (Optional) A glob pattern used to filter results (for example, foo*bar).\n            The string value must be UTF-8 encoded. See:\n            https://cloud.google.com/storage/docs/json_api/v1/objects/list#list-object-glob\n\n        :type include_folders_as_prefixes: bool\n            (Optional) If true, includes Folders and Managed Folders in the set of\n            ``prefixes`` returned by the query. Only applicable if ``delimiter`` is set to /.\n            See: https://cloud.google.com/storage/docs/managed-folders\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If true, only soft-deleted objects will be listed as distinct results in order of increasing\n            generation number. This parameter can only be used successfully if the bucket has a soft delete policy.\n            Note ``soft_deleted`` and ``versions`` cannot be set to True simultaneously. See:\n            https://cloud.google.com/storage/docs/soft-delete\n\n        :type page_size: int\n        :param page_size:\n            (Optional) Maximum number of blobs to return in each page.\n            Defaults to a value set by the API.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of all :class:`~google.cloud.storage.blob.Blob`\n                  in this bucket matching the arguments.\n        \"\"\"\n        client = self._require_client(client)\n        return client.list_blobs(\n            self,\n            max_results=max_results,\n            page_token=page_token,\n            prefix=prefix,\n            delimiter=delimiter,\n            start_offset=start_offset,\n            end_offset=end_offset,\n            include_trailing_delimiter=include_trailing_delimiter,\n            versions=versions,\n            projection=projection,\n            fields=fields,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n            match_glob=match_glob,\n            include_folders_as_prefixes=include_folders_as_prefixes,\n            soft_deleted=soft_deleted,\n        )\n\n    def list_notifications(\n        self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"List Pub / Sub notifications for this bucket.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/list\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: list of :class:`.BucketNotification`\n        :returns: notification instances\n        \"\"\"\n        client = self._require_client(client)\n        path = self.path + \"/notificationConfigs\"\n        iterator = client._list_resource(\n            path,\n            _item_to_notification,\n            timeout=timeout,\n            retry=retry,\n        )\n        iterator.bucket = self\n        return iterator\n\n    def get_notification(\n        self,\n        notification_id,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Get Pub / Sub notification for this bucket.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/notifications/get)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-print-pubsub-bucket-notification#storage_print_pubsub_bucket_notification-python).\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type notification_id: str\n        :param notification_id: The notification id to retrieve the notification configuration.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`.BucketNotification`\n        :returns: notification instance.\n        \"\"\"\n        notification = self.notification(notification_id=notification_id)\n        notification.reload(client=client, timeout=timeout, retry=retry)\n        return notification\n\n    def delete(\n        self,\n        force=False,\n        client=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Delete this bucket.\n\n        The bucket **must** be empty in order to submit a delete request. If\n        ``force=True`` is passed, this will first attempt to delete all the\n        objects / blobs in the bucket (i.e. try to empty the bucket).\n\n        If the bucket doesn't exist, this will raise\n        :class:`google.cloud.exceptions.NotFound`. If the bucket is not empty\n        (and ``force=False``), will raise :class:`google.cloud.exceptions.Conflict`.\n\n        If ``force=True`` and the bucket contains more than 256 objects / blobs\n        this will cowardly refuse to delete the objects (or the bucket). This\n        is to prevent accidental bucket deletion and to prevent extremely long\n        runtime of this method. Also note that ``force=True`` is not supported\n        in a ``Batch`` context.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type force: bool\n        :param force: If True, empties the bucket's objects then deletes it.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises: :class:`ValueError` if ``force`` is ``True`` and the bucket\n                 contains more than 256 objects / blobs.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        _add_generation_match_parameters(\n            query_params,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        if force:\n            blobs = list(\n                self.list_blobs(\n                    max_results=self._MAX_OBJECTS_FOR_ITERATION + 1,\n                    client=client,\n                    timeout=timeout,\n                    retry=retry,\n                    versions=True,\n                )\n            )\n            if len(blobs) > self._MAX_OBJECTS_FOR_ITERATION:\n                message = (\n                    \"Refusing to delete bucket with more than \"\n                    \"%d objects. If you actually want to delete \"\n                    \"this bucket, please delete the objects \"\n                    \"yourself before calling Bucket.delete().\"\n                ) % (self._MAX_OBJECTS_FOR_ITERATION,)\n                raise ValueError(message)\n\n            # Ignore 404 errors on delete.\n            self.delete_blobs(\n                blobs,\n                on_error=lambda blob: None,\n                client=client,\n                timeout=timeout,\n                retry=retry,\n                preserve_generation=True,\n            )\n\n        # We intentionally pass `_target_object=None` since a DELETE\n        # request has no response value (whether in a standard request or\n        # in a batch request).\n        client._delete_resource(\n            self.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def delete_blob(\n        self,\n        blob_name,\n        client=None,\n        generation=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Deletes a blob from the current bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blob_name: str\n        :param blob_name: A blob name to delete.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type generation: long\n        :param generation: (Optional) If present, permanently deletes a specific\n                           revision of this object.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :raises: :class:`google.cloud.exceptions.NotFound` Raises a NotFound\n                 if the blob isn't found. To suppress\n                 the exception, use :meth:`delete_blobs` by passing a no-op\n                 ``on_error`` callback.\n        \"\"\"\n        client = self._require_client(client)\n        blob = Blob(blob_name, bucket=self, generation=generation)\n\n        query_params = copy.deepcopy(blob._query_params)\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        # We intentionally pass `_target_object=None` since a DELETE\n        # request has no response value (whether in a standard request or\n        # in a batch request).\n        client._delete_resource(\n            blob.path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n    def delete_blobs(\n        self,\n        blobs,\n        on_error=None,\n        client=None,\n        preserve_generation=False,\n        timeout=_DEFAULT_TIMEOUT,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Deletes a list of blobs from the current bucket.\n\n        Uses :meth:`delete_blob` to delete each individual blob.\n\n        By default, any generation information in the list of blobs is ignored, and the\n        live versions of all blobs are deleted. Set `preserve_generation` to True\n        if blob generation should instead be propagated from the list of blobs.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blobs: list\n        :param blobs: A list of :class:`~google.cloud.storage.blob.Blob`-s or\n                      blob names to delete.\n\n        :type on_error: callable\n        :param on_error: (Optional) Takes single argument: ``blob``.\n                         Called once for each blob raising\n                         :class:`~google.cloud.exceptions.NotFound`;\n                         otherwise, the exception is propagated.\n                         Note that ``on_error`` is not supported in a ``Batch`` context.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type preserve_generation: bool\n        :param preserve_generation: (Optional) Deletes only the generation specified on the blob object,\n                                    instead of the live version, if set to True. Only :class:~google.cloud.storage.blob.Blob\n                                    objects can have their generation set in this way.\n                                    Default: False.\n\n        :type if_generation_match: list of long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the length of the list must match the length of\n            The list must match ``blobs`` item-to-item.\n\n        :type if_generation_not_match: list of long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            The list must match ``blobs`` item-to-item.\n\n        :type if_metageneration_match: list of long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            The list must match ``blobs`` item-to-item.\n\n        :type if_metageneration_not_match: list of long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            The list must match ``blobs`` item-to-item.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :raises: :class:`~google.cloud.exceptions.NotFound` (if\n                 `on_error` is not passed).\n        \"\"\"\n        _raise_if_len_differs(\n            len(blobs),\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        if_generation_match = iter(if_generation_match or [])\n        if_generation_not_match = iter(if_generation_not_match or [])\n        if_metageneration_match = iter(if_metageneration_match or [])\n        if_metageneration_not_match = iter(if_metageneration_not_match or [])\n\n        for blob in blobs:\n            try:\n                blob_name = blob\n                generation = None\n                if not isinstance(blob_name, str):\n                    blob_name = blob.name\n                    generation = blob.generation if preserve_generation else None\n\n                self.delete_blob(\n                    blob_name,\n                    client=client,\n                    generation=generation,\n                    if_generation_match=next(if_generation_match, None),\n                    if_generation_not_match=next(if_generation_not_match, None),\n                    if_metageneration_match=next(if_metageneration_match, None),\n                    if_metageneration_not_match=next(if_metageneration_not_match, None),\n                    timeout=timeout,\n                    retry=retry,\n                )\n            except NotFound:\n                if on_error is not None:\n                    on_error(blob)\n                else:\n                    raise\n\n    def copy_blob(\n        self,\n        blob,\n        destination_bucket,\n        new_name=None,\n        client=None,\n        preserve_acl=True,\n        source_generation=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Copy the given blob to the given bucket, optionally with a new name.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/objects/copy)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-copy-file#storage_copy_file-python).\n\n        :type blob: :class:`google.cloud.storage.blob.Blob`\n        :param blob: The blob to be copied.\n\n        :type destination_bucket: :class:`google.cloud.storage.bucket.Bucket`\n        :param destination_bucket: The bucket into which the blob should be\n                                   copied.\n\n        :type new_name: str\n        :param new_name: (Optional) The new name for the copied file.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type preserve_acl: bool\n        :param preserve_acl: DEPRECATED. This argument is not functional!\n                             (Optional) Copies ACL from old blob to new blob.\n                             Default: True.\n                             Note that ``preserve_acl`` is not supported in a\n                             ``Batch`` context.\n\n        :type source_generation: long\n        :param source_generation: (Optional) The generation of the blob to be\n                                  copied.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The new Blob.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        if source_generation is not None:\n            query_params[\"sourceGeneration\"] = source_generation\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n        )\n\n        if new_name is None:\n            new_name = blob.name\n\n        new_blob = Blob(bucket=destination_bucket, name=new_name)\n        api_path = blob.path + \"/copyTo\" + new_blob.path\n        copy_result = client._post_resource(\n            api_path,\n            None,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=new_blob,\n        )\n\n        if not preserve_acl:\n            new_blob.acl.save(acl={}, client=client, timeout=timeout)\n\n        new_blob._set_properties(copy_result)\n        return new_blob\n\n    def rename_blob(\n        self,\n        blob,\n        new_name,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        if_source_generation_match=None,\n        if_source_generation_not_match=None,\n        if_source_metageneration_match=None,\n        if_source_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Rename the given blob using copy and delete operations.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        Effectively, copies blob to the same bucket with a new name, then\n        deletes the blob.\n\n        .. warning::\n\n          This method will first duplicate the data and then delete the\n          old blob.  This means that with very large objects renaming\n          could be a very (temporarily) costly or a very slow operation.\n          If you need more control over the copy and deletion, instead\n          use ``google.cloud.storage.blob.Blob.copy_to`` and\n          ``google.cloud.storage.blob.Blob.delete`` directly.\n\n          Also note that this method is not fully supported in a\n          ``Batch`` context.\n\n        :type blob: :class:`google.cloud.storage.blob.Blob`\n        :param blob: The blob to be renamed.\n\n        :type new_name: str\n        :param new_name: The new name for this blob.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n            Note that the generation to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n            Note that the metageneration to be matched is that of the\n            ``destination`` blob.\n\n        :type if_source_generation_match: long\n        :param if_source_generation_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation matches the given value. Also used in the\n            (implied) delete request.\n\n        :type if_source_generation_not_match: long\n        :param if_source_generation_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's generation does not match the given value. Also used in\n            the (implied) delete request.\n\n        :type if_source_metageneration_match: long\n        :param if_source_metageneration_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration matches the given value. Also used\n            in the (implied) delete request.\n\n        :type if_source_metageneration_not_match: long\n        :param if_source_metageneration_not_match:\n            (Optional) Makes the operation conditional on whether the source\n            object's current metageneration does not match the given value.\n            Also used in the (implied) delete request.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, a conditional retry\n            policy which will only enable retries if ``if_generation_match`` or ``generation``\n            is set, in order to ensure requests are idempotent before retrying them.\n            Change the value to ``DEFAULT_RETRY`` or another `google.api_core.retry.Retry` object\n            to enable retries regardless of generation precondition setting.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: :class:`Blob`\n        :returns: The newly-renamed blob.\n        \"\"\"\n        same_name = blob.name == new_name\n\n        new_blob = self.copy_blob(\n            blob,\n            self,\n            new_name,\n            client=client,\n            timeout=timeout,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            if_source_generation_match=if_source_generation_match,\n            if_source_generation_not_match=if_source_generation_not_match,\n            if_source_metageneration_match=if_source_metageneration_match,\n            if_source_metageneration_not_match=if_source_metageneration_not_match,\n            retry=retry,\n        )\n\n        if not same_name:\n            blob.delete(\n                client=client,\n                timeout=timeout,\n                if_generation_match=if_source_generation_match,\n                if_generation_not_match=if_source_generation_not_match,\n                if_metageneration_match=if_source_metageneration_match,\n                if_metageneration_not_match=if_source_metageneration_not_match,\n                retry=retry,\n            )\n        return new_blob\n\n    def restore_blob(\n        self,\n        blob_name,\n        client=None,\n        generation=None,\n        copy_source_acl=None,\n        projection=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n    ):\n        \"\"\"Restores a soft-deleted object.\n\n        If :attr:`user_project` is set on the bucket, bills the API request to that project.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/objects/restore)\n\n        :type blob_name: str\n        :param blob_name: The name of the blob to be restored.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use. If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type generation: long\n        :param generation: (Optional) If present, selects a specific revision of this object.\n\n        :type copy_source_acl: bool\n        :param copy_source_acl: (Optional) If true, copy the soft-deleted object's access controls.\n\n        :type projection: str\n        :param projection: (Optional) Specifies the set of properties to return.\n                           If used, must be 'full' or 'noAcl'.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC.\n            The default value is ``DEFAULT_RETRY_IF_GENERATION_SPECIFIED``, which\n            only restore operations with ``if_generation_match`` or ``generation`` set\n            will be retried.\n\n            Users can configure non-default retry behavior. A ``None`` value will\n            disable retries. A ``DEFAULT_RETRY`` value will enable retries\n            even if restore operations are not guaranteed to be idempotent.\n            See [Configuring Retries](https://cloud.google.com/python/docs/reference/storage/latest/retry_timeout).\n\n        :rtype: :class:`google.cloud.storage.blob.Blob`\n        :returns: The restored Blob.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n        if generation is not None:\n            query_params[\"generation\"] = generation\n        if copy_source_acl is not None:\n            query_params[\"copySourceAcl\"] = copy_source_acl\n        if projection is not None:\n            query_params[\"projection\"] = projection\n\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        blob = Blob(bucket=self, name=blob_name)\n        api_response = client._post_resource(\n            f\"{blob.path}/restore\",\n            None,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        blob._set_properties(api_response)\n        return blob\n\n    @property\n    def cors(self):\n        \"\"\"Retrieve or set CORS policies configured for this bucket.\n\n        See http://www.w3.org/TR/cors/ and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        .. note::\n\n           The getter for this property returns a list which contains\n           *copies* of the bucket's CORS policy mappings.  Mutating the list\n           or one of its dicts has no effect unless you then re-assign the\n           dict via the setter.  E.g.:\n\n           >>> policies = bucket.cors\n           >>> policies.append({'origin': '/foo', ...})\n           >>> policies[1]['maxAgeSeconds'] = 3600\n           >>> del policies[0]\n           >>> bucket.cors = policies\n           >>> bucket.update()\n\n        :setter: Set CORS policies for this bucket.\n        :getter: Gets the CORS policies for this bucket.\n\n        :rtype: list of dictionaries\n        :returns: A sequence of mappings describing each CORS policy.\n        \"\"\"\n        return [copy.deepcopy(policy) for policy in self._properties.get(\"cors\", ())]\n\n    @cors.setter\n    def cors(self, entries):\n        \"\"\"Set CORS policies configured for this bucket.\n\n        See http://www.w3.org/TR/cors/ and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :type entries: list of dictionaries\n        :param entries: A sequence of mappings describing each CORS policy.\n        \"\"\"\n        self._patch_property(\"cors\", entries)\n\n    default_event_based_hold = _scalar_property(\"defaultEventBasedHold\")\n    \"\"\"Are uploaded objects automatically placed under an even-based hold?\n\n    If True, uploaded objects will be placed under an event-based hold to\n    be released at a future time. When released an object will then begin\n    the retention period determined by the policy retention period for the\n    object bucket.\n\n    See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n    If the property is not set locally, returns ``None``.\n\n    :rtype: bool or ``NoneType``\n    \"\"\"\n\n    @property\n    def default_kms_key_name(self):\n        \"\"\"Retrieve / set default KMS encryption key for objects in the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :setter: Set default KMS encryption key for items in this bucket.\n        :getter: Get default KMS encryption key for items in this bucket.\n\n        :rtype: str\n        :returns: Default KMS encryption key, or ``None`` if not set.\n        \"\"\"\n        encryption_config = self._properties.get(\"encryption\", {})\n        return encryption_config.get(\"defaultKmsKeyName\")\n\n    @default_kms_key_name.setter\n    def default_kms_key_name(self, value):\n        \"\"\"Set default KMS encryption key for objects in the bucket.\n\n        :type value: str or None\n        :param value: new KMS key name (None to clear any existing key).\n        \"\"\"\n        encryption_config = self._properties.get(\"encryption\", {})\n        encryption_config[\"defaultKmsKeyName\"] = value\n        self._patch_property(\"encryption\", encryption_config)\n\n    @property\n    def labels(self):\n        \"\"\"Retrieve or set labels assigned to this bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets#labels\n\n        .. note::\n\n           The getter for this property returns a dict which is a *copy*\n           of the bucket's labels.  Mutating that dict has no effect unless\n           you then re-assign the dict via the setter.  E.g.:\n\n           >>> labels = bucket.labels\n           >>> labels['new_key'] = 'some-label'\n           >>> del labels['old_key']\n           >>> bucket.labels = labels\n           >>> bucket.update()\n\n        :setter: Set labels for this bucket.\n        :getter: Gets the labels for this bucket.\n\n        :rtype: :class:`dict`\n        :returns: Name-value pairs (string->string) labelling the bucket.\n        \"\"\"\n        labels = self._properties.get(\"labels\")\n        if labels is None:\n            return {}\n        return copy.deepcopy(labels)\n\n    @labels.setter\n    def labels(self, mapping):\n        \"\"\"Set labels assigned to this bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets#labels\n\n        :type mapping: :class:`dict`\n        :param mapping: Name-value pairs (string->string) labelling the bucket.\n        \"\"\"\n        # If any labels have been expressly removed, we need to track this\n        # so that a future .patch() call can do the correct thing.\n        existing = set([k for k in self.labels.keys()])\n        incoming = set([k for k in mapping.keys()])\n        self._label_removals = self._label_removals.union(existing.difference(incoming))\n        mapping = {k: str(v) for k, v in mapping.items()}\n\n        # Actually update the labels on the object.\n        self._patch_property(\"labels\", copy.deepcopy(mapping))\n\n    @property\n    def etag(self):\n        \"\"\"Retrieve the ETag for the bucket.\n\n        See https://tools.ietf.org/html/rfc2616#section-3.11 and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: str or ``NoneType``\n        :returns: The bucket etag or ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"etag\")\n\n    @property\n    def id(self):\n        \"\"\"Retrieve the ID for the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: str or ``NoneType``\n        :returns: The ID of the bucket or ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"id\")\n\n    @property\n    def iam_configuration(self):\n        \"\"\"Retrieve IAM configuration for this bucket.\n\n        :rtype: :class:`IAMConfiguration`\n        :returns: an instance for managing the bucket's IAM configuration.\n        \"\"\"\n        info = self._properties.get(\"iamConfiguration\", {})\n        return IAMConfiguration.from_api_repr(info, self)\n\n    @property\n    def soft_delete_policy(self):\n        \"\"\"Retrieve the soft delete policy for this bucket.\n\n        See https://cloud.google.com/storage/docs/soft-delete\n\n        :rtype: :class:`SoftDeletePolicy`\n        :returns: an instance for managing the bucket's soft delete policy.\n        \"\"\"\n        policy = self._properties.get(\"softDeletePolicy\", {})\n        return SoftDeletePolicy.from_api_repr(policy, self)\n\n    @property\n    def lifecycle_rules(self):\n        \"\"\"Retrieve or set lifecycle rules configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/lifecycle and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        .. note::\n\n           The getter for this property returns a generator which yields\n           *copies* of the bucket's lifecycle rules mappings.  Mutating the\n           output dicts has no effect unless you then re-assign the dict via\n           the setter.  E.g.:\n\n           >>> rules = list(bucket.lifecycle_rules)\n           >>> rules.append({'origin': '/foo', ...})\n           >>> rules[1]['rule']['action']['type'] = 'Delete'\n           >>> del rules[0]\n           >>> bucket.lifecycle_rules = rules\n           >>> bucket.update()\n\n        :setter: Set lifecycle rules for this bucket.\n        :getter: Gets the lifecycle rules for this bucket.\n\n        :rtype: generator(dict)\n        :returns: A sequence of mappings describing each lifecycle rule.\n        \"\"\"\n        info = self._properties.get(\"lifecycle\", {})\n        for rule in info.get(\"rule\", ()):\n            action_type = rule[\"action\"][\"type\"]\n            if action_type == \"Delete\":\n                yield LifecycleRuleDelete.from_api_repr(rule)\n            elif action_type == \"SetStorageClass\":\n                yield LifecycleRuleSetStorageClass.from_api_repr(rule)\n            elif action_type == \"AbortIncompleteMultipartUpload\":\n                yield LifecycleRuleAbortIncompleteMultipartUpload.from_api_repr(rule)\n            else:\n                warnings.warn(\n                    \"Unknown lifecycle rule type received: {}. Please upgrade to the latest version of google-cloud-storage.\".format(\n                        rule\n                    ),\n                    UserWarning,\n                    stacklevel=1,\n                )\n\n    @lifecycle_rules.setter\n    def lifecycle_rules(self, rules):\n        \"\"\"Set lifecycle rules configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/lifecycle and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :type rules: list of dictionaries\n        :param rules: A sequence of mappings describing each lifecycle rule.\n        \"\"\"\n        rules = [dict(rule) for rule in rules]  # Convert helpers if needed\n        self._patch_property(\"lifecycle\", {\"rule\": rules})\n\n    def clear_lifecycle_rules(self):\n        \"\"\"Clear lifecycle rules configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/lifecycle and\n             https://cloud.google.com/storage/docs/json_api/v1/buckets\n        \"\"\"\n        self.lifecycle_rules = []\n\n    def clear_lifecyle_rules(self):\n        \"\"\"Deprecated alias for clear_lifecycle_rules.\"\"\"\n        return self.clear_lifecycle_rules()\n\n    def add_lifecycle_delete_rule(self, **kw):\n        \"\"\"Add a \"delete\" rule to lifecycle rules configured for this bucket.\n\n        This defines a [lifecycle configuration](https://cloud.google.com/storage/docs/lifecycle),\n        which is set on the bucket. For the general format of a lifecycle configuration, see the\n        [bucket resource representation for JSON](https://cloud.google.com/storage/docs/json_api/v1/buckets).\n        See also a [code sample](https://cloud.google.com/storage/docs/samples/storage-enable-bucket-lifecycle-management#storage_enable_bucket_lifecycle_management-python).\n\n        :type kw: dict\n        :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n        \"\"\"\n        rules = list(self.lifecycle_rules)\n        rules.append(LifecycleRuleDelete(**kw))\n        self.lifecycle_rules = rules\n\n    def add_lifecycle_set_storage_class_rule(self, storage_class, **kw):\n        \"\"\"Add a \"set storage class\" rule to lifecycle rules.\n\n        This defines a [lifecycle configuration](https://cloud.google.com/storage/docs/lifecycle),\n        which is set on the bucket. For the general format of a lifecycle configuration, see the\n        [bucket resource representation for JSON](https://cloud.google.com/storage/docs/json_api/v1/buckets).\n\n        :type storage_class: str, one of :attr:`STORAGE_CLASSES`.\n        :param storage_class: new storage class to assign to matching items.\n\n        :type kw: dict\n        :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n        \"\"\"\n        rules = list(self.lifecycle_rules)\n        rules.append(LifecycleRuleSetStorageClass(storage_class, **kw))\n        self.lifecycle_rules = rules\n\n    def add_lifecycle_abort_incomplete_multipart_upload_rule(self, **kw):\n        \"\"\"Add a \"abort incomplete multipart upload\" rule to lifecycle rules.\n\n        .. note::\n          The \"age\" lifecycle condition is the only supported condition\n          for this rule.\n\n        This defines a [lifecycle configuration](https://cloud.google.com/storage/docs/lifecycle),\n        which is set on the bucket. For the general format of a lifecycle configuration, see the\n        [bucket resource representation for JSON](https://cloud.google.com/storage/docs/json_api/v1/buckets).\n\n        :type kw: dict\n        :params kw: arguments passed to :class:`LifecycleRuleConditions`.\n        \"\"\"\n        rules = list(self.lifecycle_rules)\n        rules.append(LifecycleRuleAbortIncompleteMultipartUpload(**kw))\n        self.lifecycle_rules = rules\n\n    _location = _scalar_property(\"location\")\n\n    @property\n    def location(self):\n        \"\"\"Retrieve location configured for this bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets and\n        https://cloud.google.com/storage/docs/locations\n\n        Returns ``None`` if the property has not been set before creation,\n        or if the bucket's resource has not been loaded from the server.\n        :rtype: str or ``NoneType``\n        \"\"\"\n        return self._location\n\n    @location.setter\n    def location(self, value):\n        \"\"\"(Deprecated) Set `Bucket.location`\n\n        This can only be set at bucket **creation** time.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets and\n        https://cloud.google.com/storage/docs/bucket-locations\n\n        .. warning::\n\n            Assignment to 'Bucket.location' is deprecated, as it is only\n            valid before the bucket is created. Instead, pass the location\n            to `Bucket.create`.\n        \"\"\"\n        warnings.warn(_LOCATION_SETTER_MESSAGE, DeprecationWarning, stacklevel=2)\n        self._location = value\n\n    @property\n    def data_locations(self):\n        \"\"\"Retrieve the list of regional locations for custom dual-region buckets.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets and\n        https://cloud.google.com/storage/docs/locations\n\n        Returns ``None`` if the property has not been set before creation,\n        if the bucket's resource has not been loaded from the server,\n        or if the bucket is not a dual-regions bucket.\n        :rtype: list of str or ``NoneType``\n        \"\"\"\n        custom_placement_config = self._properties.get(\"customPlacementConfig\", {})\n        return custom_placement_config.get(\"dataLocations\")\n\n    @property\n    def location_type(self):\n        \"\"\"Retrieve the location type for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :getter: Gets the the location type for this bucket.\n\n        :rtype: str or ``NoneType``\n        :returns:\n            If set, one of\n            :attr:`~google.cloud.storage.constants.MULTI_REGION_LOCATION_TYPE`,\n            :attr:`~google.cloud.storage.constants.REGION_LOCATION_TYPE`, or\n            :attr:`~google.cloud.storage.constants.DUAL_REGION_LOCATION_TYPE`,\n            else ``None``.\n        \"\"\"\n        return self._properties.get(\"locationType\")\n\n    def get_logging(self):\n        \"\"\"Return info about access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs#status\n\n        :rtype: dict or None\n        :returns: a dict w/ keys, ``logBucket`` and ``logObjectPrefix``\n                  (if logging is enabled), or None (if not).\n        \"\"\"\n        info = self._properties.get(\"logging\")\n        return copy.deepcopy(info)\n\n    def enable_logging(self, bucket_name, object_prefix=\"\"):\n        \"\"\"Enable access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs\n\n        :type bucket_name: str\n        :param bucket_name: name of bucket in which to store access logs\n\n        :type object_prefix: str\n        :param object_prefix: prefix for access log filenames\n        \"\"\"\n        info = {\"logBucket\": bucket_name, \"logObjectPrefix\": object_prefix}\n        self._patch_property(\"logging\", info)\n\n    def disable_logging(self):\n        \"\"\"Disable access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs#disabling\n        \"\"\"\n        self._patch_property(\"logging\", None)\n\n    @property\n    def metageneration(self):\n        \"\"\"Retrieve the metageneration for the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: int or ``NoneType``\n        :returns: The metageneration of the bucket or ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        metageneration = self._properties.get(\"metageneration\")\n        if metageneration is not None:\n            return int(metageneration)\n\n    @property\n    def owner(self):\n        \"\"\"Retrieve info about the owner of the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: dict or ``NoneType``\n        :returns: Mapping of owner's role/ID. Returns ``None`` if the bucket's\n                  resource has not been loaded from the server.\n        \"\"\"\n        return copy.deepcopy(self._properties.get(\"owner\"))\n\n    @property\n    def project_number(self):\n        \"\"\"Retrieve the number of the project to which the bucket is assigned.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: int or ``NoneType``\n        :returns: The project number that owns the bucket or ``None`` if\n                  the bucket's resource has not been loaded from the server.\n        \"\"\"\n        project_number = self._properties.get(\"projectNumber\")\n        if project_number is not None:\n            return int(project_number)\n\n    @property\n    def retention_policy_effective_time(self):\n        \"\"\"Retrieve the effective time of the bucket's retention policy.\n\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's retention policy is\n                  effective, or ``None`` if the property is not\n                  set locally.\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            timestamp = policy.get(\"effectiveTime\")\n            if timestamp is not None:\n                return _rfc3339_nanos_to_datetime(timestamp)\n\n    @property\n    def retention_policy_locked(self):\n        \"\"\"Retrieve whthere the bucket's retention policy is locked.\n\n        :rtype: bool\n        :returns: True if the bucket's policy is locked, or else False\n                  if the policy is not locked, or the property is not\n                  set locally.\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            return policy.get(\"isLocked\")\n\n    @property\n    def retention_period(self):\n        \"\"\"Retrieve or set the retention period for items in the bucket.\n\n        :rtype: int or ``NoneType``\n        :returns: number of seconds to retain items after upload or release\n                  from event-based lock, or ``None`` if the property is not\n                  set locally.\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            period = policy.get(\"retentionPeriod\")\n            if period is not None:\n                return int(period)\n\n    @retention_period.setter\n    def retention_period(self, value):\n        \"\"\"Set the retention period for items in the bucket.\n\n        :type value: int\n        :param value:\n            number of seconds to retain items after upload or release from\n            event-based lock.\n\n        :raises ValueError: if the bucket's retention policy is locked.\n        \"\"\"\n        policy = self._properties.setdefault(\"retentionPolicy\", {})\n        if value is not None:\n            policy[\"retentionPeriod\"] = str(value)\n        else:\n            policy = None\n        self._patch_property(\"retentionPolicy\", policy)\n\n    @property\n    def self_link(self):\n        \"\"\"Retrieve the URI for the bucket.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: str or ``NoneType``\n        :returns: The self link for the bucket or ``None`` if\n                  the bucket's resource has not been loaded from the server.\n        \"\"\"\n        return self._properties.get(\"selfLink\")\n\n    @property\n    def storage_class(self):\n        \"\"\"Retrieve or set the storage class for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :setter: Set the storage class for this bucket.\n        :getter: Gets the the storage class for this bucket.\n\n        :rtype: str or ``NoneType``\n        :returns:\n            If set, one of\n            :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`,\n            or\n            :attr:`~google.cloud.storage.constants.DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS`,\n            else ``None``.\n        \"\"\"\n        return self._properties.get(\"storageClass\")\n\n    @storage_class.setter\n    def storage_class(self, value):\n        \"\"\"Set the storage class for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :type value: str\n        :param value:\n            One of\n            :attr:`~google.cloud.storage.constants.NEARLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.COLDLINE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.ARCHIVE_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.STANDARD_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.MULTI_REGIONAL_LEGACY_STORAGE_CLASS`,\n            :attr:`~google.cloud.storage.constants.REGIONAL_LEGACY_STORAGE_CLASS`,\n            or\n            :attr:`~google.cloud.storage.constants.DURABLE_REDUCED_AVAILABILITY_LEGACY_STORAGE_CLASS`,\n        \"\"\"\n        self._patch_property(\"storageClass\", value)\n\n    @property\n    def time_created(self):\n        \"\"\"Retrieve the timestamp at which the bucket was created.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"timeCreated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def updated(self):\n        \"\"\"Retrieve the timestamp at which the bucket was last updated.\n\n        See https://cloud.google.com/storage/docs/json_api/v1/buckets\n\n        :rtype: :class:`datetime.datetime` or ``NoneType``\n        :returns: Datetime object parsed from RFC3339 valid timestamp, or\n                  ``None`` if the bucket's resource has not been loaded\n                  from the server.\n        \"\"\"\n        value = self._properties.get(\"updated\")\n        if value is not None:\n            return _rfc3339_nanos_to_datetime(value)\n\n    @property\n    def versioning_enabled(self):\n        \"\"\"Is versioning enabled for this bucket?\n\n        See  https://cloud.google.com/storage/docs/object-versioning for\n        details.\n\n        :setter: Update whether versioning is enabled for this bucket.\n        :getter: Query whether versioning is enabled for this bucket.\n\n        :rtype: bool\n        :returns: True if enabled, else False.\n        \"\"\"\n        versioning = self._properties.get(\"versioning\", {})\n        return versioning.get(\"enabled\", False)\n\n    @versioning_enabled.setter\n    def versioning_enabled(self, value):\n        \"\"\"Enable versioning for this bucket.\n\n        See  https://cloud.google.com/storage/docs/object-versioning for\n        details.\n\n        :type value: convertible to boolean\n        :param value: should versioning be enabled for the bucket?\n        \"\"\"\n        self._patch_property(\"versioning\", {\"enabled\": bool(value)})\n\n    @property\n    def requester_pays(self):\n        \"\"\"Does the requester pay for API requests for this bucket?\n\n        See https://cloud.google.com/storage/docs/requester-pays for\n        details.\n\n        :setter: Update whether requester pays for this bucket.\n        :getter: Query whether requester pays for this bucket.\n\n        :rtype: bool\n        :returns: True if requester pays for API requests for the bucket,\n                  else False.\n        \"\"\"\n        versioning = self._properties.get(\"billing\", {})\n        return versioning.get(\"requesterPays\", False)\n\n    @requester_pays.setter\n    def requester_pays(self, value):\n        \"\"\"Update whether requester pays for API requests for this bucket.\n\n        See https://cloud.google.com/storage/docs/using-requester-pays for\n        details.\n\n        :type value: convertible to boolean\n        :param value: should requester pay for API requests for the bucket?\n        \"\"\"\n        self._patch_property(\"billing\", {\"requesterPays\": bool(value)})\n\n    @property\n    def autoclass_enabled(self):\n        \"\"\"Whether Autoclass is enabled for this bucket.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :setter: Update whether autoclass is enabled for this bucket.\n        :getter: Query whether autoclass is enabled for this bucket.\n\n        :rtype: bool\n        :returns: True if enabled, else False.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        return autoclass.get(\"enabled\", False)\n\n    @autoclass_enabled.setter\n    def autoclass_enabled(self, value):\n        \"\"\"Enable or disable Autoclass at the bucket-level.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :type value: convertible to boolean\n        :param value: If true, enable Autoclass for this bucket.\n                      If false, disable Autoclass for this bucket.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        autoclass[\"enabled\"] = bool(value)\n        self._patch_property(\"autoclass\", autoclass)\n\n    @property\n    def autoclass_toggle_time(self):\n        \"\"\"Retrieve the toggle time when Autoclaass was last enabled or disabled for the bucket.\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's autoclass is toggled, or ``None`` if the property is not set locally.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\")\n        if autoclass is not None:\n            timestamp = autoclass.get(\"toggleTime\")\n            if timestamp is not None:\n                return _rfc3339_nanos_to_datetime(timestamp)\n\n    @property\n    def autoclass_terminal_storage_class(self):\n        \"\"\"The storage class that objects in an Autoclass bucket eventually transition to if\n        they are not read for a certain length of time. Valid values are NEARLINE and ARCHIVE.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :setter: Set the terminal storage class for Autoclass configuration.\n        :getter: Get the terminal storage class for Autoclass configuration.\n\n        :rtype: str\n        :returns: The terminal storage class if Autoclass is enabled, else ``None``.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        return autoclass.get(\"terminalStorageClass\", None)\n\n    @autoclass_terminal_storage_class.setter\n    def autoclass_terminal_storage_class(self, value):\n        \"\"\"The storage class that objects in an Autoclass bucket eventually transition to if\n        they are not read for a certain length of time. Valid values are NEARLINE and ARCHIVE.\n\n        See https://cloud.google.com/storage/docs/using-autoclass for details.\n\n        :type value: str\n        :param value: The only valid values are `\"NEARLINE\"` and `\"ARCHIVE\"`.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\", {})\n        autoclass[\"terminalStorageClass\"] = value\n        self._patch_property(\"autoclass\", autoclass)\n\n    @property\n    def autoclass_terminal_storage_class_update_time(self):\n        \"\"\"The time at which the Autoclass terminal_storage_class field was last updated for this bucket\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's terminal_storage_class is last updated, or ``None`` if the property is not set locally.\n        \"\"\"\n        autoclass = self._properties.get(\"autoclass\")\n        if autoclass is not None:\n            timestamp = autoclass.get(\"terminalStorageClassUpdateTime\")\n            if timestamp is not None:\n                return _rfc3339_nanos_to_datetime(timestamp)\n\n    @property\n    def object_retention_mode(self):\n        \"\"\"Retrieve the object retention mode set on the bucket.\n\n        :rtype: str\n        :returns: When set to Enabled, retention configurations can be\n                  set on objects in the bucket.\n        \"\"\"\n        object_retention = self._properties.get(\"objectRetention\")\n        if object_retention is not None:\n            return object_retention.get(\"mode\")\n\n    @property\n    def hierarchical_namespace_enabled(self):\n        \"\"\"Whether hierarchical namespace is enabled for this bucket.\n\n        :setter: Update whether hierarchical namespace is enabled for this bucket.\n        :getter: Query whether hierarchical namespace is enabled for this bucket.\n\n        :rtype: bool\n        :returns: True if enabled, else False.\n        \"\"\"\n        hns = self._properties.get(\"hierarchicalNamespace\", {})\n        return hns.get(\"enabled\")\n\n    @hierarchical_namespace_enabled.setter\n    def hierarchical_namespace_enabled(self, value):\n        \"\"\"Enable or disable hierarchical namespace at the bucket-level.\n\n        :type value: convertible to boolean\n        :param value: If true, enable hierarchical namespace for this bucket.\n                      If false, disable hierarchical namespace for this bucket.\n\n        .. note::\n          To enable hierarchical namespace, you must set it at bucket creation time.\n          Currently, hierarchical namespace configuration cannot be changed after bucket creation.\n        \"\"\"\n        hns = self._properties.get(\"hierarchicalNamespace\", {})\n        hns[\"enabled\"] = bool(value)\n        self._patch_property(\"hierarchicalNamespace\", hns)\n\n    def configure_website(self, main_page_suffix=None, not_found_page=None):\n        \"\"\"Configure website-related properties.\n\n        See https://cloud.google.com/storage/docs/static-website\n\n        .. note::\n          This configures the bucket's website-related properties,controlling how\n          the service behaves when accessing bucket contents as a web site.\n          See [tutorials](https://cloud.google.com/storage/docs/hosting-static-website) and\n          [code samples](https://cloud.google.com/storage/docs/samples/storage-define-bucket-website-configuration#storage_define_bucket_website_configuration-python)\n          for more information.\n\n        :type main_page_suffix: str\n        :param main_page_suffix: The page to use as the main page\n                                 of a directory.\n                                 Typically something like index.html.\n\n        :type not_found_page: str\n        :param not_found_page: The file to use when a page isn't found.\n        \"\"\"\n        data = {\"mainPageSuffix\": main_page_suffix, \"notFoundPage\": not_found_page}\n        self._patch_property(\"website\", data)\n\n    def disable_website(self):\n        \"\"\"Disable the website configuration for this bucket.\n\n        This is really just a shortcut for setting the website-related\n        attributes to ``None``.\n        \"\"\"\n        return self.configure_website(None, None)\n\n    def get_iam_policy(\n        self,\n        client=None,\n        requested_policy_version=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Retrieve the IAM policy for the bucket.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/getIamPolicy)\n        and a [code sample](https://cloud.google.com/storage/docs/samples/storage-view-bucket-iam-members#storage_view_bucket_iam_members-python).\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type requested_policy_version: int or ``NoneType``\n        :param requested_policy_version: (Optional) The version of IAM policies to request.\n                                         If a policy with a condition is requested without\n                                         setting this, the server will return an error.\n                                         This must be set to a value of 3 to retrieve IAM\n                                         policies containing conditions. This is to prevent\n                                         client code that isn't aware of IAM conditions from\n                                         interpreting and modifying policies incorrectly.\n                                         The service might return a policy with version lower\n                                         than the one that was requested, based on the\n                                         feature syntax in the policy fetched.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``getIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        if requested_policy_version is not None:\n            query_params[\"optionsRequestedPolicyVersion\"] = requested_policy_version\n\n        info = client._get_resource(\n            f\"{self.path}/iam\",\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return Policy.from_api_repr(info)\n\n    def set_iam_policy(\n        self,\n        policy,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_ETAG_IN_JSON,\n    ):\n        \"\"\"Update the IAM policy for the bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets/setIamPolicy\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type policy: :class:`google.api_core.iam.Policy`\n        :param policy: policy instance used to update bucket's IAM policy.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.api_core.iam.Policy`\n        :returns: the policy instance, based on the resource returned from\n                  the ``setIamPolicy`` API request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam\"\n        resource = policy.to_api_repr()\n        resource[\"resourceId\"] = self.path\n\n        info = client._put_resource(\n            path,\n            resource,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n\n        return Policy.from_api_repr(info)\n\n    def test_iam_permissions(\n        self, permissions, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"API call:  test permissions\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets/testIamPermissions\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type permissions: list of string\n        :param permissions: the permissions to check\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: list of string\n        :returns: the permissions returned by the ``testIamPermissions`` API\n                  request.\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {\"permissions\": permissions}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"{self.path}/iam/testPermissions\"\n        resp = client._get_resource(\n            path,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=None,\n        )\n        return resp.get(\"permissions\", [])\n\n    def make_public(\n        self,\n        recursive=False,\n        future=False,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update bucket's ACL, granting read access to anonymous users.\n\n        :type recursive: bool\n        :param recursive: If True, this will make all blobs inside the bucket\n                          public as well.\n\n        :type future: bool\n        :param future: If True, this will make all objects created in the\n                       future public as well.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError:\n            If ``recursive`` is True, and the bucket contains more than 256\n            blobs.  This is to prevent extremely long runtime of this\n            method.  For such buckets, iterate over the blobs returned by\n            :meth:`list_blobs` and call\n            :meth:`~google.cloud.storage.blob.Blob.make_public`\n            for each blob.\n        \"\"\"\n        self.acl.all().grant_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n        if future:\n            doa = self.default_object_acl\n            if not doa.loaded:\n                doa.reload(client=client, timeout=timeout)\n            doa.all().grant_read()\n            doa.save(\n                client=client,\n                timeout=timeout,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n            )\n\n        if recursive:\n            blobs = list(\n                self.list_blobs(\n                    projection=\"full\",\n                    max_results=self._MAX_OBJECTS_FOR_ITERATION + 1,\n                    client=client,\n                    timeout=timeout,\n                )\n            )\n            if len(blobs) > self._MAX_OBJECTS_FOR_ITERATION:\n                message = (\n                    \"Refusing to make public recursively with more than \"\n                    \"%d objects. If you actually want to make every object \"\n                    \"in this bucket public, iterate through the blobs \"\n                    \"returned by 'Bucket.list_blobs()' and call \"\n                    \"'make_public' on each one.\"\n                ) % (self._MAX_OBJECTS_FOR_ITERATION,)\n                raise ValueError(message)\n\n            for blob in blobs:\n                blob.acl.all().grant_read()\n                blob.acl.save(\n                    client=client,\n                    timeout=timeout,\n                )\n\n    def make_private(\n        self,\n        recursive=False,\n        future=False,\n        client=None,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n    ):\n        \"\"\"Update bucket's ACL, revoking read access for anonymous users.\n\n        :type recursive: bool\n        :param recursive: If True, this will make all blobs inside the bucket\n                          private as well.\n\n        :type future: bool\n        :param future: If True, this will make all objects created in the\n                       future private as well.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError:\n            If ``recursive`` is True, and the bucket contains more than 256\n            blobs.  This is to prevent extremely long runtime of this\n            method.  For such buckets, iterate over the blobs returned by\n            :meth:`list_blobs` and call\n            :meth:`~google.cloud.storage.blob.Blob.make_private`\n            for each blob.\n        \"\"\"\n        self.acl.all().revoke_read()\n        self.acl.save(\n            client=client,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n\n        if future:\n            doa = self.default_object_acl\n            if not doa.loaded:\n                doa.reload(client=client, timeout=timeout)\n            doa.all().revoke_read()\n            doa.save(\n                client=client,\n                timeout=timeout,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n            )\n\n        if recursive:\n            blobs = list(\n                self.list_blobs(\n                    projection=\"full\",\n                    max_results=self._MAX_OBJECTS_FOR_ITERATION + 1,\n                    client=client,\n                    timeout=timeout,\n                )\n            )\n            if len(blobs) > self._MAX_OBJECTS_FOR_ITERATION:\n                message = (\n                    \"Refusing to make private recursively with more than \"\n                    \"%d objects. If you actually want to make every object \"\n                    \"in this bucket private, iterate through the blobs \"\n                    \"returned by 'Bucket.list_blobs()' and call \"\n                    \"'make_private' on each one.\"\n                ) % (self._MAX_OBJECTS_FOR_ITERATION,)\n                raise ValueError(message)\n\n            for blob in blobs:\n                blob.acl.all().revoke_read()\n                blob.acl.save(client=client, timeout=timeout)\n\n    def generate_upload_policy(self, conditions, expiration=None, client=None):\n        \"\"\"Create a signed upload policy for uploading objects.\n\n        This method generates and signs a policy document. You can use\n        [`policy documents`](https://cloud.google.com/storage/docs/xml-api/post-object-forms)\n        to allow visitors to a website to upload files to\n        Google Cloud Storage without giving them direct write access.\n        See a [code sample](https://cloud.google.com/storage/docs/xml-api/post-object-forms#python).\n\n        :type expiration: datetime\n        :param expiration: (Optional) Expiration in UTC. If not specified, the\n                           policy will expire in 1 hour.\n\n        :type conditions: list\n        :param conditions: A list of conditions as described in the\n                          `policy documents` documentation.\n\n        :type client: :class:`~google.cloud.storage.client.Client`\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :rtype: dict\n        :returns: A dictionary of (form field name, form field value) of form\n                  fields that should be added to your HTML upload form in order\n                  to attach the signature.\n        \"\"\"\n        client = self._require_client(client)\n        credentials = client._credentials\n        _signing.ensure_signed_credentials(credentials)\n\n        if expiration is None:\n            expiration = _NOW(_UTC).replace(tzinfo=None) + datetime.timedelta(hours=1)\n\n        conditions = conditions + [{\"bucket\": self.name}]\n\n        policy_document = {\n            \"expiration\": _datetime_to_rfc3339(expiration),\n            \"conditions\": conditions,\n        }\n\n        encoded_policy_document = base64.b64encode(\n            json.dumps(policy_document).encode(\"utf-8\")\n        )\n        signature = base64.b64encode(credentials.sign_bytes(encoded_policy_document))\n\n        fields = {\n            \"bucket\": self.name,\n            \"GoogleAccessId\": credentials.signer_email,\n            \"policy\": encoded_policy_document.decode(\"utf-8\"),\n            \"signature\": signature.decode(\"utf-8\"),\n        }\n\n        return fields\n\n    def lock_retention_policy(\n        self, client=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"Lock the bucket's retention policy.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :raises ValueError:\n            if the bucket has no metageneration (i.e., new or never reloaded);\n            if the bucket has no retention policy assigned;\n            if the bucket's retention policy is already locked.\n        \"\"\"\n        if \"metageneration\" not in self._properties:\n            raise ValueError(\"Bucket has no retention policy assigned: try 'reload'?\")\n\n        policy = self._properties.get(\"retentionPolicy\")\n\n        if policy is None:\n            raise ValueError(\"Bucket has no retention policy assigned: try 'reload'?\")\n\n        if policy.get(\"isLocked\"):\n            raise ValueError(\"Bucket's retention policy is already locked.\")\n\n        client = self._require_client(client)\n\n        query_params = {\"ifMetagenerationMatch\": self.metageneration}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = f\"/b/{self.name}/lockRetentionPolicy\"\n        api_response = client._post_resource(\n            path,\n            None,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n    def generate_signed_url(\n        self,\n        expiration=None,\n        api_access_endpoint=None,\n        method=\"GET\",\n        headers=None,\n        query_parameters=None,\n        client=None,\n        credentials=None,\n        version=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n    ):\n        \"\"\"Generates a signed URL for this bucket.\n\n        .. note::\n\n            If you are on Google Compute Engine, you can't generate a signed\n            URL using GCE service account. If you'd like to be able to generate\n            a signed URL from GCE, you can use a standard service account from a\n            JSON file rather than a GCE service account.\n\n        If you have a bucket that you want to allow access to for a set\n        amount of time, you can use this method to generate a URL that\n        is only valid within a certain time period.\n\n        If ``bucket_bound_hostname`` is set as an argument of :attr:`api_access_endpoint`,\n        ``https`` works only if using a ``CDN``.\n\n        :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n        :param expiration: Point in time when the signed URL should expire. If\n                           a ``datetime`` instance is passed without an explicit\n                           ``tzinfo`` set,  it will be assumed to be ``UTC``.\n\n        :type api_access_endpoint: str\n        :param api_access_endpoint: (Optional) URI base, for instance\n            \"https://storage.googleapis.com\". If not specified, the client's\n            api_endpoint will be used. Incompatible with bucket_bound_hostname.\n\n        :type method: str\n        :param method: The HTTP verb that will be used when requesting the URL.\n\n        :type headers: dict\n        :param headers:\n            (Optional) Additional HTTP headers to be included as part of the\n            signed URLs.  See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers\n            Requests using the signed URL *must* pass the specified header\n            (name and value) with each request for the URL.\n\n        :type query_parameters: dict\n        :param query_parameters:\n            (Optional) Additional query parameters to be included as part of the\n            signed URLs.  See:\n            https://cloud.google.com/storage/docs/xml-api/reference-headers#query\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: (Optional) The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :type credentials: :class:`google.auth.credentials.Credentials` or\n                           :class:`NoneType`\n        :param credentials: The authorization credentials to attach to requests.\n                            These credentials identify this application to the service.\n                            If none are specified, the client will attempt to ascertain\n                            the credentials from the environment.\n\n        :type version: str\n        :param version: (Optional) The version of signed credential to create.\n                        Must be one of 'v2' | 'v4'.\n\n        :type virtual_hosted_style: bool\n        :param virtual_hosted_style:\n            (Optional) If true, then construct the URL relative the bucket's\n            virtual hostname, e.g., '<bucket-name>.storage.googleapis.com'.\n            Incompatible with bucket_bound_hostname.\n\n        :type bucket_bound_hostname: str\n        :param bucket_bound_hostname:\n            (Optional) If passed, then construct the URL relative to the bucket-bound hostname.\n            Value can be a bare or with scheme, e.g., 'example.com' or 'http://example.com'.\n            Incompatible with api_access_endpoint and virtual_hosted_style.\n            See: https://cloud.google.com/storage/docs/request-endpoints#cname\n\n        :type scheme: str\n        :param scheme:\n            (Optional) If ``bucket_bound_hostname`` is passed as a bare hostname, use\n            this value as the scheme.  ``https`` will work only when using a CDN.\n            Defaults to ``\"http\"``.\n\n        :raises: :exc:`ValueError` when version is invalid or mutually exclusive arguments are used.\n        :raises: :exc:`TypeError` when expiration is not a valid type.\n        :raises: :exc:`AttributeError` if credentials is not an instance\n                of :class:`google.auth.credentials.Signing`.\n\n        :rtype: str\n        :returns: A signed URL you can use to access the resource\n                  until expiration.\n        \"\"\"\n        if version is None:\n            version = \"v2\"\n        elif version not in (\"v2\", \"v4\"):\n            raise ValueError(\"'version' must be either 'v2' or 'v4'\")\n\n        if (\n            api_access_endpoint is not None or virtual_hosted_style\n        ) and bucket_bound_hostname:\n            raise ValueError(\n                \"The bucket_bound_hostname argument is not compatible with \"\n                \"either api_access_endpoint or virtual_hosted_style.\"\n            )\n\n        if api_access_endpoint is None:\n            client = self._require_client(client)\n            api_access_endpoint = client.api_endpoint\n\n        # If you are on Google Compute Engine, you can't generate a signed URL\n        # using GCE service account.\n        # See https://github.com/googleapis/google-auth-library-python/issues/50\n        if virtual_hosted_style:\n            api_access_endpoint = _virtual_hosted_style_base_url(\n                api_access_endpoint, self.name\n            )\n            resource = \"/\"\n        elif bucket_bound_hostname:\n            api_access_endpoint = _bucket_bound_hostname_url(\n                bucket_bound_hostname, scheme\n            )\n            resource = \"/\"\n        else:\n            resource = f\"/{self.name}\"\n\n        if credentials is None:\n            client = self._require_client(client)  # May be redundant, but that's ok.\n            credentials = client._credentials\n\n        if version == \"v2\":\n            helper = generate_signed_url_v2\n        else:\n            helper = generate_signed_url_v4\n\n        return helper(\n            credentials,\n            resource=resource,\n            expiration=expiration,\n            api_access_endpoint=api_access_endpoint,\n            method=method.upper(),\n            headers=headers,\n            query_parameters=query_parameters,\n        )\n\n\nclass SoftDeletePolicy(dict):\n    \"\"\"Map a bucket's soft delete policy.\n\n    See https://cloud.google.com/storage/docs/soft-delete\n\n    :type bucket: :class:`Bucket`\n    :param bucket: Bucket for which this instance is the policy.\n\n    :type retention_duration_seconds: int\n    :param retention_duration_seconds:\n        (Optional) The period of time in seconds that soft-deleted objects in the bucket\n        will be retained and cannot be permanently deleted.\n\n    :type effective_time: :class:`datetime.datetime`\n    :param effective_time:\n        (Optional) When the bucket's soft delete policy is effective.\n        This value should normally only be set by the back-end API.\n    \"\"\"\n\n    def __init__(self, bucket, **kw):\n        data = {}\n        retention_duration_seconds = kw.get(\"retention_duration_seconds\")\n        data[\"retentionDurationSeconds\"] = retention_duration_seconds\n\n        effective_time = kw.get(\"effective_time\")\n        if effective_time is not None:\n            effective_time = _datetime_to_rfc3339(effective_time)\n        data[\"effectiveTime\"] = effective_time\n\n        super().__init__(data)\n        self._bucket = bucket\n\n    @classmethod\n    def from_api_repr(cls, resource, bucket):\n        \"\"\"Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :type bucket: :class:`Bucket`\n        :params bucket: Bucket for which this instance is the policy.\n\n        :rtype: :class:`SoftDeletePolicy`\n        :returns: Instance created from resource.\n        \"\"\"\n        instance = cls(bucket)\n        instance.update(resource)\n        return instance\n\n    @property\n    def bucket(self):\n        \"\"\"Bucket for which this instance is the policy.\n\n        :rtype: :class:`Bucket`\n        :returns: the instance's bucket.\n        \"\"\"\n        return self._bucket\n\n    @property\n    def retention_duration_seconds(self):\n        \"\"\"Get the retention duration of the bucket's soft delete policy.\n\n        :rtype: int or ``NoneType``\n        :returns: The period of time in seconds that soft-deleted objects in the bucket\n                  will be retained and cannot be permanently deleted; Or ``None`` if the\n                  property is not set.\n        \"\"\"\n        duration = self.get(\"retentionDurationSeconds\")\n        if duration is not None:\n            return int(duration)\n\n    @retention_duration_seconds.setter\n    def retention_duration_seconds(self, value):\n        \"\"\"Set the retention duration of the bucket's soft delete policy.\n\n        :type value: int\n        :param value:\n            The period of time in seconds that soft-deleted objects in the bucket\n            will be retained and cannot be permanently deleted.\n        \"\"\"\n        self[\"retentionDurationSeconds\"] = value\n        self.bucket._patch_property(\"softDeletePolicy\", self)\n\n    @property\n    def effective_time(self):\n        \"\"\"Get the effective time of the bucket's soft delete policy.\n\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's soft delte policy is\n                  effective, or ``None`` if the property is not set.\n        \"\"\"\n        timestamp = self.get(\"effectiveTime\")\n        if timestamp is not None:\n            return _rfc3339_nanos_to_datetime(timestamp)\n\n\ndef _raise_if_len_differs(expected_len, **generation_match_args):\n    \"\"\"\n    Raise an error if any generation match argument\n    is set and its len differs from the given value.\n\n    :type expected_len: int\n    :param expected_len: Expected argument length in case it's set.\n\n    :type generation_match_args: dict\n    :param generation_match_args: Lists, which length must be checked.\n\n    :raises: :exc:`ValueError` if any argument set, but has an unexpected length.\n    \"\"\"\n    for name, value in generation_match_args.items():\n        if value is not None and len(value) != expected_len:\n            raise ValueError(f\"'{name}' length must be the same as 'blobs' length\")\n", "google/cloud/storage/client.py": "# Copyright 2015 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Client for interacting with the Google Cloud Storage API.\"\"\"\n\nimport base64\nimport binascii\nimport collections\nimport datetime\nimport functools\nimport json\nimport warnings\nimport google.api_core.client_options\n\nfrom google.auth.credentials import AnonymousCredentials\n\nfrom google.api_core import page_iterator\nfrom google.cloud._helpers import _LocalStack\nfrom google.cloud.client import ClientWithProject\nfrom google.cloud.exceptions import NotFound\n\nfrom google.cloud.storage._helpers import _bucket_bound_hostname_url\nfrom google.cloud.storage._helpers import _get_api_endpoint_override\nfrom google.cloud.storage._helpers import _get_environ_project\nfrom google.cloud.storage._helpers import _get_storage_emulator_override\nfrom google.cloud.storage._helpers import _use_client_cert\nfrom google.cloud.storage._helpers import _virtual_hosted_style_base_url\nfrom google.cloud.storage._helpers import _DEFAULT_UNIVERSE_DOMAIN\nfrom google.cloud.storage._helpers import _DEFAULT_SCHEME\nfrom google.cloud.storage._helpers import _STORAGE_HOST_TEMPLATE\nfrom google.cloud.storage._helpers import _NOW\nfrom google.cloud.storage._helpers import _UTC\n\nfrom google.cloud.storage._http import Connection\nfrom google.cloud.storage._signing import (\n    get_expiration_seconds_v4,\n    get_v4_now_dtstamps,\n    ensure_signed_credentials,\n    _sign_message,\n)\nfrom google.cloud.storage.batch import Batch\nfrom google.cloud.storage.bucket import Bucket, _item_to_blob, _blobs_page_start\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.hmac_key import HMACKeyMetadata\nfrom google.cloud.storage.acl import BucketACL\nfrom google.cloud.storage.acl import DefaultObjectACL\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\n\n\n_marker = object()\n\n\nclass Client(ClientWithProject):\n    \"\"\"Client to bundle configuration needed for API requests.\n\n    :type project: str or None\n    :param project: the project which the client acts on behalf of. Will be\n                    passed when creating a topic.  If not passed,\n                    falls back to the default inferred from the environment.\n\n    :type credentials: :class:`~google.auth.credentials.Credentials`\n    :param credentials: (Optional) The OAuth2 Credentials to use for this\n                        client. If not passed (and if no ``_http`` object is\n                        passed), falls back to the default inferred from the\n                        environment.\n\n    :type _http: :class:`~requests.Session`\n    :param _http: (Optional) HTTP object to make requests. Can be any object\n                  that defines ``request()`` with the same interface as\n                  :meth:`requests.Session.request`. If not passed, an\n                  ``_http`` object is created that is bound to the\n                  ``credentials`` for the current object.\n                  This parameter should be considered private, and could\n                  change in the future.\n\n    :type client_info: :class:`~google.api_core.client_info.ClientInfo`\n    :param client_info:\n        The client info used to send a user-agent string along with API\n        requests. If ``None``, then default info will be used. Generally,\n        you only need to set this if you're developing your own library\n        or partner tool.\n\n    :type client_options: :class:`~google.api_core.client_options.ClientOptions` or :class:`dict`\n    :param client_options: (Optional) Client options used to set user options on the client.\n        A non-default universe domain or api endpoint should be set through client_options.\n\n    :type use_auth_w_custom_endpoint: bool\n    :param use_auth_w_custom_endpoint:\n        (Optional) Whether authentication is required under custom endpoints.\n        If false, uses AnonymousCredentials and bypasses authentication.\n        Defaults to True. Note this is only used when a custom endpoint is set in conjunction.\n\n    :type extra_headers: dict\n    :param extra_headers:\n        (Optional) Custom headers to be sent with the requests attached to the client.\n        For example, you can add custom audit logging headers.\n    \"\"\"\n\n    SCOPE = (\n        \"https://www.googleapis.com/auth/devstorage.full_control\",\n        \"https://www.googleapis.com/auth/devstorage.read_only\",\n        \"https://www.googleapis.com/auth/devstorage.read_write\",\n    )\n    \"\"\"The scopes required for authenticating as a Cloud Storage consumer.\"\"\"\n\n    def __init__(\n        self,\n        project=_marker,\n        credentials=None,\n        _http=None,\n        client_info=None,\n        client_options=None,\n        use_auth_w_custom_endpoint=True,\n        extra_headers={},\n    ):\n        self._base_connection = None\n\n        if project is None:\n            no_project = True\n            project = \"<none>\"\n        else:\n            no_project = False\n\n        if project is _marker:\n            project = None\n\n        # Save the initial value of constructor arguments before they\n        # are passed along, for use in __reduce__ defined elsewhere.\n        self._initial_client_info = client_info\n        self._initial_client_options = client_options\n        self._extra_headers = extra_headers\n\n        connection_kw_args = {\"client_info\": client_info}\n\n        if client_options:\n            if isinstance(client_options, dict):\n                client_options = google.api_core.client_options.from_dict(\n                    client_options\n                )\n\n        if client_options and client_options.universe_domain:\n            self._universe_domain = client_options.universe_domain\n        else:\n            self._universe_domain = None\n\n        storage_emulator_override = _get_storage_emulator_override()\n        api_endpoint_override = _get_api_endpoint_override()\n\n        # Determine the api endpoint. The rules are as follows:\n\n        # 1. If the `api_endpoint` is set in `client_options`, use that as the\n        #    endpoint.\n        if client_options and client_options.api_endpoint:\n            api_endpoint = client_options.api_endpoint\n\n        # 2. Elif the \"STORAGE_EMULATOR_HOST\" env var is set, then use that as the\n        #    endpoint.\n        elif storage_emulator_override:\n            api_endpoint = storage_emulator_override\n\n        # 3. Elif the \"API_ENDPOINT_OVERRIDE\" env var is set, then use that as the\n        #    endpoint.\n        elif api_endpoint_override:\n            api_endpoint = api_endpoint_override\n\n        # 4. Elif the `universe_domain` is set in `client_options`,\n        #    create the endpoint using that as the default.\n        #\n        #    Mutual TLS is not compatible with a non-default universe domain\n        #    at this time. If such settings are enabled along with the\n        #    \"GOOGLE_API_USE_CLIENT_CERTIFICATE\" env variable, a ValueError will\n        #    be raised.\n\n        elif self._universe_domain:\n            # The final decision of whether to use mTLS takes place in\n            # google-auth-library-python. We peek at the environment variable\n            # here only to issue an exception in case of a conflict.\n            if _use_client_cert():\n                raise ValueError(\n                    'The \"GOOGLE_API_USE_CLIENT_CERTIFICATE\" env variable is '\n                    'set to \"true\" and a non-default universe domain is '\n                    \"configured. mTLS is not supported in any universe other than\"\n                    \"googleapis.com.\"\n                )\n            api_endpoint = _DEFAULT_SCHEME + _STORAGE_HOST_TEMPLATE.format(\n                universe_domain=self._universe_domain\n            )\n\n        # 5. Else, use the default, which is to use the default\n        #    universe domain of \"googleapis.com\" and create the endpoint\n        #    \"storage.googleapis.com\" from that.\n        else:\n            api_endpoint = None\n\n        connection_kw_args[\"api_endpoint\"] = api_endpoint\n\n        self._is_emulator_set = True if storage_emulator_override else False\n\n        # If a custom endpoint is set, the client checks for credentials\n        # or finds the default credentials based on the current environment.\n        # Authentication may be bypassed under certain conditions:\n        # (1) STORAGE_EMULATOR_HOST is set (for backwards compatibility), OR\n        # (2) use_auth_w_custom_endpoint is set to False.\n        if connection_kw_args[\"api_endpoint\"] is not None:\n            if self._is_emulator_set or not use_auth_w_custom_endpoint:\n                if credentials is None:\n                    credentials = AnonymousCredentials()\n                if project is None:\n                    project = _get_environ_project()\n                if project is None:\n                    no_project = True\n                    project = \"<none>\"\n\n        super(Client, self).__init__(\n            project=project,\n            credentials=credentials,\n            client_options=client_options,\n            _http=_http,\n        )\n\n        # Validate that the universe domain of the credentials matches the\n        # universe domain of the client.\n        if self._credentials.universe_domain != self.universe_domain:\n            raise ValueError(\n                \"The configured universe domain ({client_ud}) does not match \"\n                \"the universe domain found in the credentials ({cred_ud}). If \"\n                \"you haven't configured the universe domain explicitly, \"\n                \"`googleapis.com` is the default.\".format(\n                    client_ud=self.universe_domain,\n                    cred_ud=self._credentials.universe_domain,\n                )\n            )\n\n        if no_project:\n            self.project = None\n\n        # Pass extra_headers to Connection\n        connection = Connection(self, **connection_kw_args)\n        connection.extra_headers = extra_headers\n        self._connection = connection\n        self._batch_stack = _LocalStack()\n\n    @classmethod\n    def create_anonymous_client(cls):\n        \"\"\"Factory: return client with anonymous credentials.\n\n        .. note::\n\n           Such a client has only limited access to \"public\" buckets:\n           listing their contents and downloading their blobs.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: Instance w/ anonymous credentials and no project.\n        \"\"\"\n        client = cls(project=\"<none>\", credentials=AnonymousCredentials())\n        client.project = None\n        return client\n\n    @property\n    def universe_domain(self):\n        return self._universe_domain or _DEFAULT_UNIVERSE_DOMAIN\n\n    @property\n    def api_endpoint(self):\n        return self._connection.API_BASE_URL\n\n    @property\n    def _connection(self):\n        \"\"\"Get connection or batch on the client.\n\n        :rtype: :class:`google.cloud.storage._http.Connection`\n        :returns: The connection set on the client, or the batch\n                  if one is set.\n        \"\"\"\n        if self.current_batch is not None:\n            return self.current_batch\n        else:\n            return self._base_connection\n\n    @_connection.setter\n    def _connection(self, value):\n        \"\"\"Set connection on the client.\n\n        Intended to be used by constructor (since the base class calls)\n            self._connection = connection\n        Will raise if the connection is set more than once.\n\n        :type value: :class:`google.cloud.storage._http.Connection`\n        :param value: The connection set on the client.\n\n        :raises: :class:`ValueError` if connection has already been set.\n        \"\"\"\n        if self._base_connection is not None:\n            raise ValueError(\"Connection already set on client\")\n        self._base_connection = value\n\n    def _push_batch(self, batch):\n        \"\"\"Push a batch onto our stack.\n\n        \"Protected\", intended for use by batch context mgrs.\n\n        :type batch: :class:`google.cloud.storage.batch.Batch`\n        :param batch: newly-active batch\n        \"\"\"\n        self._batch_stack.push(batch)\n\n    def _pop_batch(self):\n        \"\"\"Pop a batch from our stack.\n\n        \"Protected\", intended for use by batch context mgrs.\n\n        :raises: IndexError if the stack is empty.\n        :rtype: :class:`google.cloud.storage.batch.Batch`\n        :returns: the top-most batch/transaction, after removing it.\n        \"\"\"\n        return self._batch_stack.pop()\n\n    @property\n    def current_batch(self):\n        \"\"\"Currently-active batch.\n\n        :rtype: :class:`google.cloud.storage.batch.Batch` or ``NoneType`` (if\n                no batch is active).\n        :returns: The batch at the top of the batch stack.\n        \"\"\"\n        return self._batch_stack.top\n\n    def get_service_account_email(\n        self, project=None, timeout=_DEFAULT_TIMEOUT, retry=DEFAULT_RETRY\n    ):\n        \"\"\"Get the email address of the project's GCS service account\n\n        :type project: str\n        :param project:\n            (Optional) Project ID to use for retreiving GCS service account\n            email address.  Defaults to the client's project.\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: str\n        :returns: service account email address\n        \"\"\"\n        if project is None:\n            project = self.project\n\n        path = f\"/projects/{project}/serviceAccount\"\n        api_response = self._get_resource(path, timeout=timeout, retry=retry)\n        return api_response[\"email_address\"]\n\n    def bucket(self, bucket_name, user_project=None):\n        \"\"\"Factory constructor for bucket object.\n\n        .. note::\n          This will not make an HTTP request; it simply instantiates\n          a bucket object owned by this client.\n\n        :type bucket_name: str\n        :param bucket_name: The name of the bucket to be instantiated.\n\n        :type user_project: str\n        :param user_project: (Optional) The project ID to be billed for API\n                             requests made via the bucket.\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket`\n        :returns: The bucket object created.\n        \"\"\"\n        return Bucket(client=self, name=bucket_name, user_project=user_project)\n\n    def batch(self, raise_exception=True):\n        \"\"\"Factory constructor for batch object.\n\n        .. note::\n          This will not make an HTTP request; it simply instantiates\n          a batch object owned by this client.\n\n        :type raise_exception: bool\n        :param raise_exception:\n            (Optional) Defaults to True. If True, instead of adding exceptions\n            to the list of return responses, the final exception will be raised.\n            Note that exceptions are unwrapped after all operations are complete\n            in success or failure, and only the last exception is raised.\n\n        :rtype: :class:`google.cloud.storage.batch.Batch`\n        :returns: The batch object created.\n        \"\"\"\n        return Batch(client=self, raise_exception=raise_exception)\n\n    def _get_resource(\n        self,\n        path,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'GET' calls.\n\n        Args:\n            path str:\n                The path of the resource to fetch.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"GET\",\n            path=path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _list_resource(\n        self,\n        path,\n        item_to_value,\n        page_token=None,\n        max_results=None,\n        extra_params=None,\n        page_start=page_iterator._do_nothing_page_start,\n        page_size=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        api_request = functools.partial(\n            self._connection.api_request, timeout=timeout, retry=retry\n        )\n        return page_iterator.HTTPIterator(\n            client=self,\n            api_request=api_request,\n            path=path,\n            item_to_value=item_to_value,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_start=page_start,\n            page_size=page_size,\n        )\n\n    def _patch_resource(\n        self,\n        path,\n        data,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'PATCH' calls.\n\n        Args:\n            path str:\n                The path of the resource to fetch.\n\n            data dict:\n                The data to be patched.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"PATCH\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _put_resource(\n        self,\n        path,\n        data,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'PUT' calls.\n\n        Args:\n            path str:\n                The path of the resource to fetch.\n\n            data dict:\n                The data to be patched.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"PUT\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _post_resource(\n        self,\n        path,\n        data,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'POST' calls.\n\n        Args:\n            path str:\n                The path of the resource to which to post.\n\n            data dict:\n                The data to be posted.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource returned from the post.\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n\n        return self._connection.api_request(\n            method=\"POST\",\n            path=path,\n            data=data,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _delete_resource(\n        self,\n        path,\n        query_params=None,\n        headers=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        _target_object=None,\n    ):\n        \"\"\"Helper for bucket / blob methods making API 'DELETE' calls.\n\n        Args:\n            path str:\n                The path of the resource to delete.\n\n            query_params Optional[dict]:\n                HTTP query parameters to be passed\n\n            headers Optional[dict]:\n                HTTP headers to be passed\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            _target_object (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                :class:`~google.cloud.storage.bucket.blob`, \\\n            ]):\n                Object to which future data is to be applied -- only relevant\n                in the context of a batch.\n\n        Returns:\n            dict\n                The JSON resource fetched\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        return self._connection.api_request(\n            method=\"DELETE\",\n            path=path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=_target_object,\n        )\n\n    def _bucket_arg_to_bucket(self, bucket_or_name):\n        \"\"\"Helper to return given bucket or create new by name.\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n\n        Returns:\n            google.cloud.storage.bucket.Bucket\n                The newly created bucket or the given one.\n        \"\"\"\n        if isinstance(bucket_or_name, Bucket):\n            bucket = bucket_or_name\n            if bucket.client is None:\n                bucket._client = self\n        else:\n            bucket = Bucket(self, name=bucket_or_name)\n        return bucket\n\n    def get_bucket(\n        self,\n        bucket_or_name,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Retrieve a bucket via a GET request.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/get) and a [code sample](https://cloud.google.com/storage/docs/samples/storage-get-bucket-metadata#storage_get_bucket_metadata-python).\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            if_metageneration_match (Optional[long]):\n                Make the operation conditional on whether the\n                blob's current metageneration matches the given value.\n\n            if_metageneration_not_match (Optional[long]):\n                Make the operation conditional on whether the blob's\n                current metageneration does not match the given value.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n        Returns:\n            google.cloud.storage.bucket.Bucket\n                The bucket matching the name provided.\n\n        Raises:\n            google.cloud.exceptions.NotFound\n                If the bucket is not found.\n        \"\"\"\n        bucket = self._bucket_arg_to_bucket(bucket_or_name)\n        bucket.reload(\n            client=self,\n            timeout=timeout,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            retry=retry,\n        )\n        return bucket\n\n    def lookup_bucket(\n        self,\n        bucket_name,\n        timeout=_DEFAULT_TIMEOUT,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Get a bucket by name, returning None if not found.\n\n        You can use this if you would rather check for a None value\n        than catching a NotFound exception.\n\n        :type bucket_name: str\n        :param bucket_name: The name of the bucket to get.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match: (Optional) Make the operation conditional on whether the\n                                        blob's current metageneration matches the given value.\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match: (Optional) Make the operation conditional on whether the\n                                            blob's current metageneration does not match the given value.\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket` or ``NoneType``\n        :returns: The bucket matching the name provided or None if not found.\n        \"\"\"\n        try:\n            return self.get_bucket(\n                bucket_name,\n                timeout=timeout,\n                if_metageneration_match=if_metageneration_match,\n                if_metageneration_not_match=if_metageneration_not_match,\n                retry=retry,\n            )\n        except NotFound:\n            return None\n\n    def create_bucket(\n        self,\n        bucket_or_name,\n        requester_pays=None,\n        project=None,\n        user_project=None,\n        location=None,\n        data_locations=None,\n        predefined_acl=None,\n        predefined_default_object_acl=None,\n        enable_object_retention=False,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Create a new bucket via a POST request.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/insert) and a [code sample](https://cloud.google.com/storage/docs/samples/storage-create-bucket#storage_create_bucket-python).\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n            requester_pays (bool):\n                DEPRECATED. Use Bucket().requester_pays instead.\n                (Optional) Whether requester pays for API requests for\n                this bucket and its blobs.\n            project (str):\n                (Optional) The project under which the bucket is to be created.\n                If not passed, uses the project set on the client.\n            user_project (str):\n                (Optional) The project ID to be billed for API requests\n                made via created bucket.\n            location (str):\n                (Optional) The location of the bucket. If not passed,\n                the default location, US, will be used. If specifying a dual-region,\n                `data_locations` should be set in conjunction. See:\n                https://cloud.google.com/storage/docs/locations\n            data_locations (list of str):\n                (Optional) The list of regional locations of a custom dual-region bucket.\n                Dual-regions require exactly 2 regional locations. See:\n                https://cloud.google.com/storage/docs/locations\n            predefined_acl (str):\n                (Optional) Name of predefined ACL to apply to bucket. See:\n                https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n            predefined_default_object_acl (str):\n                (Optional) Name of predefined ACL to apply to bucket's objects. See:\n                https://cloud.google.com/storage/docs/access-control/lists#predefined-acl\n            enable_object_retention (bool):\n                (Optional) Whether object retention should be enabled on this bucket. See:\n                https://cloud.google.com/storage/docs/object-lock\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n        Returns:\n            google.cloud.storage.bucket.Bucket\n                The newly created bucket.\n\n        Raises:\n            google.cloud.exceptions.Conflict\n                If the bucket already exists.\n        \"\"\"\n        bucket = self._bucket_arg_to_bucket(bucket_or_name)\n        query_params = {}\n\n        if project is None:\n            project = self.project\n\n        # Use no project if STORAGE_EMULATOR_HOST is set\n        if self._is_emulator_set:\n            if project is None:\n                project = _get_environ_project()\n            if project is None:\n                project = \"<none>\"\n\n        # Only include the project parameter if a project is set.\n        # If a project is not set, falls back to API validation (BadRequest).\n        if project is not None:\n            query_params = {\"project\": project}\n\n        if requester_pays is not None:\n            warnings.warn(\n                \"requester_pays arg is deprecated. Use Bucket().requester_pays instead.\",\n                PendingDeprecationWarning,\n                stacklevel=1,\n            )\n            bucket.requester_pays = requester_pays\n\n        if predefined_acl is not None:\n            predefined_acl = BucketACL.validate_predefined(predefined_acl)\n            query_params[\"predefinedAcl\"] = predefined_acl\n\n        if predefined_default_object_acl is not None:\n            predefined_default_object_acl = DefaultObjectACL.validate_predefined(\n                predefined_default_object_acl\n            )\n            query_params[\"predefinedDefaultObjectAcl\"] = predefined_default_object_acl\n\n        if user_project is not None:\n            query_params[\"userProject\"] = user_project\n\n        if enable_object_retention:\n            query_params[\"enableObjectRetention\"] = enable_object_retention\n\n        properties = {key: bucket._properties[key] for key in bucket._changes}\n        properties[\"name\"] = bucket.name\n\n        if location is not None:\n            properties[\"location\"] = location\n\n        if data_locations is not None:\n            properties[\"customPlacementConfig\"] = {\"dataLocations\": data_locations}\n\n        api_response = self._post_resource(\n            \"/b\",\n            properties,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=bucket,\n        )\n\n        bucket._set_properties(api_response)\n        return bucket\n\n    def download_blob_to_file(\n        self,\n        blob_or_uri,\n        file_obj,\n        start=None,\n        end=None,\n        raw_download=False,\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        checksum=\"md5\",\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Download the contents of a blob object or blob URI into a file-like object.\n\n        See https://cloud.google.com/storage/docs/downloading-objects\n\n        Args:\n            blob_or_uri (Union[ \\\n            :class:`~google.cloud.storage.blob.Blob`, \\\n             str, \\\n            ]):\n                The blob resource to pass or URI to download.\n\n            file_obj (file):\n                A file handle to which to write the blob's data.\n\n            start (int):\n                (Optional) The first byte in a range to be downloaded.\n\n            end (int):\n                (Optional) The last byte in a range to be downloaded.\n\n            raw_download (bool):\n                (Optional) If true, download the object without any expansion.\n\n            if_etag_match (Union[str, Set[str]]):\n                (Optional) See :ref:`using-if-etag-match`\n\n            if_etag_not_match (Union[str, Set[str]]):\n                (Optional) See :ref:`using-if-etag-not-match`\n\n            if_generation_match (long):\n                (Optional) See :ref:`using-if-generation-match`\n\n            if_generation_not_match (long):\n                (Optional) See :ref:`using-if-generation-not-match`\n\n            if_metageneration_match (long):\n                (Optional) See :ref:`using-if-metageneration-match`\n\n            if_metageneration_not_match (long):\n                (Optional) See :ref:`using-if-metageneration-not-match`\n\n            timeout ([Union[float, Tuple[float, float]]]):\n                (Optional) The amount of time, in seconds, to wait\n                for the server response.  See: :ref:`configuring_timeouts`\n\n            checksum (str):\n                (Optional) The type of checksum to compute to verify the integrity\n                of the object. The response headers must contain a checksum of the\n                requested type. If the headers lack an appropriate checksum (for\n                instance in the case of transcoded or ranged downloads where the\n                remote service does not know the correct checksum, including\n                downloads where chunk_size is set) an INFO-level log will be\n                emitted. Supported values are \"md5\", \"crc32c\" and None. The default\n                is \"md5\".\n            retry (google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy)\n                (Optional) How to retry the RPC. A None value will disable\n                retries. A google.api_core.retry.Retry value will enable retries,\n                and the object will define retriable response codes and errors and\n                configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a\n                Retry object and activates it only if certain conditions are met.\n                This class exists to provide safe defaults for RPC calls that are\n                not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a\n                condition such as if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package\n                (google.cloud.storage.retry) for information on retry types and how\n                to configure them.\n\n                Media operations (downloads and uploads) do not support non-default\n                predicates in a Retry object. The default will always be used. Other\n                configuration changes for Retry objects such as delays and deadlines\n                are respected.\n        \"\"\"\n\n        if not isinstance(blob_or_uri, Blob):\n            blob_or_uri = Blob.from_string(blob_or_uri)\n\n        blob_or_uri._prep_and_do_download(\n            file_obj,\n            client=self,\n            start=start,\n            end=end,\n            raw_download=raw_download,\n            if_etag_match=if_etag_match,\n            if_etag_not_match=if_etag_not_match,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n            timeout=timeout,\n            checksum=checksum,\n            retry=retry,\n        )\n\n    def list_blobs(\n        self,\n        bucket_or_name,\n        max_results=None,\n        page_token=None,\n        prefix=None,\n        delimiter=None,\n        start_offset=None,\n        end_offset=None,\n        include_trailing_delimiter=None,\n        versions=None,\n        projection=\"noAcl\",\n        fields=None,\n        page_size=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        match_glob=None,\n        include_folders_as_prefixes=None,\n        soft_deleted=None,\n    ):\n        \"\"\"Return an iterator used to find blobs in the bucket.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        .. note::\n          List prefixes (directories) in a bucket using a prefix and delimiter.\n          See a [code sample](https://cloud.google.com/storage/docs/samples/storage-list-files-with-prefix#storage_list_files_with_prefix-python)\n          listing objects using a prefix filter.\n\n        Args:\n            bucket_or_name (Union[ \\\n                :class:`~google.cloud.storage.bucket.Bucket`, \\\n                 str, \\\n            ]):\n                The bucket resource to pass or name to create.\n\n            max_results (int):\n                (Optional) The maximum number of blobs to return.\n\n            page_token (str):\n                (Optional) If present, return the next batch of blobs, using the\n                value, which must correspond to the ``nextPageToken`` value\n                returned in the previous response.  Deprecated: use the ``pages``\n                property of the returned iterator instead of manually passing the\n                token.\n\n            prefix (str):\n                (Optional) Prefix used to filter blobs.\n\n            delimiter (str):\n                (Optional) Delimiter, used with ``prefix`` to\n                emulate hierarchy.\n\n            start_offset (str):\n                (Optional) Filter results to objects whose names are\n                lexicographically equal to or after ``startOffset``. If\n                ``endOffset`` is also set, the objects listed will have names\n                between ``startOffset`` (inclusive) and ``endOffset``\n                (exclusive).\n\n            end_offset (str):\n                (Optional) Filter results to objects whose names are\n                lexicographically before ``endOffset``. If ``startOffset`` is\n                also set, the objects listed will have names between\n                ``startOffset`` (inclusive) and ``endOffset`` (exclusive).\n\n            include_trailing_delimiter (boolean):\n                (Optional) If true, objects that end in exactly one instance of\n                ``delimiter`` will have their metadata included in ``items`` in\n                addition to ``prefixes``.\n\n            versions (bool):\n                (Optional) Whether object versions should be returned\n                as separate blobs.\n\n            projection (str):\n                (Optional) If used, must be 'full' or 'noAcl'.\n                Defaults to ``'noAcl'``. Specifies the set of\n                properties to return.\n\n            fields (str):\n                (Optional) Selector specifying which fields to include\n                in a partial response. Must be a list of fields. For\n                example to get a partial response with just the next\n                page token and the name and language of each blob returned:\n                ``'items(name,contentLanguage),nextPageToken'``.\n                See: https://cloud.google.com/storage/docs/json_api/v1/parameters#fields\n\n            page_size (int):\n                (Optional) Maximum number of blobs to return in each page.\n                Defaults to a value set by the API.\n\n            timeout (Optional[Union[float, Tuple[float, float]]]):\n                The amount of time, in seconds, to wait for the server response.\n\n                Can also be passed as a tuple (connect_timeout, read_timeout).\n                See :meth:`requests.Session.request` documentation for details.\n\n            retry (Optional[Union[google.api_core.retry.Retry, google.cloud.storage.retry.ConditionalRetryPolicy]]):\n                How to retry the RPC. A None value will disable retries.\n                A google.api_core.retry.Retry value will enable retries, and the object will\n                define retriable response codes and errors and configure backoff and timeout options.\n\n                A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n                activates it only if certain conditions are met. This class exists to provide safe defaults\n                for RPC calls that are not technically safe to retry normally (due to potential data\n                duplication or other side-effects) but become safe to retry if a condition such as\n                if_metageneration_match is set.\n\n                See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n                information on retry types and how to configure them.\n\n            match_glob (str):\n                (Optional) A glob pattern used to filter results (for example, foo*bar).\n                The string value must be UTF-8 encoded. See:\n                https://cloud.google.com/storage/docs/json_api/v1/objects/list#list-object-glob\n\n            include_folders_as_prefixes (bool):\n                (Optional) If true, includes Folders and Managed Folders in the set of\n                ``prefixes`` returned by the query. Only applicable if ``delimiter`` is set to /.\n                See: https://cloud.google.com/storage/docs/managed-folders\n\n            soft_deleted (bool):\n                (Optional) If true, only soft-deleted objects will be listed as distinct results in order of increasing\n                generation number. This parameter can only be used successfully if the bucket has a soft delete policy.\n                Note ``soft_deleted`` and ``versions`` cannot be set to True simultaneously. See:\n                https://cloud.google.com/storage/docs/soft-delete\n\n        Returns:\n            Iterator of all :class:`~google.cloud.storage.blob.Blob`\n            in this bucket matching the arguments. The RPC call\n            returns a response when the iterator is consumed.\n\n            As part of the response, you'll also get back an iterator.prefixes entity that lists object names\n            up to and including the requested delimiter. Duplicate entries are omitted from this list.\n        \"\"\"\n        bucket = self._bucket_arg_to_bucket(bucket_or_name)\n\n        extra_params = {\"projection\": projection}\n\n        if prefix is not None:\n            extra_params[\"prefix\"] = prefix\n\n        if delimiter is not None:\n            extra_params[\"delimiter\"] = delimiter\n\n        if match_glob is not None:\n            extra_params[\"matchGlob\"] = match_glob\n\n        if start_offset is not None:\n            extra_params[\"startOffset\"] = start_offset\n\n        if end_offset is not None:\n            extra_params[\"endOffset\"] = end_offset\n\n        if include_trailing_delimiter is not None:\n            extra_params[\"includeTrailingDelimiter\"] = include_trailing_delimiter\n\n        if versions is not None:\n            extra_params[\"versions\"] = versions\n\n        if fields is not None:\n            extra_params[\"fields\"] = fields\n\n        if include_folders_as_prefixes is not None:\n            extra_params[\"includeFoldersAsPrefixes\"] = include_folders_as_prefixes\n\n        if soft_deleted is not None:\n            extra_params[\"softDeleted\"] = soft_deleted\n\n        if bucket.user_project is not None:\n            extra_params[\"userProject\"] = bucket.user_project\n\n        path = bucket.path + \"/o\"\n        iterator = self._list_resource(\n            path,\n            _item_to_blob,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_start=_blobs_page_start,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n        iterator.bucket = bucket\n        iterator.prefixes = set()\n        return iterator\n\n    def list_buckets(\n        self,\n        max_results=None,\n        page_token=None,\n        prefix=None,\n        projection=\"noAcl\",\n        fields=None,\n        project=None,\n        page_size=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"Get all buckets in the project associated to the client.\n\n        This will not populate the list of blobs available in each\n        bucket.\n\n        See [API reference docs](https://cloud.google.com/storage/docs/json_api/v1/buckets/list) and a [code sample](https://cloud.google.com/storage/docs/samples/storage-list-buckets#storage_list_buckets-python).\n\n        :type max_results: int\n        :param max_results: (Optional) The maximum number of buckets to return.\n\n        :type page_token: str\n        :param page_token:\n            (Optional) If present, return the next batch of buckets, using the\n            value, which must correspond to the ``nextPageToken`` value\n            returned in the previous response.  Deprecated: use the ``pages``\n            property of the returned iterator instead of manually passing the\n            token.\n\n        :type prefix: str\n        :param prefix: (Optional) Filter results to buckets whose names begin\n                       with this prefix.\n\n        :type projection: str\n        :param projection:\n            (Optional) Specifies the set of properties to return. If used, must\n            be 'full' or 'noAcl'. Defaults to 'noAcl'.\n\n        :type fields: str\n        :param fields:\n            (Optional) Selector specifying which fields to include in a partial\n            response. Must be a list of fields. For example to get a partial\n            response with just the next page token and the language of each\n            bucket returned: 'items/id,nextPageToken'\n\n        :type project: str\n        :param project: (Optional) The project whose buckets are to be listed.\n                        If not passed, uses the project set on the client.\n\n        :type page_size: int\n        :param page_size: (Optional) Maximum number of buckets to return in each page.\n            Defaults to a value set by the API.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :raises ValueError: if both ``project`` is ``None`` and the client's\n                            project is also ``None``.\n        :returns: Iterator of all :class:`~google.cloud.storage.bucket.Bucket`\n                  belonging to this project.\n        \"\"\"\n        extra_params = {}\n\n        if project is None:\n            project = self.project\n\n        # Use no project if STORAGE_EMULATOR_HOST is set\n        if self._is_emulator_set:\n            if project is None:\n                project = _get_environ_project()\n            if project is None:\n                project = \"<none>\"\n\n        # Only include the project parameter if a project is set.\n        # If a project is not set, falls back to API validation (BadRequest).\n        if project is not None:\n            extra_params = {\"project\": project}\n\n        if prefix is not None:\n            extra_params[\"prefix\"] = prefix\n\n        extra_params[\"projection\"] = projection\n\n        if fields is not None:\n            extra_params[\"fields\"] = fields\n\n        return self._list_resource(\n            \"/b\",\n            _item_to_bucket,\n            page_token=page_token,\n            max_results=max_results,\n            extra_params=extra_params,\n            page_size=page_size,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def create_hmac_key(\n        self,\n        service_account_email,\n        project_id=None,\n        user_project=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=None,\n    ):\n        \"\"\"Create an HMAC key for a service account.\n\n        :type service_account_email: str\n        :param service_account_email: e-mail address of the service account\n\n        :type project_id: str\n        :param project_id: (Optional) Explicit project ID for the key.\n            Defaults to the client's project.\n\n        :type user_project: str\n        :param user_project: (Optional) This parameter is currently ignored.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry: (Optional) How to retry the RPC. A None value will disable retries.\n            A google.api_core.retry.Retry value will enable retries, and the object will\n            define retriable response codes and errors and configure backoff and timeout options.\n\n            A google.cloud.storage.retry.ConditionalRetryPolicy value wraps a Retry object and\n            activates it only if certain conditions are met. This class exists to provide safe defaults\n            for RPC calls that are not technically safe to retry normally (due to potential data\n            duplication or other side-effects) but become safe to retry if a condition such as\n            if_metageneration_match is set.\n\n            See the retry.py source code and docstrings in this package (google.cloud.storage.retry) for\n            information on retry types and how to configure them.\n\n        :rtype:\n            Tuple[:class:`~google.cloud.storage.hmac_key.HMACKeyMetadata`, str]\n        :returns: metadata for the created key, plus the bytes of the key's secret, which is an 40-character base64-encoded string.\n        \"\"\"\n        if project_id is None:\n            project_id = self.project\n\n        path = f\"/projects/{project_id}/hmacKeys\"\n        qs_params = {\"serviceAccountEmail\": service_account_email}\n\n        if user_project is not None:\n            qs_params[\"userProject\"] = user_project\n\n        api_response = self._post_resource(\n            path,\n            None,\n            query_params=qs_params,\n            timeout=timeout,\n            retry=retry,\n        )\n        metadata = HMACKeyMetadata(self)\n        metadata._properties = api_response[\"metadata\"]\n        secret = api_response[\"secret\"]\n        return metadata, secret\n\n    def list_hmac_keys(\n        self,\n        max_results=None,\n        service_account_email=None,\n        show_deleted_keys=None,\n        project_id=None,\n        user_project=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n    ):\n        \"\"\"List HMAC keys for a project.\n\n        :type max_results: int\n        :param max_results:\n            (Optional) Max number of keys to return in a given page.\n\n        :type service_account_email: str\n        :param service_account_email:\n            (Optional) Limit keys to those created by the given service account.\n\n        :type show_deleted_keys: bool\n        :param show_deleted_keys:\n            (Optional) Included deleted keys in the list. Default is to\n            exclude them.\n\n        :type project_id: str\n        :param project_id: (Optional) Explicit project ID for the key.\n            Defaults to the client's project.\n\n        :type user_project: str\n        :param user_project: (Optional) This parameter is currently ignored.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :rtype:\n            Tuple[:class:`~google.cloud.storage.hmac_key.HMACKeyMetadata`, str]\n        :returns: metadata for the created key, plus the bytes of the key's secret, which is an 40-character base64-encoded string.\n        \"\"\"\n        if project_id is None:\n            project_id = self.project\n\n        path = f\"/projects/{project_id}/hmacKeys\"\n        extra_params = {}\n\n        if service_account_email is not None:\n            extra_params[\"serviceAccountEmail\"] = service_account_email\n\n        if show_deleted_keys is not None:\n            extra_params[\"showDeletedKeys\"] = show_deleted_keys\n\n        if user_project is not None:\n            extra_params[\"userProject\"] = user_project\n\n        return self._list_resource(\n            path,\n            _item_to_hmac_key_metadata,\n            max_results=max_results,\n            extra_params=extra_params,\n            timeout=timeout,\n            retry=retry,\n        )\n\n    def get_hmac_key_metadata(\n        self, access_id, project_id=None, user_project=None, timeout=_DEFAULT_TIMEOUT\n    ):\n        \"\"\"Return a metadata instance for the given HMAC key.\n\n        :type access_id: str\n        :param access_id: Unique ID of an existing key.\n\n        :type project_id: str\n        :param project_id: (Optional) Project ID of an existing key.\n            Defaults to client's project.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type user_project: str\n        :param user_project: (Optional) This parameter is currently ignored.\n        \"\"\"\n        metadata = HMACKeyMetadata(self, access_id, project_id, user_project)\n        metadata.reload(timeout=timeout)  # raises NotFound for missing key\n        return metadata\n\n    def generate_signed_post_policy_v4(\n        self,\n        bucket_name,\n        blob_name,\n        expiration,\n        conditions=None,\n        fields=None,\n        credentials=None,\n        virtual_hosted_style=False,\n        bucket_bound_hostname=None,\n        scheme=\"http\",\n        service_account_email=None,\n        access_token=None,\n    ):\n        \"\"\"Generate a V4 signed policy object. Generated policy object allows user to upload objects with a POST request.\n\n        .. note::\n\n            Assumes ``credentials`` implements the\n            :class:`google.auth.credentials.Signing` interface. Also assumes\n            ``credentials`` has a ``service_account_email`` property which\n            identifies the credentials.\n\n        See a [code sample](https://github.com/googleapis/python-storage/blob/main/samples/snippets/storage_generate_signed_post_policy_v4.py).\n\n        :type bucket_name: str\n        :param bucket_name: Bucket name.\n\n        :type blob_name: str\n        :param blob_name: Object name.\n\n        :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n        :param expiration: Policy expiration time. If a ``datetime`` instance is\n                           passed without an explicit ``tzinfo`` set,  it will be\n                           assumed to be ``UTC``.\n\n        :type conditions: list\n        :param conditions: (Optional) List of POST policy conditions, which are\n                           used to restrict what is allowed in the request.\n\n        :type fields: dict\n        :param fields: (Optional) Additional elements to include into request.\n\n        :type credentials: :class:`google.auth.credentials.Signing`\n        :param credentials: (Optional) Credentials object with an associated private\n                            key to sign text.\n\n        :type virtual_hosted_style: bool\n        :param virtual_hosted_style:\n            (Optional) If True, construct the URL relative to the bucket\n            virtual hostname, e.g., '<bucket-name>.storage.googleapis.com'.\n            Incompatible with bucket_bound_hostname.\n\n        :type bucket_bound_hostname: str\n        :param bucket_bound_hostname:\n            (Optional) If passed, construct the URL relative to the bucket-bound hostname.\n            Value can be bare or with a scheme, e.g., 'example.com' or 'http://example.com'.\n            Incompatible with virtual_hosted_style.\n            See: https://cloud.google.com/storage/docs/request-endpoints#cname\n\n        :type scheme: str\n        :param scheme:\n            (Optional) If ``bucket_bound_hostname`` is passed as a bare hostname, use\n            this value as a scheme. ``https`` will work only when using a CDN.\n            Defaults to ``\"http\"``.\n\n        :type service_account_email: str\n        :param service_account_email: (Optional) E-mail address of the service account.\n\n        :type access_token: str\n        :param access_token: (Optional) Access token for a service account.\n\n        :raises: :exc:`ValueError` when mutually exclusive arguments are used.\n\n        :rtype: dict\n        :returns: Signed POST policy.\n        \"\"\"\n        if virtual_hosted_style and bucket_bound_hostname:\n            raise ValueError(\n                \"Only one of virtual_hosted_style and bucket_bound_hostname \"\n                \"can be specified.\"\n            )\n\n        credentials = self._credentials if credentials is None else credentials\n        ensure_signed_credentials(credentials)\n\n        # prepare policy conditions and fields\n        timestamp, datestamp = get_v4_now_dtstamps()\n\n        x_goog_credential = \"{email}/{datestamp}/auto/storage/goog4_request\".format(\n            email=credentials.signer_email, datestamp=datestamp\n        )\n        required_conditions = [\n            {\"bucket\": bucket_name},\n            {\"key\": blob_name},\n            {\"x-goog-date\": timestamp},\n            {\"x-goog-credential\": x_goog_credential},\n            {\"x-goog-algorithm\": \"GOOG4-RSA-SHA256\"},\n        ]\n\n        conditions = conditions or []\n        policy_fields = {}\n        for key, value in sorted((fields or {}).items()):\n            if not key.startswith(\"x-ignore-\"):\n                policy_fields[key] = value\n                conditions.append({key: value})\n\n        conditions += required_conditions\n\n        # calculate policy expiration time\n        now = _NOW(_UTC).replace(tzinfo=None)\n        if expiration is None:\n            expiration = now + datetime.timedelta(hours=1)\n\n        policy_expires = now + datetime.timedelta(\n            seconds=get_expiration_seconds_v4(expiration)\n        )\n\n        # encode policy for signing\n        policy = json.dumps(\n            collections.OrderedDict(\n                sorted(\n                    {\n                        \"conditions\": conditions,\n                        \"expiration\": policy_expires.isoformat() + \"Z\",\n                    }.items()\n                )\n            ),\n            separators=(\",\", \":\"),\n        )\n        str_to_sign = base64.b64encode(policy.encode(\"utf-8\"))\n\n        # sign the policy and get its cryptographic signature\n        if access_token and service_account_email:\n            signature = _sign_message(str_to_sign, access_token, service_account_email)\n            signature_bytes = base64.b64decode(signature)\n        else:\n            signature_bytes = credentials.sign_bytes(str_to_sign)\n\n        # get hexadecimal representation of the signature\n        signature = binascii.hexlify(signature_bytes).decode(\"utf-8\")\n\n        policy_fields.update(\n            {\n                \"key\": blob_name,\n                \"x-goog-algorithm\": \"GOOG4-RSA-SHA256\",\n                \"x-goog-credential\": x_goog_credential,\n                \"x-goog-date\": timestamp,\n                \"x-goog-signature\": signature,\n                \"policy\": str_to_sign.decode(\"utf-8\"),\n            }\n        )\n        # designate URL\n        if virtual_hosted_style:\n            url = _virtual_hosted_style_base_url(\n                self.api_endpoint, bucket_name, trailing_slash=True\n            )\n        elif bucket_bound_hostname:\n            url = f\"{_bucket_bound_hostname_url(bucket_bound_hostname, scheme)}/\"\n        else:\n            url = f\"{self.api_endpoint}/{bucket_name}/\"\n\n        return {\"url\": url, \"fields\": policy_fields}\n\n\ndef _item_to_bucket(iterator, item):\n    \"\"\"Convert a JSON bucket to the native object.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a bucket.\n\n    :rtype: :class:`.Bucket`\n    :returns: The next bucket in the page.\n    \"\"\"\n    name = item.get(\"name\")\n    bucket = Bucket(iterator.client, name)\n    bucket._set_properties(item)\n    return bucket\n\n\ndef _item_to_hmac_key_metadata(iterator, item):\n    \"\"\"Convert a JSON key metadata resource to the native object.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a key metadata instance.\n\n    :rtype: :class:`~google.cloud.storage.hmac_key.HMACKeyMetadata`\n    :returns: The next key metadata instance in the page.\n    \"\"\"\n    metadata = HMACKeyMetadata(iterator.client)\n    metadata._properties = item\n    return metadata\n", "google/cloud/storage/batch.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Batch updates / deletes of storage buckets / blobs.\n\nA batch request is a single standard HTTP request containing multiple Cloud Storage JSON API calls.\nWithin this main HTTP request, there are multiple parts which each contain a nested HTTP request.\nThe body of each part is itself a complete HTTP request, with its own verb, URL, headers, and body.\n\nNote that Cloud Storage does not support batch operations for uploading or downloading.\nAdditionally, the current batch design does not support library methods whose return values\ndepend on the response payload. See more details in the [Sending Batch Requests official guide](https://cloud.google.com/storage/docs/batch).\n\nExamples of situations when you might want to use the Batch module:\n``blob.patch()``\n``blob.update()``\n``blob.delete()``\n``bucket.delete_blob()``\n``bucket.patch()``\n``bucket.update()``\n\"\"\"\nfrom email.encoders import encode_noop\nfrom email.generator import Generator\nfrom email.mime.application import MIMEApplication\nfrom email.mime.multipart import MIMEMultipart\nfrom email.parser import Parser\nimport io\nimport json\n\nimport requests\n\nfrom google.cloud import _helpers\nfrom google.cloud import exceptions\nfrom google.cloud.storage._http import Connection\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\n\n\nclass MIMEApplicationHTTP(MIMEApplication):\n    \"\"\"MIME type for ``application/http``.\n\n    Constructs payload from headers and body\n\n    :type method: str\n    :param method: HTTP method\n\n    :type uri: str\n    :param uri: URI for HTTP request\n\n    :type headers:  dict\n    :param headers: HTTP headers\n\n    :type body: str\n    :param body: (Optional) HTTP payload\n\n    \"\"\"\n\n    def __init__(self, method, uri, headers, body):\n        if isinstance(body, dict):\n            body = json.dumps(body)\n            headers[\"Content-Type\"] = \"application/json\"\n            headers[\"Content-Length\"] = len(body)\n        if body is None:\n            body = \"\"\n        lines = [f\"{method} {uri} HTTP/1.1\"]\n        lines.extend([f\"{key}: {value}\" for key, value in sorted(headers.items())])\n        lines.append(\"\")\n        lines.append(body)\n        payload = \"\\r\\n\".join(lines)\n        super().__init__(payload, \"http\", encode_noop)\n\n\nclass _FutureDict(object):\n    \"\"\"Class to hold a future value for a deferred request.\n\n    Used by for requests that get sent in a :class:`Batch`.\n    \"\"\"\n\n    @staticmethod\n    def get(key, default=None):\n        \"\"\"Stand-in for dict.get.\n\n        :type key: object\n        :param key: Hashable dictionary key.\n\n        :type default: object\n        :param default: Fallback value to dict.get.\n\n        :raises: :class:`KeyError` always since the future is intended to fail\n                 as a dictionary.\n        \"\"\"\n        raise KeyError(f\"Cannot get({key!r}, default={default!r}) on a future\")\n\n    def __getitem__(self, key):\n        \"\"\"Stand-in for dict[key].\n\n        :type key: object\n        :param key: Hashable dictionary key.\n\n        :raises: :class:`KeyError` always since the future is intended to fail\n                 as a dictionary.\n        \"\"\"\n        raise KeyError(f\"Cannot get item {key!r} from a future\")\n\n    def __setitem__(self, key, value):\n        \"\"\"Stand-in for dict[key] = value.\n\n        :type key: object\n        :param key: Hashable dictionary key.\n\n        :type value: object\n        :param value: Dictionary value.\n\n        :raises: :class:`KeyError` always since the future is intended to fail\n                 as a dictionary.\n        \"\"\"\n        raise KeyError(f\"Cannot set {key!r} -> {value!r} on a future\")\n\n\nclass _FutureResponse(requests.Response):\n    \"\"\"Reponse that returns a placeholder dictionary for a batched requests.\"\"\"\n\n    def __init__(self, future_dict):\n        super(_FutureResponse, self).__init__()\n        self._future_dict = future_dict\n        self.status_code = 204\n\n    def json(self):\n        return self._future_dict\n\n    @property\n    def content(self):\n        return self._future_dict\n\n\nclass Batch(Connection):\n    \"\"\"Proxy an underlying connection, batching up change operations.\n\n    .. warning::\n\n        Cloud Storage does not support batch operations for uploading or downloading.\n        Additionally, the current batch design does not support library methods whose\n        return values depend on the response payload.\n\n    :type client: :class:`google.cloud.storage.client.Client`\n    :param client: The client to use for making connections.\n\n    :type raise_exception: bool\n    :param raise_exception:\n        (Optional) Defaults to True. If True, instead of adding exceptions\n        to the list of return responses, the final exception will be raised.\n        Note that exceptions are unwrapped after all operations are complete\n        in success or failure, and only the last exception is raised.\n    \"\"\"\n\n    _MAX_BATCH_SIZE = 1000\n\n    def __init__(self, client, raise_exception=True):\n        api_endpoint = client._connection.API_BASE_URL\n        client_info = client._connection._client_info\n        super(Batch, self).__init__(\n            client, client_info=client_info, api_endpoint=api_endpoint\n        )\n        self._requests = []\n        self._target_objects = []\n        self._responses = []\n        self._raise_exception = raise_exception\n\n    def _do_request(\n        self, method, url, headers, data, target_object, timeout=_DEFAULT_TIMEOUT\n    ):\n        \"\"\"Override Connection:  defer actual HTTP request.\n\n        Only allow up to ``_MAX_BATCH_SIZE`` requests to be deferred.\n\n        :type method: str\n        :param method: The HTTP method to use in the request.\n\n        :type url: str\n        :param url: The URL to send the request to.\n\n        :type headers: dict\n        :param headers: A dictionary of HTTP headers to send with the request.\n\n        :type data: str\n        :param data: The data to send as the body of the request.\n\n        :type target_object: object\n        :param target_object:\n            (Optional) This allows us to enable custom behavior in our batch\n            connection. Here we defer an HTTP request and complete\n            initialization of the object at a later time.\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :rtype: tuple of ``response`` (a dictionary of sorts)\n                and ``content`` (a string).\n        :returns: The HTTP response object and the content of the response.\n        \"\"\"\n        if len(self._requests) >= self._MAX_BATCH_SIZE:\n            raise ValueError(\n                \"Too many deferred requests (max %d)\" % self._MAX_BATCH_SIZE\n            )\n        self._requests.append((method, url, headers, data, timeout))\n        result = _FutureDict()\n        self._target_objects.append(target_object)\n        if target_object is not None:\n            target_object._properties = result\n        return _FutureResponse(result)\n\n    def _prepare_batch_request(self):\n        \"\"\"Prepares headers and body for a batch request.\n\n        :rtype: tuple (dict, str)\n        :returns: The pair of headers and body of the batch request to be sent.\n        :raises: :class:`ValueError` if no requests have been deferred.\n        \"\"\"\n        if len(self._requests) == 0:\n            raise ValueError(\"No deferred requests\")\n\n        multi = MIMEMultipart()\n\n        # Use timeout of last request, default to _DEFAULT_TIMEOUT\n        timeout = _DEFAULT_TIMEOUT\n        for method, uri, headers, body, _timeout in self._requests:\n            subrequest = MIMEApplicationHTTP(method, uri, headers, body)\n            multi.attach(subrequest)\n            timeout = _timeout\n\n        buf = io.StringIO()\n        generator = Generator(buf, False, 0)\n        generator.flatten(multi)\n        payload = buf.getvalue()\n\n        # Strip off redundant header text\n        _, body = payload.split(\"\\n\\n\", 1)\n        return dict(multi._headers), body, timeout\n\n    def _finish_futures(self, responses, raise_exception=True):\n        \"\"\"Apply all the batch responses to the futures created.\n\n        :type responses: list of (headers, payload) tuples.\n        :param responses: List of headers and payloads from each response in\n                          the batch.\n\n        :type raise_exception: bool\n        :param raise_exception:\n            (Optional) Defaults to True. If True, instead of adding exceptions\n            to the list of return responses, the final exception will be raised.\n            Note that exceptions are unwrapped after all operations are complete\n            in success or failure, and only the last exception is raised.\n\n        :raises: :class:`ValueError` if no requests have been deferred.\n        \"\"\"\n        # If a bad status occurs, we track it, but don't raise an exception\n        # until all futures have been populated.\n        # If raise_exception=False, we add exceptions to the list of responses.\n        exception_args = None\n\n        if len(self._target_objects) != len(responses):  # pragma: NO COVER\n            raise ValueError(\"Expected a response for every request.\")\n\n        for target_object, subresponse in zip(self._target_objects, responses):\n            # For backwards compatibility, only the final exception will be raised.\n            # Set raise_exception=False to include all exceptions to the list of return responses.\n            if not 200 <= subresponse.status_code < 300 and raise_exception:\n                exception_args = exception_args or subresponse\n            elif target_object is not None:\n                try:\n                    target_object._properties = subresponse.json()\n                except ValueError:\n                    target_object._properties = subresponse.content\n\n        if exception_args is not None:\n            raise exceptions.from_http_response(exception_args)\n\n    def finish(self, raise_exception=True):\n        \"\"\"Submit a single `multipart/mixed` request with deferred requests.\n\n        :type raise_exception: bool\n        :param raise_exception:\n            (Optional) Defaults to True. If True, instead of adding exceptions\n            to the list of return responses, the final exception will be raised.\n            Note that exceptions are unwrapped after all operations are complete\n            in success or failure, and only the last exception is raised.\n\n        :rtype: list of tuples\n        :returns: one ``(headers, payload)`` tuple per deferred request.\n        \"\"\"\n        headers, body, timeout = self._prepare_batch_request()\n\n        url = f\"{self.API_BASE_URL}/batch/storage/v1\"\n\n        # Use the private ``_base_connection`` rather than the property\n        # ``_connection``, since the property may be this\n        # current batch.\n        response = self._client._base_connection._make_request(\n            \"POST\", url, data=body, headers=headers, timeout=timeout\n        )\n\n        # Raise exception if the top-level batch request fails\n        if not 200 <= response.status_code < 300:\n            raise exceptions.from_http_response(response)\n\n        responses = list(_unpack_batch_response(response))\n        self._finish_futures(responses, raise_exception=raise_exception)\n        self._responses = responses\n        return responses\n\n    def current(self):\n        \"\"\"Return the topmost batch, or None.\"\"\"\n        return self._client.current_batch\n\n    def __enter__(self):\n        self._client._push_batch(self)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        try:\n            if exc_type is None:\n                self.finish(raise_exception=self._raise_exception)\n        finally:\n            self._client._pop_batch()\n\n\ndef _generate_faux_mime_message(parser, response):\n    \"\"\"Convert response, content -> (multipart) email.message.\n\n    Helper for _unpack_batch_response.\n    \"\"\"\n    # We coerce to bytes to get consistent concat across\n    # Py2 and Py3. Percent formatting is insufficient since\n    # it includes the b in Py3.\n    content_type = _helpers._to_bytes(response.headers.get(\"content-type\", \"\"))\n\n    faux_message = b\"\".join(\n        [b\"Content-Type: \", content_type, b\"\\nMIME-Version: 1.0\\n\\n\", response.content]\n    )\n\n    return parser.parsestr(faux_message.decode(\"utf-8\"))\n\n\ndef _unpack_batch_response(response):\n    \"\"\"Convert requests.Response -> [(headers, payload)].\n\n    Creates a generator of tuples of emulating the responses to\n    :meth:`requests.Session.request`.\n\n    :type response: :class:`requests.Response`\n    :param response: HTTP response / headers from a request.\n    \"\"\"\n    parser = Parser()\n    message = _generate_faux_mime_message(parser, response)\n\n    if not isinstance(message._payload, list):  # pragma: NO COVER\n        raise ValueError(\"Bad response:  not multi-part\")\n\n    for subrequest in message._payload:\n        status_line, rest = subrequest._payload.split(\"\\n\", 1)\n        _, status, _ = status_line.split(\" \", 2)\n        sub_message = parser.parsestr(rest)\n        payload = sub_message._payload\n        msg_headers = dict(sub_message._headers)\n        content_id = msg_headers.get(\"Content-ID\")\n\n        subresponse = requests.Response()\n        subresponse.request = requests.Request(\n            method=\"BATCH\", url=f\"contentid://{content_id}\"\n        ).prepare()\n        subresponse.status_code = int(status)\n        subresponse.headers.update(msg_headers)\n        subresponse._content = payload.encode(\"utf-8\")\n\n        yield subresponse\n", "google/cloud/storage/__init__.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Shortcut methods for getting set up with Google Cloud Storage.\n\nYou'll typically use these to get started with the API:\n\n.. literalinclude:: snippets.py\n    :start-after: START storage_get_started\n    :end-before: END storage_get_started\n    :dedent: 4\n\nThe main concepts with this API are:\n\n- :class:`~google.cloud.storage.bucket.Bucket` which represents a particular\n  bucket (akin to a mounted disk on a computer).\n\n- :class:`~google.cloud.storage.blob.Blob` which represents a pointer to a\n  particular entity in Cloud Storage (akin to a file path on a remote\n  machine).\n\"\"\"\n\nfrom google.cloud.storage.version import __version__\nfrom google.cloud.storage.batch import Batch\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.bucket import Bucket\nfrom google.cloud.storage.client import Client\n\n\n__all__ = [\"__version__\", \"Batch\", \"Blob\", \"Bucket\", \"Client\"]\n", "google/cloud/storage/_helpers.py": "# Copyright 2014 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helper functions for Cloud Storage utility classes.\n\nThese are *not* part of the API.\n\"\"\"\n\nimport base64\nimport datetime\nfrom hashlib import md5\nimport os\nfrom urllib.parse import urlsplit\nfrom urllib.parse import urlunsplit\nfrom uuid import uuid4\n\nfrom google import resumable_media\nfrom google.auth import environment_vars\nfrom google.cloud.storage.constants import _DEFAULT_TIMEOUT\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom google.cloud.storage.retry import DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED\n\n\nSTORAGE_EMULATOR_ENV_VAR = \"STORAGE_EMULATOR_HOST\"  # Despite name, includes scheme.\n\"\"\"Environment variable defining host for Storage emulator.\"\"\"\n\n_API_ENDPOINT_OVERRIDE_ENV_VAR = \"API_ENDPOINT_OVERRIDE\"  # Includes scheme.\n\"\"\"This is an experimental configuration variable. Use api_endpoint instead.\"\"\"\n\n_API_VERSION_OVERRIDE_ENV_VAR = \"API_VERSION_OVERRIDE\"\n\"\"\"This is an experimental configuration variable used for internal testing.\"\"\"\n\n_DEFAULT_UNIVERSE_DOMAIN = \"googleapis.com\"\n\n_STORAGE_HOST_TEMPLATE = \"storage.{universe_domain}\"\n\n_TRUE_DEFAULT_STORAGE_HOST = _STORAGE_HOST_TEMPLATE.format(\n    universe_domain=_DEFAULT_UNIVERSE_DOMAIN\n)\n\n_DEFAULT_SCHEME = \"https://\"\n\n_API_VERSION = os.getenv(_API_VERSION_OVERRIDE_ENV_VAR, \"v1\")\n\"\"\"API version of the default storage host\"\"\"\n\n# etag match parameters in snake case and equivalent header\n_ETAG_MATCH_PARAMETERS = (\n    (\"if_etag_match\", \"If-Match\"),\n    (\"if_etag_not_match\", \"If-None-Match\"),\n)\n\n# generation match parameters in camel and snake cases\n_GENERATION_MATCH_PARAMETERS = (\n    (\"if_generation_match\", \"ifGenerationMatch\"),\n    (\"if_generation_not_match\", \"ifGenerationNotMatch\"),\n    (\"if_metageneration_match\", \"ifMetagenerationMatch\"),\n    (\"if_metageneration_not_match\", \"ifMetagenerationNotMatch\"),\n    (\"if_source_generation_match\", \"ifSourceGenerationMatch\"),\n    (\"if_source_generation_not_match\", \"ifSourceGenerationNotMatch\"),\n    (\"if_source_metageneration_match\", \"ifSourceMetagenerationMatch\"),\n    (\"if_source_metageneration_not_match\", \"ifSourceMetagenerationNotMatch\"),\n)\n\n_NUM_RETRIES_MESSAGE = (\n    \"`num_retries` has been deprecated and will be removed in a future \"\n    \"release. Use the `retry` argument with a Retry or ConditionalRetryPolicy \"\n    \"object, or None, instead.\"\n)\n\n# _NOW() returns the current local date and time.\n# It is preferred to use timezone-aware datetimes _NOW(_UTC),\n# which returns the current UTC date and time.\n_NOW = datetime.datetime.now\n_UTC = datetime.timezone.utc\n\n\ndef _get_storage_emulator_override():\n    return os.environ.get(STORAGE_EMULATOR_ENV_VAR, None)\n\n\ndef _get_default_storage_base_url():\n    return os.getenv(\n        _API_ENDPOINT_OVERRIDE_ENV_VAR, _DEFAULT_SCHEME + _TRUE_DEFAULT_STORAGE_HOST\n    )\n\n\ndef _get_api_endpoint_override():\n    \"\"\"This is an experimental configuration variable. Use api_endpoint instead.\"\"\"\n    if _get_default_storage_base_url() != _DEFAULT_SCHEME + _TRUE_DEFAULT_STORAGE_HOST:\n        return _get_default_storage_base_url()\n    return None\n\n\ndef _virtual_hosted_style_base_url(url, bucket, trailing_slash=False):\n    \"\"\"Returns the scheme and netloc sections of the url, with the bucket\n    prepended to the netloc.\n\n    Not intended for use with netlocs which include a username and password.\n    \"\"\"\n    parsed_url = urlsplit(url)\n    new_netloc = f\"{bucket}.{parsed_url.netloc}\"\n    base_url = urlunsplit(\n        (parsed_url.scheme, new_netloc, \"/\" if trailing_slash else \"\", \"\", \"\")\n    )\n    return base_url\n\n\ndef _use_client_cert():\n    return os.getenv(\"GOOGLE_API_USE_CLIENT_CERTIFICATE\") == \"true\"\n\n\ndef _get_environ_project():\n    return os.getenv(\n        environment_vars.PROJECT,\n        os.getenv(environment_vars.LEGACY_PROJECT),\n    )\n\n\ndef _validate_name(name):\n    \"\"\"Pre-flight ``Bucket`` name validation.\n\n    :type name: str or :data:`NoneType`\n    :param name: Proposed bucket name.\n\n    :rtype: str or :data:`NoneType`\n    :returns: ``name`` if valid.\n    \"\"\"\n    if name is None:\n        return\n\n    # The first and last characters must be alphanumeric.\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\n        raise ValueError(\"Bucket names must start and end with a number or letter.\")\n    return name\n\n\nclass _PropertyMixin(object):\n    \"\"\"Abstract mixin for cloud storage classes with associated properties.\n\n    Non-abstract subclasses should implement:\n      - path\n      - client\n      - user_project\n\n    :type name: str\n    :param name: The name of the object. Bucket names must start and end with a\n                 number or letter.\n    \"\"\"\n\n    def __init__(self, name=None):\n        self.name = name\n        self._properties = {}\n        self._changes = set()\n\n    @property\n    def path(self):\n        \"\"\"Abstract getter for the object path.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def client(self):\n        \"\"\"Abstract getter for the object client.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def user_project(self):\n        \"\"\"Abstract getter for the object user_project.\"\"\"\n        raise NotImplementedError\n\n    def _require_client(self, client):\n        \"\"\"Check client or verify over-ride.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :rtype: :class:`google.cloud.storage.client.Client`\n        :returns: The client passed in or the currently bound client.\n        \"\"\"\n        if client is None:\n            client = self.client\n        return client\n\n    def _encryption_headers(self):\n        \"\"\"Return any encryption headers needed to fetch the object.\n\n        .. note::\n           Defined here because :meth:`reload` calls it, but this method is\n           really only relevant for :class:`~google.cloud.storage.blob.Blob`.\n\n        :rtype: dict\n        :returns: a mapping of encryption-related headers.\n        \"\"\"\n        return {}\n\n    @property\n    def _query_params(self):\n        \"\"\"Default query parameters.\"\"\"\n        params = {}\n        if self.user_project is not None:\n            params[\"userProject\"] = self.user_project\n        return params\n\n    def reload(\n        self,\n        client=None,\n        projection=\"noAcl\",\n        if_etag_match=None,\n        if_etag_not_match=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY,\n        soft_deleted=None,\n    ):\n        \"\"\"Reload properties from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type projection: str\n        :param projection: (Optional) If used, must be 'full' or 'noAcl'.\n                           Defaults to ``'noAcl'``. Specifies the set of\n                           properties to return.\n\n        :type if_etag_match: Union[str, Set[str]]\n        :param if_etag_match: (Optional) See :ref:`using-if-etag-match`\n\n        :type if_etag_not_match: Union[str, Set[str]])\n        :param if_etag_not_match: (Optional) See :ref:`using-if-etag-not-match`\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type soft_deleted: bool\n        :param soft_deleted:\n            (Optional) If True, looks for a soft-deleted object. Will only return\n            the object metadata if the object exists and is in a soft-deleted state.\n            :attr:`generation` is required to be set on the blob if ``soft_deleted`` is set to True.\n            See: https://cloud.google.com/storage/docs/soft-delete\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        # Pass only '?projection=noAcl' here because 'acl' and related\n        # are handled via custom endpoints.\n        query_params[\"projection\"] = projection\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        if soft_deleted is not None:\n            query_params[\"softDeleted\"] = soft_deleted\n        headers = self._encryption_headers()\n        _add_etag_match_headers(\n            headers, if_etag_match=if_etag_match, if_etag_not_match=if_etag_not_match\n        )\n        api_response = client._get_resource(\n            self.path,\n            query_params=query_params,\n            headers=headers,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n    def _patch_property(self, name, value):\n        \"\"\"Update field of this object's properties.\n\n        This method will only update the field provided and will not\n        touch the other fields.\n\n        It **will not** reload the properties from the server. The behavior is\n        local only and syncing occurs via :meth:`patch`.\n\n        :type name: str\n        :param name: The field name to update.\n\n        :type value: object\n        :param value: The value being updated.\n        \"\"\"\n        self._changes.add(name)\n        self._properties[name] = value\n\n    def _set_properties(self, value):\n        \"\"\"Set the properties for the current object.\n\n        :type value: dict or :class:`google.cloud.storage.batch._FutureDict`\n        :param value: The properties to be set.\n        \"\"\"\n        self._properties = value\n        # If the values are reset, the changes must as well.\n        self._changes = set()\n\n    def patch(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        override_unlocked_retention=False,\n    ):\n        \"\"\"Sends all changed properties in a PATCH request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type override_unlocked_retention: bool\n        :param override_unlocked_retention:\n            (Optional) override_unlocked_retention must be set to True if the operation includes\n            a retention property that changes the mode from Unlocked to Locked, reduces the\n            retainUntilTime, or removes the retention configuration from the object. See:\n            https://cloud.google.com/storage/docs/json_api/v1/objects/patch\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        # Pass '?projection=full' here because 'PATCH' documented not\n        # to work properly w/ 'noAcl'.\n        query_params[\"projection\"] = \"full\"\n        if override_unlocked_retention:\n            query_params[\"overrideUnlockedRetention\"] = override_unlocked_retention\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n        update_properties = {key: self._properties[key] for key in self._changes}\n\n        # Make the API call.\n        api_response = client._patch_resource(\n            self.path,\n            update_properties,\n            query_params=query_params,\n            _target_object=self,\n            timeout=timeout,\n            retry=retry,\n        )\n        self._set_properties(api_response)\n\n    def update(\n        self,\n        client=None,\n        if_generation_match=None,\n        if_generation_not_match=None,\n        if_metageneration_match=None,\n        if_metageneration_not_match=None,\n        timeout=_DEFAULT_TIMEOUT,\n        retry=DEFAULT_RETRY_IF_METAGENERATION_SPECIFIED,\n        override_unlocked_retention=False,\n    ):\n        \"\"\"Sends all properties in a PUT request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use. If not passed, falls back to the\n                       ``client`` stored on the current object.\n\n        :type if_generation_match: long\n        :param if_generation_match:\n            (Optional) See :ref:`using-if-generation-match`\n\n        :type if_generation_not_match: long\n        :param if_generation_not_match:\n            (Optional) See :ref:`using-if-generation-not-match`\n\n        :type if_metageneration_match: long\n        :param if_metageneration_match:\n            (Optional) See :ref:`using-if-metageneration-match`\n\n        :type if_metageneration_not_match: long\n        :param if_metageneration_not_match:\n            (Optional) See :ref:`using-if-metageneration-not-match`\n\n        :type timeout: float or tuple\n        :param timeout:\n            (Optional) The amount of time, in seconds, to wait\n            for the server response.  See: :ref:`configuring_timeouts`\n\n        :type retry: google.api_core.retry.Retry or google.cloud.storage.retry.ConditionalRetryPolicy\n        :param retry:\n            (Optional) How to retry the RPC. See: :ref:`configuring_retries`\n\n        :type override_unlocked_retention: bool\n        :param override_unlocked_retention:\n            (Optional) override_unlocked_retention must be set to True if the operation includes\n            a retention property that changes the mode from Unlocked to Locked, reduces the\n            retainUntilTime, or removes the retention configuration from the object. See:\n            https://cloud.google.com/storage/docs/json_api/v1/objects/patch\n        \"\"\"\n        client = self._require_client(client)\n\n        query_params = self._query_params\n        query_params[\"projection\"] = \"full\"\n        if override_unlocked_retention:\n            query_params[\"overrideUnlockedRetention\"] = override_unlocked_retention\n        _add_generation_match_parameters(\n            query_params,\n            if_generation_match=if_generation_match,\n            if_generation_not_match=if_generation_not_match,\n            if_metageneration_match=if_metageneration_match,\n            if_metageneration_not_match=if_metageneration_not_match,\n        )\n\n        api_response = client._put_resource(\n            self.path,\n            self._properties,\n            query_params=query_params,\n            timeout=timeout,\n            retry=retry,\n            _target_object=self,\n        )\n        self._set_properties(api_response)\n\n\ndef _scalar_property(fieldname):\n    \"\"\"Create a property descriptor around the :class:`_PropertyMixin` helpers.\"\"\"\n\n    def _getter(self):\n        \"\"\"Scalar property getter.\"\"\"\n        return self._properties.get(fieldname)\n\n    def _setter(self, value):\n        \"\"\"Scalar property setter.\"\"\"\n        self._patch_property(fieldname, value)\n\n    return property(_getter, _setter)\n\n\ndef _write_buffer_to_hash(buffer_object, hash_obj, digest_block_size=8192):\n    \"\"\"Read blocks from a buffer and update a hash with them.\n\n    :type buffer_object: bytes buffer\n    :param buffer_object: Buffer containing bytes used to update a hash object.\n\n    :type hash_obj: object that implements update\n    :param hash_obj: A hash object (MD5 or CRC32-C).\n\n    :type digest_block_size: int\n    :param digest_block_size: The block size to write to the hash.\n                              Defaults to 8192.\n    \"\"\"\n    block = buffer_object.read(digest_block_size)\n\n    while len(block) > 0:\n        hash_obj.update(block)\n        # Update the block for the next iteration.\n        block = buffer_object.read(digest_block_size)\n\n\ndef _base64_md5hash(buffer_object):\n    \"\"\"Get MD5 hash of bytes (as base64).\n\n    :type buffer_object: bytes buffer\n    :param buffer_object: Buffer containing bytes used to compute an MD5\n                          hash (as base64).\n\n    :rtype: str\n    :returns: A base64 encoded digest of the MD5 hash.\n    \"\"\"\n    hash_obj = md5()\n    _write_buffer_to_hash(buffer_object, hash_obj)\n    digest_bytes = hash_obj.digest()\n    return base64.b64encode(digest_bytes)\n\n\ndef _add_etag_match_headers(headers, **match_parameters):\n    \"\"\"Add generation match parameters into the given parameters list.\n\n    :type headers: dict\n    :param headers: Headers dict.\n\n    :type match_parameters: dict\n    :param match_parameters: if*etag*match parameters to add.\n    \"\"\"\n    for snakecase_name, header_name in _ETAG_MATCH_PARAMETERS:\n        value = match_parameters.get(snakecase_name)\n\n        if value is not None:\n            if isinstance(value, str):\n                value = [value]\n            headers[header_name] = \", \".join(value)\n\n\ndef _add_generation_match_parameters(parameters, **match_parameters):\n    \"\"\"Add generation match parameters into the given parameters list.\n\n    :type parameters: list or dict\n    :param parameters: Parameters list or dict.\n\n    :type match_parameters: dict\n    :param match_parameters: if*generation*match parameters to add.\n\n    :raises: :exc:`ValueError` if ``parameters`` is not a ``list()``\n             or a ``dict()``.\n    \"\"\"\n    for snakecase_name, camelcase_name in _GENERATION_MATCH_PARAMETERS:\n        value = match_parameters.get(snakecase_name)\n\n        if value is not None:\n            if isinstance(parameters, list):\n                parameters.append((camelcase_name, value))\n\n            elif isinstance(parameters, dict):\n                parameters[camelcase_name] = value\n\n            else:\n                raise ValueError(\n                    \"`parameters` argument should be a dict() or a list().\"\n                )\n\n\ndef _raise_if_more_than_one_set(**kwargs):\n    \"\"\"Raise ``ValueError`` exception if more than one parameter was set.\n\n    :type error: :exc:`ValueError`\n    :param error: Description of which fields were set\n\n    :raises: :class:`~ValueError` containing the fields that were set\n    \"\"\"\n    if sum(arg is not None for arg in kwargs.values()) > 1:\n        escaped_keys = [f\"'{name}'\" for name in kwargs.keys()]\n\n        keys_but_last = \", \".join(escaped_keys[:-1])\n        last_key = escaped_keys[-1]\n\n        msg = f\"Pass at most one of {keys_but_last} and {last_key}\"\n\n        raise ValueError(msg)\n\n\ndef _bucket_bound_hostname_url(host, scheme=None):\n    \"\"\"Helper to build bucket bound hostname URL.\n\n    :type host: str\n    :param host: Host name.\n\n    :type scheme: str\n    :param scheme: (Optional) Web scheme. If passed, use it\n                   as a scheme in the result URL.\n\n    :rtype: str\n    :returns: A bucket bound hostname URL.\n    \"\"\"\n    url_parts = urlsplit(host)\n    if url_parts.scheme and url_parts.netloc:\n        return host\n\n    return f\"{scheme}://{host}\"\n\n\ndef _api_core_retry_to_resumable_media_retry(retry, num_retries=None):\n    \"\"\"Convert google.api.core.Retry to google.resumable_media.RetryStrategy.\n\n    Custom predicates are not translated.\n\n    :type retry: google.api_core.Retry\n    :param retry: (Optional) The google.api_core.Retry object to translate.\n\n    :type num_retries: int\n    :param num_retries: (Optional) The number of retries desired. This is\n        supported for backwards compatibility and is mutually exclusive with\n        `retry`.\n\n    :rtype: google.resumable_media.RetryStrategy\n    :returns: A RetryStrategy with all applicable attributes copied from input,\n              or a RetryStrategy with max_retries set to 0 if None was input.\n    \"\"\"\n\n    if retry is not None and num_retries is not None:\n        raise ValueError(\"num_retries and retry arguments are mutually exclusive\")\n\n    elif retry is not None:\n        return resumable_media.RetryStrategy(\n            max_sleep=retry._maximum,\n            max_cumulative_retry=retry._deadline,\n            initial_delay=retry._initial,\n            multiplier=retry._multiplier,\n        )\n    elif num_retries is not None:\n        return resumable_media.RetryStrategy(max_retries=num_retries)\n    else:\n        return resumable_media.RetryStrategy(max_retries=0)\n\n\ndef _get_invocation_id():\n    return \"gccl-invocation-id/\" + str(uuid4())\n\n\ndef _get_default_headers(\n    user_agent,\n    content_type=\"application/json; charset=UTF-8\",\n    x_upload_content_type=None,\n    command=None,\n):\n    \"\"\"Get the headers for a request.\n\n    :type user_agent: str\n    :param user_agent: The user-agent for requests.\n\n    :type command: str\n    :param command:\n        (Optional) Information about which interface for the operation was\n        used, to be included in the X-Goog-API-Client header. Please leave\n        as None unless otherwise directed.\n\n    :rtype: dict\n    :returns: The headers to be used for the request.\n    \"\"\"\n    x_goog_api_client = f\"{user_agent} {_get_invocation_id()}\"\n\n    if command:\n        x_goog_api_client += f\" gccl-gcs-cmd/{command}\"\n\n    return {\n        \"Accept\": \"application/json\",\n        \"Accept-Encoding\": \"gzip, deflate\",\n        \"User-Agent\": user_agent,\n        \"X-Goog-API-Client\": x_goog_api_client,\n        \"content-type\": content_type,\n        \"x-upload-content-type\": x_upload_content_type or content_type,\n    }\n"}