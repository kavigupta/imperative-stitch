{"igor.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Helper for building, testing, and linting coverage.py.\n\nTo get portability, all these operations are written in Python here instead\nof in shell scripts, batch files, or Makefiles.\n\n\"\"\"\n\nimport contextlib\nimport datetime\nimport glob\nimport inspect\nimport itertools\nimport os\nimport platform\nimport pprint\nimport re\nimport subprocess\nimport sys\nimport sysconfig\nimport textwrap\nimport types\nimport warnings\nimport zipfile\n\ntry:\n    import pytest\nexcept ImportError:\n    # We want to be able to run this for some tasks that don't need pytest.\n    pytest = None\n\n# Constants derived the same as in coverage/env.py.  We can't import\n# that file here, it would be evaluated too early and not get the\n# settings we make in this file.\n\nCPYTHON = platform.python_implementation() == \"CPython\"\nPYPY = platform.python_implementation() == \"PyPy\"\n\n\n@contextlib.contextmanager\ndef ignore_warnings():\n    \"\"\"Context manager to ignore warning within the with statement.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        yield\n\n\nVERBOSITY = int(os.getenv(\"COVERAGE_IGOR_VERBOSE\", \"0\"))\n\n# Functions named do_* are executable from the command line: do_blah is run\n# by \"python igor.py blah\".\n\n\ndef do_show_env():\n    \"\"\"Show the environment variables.\"\"\"\n    print(\"Environment:\")\n    for env in sorted(os.environ):\n        print(f\"  {env} = {os.environ[env]!r}\")\n\n\ndef do_remove_extension(*args):\n    \"\"\"Remove the compiled C extension, no matter what its name.\"\"\"\n\n    so_patterns = \"\"\"\n        tracer.so\n        tracer.*.so\n        tracer.pyd\n        tracer.*.pyd\n        \"\"\".split()\n\n    if \"--from-install\" in args:\n        # Get the install location using a subprocess to avoid\n        # locking the file we are about to delete\n        root = os.path.dirname(\n            subprocess.check_output(\n                [\n                    sys.executable,\n                    \"-Xutf8\",\n                    \"-c\",\n                    \"import coverage; print(coverage.__file__)\",\n                ],\n                encoding=\"utf-8\",\n            ).strip(),\n        )\n        roots = [root]\n    else:\n        roots = [\"coverage\", \"build/*/coverage\"]\n\n    for root, pattern in itertools.product(roots, so_patterns):\n        pattern = os.path.join(root, pattern.strip())\n        if VERBOSITY:\n            print(f\"Searching for {pattern}\")\n        for filename in glob.glob(pattern):\n            if os.path.exists(filename):\n                if VERBOSITY:\n                    print(f\"Removing {filename}\")\n                try:\n                    os.remove(filename)\n                except OSError as exc:\n                    if VERBOSITY:\n                        print(f\"Couldn't remove {filename}: {exc}\")\n\n\ndef label_for_core(core):\n    \"\"\"Get the label for these tests.\"\"\"\n    if core == \"pytrace\":\n        return \"with Python tracer\"\n    elif core == \"ctrace\":\n        return \"with C tracer\"\n    elif core == \"sysmon\":\n        return \"with sys.monitoring\"\n    else:\n        raise ValueError(f\"Bad core: {core!r}\")\n\n\ndef should_skip(core):\n    \"\"\"Is there a reason to skip these tests?\n\n    Return empty string to run tests, or a message about why we are skipping\n    the tests.\n    \"\"\"\n    skipper = \"\"\n\n    # $set_env.py: COVERAGE_TEST_CORES - List of cores to run\n    test_cores = os.getenv(\"COVERAGE_TEST_CORES\")\n    if test_cores:\n        if core not in test_cores:\n            skipper = f\"core {core} not in COVERAGE_TEST_CORES={test_cores}\"\n    else:\n        # $set_env.py: COVERAGE_ONE_CORE - Only run tests for one core.\n        only_one = os.getenv(\"COVERAGE_ONE_CORE\")\n        if only_one:\n            if CPYTHON:\n                if sys.version_info >= (3, 12):\n                    if core != \"sysmon\":\n                        skipper = f\"Only one core: not running {core}\"\n                elif core != \"ctrace\":\n                    skipper = f\"Only one core: not running {core}\"\n            else:\n                if core != \"pytrace\":\n                    skipper = f\"No C core for {platform.python_implementation()}\"\n\n    if skipper:\n        msg = \"Skipping tests \" + label_for_core(core)\n        if len(skipper) > 1:\n            msg += \": \" + skipper\n    else:\n        msg = \"\"\n\n    return msg\n\n\ndef make_env_id(core):\n    \"\"\"An environment id that will keep all the test runs distinct.\"\"\"\n    impl = platform.python_implementation().lower()\n    version = \"{}{}\".format(*sys.version_info[:2])\n    if PYPY:\n        version += \"_{}{}\".format(*sys.pypy_version_info[:2])\n    env_id = f\"{impl}{version}_{core}\"\n    return env_id\n\n\ndef run_tests(core, *runner_args):\n    \"\"\"The actual running of tests.\"\"\"\n    if \"COVERAGE_TESTING\" not in os.environ:\n        os.environ[\"COVERAGE_TESTING\"] = \"True\"\n    print_banner(label_for_core(core))\n\n    return pytest.main(list(runner_args))\n\n\ndef run_tests_with_coverage(core, *runner_args):\n    \"\"\"Run tests, but with coverage.\"\"\"\n    # Need to define this early enough that the first import of env.py sees it.\n    os.environ[\"COVERAGE_TESTING\"] = \"True\"\n    os.environ[\"COVERAGE_PROCESS_START\"] = os.path.abspath(\"metacov.ini\")\n    os.environ[\"COVERAGE_HOME\"] = os.getcwd()\n    context = os.getenv(\"COVERAGE_CONTEXT\")\n    if context:\n        if context[0] == \"$\":\n            context = os.environ[context[1:]]\n        os.environ[\"COVERAGE_CONTEXT\"] = context + \".\" + core\n\n    # Create the .pth file that will let us measure coverage in sub-processes.\n    # The .pth file seems to have to be alphabetically after easy-install.pth\n    # or the sys.path entries aren't created right?\n    # There's an entry in \"make clean\" to get rid of this file.\n    pth_dir = sysconfig.get_path(\"purelib\")\n    pth_path = os.path.join(pth_dir, \"zzz_metacov.pth\")\n    with open(pth_path, \"w\") as pth_file:\n        pth_file.write(\"import coverage; coverage.process_startup()\\n\")\n\n    suffix = f\"{make_env_id(core)}_{platform.platform()}\"\n    os.environ[\"COVERAGE_METAFILE\"] = os.path.abspath(\".metacov.\" + suffix)\n\n    import coverage\n\n    cov = coverage.Coverage(config_file=\"metacov.ini\")\n    cov._warn_unimported_source = False\n    cov._warn_preimported_source = False\n    cov._metacov = True\n    cov.start()\n\n    try:\n        # Re-import coverage to get it coverage tested!  I don't understand all\n        # the mechanics here, but if I don't carry over the imported modules\n        # (in covmods), then things go haywire (os is None, eventually).\n        covmods = {}\n        covdir = os.path.split(coverage.__file__)[0]\n        # We have to make a list since we'll be deleting in the loop.\n        modules = list(sys.modules.items())\n        for name, mod in modules:\n            if name.startswith(\"coverage\"):\n                if getattr(mod, \"__file__\", \"??\").startswith(covdir):\n                    covmods[name] = mod\n                    del sys.modules[name]\n        import coverage  # pylint: disable=reimported\n\n        sys.modules.update(covmods)\n\n        # Run tests, with the arguments from our command line.\n        status = run_tests(core, *runner_args)\n\n    finally:\n        cov.stop()\n        os.remove(pth_path)\n\n    cov.save()\n    return status\n\n\ndef do_combine_html():\n    \"\"\"Combine data from a meta-coverage run, and make the HTML report.\"\"\"\n    import coverage\n\n    os.environ[\"COVERAGE_HOME\"] = os.getcwd()\n    cov = coverage.Coverage(config_file=\"metacov.ini\")\n    cov.load()\n    cov.combine()\n    cov.save()\n    # A new Coverage to turn on messages. Better would be to have tighter\n    # control over message verbosity...\n    cov = coverage.Coverage(config_file=\"metacov.ini\", messages=True)\n    cov.load()\n    show_contexts = bool(\n        os.getenv(\"COVERAGE_DYNCTX\") or os.getenv(\"COVERAGE_CONTEXT\"),\n    )\n    cov.html_report(show_contexts=show_contexts)\n    cov.json_report(show_contexts=show_contexts, pretty_print=True)\n\n\ndef do_test_with_core(core, *runner_args):\n    \"\"\"Run tests with a particular core.\"\"\"\n    # If we should skip these tests, skip them.\n    skip_msg = should_skip(core)\n    if skip_msg:\n        print(skip_msg)\n        return None\n\n    os.environ[\"COVERAGE_CORE\"] = core\n    if os.getenv(\"COVERAGE_COVERAGE\", \"no\") == \"yes\":\n        return run_tests_with_coverage(core, *runner_args)\n    else:\n        return run_tests(core, *runner_args)\n\n\ndef do_zip_mods():\n    \"\"\"Build the zip files needed for tests.\"\"\"\n    with zipfile.ZipFile(\"tests/zipmods.zip\", \"w\") as zf:\n        # Take some files from disk.\n        zf.write(\"tests/covmodzip1.py\", \"covmodzip1.py\")\n\n        # The others will be various encodings.\n        source = textwrap.dedent(\n            \"\"\"\\\n            # coding: {encoding}\n            text = u\"{text}\"\n            ords = {ords}\n            assert [ord(c) for c in text] == ords\n            print(u\"All OK with {encoding}\")\n            encoding = \"{encoding}\"\n            \"\"\",\n        )\n        # These encodings should match the list in tests/test_python.py\n        details = [\n            (\"utf-8\", \"\u24d7\u24d4\u24db\u24db\u24de, \u24e6\u24de\u24e1\u24db\u24d3\"),\n            (\"gb2312\", \"\u4f60\u597d\uff0c\u4e16\u754c\"),\n            (\"hebrew\", \"\u05e9\u05dc\u05d5\u05dd, \u05e2\u05d5\u05dc\u05dd\"),\n            (\"shift_jis\", \"\u3053\u3093\u306b\u3061\u306f\u4e16\u754c\"),\n            (\"cp1252\", \"\u201chi\u201d\"),\n        ]\n        for encoding, text in details:\n            filename = f\"encoded_{encoding}.py\"\n            ords = [ord(c) for c in text]\n            source_text = source.format(encoding=encoding, text=text, ords=ords)\n            zf.writestr(filename, source_text.encode(encoding))\n\n    with zipfile.ZipFile(\"tests/zip1.zip\", \"w\") as zf:\n        zf.write(\"tests/zipsrc/zip1/__init__.py\", \"zip1/__init__.py\")\n        zf.write(\"tests/zipsrc/zip1/zip1.py\", \"zip1/zip1.py\")\n\n    with zipfile.ZipFile(\"tests/covmain.zip\", \"w\") as zf:\n        zf.write(\"coverage/__main__.py\", \"__main__.py\")\n\n\ndef print_banner(label):\n    \"\"\"Print the version of Python.\"\"\"\n    try:\n        impl = platform.python_implementation()\n    except AttributeError:\n        impl = \"Python\"\n\n    version = platform.python_version()\n\n    if PYPY:\n        version += \" (pypy %s)\" % \".\".join(str(v) for v in sys.pypy_version_info)\n\n    rev = platform.python_revision()\n    if rev:\n        version += f\" (rev {rev})\"\n\n    try:\n        which_python = os.path.relpath(sys.executable)\n    except ValueError:\n        # On Windows having a python executable on a different drive\n        # than the sources cannot be relative.\n        which_python = sys.executable\n    print(f\"=== {impl} {version} {label} ({which_python}) ===\")\n    sys.stdout.flush()\n\n\ndef do_quietly(command):\n    \"\"\"Run a command in a shell, and suppress all output.\"\"\"\n    proc = subprocess.run(\n        command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n    )\n    return proc.returncode\n\n\ndef get_release_facts():\n    \"\"\"Return an object with facts about the current release.\"\"\"\n    import coverage\n    import coverage.version\n\n    facts = types.SimpleNamespace()\n    facts.ver = coverage.__version__\n    mjr, mnr, mcr, rel, ser = facts.vi = coverage.version_info\n    facts.dev = coverage.version._dev\n    facts.shortver = f\"{mjr}.{mnr}.{mcr}\"\n    facts.anchor = facts.shortver.replace(\".\", \"-\")\n    if rel == \"final\":\n        facts.next_vi = (mjr, mnr, mcr + 1, \"alpha\", 0)\n    else:\n        facts.anchor += f\"{rel[0]}{ser}\"\n        facts.next_vi = (mjr, mnr, mcr, rel, ser + 1)\n\n    facts.now = datetime.datetime.now()\n    facts.branch = subprocess.getoutput(\"git rev-parse --abbrev-ref @\")\n    facts.sha = subprocess.getoutput(\"git rev-parse @\")\n    return facts\n\n\ndef update_file(fname, pattern, replacement):\n    \"\"\"Update the contents of a file, replacing pattern with replacement.\"\"\"\n    with open(fname) as fobj:\n        old_text = fobj.read()\n\n    new_text = re.sub(pattern, replacement, old_text, count=1)\n\n    if new_text != old_text:\n        print(f\"Updating {fname}\")\n        with open(fname, \"w\") as fobj:\n            fobj.write(new_text)\n\n\nUNRELEASED = \"Unreleased\\n----------\"\nSCRIV_START = \".. scriv-start-here\\n\\n\"\n\n\ndef do_edit_for_release():\n    \"\"\"Edit a few files in preparation for a release.\"\"\"\n    facts = get_release_facts()\n\n    if facts.dev:\n        print(f\"**\\n** This is a dev release: {facts.ver}\\n**\\n\\nNo edits\")\n        return\n\n    # NOTICE.txt\n    update_file(\n        \"NOTICE.txt\", r\"Copyright 2004.*? Ned\", f\"Copyright 2004-{facts.now:%Y} Ned\",\n    )\n\n    # CHANGES.rst\n    title = f\"Version {facts.ver} \u2014 {facts.now:%Y-%m-%d}\"\n    rule = \"-\" * len(title)\n    new_head = f\".. _changes_{facts.anchor}:\\n\\n{title}\\n{rule}\"\n\n    update_file(\"CHANGES.rst\", re.escape(SCRIV_START), \"\")\n    update_file(\"CHANGES.rst\", re.escape(UNRELEASED), SCRIV_START + new_head)\n\n    # doc/conf.py\n    new_conf = textwrap.dedent(\n        f\"\"\"\\\n        # @@@ editable\n        copyright = \"2009\\N{EN DASH}{facts.now:%Y}, Ned Batchelder\" # pylint: disable=redefined-builtin\n        # The short X.Y.Z version.\n        version = \"{facts.shortver}\"\n        # The full version, including alpha/beta/rc tags.\n        release = \"{facts.ver}\"\n        # The date of release, in \"monthname day, year\" format.\n        release_date = \"{facts.now:%B %-d, %Y}\"\n        # @@@ end\n        \"\"\",\n    )\n    update_file(\"doc/conf.py\", r\"(?s)# @@@ editable\\n.*# @@@ end\\n\", new_conf)\n\n\ndef do_bump_version():\n    \"\"\"Edit a few files right after a release to bump the version.\"\"\"\n    facts = get_release_facts()\n\n    # CHANGES.rst\n    update_file(\n        \"CHANGES.rst\",\n        re.escape(SCRIV_START),\n        f\"{UNRELEASED}\\n\\nNothing yet.\\n\\n\\n\" + SCRIV_START,\n    )\n\n    # coverage/version.py\n    next_version = f\"version_info = {facts.next_vi}\\n_dev = 1\".replace(\"'\", '\"')\n    update_file(\n        \"coverage/version.py\", r\"(?m)^version_info = .*\\n_dev = \\d+$\", next_version,\n    )\n\n\ndef do_cheats():\n    \"\"\"Show a cheatsheet of useful things during releasing.\"\"\"\n    facts = get_release_facts()\n    pprint.pprint(facts.__dict__)\n    print()\n    print(f\"Coverage version is {facts.ver}\")\n\n    repo = \"nedbat/coveragepy\"\n    github = f\"https://github.com/{repo}\"\n    egg = \"egg=coverage==0.0\"  # to force a re-install\n    print(\n        f\"https://coverage.readthedocs.io/en/{facts.ver}/changes.html#changes-{facts.anchor}\",\n    )\n\n    print(\n        \"\\n## For GitHub commenting:\\n\"\n        + \"This is now released as part of \"\n        + f\"[coverage {facts.ver}](https://pypi.org/project/coverage/{facts.ver}).\",\n    )\n\n    print(\"\\n## To install this code:\")\n    if facts.branch == \"master\":\n        print(f\"python3 -m pip install git+{github}#{egg}\")\n    else:\n        print(f\"python3 -m pip install git+{github}@{facts.branch}#{egg}\")\n    print(f\"python3 -m pip install git+{github}@{facts.sha[:20]}#{egg}\")\n\n    print(\"\\n## To read this code on GitHub:\")\n    print(f\"https://github.com/nedbat/coveragepy/commit/{facts.sha}\")\n    print(f\"https://github.com/nedbat/coveragepy/commits/{facts.sha}\")\n    print(f\"https://github.com/nedbat/coveragepy/tree/{facts.branch}\")\n\n    print(\n        \"\\n## For other collaborators to get this code:\\n\"\n        + f\"git clone {github}\\n\"\n        + f\"cd {repo.partition('/')[-1]}\\n\"\n        + f\"git checkout {facts.sha}\",\n    )\n\n\ndef do_help():\n    \"\"\"List the available commands\"\"\"\n    items = list(globals().items())\n    items.sort()\n    for name, value in items:\n        if name.startswith(\"do_\"):\n            print(f\"{name[3:]:<20}{value.__doc__}\")\n\n\ndef analyze_args(function):\n    \"\"\"What kind of args does `function` expect?\n\n    Returns:\n        star, num_pos:\n            star(boolean): Does `function` accept *args?\n            num_args(int): How many positional arguments does `function` have?\n    \"\"\"\n    argspec = inspect.getfullargspec(function)\n    return bool(argspec.varargs), len(argspec.args)\n\n\ndef main(args):\n    \"\"\"Main command-line execution for igor.\n\n    Verbs are taken from the command line, and extra words taken as directed\n    by the arguments needed by the handler.\n\n    \"\"\"\n    while args:\n        verb = args.pop(0)\n        handler = globals().get(\"do_\" + verb)\n        if handler is None:\n            print(f\"*** No handler for {verb!r}\")\n            return 1\n        star, num_args = analyze_args(handler)\n        if star:\n            # Handler has *args, give it all the rest of the command line.\n            handler_args = args\n            args = []\n        else:\n            # Handler has specific arguments, give it only what it needs.\n            handler_args = args[:num_args]\n            args = args[num_args:]\n        ret = handler(*handler_args)\n        # If a handler returns a failure-like value, stop.\n        if ret:\n            return ret\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv[1:]))\n", "setup.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Code coverage measurement for Python\"\"\"\n\n# Distutils setup for coverage.py\n# This file is used unchanged under all versions of Python.\n\nimport os\nimport sys\n\n# Setuptools has to be imported before distutils, or things break.\nfrom setuptools import setup\nfrom distutils.core import Extension  # pylint: disable=wrong-import-order\nfrom setuptools.command.build_ext import build_ext  # pylint: disable=wrong-import-order\nfrom distutils import errors  # pylint: disable=wrong-import-order\n\n# Get or massage our metadata.  We exec coverage/version.py so we can avoid\n# importing the product code into setup.py.\n\n# PYVERSIONS\nclassifiers = \"\"\"\\\nEnvironment :: Console\nIntended Audience :: Developers\nLicense :: OSI Approved :: Apache Software License\nOperating System :: OS Independent\nProgramming Language :: Python\nProgramming Language :: Python :: 3\nProgramming Language :: Python :: 3.8\nProgramming Language :: Python :: 3.9\nProgramming Language :: Python :: 3.10\nProgramming Language :: Python :: 3.11\nProgramming Language :: Python :: 3.12\nProgramming Language :: Python :: 3.13\nProgramming Language :: Python :: Implementation :: CPython\nProgramming Language :: Python :: Implementation :: PyPy\nTopic :: Software Development :: Quality Assurance\nTopic :: Software Development :: Testing\n\"\"\"\n\ncov_ver_py = os.path.join(os.path.split(__file__)[0], \"coverage/version.py\")\nwith open(cov_ver_py) as version_file:\n    # __doc__ will be overwritten by version.py.\n    doc = __doc__\n    # Keep pylint happy.\n    __version__ = __url__ = version_info = \"\"\n    # Execute the code in version.py.\n    exec(compile(version_file.read(), cov_ver_py, \"exec\", dont_inherit=True))\n\nwith open(\"README.rst\") as readme:\n    readme_text = readme.read()\n\ntemp_url = __url__.replace(\"readthedocs\", \"@@\")\nassert \"@@\" not in readme_text\nlong_description = (\n    readme_text.replace(\"https://coverage.readthedocs.io/en/latest\", temp_url)\n    .replace(\"https://coverage.readthedocs.io\", temp_url)\n    .replace(\"@@\", \"readthedocs\")\n)\n\nwith open(\"CONTRIBUTORS.txt\", \"rb\") as contributors:\n    paras = contributors.read().split(b\"\\n\\n\")\n    num_others = len(paras[-1].splitlines())\n    num_others += 1  # Count Gareth Rees, who is mentioned in the top paragraph.\n\nclassifier_list = classifiers.splitlines()\n\nif version_info[3] == \"alpha\":\n    devstat = \"3 - Alpha\"\nelif version_info[3] in [\"beta\", \"candidate\"]:\n    devstat = \"4 - Beta\"\nelse:\n    assert version_info[3] == \"final\"\n    devstat = \"5 - Production/Stable\"\nclassifier_list.append(f\"Development Status :: {devstat}\")\n\n# Create the keyword arguments for setup()\n\nsetup_args = dict(\n    name=\"coverage\",\n    version=__version__,\n    packages=[\n        \"coverage\",\n    ],\n    package_data={\n        \"coverage\": [\n            \"htmlfiles/*.*\",\n            \"py.typed\",\n        ],\n    },\n    entry_points={\n        # Install a script as \"coverage\", and as \"coverage3\", and as\n        # \"coverage-3.7\" (or whatever).\n        \"console_scripts\": [\n            \"coverage = coverage.cmdline:main\",\n            \"coverage%d = coverage.cmdline:main\" % sys.version_info[:1],\n            \"coverage-%d.%d = coverage.cmdline:main\" % sys.version_info[:2],\n        ],\n    },\n    extras_require={\n        # Enable pyproject.toml support.\n        \"toml\": ['tomli; python_full_version<=\"3.11.0a6\"'],\n    },\n    # We need to get HTML assets from our htmlfiles directory.\n    zip_safe=False,\n    author=f\"Ned Batchelder and {num_others} others\",\n    author_email=\"ned@nedbatchelder.com\",\n    description=doc,\n    long_description=long_description,\n    long_description_content_type=\"text/x-rst\",\n    keywords=\"code coverage testing\",\n    license=\"Apache-2.0\",\n    license_files=[\"LICENSE.txt\"],\n    classifiers=classifier_list,\n    url=\"https://github.com/nedbat/coveragepy\",\n    project_urls={\n        \"Documentation\": __url__,\n        \"Funding\": (\n            \"https://tidelift.com/subscription/pkg/pypi-coverage\"\n            + \"?utm_source=pypi-coverage&utm_medium=referral&utm_campaign=pypi\"\n        ),\n        \"Issues\": \"https://github.com/nedbat/coveragepy/issues\",\n        \"Mastodon\": \"https://hachyderm.io/@coveragepy\",\n        \"Mastodon (nedbat)\": \"https://hachyderm.io/@nedbat\",\n    },\n    python_requires=\">=3.8\",  # minimum of PYVERSIONS\n)\n\n# A replacement for the build_ext command which raises a single exception\n# if the build fails, so we can fallback nicely.\n\next_errors = (\n    errors.CCompilerError,\n    errors.DistutilsExecError,\n    errors.DistutilsPlatformError,\n)\nif sys.platform == \"win32\":\n    # distutils.msvc9compiler can raise an IOError when failing to\n    # find the compiler\n    ext_errors += (IOError,)\n\n\nclass BuildFailed(Exception):\n    \"\"\"Raise this to indicate the C extension wouldn't build.\"\"\"\n\n    def __init__(self):\n        Exception.__init__(self)\n        self.cause = sys.exc_info()[1]  # work around py 2/3 different syntax\n\n\nclass ve_build_ext(build_ext):\n    \"\"\"Build C extensions, but fail with a straightforward exception.\"\"\"\n\n    def run(self):\n        \"\"\"Wrap `run` with `BuildFailed`.\"\"\"\n        try:\n            build_ext.run(self)\n        except errors.DistutilsPlatformError as exc:\n            raise BuildFailed() from exc\n\n    def build_extension(self, ext):\n        \"\"\"Wrap `build_extension` with `BuildFailed`.\"\"\"\n        try:\n            # Uncomment to test compile failure handling:\n            #   raise errors.CCompilerError(\"OOPS\")\n            build_ext.build_extension(self, ext)\n        except ext_errors as exc:\n            raise BuildFailed() from exc\n        except ValueError as err:\n            # this can happen on Windows 64 bit, see Python issue 7511\n            if \"'path'\" in str(err):  # works with both py 2/3\n                raise BuildFailed() from err\n            raise\n\n\n# There are a few reasons we might not be able to compile the C extension.\n# Figure out if we should attempt the C extension or not.\n\ncompile_extension = True\n\nif \"__pypy__\" in sys.builtin_module_names:\n    # Pypy can't compile C extensions\n    compile_extension = False\n\nif compile_extension:\n    setup_args.update(\n        dict(\n            ext_modules=[\n                Extension(\n                    \"coverage.tracer\",\n                    sources=[\n                        \"coverage/ctracer/datastack.c\",\n                        \"coverage/ctracer/filedisp.c\",\n                        \"coverage/ctracer/module.c\",\n                        \"coverage/ctracer/tracer.c\",\n                    ],\n                ),\n            ],\n            cmdclass={\n                \"build_ext\": ve_build_ext,\n            },\n        ),\n    )\n\n\ndef main():\n    \"\"\"Actually invoke setup() with the arguments we built above.\"\"\"\n    # For a variety of reasons, it might not be possible to install the C\n    # extension.  Try it with, and if it fails, try it without.\n    try:\n        setup(**setup_args)\n    except BuildFailed as exc:\n        msg = \"Couldn't install with extension module, trying without it...\"\n        exc_msg = f\"{exc.__class__.__name__}: {exc.cause}\"\n        print(f\"**\\n** {msg}\\n** {exc_msg}\\n**\")\n\n        del setup_args[\"ext_modules\"]\n        setup(**setup_args)\n\n\nif __name__ == \"__main__\":\n    main()\n", "__main__.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Be able to execute coverage.py by pointing Python at a working tree.\"\"\"\n\nimport runpy\nimport os\n\nPKG = \"coverage\"\n\nrun_globals = runpy.run_module(PKG, run_name=\"__main__\", alter_sys=True)\nexecuted = os.path.splitext(os.path.basename(run_globals[\"__file__\"]))[0]\n", "coverage/debug.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Control of and utilities for debugging.\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport contextlib\nimport functools\nimport inspect\nimport itertools\nimport os\nimport pprint\nimport re\nimport reprlib\nimport sys\nimport traceback\nimport types\nimport _thread\n\nfrom typing import (\n    overload,\n    Any, Callable, IO, Iterable, Iterator, Mapping,\n)\n\nfrom coverage.misc import human_sorted_items, isolate_module\nfrom coverage.types import AnyCallable, TWritable\n\nos = isolate_module(os)\n\n\n# When debugging, it can be helpful to force some options, especially when\n# debugging the configuration mechanisms you usually use to control debugging!\n# This is a list of forced debugging options.\nFORCED_DEBUG: list[str] = []\nFORCED_DEBUG_FILE = None\n\n\nclass DebugControl:\n    \"\"\"Control and output for debugging.\"\"\"\n\n    show_repr_attr = False      # For auto_repr\n\n    def __init__(\n        self,\n        options: Iterable[str],\n        output: IO[str] | None,\n        file_name: str | None = None,\n    ) -> None:\n        \"\"\"Configure the options and output file for debugging.\"\"\"\n        self.options = list(options) + FORCED_DEBUG\n        self.suppress_callers = False\n\n        filters = []\n        if self.should(\"process\"):\n            filters.append(CwdTracker().filter)\n            filters.append(ProcessTracker().filter)\n        if self.should(\"pytest\"):\n            filters.append(PytestTracker().filter)\n        if self.should(\"pid\"):\n            filters.append(add_pid_and_tid)\n\n        self.output = DebugOutputFile.get_one(\n            output,\n            file_name=file_name,\n            filters=filters,\n        )\n        self.raw_output = self.output.outfile\n\n    def __repr__(self) -> str:\n        return f\"<DebugControl options={self.options!r} raw_output={self.raw_output!r}>\"\n\n    def should(self, option: str) -> bool:\n        \"\"\"Decide whether to output debug information in category `option`.\"\"\"\n        if option == \"callers\" and self.suppress_callers:\n            return False\n        return (option in self.options)\n\n    @contextlib.contextmanager\n    def without_callers(self) -> Iterator[None]:\n        \"\"\"A context manager to prevent call stacks from being logged.\"\"\"\n        old = self.suppress_callers\n        self.suppress_callers = True\n        try:\n            yield\n        finally:\n            self.suppress_callers = old\n\n    def write(self, msg: str, *, exc: BaseException | None = None) -> None:\n        \"\"\"Write a line of debug output.\n\n        `msg` is the line to write. A newline will be appended.\n\n        If `exc` is provided, a stack trace of the exception will be written\n        after the message.\n\n        \"\"\"\n        self.output.write(msg + \"\\n\")\n        if exc is not None:\n            self.output.write(\"\".join(traceback.format_exception(None, exc, exc.__traceback__)))\n        if self.should(\"self\"):\n            caller_self = inspect.stack()[1][0].f_locals.get(\"self\")\n            if caller_self is not None:\n                self.output.write(f\"self: {caller_self!r}\\n\")\n        if self.should(\"callers\"):\n            dump_stack_frames(out=self.output, skip=1)\n        self.output.flush()\n\n\nclass NoDebugging(DebugControl):\n    \"\"\"A replacement for DebugControl that will never try to do anything.\"\"\"\n    def __init__(self) -> None:\n        # pylint: disable=super-init-not-called\n        ...\n\n    def should(self, option: str) -> bool:\n        \"\"\"Should we write debug messages?  Never.\"\"\"\n        return False\n\n    def write(self, msg: str, *, exc: BaseException | None = None) -> None:\n        \"\"\"This will never be called.\"\"\"\n        raise AssertionError(\"NoDebugging.write should never be called.\")\n\n\ndef info_header(label: str) -> str:\n    \"\"\"Make a nice header string.\"\"\"\n    return \"--{:-<60s}\".format(\" \"+label+\" \")\n\n\ndef info_formatter(info: Iterable[tuple[str, Any]]) -> Iterator[str]:\n    \"\"\"Produce a sequence of formatted lines from info.\n\n    `info` is a sequence of pairs (label, data).  The produced lines are\n    nicely formatted, ready to print.\n\n    \"\"\"\n    info = list(info)\n    if not info:\n        return\n    label_len = 30\n    assert all(len(l) < label_len for l, _ in info)\n    for label, data in info:\n        if data == []:\n            data = \"-none-\"\n        if isinstance(data, tuple) and len(repr(tuple(data))) < 30:\n            # Convert to tuple to scrub namedtuples.\n            yield \"%*s: %r\" % (label_len, label, tuple(data))\n        elif isinstance(data, (list, set, tuple)):\n            prefix = \"%*s:\" % (label_len, label)\n            for e in data:\n                yield \"%*s %s\" % (label_len+1, prefix, e)\n                prefix = \"\"\n        else:\n            yield \"%*s: %s\" % (label_len, label, data)\n\n\ndef write_formatted_info(\n    write: Callable[[str], None],\n    header: str,\n    info: Iterable[tuple[str, Any]],\n) -> None:\n    \"\"\"Write a sequence of (label,data) pairs nicely.\n\n    `write` is a function write(str) that accepts each line of output.\n    `header` is a string to start the section.  `info` is a sequence of\n    (label, data) pairs, where label is a str, and data can be a single\n    value, or a list/set/tuple.\n\n    \"\"\"\n    write(info_header(header))\n    for line in info_formatter(info):\n        write(f\" {line}\")\n\n\ndef exc_one_line(exc: Exception) -> str:\n    \"\"\"Get a one-line summary of an exception, including class name and message.\"\"\"\n    lines = traceback.format_exception_only(type(exc), exc)\n    return \"|\".join(l.rstrip() for l in lines)\n\n\n_FILENAME_REGEXES: list[tuple[str, str]] = [\n    (r\".*[/\\\\]pytest-of-.*[/\\\\]pytest-\\d+([/\\\\]popen-gw\\d+)?\", \"tmp:\"),\n]\n_FILENAME_SUBS: list[tuple[str, str]] = []\n\n@overload\ndef short_filename(filename: str) -> str:\n    pass\n\n@overload\ndef short_filename(filename: None) -> None:\n    pass\n\ndef short_filename(filename: str | None) -> str | None:\n    \"\"\"Shorten a file name. Directories are replaced by prefixes like 'syspath:'\"\"\"\n    if not _FILENAME_SUBS:\n        for pathdir in sys.path:\n            _FILENAME_SUBS.append((pathdir, \"syspath:\"))\n        import coverage\n        _FILENAME_SUBS.append((os.path.dirname(coverage.__file__), \"cov:\"))\n        _FILENAME_SUBS.sort(key=(lambda pair: len(pair[0])), reverse=True)\n    if filename is not None:\n        for pat, sub in _FILENAME_REGEXES:\n            filename = re.sub(pat, sub, filename)\n        for before, after in _FILENAME_SUBS:\n            filename = filename.replace(before, after)\n    return filename\n\n\ndef short_stack(\n    skip: int = 0,\n    full: bool = False,\n    frame_ids: bool = False,\n    short_filenames: bool = False,\n) -> str:\n    \"\"\"Return a string summarizing the call stack.\n\n    The string is multi-line, with one line per stack frame. Each line shows\n    the function name, the file name, and the line number:\n\n        ...\n        start_import_stop : /Users/ned/coverage/trunk/tests/coveragetest.py:95\n        import_local_file : /Users/ned/coverage/trunk/tests/coveragetest.py:81\n        import_local_file : /Users/ned/coverage/trunk/coverage/backward.py:159\n        ...\n\n    `skip` is the number of closest immediate frames to skip, so that debugging\n    functions can call this and not be included in the result.\n\n    If `full` is true, then include all frames.  Otherwise, initial \"boring\"\n    frames (ones in site-packages and earlier) are omitted.\n\n    `short_filenames` will shorten filenames using `short_filename`, to reduce\n    the amount of repetitive noise in stack traces.\n\n    \"\"\"\n    # Regexes in initial frames that we don't care about.\n    BORING_PRELUDE = [\n        \"<string>\",             # pytest-xdist has string execution.\n        r\"\\bigor.py$\",          # Our test runner.\n        r\"\\bsite-packages\\b\",   # pytest etc getting to our tests.\n    ]\n\n    stack: Iterable[inspect.FrameInfo] = inspect.stack()[:skip:-1]\n    if not full:\n        for pat in BORING_PRELUDE:\n            stack = itertools.dropwhile(\n                (lambda fi, pat=pat: re.search(pat, fi.filename)),  # type: ignore[misc]\n                stack,\n            )\n    lines = []\n    for frame_info in stack:\n        line = f\"{frame_info.function:>30s} : \"\n        if frame_ids:\n            line += f\"{id(frame_info.frame):#x} \"\n        filename = frame_info.filename\n        if short_filenames:\n            filename = short_filename(filename)\n        line += f\"{filename}:{frame_info.lineno}\"\n        lines.append(line)\n    return \"\\n\".join(lines)\n\n\ndef dump_stack_frames(out: TWritable, skip: int = 0) -> None:\n    \"\"\"Print a summary of the stack to `out`.\"\"\"\n    out.write(short_stack(skip=skip+1) + \"\\n\")\n\n\ndef clipped_repr(text: str, numchars: int = 50) -> str:\n    \"\"\"`repr(text)`, but limited to `numchars`.\"\"\"\n    r = reprlib.Repr()\n    r.maxstring = numchars\n    return r.repr(text)\n\n\ndef short_id(id64: int) -> int:\n    \"\"\"Given a 64-bit id, make a shorter 16-bit one.\"\"\"\n    id16 = 0\n    for offset in range(0, 64, 16):\n        id16 ^= id64 >> offset\n    return id16 & 0xFFFF\n\n\ndef add_pid_and_tid(text: str) -> str:\n    \"\"\"A filter to add pid and tid to debug messages.\"\"\"\n    # Thread ids are useful, but too long. Make a shorter one.\n    tid = f\"{short_id(_thread.get_ident()):04x}\"\n    text = f\"{os.getpid():5d}.{tid}: {text}\"\n    return text\n\n\nAUTO_REPR_IGNORE = {\"$coverage.object_id\"}\n\ndef auto_repr(self: Any) -> str:\n    \"\"\"A function implementing an automatic __repr__ for debugging.\"\"\"\n    show_attrs = (\n        (k, v) for k, v in self.__dict__.items()\n        if getattr(v, \"show_repr_attr\", True)\n        and not inspect.ismethod(v)\n        and k not in AUTO_REPR_IGNORE\n    )\n    return \"<{klass} @{id:#x}{attrs}>\".format(\n        klass=self.__class__.__name__,\n        id=id(self),\n        attrs=\"\".join(f\" {k}={v!r}\" for k, v in show_attrs),\n    )\n\n\ndef simplify(v: Any) -> Any:                                # pragma: debugging\n    \"\"\"Turn things which are nearly dict/list/etc into dict/list/etc.\"\"\"\n    if isinstance(v, dict):\n        return {k:simplify(vv) for k, vv in v.items()}\n    elif isinstance(v, (list, tuple)):\n        return type(v)(simplify(vv) for vv in v)\n    elif hasattr(v, \"__dict__\"):\n        return simplify({\".\"+k: v for k, v in v.__dict__.items()})\n    else:\n        return v\n\n\ndef pp(v: Any) -> None:                                     # pragma: debugging\n    \"\"\"Debug helper to pretty-print data, including SimpleNamespace objects.\"\"\"\n    # Might not be needed in 3.9+\n    pprint.pprint(simplify(v))\n\n\ndef filter_text(text: str, filters: Iterable[Callable[[str], str]]) -> str:\n    \"\"\"Run `text` through a series of filters.\n\n    `filters` is a list of functions. Each takes a string and returns a\n    string.  Each is run in turn. After each filter, the text is split into\n    lines, and each line is passed through the next filter.\n\n    Returns: the final string that results after all of the filters have\n    run.\n\n    \"\"\"\n    clean_text = text.rstrip()\n    ending = text[len(clean_text):]\n    text = clean_text\n    for filter_fn in filters:\n        lines = []\n        for line in text.splitlines():\n            lines.extend(filter_fn(line).splitlines())\n        text = \"\\n\".join(lines)\n    return text + ending\n\n\nclass CwdTracker:\n    \"\"\"A class to add cwd info to debug messages.\"\"\"\n    def __init__(self) -> None:\n        self.cwd: str | None = None\n\n    def filter(self, text: str) -> str:\n        \"\"\"Add a cwd message for each new cwd.\"\"\"\n        cwd = os.getcwd()\n        if cwd != self.cwd:\n            text = f\"cwd is now {cwd!r}\\n\" + text\n            self.cwd = cwd\n        return text\n\n\nclass ProcessTracker:\n    \"\"\"Track process creation for debug logging.\"\"\"\n    def __init__(self) -> None:\n        self.pid: int = os.getpid()\n        self.did_welcome = False\n\n    def filter(self, text: str) -> str:\n        \"\"\"Add a message about how new processes came to be.\"\"\"\n        welcome = \"\"\n        pid = os.getpid()\n        if self.pid != pid:\n            welcome = f\"New process: forked {self.pid} -> {pid}\\n\"\n            self.pid = pid\n        elif not self.did_welcome:\n            argv = getattr(sys, \"argv\", None)\n            welcome = (\n                f\"New process: {pid=}, executable: {sys.executable!r}\\n\"\n                + f\"New process: cmd: {argv!r}\\n\"\n            )\n            if hasattr(os, \"getppid\"):\n                welcome += f\"New process parent pid: {os.getppid()!r}\\n\"\n\n        if welcome:\n            self.did_welcome = True\n            return welcome + text\n        else:\n            return text\n\n\nclass PytestTracker:\n    \"\"\"Track the current pytest test name to add to debug messages.\"\"\"\n    def __init__(self) -> None:\n        self.test_name: str | None = None\n\n    def filter(self, text: str) -> str:\n        \"\"\"Add a message when the pytest test changes.\"\"\"\n        test_name = os.getenv(\"PYTEST_CURRENT_TEST\")\n        if test_name != self.test_name:\n            text = f\"Pytest context: {test_name}\\n\" + text\n            self.test_name = test_name\n        return text\n\n\nclass DebugOutputFile:\n    \"\"\"A file-like object that includes pid and cwd information.\"\"\"\n    def __init__(\n        self,\n        outfile: IO[str] | None,\n        filters: Iterable[Callable[[str], str]],\n    ):\n        self.outfile = outfile\n        self.filters = list(filters)\n        self.pid = os.getpid()\n\n    @classmethod\n    def get_one(\n        cls,\n        fileobj: IO[str] | None = None,\n        file_name: str | None = None,\n        filters: Iterable[Callable[[str], str]] = (),\n        interim: bool = False,\n    ) -> DebugOutputFile:\n        \"\"\"Get a DebugOutputFile.\n\n        If `fileobj` is provided, then a new DebugOutputFile is made with it.\n\n        If `fileobj` isn't provided, then a file is chosen (`file_name` if\n        provided, or COVERAGE_DEBUG_FILE, or stderr), and a process-wide\n        singleton DebugOutputFile is made.\n\n        `filters` are the text filters to apply to the stream to annotate with\n        pids, etc.\n\n        If `interim` is true, then a future `get_one` can replace this one.\n\n        \"\"\"\n        if fileobj is not None:\n            # Make DebugOutputFile around the fileobj passed.\n            return cls(fileobj, filters)\n\n        the_one, is_interim = cls._get_singleton_data()\n        if the_one is None or is_interim:\n            if file_name is not None:\n                fileobj = open(file_name, \"a\", encoding=\"utf-8\")\n            else:\n                # $set_env.py: COVERAGE_DEBUG_FILE - Where to write debug output\n                file_name = os.getenv(\"COVERAGE_DEBUG_FILE\", FORCED_DEBUG_FILE)\n                if file_name in (\"stdout\", \"stderr\"):\n                    fileobj = getattr(sys, file_name)\n                elif file_name:\n                    fileobj = open(file_name, \"a\", encoding=\"utf-8\")\n                    atexit.register(fileobj.close)\n                else:\n                    fileobj = sys.stderr\n            the_one = cls(fileobj, filters)\n            cls._set_singleton_data(the_one, interim)\n\n        if not(the_one.filters):\n            the_one.filters = list(filters)\n        return the_one\n\n    # Because of the way igor.py deletes and re-imports modules,\n    # this class can be defined more than once. But we really want\n    # a process-wide singleton. So stash it in sys.modules instead of\n    # on a class attribute. Yes, this is aggressively gross.\n\n    SYS_MOD_NAME = \"$coverage.debug.DebugOutputFile.the_one\"\n    SINGLETON_ATTR = \"the_one_and_is_interim\"\n\n    @classmethod\n    def _set_singleton_data(cls, the_one: DebugOutputFile, interim: bool) -> None:\n        \"\"\"Set the one DebugOutputFile to rule them all.\"\"\"\n        singleton_module = types.ModuleType(cls.SYS_MOD_NAME)\n        setattr(singleton_module, cls.SINGLETON_ATTR, (the_one, interim))\n        sys.modules[cls.SYS_MOD_NAME] = singleton_module\n\n    @classmethod\n    def _get_singleton_data(cls) -> tuple[DebugOutputFile | None, bool]:\n        \"\"\"Get the one DebugOutputFile.\"\"\"\n        singleton_module = sys.modules.get(cls.SYS_MOD_NAME)\n        return getattr(singleton_module, cls.SINGLETON_ATTR, (None, True))\n\n    @classmethod\n    def _del_singleton_data(cls) -> None:\n        \"\"\"Delete the one DebugOutputFile, just for tests to use.\"\"\"\n        if cls.SYS_MOD_NAME in sys.modules:\n            del sys.modules[cls.SYS_MOD_NAME]\n\n    def write(self, text: str) -> None:\n        \"\"\"Just like file.write, but filter through all our filters.\"\"\"\n        assert self.outfile is not None\n        self.outfile.write(filter_text(text, self.filters))\n        self.outfile.flush()\n\n    def flush(self) -> None:\n        \"\"\"Flush our file.\"\"\"\n        assert self.outfile is not None\n        self.outfile.flush()\n\n\ndef log(msg: str, stack: bool = False) -> None:             # pragma: debugging\n    \"\"\"Write a log message as forcefully as possible.\"\"\"\n    out = DebugOutputFile.get_one(interim=True)\n    out.write(msg+\"\\n\")\n    if stack:\n        dump_stack_frames(out=out, skip=1)\n\n\ndef decorate_methods(\n    decorator: Callable[..., Any],\n    butnot: Iterable[str] = (),\n    private: bool = False,\n) -> Callable[..., Any]:                                    # pragma: debugging\n    \"\"\"A class decorator to apply a decorator to methods.\"\"\"\n    def _decorator(cls):                                    # type: ignore[no-untyped-def]\n        for name, meth in inspect.getmembers(cls, inspect.isroutine):\n            if name not in cls.__dict__:\n                continue\n            if name != \"__init__\":\n                if not private and name.startswith(\"_\"):\n                    continue\n            if name in butnot:\n                continue\n            setattr(cls, name, decorator(meth))\n        return cls\n    return _decorator\n\n\ndef break_in_pudb(func: AnyCallable) -> AnyCallable:  # pragma: debugging\n    \"\"\"A function decorator to stop in the debugger for each call.\"\"\"\n    @functools.wraps(func)\n    def _wrapper(*args: Any, **kwargs: Any) -> Any:\n        import pudb\n        sys.stdout = sys.__stdout__\n        pudb.set_trace()\n        return func(*args, **kwargs)\n    return _wrapper\n\n\nOBJ_IDS = itertools.count()\nCALLS = itertools.count()\nOBJ_ID_ATTR = \"$coverage.object_id\"\n\ndef show_calls(\n    show_args: bool = True,\n    show_stack: bool = False,\n    show_return: bool = False,\n) -> Callable[..., Any]:                                    # pragma: debugging\n    \"\"\"A method decorator to debug-log each call to the function.\"\"\"\n    def _decorator(func: AnyCallable) -> AnyCallable:\n        @functools.wraps(func)\n        def _wrapper(self: Any, *args: Any, **kwargs: Any) -> Any:\n            oid = getattr(self, OBJ_ID_ATTR, None)\n            if oid is None:\n                oid = f\"{os.getpid():08d} {next(OBJ_IDS):04d}\"\n                setattr(self, OBJ_ID_ATTR, oid)\n            extra = \"\"\n            if show_args:\n                eargs = \", \".join(map(repr, args))\n                ekwargs = \", \".join(\"{}={!r}\".format(*item) for item in kwargs.items())\n                extra += \"(\"\n                extra += eargs\n                if eargs and ekwargs:\n                    extra += \", \"\n                extra += ekwargs\n                extra += \")\"\n            if show_stack:\n                extra += \" @ \"\n                extra += \"; \".join(short_stack(short_filenames=True).splitlines())\n            callid = next(CALLS)\n            msg = f\"{oid} {callid:04d} {func.__name__}{extra}\\n\"\n            DebugOutputFile.get_one(interim=True).write(msg)\n            ret = func(self, *args, **kwargs)\n            if show_return:\n                msg = f\"{oid} {callid:04d} {func.__name__} return {ret!r}\\n\"\n                DebugOutputFile.get_one(interim=True).write(msg)\n            return ret\n        return _wrapper\n    return _decorator\n\n\ndef relevant_environment_display(env: Mapping[str, str]) -> list[tuple[str, str]]:\n    \"\"\"Filter environment variables for a debug display.\n\n    Select variables to display (with COV or PY in the name, or HOME, TEMP, or\n    TMP), and also cloak sensitive values with asterisks.\n\n    Arguments:\n        env: a dict of environment variable names and values.\n\n    Returns:\n        A list of pairs (name, value) to show.\n\n    \"\"\"\n    slugs = {\"COV\", \"PY\"}\n    include = {\"HOME\", \"TEMP\", \"TMP\"}\n    cloak = {\"API\", \"TOKEN\", \"KEY\", \"SECRET\", \"PASS\", \"SIGNATURE\"}\n\n    to_show = []\n    for name, val in env.items():\n        keep = False\n        if name in include:\n            keep = True\n        elif any(slug in name for slug in slugs):\n            keep = True\n        if keep:\n            if any(slug in name for slug in cloak):\n                val = re.sub(r\"\\w\", \"*\", val)\n            to_show.append((name, val))\n    return human_sorted_items(to_show)\n", "coverage/plugin.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\n.. versionadded:: 4.0\n\nPlug-in interfaces for coverage.py.\n\nCoverage.py supports a few different kinds of plug-ins that change its\nbehavior:\n\n* File tracers implement tracing of non-Python file types.\n\n* Configurers add custom configuration, using Python code to change the\n  configuration.\n\n* Dynamic context switchers decide when the dynamic context has changed, for\n  example, to record what test function produced the coverage.\n\nTo write a coverage.py plug-in, create a module with a subclass of\n:class:`~coverage.CoveragePlugin`.  You will override methods in your class to\nparticipate in various aspects of coverage.py's processing.\nDifferent types of plug-ins have to override different methods.\n\nAny plug-in can optionally implement :meth:`~coverage.CoveragePlugin.sys_info`\nto provide debugging information about their operation.\n\nYour module must also contain a ``coverage_init`` function that registers an\ninstance of your plug-in class::\n\n    import coverage\n\n    class MyPlugin(coverage.CoveragePlugin):\n        ...\n\n    def coverage_init(reg, options):\n        reg.add_file_tracer(MyPlugin())\n\nYou use the `reg` parameter passed to your ``coverage_init`` function to\nregister your plug-in object.  The registration method you call depends on\nwhat kind of plug-in it is.\n\nIf your plug-in takes options, the `options` parameter is a dictionary of your\nplug-in's options from the coverage.py configuration file.  Use them however\nyou want to configure your object before registering it.\n\nCoverage.py will store its own information on your plug-in object, using\nattributes whose names start with ``_coverage_``.  Don't be startled.\n\n.. warning::\n    Plug-ins are imported by coverage.py before it begins measuring code.\n    If you write a plugin in your own project, it might import your product\n    code before coverage.py can start measuring.  This can result in your\n    own code being reported as missing.\n\n    One solution is to put your plugins in your project tree, but not in\n    your importable Python package.\n\n\n.. _file_tracer_plugins:\n\nFile Tracers\n============\n\nFile tracers implement measurement support for non-Python files.  File tracers\nimplement the :meth:`~coverage.CoveragePlugin.file_tracer` method to claim\nfiles and the :meth:`~coverage.CoveragePlugin.file_reporter` method to report\non those files.\n\nIn your ``coverage_init`` function, use the ``add_file_tracer`` method to\nregister your file tracer.\n\n\n.. _configurer_plugins:\n\nConfigurers\n===========\n\n.. versionadded:: 4.5\n\nConfigurers modify the configuration of coverage.py during start-up.\nConfigurers implement the :meth:`~coverage.CoveragePlugin.configure` method to\nchange the configuration.\n\nIn your ``coverage_init`` function, use the ``add_configurer`` method to\nregister your configurer.\n\n\n.. _dynamic_context_plugins:\n\nDynamic Context Switchers\n=========================\n\n.. versionadded:: 5.0\n\nDynamic context switcher plugins implement the\n:meth:`~coverage.CoveragePlugin.dynamic_context` method to dynamically compute\nthe context label for each measured frame.\n\nComputed context labels are useful when you want to group measured data without\nmodifying the source code.\n\nFor example, you could write a plugin that checks `frame.f_code` to inspect\nthe currently executed method, and set the context label to a fully qualified\nmethod name if it's an instance method of `unittest.TestCase` and the method\nname starts with 'test'.  Such a plugin would provide basic coverage grouping\nby test and could be used with test runners that have no built-in coveragepy\nsupport.\n\nIn your ``coverage_init`` function, use the ``add_dynamic_context`` method to\nregister your dynamic context switcher.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport dataclasses\nimport functools\n\nfrom types import FrameType\nfrom typing import Any, Iterable\n\nfrom coverage import files\nfrom coverage.misc import _needs_to_implement\nfrom coverage.types import TArc, TConfigurable, TLineNo, TSourceTokenLines\n\n\nclass CoveragePlugin:\n    \"\"\"Base class for coverage.py plug-ins.\"\"\"\n\n    _coverage_plugin_name: str\n    _coverage_enabled: bool\n\n    def file_tracer(self, filename: str) -> FileTracer | None: # pylint: disable=unused-argument\n        \"\"\"Get a :class:`FileTracer` object for a file.\n\n        Plug-in type: file tracer.\n\n        Every Python source file is offered to your plug-in to give it a chance\n        to take responsibility for tracing the file.  If your plug-in can\n        handle the file, it should return a :class:`FileTracer` object.\n        Otherwise return None.\n\n        There is no way to register your plug-in for particular files.\n        Instead, this method is invoked for all  files as they are executed,\n        and the plug-in decides whether it can trace the file or not.\n        Be prepared for `filename` to refer to all kinds of files that have\n        nothing to do with your plug-in.\n\n        The file name will be a Python file being executed.  There are two\n        broad categories of behavior for a plug-in, depending on the kind of\n        files your plug-in supports:\n\n        * Static file names: each of your original source files has been\n          converted into a distinct Python file.  Your plug-in is invoked with\n          the Python file name, and it maps it back to its original source\n          file.\n\n        * Dynamic file names: all of your source files are executed by the same\n          Python file.  In this case, your plug-in implements\n          :meth:`FileTracer.dynamic_source_filename` to provide the actual\n          source file for each execution frame.\n\n        `filename` is a string, the path to the file being considered.  This is\n        the absolute real path to the file.  If you are comparing to other\n        paths, be sure to take this into account.\n\n        Returns a :class:`FileTracer` object to use to trace `filename`, or\n        None if this plug-in cannot trace this file.\n\n        \"\"\"\n        return None\n\n    def file_reporter(\n        self,\n        filename: str,                  # pylint: disable=unused-argument\n    ) -> FileReporter | str:      # str should be Literal[\"python\"]\n        \"\"\"Get the :class:`FileReporter` class to use for a file.\n\n        Plug-in type: file tracer.\n\n        This will only be invoked if `filename` returns non-None from\n        :meth:`file_tracer`.  It's an error to return None from this method.\n\n        Returns a :class:`FileReporter` object to use to report on `filename`,\n        or the string `\"python\"` to have coverage.py treat the file as Python.\n\n        \"\"\"\n        _needs_to_implement(self, \"file_reporter\")\n\n    def dynamic_context(\n        self,\n        frame: FrameType,               # pylint: disable=unused-argument\n    ) -> str | None:\n        \"\"\"Get the dynamically computed context label for `frame`.\n\n        Plug-in type: dynamic context.\n\n        This method is invoked for each frame when outside of a dynamic\n        context, to see if a new dynamic context should be started.  If it\n        returns a string, a new context label is set for this and deeper\n        frames.  The dynamic context ends when this frame returns.\n\n        Returns a string to start a new dynamic context, or None if no new\n        context should be started.\n\n        \"\"\"\n        return None\n\n    def find_executable_files(\n        self,\n        src_dir: str,                   # pylint: disable=unused-argument\n    ) -> Iterable[str]:\n        \"\"\"Yield all of the executable files in `src_dir`, recursively.\n\n        Plug-in type: file tracer.\n\n        Executability is a plug-in-specific property, but generally means files\n        which would have been considered for coverage analysis, had they been\n        included automatically.\n\n        Returns or yields a sequence of strings, the paths to files that could\n        have been executed, including files that had been executed.\n\n        \"\"\"\n        return []\n\n    def configure(self, config: TConfigurable) -> None:\n        \"\"\"Modify the configuration of coverage.py.\n\n        Plug-in type: configurer.\n\n        This method is called during coverage.py start-up, to give your plug-in\n        a chance to change the configuration.  The `config` parameter is an\n        object with :meth:`~coverage.Coverage.get_option` and\n        :meth:`~coverage.Coverage.set_option` methods.  Do not call any other\n        methods on the `config` object.\n\n        \"\"\"\n        pass\n\n    def sys_info(self) -> Iterable[tuple[str, Any]]:\n        \"\"\"Get a list of information useful for debugging.\n\n        Plug-in type: any.\n\n        This method will be invoked for ``--debug=sys``.  Your\n        plug-in can return any information it wants to be displayed.\n\n        Returns a list of pairs: `[(name, value), ...]`.\n\n        \"\"\"\n        return []\n\n\nclass CoveragePluginBase:\n    \"\"\"Plugins produce specialized objects, which point back to the original plugin.\"\"\"\n    _coverage_plugin: CoveragePlugin\n\n\nclass FileTracer(CoveragePluginBase):\n    \"\"\"Support needed for files during the execution phase.\n\n    File tracer plug-ins implement subclasses of FileTracer to return from\n    their :meth:`~CoveragePlugin.file_tracer` method.\n\n    You may construct this object from :meth:`CoveragePlugin.file_tracer` any\n    way you like.  A natural choice would be to pass the file name given to\n    `file_tracer`.\n\n    `FileTracer` objects should only be created in the\n    :meth:`CoveragePlugin.file_tracer` method.\n\n    See :ref:`howitworks` for details of the different coverage.py phases.\n\n    \"\"\"\n\n    def source_filename(self) -> str:\n        \"\"\"The source file name for this file.\n\n        This may be any file name you like.  A key responsibility of a plug-in\n        is to own the mapping from Python execution back to whatever source\n        file name was originally the source of the code.\n\n        See :meth:`CoveragePlugin.file_tracer` for details about static and\n        dynamic file names.\n\n        Returns the file name to credit with this execution.\n\n        \"\"\"\n        _needs_to_implement(self, \"source_filename\")\n\n    def has_dynamic_source_filename(self) -> bool:\n        \"\"\"Does this FileTracer have dynamic source file names?\n\n        FileTracers can provide dynamically determined file names by\n        implementing :meth:`dynamic_source_filename`.  Invoking that function\n        is expensive. To determine whether to invoke it, coverage.py uses the\n        result of this function to know if it needs to bother invoking\n        :meth:`dynamic_source_filename`.\n\n        See :meth:`CoveragePlugin.file_tracer` for details about static and\n        dynamic file names.\n\n        Returns True if :meth:`dynamic_source_filename` should be called to get\n        dynamic source file names.\n\n        \"\"\"\n        return False\n\n    def dynamic_source_filename(\n        self,\n        filename: str,                  # pylint: disable=unused-argument\n        frame: FrameType,               # pylint: disable=unused-argument\n    ) -> str | None:\n        \"\"\"Get a dynamically computed source file name.\n\n        Some plug-ins need to compute the source file name dynamically for each\n        frame.\n\n        This function will not be invoked if\n        :meth:`has_dynamic_source_filename` returns False.\n\n        Returns the source file name for this frame, or None if this frame\n        shouldn't be measured.\n\n        \"\"\"\n        return None\n\n    def line_number_range(self, frame: FrameType) -> tuple[TLineNo, TLineNo]:\n        \"\"\"Get the range of source line numbers for a given a call frame.\n\n        The call frame is examined, and the source line number in the original\n        file is returned.  The return value is a pair of numbers, the starting\n        line number and the ending line number, both inclusive.  For example,\n        returning (5, 7) means that lines 5, 6, and 7 should be considered\n        executed.\n\n        This function might decide that the frame doesn't indicate any lines\n        from the source file were executed.  Return (-1, -1) in this case to\n        tell coverage.py that no lines should be recorded for this frame.\n\n        \"\"\"\n        lineno = frame.f_lineno\n        return lineno, lineno\n\n\n@dataclasses.dataclass\nclass CodeRegion:\n    \"\"\"Data for a region of code found by :meth:`FileReporter.code_regions`.\"\"\"\n\n    #: The kind of region, like `\"function\"` or `\"class\"`. Must be one of the\n    #: singular values returned by :meth:`FileReporter.code_region_kinds`.\n    kind: str\n\n    #: The name of the region. For example, a function or class name.\n    name: str\n\n    #: The line in the source file to link to when navigating to the region.\n    #: Can be a line not mentioned in `lines`.\n    start: int\n\n    #: The lines in the region. Should be lines that could be executed in the\n    #: region.  For example, a class region includes all of the lines in the\n    #: methods of the class, but not the lines defining class attributes, since\n    #: they are executed on import, not as part of exercising the class.  The\n    #: set can include non-executable lines like blanks and comments.\n    lines: set[int]\n\n    def __lt__(self, other: CodeRegion) -> bool:\n        \"\"\"To support sorting to make test-writing easier.\"\"\"\n        if self.name == other.name:\n            return min(self.lines) < min(other.lines)\n        return self.name < other.name\n\n\n@functools.total_ordering\nclass FileReporter(CoveragePluginBase):\n    \"\"\"Support needed for files during the analysis and reporting phases.\n\n    File tracer plug-ins implement a subclass of `FileReporter`, and return\n    instances from their :meth:`CoveragePlugin.file_reporter` method.\n\n    There are many methods here, but only :meth:`lines` is required, to provide\n    the set of executable lines in the file.\n\n    See :ref:`howitworks` for details of the different coverage.py phases.\n\n    \"\"\"\n\n    def __init__(self, filename: str) -> None:\n        \"\"\"Simple initialization of a `FileReporter`.\n\n        The `filename` argument is the path to the file being reported.  This\n        will be available as the `.filename` attribute on the object.  Other\n        method implementations on this base class rely on this attribute.\n\n        \"\"\"\n        self.filename = filename\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__} filename={self.filename!r}>\"\n\n    def relative_filename(self) -> str:\n        \"\"\"Get the relative file name for this file.\n\n        This file path will be displayed in reports.  The default\n        implementation will supply the actual project-relative file path.  You\n        only need to supply this method if you have an unusual syntax for file\n        paths.\n\n        \"\"\"\n        return files.relative_filename(self.filename)\n\n    def source(self) -> str:\n        \"\"\"Get the source for the file.\n\n        Returns a Unicode string.\n\n        The base implementation simply reads the `self.filename` file and\n        decodes it as UTF-8.  Override this method if your file isn't readable\n        as a text file, or if you need other encoding support.\n\n        \"\"\"\n        with open(self.filename, encoding=\"utf-8\") as f:\n            return f.read()\n\n    def lines(self) -> set[TLineNo]:\n        \"\"\"Get the executable lines in this file.\n\n        Your plug-in must determine which lines in the file were possibly\n        executable.  This method returns a set of those line numbers.\n\n        Returns a set of line numbers.\n\n        \"\"\"\n        _needs_to_implement(self, \"lines\")\n\n    def excluded_lines(self) -> set[TLineNo]:\n        \"\"\"Get the excluded executable lines in this file.\n\n        Your plug-in can use any method it likes to allow the user to exclude\n        executable lines from consideration.\n\n        Returns a set of line numbers.\n\n        The base implementation returns the empty set.\n\n        \"\"\"\n        return set()\n\n    def translate_lines(self, lines: Iterable[TLineNo]) -> set[TLineNo]:\n        \"\"\"Translate recorded lines into reported lines.\n\n        Some file formats will want to report lines slightly differently than\n        they are recorded.  For example, Python records the last line of a\n        multi-line statement, but reports are nicer if they mention the first\n        line.\n\n        Your plug-in can optionally define this method to perform these kinds\n        of adjustment.\n\n        `lines` is a sequence of integers, the recorded line numbers.\n\n        Returns a set of integers, the adjusted line numbers.\n\n        The base implementation returns the numbers unchanged.\n\n        \"\"\"\n        return set(lines)\n\n    def arcs(self) -> set[TArc]:\n        \"\"\"Get the executable arcs in this file.\n\n        To support branch coverage, your plug-in needs to be able to indicate\n        possible execution paths, as a set of line number pairs.  Each pair is\n        a `(prev, next)` pair indicating that execution can transition from the\n        `prev` line number to the `next` line number.\n\n        Returns a set of pairs of line numbers.  The default implementation\n        returns an empty set.\n\n        \"\"\"\n        return set()\n\n    def no_branch_lines(self) -> set[TLineNo]:\n        \"\"\"Get the lines excused from branch coverage in this file.\n\n        Your plug-in can use any method it likes to allow the user to exclude\n        lines from consideration of branch coverage.\n\n        Returns a set of line numbers.\n\n        The base implementation returns the empty set.\n\n        \"\"\"\n        return set()\n\n    def translate_arcs(self, arcs: Iterable[TArc]) -> set[TArc]:\n        \"\"\"Translate recorded arcs into reported arcs.\n\n        Similar to :meth:`translate_lines`, but for arcs.  `arcs` is a set of\n        line number pairs.\n\n        Returns a set of line number pairs.\n\n        The default implementation returns `arcs` unchanged.\n\n        \"\"\"\n        return set(arcs)\n\n    def exit_counts(self) -> dict[TLineNo, int]:\n        \"\"\"Get a count of exits from that each line.\n\n        To determine which lines are branches, coverage.py looks for lines that\n        have more than one exit.  This function creates a dict mapping each\n        executable line number to a count of how many exits it has.\n\n        To be honest, this feels wrong, and should be refactored.  Let me know\n        if you attempt to implement this method in your plug-in...\n\n        \"\"\"\n        return {}\n\n    def missing_arc_description(\n        self,\n        start: TLineNo,\n        end: TLineNo,\n        executed_arcs: Iterable[TArc] | None = None,     # pylint: disable=unused-argument\n    ) -> str:\n        \"\"\"Provide an English sentence describing a missing arc.\n\n        The `start` and `end` arguments are the line numbers of the missing\n        arc. Negative numbers indicate entering or exiting code objects.\n\n        The `executed_arcs` argument is a set of line number pairs, the arcs\n        that were executed in this file.\n\n        By default, this simply returns the string \"Line {start} didn't jump\n        to {end}\".\n\n        \"\"\"\n        return f\"Line {start} didn't jump to line {end}\"\n\n    def source_token_lines(self) -> TSourceTokenLines:\n        \"\"\"Generate a series of tokenized lines, one for each line in `source`.\n\n        These tokens are used for syntax-colored reports.\n\n        Each line is a list of pairs, each pair is a token::\n\n            [(\"key\", \"def\"), (\"ws\", \" \"), (\"nam\", \"hello\"), (\"op\", \"(\"), ... ]\n\n        Each pair has a token class, and the token text.  The token classes\n        are:\n\n        * ``\"com\"``: a comment\n        * ``\"key\"``: a keyword\n        * ``\"nam\"``: a name, or identifier\n        * ``\"num\"``: a number\n        * ``\"op\"``: an operator\n        * ``\"str\"``: a string literal\n        * ``\"ws\"``: some white space\n        * ``\"txt\"``: some other kind of text\n\n        If you concatenate all the token texts, and then join them with\n        newlines, you should have your original source back.\n\n        The default implementation simply returns each line tagged as\n        ``\"txt\"``.\n\n        \"\"\"\n        for line in self.source().splitlines():\n            yield [(\"txt\", line)]\n\n    def code_regions(self) -> Iterable[CodeRegion]:\n        \"\"\"Identify regions in the source file for finer reporting than by file.\n\n        Returns an iterable of :class:`CodeRegion` objects.  The kinds reported\n        should be in the possibilities returned by :meth:`code_region_kinds`.\n\n        \"\"\"\n        return []\n\n    def code_region_kinds(self) -> Iterable[tuple[str, str]]:\n        \"\"\"Return the kinds of code regions this plugin can find.\n\n        The returned pairs are the singular and plural forms of the kinds::\n\n            [\n                (\"function\", \"functions\"),\n                (\"class\", \"classes\"),\n            ]\n\n        This will usually be hard-coded, but could also differ by the specific\n        source file involved.\n\n        \"\"\"\n        return []\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, FileReporter) and self.filename == other.filename\n\n    def __lt__(self, other: Any) -> bool:\n        return isinstance(other, FileReporter) and self.filename < other.filename\n\n    # This object doesn't need to be hashed.\n    __hash__ = None         # type: ignore[assignment]\n", "coverage/sqlitedb.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"SQLite abstraction for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport re\nimport sqlite3\n\nfrom typing import cast, Any, Iterable, Iterator, Tuple\n\nfrom coverage.debug import auto_repr, clipped_repr, exc_one_line\nfrom coverage.exceptions import DataError\nfrom coverage.types import TDebugCtl\n\n\nclass SqliteDb:\n    \"\"\"A simple abstraction over a SQLite database.\n\n    Use as a context manager, then you can use it like a\n    :class:`python:sqlite3.Connection` object::\n\n        with SqliteDb(filename, debug_control) as db:\n            with db.execute(\"select a, b from some_table\") as cur:\n                for a, b in cur:\n                    etc(a, b)\n\n    \"\"\"\n    def __init__(self, filename: str, debug: TDebugCtl) -> None:\n        self.debug = debug\n        self.filename = filename\n        self.nest = 0\n        self.con: sqlite3.Connection | None = None\n\n    __repr__ = auto_repr\n\n    def _connect(self) -> None:\n        \"\"\"Connect to the db and do universal initialization.\"\"\"\n        if self.con is not None:\n            return\n\n        # It can happen that Python switches threads while the tracer writes\n        # data. The second thread will also try to write to the data,\n        # effectively causing a nested context. However, given the idempotent\n        # nature of the tracer operations, sharing a connection among threads\n        # is not a problem.\n        if self.debug.should(\"sql\"):\n            self.debug.write(f\"Connecting to {self.filename!r}\")\n        try:\n            self.con = sqlite3.connect(self.filename, check_same_thread=False)\n        except sqlite3.Error as exc:\n            raise DataError(f\"Couldn't use data file {self.filename!r}: {exc}\") from exc\n\n        if self.debug.should(\"sql\"):\n            self.debug.write(f\"Connected to {self.filename!r} as {self.con!r}\")\n\n        self.con.create_function(\"REGEXP\", 2, lambda txt, pat: re.search(txt, pat) is not None)\n\n        # Turning off journal_mode can speed up writing. It can't always be\n        # disabled, so we have to be prepared for *-journal files elsewhere.\n        # In Python 3.12+, we can change the config to allow journal_mode=off.\n        if hasattr(sqlite3, \"SQLITE_DBCONFIG_DEFENSIVE\"):\n            # Turn off defensive mode, so that journal_mode=off can succeed.\n            self.con.setconfig(                     # type: ignore[attr-defined, unused-ignore]\n                sqlite3.SQLITE_DBCONFIG_DEFENSIVE, False,\n            )\n\n        # This pragma makes writing faster. It disables rollbacks, but we never need them.\n        self.execute_void(\"pragma journal_mode=off\")\n\n        # This pragma makes writing faster. It can fail in unusual situations\n        # (https://github.com/nedbat/coveragepy/issues/1646), so use fail_ok=True\n        # to keep things going.\n        self.execute_void(\"pragma synchronous=off\", fail_ok=True)\n\n    def close(self) -> None:\n        \"\"\"If needed, close the connection.\"\"\"\n        if self.con is not None and self.filename != \":memory:\":\n            if self.debug.should(\"sql\"):\n                self.debug.write(f\"Closing {self.con!r} on {self.filename!r}\")\n            self.con.close()\n            self.con = None\n\n    def __enter__(self) -> SqliteDb:\n        if self.nest == 0:\n            self._connect()\n            assert self.con is not None\n            self.con.__enter__()\n        self.nest += 1\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:     # type: ignore[no-untyped-def]\n        self.nest -= 1\n        if self.nest == 0:\n            try:\n                assert self.con is not None\n                self.con.__exit__(exc_type, exc_value, traceback)\n                self.close()\n            except Exception as exc:\n                if self.debug.should(\"sql\"):\n                    self.debug.write(f\"EXCEPTION from __exit__: {exc_one_line(exc)}\")\n                raise DataError(f\"Couldn't end data file {self.filename!r}: {exc}\") from exc\n\n    def _execute(self, sql: str, parameters: Iterable[Any]) -> sqlite3.Cursor:\n        \"\"\"Same as :meth:`python:sqlite3.Connection.execute`.\"\"\"\n        if self.debug.should(\"sql\"):\n            tail = f\" with {parameters!r}\" if parameters else \"\"\n            self.debug.write(f\"Executing {sql!r}{tail}\")\n        try:\n            assert self.con is not None\n            try:\n                return self.con.execute(sql, parameters)    # type: ignore[arg-type]\n            except Exception:\n                # In some cases, an error might happen that isn't really an\n                # error.  Try again immediately.\n                # https://github.com/nedbat/coveragepy/issues/1010\n                return self.con.execute(sql, parameters)    # type: ignore[arg-type]\n        except sqlite3.Error as exc:\n            msg = str(exc)\n            if self.filename != \":memory:\":\n                try:\n                    # `execute` is the first thing we do with the database, so try\n                    # hard to provide useful hints if something goes wrong now.\n                    with open(self.filename, \"rb\") as bad_file:\n                        cov4_sig = b\"!coverage.py: This is a private format\"\n                        if bad_file.read(len(cov4_sig)) == cov4_sig:\n                            msg = (\n                                \"Looks like a coverage 4.x data file. \" +\n                                \"Are you mixing versions of coverage?\"\n                            )\n                except Exception:\n                    pass\n            if self.debug.should(\"sql\"):\n                self.debug.write(f\"EXCEPTION from execute: {exc_one_line(exc)}\")\n            raise DataError(f\"Couldn't use data file {self.filename!r}: {msg}\") from exc\n\n    @contextlib.contextmanager\n    def execute(\n        self,\n        sql: str,\n        parameters: Iterable[Any] = (),\n    ) -> Iterator[sqlite3.Cursor]:\n        \"\"\"Context managed :meth:`python:sqlite3.Connection.execute`.\n\n        Use with a ``with`` statement to auto-close the returned cursor.\n        \"\"\"\n        cur = self._execute(sql, parameters)\n        try:\n            yield cur\n        finally:\n            cur.close()\n\n    def execute_void(self, sql: str, parameters: Iterable[Any] = (), fail_ok: bool = False) -> None:\n        \"\"\"Same as :meth:`python:sqlite3.Connection.execute` when you don't need the cursor.\n\n        If `fail_ok` is True, then SQLite errors are ignored.\n        \"\"\"\n        try:\n            # PyPy needs the .close() calls here, or sqlite gets twisted up:\n            # https://bitbucket.org/pypy/pypy/issues/2872/default-isolation-mode-is-different-on\n            self._execute(sql, parameters).close()\n        except DataError:\n            if not fail_ok:\n                raise\n\n    def execute_for_rowid(self, sql: str, parameters: Iterable[Any] = ()) -> int:\n        \"\"\"Like execute, but returns the lastrowid.\"\"\"\n        with self.execute(sql, parameters) as cur:\n            assert cur.lastrowid is not None\n            rowid: int = cur.lastrowid\n        if self.debug.should(\"sqldata\"):\n            self.debug.write(f\"Row id result: {rowid!r}\")\n        return rowid\n\n    def execute_one(self, sql: str, parameters: Iterable[Any] = ()) -> tuple[Any, ...] | None:\n        \"\"\"Execute a statement and return the one row that results.\n\n        This is like execute(sql, parameters).fetchone(), except it is\n        correct in reading the entire result set.  This will raise an\n        exception if more than one row results.\n\n        Returns a row, or None if there were no rows.\n        \"\"\"\n        with self.execute(sql, parameters) as cur:\n            rows = list(cur)\n        if len(rows) == 0:\n            return None\n        elif len(rows) == 1:\n            return cast(Tuple[Any, ...], rows[0])\n        else:\n            raise AssertionError(f\"SQL {sql!r} shouldn't return {len(rows)} rows\")\n\n    def _executemany(self, sql: str, data: list[Any]) -> sqlite3.Cursor:\n        \"\"\"Same as :meth:`python:sqlite3.Connection.executemany`.\"\"\"\n        if self.debug.should(\"sql\"):\n            final = \":\" if self.debug.should(\"sqldata\") else \"\"\n            self.debug.write(f\"Executing many {sql!r} with {len(data)} rows{final}\")\n            if self.debug.should(\"sqldata\"):\n                for i, row in enumerate(data):\n                    self.debug.write(f\"{i:4d}: {row!r}\")\n        assert self.con is not None\n        try:\n            return self.con.executemany(sql, data)\n        except Exception:\n            # In some cases, an error might happen that isn't really an\n            # error.  Try again immediately.\n            # https://github.com/nedbat/coveragepy/issues/1010\n            return self.con.executemany(sql, data)\n\n    def executemany_void(self, sql: str, data: Iterable[Any]) -> None:\n        \"\"\"Same as :meth:`python:sqlite3.Connection.executemany` when you don't need the cursor.\"\"\"\n        data = list(data)\n        if data:\n            self._executemany(sql, data).close()\n\n    def executescript(self, script: str) -> None:\n        \"\"\"Same as :meth:`python:sqlite3.Connection.executescript`.\"\"\"\n        if self.debug.should(\"sql\"):\n            self.debug.write(\"Executing script with {} chars: {}\".format(\n                len(script), clipped_repr(script, 100),\n            ))\n        assert self.con is not None\n        self.con.executescript(script).close()\n\n    def dump(self) -> str:\n        \"\"\"Return a multi-line string, the SQL dump of the database.\"\"\"\n        assert self.con is not None\n        return \"\\n\".join(self.con.iterdump())\n", "coverage/html.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"HTML reporting for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport dataclasses\nimport datetime\nimport functools\nimport json\nimport os\nimport re\nimport string\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Iterable, TYPE_CHECKING\n\nimport coverage\nfrom coverage.data import CoverageData, add_data_to_hash\nfrom coverage.exceptions import NoDataError\nfrom coverage.files import flat_rootname\nfrom coverage.misc import (\n    ensure_dir, file_be_gone, Hasher, isolate_module, format_local_datetime,\n    human_sorted, plural, stdout_link,\n)\nfrom coverage.report_core import get_analysis_to_report\nfrom coverage.results import Analysis, Numbers\nfrom coverage.templite import Templite\nfrom coverage.types import TLineNo, TMorf\nfrom coverage.version import __url__\n\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n    from coverage.plugins import FileReporter\n\n\nos = isolate_module(os)\n\n\ndef data_filename(fname: str) -> str:\n    \"\"\"Return the path to an \"htmlfiles\" data file of ours.\n    \"\"\"\n    static_dir = os.path.join(os.path.dirname(__file__), \"htmlfiles\")\n    static_filename = os.path.join(static_dir, fname)\n    return static_filename\n\n\ndef read_data(fname: str) -> str:\n    \"\"\"Return the contents of a data file of ours.\"\"\"\n    with open(data_filename(fname)) as data_file:\n        return data_file.read()\n\n\ndef write_html(fname: str, html: str) -> None:\n    \"\"\"Write `html` to `fname`, properly encoded.\"\"\"\n    html = re.sub(r\"(\\A\\s+)|(\\s+$)\", \"\", html, flags=re.MULTILINE) + \"\\n\"\n    with open(fname, \"wb\") as fout:\n        fout.write(html.encode(\"ascii\", \"xmlcharrefreplace\"))\n\n\n@dataclass\nclass LineData:\n    \"\"\"The data for each source line of HTML output.\"\"\"\n    tokens: list[tuple[str, str]]\n    number: TLineNo\n    category: str\n    contexts: list[str]\n    contexts_label: str\n    context_list: list[str]\n    short_annotations: list[str]\n    long_annotations: list[str]\n    html: str = \"\"\n    context_str: str | None = None\n    annotate: str | None = None\n    annotate_long: str | None = None\n    css_class: str = \"\"\n\n\n@dataclass\nclass FileData:\n    \"\"\"The data for each source file of HTML output.\"\"\"\n    relative_filename: str\n    nums: Numbers\n    lines: list[LineData]\n\n\n@dataclass\nclass IndexItem:\n    \"\"\"Information for each index entry, to render an index page.\"\"\"\n    url: str = \"\"\n    file: str = \"\"\n    description: str = \"\"\n    nums: Numbers = field(default_factory=Numbers)\n\n\n@dataclass\nclass IndexPage:\n    \"\"\"Data for each index page.\"\"\"\n    noun: str\n    plural: str\n    filename: str\n    summaries: list[IndexItem]\n    totals: Numbers\n    skipped_covered_count: int\n    skipped_empty_count: int\n\n\nclass HtmlDataGeneration:\n    \"\"\"Generate structured data to be turned into HTML reports.\"\"\"\n\n    EMPTY = \"(empty)\"\n\n    def __init__(self, cov: Coverage) -> None:\n        self.coverage = cov\n        self.config = self.coverage.config\n        self.data = self.coverage.get_data()\n        self.has_arcs = self.data.has_arcs()\n        if self.config.show_contexts:\n            if self.data.measured_contexts() == {\"\"}:\n                self.coverage._warn(\"No contexts were measured\")\n        self.data.set_query_contexts(self.config.report_contexts)\n\n    def data_for_file(self, fr: FileReporter, analysis: Analysis) -> FileData:\n        \"\"\"Produce the data needed for one file's report.\"\"\"\n        if self.has_arcs:\n            missing_branch_arcs = analysis.missing_branch_arcs()\n            arcs_executed = analysis.arcs_executed\n        else:\n            missing_branch_arcs = {}\n            arcs_executed = []\n\n        if self.config.show_contexts:\n            contexts_by_lineno = self.data.contexts_by_lineno(analysis.filename)\n\n        lines = []\n\n        for lineno, tokens in enumerate(fr.source_token_lines(), start=1):\n            # Figure out how to mark this line.\n            category = \"\"\n            short_annotations = []\n            long_annotations = []\n\n            if lineno in analysis.excluded:\n                category = \"exc\"\n            elif lineno in analysis.missing:\n                category = \"mis\"\n            elif self.has_arcs and lineno in missing_branch_arcs:\n                category = \"par\"\n                for b in missing_branch_arcs[lineno]:\n                    if b < 0:\n                        short_annotations.append(\"exit\")\n                    else:\n                        short_annotations.append(str(b))\n                    long_annotations.append(fr.missing_arc_description(lineno, b, arcs_executed))\n            elif lineno in analysis.statements:\n                category = \"run\"\n\n            contexts = []\n            contexts_label = \"\"\n            context_list = []\n            if category and self.config.show_contexts:\n                contexts = human_sorted(c or self.EMPTY for c in contexts_by_lineno.get(lineno, ()))\n                if contexts == [self.EMPTY]:\n                    contexts_label = self.EMPTY\n                else:\n                    contexts_label = f\"{len(contexts)} ctx\"\n                    context_list = contexts\n\n            lines.append(LineData(\n                tokens=tokens,\n                number=lineno,\n                category=category,\n                contexts=contexts,\n                contexts_label=contexts_label,\n                context_list=context_list,\n                short_annotations=short_annotations,\n                long_annotations=long_annotations,\n            ))\n\n        file_data = FileData(\n            relative_filename=fr.relative_filename(),\n            nums=analysis.numbers,\n            lines=lines,\n        )\n\n        return file_data\n\n\nclass FileToReport:\n    \"\"\"A file we're considering reporting.\"\"\"\n    def __init__(self, fr: FileReporter, analysis: Analysis) -> None:\n        self.fr = fr\n        self.analysis = analysis\n        self.rootname = flat_rootname(fr.relative_filename())\n        self.html_filename = self.rootname + \".html\"\n        self.prev_html = self.next_html = \"\"\n\n\nHTML_SAFE = string.ascii_letters + string.digits + \"!#$%'()*+,-./:;=?@[]^_`{|}~\"\n\n@functools.lru_cache(maxsize=None)\ndef encode_int(n: int) -> str:\n    \"\"\"Create a short HTML-safe string from an integer, using HTML_SAFE.\"\"\"\n    if n == 0:\n        return HTML_SAFE[0]\n\n    r = []\n    while n:\n        n, t = divmod(n, len(HTML_SAFE))\n        r.append(HTML_SAFE[t])\n    return \"\".join(r)\n\n\nclass HtmlReporter:\n    \"\"\"HTML reporting.\"\"\"\n\n    # These files will be copied from the htmlfiles directory to the output\n    # directory.\n    STATIC_FILES = [\n        \"style.css\",\n        \"coverage_html.js\",\n        \"keybd_closed.png\",\n        \"favicon_32.png\",\n    ]\n\n    def __init__(self, cov: Coverage) -> None:\n        self.coverage = cov\n        self.config = self.coverage.config\n        self.directory = self.config.html_dir\n\n        self.skip_covered = self.config.html_skip_covered\n        if self.skip_covered is None:\n            self.skip_covered = self.config.skip_covered\n        self.skip_empty = self.config.html_skip_empty\n        if self.skip_empty is None:\n            self.skip_empty = self.config.skip_empty\n\n        title = self.config.html_title\n\n        self.extra_css = bool(self.config.extra_css)\n\n        self.data = self.coverage.get_data()\n        self.has_arcs = self.data.has_arcs()\n\n        self.index_pages: dict[str, IndexPage] = {\n            \"file\": self.new_index_page(\"file\", \"files\"),\n        }\n        self.incr = IncrementalChecker(self.directory)\n        self.datagen = HtmlDataGeneration(self.coverage)\n        self.directory_was_empty = False\n        self.first_fr = None\n        self.final_fr = None\n\n        self.template_globals = {\n            # Functions available in the templates.\n            \"escape\": escape,\n            \"pair\": pair,\n            \"len\": len,\n\n            # Constants for this report.\n            \"__url__\": __url__,\n            \"__version__\": coverage.__version__,\n            \"title\": title,\n            \"time_stamp\": format_local_datetime(datetime.datetime.now()),\n            \"extra_css\": self.extra_css,\n            \"has_arcs\": self.has_arcs,\n            \"show_contexts\": self.config.show_contexts,\n            \"statics\": {},\n\n            # Constants for all reports.\n            # These css classes determine which lines are highlighted by default.\n            \"category\": {\n                \"exc\": \"exc show_exc\",\n                \"mis\": \"mis show_mis\",\n                \"par\": \"par run show_par\",\n                \"run\": \"run\",\n            },\n        }\n        self.index_tmpl = Templite(read_data(\"index.html\"), self.template_globals)\n        self.pyfile_html_source = read_data(\"pyfile.html\")\n        self.source_tmpl = Templite(self.pyfile_html_source, self.template_globals)\n\n    def new_index_page(self, noun: str, plural_noun: str) -> IndexPage:\n        \"\"\"Create an IndexPage for a kind of region.\"\"\"\n        return IndexPage(\n            noun=noun,\n            plural=plural_noun,\n            filename=\"index.html\" if noun == \"file\" else f\"{noun}_index.html\",\n            summaries=[],\n            totals=Numbers(precision=self.config.precision),\n            skipped_covered_count=0,\n            skipped_empty_count=0,\n        )\n\n    def report(self, morfs: Iterable[TMorf] | None) -> float:\n        \"\"\"Generate an HTML report for `morfs`.\n\n        `morfs` is a list of modules or file names.\n\n        \"\"\"\n        # Read the status data and check that this run used the same\n        # global data as the last run.\n        self.incr.read()\n        self.incr.check_global_data(self.config, self.pyfile_html_source)\n\n        # Process all the files. For each page we need to supply a link\n        # to the next and previous page.\n        files_to_report = []\n\n        have_data = False\n        for fr, analysis in get_analysis_to_report(self.coverage, morfs):\n            have_data = True\n            ftr = FileToReport(fr, analysis)\n            if self.should_report(analysis, self.index_pages[\"file\"]):\n                files_to_report.append(ftr)\n            else:\n                file_be_gone(os.path.join(self.directory, ftr.html_filename))\n\n        if not have_data:\n            raise NoDataError(\"No data to report.\")\n\n        self.make_directory()\n        self.make_local_static_report_files()\n\n        if files_to_report:\n            for ftr1, ftr2 in zip(files_to_report[:-1], files_to_report[1:]):\n                ftr1.next_html = ftr2.html_filename\n                ftr2.prev_html = ftr1.html_filename\n            files_to_report[0].prev_html = \"index.html\"\n            files_to_report[-1].next_html = \"index.html\"\n\n        for ftr in files_to_report:\n            self.write_html_page(ftr)\n            for noun, plural_noun in ftr.fr.code_region_kinds():\n                if noun not in self.index_pages:\n                    self.index_pages[noun] = self.new_index_page(noun, plural_noun)\n\n        # Write the index page.\n        if files_to_report:\n            first_html = files_to_report[0].html_filename\n            final_html = files_to_report[-1].html_filename\n        else:\n            first_html = final_html = \"index.html\"\n        self.write_file_index_page(first_html, final_html)\n\n        # Write function and class index pages.\n        self.write_region_index_pages(files_to_report)\n\n        return (\n            self.index_pages[\"file\"].totals.n_statements\n            and self.index_pages[\"file\"].totals.pc_covered\n        )\n\n    def make_directory(self) -> None:\n        \"\"\"Make sure our htmlcov directory exists.\"\"\"\n        ensure_dir(self.directory)\n        if not os.listdir(self.directory):\n            self.directory_was_empty = True\n\n    def copy_static_file(self, src: str, slug: str = \"\") -> None:\n        \"\"\"Copy a static file into the output directory with cache busting.\"\"\"\n        with open(src, \"rb\") as f:\n            text = f.read()\n        h = Hasher()\n        h.update(text)\n        cache_bust = h.hexdigest()[:8]\n        src_base = os.path.basename(src)\n        dest = src_base.replace(\".\", f\"_cb_{cache_bust}.\")\n        if not slug:\n            slug = src_base.replace(\".\", \"_\")\n        self.template_globals[\"statics\"][slug] = dest # type: ignore\n        with open(os.path.join(self.directory, dest), \"wb\") as f:\n            f.write(text)\n\n    def make_local_static_report_files(self) -> None:\n        \"\"\"Make local instances of static files for HTML report.\"\"\"\n\n        # The files we provide must always be copied.\n        for static in self.STATIC_FILES:\n            self.copy_static_file(data_filename(static))\n\n        # The user may have extra CSS they want copied.\n        if self.extra_css:\n            assert self.config.extra_css is not None\n            self.copy_static_file(self.config.extra_css, slug=\"extra_css\")\n\n        # Only write the .gitignore file if the directory was originally empty.\n        # .gitignore can't be copied from the source tree because if it was in\n        # the source tree, it would stop the static files from being checked in.\n        if self.directory_was_empty:\n            with open(os.path.join(self.directory, \".gitignore\"), \"w\") as fgi:\n                fgi.write(\"# Created by coverage.py\\n*\\n\")\n\n    def should_report(self, analysis: Analysis, index_page: IndexPage) -> bool:\n        \"\"\"Determine if we'll report this file or region.\"\"\"\n        # Get the numbers for this file.\n        nums = analysis.numbers\n        index_page.totals += nums\n\n        if self.skip_covered:\n            # Don't report on 100% files.\n            no_missing_lines = (nums.n_missing == 0)\n            no_missing_branches = (nums.n_partial_branches == 0)\n            if no_missing_lines and no_missing_branches:\n                index_page.skipped_covered_count += 1\n                return False\n\n        if self.skip_empty:\n            # Don't report on empty files.\n            if nums.n_statements == 0:\n                index_page.skipped_empty_count += 1\n                return False\n\n        return True\n\n    def write_html_page(self, ftr: FileToReport) -> None:\n        \"\"\"Generate an HTML page for one source file.\n\n        If the page on disk is already correct based on our incremental status\n        checking, then the page doesn't have to be generated, and this function\n        only does page summary bookkeeping.\n\n        \"\"\"\n        # Find out if the page on disk is already correct.\n        if self.incr.can_skip_file(self.data, ftr.fr, ftr.rootname):\n            self.index_pages[\"file\"].summaries.append(self.incr.index_info(ftr.rootname))\n            return\n\n        # Write the HTML page for this source file.\n        file_data = self.datagen.data_for_file(ftr.fr, ftr.analysis)\n\n        contexts = collections.Counter(c for cline in file_data.lines for c in cline.contexts)\n        context_codes = {y: i for (i, y) in enumerate(x[0] for x in contexts.most_common())}\n        if context_codes:\n            contexts_json = json.dumps(\n                {encode_int(v): k for (k, v) in context_codes.items()},\n                indent=2,\n            )\n        else:\n            contexts_json = None\n\n        for ldata in file_data.lines:\n            # Build the HTML for the line.\n            html_parts = []\n            for tok_type, tok_text in ldata.tokens:\n                if tok_type == \"ws\":\n                    html_parts.append(escape(tok_text))\n                else:\n                    tok_html = escape(tok_text) or \"&nbsp;\"\n                    html_parts.append(f'<span class=\"{tok_type}\">{tok_html}</span>')\n            ldata.html = \"\".join(html_parts)\n            if ldata.context_list:\n                encoded_contexts = [\n                    encode_int(context_codes[c_context]) for c_context in ldata.context_list\n                ]\n                code_width = max(len(ec) for ec in encoded_contexts)\n                ldata.context_str = (\n                    str(code_width)\n                    + \"\".join(ec.ljust(code_width) for ec in encoded_contexts)\n                )\n            else:\n                ldata.context_str = \"\"\n\n            if ldata.short_annotations:\n                # 202F is NARROW NO-BREAK SPACE.\n                # 219B is RIGHTWARDS ARROW WITH STROKE.\n                ldata.annotate = \",&nbsp;&nbsp; \".join(\n                    f\"{ldata.number}&#x202F;&#x219B;&#x202F;{d}\"\n                    for d in ldata.short_annotations\n                )\n            else:\n                ldata.annotate = None\n\n            if ldata.long_annotations:\n                longs = ldata.long_annotations\n                if len(longs) == 1:\n                    ldata.annotate_long = longs[0]\n                else:\n                    ldata.annotate_long = \"{:d} missed branches: {}\".format(\n                        len(longs),\n                        \", \".join(\n                            f\"{num:d}) {ann_long}\"\n                            for num, ann_long in enumerate(longs, start=1)\n                        ),\n                    )\n            else:\n                ldata.annotate_long = None\n\n            css_classes = []\n            if ldata.category:\n                css_classes.append(\n                    self.template_globals[\"category\"][ldata.category],   # type: ignore[index]\n                )\n            ldata.css_class = \" \".join(css_classes) or \"pln\"\n\n        html_path = os.path.join(self.directory, ftr.html_filename)\n        html = self.source_tmpl.render({\n            **file_data.__dict__,\n            \"contexts_json\": contexts_json,\n            \"prev_html\": ftr.prev_html,\n            \"next_html\": ftr.next_html,\n        })\n        write_html(html_path, html)\n\n        # Save this file's information for the index page.\n        index_info = IndexItem(\n            url = ftr.html_filename,\n            file = escape(ftr.fr.relative_filename()),\n            nums = ftr.analysis.numbers,\n        )\n        self.index_pages[\"file\"].summaries.append(index_info)\n        self.incr.set_index_info(ftr.rootname, index_info)\n\n    def write_file_index_page(self, first_html: str, final_html: str) -> None:\n        \"\"\"Write the file index page for this report.\"\"\"\n        index_file = self.write_index_page(\n            self.index_pages[\"file\"],\n            first_html=first_html,\n            final_html=final_html,\n        )\n\n        print_href = stdout_link(index_file, f\"file://{os.path.abspath(index_file)}\")\n        self.coverage._message(f\"Wrote HTML report to {print_href}\")\n\n        # Write the latest hashes for next time.\n        self.incr.write()\n\n    def write_region_index_pages(self, files_to_report: Iterable[FileToReport]) -> None:\n        \"\"\"Write the other index pages for this report.\"\"\"\n        for ftr in files_to_report:\n            region_nouns = [pair[0] for pair in ftr.fr.code_region_kinds()]\n            num_lines = len(ftr.fr.source().splitlines())\n            outside_lines = set(range(1, num_lines + 1))\n            regions = ftr.fr.code_regions()\n\n            for noun in region_nouns:\n                page_data = self.index_pages[noun]\n\n                for region in regions:\n                    if region.kind != noun:\n                        continue\n                    outside_lines -= region.lines\n                    analysis = ftr.analysis.narrow(region.lines)\n                    if not self.should_report(analysis, page_data):\n                        continue\n                    sorting_name = region.name.rpartition(\".\")[-1].lstrip(\"_\")\n                    page_data.summaries.append(IndexItem(\n                        url=f\"{ftr.html_filename}#t{region.start}\",\n                        file=escape(ftr.fr.relative_filename()),\n                        description=(\n                            f\"<data value='{escape(sorting_name)}'>\"\n                            + escape(region.name)\n                            + \"</data>\"\n                        ),\n                        nums=analysis.numbers,\n                    ))\n\n                analysis = ftr.analysis.narrow(outside_lines)\n                if self.should_report(analysis, page_data):\n                    page_data.summaries.append(IndexItem(\n                        url=ftr.html_filename,\n                        file=escape(ftr.fr.relative_filename()),\n                        description=(\n                            \"<data value=''>\"\n                            + f\"<span class='no-noun'>(no {escape(noun)})</span>\"\n                            + \"</data>\"\n                        ),\n                        nums=analysis.numbers,\n                    ))\n\n        for noun, index_page in self.index_pages.items():\n            if noun != \"file\":\n                self.write_index_page(index_page)\n\n    def write_index_page(self, index_page: IndexPage, **kwargs: str) -> str:\n        \"\"\"Write an index page specified by `index_page`.\n\n        Returns the filename created.\n        \"\"\"\n        skipped_covered_msg = skipped_empty_msg = \"\"\n        if n := index_page.skipped_covered_count:\n            word = plural(n, index_page.noun, index_page.plural)\n            skipped_covered_msg = f\"{n} {word} skipped due to complete coverage.\"\n        if n := index_page.skipped_empty_count:\n            word = plural(n, index_page.noun, index_page.plural)\n            skipped_empty_msg = f\"{n} empty {word} skipped.\"\n\n        index_buttons = [\n            {\n                \"label\": ip.plural.title(),\n                \"url\": ip.filename if ip.noun != index_page.noun else \"\",\n                \"current\": ip.noun == index_page.noun,\n            }\n            for ip in self.index_pages.values()\n        ]\n        render_data = {\n            \"regions\": index_page.summaries,\n            \"totals\": index_page.totals,\n            \"noun\": index_page.noun,\n            \"region_noun\": index_page.noun if index_page.noun != \"file\" else \"\",\n            \"skip_covered\": self.skip_covered,\n            \"skipped_covered_msg\": skipped_covered_msg,\n            \"skipped_empty_msg\": skipped_empty_msg,\n            \"first_html\": \"\",\n            \"final_html\": \"\",\n            \"index_buttons\": index_buttons,\n        }\n        render_data.update(kwargs)\n        html = self.index_tmpl.render(render_data)\n\n        index_file = os.path.join(self.directory, index_page.filename)\n        write_html(index_file, html)\n        return index_file\n\n\n@dataclass\nclass FileInfo:\n    \"\"\"Summary of the information from last rendering, to avoid duplicate work.\"\"\"\n    hash: str = \"\"\n    index: IndexItem = field(default_factory=IndexItem)\n\n\nclass IncrementalChecker:\n    \"\"\"Logic and data to support incremental reporting.\n\n    When generating an HTML report, often only a few of the source files have\n    changed since the last time we made the HTML report.  This means previously\n    created HTML pages can be reused without generating them again, speeding\n    the command.\n\n    This class manages a JSON data file that captures enough information to\n    know whether an HTML page for a .py file needs to be regenerated or not.\n    The data file also needs to store all the information needed to create the\n    entry for the file on the index page so that if the HTML page is reused,\n    the index page can still be created to refer to it.\n\n    The data looks like::\n\n        {\n            \"note\": \"This file is an internal implementation detail ...\",\n            // A fixed number indicating the data format.  STATUS_FORMAT\n            \"format\": 5,\n            // The version of coverage.py\n            \"version\": \"7.4.4\",\n            // A hash of a number of global things, including the configuration\n            // settings and the pyfile.html template itself.\n            \"globals\": \"540ee119c15d52a68a53fe6f0897346d\",\n            \"files\": {\n                // An entry for each source file keyed by the flat_rootname().\n                \"z_7b071bdc2a35fa80___init___py\": {\n                    // Hash of the source, the text of the .py file.\n                    \"hash\": \"e45581a5b48f879f301c0f30bf77a50c\",\n                    // Information for the index.html file.\n                    \"index\": {\n                        \"url\": \"z_7b071bdc2a35fa80___init___py.html\",\n                        \"file\": \"cogapp/__init__.py\",\n                        \"description\": \"\",\n                        // The Numbers for this file.\n                        \"nums\": { \"precision\": 2, \"n_files\": 1, \"n_statements\": 43, ... }\n                    }\n                },\n                ...\n            }\n        }\n\n    \"\"\"\n\n    STATUS_FILE = \"status.json\"\n    STATUS_FORMAT = 5\n    NOTE = (\n        \"This file is an internal implementation detail to speed up HTML report\"\n        + \" generation. Its format can change at any time. You might be looking\"\n        + \" for the JSON report: https://coverage.rtfd.io/cmd.html#cmd-json\"\n    )\n\n    def __init__(self, directory: str) -> None:\n        self.directory = directory\n        self._reset()\n\n    def _reset(self) -> None:\n        \"\"\"Initialize to empty. Causes all files to be reported.\"\"\"\n        self.globals = \"\"\n        self.files: dict[str, FileInfo] = {}\n\n    def read(self) -> None:\n        \"\"\"Read the information we stored last time.\"\"\"\n        try:\n            status_file = os.path.join(self.directory, self.STATUS_FILE)\n            with open(status_file) as fstatus:\n                status = json.load(fstatus)\n        except (OSError, ValueError):\n            # Status file is missing or malformed.\n            usable = False\n        else:\n            if status[\"format\"] != self.STATUS_FORMAT:\n                usable = False\n            elif status[\"version\"] != coverage.__version__:\n                usable = False\n            else:\n                usable = True\n\n        if usable:\n            self.files = {}\n            for filename, filedict in status[\"files\"].items():\n                indexdict = filedict[\"index\"]\n                index_item = IndexItem(**indexdict)\n                index_item.nums = Numbers(**indexdict[\"nums\"])\n                fileinfo = FileInfo(\n                    hash=filedict[\"hash\"],\n                    index=index_item,\n                )\n                self.files[filename] = fileinfo\n            self.globals = status[\"globals\"]\n        else:\n            self._reset()\n\n    def write(self) -> None:\n        \"\"\"Write the current status.\"\"\"\n        status_file = os.path.join(self.directory, self.STATUS_FILE)\n        status_data = {\n            \"note\": self.NOTE,\n            \"format\": self.STATUS_FORMAT,\n            \"version\": coverage.__version__,\n            \"globals\": self.globals,\n            \"files\": {\n                fname: dataclasses.asdict(finfo)\n                for fname, finfo in self.files.items()\n            },\n        }\n        with open(status_file, \"w\") as fout:\n            json.dump(status_data, fout, separators=(\",\", \":\"))\n\n    def check_global_data(self, *data: Any) -> None:\n        \"\"\"Check the global data that can affect incremental reporting.\n\n        Pass in whatever global information could affect the content of the\n        HTML pages.  If the global data has changed since last time, this will\n        clear the data so that all files are regenerated.\n\n        \"\"\"\n        m = Hasher()\n        for d in data:\n            m.update(d)\n        these_globals = m.hexdigest()\n        if self.globals != these_globals:\n            self._reset()\n            self.globals = these_globals\n\n    def can_skip_file(self, data: CoverageData, fr: FileReporter, rootname: str) -> bool:\n        \"\"\"Can we skip reporting this file?\n\n        `data` is a CoverageData object, `fr` is a `FileReporter`, and\n        `rootname` is the name being used for the file.\n\n        Returns True if the HTML page is fine as-is, False if we need to recreate\n        the HTML page.\n\n        \"\"\"\n        m = Hasher()\n        m.update(fr.source().encode(\"utf-8\"))\n        add_data_to_hash(data, fr.filename, m)\n        this_hash = m.hexdigest()\n\n        file_info = self.files.setdefault(rootname, FileInfo())\n\n        if this_hash == file_info.hash:\n            # Nothing has changed to require the file to be reported again.\n            return True\n        else:\n            # File has changed, record the latest hash and force regeneration.\n            file_info.hash = this_hash\n            return False\n\n    def index_info(self, fname: str) -> IndexItem:\n        \"\"\"Get the information for index.html for `fname`.\"\"\"\n        return self.files.get(fname, FileInfo()).index\n\n    def set_index_info(self, fname: str, info: IndexItem) -> None:\n        \"\"\"Set the information for index.html for `fname`.\"\"\"\n        self.files.setdefault(fname, FileInfo()).index = info\n\n\n# Helpers for templates and generating HTML\n\ndef escape(t: str) -> str:\n    \"\"\"HTML-escape the text in `t`.\n\n    This is only suitable for HTML text, not attributes.\n\n    \"\"\"\n    # Convert HTML special chars into HTML entities.\n    return t.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\")\n\n\ndef pair(ratio: tuple[int, int]) -> str:\n    \"\"\"Format a pair of numbers so JavaScript can read them in an attribute.\"\"\"\n    return \"{} {}\".format(*ratio)\n", "coverage/bytecode.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Bytecode manipulation for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nfrom types import CodeType\nfrom typing import Iterator\n\n\ndef code_objects(code: CodeType) -> Iterator[CodeType]:\n    \"\"\"Iterate over all the code objects in `code`.\"\"\"\n    stack = [code]\n    while stack:\n        # We're going to return the code object on the stack, but first\n        # push its children for later returning.\n        code = stack.pop()\n        for c in code.co_consts:\n            if isinstance(c, CodeType):\n                stack.append(c)\n        yield code\n", "coverage/plugin_support.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Support for plugins.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport os.path\nimport sys\n\nfrom types import FrameType\nfrom typing import Any, Iterable, Iterator\n\nfrom coverage.exceptions import PluginError\nfrom coverage.misc import isolate_module\nfrom coverage.plugin import CoveragePlugin, FileTracer, FileReporter\nfrom coverage.types import (\n    TArc, TConfigurable, TDebugCtl, TLineNo, TPluginConfig, TSourceTokenLines,\n)\n\nos = isolate_module(os)\n\n\nclass Plugins:\n    \"\"\"The currently loaded collection of coverage.py plugins.\"\"\"\n\n    def __init__(self) -> None:\n        self.order: list[CoveragePlugin] = []\n        self.names: dict[str, CoveragePlugin] = {}\n        self.file_tracers: list[CoveragePlugin] = []\n        self.configurers: list[CoveragePlugin] = []\n        self.context_switchers: list[CoveragePlugin] = []\n\n        self.current_module: str | None = None\n        self.debug: TDebugCtl | None\n\n    @classmethod\n    def load_plugins(\n        cls,\n        modules: Iterable[str],\n        config: TPluginConfig,\n        debug: TDebugCtl | None = None,\n    ) -> Plugins:\n        \"\"\"Load plugins from `modules`.\n\n        Returns a Plugins object with the loaded and configured plugins.\n\n        \"\"\"\n        plugins = cls()\n        plugins.debug = debug\n\n        for module in modules:\n            plugins.current_module = module\n            __import__(module)\n            mod = sys.modules[module]\n\n            coverage_init = getattr(mod, \"coverage_init\", None)\n            if not coverage_init:\n                raise PluginError(\n                    f\"Plugin module {module!r} didn't define a coverage_init function\",\n                )\n\n            options = config.get_plugin_options(module)\n            coverage_init(plugins, options)\n\n        plugins.current_module = None\n        return plugins\n\n    def add_file_tracer(self, plugin: CoveragePlugin) -> None:\n        \"\"\"Add a file tracer plugin.\n\n        `plugin` is an instance of a third-party plugin class.  It must\n        implement the :meth:`CoveragePlugin.file_tracer` method.\n\n        \"\"\"\n        self._add_plugin(plugin, self.file_tracers)\n\n    def add_configurer(self, plugin: CoveragePlugin) -> None:\n        \"\"\"Add a configuring plugin.\n\n        `plugin` is an instance of a third-party plugin class. It must\n        implement the :meth:`CoveragePlugin.configure` method.\n\n        \"\"\"\n        self._add_plugin(plugin, self.configurers)\n\n    def add_dynamic_context(self, plugin: CoveragePlugin) -> None:\n        \"\"\"Add a dynamic context plugin.\n\n        `plugin` is an instance of a third-party plugin class.  It must\n        implement the :meth:`CoveragePlugin.dynamic_context` method.\n\n        \"\"\"\n        self._add_plugin(plugin, self.context_switchers)\n\n    def add_noop(self, plugin: CoveragePlugin) -> None:\n        \"\"\"Add a plugin that does nothing.\n\n        This is only useful for testing the plugin support.\n\n        \"\"\"\n        self._add_plugin(plugin, None)\n\n    def _add_plugin(\n        self,\n        plugin: CoveragePlugin,\n        specialized: list[CoveragePlugin] | None,\n    ) -> None:\n        \"\"\"Add a plugin object.\n\n        `plugin` is a :class:`CoveragePlugin` instance to add.  `specialized`\n        is a list to append the plugin to.\n\n        \"\"\"\n        plugin_name = f\"{self.current_module}.{plugin.__class__.__name__}\"\n        if self.debug and self.debug.should(\"plugin\"):\n            self.debug.write(f\"Loaded plugin {self.current_module!r}: {plugin!r}\")\n            labelled = LabelledDebug(f\"plugin {self.current_module!r}\", self.debug)\n            plugin = DebugPluginWrapper(plugin, labelled)\n\n        plugin._coverage_plugin_name = plugin_name\n        plugin._coverage_enabled = True\n        self.order.append(plugin)\n        self.names[plugin_name] = plugin\n        if specialized is not None:\n            specialized.append(plugin)\n\n    def __bool__(self) -> bool:\n        return bool(self.order)\n\n    def __iter__(self) -> Iterator[CoveragePlugin]:\n        return iter(self.order)\n\n    def get(self, plugin_name: str) -> CoveragePlugin:\n        \"\"\"Return a plugin by name.\"\"\"\n        return self.names[plugin_name]\n\n\nclass LabelledDebug:\n    \"\"\"A Debug writer, but with labels for prepending to the messages.\"\"\"\n\n    def __init__(self, label: str, debug: TDebugCtl, prev_labels: Iterable[str] = ()):\n        self.labels = list(prev_labels) + [label]\n        self.debug = debug\n\n    def add_label(self, label: str) -> LabelledDebug:\n        \"\"\"Add a label to the writer, and return a new `LabelledDebug`.\"\"\"\n        return LabelledDebug(label, self.debug, self.labels)\n\n    def message_prefix(self) -> str:\n        \"\"\"The prefix to use on messages, combining the labels.\"\"\"\n        prefixes = self.labels + [\"\"]\n        return \":\\n\".join(\"  \"*i+label for i, label in enumerate(prefixes))\n\n    def write(self, message: str) -> None:\n        \"\"\"Write `message`, but with the labels prepended.\"\"\"\n        self.debug.write(f\"{self.message_prefix()}{message}\")\n\n\nclass DebugPluginWrapper(CoveragePlugin):\n    \"\"\"Wrap a plugin, and use debug to report on what it's doing.\"\"\"\n\n    def __init__(self, plugin: CoveragePlugin, debug: LabelledDebug) -> None:\n        super().__init__()\n        self.plugin = plugin\n        self.debug = debug\n\n    def file_tracer(self, filename: str) -> FileTracer | None:\n        tracer = self.plugin.file_tracer(filename)\n        self.debug.write(f\"file_tracer({filename!r}) --> {tracer!r}\")\n        if tracer:\n            debug = self.debug.add_label(f\"file {filename!r}\")\n            tracer = DebugFileTracerWrapper(tracer, debug)\n        return tracer\n\n    def file_reporter(self, filename: str) -> FileReporter | str:\n        reporter = self.plugin.file_reporter(filename)\n        assert isinstance(reporter, FileReporter)\n        self.debug.write(f\"file_reporter({filename!r}) --> {reporter!r}\")\n        if reporter:\n            debug = self.debug.add_label(f\"file {filename!r}\")\n            reporter = DebugFileReporterWrapper(filename, reporter, debug)\n        return reporter\n\n    def dynamic_context(self, frame: FrameType) -> str | None:\n        context = self.plugin.dynamic_context(frame)\n        self.debug.write(f\"dynamic_context({frame!r}) --> {context!r}\")\n        return context\n\n    def find_executable_files(self, src_dir: str) -> Iterable[str]:\n        executable_files = self.plugin.find_executable_files(src_dir)\n        self.debug.write(f\"find_executable_files({src_dir!r}) --> {executable_files!r}\")\n        return executable_files\n\n    def configure(self, config: TConfigurable) -> None:\n        self.debug.write(f\"configure({config!r})\")\n        self.plugin.configure(config)\n\n    def sys_info(self) -> Iterable[tuple[str, Any]]:\n        return self.plugin.sys_info()\n\n\nclass DebugFileTracerWrapper(FileTracer):\n    \"\"\"A debugging `FileTracer`.\"\"\"\n\n    def __init__(self, tracer: FileTracer, debug: LabelledDebug) -> None:\n        self.tracer = tracer\n        self.debug = debug\n\n    def _show_frame(self, frame: FrameType) -> str:\n        \"\"\"A short string identifying a frame, for debug messages.\"\"\"\n        return \"%s@%d\" % (\n            os.path.basename(frame.f_code.co_filename),\n            frame.f_lineno,\n        )\n\n    def source_filename(self) -> str:\n        sfilename = self.tracer.source_filename()\n        self.debug.write(f\"source_filename() --> {sfilename!r}\")\n        return sfilename\n\n    def has_dynamic_source_filename(self) -> bool:\n        has = self.tracer.has_dynamic_source_filename()\n        self.debug.write(f\"has_dynamic_source_filename() --> {has!r}\")\n        return has\n\n    def dynamic_source_filename(self, filename: str, frame: FrameType) -> str | None:\n        dyn = self.tracer.dynamic_source_filename(filename, frame)\n        self.debug.write(\"dynamic_source_filename({!r}, {}) --> {!r}\".format(\n            filename, self._show_frame(frame), dyn,\n        ))\n        return dyn\n\n    def line_number_range(self, frame: FrameType) -> tuple[TLineNo, TLineNo]:\n        pair = self.tracer.line_number_range(frame)\n        self.debug.write(f\"line_number_range({self._show_frame(frame)}) --> {pair!r}\")\n        return pair\n\n\nclass DebugFileReporterWrapper(FileReporter):\n    \"\"\"A debugging `FileReporter`.\"\"\"\n\n    def __init__(self, filename: str, reporter: FileReporter, debug: LabelledDebug) -> None:\n        super().__init__(filename)\n        self.reporter = reporter\n        self.debug = debug\n\n    def relative_filename(self) -> str:\n        ret = self.reporter.relative_filename()\n        self.debug.write(f\"relative_filename() --> {ret!r}\")\n        return ret\n\n    def lines(self) -> set[TLineNo]:\n        ret = self.reporter.lines()\n        self.debug.write(f\"lines() --> {ret!r}\")\n        return ret\n\n    def excluded_lines(self) -> set[TLineNo]:\n        ret = self.reporter.excluded_lines()\n        self.debug.write(f\"excluded_lines() --> {ret!r}\")\n        return ret\n\n    def translate_lines(self, lines: Iterable[TLineNo]) -> set[TLineNo]:\n        ret = self.reporter.translate_lines(lines)\n        self.debug.write(f\"translate_lines({lines!r}) --> {ret!r}\")\n        return ret\n\n    def translate_arcs(self, arcs: Iterable[TArc]) -> set[TArc]:\n        ret = self.reporter.translate_arcs(arcs)\n        self.debug.write(f\"translate_arcs({arcs!r}) --> {ret!r}\")\n        return ret\n\n    def no_branch_lines(self) -> set[TLineNo]:\n        ret = self.reporter.no_branch_lines()\n        self.debug.write(f\"no_branch_lines() --> {ret!r}\")\n        return ret\n\n    def exit_counts(self) -> dict[TLineNo, int]:\n        ret = self.reporter.exit_counts()\n        self.debug.write(f\"exit_counts() --> {ret!r}\")\n        return ret\n\n    def arcs(self) -> set[TArc]:\n        ret = self.reporter.arcs()\n        self.debug.write(f\"arcs() --> {ret!r}\")\n        return ret\n\n    def source(self) -> str:\n        ret = self.reporter.source()\n        self.debug.write(\"source() --> %d chars\" % (len(ret),))\n        return ret\n\n    def source_token_lines(self) -> TSourceTokenLines:\n        ret = list(self.reporter.source_token_lines())\n        self.debug.write(\"source_token_lines() --> %d tokens\" % (len(ret),))\n        return ret\n", "coverage/templite.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"A simple Python template renderer, for a nano-subset of Django syntax.\n\nFor a detailed discussion of this code, see this chapter from 500 Lines:\nhttp://aosabook.org/en/500L/a-template-engine.html\n\n\"\"\"\n\n# Coincidentally named the same as http://code.activestate.com/recipes/496702/\n\nfrom __future__ import annotations\n\nimport re\n\nfrom typing import (\n    Any, Callable, Dict, NoReturn, cast,\n)\n\n\nclass TempliteSyntaxError(ValueError):\n    \"\"\"Raised when a template has a syntax error.\"\"\"\n    pass\n\n\nclass TempliteValueError(ValueError):\n    \"\"\"Raised when an expression won't evaluate in a template.\"\"\"\n    pass\n\n\nclass CodeBuilder:\n    \"\"\"Build source code conveniently.\"\"\"\n\n    def __init__(self, indent: int = 0) -> None:\n        self.code: list[str | CodeBuilder] = []\n        self.indent_level = indent\n\n    def __str__(self) -> str:\n        return \"\".join(str(c) for c in self.code)\n\n    def add_line(self, line: str) -> None:\n        \"\"\"Add a line of source to the code.\n\n        Indentation and newline will be added for you, don't provide them.\n\n        \"\"\"\n        self.code.extend([\" \" * self.indent_level, line, \"\\n\"])\n\n    def add_section(self) -> CodeBuilder:\n        \"\"\"Add a section, a sub-CodeBuilder.\"\"\"\n        section = CodeBuilder(self.indent_level)\n        self.code.append(section)\n        return section\n\n    INDENT_STEP = 4      # PEP8 says so!\n\n    def indent(self) -> None:\n        \"\"\"Increase the current indent for following lines.\"\"\"\n        self.indent_level += self.INDENT_STEP\n\n    def dedent(self) -> None:\n        \"\"\"Decrease the current indent for following lines.\"\"\"\n        self.indent_level -= self.INDENT_STEP\n\n    def get_globals(self) -> dict[str, Any]:\n        \"\"\"Execute the code, and return a dict of globals it defines.\"\"\"\n        # A check that the caller really finished all the blocks they started.\n        assert self.indent_level == 0\n        # Get the Python source as a single string.\n        python_source = str(self)\n        # Execute the source, defining globals, and return them.\n        global_namespace: dict[str, Any] = {}\n        exec(python_source, global_namespace)\n        return global_namespace\n\n\nclass Templite:\n    \"\"\"A simple template renderer, for a nano-subset of Django syntax.\n\n    Supported constructs are extended variable access::\n\n        {{var.modifier.modifier|filter|filter}}\n\n    loops::\n\n        {% for var in list %}...{% endfor %}\n\n    and ifs::\n\n        {% if var %}...{% endif %}\n\n    Comments are within curly-hash markers::\n\n        {# This will be ignored #}\n\n    Lines between `{% joined %}` and `{% endjoined %}` will have lines stripped\n    and joined.  Be careful, this could join words together!\n\n    Any of these constructs can have a hyphen at the end (`-}}`, `-%}`, `-#}`),\n    which will collapse the white space following the tag.\n\n    Construct a Templite with the template text, then use `render` against a\n    dictionary context to create a finished string::\n\n        templite = Templite('''\n            <h1>Hello {{name|upper}}!</h1>\n            {% for topic in topics %}\n                <p>You are interested in {{topic}}.</p>\n            {% endif %}\n            ''',\n            {\"upper\": str.upper},\n        )\n        text = templite.render({\n            \"name\": \"Ned\",\n            \"topics\": [\"Python\", \"Geometry\", \"Juggling\"],\n        })\n\n    \"\"\"\n    def __init__(self, text: str, *contexts: dict[str, Any]) -> None:\n        \"\"\"Construct a Templite with the given `text`.\n\n        `contexts` are dictionaries of values to use for future renderings.\n        These are good for filters and global values.\n\n        \"\"\"\n        self.context = {}\n        for context in contexts:\n            self.context.update(context)\n\n        self.all_vars: set[str] = set()\n        self.loop_vars: set[str] = set()\n\n        # We construct a function in source form, then compile it and hold onto\n        # it, and execute it to render the template.\n        code = CodeBuilder()\n\n        code.add_line(\"def render_function(context, do_dots):\")\n        code.indent()\n        vars_code = code.add_section()\n        code.add_line(\"result = []\")\n        code.add_line(\"append_result = result.append\")\n        code.add_line(\"extend_result = result.extend\")\n        code.add_line(\"to_str = str\")\n\n        buffered: list[str] = []\n\n        def flush_output() -> None:\n            \"\"\"Force `buffered` to the code builder.\"\"\"\n            if len(buffered) == 1:\n                code.add_line(\"append_result(%s)\" % buffered[0])\n            elif len(buffered) > 1:\n                code.add_line(\"extend_result([%s])\" % \", \".join(buffered))\n            del buffered[:]\n\n        ops_stack = []\n\n        # Split the text to form a list of tokens.\n        tokens = re.split(r\"(?s)({{.*?}}|{%.*?%}|{#.*?#})\", text)\n\n        squash = in_joined = False\n\n        for token in tokens:\n            if token.startswith(\"{\"):\n                start, end = 2, -2\n                squash = (token[-3] == \"-\")\n                if squash:\n                    end = -3\n\n                if token.startswith(\"{#\"):\n                    # Comment: ignore it and move on.\n                    continue\n                elif token.startswith(\"{{\"):\n                    # An expression to evaluate.\n                    expr = self._expr_code(token[start:end].strip())\n                    buffered.append(\"to_str(%s)\" % expr)\n                else:\n                    # token.startswith(\"{%\")\n                    # Action tag: split into words and parse further.\n                    flush_output()\n\n                    words = token[start:end].strip().split()\n                    if words[0] == \"if\":\n                        # An if statement: evaluate the expression to determine if.\n                        if len(words) != 2:\n                            self._syntax_error(\"Don't understand if\", token)\n                        ops_stack.append(\"if\")\n                        code.add_line(\"if %s:\" % self._expr_code(words[1]))\n                        code.indent()\n                    elif words[0] == \"for\":\n                        # A loop: iterate over expression result.\n                        if len(words) != 4 or words[2] != \"in\":\n                            self._syntax_error(\"Don't understand for\", token)\n                        ops_stack.append(\"for\")\n                        self._variable(words[1], self.loop_vars)\n                        code.add_line(\n                            f\"for c_{words[1]} in {self._expr_code(words[3])}:\",\n                        )\n                        code.indent()\n                    elif words[0] == \"joined\":\n                        ops_stack.append(\"joined\")\n                        in_joined = True\n                    elif words[0].startswith(\"end\"):\n                        # Endsomething.  Pop the ops stack.\n                        if len(words) != 1:\n                            self._syntax_error(\"Don't understand end\", token)\n                        end_what = words[0][3:]\n                        if not ops_stack:\n                            self._syntax_error(\"Too many ends\", token)\n                        start_what = ops_stack.pop()\n                        if start_what != end_what:\n                            self._syntax_error(\"Mismatched end tag\", end_what)\n                        if end_what == \"joined\":\n                            in_joined = False\n                        else:\n                            code.dedent()\n                    else:\n                        self._syntax_error(\"Don't understand tag\", words[0])\n            else:\n                # Literal content.  If it isn't empty, output it.\n                if in_joined:\n                    token = re.sub(r\"\\s*\\n\\s*\", \"\", token.strip())\n                elif squash:\n                    token = token.lstrip()\n                if token:\n                    buffered.append(repr(token))\n\n        if ops_stack:\n            self._syntax_error(\"Unmatched action tag\", ops_stack[-1])\n\n        flush_output()\n\n        for var_name in self.all_vars - self.loop_vars:\n            vars_code.add_line(f\"c_{var_name} = context[{var_name!r}]\")\n\n        code.add_line(\"return ''.join(result)\")\n        code.dedent()\n        self._render_function = cast(\n            Callable[\n                [Dict[str, Any], Callable[..., Any]],\n                str,\n            ],\n            code.get_globals()[\"render_function\"],\n        )\n\n    def _expr_code(self, expr: str) -> str:\n        \"\"\"Generate a Python expression for `expr`.\"\"\"\n        if \"|\" in expr:\n            pipes = expr.split(\"|\")\n            code = self._expr_code(pipes[0])\n            for func in pipes[1:]:\n                self._variable(func, self.all_vars)\n                code = f\"c_{func}({code})\"\n        elif \".\" in expr:\n            dots = expr.split(\".\")\n            code = self._expr_code(dots[0])\n            args = \", \".join(repr(d) for d in dots[1:])\n            code = f\"do_dots({code}, {args})\"\n        else:\n            self._variable(expr, self.all_vars)\n            code = \"c_%s\" % expr\n        return code\n\n    def _syntax_error(self, msg: str, thing: Any) -> NoReturn:\n        \"\"\"Raise a syntax error using `msg`, and showing `thing`.\"\"\"\n        raise TempliteSyntaxError(f\"{msg}: {thing!r}\")\n\n    def _variable(self, name: str, vars_set: set[str]) -> None:\n        \"\"\"Track that `name` is used as a variable.\n\n        Adds the name to `vars_set`, a set of variable names.\n\n        Raises an syntax error if `name` is not a valid name.\n\n        \"\"\"\n        if not re.match(r\"[_a-zA-Z][_a-zA-Z0-9]*$\", name):\n            self._syntax_error(\"Not a valid name\", name)\n        vars_set.add(name)\n\n    def render(self, context: dict[str, Any] | None = None) -> str:\n        \"\"\"Render this template by applying it to `context`.\n\n        `context` is a dictionary of values to use in this rendering.\n\n        \"\"\"\n        # Make the complete context we'll use.\n        render_context = dict(self.context)\n        if context:\n            render_context.update(context)\n        return self._render_function(render_context, self._do_dots)\n\n    def _do_dots(self, value: Any, *dots: str) -> Any:\n        \"\"\"Evaluate dotted expressions at run-time.\"\"\"\n        for dot in dots:\n            try:\n                value = getattr(value, dot)\n            except AttributeError:\n                try:\n                    value = value[dot]\n                except (TypeError, KeyError) as exc:\n                    raise TempliteValueError(\n                        f\"Couldn't evaluate {value!r}.{dot}\",\n                    ) from exc\n            if callable(value):\n                value = value()\n        return value\n", "coverage/config.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Config file for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport configparser\nimport copy\nimport os\nimport os.path\nimport re\n\nfrom typing import (\n    Any, Callable, Iterable, Union,\n)\n\nfrom coverage.exceptions import ConfigError\nfrom coverage.misc import isolate_module, human_sorted_items, substitute_variables\nfrom coverage.tomlconfig import TomlConfigParser, TomlDecodeError\nfrom coverage.types import (\n    TConfigurable, TConfigSectionIn, TConfigValueIn, TConfigSectionOut,\n    TConfigValueOut, TPluginConfig,\n)\n\nos = isolate_module(os)\n\n\nclass HandyConfigParser(configparser.ConfigParser):\n    \"\"\"Our specialization of ConfigParser.\"\"\"\n\n    def __init__(self, our_file: bool) -> None:\n        \"\"\"Create the HandyConfigParser.\n\n        `our_file` is True if this config file is specifically for coverage,\n        False if we are examining another config file (tox.ini, setup.cfg)\n        for possible settings.\n        \"\"\"\n\n        super().__init__(interpolation=None)\n        self.section_prefixes = [\"coverage:\"]\n        if our_file:\n            self.section_prefixes.append(\"\")\n\n    def read( # type: ignore[override]\n        self,\n        filenames: Iterable[str],\n        encoding_unused: str | None = None,\n    ) -> list[str]:\n        \"\"\"Read a file name as UTF-8 configuration data.\"\"\"\n        return super().read(filenames, encoding=\"utf-8\")\n\n    def real_section(self, section: str) -> str | None:\n        \"\"\"Get the actual name of a section.\"\"\"\n        for section_prefix in self.section_prefixes:\n            real_section = section_prefix + section\n            has = super().has_section(real_section)\n            if has:\n                return real_section\n        return None\n\n    def has_option(self, section: str, option: str) -> bool:\n        real_section = self.real_section(section)\n        if real_section is not None:\n            return super().has_option(real_section, option)\n        return False\n\n    def has_section(self, section: str) -> bool:\n        return bool(self.real_section(section))\n\n    def options(self, section: str) -> list[str]:\n        real_section = self.real_section(section)\n        if real_section is not None:\n            return super().options(real_section)\n        raise ConfigError(f\"No section: {section!r}\")\n\n    def get_section(self, section: str) -> TConfigSectionOut:\n        \"\"\"Get the contents of a section, as a dictionary.\"\"\"\n        d: dict[str, TConfigValueOut] = {}\n        for opt in self.options(section):\n            d[opt] = self.get(section, opt)\n        return d\n\n    def get(self, section: str, option: str, *args: Any, **kwargs: Any) -> str: # type: ignore\n        \"\"\"Get a value, replacing environment variables also.\n\n        The arguments are the same as `ConfigParser.get`, but in the found\n        value, ``$WORD`` or ``${WORD}`` are replaced by the value of the\n        environment variable ``WORD``.\n\n        Returns the finished value.\n\n        \"\"\"\n        for section_prefix in self.section_prefixes:\n            real_section = section_prefix + section\n            if super().has_option(real_section, option):\n                break\n        else:\n            raise ConfigError(f\"No option {option!r} in section: {section!r}\")\n\n        v: str = super().get(real_section, option, *args, **kwargs)\n        v = substitute_variables(v, os.environ)\n        return v\n\n    def getlist(self, section: str, option: str) -> list[str]:\n        \"\"\"Read a list of strings.\n\n        The value of `section` and `option` is treated as a comma- and newline-\n        separated list of strings.  Each value is stripped of white space.\n\n        Returns the list of strings.\n\n        \"\"\"\n        value_list = self.get(section, option)\n        values = []\n        for value_line in value_list.split(\"\\n\"):\n            for value in value_line.split(\",\"):\n                value = value.strip()\n                if value:\n                    values.append(value)\n        return values\n\n    def getregexlist(self, section: str, option: str) -> list[str]:\n        \"\"\"Read a list of full-line regexes.\n\n        The value of `section` and `option` is treated as a newline-separated\n        list of regexes.  Each value is stripped of white space.\n\n        Returns the list of strings.\n\n        \"\"\"\n        line_list = self.get(section, option)\n        value_list = []\n        for value in line_list.splitlines():\n            value = value.strip()\n            try:\n                re.compile(value)\n            except re.error as e:\n                raise ConfigError(\n                    f\"Invalid [{section}].{option} value {value!r}: {e}\",\n                ) from e\n            if value:\n                value_list.append(value)\n        return value_list\n\n\nTConfigParser = Union[HandyConfigParser, TomlConfigParser]\n\n\n# The default line exclusion regexes.\nDEFAULT_EXCLUDE = [\n    r\"#\\s*(pragma|PRAGMA)[:\\s]?\\s*(no|NO)\\s*(cover|COVER)\",\n]\n\n# The default partial branch regexes, to be modified by the user.\nDEFAULT_PARTIAL = [\n    r\"#\\s*(pragma|PRAGMA)[:\\s]?\\s*(no|NO)\\s*(branch|BRANCH)\",\n]\n\n# The default partial branch regexes, based on Python semantics.\n# These are any Python branching constructs that can't actually execute all\n# their branches.\nDEFAULT_PARTIAL_ALWAYS = [\n    \"while (True|1|False|0):\",\n    \"if (True|1|False|0):\",\n]\n\n\nclass CoverageConfig(TConfigurable, TPluginConfig):\n    \"\"\"Coverage.py configuration.\n\n    The attributes of this class are the various settings that control the\n    operation of coverage.py.\n\n    \"\"\"\n    # pylint: disable=too-many-instance-attributes\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the configuration attributes to their defaults.\"\"\"\n        # Metadata about the config.\n        # We tried to read these config files.\n        self.config_files_attempted: list[str] = []\n        # We did read these config files, but maybe didn't find any content for us.\n        self.config_files_read: list[str] = []\n        # The file that gave us our configuration.\n        self.config_file: str | None = None\n        self._config_contents: bytes | None = None\n\n        # Defaults for [run] and [report]\n        self._include = None\n        self._omit = None\n\n        # Defaults for [run]\n        self.branch = False\n        self.command_line: str | None = None\n        self.concurrency: list[str] = []\n        self.context: str | None = None\n        self.cover_pylib = False\n        self.data_file = \".coverage\"\n        self.debug: list[str] = []\n        self.debug_file: str | None = None\n        self.disable_warnings: list[str] = []\n        self.dynamic_context: str | None = None\n        self.parallel = False\n        self.plugins: list[str] = []\n        self.relative_files = False\n        self.run_include: list[str] = []\n        self.run_omit: list[str] = []\n        self.sigterm = False\n        self.source: list[str] | None = None\n        self.source_pkgs: list[str] = []\n        self.timid = False\n        self._crash: str | None = None\n\n        # Defaults for [report]\n        self.exclude_list = DEFAULT_EXCLUDE[:]\n        self.exclude_also: list[str] = []\n        self.fail_under = 0.0\n        self.format: str | None = None\n        self.ignore_errors = False\n        self.include_namespace_packages = False\n        self.report_include: list[str] | None = None\n        self.report_omit: list[str] | None = None\n        self.partial_always_list = DEFAULT_PARTIAL_ALWAYS[:]\n        self.partial_list = DEFAULT_PARTIAL[:]\n        self.precision = 0\n        self.report_contexts: list[str] | None = None\n        self.show_missing = False\n        self.skip_covered = False\n        self.skip_empty = False\n        self.sort: str | None = None\n\n        # Defaults for [html]\n        self.extra_css: str | None = None\n        self.html_dir = \"htmlcov\"\n        self.html_skip_covered: bool | None = None\n        self.html_skip_empty: bool | None = None\n        self.html_title = \"Coverage report\"\n        self.show_contexts = False\n\n        # Defaults for [xml]\n        self.xml_output = \"coverage.xml\"\n        self.xml_package_depth = 99\n\n        # Defaults for [json]\n        self.json_output = \"coverage.json\"\n        self.json_pretty_print = False\n        self.json_show_contexts = False\n\n        # Defaults for [lcov]\n        self.lcov_output = \"coverage.lcov\"\n\n        # Defaults for [paths]\n        self.paths: dict[str, list[str]] = {}\n\n        # Options for plugins\n        self.plugin_options: dict[str, TConfigSectionOut] = {}\n\n    MUST_BE_LIST = {\n        \"debug\", \"concurrency\", \"plugins\",\n        \"report_omit\", \"report_include\",\n        \"run_omit\", \"run_include\",\n    }\n\n    def from_args(self, **kwargs: TConfigValueIn) -> None:\n        \"\"\"Read config values from `kwargs`.\"\"\"\n        for k, v in kwargs.items():\n            if v is not None:\n                if k in self.MUST_BE_LIST and isinstance(v, str):\n                    v = [v]\n                setattr(self, k, v)\n\n    def from_file(self, filename: str, warn: Callable[[str], None], our_file: bool) -> bool:\n        \"\"\"Read configuration from a .rc file.\n\n        `filename` is a file name to read.\n\n        `our_file` is True if this config file is specifically for coverage,\n        False if we are examining another config file (tox.ini, setup.cfg)\n        for possible settings.\n\n        Returns True or False, whether the file could be read, and it had some\n        coverage.py settings in it.\n\n        \"\"\"\n        _, ext = os.path.splitext(filename)\n        cp: TConfigParser\n        if ext == \".toml\":\n            cp = TomlConfigParser(our_file)\n        else:\n            cp = HandyConfigParser(our_file)\n\n        self.config_files_attempted.append(os.path.abspath(filename))\n\n        try:\n            files_read = cp.read(filename)\n        except (configparser.Error, TomlDecodeError) as err:\n            raise ConfigError(f\"Couldn't read config file {filename}: {err}\") from err\n        if not files_read:\n            return False\n\n        self.config_files_read.extend(map(os.path.abspath, files_read))\n\n        any_set = False\n        try:\n            for option_spec in self.CONFIG_FILE_OPTIONS:\n                was_set = self._set_attr_from_config_option(cp, *option_spec)\n                if was_set:\n                    any_set = True\n        except ValueError as err:\n            raise ConfigError(f\"Couldn't read config file {filename}: {err}\") from err\n\n        # Check that there are no unrecognized options.\n        all_options = collections.defaultdict(set)\n        for option_spec in self.CONFIG_FILE_OPTIONS:\n            section, option = option_spec[1].split(\":\")\n            all_options[section].add(option)\n\n        for section, options in all_options.items():\n            real_section = cp.real_section(section)\n            if real_section:\n                for unknown in set(cp.options(section)) - options:\n                    warn(\n                        \"Unrecognized option '[{}] {}=' in config file {}\".format(\n                            real_section, unknown, filename,\n                        ),\n                    )\n\n        # [paths] is special\n        if cp.has_section(\"paths\"):\n            for option in cp.options(\"paths\"):\n                self.paths[option] = cp.getlist(\"paths\", option)\n                any_set = True\n\n        # plugins can have options\n        for plugin in self.plugins:\n            if cp.has_section(plugin):\n                self.plugin_options[plugin] = cp.get_section(plugin)\n                any_set = True\n\n        # Was this file used as a config file? If it's specifically our file,\n        # then it was used.  If we're piggybacking on someone else's file,\n        # then it was only used if we found some settings in it.\n        if our_file:\n            used = True\n        else:\n            used = any_set\n\n        if used:\n            self.config_file = os.path.abspath(filename)\n            with open(filename, \"rb\") as f:\n                self._config_contents = f.read()\n\n        return used\n\n    def copy(self) -> CoverageConfig:\n        \"\"\"Return a copy of the configuration.\"\"\"\n        return copy.deepcopy(self)\n\n    CONCURRENCY_CHOICES = {\"thread\", \"gevent\", \"greenlet\", \"eventlet\", \"multiprocessing\"}\n\n    CONFIG_FILE_OPTIONS = [\n        # These are *args for _set_attr_from_config_option:\n        #   (attr, where, type_=\"\")\n        #\n        #   attr is the attribute to set on the CoverageConfig object.\n        #   where is the section:name to read from the configuration file.\n        #   type_ is the optional type to apply, by using .getTYPE to read the\n        #       configuration value from the file.\n\n        # [run]\n        (\"branch\", \"run:branch\", \"boolean\"),\n        (\"command_line\", \"run:command_line\"),\n        (\"concurrency\", \"run:concurrency\", \"list\"),\n        (\"context\", \"run:context\"),\n        (\"cover_pylib\", \"run:cover_pylib\", \"boolean\"),\n        (\"data_file\", \"run:data_file\"),\n        (\"debug\", \"run:debug\", \"list\"),\n        (\"debug_file\", \"run:debug_file\"),\n        (\"disable_warnings\", \"run:disable_warnings\", \"list\"),\n        (\"dynamic_context\", \"run:dynamic_context\"),\n        (\"parallel\", \"run:parallel\", \"boolean\"),\n        (\"plugins\", \"run:plugins\", \"list\"),\n        (\"relative_files\", \"run:relative_files\", \"boolean\"),\n        (\"run_include\", \"run:include\", \"list\"),\n        (\"run_omit\", \"run:omit\", \"list\"),\n        (\"sigterm\", \"run:sigterm\", \"boolean\"),\n        (\"source\", \"run:source\", \"list\"),\n        (\"source_pkgs\", \"run:source_pkgs\", \"list\"),\n        (\"timid\", \"run:timid\", \"boolean\"),\n        (\"_crash\", \"run:_crash\"),\n\n        # [report]\n        (\"exclude_list\", \"report:exclude_lines\", \"regexlist\"),\n        (\"exclude_also\", \"report:exclude_also\", \"regexlist\"),\n        (\"fail_under\", \"report:fail_under\", \"float\"),\n        (\"format\", \"report:format\"),\n        (\"ignore_errors\", \"report:ignore_errors\", \"boolean\"),\n        (\"include_namespace_packages\", \"report:include_namespace_packages\", \"boolean\"),\n        (\"partial_always_list\", \"report:partial_branches_always\", \"regexlist\"),\n        (\"partial_list\", \"report:partial_branches\", \"regexlist\"),\n        (\"precision\", \"report:precision\", \"int\"),\n        (\"report_contexts\", \"report:contexts\", \"list\"),\n        (\"report_include\", \"report:include\", \"list\"),\n        (\"report_omit\", \"report:omit\", \"list\"),\n        (\"show_missing\", \"report:show_missing\", \"boolean\"),\n        (\"skip_covered\", \"report:skip_covered\", \"boolean\"),\n        (\"skip_empty\", \"report:skip_empty\", \"boolean\"),\n        (\"sort\", \"report:sort\"),\n\n        # [html]\n        (\"extra_css\", \"html:extra_css\"),\n        (\"html_dir\", \"html:directory\"),\n        (\"html_skip_covered\", \"html:skip_covered\", \"boolean\"),\n        (\"html_skip_empty\", \"html:skip_empty\", \"boolean\"),\n        (\"html_title\", \"html:title\"),\n        (\"show_contexts\", \"html:show_contexts\", \"boolean\"),\n\n        # [xml]\n        (\"xml_output\", \"xml:output\"),\n        (\"xml_package_depth\", \"xml:package_depth\", \"int\"),\n\n        # [json]\n        (\"json_output\", \"json:output\"),\n        (\"json_pretty_print\", \"json:pretty_print\", \"boolean\"),\n        (\"json_show_contexts\", \"json:show_contexts\", \"boolean\"),\n\n        # [lcov]\n        (\"lcov_output\", \"lcov:output\"),\n    ]\n\n    def _set_attr_from_config_option(\n        self,\n        cp: TConfigParser,\n        attr: str,\n        where: str,\n        type_: str = \"\",\n    ) -> bool:\n        \"\"\"Set an attribute on self if it exists in the ConfigParser.\n\n        Returns True if the attribute was set.\n\n        \"\"\"\n        section, option = where.split(\":\")\n        if cp.has_option(section, option):\n            method = getattr(cp, \"get\" + type_)\n            setattr(self, attr, method(section, option))\n            return True\n        return False\n\n    def get_plugin_options(self, plugin: str) -> TConfigSectionOut:\n        \"\"\"Get a dictionary of options for the plugin named `plugin`.\"\"\"\n        return self.plugin_options.get(plugin, {})\n\n    def set_option(self, option_name: str, value: TConfigValueIn | TConfigSectionIn) -> None:\n        \"\"\"Set an option in the configuration.\n\n        `option_name` is a colon-separated string indicating the section and\n        option name.  For example, the ``branch`` option in the ``[run]``\n        section of the config file would be indicated with `\"run:branch\"`.\n\n        `value` is the new value for the option.\n\n        \"\"\"\n        # Special-cased options.\n        if option_name == \"paths\":\n            self.paths = value  # type: ignore[assignment]\n            return\n\n        # Check all the hard-coded options.\n        for option_spec in self.CONFIG_FILE_OPTIONS:\n            attr, where = option_spec[:2]\n            if where == option_name:\n                setattr(self, attr, value)\n                return\n\n        # See if it's a plugin option.\n        plugin_name, _, key = option_name.partition(\":\")\n        if key and plugin_name in self.plugins:\n            self.plugin_options.setdefault(plugin_name, {})[key] = value # type: ignore[index]\n            return\n\n        # If we get here, we didn't find the option.\n        raise ConfigError(f\"No such option: {option_name!r}\")\n\n    def get_option(self, option_name: str) -> TConfigValueOut | None:\n        \"\"\"Get an option from the configuration.\n\n        `option_name` is a colon-separated string indicating the section and\n        option name.  For example, the ``branch`` option in the ``[run]``\n        section of the config file would be indicated with `\"run:branch\"`.\n\n        Returns the value of the option.\n\n        \"\"\"\n        # Special-cased options.\n        if option_name == \"paths\":\n            return self.paths  # type: ignore[return-value]\n\n        # Check all the hard-coded options.\n        for option_spec in self.CONFIG_FILE_OPTIONS:\n            attr, where = option_spec[:2]\n            if where == option_name:\n                return getattr(self, attr)  # type: ignore[no-any-return]\n\n        # See if it's a plugin option.\n        plugin_name, _, key = option_name.partition(\":\")\n        if key and plugin_name in self.plugins:\n            return self.plugin_options.get(plugin_name, {}).get(key)\n\n        # If we get here, we didn't find the option.\n        raise ConfigError(f\"No such option: {option_name!r}\")\n\n    def post_process_file(self, path: str) -> str:\n        \"\"\"Make final adjustments to a file path to make it usable.\"\"\"\n        return os.path.expanduser(path)\n\n    def post_process(self) -> None:\n        \"\"\"Make final adjustments to settings to make them usable.\"\"\"\n        self.data_file = self.post_process_file(self.data_file)\n        self.html_dir = self.post_process_file(self.html_dir)\n        self.xml_output = self.post_process_file(self.xml_output)\n        self.paths = {\n            k: [self.post_process_file(f) for f in v]\n            for k, v in self.paths.items()\n        }\n        self.exclude_list += self.exclude_also\n\n    def debug_info(self) -> list[tuple[str, Any]]:\n        \"\"\"Make a list of (name, value) pairs for writing debug info.\"\"\"\n        return human_sorted_items(\n            (k, v) for k, v in self.__dict__.items() if not k.startswith(\"_\")\n        )\n\n\ndef config_files_to_try(config_file: bool | str) -> list[tuple[str, bool, bool]]:\n    \"\"\"What config files should we try to read?\n\n    Returns a list of tuples:\n        (filename, is_our_file, was_file_specified)\n    \"\"\"\n\n    # Some API users were specifying \".coveragerc\" to mean the same as\n    # True, so make it so.\n    if config_file == \".coveragerc\":\n        config_file = True\n    specified_file = (config_file is not True)\n    if not specified_file:\n        # No file was specified. Check COVERAGE_RCFILE.\n        rcfile = os.getenv(\"COVERAGE_RCFILE\")\n        if rcfile:\n            config_file = rcfile\n            specified_file = True\n    if not specified_file:\n        # Still no file specified. Default to .coveragerc\n        config_file = \".coveragerc\"\n    assert isinstance(config_file, str)\n    files_to_try = [\n        (config_file, True, specified_file),\n        (\"setup.cfg\", False, False),\n        (\"tox.ini\", False, False),\n        (\"pyproject.toml\", False, False),\n    ]\n    return files_to_try\n\n\ndef read_coverage_config(\n    config_file: bool | str,\n    warn: Callable[[str], None],\n    **kwargs: TConfigValueIn,\n) -> CoverageConfig:\n    \"\"\"Read the coverage.py configuration.\n\n    Arguments:\n        config_file: a boolean or string, see the `Coverage` class for the\n            tricky details.\n        warn: a function to issue warnings.\n        all others: keyword arguments from the `Coverage` class, used for\n            setting values in the configuration.\n\n    Returns:\n        config:\n            config is a CoverageConfig object read from the appropriate\n            configuration file.\n\n    \"\"\"\n    # Build the configuration from a number of sources:\n    # 1) defaults:\n    config = CoverageConfig()\n\n    # 2) from a file:\n    if config_file:\n        files_to_try = config_files_to_try(config_file)\n\n        for fname, our_file, specified_file in files_to_try:\n            config_read = config.from_file(fname, warn, our_file=our_file)\n            if config_read:\n                break\n            if specified_file:\n                raise ConfigError(f\"Couldn't read {fname!r} as a config file\")\n\n    # 3) from environment variables:\n    env_data_file = os.getenv(\"COVERAGE_FILE\")\n    if env_data_file:\n        config.data_file = env_data_file\n    # $set_env.py: COVERAGE_DEBUG - Debug options: https://coverage.rtfd.io/cmd.html#debug\n    debugs = os.getenv(\"COVERAGE_DEBUG\")\n    if debugs:\n        config.debug.extend(d.strip() for d in debugs.split(\",\"))\n\n    # 4) from constructor arguments:\n    config.from_args(**kwargs)\n\n    # 5) for our benchmark, force settings using a secret environment variable:\n    force_file = os.getenv(\"COVERAGE_FORCE_CONFIG\")\n    if force_file:\n        config.from_file(force_file, warn, our_file=True)\n\n    # Once all the config has been collected, there's a little post-processing\n    # to do.\n    config.post_process()\n\n    return config\n", "coverage/regions.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Find functions and classes in Python code.\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport dataclasses\n\nfrom typing import cast\n\nfrom coverage.plugin import CodeRegion\n\n\n@dataclasses.dataclass\nclass Context:\n    \"\"\"The nested named context of a function or class.\"\"\"\n    name: str\n    kind: str\n    lines: set[int]\n\n\nclass RegionFinder:\n    \"\"\"An ast visitor that will find and track regions of code.\n\n    Functions and classes are tracked by name. Results are in the .regions\n    attribute.\n\n    \"\"\"\n    def __init__(self) -> None:\n        self.regions: list[CodeRegion] = []\n        self.context: list[Context] = []\n\n    def parse_source(self, source: str) -> None:\n        \"\"\"Parse `source` and walk the ast to populate the .regions attribute.\"\"\"\n        self.handle_node(ast.parse(source))\n\n    def fq_node_name(self) -> str:\n        \"\"\"Get the current fully qualified name we're processing.\"\"\"\n        return \".\".join(c.name for c in self.context)\n\n    def handle_node(self, node: ast.AST) -> None:\n        \"\"\"Recursively handle any node.\"\"\"\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n            self.handle_FunctionDef(node)\n        elif isinstance(node, ast.ClassDef):\n            self.handle_ClassDef(node)\n        else:\n            self.handle_node_body(node)\n\n    def handle_node_body(self, node: ast.AST) -> None:\n        \"\"\"Recursively handle the nodes in this node's body, if any.\"\"\"\n        for body_node in getattr(node, \"body\", ()):\n            self.handle_node(body_node)\n\n    def handle_FunctionDef(self, node: ast.FunctionDef | ast.AsyncFunctionDef) -> None:\n        \"\"\"Called for `def` or `async def`.\"\"\"\n        lines = set(range(node.body[0].lineno, cast(int, node.body[-1].end_lineno) + 1))\n        if self.context and self.context[-1].kind == \"class\":\n            # Function bodies are part of their enclosing class.\n            self.context[-1].lines |= lines\n        # Function bodies should be excluded from the nearest enclosing function.\n        for ancestor in reversed(self.context):\n            if ancestor.kind == \"function\":\n                ancestor.lines -= lines\n                break\n        self.context.append(Context(node.name, \"function\", lines))\n        self.regions.append(\n            CodeRegion(\n                kind=\"function\",\n                name=self.fq_node_name(),\n                start=node.lineno,\n                lines=lines,\n            )\n        )\n        self.handle_node_body(node)\n        self.context.pop()\n\n    def handle_ClassDef(self, node: ast.ClassDef) -> None:\n        \"\"\"Called for `class`.\"\"\"\n        # The lines for a class are the lines in the methods of the class.\n        # We start empty, and count on visit_FunctionDef to add the lines it\n        # finds.\n        lines: set[int] = set()\n        self.context.append(Context(node.name, \"class\", lines))\n        self.regions.append(\n            CodeRegion(\n                kind=\"class\",\n                name=self.fq_node_name(),\n                start=node.lineno,\n                lines=lines,\n            )\n        )\n        self.handle_node_body(node)\n        self.context.pop()\n        # Class bodies should be excluded from the enclosing classes.\n        for ancestor in reversed(self.context):\n            if ancestor.kind == \"class\":\n                ancestor.lines -= lines\n\n\ndef code_regions(source: str) -> list[CodeRegion]:\n    \"\"\"Find function and class regions in source code.\n\n    Analyzes the code in `source`, and returns a list of :class:`CodeRegion`\n    objects describing functions and classes as regions of the code::\n\n        [\n            CodeRegion(kind=\"function\", name=\"func1\", start=8, lines={10, 11, 12}),\n            CodeRegion(kind=\"function\", name=\"MyClass.method\", start=30, lines={34, 35, 36}),\n            CodeRegion(kind=\"class\", name=\"MyClass\", start=25, lines={34, 35, 36}),\n        ]\n\n    The line numbers will include comments and blank lines.  Later processing\n    will need to ignore those lines as needed.\n\n    Nested functions and classes are excluded from their enclosing region.  No\n    line should be reported as being part of more than one function, or more\n    than one class.  Lines in methods are reported as being in a function and\n    in a class.\n\n    \"\"\"\n    rf = RegionFinder()\n    rf.parse_source(source)\n    return rf.regions\n", "coverage/exceptions.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Exceptions coverage.py can raise.\"\"\"\n\nfrom __future__ import annotations\n\nclass _BaseCoverageException(Exception):\n    \"\"\"The base-base of all Coverage exceptions.\"\"\"\n    pass\n\n\nclass CoverageException(_BaseCoverageException):\n    \"\"\"The base class of all exceptions raised by Coverage.py.\"\"\"\n    pass\n\n\nclass ConfigError(_BaseCoverageException):\n    \"\"\"A problem with a config file, or a value in one.\"\"\"\n    pass\n\n\nclass DataError(CoverageException):\n    \"\"\"An error in using a data file.\"\"\"\n    pass\n\nclass NoDataError(CoverageException):\n    \"\"\"We didn't have data to work with.\"\"\"\n    pass\n\n\nclass NoSource(CoverageException):\n    \"\"\"We couldn't find the source for a module.\"\"\"\n    pass\n\n\nclass NoCode(NoSource):\n    \"\"\"We couldn't find any code at all.\"\"\"\n    pass\n\n\nclass NotPython(CoverageException):\n    \"\"\"A source file turned out not to be parsable Python.\"\"\"\n    pass\n\n\nclass PluginError(CoverageException):\n    \"\"\"A plugin misbehaved.\"\"\"\n    pass\n\n\nclass _ExceptionDuringRun(CoverageException):\n    \"\"\"An exception happened while running customer code.\n\n    Construct it with three arguments, the values from `sys.exc_info`.\n\n    \"\"\"\n    pass\n\n\nclass CoverageWarning(Warning):\n    \"\"\"A warning from Coverage.py.\"\"\"\n    pass\n", "coverage/inorout.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Determining whether files are being measured/reported or not.\"\"\"\n\nfrom __future__ import annotations\n\nimport importlib.util\nimport inspect\nimport itertools\nimport os\nimport platform\nimport re\nimport sys\nimport sysconfig\nimport traceback\n\nfrom types import FrameType, ModuleType\nfrom typing import (\n    cast, Any, Iterable, TYPE_CHECKING,\n)\n\nfrom coverage import env\nfrom coverage.disposition import FileDisposition, disposition_init\nfrom coverage.exceptions import CoverageException, PluginError\nfrom coverage.files import TreeMatcher, GlobMatcher, ModuleMatcher\nfrom coverage.files import prep_patterns, find_python_files, canonical_filename\nfrom coverage.misc import sys_modules_saved\nfrom coverage.python import source_for_file, source_for_morf\nfrom coverage.types import TFileDisposition, TMorf, TWarnFn, TDebugCtl\n\nif TYPE_CHECKING:\n    from coverage.config import CoverageConfig\n    from coverage.plugin_support import Plugins\n\n\n# Pypy has some unusual stuff in the \"stdlib\".  Consider those locations\n# when deciding where the stdlib is.  These modules are not used for anything,\n# they are modules importable from the pypy lib directories, so that we can\n# find those directories.\nmodules_we_happen_to_have: list[ModuleType] = [\n    inspect, itertools, os, platform, re, sysconfig, traceback,\n]\n\nif env.PYPY:\n    try:\n        import _structseq\n        modules_we_happen_to_have.append(_structseq)\n    except ImportError:\n        pass\n\n    try:\n        import _pypy_irc_topic\n        modules_we_happen_to_have.append(_pypy_irc_topic)\n    except ImportError:\n        pass\n\n\ndef canonical_path(morf: TMorf, directory: bool = False) -> str:\n    \"\"\"Return the canonical path of the module or file `morf`.\n\n    If the module is a package, then return its directory. If it is a\n    module, then return its file, unless `directory` is True, in which\n    case return its enclosing directory.\n\n    \"\"\"\n    morf_path = canonical_filename(source_for_morf(morf))\n    if morf_path.endswith(\"__init__.py\") or directory:\n        morf_path = os.path.split(morf_path)[0]\n    return morf_path\n\n\ndef name_for_module(filename: str, frame: FrameType | None) -> str:\n    \"\"\"Get the name of the module for a filename and frame.\n\n    For configurability's sake, we allow __main__ modules to be matched by\n    their importable name.\n\n    If loaded via runpy (aka -m), we can usually recover the \"original\"\n    full dotted module name, otherwise, we resort to interpreting the\n    file name to get the module's name.  In the case that the module name\n    can't be determined, None is returned.\n\n    \"\"\"\n    module_globals = frame.f_globals if frame is not None else {}\n    dunder_name: str = module_globals.get(\"__name__\", None)\n\n    if isinstance(dunder_name, str) and dunder_name != \"__main__\":\n        # This is the usual case: an imported module.\n        return dunder_name\n\n    spec = module_globals.get(\"__spec__\", None)\n    if spec:\n        fullname = spec.name\n        if isinstance(fullname, str) and fullname != \"__main__\":\n            # Module loaded via: runpy -m\n            return fullname\n\n    # Script as first argument to Python command line.\n    inspectedname = inspect.getmodulename(filename)\n    if inspectedname is not None:\n        return inspectedname\n    else:\n        return dunder_name\n\n\ndef module_is_namespace(mod: ModuleType) -> bool:\n    \"\"\"Is the module object `mod` a PEP420 namespace module?\"\"\"\n    return hasattr(mod, \"__path__\") and getattr(mod, \"__file__\", None) is None\n\n\ndef module_has_file(mod: ModuleType) -> bool:\n    \"\"\"Does the module object `mod` have an existing __file__ ?\"\"\"\n    mod__file__ = getattr(mod, \"__file__\", None)\n    if mod__file__ is None:\n        return False\n    return os.path.exists(mod__file__)\n\n\ndef file_and_path_for_module(modulename: str) -> tuple[str | None, list[str]]:\n    \"\"\"Find the file and search path for `modulename`.\n\n    Returns:\n        filename: The filename of the module, or None.\n        path: A list (possibly empty) of directories to find submodules in.\n\n    \"\"\"\n    filename = None\n    path = []\n    try:\n        spec = importlib.util.find_spec(modulename)\n    except Exception:\n        pass\n    else:\n        if spec is not None:\n            filename = spec.origin\n            path = list(spec.submodule_search_locations or ())\n    return filename, path\n\n\ndef add_stdlib_paths(paths: set[str]) -> None:\n    \"\"\"Add paths where the stdlib can be found to the set `paths`.\"\"\"\n    # Look at where some standard modules are located. That's the\n    # indication for \"installed with the interpreter\". In some\n    # environments (virtualenv, for example), these modules may be\n    # spread across a few locations. Look at all the candidate modules\n    # we've imported, and take all the different ones.\n    for m in modules_we_happen_to_have:\n        if hasattr(m, \"__file__\"):\n            paths.add(canonical_path(m, directory=True))\n\n\ndef add_third_party_paths(paths: set[str]) -> None:\n    \"\"\"Add locations for third-party packages to the set `paths`.\"\"\"\n    # Get the paths that sysconfig knows about.\n    scheme_names = set(sysconfig.get_scheme_names())\n\n    for scheme in scheme_names:\n        # https://foss.heptapod.net/pypy/pypy/-/issues/3433\n        better_scheme = \"pypy_posix\" if scheme == \"pypy\" else scheme\n        if os.name in better_scheme.split(\"_\"):\n            config_paths = sysconfig.get_paths(scheme)\n            for path_name in [\"platlib\", \"purelib\", \"scripts\"]:\n                paths.add(config_paths[path_name])\n\n\ndef add_coverage_paths(paths: set[str]) -> None:\n    \"\"\"Add paths where coverage.py code can be found to the set `paths`.\"\"\"\n    cover_path = canonical_path(__file__, directory=True)\n    paths.add(cover_path)\n    if env.TESTING:\n        # Don't include our own test code.\n        paths.add(os.path.join(cover_path, \"tests\"))\n\n\nclass InOrOut:\n    \"\"\"Machinery for determining what files to measure.\"\"\"\n\n    def __init__(\n        self,\n        config: CoverageConfig,\n        warn: TWarnFn,\n        debug: TDebugCtl | None,\n        include_namespace_packages: bool,\n    ) -> None:\n        self.warn = warn\n        self.debug = debug\n        self.include_namespace_packages = include_namespace_packages\n\n        self.source: list[str] = []\n        self.source_pkgs: list[str] = []\n        self.source_pkgs.extend(config.source_pkgs)\n        for src in config.source or []:\n            if os.path.isdir(src):\n                self.source.append(canonical_filename(src))\n            else:\n                self.source_pkgs.append(src)\n        self.source_pkgs_unmatched = self.source_pkgs[:]\n\n        self.include = prep_patterns(config.run_include)\n        self.omit = prep_patterns(config.run_omit)\n\n        # The directories for files considered \"installed with the interpreter\".\n        self.pylib_paths: set[str] = set()\n        if not config.cover_pylib:\n            add_stdlib_paths(self.pylib_paths)\n\n        # To avoid tracing the coverage.py code itself, we skip anything\n        # located where we are.\n        self.cover_paths: set[str] = set()\n        add_coverage_paths(self.cover_paths)\n\n        # Find where third-party packages are installed.\n        self.third_paths: set[str] = set()\n        add_third_party_paths(self.third_paths)\n\n        def _debug(msg: str) -> None:\n            if self.debug:\n                self.debug.write(msg)\n\n        # The matchers for should_trace.\n\n        # Generally useful information\n        _debug(\"sys.path:\" + \"\".join(f\"\\n    {p}\" for p in sys.path))\n\n        # Create the matchers we need for should_trace\n        self.source_match = None\n        self.source_pkgs_match = None\n        self.pylib_match = None\n        self.include_match = self.omit_match = None\n\n        if self.source or self.source_pkgs:\n            against = []\n            if self.source:\n                self.source_match = TreeMatcher(self.source, \"source\")\n                against.append(f\"trees {self.source_match!r}\")\n            if self.source_pkgs:\n                self.source_pkgs_match = ModuleMatcher(self.source_pkgs, \"source_pkgs\")\n                against.append(f\"modules {self.source_pkgs_match!r}\")\n            _debug(\"Source matching against \" + \" and \".join(against))\n        else:\n            if self.pylib_paths:\n                self.pylib_match = TreeMatcher(self.pylib_paths, \"pylib\")\n                _debug(f\"Python stdlib matching: {self.pylib_match!r}\")\n        if self.include:\n            self.include_match = GlobMatcher(self.include, \"include\")\n            _debug(f\"Include matching: {self.include_match!r}\")\n        if self.omit:\n            self.omit_match = GlobMatcher(self.omit, \"omit\")\n            _debug(f\"Omit matching: {self.omit_match!r}\")\n\n        self.cover_match = TreeMatcher(self.cover_paths, \"coverage\")\n        _debug(f\"Coverage code matching: {self.cover_match!r}\")\n\n        self.third_match = TreeMatcher(self.third_paths, \"third\")\n        _debug(f\"Third-party lib matching: {self.third_match!r}\")\n\n        # Check if the source we want to measure has been installed as a\n        # third-party package.\n        # Is the source inside a third-party area?\n        self.source_in_third_paths = set()\n        with sys_modules_saved():\n            for pkg in self.source_pkgs:\n                try:\n                    modfile, path = file_and_path_for_module(pkg)\n                    _debug(f\"Imported source package {pkg!r} as {modfile!r}\")\n                except CoverageException as exc:\n                    _debug(f\"Couldn't import source package {pkg!r}: {exc}\")\n                    continue\n                if modfile:\n                    if self.third_match.match(modfile):\n                        _debug(\n                            f\"Source in third-party: source_pkg {pkg!r} at {modfile!r}\",\n                        )\n                        self.source_in_third_paths.add(canonical_path(source_for_file(modfile)))\n                else:\n                    for pathdir in path:\n                        if self.third_match.match(pathdir):\n                            _debug(\n                                f\"Source in third-party: {pkg!r} path directory at {pathdir!r}\",\n                            )\n                            self.source_in_third_paths.add(pathdir)\n\n        for src in self.source:\n            if self.third_match.match(src):\n                _debug(f\"Source in third-party: source directory {src!r}\")\n                self.source_in_third_paths.add(src)\n        self.source_in_third_match = TreeMatcher(self.source_in_third_paths, \"source_in_third\")\n        _debug(f\"Source in third-party matching: {self.source_in_third_match}\")\n\n        self.plugins: Plugins\n        self.disp_class: type[TFileDisposition] = FileDisposition\n\n    def should_trace(self, filename: str, frame: FrameType | None = None) -> TFileDisposition:\n        \"\"\"Decide whether to trace execution in `filename`, with a reason.\n\n        This function is called from the trace function.  As each new file name\n        is encountered, this function determines whether it is traced or not.\n\n        Returns a FileDisposition object.\n\n        \"\"\"\n        original_filename = filename\n        disp = disposition_init(self.disp_class, filename)\n\n        def nope(disp: TFileDisposition, reason: str) -> TFileDisposition:\n            \"\"\"Simple helper to make it easy to return NO.\"\"\"\n            disp.trace = False\n            disp.reason = reason\n            return disp\n\n        if original_filename.startswith(\"<\"):\n            return nope(disp, \"original file name is not real\")\n\n        if frame is not None:\n            # Compiled Python files have two file names: frame.f_code.co_filename is\n            # the file name at the time the .pyc was compiled.  The second name is\n            # __file__, which is where the .pyc was actually loaded from.  Since\n            # .pyc files can be moved after compilation (for example, by being\n            # installed), we look for __file__ in the frame and prefer it to the\n            # co_filename value.\n            dunder_file = frame.f_globals and frame.f_globals.get(\"__file__\")\n            if dunder_file:\n                filename = source_for_file(dunder_file)\n                if original_filename and not original_filename.startswith(\"<\"):\n                    orig = os.path.basename(original_filename)\n                    if orig != os.path.basename(filename):\n                        # Files shouldn't be renamed when moved. This happens when\n                        # exec'ing code.  If it seems like something is wrong with\n                        # the frame's file name, then just use the original.\n                        filename = original_filename\n\n        if not filename:\n            # Empty string is pretty useless.\n            return nope(disp, \"empty string isn't a file name\")\n\n        if filename.startswith(\"memory:\"):\n            return nope(disp, \"memory isn't traceable\")\n\n        if filename.startswith(\"<\"):\n            # Lots of non-file execution is represented with artificial\n            # file names like \"<string>\", \"<doctest readme.txt[0]>\", or\n            # \"<exec_function>\".  Don't ever trace these executions, since we\n            # can't do anything with the data later anyway.\n            return nope(disp, \"file name is not real\")\n\n        canonical = canonical_filename(filename)\n        disp.canonical_filename = canonical\n\n        # Try the plugins, see if they have an opinion about the file.\n        plugin = None\n        for plugin in self.plugins.file_tracers:\n            if not plugin._coverage_enabled:\n                continue\n\n            try:\n                file_tracer = plugin.file_tracer(canonical)\n                if file_tracer is not None:\n                    file_tracer._coverage_plugin = plugin\n                    disp.trace = True\n                    disp.file_tracer = file_tracer\n                    if file_tracer.has_dynamic_source_filename():\n                        disp.has_dynamic_filename = True\n                    else:\n                        disp.source_filename = canonical_filename(\n                            file_tracer.source_filename(),\n                        )\n                    break\n            except Exception:\n                plugin_name = plugin._coverage_plugin_name\n                tb = traceback.format_exc()\n                self.warn(f\"Disabling plug-in {plugin_name!r} due to an exception:\\n{tb}\")\n                plugin._coverage_enabled = False\n                continue\n        else:\n            # No plugin wanted it: it's Python.\n            disp.trace = True\n            disp.source_filename = canonical\n\n        if not disp.has_dynamic_filename:\n            if not disp.source_filename:\n                raise PluginError(\n                    f\"Plugin {plugin!r} didn't set source_filename for '{disp.original_filename}'\",\n                )\n            reason = self.check_include_omit_etc(disp.source_filename, frame)\n            if reason:\n                nope(disp, reason)\n\n        return disp\n\n    def check_include_omit_etc(self, filename: str, frame: FrameType | None) -> str | None:\n        \"\"\"Check a file name against the include, omit, etc, rules.\n\n        Returns a string or None.  String means, don't trace, and is the reason\n        why.  None means no reason found to not trace.\n\n        \"\"\"\n        modulename = name_for_module(filename, frame)\n\n        # If the user specified source or include, then that's authoritative\n        # about the outer bound of what to measure and we don't have to apply\n        # any canned exclusions. If they didn't, then we have to exclude the\n        # stdlib and coverage.py directories.\n        if self.source_match or self.source_pkgs_match:\n            extra = \"\"\n            ok = False\n            if self.source_pkgs_match:\n                if self.source_pkgs_match.match(modulename):\n                    ok = True\n                    if modulename in self.source_pkgs_unmatched:\n                        self.source_pkgs_unmatched.remove(modulename)\n                else:\n                    extra = f\"module {modulename!r} \"\n            if not ok and self.source_match:\n                if self.source_match.match(filename):\n                    ok = True\n            if not ok:\n                return extra + \"falls outside the --source spec\"\n            if self.third_match.match(filename) and not self.source_in_third_match.match(filename):\n                return \"inside --source, but is third-party\"\n        elif self.include_match:\n            if not self.include_match.match(filename):\n                return \"falls outside the --include trees\"\n        else:\n            # We exclude the coverage.py code itself, since a little of it\n            # will be measured otherwise.\n            if self.cover_match.match(filename):\n                return \"is part of coverage.py\"\n\n            # If we aren't supposed to trace installed code, then check if this\n            # is near the Python standard library and skip it if so.\n            if self.pylib_match and self.pylib_match.match(filename):\n                return \"is in the stdlib\"\n\n            # Exclude anything in the third-party installation areas.\n            if self.third_match.match(filename):\n                return \"is a third-party module\"\n\n        # Check the file against the omit pattern.\n        if self.omit_match and self.omit_match.match(filename):\n            return \"is inside an --omit pattern\"\n\n        # No point tracing a file we can't later write to SQLite.\n        try:\n            filename.encode(\"utf-8\")\n        except UnicodeEncodeError:\n            return \"non-encodable filename\"\n\n        # No reason found to skip this file.\n        return None\n\n    def warn_conflicting_settings(self) -> None:\n        \"\"\"Warn if there are settings that conflict.\"\"\"\n        if self.include:\n            if self.source or self.source_pkgs:\n                self.warn(\"--include is ignored because --source is set\", slug=\"include-ignored\")\n\n    def warn_already_imported_files(self) -> None:\n        \"\"\"Warn if files have already been imported that we will be measuring.\"\"\"\n        if self.include or self.source or self.source_pkgs:\n            warned = set()\n            for mod in list(sys.modules.values()):\n                filename = getattr(mod, \"__file__\", None)\n                if filename is None:\n                    continue\n                if filename in warned:\n                    continue\n\n                if len(getattr(mod, \"__path__\", ())) > 1:\n                    # A namespace package, which confuses this code, so ignore it.\n                    continue\n\n                disp = self.should_trace(filename)\n                if disp.has_dynamic_filename:\n                    # A plugin with dynamic filenames: the Python file\n                    # shouldn't cause a warning, since it won't be the subject\n                    # of tracing anyway.\n                    continue\n                if disp.trace:\n                    msg = f\"Already imported a file that will be measured: {filename}\"\n                    self.warn(msg, slug=\"already-imported\")\n                    warned.add(filename)\n                elif self.debug and self.debug.should(\"trace\"):\n                    self.debug.write(\n                        \"Didn't trace already imported file {!r}: {}\".format(\n                            disp.original_filename, disp.reason,\n                        ),\n                    )\n\n    def warn_unimported_source(self) -> None:\n        \"\"\"Warn about source packages that were of interest, but never traced.\"\"\"\n        for pkg in self.source_pkgs_unmatched:\n            self._warn_about_unmeasured_code(pkg)\n\n    def _warn_about_unmeasured_code(self, pkg: str) -> None:\n        \"\"\"Warn about a package or module that we never traced.\n\n        `pkg` is a string, the name of the package or module.\n\n        \"\"\"\n        mod = sys.modules.get(pkg)\n        if mod is None:\n            self.warn(f\"Module {pkg} was never imported.\", slug=\"module-not-imported\")\n            return\n\n        if module_is_namespace(mod):\n            # A namespace package. It's OK for this not to have been traced,\n            # since there is no code directly in it.\n            return\n\n        if not module_has_file(mod):\n            self.warn(f\"Module {pkg} has no Python source.\", slug=\"module-not-python\")\n            return\n\n        # The module was in sys.modules, and seems like a module with code, but\n        # we never measured it. I guess that means it was imported before\n        # coverage even started.\n        msg = f\"Module {pkg} was previously imported, but not measured\"\n        self.warn(msg, slug=\"module-not-measured\")\n\n    def find_possibly_unexecuted_files(self) -> Iterable[tuple[str, str | None]]:\n        \"\"\"Find files in the areas of interest that might be untraced.\n\n        Yields pairs: file path, and responsible plug-in name.\n        \"\"\"\n        for pkg in self.source_pkgs:\n            if (pkg not in sys.modules or\n                not module_has_file(sys.modules[pkg])):\n                continue\n            pkg_file = source_for_file(cast(str, sys.modules[pkg].__file__))\n            yield from self._find_executable_files(canonical_path(pkg_file))\n\n        for src in self.source:\n            yield from self._find_executable_files(src)\n\n    def _find_plugin_files(self, src_dir: str) -> Iterable[tuple[str, str]]:\n        \"\"\"Get executable files from the plugins.\"\"\"\n        for plugin in self.plugins.file_tracers:\n            for x_file in plugin.find_executable_files(src_dir):\n                yield x_file, plugin._coverage_plugin_name\n\n    def _find_executable_files(self, src_dir: str) -> Iterable[tuple[str, str | None]]:\n        \"\"\"Find executable files in `src_dir`.\n\n        Search for files in `src_dir` that can be executed because they\n        are probably importable. Don't include ones that have been omitted\n        by the configuration.\n\n        Yield the file path, and the plugin name that handles the file.\n\n        \"\"\"\n        py_files = (\n            (py_file, None) for py_file in\n            find_python_files(src_dir, self.include_namespace_packages)\n        )\n        plugin_files = self._find_plugin_files(src_dir)\n\n        for file_path, plugin_name in itertools.chain(py_files, plugin_files):\n            file_path = canonical_filename(file_path)\n            if self.omit_match and self.omit_match.match(file_path):\n                # Turns out this file was omitted, so don't pull it back\n                # in as un-executed.\n                continue\n            yield file_path, plugin_name\n\n    def sys_info(self) -> Iterable[tuple[str, Any]]:\n        \"\"\"Our information for Coverage.sys_info.\n\n        Returns a list of (key, value) pairs.\n        \"\"\"\n        info = [\n            (\"coverage_paths\", self.cover_paths),\n            (\"stdlib_paths\", self.pylib_paths),\n            (\"third_party_paths\", self.third_paths),\n            (\"source_in_third_party_paths\", self.source_in_third_paths),\n        ]\n\n        matcher_names = [\n            \"source_match\", \"source_pkgs_match\",\n            \"include_match\", \"omit_match\",\n            \"cover_match\", \"pylib_match\", \"third_match\", \"source_in_third_match\",\n        ]\n\n        for matcher_name in matcher_names:\n            matcher = getattr(self, matcher_name)\n            if matcher:\n                matcher_info = matcher.info()\n            else:\n                matcher_info = \"-none-\"\n            info.append((matcher_name, matcher_info))\n\n        return info\n", "coverage/pytracer.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Raw data collector for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport dis\nimport itertools\nimport sys\nimport threading\n\nfrom types import FrameType, ModuleType\nfrom typing import Any, Callable, Set, cast\n\nfrom coverage import env\nfrom coverage.types import (\n    TArc, TFileDisposition, TLineNo, TTraceData, TTraceFileData, TTraceFn,\n    TracerCore, TWarnFn,\n)\n\n# We need the YIELD_VALUE opcode below, in a comparison-friendly form.\n# PYVERSIONS: RESUME is new in Python3.11\nRESUME = dis.opmap.get(\"RESUME\")\nRETURN_VALUE = dis.opmap[\"RETURN_VALUE\"]\nif RESUME is None:\n    YIELD_VALUE = dis.opmap[\"YIELD_VALUE\"]\n    YIELD_FROM = dis.opmap[\"YIELD_FROM\"]\n    YIELD_FROM_OFFSET = 0 if env.PYPY else 2\nelse:\n    YIELD_VALUE = YIELD_FROM = YIELD_FROM_OFFSET = -1\n\n# When running meta-coverage, this file can try to trace itself, which confuses\n# everything.  Don't trace ourselves.\n\nTHIS_FILE = __file__.rstrip(\"co\")\n\nclass PyTracer(TracerCore):\n    \"\"\"Python implementation of the raw data tracer.\"\"\"\n\n    # Because of poor implementations of trace-function-manipulating tools,\n    # the Python trace function must be kept very simple.  In particular, there\n    # must be only one function ever set as the trace function, both through\n    # sys.settrace, and as the return value from the trace function.  Put\n    # another way, the trace function must always return itself.  It cannot\n    # swap in other functions, or return None to avoid tracing a particular\n    # frame.\n    #\n    # The trace manipulator that introduced this restriction is DecoratorTools,\n    # which sets a trace function, and then later restores the pre-existing one\n    # by calling sys.settrace with a function it found in the current frame.\n    #\n    # Systems that use DecoratorTools (or similar trace manipulations) must use\n    # PyTracer to get accurate results.  The command-line --timid argument is\n    # used to force the use of this tracer.\n\n    tracer_ids = itertools.count()\n\n    def __init__(self) -> None:\n        # Which tracer are we?\n        self.id = next(self.tracer_ids)\n\n        # Attributes set from the collector:\n        self.data: TTraceData\n        self.trace_arcs = False\n        self.should_trace: Callable[[str, FrameType], TFileDisposition]\n        self.should_trace_cache: dict[str, TFileDisposition | None]\n        self.should_start_context: Callable[[FrameType], str | None] | None = None\n        self.switch_context: Callable[[str | None], None] | None = None\n        self.lock_data: Callable[[], None]\n        self.unlock_data: Callable[[], None]\n        self.warn: TWarnFn\n\n        # The threading module to use, if any.\n        self.threading: ModuleType | None = None\n\n        self.cur_file_data: TTraceFileData | None = None\n        self.last_line: TLineNo = 0\n        self.cur_file_name: str | None = None\n        self.context: str | None = None\n        self.started_context = False\n\n        # The data_stack parallels the Python call stack. Each entry is\n        # information about an active frame, a four-element tuple:\n        #   [0] The TTraceData for this frame's file. Could be None if we\n        #           aren't tracing this frame.\n        #   [1] The current file name for the frame. None if we aren't tracing\n        #           this frame.\n        #   [2] The last line number executed in this frame.\n        #   [3] Boolean: did this frame start a new context?\n        self.data_stack: list[tuple[TTraceFileData | None, str | None, TLineNo, bool]] = []\n        self.thread: threading.Thread | None = None\n        self.stopped = False\n        self._activity = False\n\n        self.in_atexit = False\n        # On exit, self.in_atexit = True\n        atexit.register(setattr, self, \"in_atexit\", True)\n\n        # Cache a bound method on the instance, so that we don't have to\n        # re-create a bound method object all the time.\n        self._cached_bound_method_trace: TTraceFn = self._trace\n\n    def __repr__(self) -> str:\n        points = sum(len(v) for v in self.data.values())\n        files = len(self.data)\n        return f\"<PyTracer at {id(self):#x}: {points} data points in {files} files>\"\n\n    def log(self, marker: str, *args: Any) -> None:\n        \"\"\"For hard-core logging of what this tracer is doing.\"\"\"\n        with open(\"/tmp/debug_trace.txt\", \"a\") as f:\n            f.write(f\"{marker} {self.id}[{len(self.data_stack)}]\")\n            if 0:   # if you want thread ids..\n                f.write(\".{:x}.{:x}\".format(                    # type: ignore[unreachable]\n                    self.thread.ident,\n                    self.threading.current_thread().ident,\n                ))\n            f.write(\" {}\".format(\" \".join(map(str, args))))\n            if 0:   # if you want callers..\n                f.write(\" | \")                                  # type: ignore[unreachable]\n                stack = \" / \".join(\n                    (fname or \"???\").rpartition(\"/\")[-1]\n                    for _, fname, _, _ in self.data_stack\n                )\n                f.write(stack)\n            f.write(\"\\n\")\n\n    def _trace(\n        self,\n        frame: FrameType,\n        event: str,\n        arg: Any,                               # pylint: disable=unused-argument\n        lineno: TLineNo | None = None,       # pylint: disable=unused-argument\n    ) -> TTraceFn | None:\n        \"\"\"The trace function passed to sys.settrace.\"\"\"\n\n        if THIS_FILE in frame.f_code.co_filename:\n            return None\n\n        # f = frame; code = f.f_code\n        # self.log(\":\", f\"{code.co_filename} {f.f_lineno} {code.co_name}()\", event)\n\n        if (self.stopped and sys.gettrace() == self._cached_bound_method_trace):    # pylint: disable=comparison-with-callable\n            # The PyTrace.stop() method has been called, possibly by another\n            # thread, let's deactivate ourselves now.\n            if 0:\n                f = frame                           # type: ignore[unreachable]\n                self.log(\"---\\nX\", f.f_code.co_filename, f.f_lineno)\n                while f:\n                    self.log(\">\", f.f_code.co_filename, f.f_lineno, f.f_code.co_name, f.f_trace)\n                    f = f.f_back\n            sys.settrace(None)\n            try:\n                self.cur_file_data, self.cur_file_name, self.last_line, self.started_context = (\n                    self.data_stack.pop()\n                )\n            except IndexError:\n                self.log(\n                    \"Empty stack!\",\n                    frame.f_code.co_filename,\n                    frame.f_lineno,\n                    frame.f_code.co_name,\n                )\n            return None\n\n        # if event != \"call\" and frame.f_code.co_filename != self.cur_file_name:\n        #     self.log(\"---\\n*\", frame.f_code.co_filename, self.cur_file_name, frame.f_lineno)\n\n        if event == \"call\":\n            # Should we start a new context?\n            if self.should_start_context and self.context is None:\n                context_maybe = self.should_start_context(frame)    # pylint: disable=not-callable\n                if context_maybe is not None:\n                    self.context = context_maybe\n                    started_context = True\n                    assert self.switch_context is not None\n                    self.switch_context(self.context)   # pylint: disable=not-callable\n                else:\n                    started_context = False\n            else:\n                started_context = False\n            self.started_context = started_context\n\n            # Entering a new frame.  Decide if we should trace in this file.\n            self._activity = True\n            self.data_stack.append(\n                (\n                    self.cur_file_data,\n                    self.cur_file_name,\n                    self.last_line,\n                    started_context,\n                ),\n            )\n\n            # Improve tracing performance: when calling a function, both caller\n            # and callee are often within the same file. if that's the case, we\n            # don't have to re-check whether to trace the corresponding\n            # function (which is a little bit expensive since it involves\n            # dictionary lookups). This optimization is only correct if we\n            # didn't start a context.\n            filename = frame.f_code.co_filename\n            if filename != self.cur_file_name or started_context:\n                self.cur_file_name = filename\n                disp = self.should_trace_cache.get(filename)\n                if disp is None:\n                    disp = self.should_trace(filename, frame)\n                    self.should_trace_cache[filename] = disp\n\n                self.cur_file_data = None\n                if disp.trace:\n                    tracename = disp.source_filename\n                    assert tracename is not None\n                    self.lock_data()\n                    try:\n                        if tracename not in self.data:\n                            self.data[tracename] = set()\n                    finally:\n                        self.unlock_data()\n                    self.cur_file_data = self.data[tracename]\n                else:\n                    frame.f_trace_lines = False\n            elif not self.cur_file_data:\n                frame.f_trace_lines = False\n\n            # The call event is really a \"start frame\" event, and happens for\n            # function calls and re-entering generators.  The f_lasti field is\n            # -1 for calls, and a real offset for generators.  Use <0 as the\n            # line number for calls, and the real line number for generators.\n            if RESUME is not None:\n                # The current opcode is guaranteed to be RESUME. The argument\n                # determines what kind of resume it is.\n                oparg = frame.f_code.co_code[frame.f_lasti + 1]\n                real_call = (oparg == 0)\n            else:\n                real_call = (getattr(frame, \"f_lasti\", -1) < 0)\n            if real_call:\n                self.last_line = -frame.f_code.co_firstlineno\n            else:\n                self.last_line = frame.f_lineno\n\n        elif event == \"line\":\n            # Record an executed line.\n            if self.cur_file_data is not None:\n                flineno: TLineNo = frame.f_lineno\n\n                if self.trace_arcs:\n                    cast(Set[TArc], self.cur_file_data).add((self.last_line, flineno))\n                else:\n                    cast(Set[TLineNo], self.cur_file_data).add(flineno)\n                self.last_line = flineno\n\n        elif event == \"return\":\n            if self.trace_arcs and self.cur_file_data:\n                # Record an arc leaving the function, but beware that a\n                # \"return\" event might just mean yielding from a generator.\n                code = frame.f_code.co_code\n                lasti = frame.f_lasti\n                if RESUME is not None:\n                    if len(code) == lasti + 2:\n                        # A return from the end of a code object is a real return.\n                        real_return = True\n                    else:\n                        # It is a real return if we aren't going to resume next.\n                        if env.PYBEHAVIOR.lasti_is_yield:\n                            lasti += 2\n                        real_return = (code[lasti] != RESUME)\n                else:\n                    if code[lasti] == RETURN_VALUE:\n                        real_return = True\n                    elif code[lasti] == YIELD_VALUE:\n                        real_return = False\n                    elif len(code) <= lasti + YIELD_FROM_OFFSET:\n                        real_return = True\n                    elif code[lasti + YIELD_FROM_OFFSET] == YIELD_FROM:\n                        real_return = False\n                    else:\n                        real_return = True\n                if real_return:\n                    first = frame.f_code.co_firstlineno\n                    cast(Set[TArc], self.cur_file_data).add((self.last_line, -first))\n\n            # Leaving this function, pop the filename stack.\n            self.cur_file_data, self.cur_file_name, self.last_line, self.started_context = (\n                self.data_stack.pop()\n            )\n            # Leaving a context?\n            if self.started_context:\n                assert self.switch_context is not None\n                self.context = None\n                self.switch_context(None)   # pylint: disable=not-callable\n        return self._cached_bound_method_trace\n\n    def start(self) -> TTraceFn:\n        \"\"\"Start this Tracer.\n\n        Return a Python function suitable for use with sys.settrace().\n\n        \"\"\"\n        self.stopped = False\n        if self.threading:\n            if self.thread is None:\n                self.thread = self.threading.current_thread()\n\n        sys.settrace(self._cached_bound_method_trace)\n        return self._cached_bound_method_trace\n\n    def stop(self) -> None:\n        \"\"\"Stop this Tracer.\"\"\"\n        # Get the active tracer callback before setting the stop flag to be\n        # able to detect if the tracer was changed prior to stopping it.\n        tf = sys.gettrace()\n\n        # Set the stop flag. The actual call to sys.settrace(None) will happen\n        # in the self._trace callback itself to make sure to call it from the\n        # right thread.\n        self.stopped = True\n\n        if self.threading:\n            assert self.thread is not None\n            if self.thread.ident != self.threading.current_thread().ident:\n                # Called on a different thread than started us: we can't unhook\n                # ourselves, but we've set the flag that we should stop, so we\n                # won't do any more tracing.\n                #self.log(\"~\", \"stopping on different threads\")\n                return\n\n        # PyPy clears the trace function before running atexit functions,\n        # so don't warn if we are in atexit on PyPy and the trace function\n        # has changed to None.  Metacoverage also messes this up, so don't\n        # warn if we are measuring ourselves.\n        suppress_warning = (\n            (env.PYPY and self.in_atexit and tf is None)\n            or env.METACOV\n        )\n        if self.warn and not suppress_warning:\n            if tf != self._cached_bound_method_trace:   # pylint: disable=comparison-with-callable\n                self.warn(\n                    \"Trace function changed, data is likely wrong: \" +\n                    f\"{tf!r} != {self._cached_bound_method_trace!r}\",\n                    slug=\"trace-changed\",\n                )\n\n    def activity(self) -> bool:\n        \"\"\"Has there been any activity?\"\"\"\n        return self._activity\n\n    def reset_activity(self) -> None:\n        \"\"\"Reset the activity() flag.\"\"\"\n        self._activity = False\n\n    def get_stats(self) -> dict[str, int] | None:\n        \"\"\"Return a dictionary of statistics, or None.\"\"\"\n        return None\n", "coverage/numbits.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nFunctions to manipulate packed binary representations of number sets.\n\nTo save space, coverage stores sets of line numbers in SQLite using a packed\nbinary representation called a numbits.  A numbits is a set of positive\nintegers.\n\nA numbits is stored as a blob in the database.  The exact meaning of the bytes\nin the blobs should be considered an implementation detail that might change in\nthe future.  Use these functions to work with those binary blobs of data.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sqlite3\n\nfrom itertools import zip_longest\nfrom typing import Iterable\n\n\ndef nums_to_numbits(nums: Iterable[int]) -> bytes:\n    \"\"\"Convert `nums` into a numbits.\n\n    Arguments:\n        nums: a reusable iterable of integers, the line numbers to store.\n\n    Returns:\n        A binary blob.\n    \"\"\"\n    try:\n        nbytes = max(nums) // 8 + 1\n    except ValueError:\n        # nums was empty.\n        return b\"\"\n    b = bytearray(nbytes)\n    for num in nums:\n        b[num//8] |= 1 << num % 8\n    return bytes(b)\n\n\ndef numbits_to_nums(numbits: bytes) -> list[int]:\n    \"\"\"Convert a numbits into a list of numbers.\n\n    Arguments:\n        numbits: a binary blob, the packed number set.\n\n    Returns:\n        A list of ints.\n\n    When registered as a SQLite function by :func:`register_sqlite_functions`,\n    this returns a string, a JSON-encoded list of ints.\n\n    \"\"\"\n    nums = []\n    for byte_i, byte in enumerate(numbits):\n        for bit_i in range(8):\n            if (byte & (1 << bit_i)):\n                nums.append(byte_i * 8 + bit_i)\n    return nums\n\n\ndef numbits_union(numbits1: bytes, numbits2: bytes) -> bytes:\n    \"\"\"Compute the union of two numbits.\n\n    Returns:\n        A new numbits, the union of `numbits1` and `numbits2`.\n    \"\"\"\n    byte_pairs = zip_longest(numbits1, numbits2, fillvalue=0)\n    return bytes(b1 | b2 for b1, b2 in byte_pairs)\n\n\ndef numbits_intersection(numbits1: bytes, numbits2: bytes) -> bytes:\n    \"\"\"Compute the intersection of two numbits.\n\n    Returns:\n        A new numbits, the intersection `numbits1` and `numbits2`.\n    \"\"\"\n    byte_pairs = zip_longest(numbits1, numbits2, fillvalue=0)\n    intersection_bytes = bytes(b1 & b2 for b1, b2 in byte_pairs)\n    return intersection_bytes.rstrip(b\"\\0\")\n\n\ndef numbits_any_intersection(numbits1: bytes, numbits2: bytes) -> bool:\n    \"\"\"Is there any number that appears in both numbits?\n\n    Determine whether two number sets have a non-empty intersection. This is\n    faster than computing the intersection.\n\n    Returns:\n        A bool, True if there is any number in both `numbits1` and `numbits2`.\n    \"\"\"\n    byte_pairs = zip_longest(numbits1, numbits2, fillvalue=0)\n    return any(b1 & b2 for b1, b2 in byte_pairs)\n\n\ndef num_in_numbits(num: int, numbits: bytes) -> bool:\n    \"\"\"Does the integer `num` appear in `numbits`?\n\n    Returns:\n        A bool, True if `num` is a member of `numbits`.\n    \"\"\"\n    nbyte, nbit = divmod(num, 8)\n    if nbyte >= len(numbits):\n        return False\n    return bool(numbits[nbyte] & (1 << nbit))\n\n\ndef register_sqlite_functions(connection: sqlite3.Connection) -> None:\n    \"\"\"\n    Define numbits functions in a SQLite connection.\n\n    This defines these functions for use in SQLite statements:\n\n    * :func:`numbits_union`\n    * :func:`numbits_intersection`\n    * :func:`numbits_any_intersection`\n    * :func:`num_in_numbits`\n    * :func:`numbits_to_nums`\n\n    `connection` is a :class:`sqlite3.Connection <python:sqlite3.Connection>`\n    object.  After creating the connection, pass it to this function to\n    register the numbits functions.  Then you can use numbits functions in your\n    queries::\n\n        import sqlite3\n        from coverage.numbits import register_sqlite_functions\n\n        conn = sqlite3.connect(\"example.db\")\n        register_sqlite_functions(conn)\n        c = conn.cursor()\n        # Kind of a nonsense query:\n        # Find all the files and contexts that executed line 47 in any file:\n        c.execute(\n            \"select file_id, context_id from line_bits where num_in_numbits(?, numbits)\",\n            (47,)\n        )\n    \"\"\"\n    connection.create_function(\"numbits_union\", 2, numbits_union)\n    connection.create_function(\"numbits_intersection\", 2, numbits_intersection)\n    connection.create_function(\"numbits_any_intersection\", 2, numbits_any_intersection)\n    connection.create_function(\"num_in_numbits\", 2, num_in_numbits)\n    connection.create_function(\"numbits_to_nums\", 1, lambda b: json.dumps(numbits_to_nums(b)))\n", "coverage/xmlreport.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"XML reporting for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport os.path\nimport sys\nimport time\nimport xml.dom.minidom\n\nfrom dataclasses import dataclass\nfrom typing import Any, IO, Iterable, TYPE_CHECKING\n\nfrom coverage import __version__, files\nfrom coverage.misc import isolate_module, human_sorted, human_sorted_items\nfrom coverage.plugin import FileReporter\nfrom coverage.report_core import get_analysis_to_report\nfrom coverage.results import Analysis\nfrom coverage.types import TMorf\nfrom coverage.version import __url__\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n\nos = isolate_module(os)\n\n\nDTD_URL = \"https://raw.githubusercontent.com/cobertura/web/master/htdocs/xml/coverage-04.dtd\"\n\n\ndef rate(hit: int, num: int) -> str:\n    \"\"\"Return the fraction of `hit`/`num`, as a string.\"\"\"\n    if num == 0:\n        return \"1\"\n    else:\n        return \"%.4g\" % (hit / num)\n\n\n@dataclass\nclass PackageData:\n    \"\"\"Data we keep about each \"package\" (in Java terms).\"\"\"\n    elements: dict[str, xml.dom.minidom.Element]\n    hits: int\n    lines: int\n    br_hits: int\n    branches: int\n\n\ndef appendChild(parent: Any, child: Any) -> None:\n    \"\"\"Append a child to a parent, in a way mypy will shut up about.\"\"\"\n    parent.appendChild(child)\n\n\nclass XmlReporter:\n    \"\"\"A reporter for writing Cobertura-style XML coverage results.\"\"\"\n\n    report_type = \"XML report\"\n\n    def __init__(self, coverage: Coverage) -> None:\n        self.coverage = coverage\n        self.config = self.coverage.config\n\n        self.source_paths = set()\n        if self.config.source:\n            for src in self.config.source:\n                if os.path.exists(src):\n                    if self.config.relative_files:\n                        src = src.rstrip(r\"\\/\")\n                    else:\n                        src = files.canonical_filename(src)\n                    self.source_paths.add(src)\n        self.packages: dict[str, PackageData] = {}\n        self.xml_out: xml.dom.minidom.Document\n\n    def report(self, morfs: Iterable[TMorf] | None, outfile: IO[str] | None = None) -> float:\n        \"\"\"Generate a Cobertura-compatible XML report for `morfs`.\n\n        `morfs` is a list of modules or file names.\n\n        `outfile` is a file object to write the XML to.\n\n        \"\"\"\n        # Initial setup.\n        outfile = outfile or sys.stdout\n        has_arcs = self.coverage.get_data().has_arcs()\n\n        # Create the DOM that will store the data.\n        impl = xml.dom.minidom.getDOMImplementation()\n        assert impl is not None\n        self.xml_out = impl.createDocument(None, \"coverage\", None)\n\n        # Write header stuff.\n        xcoverage = self.xml_out.documentElement\n        xcoverage.setAttribute(\"version\", __version__)\n        xcoverage.setAttribute(\"timestamp\", str(int(time.time()*1000)))\n        xcoverage.appendChild(self.xml_out.createComment(\n            f\" Generated by coverage.py: {__url__} \",\n        ))\n        xcoverage.appendChild(self.xml_out.createComment(f\" Based on {DTD_URL} \"))\n\n        # Call xml_file for each file in the data.\n        for fr, analysis in get_analysis_to_report(self.coverage, morfs):\n            self.xml_file(fr, analysis, has_arcs)\n\n        xsources = self.xml_out.createElement(\"sources\")\n        xcoverage.appendChild(xsources)\n\n        # Populate the XML DOM with the source info.\n        for path in human_sorted(self.source_paths):\n            xsource = self.xml_out.createElement(\"source\")\n            appendChild(xsources, xsource)\n            txt = self.xml_out.createTextNode(path)\n            appendChild(xsource, txt)\n\n        lnum_tot, lhits_tot = 0, 0\n        bnum_tot, bhits_tot = 0, 0\n\n        xpackages = self.xml_out.createElement(\"packages\")\n        xcoverage.appendChild(xpackages)\n\n        # Populate the XML DOM with the package info.\n        for pkg_name, pkg_data in human_sorted_items(self.packages.items()):\n            xpackage = self.xml_out.createElement(\"package\")\n            appendChild(xpackages, xpackage)\n            xclasses = self.xml_out.createElement(\"classes\")\n            appendChild(xpackage, xclasses)\n            for _, class_elt in human_sorted_items(pkg_data.elements.items()):\n                appendChild(xclasses, class_elt)\n            xpackage.setAttribute(\"name\", pkg_name.replace(os.sep, \".\"))\n            xpackage.setAttribute(\"line-rate\", rate(pkg_data.hits, pkg_data.lines))\n            if has_arcs:\n                branch_rate = rate(pkg_data.br_hits, pkg_data.branches)\n            else:\n                branch_rate = \"0\"\n            xpackage.setAttribute(\"branch-rate\", branch_rate)\n            xpackage.setAttribute(\"complexity\", \"0\")\n\n            lhits_tot += pkg_data.hits\n            lnum_tot += pkg_data.lines\n            bhits_tot += pkg_data.br_hits\n            bnum_tot += pkg_data.branches\n\n        xcoverage.setAttribute(\"lines-valid\", str(lnum_tot))\n        xcoverage.setAttribute(\"lines-covered\", str(lhits_tot))\n        xcoverage.setAttribute(\"line-rate\", rate(lhits_tot, lnum_tot))\n        if has_arcs:\n            xcoverage.setAttribute(\"branches-valid\", str(bnum_tot))\n            xcoverage.setAttribute(\"branches-covered\", str(bhits_tot))\n            xcoverage.setAttribute(\"branch-rate\", rate(bhits_tot, bnum_tot))\n        else:\n            xcoverage.setAttribute(\"branches-covered\", \"0\")\n            xcoverage.setAttribute(\"branches-valid\", \"0\")\n            xcoverage.setAttribute(\"branch-rate\", \"0\")\n        xcoverage.setAttribute(\"complexity\", \"0\")\n\n        # Write the output file.\n        outfile.write(serialize_xml(self.xml_out))\n\n        # Return the total percentage.\n        denom = lnum_tot + bnum_tot\n        if denom == 0:\n            pct = 0.0\n        else:\n            pct = 100.0 * (lhits_tot + bhits_tot) / denom\n        return pct\n\n    def xml_file(self, fr: FileReporter, analysis: Analysis, has_arcs: bool) -> None:\n        \"\"\"Add to the XML report for a single file.\"\"\"\n\n        if self.config.skip_empty:\n            if analysis.numbers.n_statements == 0:\n                return\n\n        # Create the \"lines\" and \"package\" XML elements, which\n        # are populated later.  Note that a package == a directory.\n        filename = fr.filename.replace(\"\\\\\", \"/\")\n        for source_path in self.source_paths:\n            if not self.config.relative_files:\n                source_path = files.canonical_filename(source_path)\n            if filename.startswith(source_path.replace(\"\\\\\", \"/\") + \"/\"):\n                rel_name = filename[len(source_path)+1:]\n                break\n        else:\n            rel_name = fr.relative_filename().replace(\"\\\\\", \"/\")\n            self.source_paths.add(fr.filename[:-len(rel_name)].rstrip(r\"\\/\"))\n\n        dirname = os.path.dirname(rel_name) or \".\"\n        dirname = \"/\".join(dirname.split(\"/\")[:self.config.xml_package_depth])\n        package_name = dirname.replace(\"/\", \".\")\n\n        package = self.packages.setdefault(package_name, PackageData({}, 0, 0, 0, 0))\n\n        xclass: xml.dom.minidom.Element = self.xml_out.createElement(\"class\")\n\n        appendChild(xclass, self.xml_out.createElement(\"methods\"))\n\n        xlines = self.xml_out.createElement(\"lines\")\n        appendChild(xclass, xlines)\n\n        xclass.setAttribute(\"name\", os.path.relpath(rel_name, dirname))\n        xclass.setAttribute(\"filename\", rel_name.replace(\"\\\\\", \"/\"))\n        xclass.setAttribute(\"complexity\", \"0\")\n\n        branch_stats = analysis.branch_stats()\n        missing_branch_arcs = analysis.missing_branch_arcs()\n\n        # For each statement, create an XML \"line\" element.\n        for line in sorted(analysis.statements):\n            xline = self.xml_out.createElement(\"line\")\n            xline.setAttribute(\"number\", str(line))\n\n            # Q: can we get info about the number of times a statement is\n            # executed?  If so, that should be recorded here.\n            xline.setAttribute(\"hits\", str(int(line not in analysis.missing)))\n\n            if has_arcs:\n                if line in branch_stats:\n                    total, taken = branch_stats[line]\n                    xline.setAttribute(\"branch\", \"true\")\n                    xline.setAttribute(\n                        \"condition-coverage\",\n                        \"%d%% (%d/%d)\" % (100*taken//total, taken, total),\n                    )\n                if line in missing_branch_arcs:\n                    annlines = [\"exit\" if b < 0 else str(b) for b in missing_branch_arcs[line]]\n                    xline.setAttribute(\"missing-branches\", \",\".join(annlines))\n            appendChild(xlines, xline)\n\n        class_lines = len(analysis.statements)\n        class_hits = class_lines - len(analysis.missing)\n\n        if has_arcs:\n            class_branches = sum(t for t, k in branch_stats.values())\n            missing_branches = sum(t - k for t, k in branch_stats.values())\n            class_br_hits = class_branches - missing_branches\n        else:\n            class_branches = 0\n            class_br_hits = 0\n\n        # Finalize the statistics that are collected in the XML DOM.\n        xclass.setAttribute(\"line-rate\", rate(class_hits, class_lines))\n        if has_arcs:\n            branch_rate = rate(class_br_hits, class_branches)\n        else:\n            branch_rate = \"0\"\n        xclass.setAttribute(\"branch-rate\", branch_rate)\n\n        package.elements[rel_name] = xclass\n        package.hits += class_hits\n        package.lines += class_lines\n        package.br_hits += class_br_hits\n        package.branches += class_branches\n\n\ndef serialize_xml(dom: xml.dom.minidom.Document) -> str:\n    \"\"\"Serialize a minidom node to XML.\"\"\"\n    return dom.toprettyxml()\n", "coverage/annotate.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Source file annotation for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\n\nfrom typing import Iterable, TYPE_CHECKING\n\nfrom coverage.files import flat_rootname\nfrom coverage.misc import ensure_dir, isolate_module\nfrom coverage.plugin import FileReporter\nfrom coverage.report_core import get_analysis_to_report\nfrom coverage.results import Analysis\nfrom coverage.types import TMorf\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n\nos = isolate_module(os)\n\n\nclass AnnotateReporter:\n    \"\"\"Generate annotated source files showing line coverage.\n\n    This reporter creates annotated copies of the measured source files. Each\n    .py file is copied as a .py,cover file, with a left-hand margin annotating\n    each line::\n\n        > def h(x):\n        -     if 0:   #pragma: no cover\n        -         pass\n        >     if x == 1:\n        !         a = 1\n        >     else:\n        >         a = 2\n\n        > h(2)\n\n    Executed lines use \">\", lines not executed use \"!\", lines excluded from\n    consideration use \"-\".\n\n    \"\"\"\n\n    def __init__(self, coverage: Coverage) -> None:\n        self.coverage = coverage\n        self.config = self.coverage.config\n        self.directory: str | None = None\n\n    blank_re = re.compile(r\"\\s*(#|$)\")\n    else_re = re.compile(r\"\\s*else\\s*:\\s*(#|$)\")\n\n    def report(self, morfs: Iterable[TMorf] | None, directory: str | None = None) -> None:\n        \"\"\"Run the report.\n\n        See `coverage.report()` for arguments.\n\n        \"\"\"\n        self.directory = directory\n        self.coverage.get_data()\n        for fr, analysis in get_analysis_to_report(self.coverage, morfs):\n            self.annotate_file(fr, analysis)\n\n    def annotate_file(self, fr: FileReporter, analysis: Analysis) -> None:\n        \"\"\"Annotate a single file.\n\n        `fr` is the FileReporter for the file to annotate.\n\n        \"\"\"\n        statements = sorted(analysis.statements)\n        missing = sorted(analysis.missing)\n        excluded = sorted(analysis.excluded)\n\n        if self.directory:\n            ensure_dir(self.directory)\n            dest_file = os.path.join(self.directory, flat_rootname(fr.relative_filename()))\n            if dest_file.endswith(\"_py\"):\n                dest_file = dest_file[:-3] + \".py\"\n            dest_file += \",cover\"\n        else:\n            dest_file = fr.filename + \",cover\"\n\n        with open(dest_file, \"w\", encoding=\"utf-8\") as dest:\n            i = j = 0\n            covered = True\n            source = fr.source()\n            for lineno, line in enumerate(source.splitlines(True), start=1):\n                while i < len(statements) and statements[i] < lineno:\n                    i += 1\n                while j < len(missing) and missing[j] < lineno:\n                    j += 1\n                if i < len(statements) and statements[i] == lineno:\n                    covered = j >= len(missing) or missing[j] > lineno\n                if self.blank_re.match(line):\n                    dest.write(\"  \")\n                elif self.else_re.match(line):\n                    # Special logic for lines containing only \"else:\".\n                    if j >= len(missing):\n                        dest.write(\"> \")\n                    elif statements[i] == missing[j]:\n                        dest.write(\"! \")\n                    else:\n                        dest.write(\"> \")\n                elif lineno in excluded:\n                    dest.write(\"- \")\n                elif covered:\n                    dest.write(\"> \")\n                else:\n                    dest.write(\"! \")\n\n                dest.write(line)\n", "coverage/report.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Summary reporting\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\nfrom typing import Any, IO, Iterable, TYPE_CHECKING\n\nfrom coverage.exceptions import ConfigError, NoDataError\nfrom coverage.misc import human_sorted_items\nfrom coverage.plugin import FileReporter\nfrom coverage.report_core import get_analysis_to_report\nfrom coverage.results import Analysis, Numbers\nfrom coverage.types import TMorf\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n\n\nclass SummaryReporter:\n    \"\"\"A reporter for writing the summary report.\"\"\"\n\n    def __init__(self, coverage: Coverage) -> None:\n        self.coverage = coverage\n        self.config = self.coverage.config\n        self.branches = coverage.get_data().has_arcs()\n        self.outfile: IO[str] | None = None\n        self.output_format = self.config.format or \"text\"\n        if self.output_format not in {\"text\", \"markdown\", \"total\"}:\n            raise ConfigError(f\"Unknown report format choice: {self.output_format!r}\")\n        self.fr_analysis: list[tuple[FileReporter, Analysis]] = []\n        self.skipped_count = 0\n        self.empty_count = 0\n        self.total = Numbers(precision=self.config.precision)\n\n    def write(self, line: str) -> None:\n        \"\"\"Write a line to the output, adding a newline.\"\"\"\n        assert self.outfile is not None\n        self.outfile.write(line.rstrip())\n        self.outfile.write(\"\\n\")\n\n    def write_items(self, items: Iterable[str]) -> None:\n        \"\"\"Write a list of strings, joined together.\"\"\"\n        self.write(\"\".join(items))\n\n    def _report_text(\n        self,\n        header: list[str],\n        lines_values: list[list[Any]],\n        total_line: list[Any],\n        end_lines: list[str],\n    ) -> None:\n        \"\"\"Internal method that prints report data in text format.\n\n        `header` is a list with captions.\n        `lines_values` is list of lists of sortable values.\n        `total_line` is a list with values of the total line.\n        `end_lines` is a list of ending lines with information about skipped files.\n\n        \"\"\"\n        # Prepare the formatting strings, header, and column sorting.\n        max_name = max([len(line[0]) for line in lines_values] + [5]) + 1\n        max_n = max(len(total_line[header.index(\"Cover\")]) + 2, len(\" Cover\")) + 1\n        max_n = max([max_n] + [len(line[header.index(\"Cover\")]) + 2 for line in lines_values])\n        formats = dict(\n            Name=\"{:{name_len}}\",\n            Stmts=\"{:>7}\",\n            Miss=\"{:>7}\",\n            Branch=\"{:>7}\",\n            BrPart=\"{:>7}\",\n            Cover=\"{:>{n}}\",\n            Missing=\"{:>10}\",\n        )\n        header_items = [\n            formats[item].format(item, name_len=max_name, n=max_n)\n            for item in header\n        ]\n        header_str = \"\".join(header_items)\n        rule = \"-\" * len(header_str)\n\n        # Write the header\n        self.write(header_str)\n        self.write(rule)\n\n        formats.update(dict(Cover=\"{:>{n}}%\"), Missing=\"   {:9}\")\n        for values in lines_values:\n            # build string with line values\n            line_items = [\n                formats[item].format(str(value),\n                name_len=max_name, n=max_n-1) for item, value in zip(header, values)\n            ]\n            self.write_items(line_items)\n\n        # Write a TOTAL line\n        if lines_values:\n            self.write(rule)\n\n        line_items = [\n            formats[item].format(str(value),\n            name_len=max_name, n=max_n-1) for item, value in zip(header, total_line)\n        ]\n        self.write_items(line_items)\n\n        for end_line in end_lines:\n            self.write(end_line)\n\n    def _report_markdown(\n        self,\n        header: list[str],\n        lines_values: list[list[Any]],\n        total_line: list[Any],\n        end_lines: list[str],\n    ) -> None:\n        \"\"\"Internal method that prints report data in markdown format.\n\n        `header` is a list with captions.\n        `lines_values` is a sorted list of lists containing coverage information.\n        `total_line` is a list with values of the total line.\n        `end_lines` is a list of ending lines with information about skipped files.\n\n        \"\"\"\n        # Prepare the formatting strings, header, and column sorting.\n        max_name = max((len(line[0].replace(\"_\", \"\\\\_\")) for line in lines_values), default=0)\n        max_name = max(max_name, len(\"**TOTAL**\")) + 1\n        formats = dict(\n            Name=\"| {:{name_len}}|\",\n            Stmts=\"{:>9} |\",\n            Miss=\"{:>9} |\",\n            Branch=\"{:>9} |\",\n            BrPart=\"{:>9} |\",\n            Cover=\"{:>{n}} |\",\n            Missing=\"{:>10} |\",\n        )\n        max_n = max(len(total_line[header.index(\"Cover\")]) + 6, len(\" Cover \"))\n        header_items = [formats[item].format(item, name_len=max_name, n=max_n) for item in header]\n        header_str = \"\".join(header_items)\n        rule_str = \"|\" + \" \".join([\"- |\".rjust(len(header_items[0])-1, \"-\")] +\n            [\"-: |\".rjust(len(item)-1, \"-\") for item in header_items[1:]],\n        )\n\n        # Write the header\n        self.write(header_str)\n        self.write(rule_str)\n\n        for values in lines_values:\n            # build string with line values\n            formats.update(dict(Cover=\"{:>{n}}% |\"))\n            line_items = [\n                formats[item].format(str(value).replace(\"_\", \"\\\\_\"), name_len=max_name, n=max_n-1)\n                for item, value in zip(header, values)\n            ]\n            self.write_items(line_items)\n\n        # Write the TOTAL line\n        formats.update(dict(Name=\"|{:>{name_len}} |\", Cover=\"{:>{n}} |\"))\n        total_line_items: list[str] = []\n        for item, value in zip(header, total_line):\n            if value == \"\":\n                insert = value\n            elif item == \"Cover\":\n                insert = f\" **{value}%**\"\n            else:\n                insert = f\" **{value}**\"\n            total_line_items += formats[item].format(insert, name_len=max_name, n=max_n)\n        self.write_items(total_line_items)\n        for end_line in end_lines:\n            self.write(end_line)\n\n    def report(self, morfs: Iterable[TMorf] | None, outfile: IO[str] | None = None) -> float:\n        \"\"\"Writes a report summarizing coverage statistics per module.\n\n        `outfile` is a text-mode file object to write the summary to.\n\n        \"\"\"\n        self.outfile = outfile or sys.stdout\n\n        self.coverage.get_data().set_query_contexts(self.config.report_contexts)\n        for fr, analysis in get_analysis_to_report(self.coverage, morfs):\n            self.report_one_file(fr, analysis)\n\n        if not self.total.n_files and not self.skipped_count:\n            raise NoDataError(\"No data to report.\")\n\n        if self.output_format == \"total\":\n            self.write(self.total.pc_covered_str)\n        else:\n            self.tabular_report()\n\n        return self.total.pc_covered\n\n    def tabular_report(self) -> None:\n        \"\"\"Writes tabular report formats.\"\"\"\n        # Prepare the header line and column sorting.\n        header = [\"Name\", \"Stmts\", \"Miss\"]\n        if self.branches:\n            header += [\"Branch\", \"BrPart\"]\n        header += [\"Cover\"]\n        if self.config.show_missing:\n            header += [\"Missing\"]\n\n        column_order = dict(name=0, stmts=1, miss=2, cover=-1)\n        if self.branches:\n            column_order.update(dict(branch=3, brpart=4))\n\n        # `lines_values` is list of lists of sortable values.\n        lines_values = []\n\n        for (fr, analysis) in self.fr_analysis:\n            nums = analysis.numbers\n\n            args = [fr.relative_filename(), nums.n_statements, nums.n_missing]\n            if self.branches:\n                args += [nums.n_branches, nums.n_partial_branches]\n            args += [nums.pc_covered_str]\n            if self.config.show_missing:\n                args += [analysis.missing_formatted(branches=True)]\n            args += [nums.pc_covered]\n            lines_values.append(args)\n\n        # Line sorting.\n        sort_option = (self.config.sort or \"name\").lower()\n        reverse = False\n        if sort_option[0] == \"-\":\n            reverse = True\n            sort_option = sort_option[1:]\n        elif sort_option[0] == \"+\":\n            sort_option = sort_option[1:]\n        sort_idx = column_order.get(sort_option)\n        if sort_idx is None:\n            raise ConfigError(f\"Invalid sorting option: {self.config.sort!r}\")\n        if sort_option == \"name\":\n            lines_values = human_sorted_items(lines_values, reverse=reverse)\n        else:\n            lines_values.sort(\n                key=lambda line: (line[sort_idx], line[0]),\n                reverse=reverse,\n            )\n\n        # Calculate total if we had at least one file.\n        total_line = [\"TOTAL\", self.total.n_statements, self.total.n_missing]\n        if self.branches:\n            total_line += [self.total.n_branches, self.total.n_partial_branches]\n        total_line += [self.total.pc_covered_str]\n        if self.config.show_missing:\n            total_line += [\"\"]\n\n        # Create other final lines.\n        end_lines = []\n        if self.config.skip_covered and self.skipped_count:\n            file_suffix = \"s\" if self.skipped_count>1 else \"\"\n            end_lines.append(\n                f\"\\n{self.skipped_count} file{file_suffix} skipped due to complete coverage.\",\n            )\n        if self.config.skip_empty and self.empty_count:\n            file_suffix = \"s\" if self.empty_count > 1 else \"\"\n            end_lines.append(f\"\\n{self.empty_count} empty file{file_suffix} skipped.\")\n\n        if self.output_format == \"markdown\":\n            formatter = self._report_markdown\n        else:\n            formatter = self._report_text\n        formatter(header, lines_values, total_line, end_lines)\n\n    def report_one_file(self, fr: FileReporter, analysis: Analysis) -> None:\n        \"\"\"Report on just one file, the callback from report().\"\"\"\n        nums = analysis.numbers\n        self.total += nums\n\n        no_missing_lines = (nums.n_missing == 0)\n        no_missing_branches = (nums.n_partial_branches == 0)\n        if self.config.skip_covered and no_missing_lines and no_missing_branches:\n            # Don't report on 100% files.\n            self.skipped_count += 1\n        elif self.config.skip_empty and nums.n_statements == 0:\n            # Don't report on empty files.\n            self.empty_count += 1\n        else:\n            self.fr_analysis.append((fr, analysis))\n", "coverage/jsonreport.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Json reporting for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport json\nimport sys\n\nfrom typing import Any, IO, Iterable, TYPE_CHECKING\n\nfrom coverage import __version__\nfrom coverage.report_core import get_analysis_to_report\nfrom coverage.results import Analysis, Numbers\nfrom coverage.types import TMorf, TLineNo\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n    from coverage.data import CoverageData\n\n\n# \"Version 1\" had no format number at all.\n# 2: add the meta.format field.\nFORMAT_VERSION = 2\n\nclass JsonReporter:\n    \"\"\"A reporter for writing JSON coverage results.\"\"\"\n\n    report_type = \"JSON report\"\n\n    def __init__(self, coverage: Coverage) -> None:\n        self.coverage = coverage\n        self.config = self.coverage.config\n        self.total = Numbers(self.config.precision)\n        self.report_data: dict[str, Any] = {}\n\n    def report(self, morfs: Iterable[TMorf] | None, outfile: IO[str]) -> float:\n        \"\"\"Generate a json report for `morfs`.\n\n        `morfs` is a list of modules or file names.\n\n        `outfile` is a file object to write the json to.\n\n        \"\"\"\n        outfile = outfile or sys.stdout\n        coverage_data = self.coverage.get_data()\n        coverage_data.set_query_contexts(self.config.report_contexts)\n        self.report_data[\"meta\"] = {\n            \"format\": FORMAT_VERSION,\n            \"version\": __version__,\n            \"timestamp\": datetime.datetime.now().isoformat(),\n            \"branch_coverage\": coverage_data.has_arcs(),\n            \"show_contexts\": self.config.json_show_contexts,\n        }\n\n        measured_files = {}\n        for file_reporter, analysis in get_analysis_to_report(self.coverage, morfs):\n            measured_files[file_reporter.relative_filename()] = self.report_one_file(\n                coverage_data,\n                analysis,\n            )\n\n        self.report_data[\"files\"] = measured_files\n\n        self.report_data[\"totals\"] = {\n            \"covered_lines\": self.total.n_executed,\n            \"num_statements\": self.total.n_statements,\n            \"percent_covered\": self.total.pc_covered,\n            \"percent_covered_display\": self.total.pc_covered_str,\n            \"missing_lines\": self.total.n_missing,\n            \"excluded_lines\": self.total.n_excluded,\n        }\n\n        if coverage_data.has_arcs():\n            self.report_data[\"totals\"].update({\n                \"num_branches\": self.total.n_branches,\n                \"num_partial_branches\": self.total.n_partial_branches,\n                \"covered_branches\": self.total.n_executed_branches,\n                \"missing_branches\": self.total.n_missing_branches,\n            })\n\n        json.dump(\n            self.report_data,\n            outfile,\n            indent=(4 if self.config.json_pretty_print else None),\n        )\n\n        return self.total.n_statements and self.total.pc_covered\n\n    def report_one_file(self, coverage_data: CoverageData, analysis: Analysis) -> dict[str, Any]:\n        \"\"\"Extract the relevant report data for a single file.\"\"\"\n        nums = analysis.numbers\n        self.total += nums\n        summary = {\n            \"covered_lines\": nums.n_executed,\n            \"num_statements\": nums.n_statements,\n            \"percent_covered\": nums.pc_covered,\n            \"percent_covered_display\": nums.pc_covered_str,\n            \"missing_lines\": nums.n_missing,\n            \"excluded_lines\": nums.n_excluded,\n        }\n        reported_file = {\n            \"executed_lines\": sorted(analysis.executed),\n            \"summary\": summary,\n            \"missing_lines\": sorted(analysis.missing),\n            \"excluded_lines\": sorted(analysis.excluded),\n        }\n        if self.config.json_show_contexts:\n            reported_file[\"contexts\"] = coverage_data.contexts_by_lineno(analysis.filename)\n        if coverage_data.has_arcs():\n            summary.update({\n                \"num_branches\": nums.n_branches,\n                \"num_partial_branches\": nums.n_partial_branches,\n                \"covered_branches\": nums.n_executed_branches,\n                \"missing_branches\": nums.n_missing_branches,\n            })\n            reported_file[\"executed_branches\"] = list(\n                _convert_branch_arcs(analysis.executed_branch_arcs()),\n            )\n            reported_file[\"missing_branches\"] = list(\n                _convert_branch_arcs(analysis.missing_branch_arcs()),\n            )\n        return reported_file\n\n\ndef _convert_branch_arcs(\n    branch_arcs: dict[TLineNo, list[TLineNo]],\n) -> Iterable[tuple[TLineNo, TLineNo]]:\n    \"\"\"Convert branch arcs to a list of two-element tuples.\"\"\"\n    for source, targets in branch_arcs.items():\n        for target in targets:\n            yield source, target\n", "coverage/version.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"The version and URL for coverage.py\"\"\"\n# This file is exec'ed in setup.py, don't import anything!\n\nfrom __future__ import annotations\n\n# version_info: same semantics as sys.version_info.\n# _dev: the .devN suffix if any.\nversion_info = (7, 5, 5, \"alpha\", 0)\n_dev = 1\n\n\ndef _make_version(\n    major: int,\n    minor: int,\n    micro: int,\n    releaselevel: str = \"final\",\n    serial: int = 0,\n    dev: int = 0,\n) -> str:\n    \"\"\"Create a readable version string from version_info tuple components.\"\"\"\n    assert releaselevel in [\"alpha\", \"beta\", \"candidate\", \"final\"]\n    version = \"%d.%d.%d\" % (major, minor, micro)\n    if releaselevel != \"final\":\n        short = {\"alpha\": \"a\", \"beta\": \"b\", \"candidate\": \"rc\"}[releaselevel]\n        version += f\"{short}{serial}\"\n    if dev != 0:\n        version += f\".dev{dev}\"\n    return version\n\n\ndef _make_url(\n    major: int,\n    minor: int,\n    micro: int,\n    releaselevel: str,\n    serial: int = 0,\n    dev: int = 0,\n) -> str:\n    \"\"\"Make the URL people should start at for this version of coverage.py.\"\"\"\n    return (\n        \"https://coverage.readthedocs.io/en/\"\n        + _make_version(major, minor, micro, releaselevel, serial, dev)\n    )\n\n\n__version__ = _make_version(*version_info, _dev)\n__url__ = _make_url(*version_info, _dev)\n", "coverage/cmdline.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Command-line support for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport glob\nimport optparse     # pylint: disable=deprecated-module\nimport os\nimport os.path\nimport shlex\nimport sys\nimport textwrap\nimport traceback\n\nfrom typing import cast, Any, NoReturn\n\nimport coverage\nfrom coverage import Coverage\nfrom coverage import env\nfrom coverage.collector import HAS_CTRACER\nfrom coverage.config import CoverageConfig\nfrom coverage.control import DEFAULT_DATAFILE\nfrom coverage.data import combinable_files, debug_data_file\nfrom coverage.debug import info_header, short_stack, write_formatted_info\nfrom coverage.exceptions import _BaseCoverageException, _ExceptionDuringRun, NoSource\nfrom coverage.execfile import PyRunner\nfrom coverage.results import display_covered, should_fail_under\nfrom coverage.version import __url__\n\n# When adding to this file, alphabetization is important.  Look for\n# \"alphabetize\" comments throughout.\n\nclass Opts:\n    \"\"\"A namespace class for individual options we'll build parsers from.\"\"\"\n\n    # Keep these entries alphabetized (roughly) by the option name as it\n    # appears on the command line.\n\n    append = optparse.make_option(\n        \"-a\", \"--append\", action=\"store_true\",\n        help=\"Append coverage data to .coverage, otherwise it starts clean each time.\",\n    )\n    branch = optparse.make_option(\n        \"\", \"--branch\", action=\"store_true\",\n        help=\"Measure branch coverage in addition to statement coverage.\",\n    )\n    concurrency = optparse.make_option(\n        \"\", \"--concurrency\", action=\"store\", metavar=\"LIBS\",\n        help=(\n            \"Properly measure code using a concurrency library. \" +\n            \"Valid values are: {}, or a comma-list of them.\"\n        ).format(\", \".join(sorted(CoverageConfig.CONCURRENCY_CHOICES))),\n    )\n    context = optparse.make_option(\n        \"\", \"--context\", action=\"store\", metavar=\"LABEL\",\n        help=\"The context label to record for this coverage run.\",\n    )\n    contexts = optparse.make_option(\n        \"\", \"--contexts\", action=\"store\", metavar=\"REGEX1,REGEX2,...\",\n        help=(\n            \"Only display data from lines covered in the given contexts. \" +\n            \"Accepts Python regexes, which must be quoted.\"\n        ),\n    )\n    datafile = optparse.make_option(\n        \"\", \"--data-file\", action=\"store\", metavar=\"DATAFILE\",\n        help=(\n            \"Base name of the data files to operate on. \" +\n            \"Defaults to '.coverage'. [env: COVERAGE_FILE]\"\n        ),\n    )\n    datafle_input = optparse.make_option(\n        \"\", \"--data-file\", action=\"store\", metavar=\"INFILE\",\n        help=(\n            \"Read coverage data for report generation from this file. \" +\n            \"Defaults to '.coverage'. [env: COVERAGE_FILE]\"\n        ),\n    )\n    datafile_output = optparse.make_option(\n        \"\", \"--data-file\", action=\"store\", metavar=\"OUTFILE\",\n        help=(\n            \"Write the recorded coverage data to this file. \" +\n            \"Defaults to '.coverage'. [env: COVERAGE_FILE]\"\n        ),\n    )\n    debug = optparse.make_option(\n        \"\", \"--debug\", action=\"store\", metavar=\"OPTS\",\n        help=\"Debug options, separated by commas. [env: COVERAGE_DEBUG]\",\n    )\n    directory = optparse.make_option(\n        \"-d\", \"--directory\", action=\"store\", metavar=\"DIR\",\n        help=\"Write the output files to DIR.\",\n    )\n    fail_under = optparse.make_option(\n        \"\", \"--fail-under\", action=\"store\", metavar=\"MIN\", type=\"float\",\n        help=\"Exit with a status of 2 if the total coverage is less than MIN.\",\n    )\n    format = optparse.make_option(\n        \"\", \"--format\", action=\"store\", metavar=\"FORMAT\",\n        help=\"Output format, either text (default), markdown, or total.\",\n    )\n    help = optparse.make_option(\n        \"-h\", \"--help\", action=\"store_true\",\n        help=\"Get help on this command.\",\n    )\n    ignore_errors = optparse.make_option(\n        \"-i\", \"--ignore-errors\", action=\"store_true\",\n        help=\"Ignore errors while reading source files.\",\n    )\n    include = optparse.make_option(\n        \"\", \"--include\", action=\"store\", metavar=\"PAT1,PAT2,...\",\n        help=(\n            \"Include only files whose paths match one of these patterns. \" +\n            \"Accepts shell-style wildcards, which must be quoted.\"\n        ),\n    )\n    keep = optparse.make_option(\n        \"\", \"--keep\", action=\"store_true\",\n        help=\"Keep original coverage files, otherwise they are deleted.\",\n    )\n    pylib = optparse.make_option(\n        \"-L\", \"--pylib\", action=\"store_true\",\n        help=(\n            \"Measure coverage even inside the Python installed library, \" +\n            \"which isn't done by default.\"\n        ),\n    )\n    show_missing = optparse.make_option(\n        \"-m\", \"--show-missing\", action=\"store_true\",\n        help=\"Show line numbers of statements in each module that weren't executed.\",\n    )\n    module = optparse.make_option(\n        \"-m\", \"--module\", action=\"store_true\",\n        help=(\n            \"<pyfile> is an importable Python module, not a script path, \" +\n            \"to be run as 'python -m' would run it.\"\n        ),\n    )\n    omit = optparse.make_option(\n        \"\", \"--omit\", action=\"store\", metavar=\"PAT1,PAT2,...\",\n        help=(\n            \"Omit files whose paths match one of these patterns. \" +\n            \"Accepts shell-style wildcards, which must be quoted.\"\n        ),\n    )\n    output_xml = optparse.make_option(\n        \"-o\", \"\", action=\"store\", dest=\"outfile\", metavar=\"OUTFILE\",\n        help=\"Write the XML report to this file. Defaults to 'coverage.xml'\",\n    )\n    output_json = optparse.make_option(\n        \"-o\", \"\", action=\"store\", dest=\"outfile\", metavar=\"OUTFILE\",\n        help=\"Write the JSON report to this file. Defaults to 'coverage.json'\",\n    )\n    output_lcov = optparse.make_option(\n        \"-o\", \"\", action=\"store\", dest=\"outfile\", metavar=\"OUTFILE\",\n        help=\"Write the LCOV report to this file. Defaults to 'coverage.lcov'\",\n    )\n    json_pretty_print = optparse.make_option(\n        \"\", \"--pretty-print\", action=\"store_true\",\n        help=\"Format the JSON for human readers.\",\n    )\n    parallel_mode = optparse.make_option(\n        \"-p\", \"--parallel-mode\", action=\"store_true\",\n        help=(\n            \"Append the machine name, process id and random number to the \" +\n            \"data file name to simplify collecting data from \" +\n            \"many processes.\"\n        ),\n    )\n    precision = optparse.make_option(\n        \"\", \"--precision\", action=\"store\", metavar=\"N\", type=int,\n        help=(\n            \"Number of digits after the decimal point to display for \" +\n            \"reported coverage percentages.\"\n        ),\n    )\n    quiet = optparse.make_option(\n        \"-q\", \"--quiet\", action=\"store_true\",\n        help=\"Don't print messages about what is happening.\",\n    )\n    rcfile = optparse.make_option(\n        \"\", \"--rcfile\", action=\"store\",\n        help=(\n            \"Specify configuration file. \" +\n            \"By default '.coveragerc', 'setup.cfg', 'tox.ini', and \" +\n            \"'pyproject.toml' are tried. [env: COVERAGE_RCFILE]\"\n        ),\n    )\n    show_contexts = optparse.make_option(\n        \"--show-contexts\", action=\"store_true\",\n        help=\"Show contexts for covered lines.\",\n    )\n    skip_covered = optparse.make_option(\n        \"--skip-covered\", action=\"store_true\",\n        help=\"Skip files with 100% coverage.\",\n    )\n    no_skip_covered = optparse.make_option(\n        \"--no-skip-covered\", action=\"store_false\", dest=\"skip_covered\",\n        help=\"Disable --skip-covered.\",\n    )\n    skip_empty = optparse.make_option(\n        \"--skip-empty\", action=\"store_true\",\n        help=\"Skip files with no code.\",\n    )\n    sort = optparse.make_option(\n        \"--sort\", action=\"store\", metavar=\"COLUMN\",\n        help=(\n            \"Sort the report by the named column: name, stmts, miss, branch, brpart, or cover. \" +\n             \"Default is name.\"\n        ),\n    )\n    source = optparse.make_option(\n        \"\", \"--source\", action=\"store\", metavar=\"SRC1,SRC2,...\",\n        help=\"A list of directories or importable names of code to measure.\",\n    )\n    timid = optparse.make_option(\n        \"\", \"--timid\", action=\"store_true\",\n        help=\"Use the slower Python trace function core.\",\n    )\n    title = optparse.make_option(\n        \"\", \"--title\", action=\"store\", metavar=\"TITLE\",\n        help=\"A text string to use as the title on the HTML.\",\n    )\n    version = optparse.make_option(\n        \"\", \"--version\", action=\"store_true\",\n        help=\"Display version information and exit.\",\n    )\n\n\nclass CoverageOptionParser(optparse.OptionParser):\n    \"\"\"Base OptionParser for coverage.py.\n\n    Problems don't exit the program.\n    Defaults are initialized for all options.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        kwargs[\"add_help_option\"] = False\n        super().__init__(*args, **kwargs)\n        self.set_defaults(\n            # Keep these arguments alphabetized by their names.\n            action=None,\n            append=None,\n            branch=None,\n            concurrency=None,\n            context=None,\n            contexts=None,\n            data_file=None,\n            debug=None,\n            directory=None,\n            fail_under=None,\n            format=None,\n            help=None,\n            ignore_errors=None,\n            include=None,\n            keep=None,\n            module=None,\n            omit=None,\n            parallel_mode=None,\n            precision=None,\n            pylib=None,\n            quiet=None,\n            rcfile=True,\n            show_contexts=None,\n            show_missing=None,\n            skip_covered=None,\n            skip_empty=None,\n            sort=None,\n            source=None,\n            timid=None,\n            title=None,\n            version=None,\n        )\n\n        self.disable_interspersed_args()\n\n    class OptionParserError(Exception):\n        \"\"\"Used to stop the optparse error handler ending the process.\"\"\"\n        pass\n\n    def parse_args_ok(self, args: list[str]) -> tuple[bool, optparse.Values | None, list[str]]:\n        \"\"\"Call optparse.parse_args, but return a triple:\n\n        (ok, options, args)\n\n        \"\"\"\n        try:\n            options, args = super().parse_args(args)\n        except self.OptionParserError:\n            return False, None, []\n        return True, options, args\n\n    def error(self, msg: str) -> NoReturn:\n        \"\"\"Override optparse.error so sys.exit doesn't get called.\"\"\"\n        show_help(msg)\n        raise self.OptionParserError\n\n\nclass GlobalOptionParser(CoverageOptionParser):\n    \"\"\"Command-line parser for coverage.py global option arguments.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.add_options([\n            Opts.help,\n            Opts.version,\n        ])\n\n\nclass CmdOptionParser(CoverageOptionParser):\n    \"\"\"Parse one of the new-style commands for coverage.py.\"\"\"\n\n    def __init__(\n        self,\n        action: str,\n        options: list[optparse.Option],\n        description: str,\n        usage: str | None = None,\n    ):\n        \"\"\"Create an OptionParser for a coverage.py command.\n\n        `action` is the slug to put into `options.action`.\n        `options` is a list of Option's for the command.\n        `description` is the description of the command, for the help text.\n        `usage` is the usage string to display in help.\n\n        \"\"\"\n        if usage:\n            usage = \"%prog \" + usage\n        super().__init__(\n            usage=usage,\n            description=description,\n        )\n        self.set_defaults(action=action)\n        self.add_options(options)\n        self.cmd = action\n\n    def __eq__(self, other: str) -> bool:       # type: ignore[override]\n        # A convenience equality, so that I can put strings in unit test\n        # results, and they will compare equal to objects.\n        return (other == f\"<CmdOptionParser:{self.cmd}>\")\n\n    __hash__ = None         # type: ignore[assignment]\n\n    def get_prog_name(self) -> str:\n        \"\"\"Override of an undocumented function in optparse.OptionParser.\"\"\"\n        program_name = super().get_prog_name()\n\n        # Include the sub-command for this parser as part of the command.\n        return f\"{program_name} {self.cmd}\"\n\n# In lists of Opts, keep them alphabetized by the option names as they appear\n# on the command line, since these lists determine the order of the options in\n# the help output.\n#\n# In COMMANDS, keep the keys (command names) alphabetized.\n\nGLOBAL_ARGS = [\n    Opts.debug,\n    Opts.help,\n    Opts.rcfile,\n]\n\nCOMMANDS = {\n    \"annotate\": CmdOptionParser(\n        \"annotate\",\n        [\n            Opts.directory,\n            Opts.datafle_input,\n            Opts.ignore_errors,\n            Opts.include,\n            Opts.omit,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] [modules]\",\n        description=(\n            \"Make annotated copies of the given files, marking statements that are executed \" +\n            \"with > and statements that are missed with !.\"\n        ),\n    ),\n\n    \"combine\": CmdOptionParser(\n        \"combine\",\n        [\n            Opts.append,\n            Opts.datafile,\n            Opts.keep,\n            Opts.quiet,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] <path1> <path2> ... <pathN>\",\n        description=(\n            \"Combine data from multiple coverage files. \" +\n            \"The combined results are written to a single \" +\n            \"file representing the union of the data. The positional \" +\n            \"arguments are data files or directories containing data files. \" +\n            \"If no paths are provided, data files in the default data file's \" +\n            \"directory are combined.\"\n        ),\n    ),\n\n    \"debug\": CmdOptionParser(\n        \"debug\", GLOBAL_ARGS,\n        usage=\"<topic>\",\n        description=(\n            \"Display information about the internals of coverage.py, \" +\n            \"for diagnosing problems. \" +\n            \"Topics are: \" +\n                \"'data' to show a summary of the collected data; \" +\n                \"'sys' to show installation information; \" +\n                \"'config' to show the configuration; \" +\n                \"'premain' to show what is calling coverage; \" +\n                \"'pybehave' to show internal flags describing Python behavior.\"\n        ),\n    ),\n\n    \"erase\": CmdOptionParser(\n        \"erase\",\n        [\n            Opts.datafile,\n            ] + GLOBAL_ARGS,\n        description=\"Erase previously collected coverage data.\",\n    ),\n\n    \"help\": CmdOptionParser(\n        \"help\", GLOBAL_ARGS,\n        usage=\"[command]\",\n        description=\"Describe how to use coverage.py\",\n    ),\n\n    \"html\": CmdOptionParser(\n        \"html\",\n        [\n            Opts.contexts,\n            Opts.directory,\n            Opts.datafle_input,\n            Opts.fail_under,\n            Opts.ignore_errors,\n            Opts.include,\n            Opts.omit,\n            Opts.precision,\n            Opts.quiet,\n            Opts.show_contexts,\n            Opts.skip_covered,\n            Opts.no_skip_covered,\n            Opts.skip_empty,\n            Opts.title,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] [modules]\",\n        description=(\n            \"Create an HTML report of the coverage of the files.  \" +\n            \"Each file gets its own page, with the source decorated to show \" +\n            \"executed, excluded, and missed lines.\"\n        ),\n    ),\n\n    \"json\": CmdOptionParser(\n        \"json\",\n        [\n            Opts.contexts,\n            Opts.datafle_input,\n            Opts.fail_under,\n            Opts.ignore_errors,\n            Opts.include,\n            Opts.omit,\n            Opts.output_json,\n            Opts.json_pretty_print,\n            Opts.quiet,\n            Opts.show_contexts,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] [modules]\",\n        description=\"Generate a JSON report of coverage results.\",\n    ),\n\n    \"lcov\": CmdOptionParser(\n        \"lcov\",\n        [\n            Opts.datafle_input,\n            Opts.fail_under,\n            Opts.ignore_errors,\n            Opts.include,\n            Opts.output_lcov,\n            Opts.omit,\n            Opts.quiet,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] [modules]\",\n        description=\"Generate an LCOV report of coverage results.\",\n    ),\n\n    \"report\": CmdOptionParser(\n        \"report\",\n        [\n            Opts.contexts,\n            Opts.datafle_input,\n            Opts.fail_under,\n            Opts.format,\n            Opts.ignore_errors,\n            Opts.include,\n            Opts.omit,\n            Opts.precision,\n            Opts.sort,\n            Opts.show_missing,\n            Opts.skip_covered,\n            Opts.no_skip_covered,\n            Opts.skip_empty,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] [modules]\",\n        description=\"Report coverage statistics on modules.\",\n    ),\n\n    \"run\": CmdOptionParser(\n        \"run\",\n        [\n            Opts.append,\n            Opts.branch,\n            Opts.concurrency,\n            Opts.context,\n            Opts.datafile_output,\n            Opts.include,\n            Opts.module,\n            Opts.omit,\n            Opts.pylib,\n            Opts.parallel_mode,\n            Opts.source,\n            Opts.timid,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] <pyfile> [program options]\",\n        description=\"Run a Python program, measuring code execution.\",\n    ),\n\n    \"xml\": CmdOptionParser(\n        \"xml\",\n        [\n            Opts.datafle_input,\n            Opts.fail_under,\n            Opts.ignore_errors,\n            Opts.include,\n            Opts.omit,\n            Opts.output_xml,\n            Opts.quiet,\n            Opts.skip_empty,\n            ] + GLOBAL_ARGS,\n        usage=\"[options] [modules]\",\n        description=\"Generate an XML report of coverage results.\",\n    ),\n}\n\n\ndef show_help(\n    error: str | None = None,\n    topic: str | None = None,\n    parser: optparse.OptionParser | None = None,\n) -> None:\n    \"\"\"Display an error message, or the named topic.\"\"\"\n    assert error or topic or parser\n\n    program_path = sys.argv[0]\n    if program_path.endswith(os.path.sep + \"__main__.py\"):\n        # The path is the main module of a package; get that path instead.\n        program_path = os.path.dirname(program_path)\n    program_name = os.path.basename(program_path)\n    if env.WINDOWS:\n        # entry_points={\"console_scripts\":...} on Windows makes files\n        # called coverage.exe, coverage3.exe, and coverage-3.5.exe. These\n        # invoke coverage-script.py, coverage3-script.py, and\n        # coverage-3.5-script.py.  argv[0] is the .py file, but we want to\n        # get back to the original form.\n        auto_suffix = \"-script.py\"\n        if program_name.endswith(auto_suffix):\n            program_name = program_name[:-len(auto_suffix)]\n\n    help_params = dict(coverage.__dict__)\n    help_params[\"__url__\"] = __url__\n    help_params[\"program_name\"] = program_name\n    if HAS_CTRACER:\n        help_params[\"extension_modifier\"] = \"with C extension\"\n    else:\n        help_params[\"extension_modifier\"] = \"without C extension\"\n\n    if error:\n        print(error, file=sys.stderr)\n        print(f\"Use '{program_name} help' for help.\", file=sys.stderr)\n    elif parser:\n        print(parser.format_help().strip())\n        print()\n    else:\n        assert topic is not None\n        help_msg = textwrap.dedent(HELP_TOPICS.get(topic, \"\")).strip()\n        if help_msg:\n            print(help_msg.format(**help_params))\n        else:\n            print(f\"Don't know topic {topic!r}\")\n    print(\"Full documentation is at {__url__}\".format(**help_params))\n\n\nOK, ERR, FAIL_UNDER = 0, 1, 2\n\n\nclass CoverageScript:\n    \"\"\"The command-line interface to coverage.py.\"\"\"\n\n    def __init__(self) -> None:\n        self.global_option = False\n        self.coverage: Coverage\n\n    def command_line(self, argv: list[str]) -> int:\n        \"\"\"The bulk of the command line interface to coverage.py.\n\n        `argv` is the argument list to process.\n\n        Returns 0 if all is well, 1 if something went wrong.\n\n        \"\"\"\n        # Collect the command-line options.\n        if not argv:\n            show_help(topic=\"minimum_help\")\n            return OK\n\n        # The command syntax we parse depends on the first argument.  Global\n        # switch syntax always starts with an option.\n        parser: optparse.OptionParser | None\n        self.global_option = argv[0].startswith(\"-\")\n        if self.global_option:\n            parser = GlobalOptionParser()\n        else:\n            parser = COMMANDS.get(argv[0])\n            if not parser:\n                show_help(f\"Unknown command: {argv[0]!r}\")\n                return ERR\n            argv = argv[1:]\n\n        ok, options, args = parser.parse_args_ok(argv)\n        if not ok:\n            return ERR\n        assert options is not None\n\n        # Handle help and version.\n        if self.do_help(options, args, parser):\n            return OK\n\n        # Listify the list options.\n        source = unshell_list(options.source)\n        omit = unshell_list(options.omit)\n        include = unshell_list(options.include)\n        debug = unshell_list(options.debug)\n        contexts = unshell_list(options.contexts)\n\n        if options.concurrency is not None:\n            concurrency = options.concurrency.split(\",\")\n        else:\n            concurrency = None\n\n        # Do something.\n        self.coverage = Coverage(\n            data_file=options.data_file or DEFAULT_DATAFILE,\n            data_suffix=options.parallel_mode,\n            cover_pylib=options.pylib,\n            timid=options.timid,\n            branch=options.branch,\n            config_file=options.rcfile,\n            source=source,\n            omit=omit,\n            include=include,\n            debug=debug,\n            concurrency=concurrency,\n            check_preimported=True,\n            context=options.context,\n            messages=not options.quiet,\n        )\n\n        if options.action == \"debug\":\n            return self.do_debug(args)\n\n        elif options.action == \"erase\":\n            self.coverage.erase()\n            return OK\n\n        elif options.action == \"run\":\n            return self.do_run(options, args)\n\n        elif options.action == \"combine\":\n            if options.append:\n                self.coverage.load()\n            data_paths = args or None\n            self.coverage.combine(data_paths, strict=True, keep=bool(options.keep))\n            self.coverage.save()\n            return OK\n\n        # Remaining actions are reporting, with some common options.\n        report_args = dict(\n            morfs=unglob_args(args),\n            ignore_errors=options.ignore_errors,\n            omit=omit,\n            include=include,\n            contexts=contexts,\n        )\n\n        # We need to be able to import from the current directory, because\n        # plugins may try to, for example, to read Django settings.\n        sys.path.insert(0, \"\")\n\n        self.coverage.load()\n\n        total = None\n        if options.action == \"report\":\n            total = self.coverage.report(\n                precision=options.precision,\n                show_missing=options.show_missing,\n                skip_covered=options.skip_covered,\n                skip_empty=options.skip_empty,\n                sort=options.sort,\n                output_format=options.format,\n                **report_args,\n            )\n        elif options.action == \"annotate\":\n            self.coverage.annotate(directory=options.directory, **report_args)\n        elif options.action == \"html\":\n            total = self.coverage.html_report(\n                directory=options.directory,\n                precision=options.precision,\n                skip_covered=options.skip_covered,\n                skip_empty=options.skip_empty,\n                show_contexts=options.show_contexts,\n                title=options.title,\n                **report_args,\n            )\n        elif options.action == \"xml\":\n            total = self.coverage.xml_report(\n                outfile=options.outfile,\n                skip_empty=options.skip_empty,\n                **report_args,\n            )\n        elif options.action == \"json\":\n            total = self.coverage.json_report(\n                outfile=options.outfile,\n                pretty_print=options.pretty_print,\n                show_contexts=options.show_contexts,\n                **report_args,\n            )\n        elif options.action == \"lcov\":\n            total = self.coverage.lcov_report(\n                outfile=options.outfile,\n                **report_args,\n            )\n        else:\n            # There are no other possible actions.\n            raise AssertionError\n\n        if total is not None:\n            # Apply the command line fail-under options, and then use the config\n            # value, so we can get fail_under from the config file.\n            if options.fail_under is not None:\n                self.coverage.set_option(\"report:fail_under\", options.fail_under)\n            if options.precision is not None:\n                self.coverage.set_option(\"report:precision\", options.precision)\n\n            fail_under = cast(float, self.coverage.get_option(\"report:fail_under\"))\n            precision = cast(int, self.coverage.get_option(\"report:precision\"))\n            if should_fail_under(total, fail_under, precision):\n                msg = \"total of {total} is less than fail-under={fail_under:.{p}f}\".format(\n                    total=display_covered(total, precision),\n                    fail_under=fail_under,\n                    p=precision,\n                )\n                print(\"Coverage failure:\", msg)\n                return FAIL_UNDER\n\n        return OK\n\n    def do_help(\n        self,\n        options: optparse.Values,\n        args: list[str],\n        parser: optparse.OptionParser,\n    ) -> bool:\n        \"\"\"Deal with help requests.\n\n        Return True if it handled the request, False if not.\n\n        \"\"\"\n        # Handle help.\n        if options.help:\n            if self.global_option:\n                show_help(topic=\"help\")\n            else:\n                show_help(parser=parser)\n            return True\n\n        if options.action == \"help\":\n            if args:\n                for a in args:\n                    parser_maybe = COMMANDS.get(a)\n                    if parser_maybe is not None:\n                        show_help(parser=parser_maybe)\n                    else:\n                        show_help(topic=a)\n            else:\n                show_help(topic=\"help\")\n            return True\n\n        # Handle version.\n        if options.version:\n            show_help(topic=\"version\")\n            return True\n\n        return False\n\n    def do_run(self, options: optparse.Values, args: list[str]) -> int:\n        \"\"\"Implementation of 'coverage run'.\"\"\"\n\n        if not args:\n            if options.module:\n                # Specified -m with nothing else.\n                show_help(\"No module specified for -m\")\n                return ERR\n            command_line = cast(str, self.coverage.get_option(\"run:command_line\"))\n            if command_line is not None:\n                args = shlex.split(command_line)\n                if args and args[0] in {\"-m\", \"--module\"}:\n                    options.module = True\n                    args = args[1:]\n        if not args:\n            show_help(\"Nothing to do.\")\n            return ERR\n\n        if options.append and self.coverage.get_option(\"run:parallel\"):\n            show_help(\"Can't append to data files in parallel mode.\")\n            return ERR\n\n        if options.concurrency == \"multiprocessing\":\n            # Can't set other run-affecting command line options with\n            # multiprocessing.\n            for opt_name in [\"branch\", \"include\", \"omit\", \"pylib\", \"source\", \"timid\"]:\n                # As it happens, all of these options have no default, meaning\n                # they will be None if they have not been specified.\n                if getattr(options, opt_name) is not None:\n                    show_help(\n                        \"Options affecting multiprocessing must only be specified \" +\n                        \"in a configuration file.\\n\" +\n                        f\"Remove --{opt_name} from the command line.\",\n                    )\n                    return ERR\n\n        os.environ[\"COVERAGE_RUN\"] = \"true\"\n\n        runner = PyRunner(args, as_module=bool(options.module))\n        runner.prepare()\n\n        if options.append:\n            self.coverage.load()\n\n        # Run the script.\n        self.coverage.start()\n        code_ran = True\n        try:\n            runner.run()\n        except NoSource:\n            code_ran = False\n            raise\n        finally:\n            self.coverage.stop()\n            if code_ran:\n                self.coverage.save()\n\n        return OK\n\n    def do_debug(self, args: list[str]) -> int:\n        \"\"\"Implementation of 'coverage debug'.\"\"\"\n\n        if not args:\n            show_help(\"What information would you like: config, data, sys, premain, pybehave?\")\n            return ERR\n        if args[1:]:\n            show_help(\"Only one topic at a time, please\")\n            return ERR\n\n        if args[0] == \"sys\":\n            write_formatted_info(print, \"sys\", self.coverage.sys_info())\n        elif args[0] == \"data\":\n            print(info_header(\"data\"))\n            data_file = self.coverage.config.data_file\n            debug_data_file(data_file)\n            for filename in combinable_files(data_file):\n                print(\"-----\")\n                debug_data_file(filename)\n        elif args[0] == \"config\":\n            write_formatted_info(print, \"config\", self.coverage.config.debug_info())\n        elif args[0] == \"premain\":\n            print(info_header(\"premain\"))\n            print(short_stack(full=True))\n        elif args[0] == \"pybehave\":\n            write_formatted_info(print, \"pybehave\", env.debug_info())\n        else:\n            show_help(f\"Don't know what you mean by {args[0]!r}\")\n            return ERR\n\n        return OK\n\n\ndef unshell_list(s: str) -> list[str] | None:\n    \"\"\"Turn a command-line argument into a list.\"\"\"\n    if not s:\n        return None\n    if env.WINDOWS:\n        # When running coverage.py as coverage.exe, some of the behavior\n        # of the shell is emulated: wildcards are expanded into a list of\n        # file names.  So you have to single-quote patterns on the command\n        # line, but (not) helpfully, the single quotes are included in the\n        # argument, so we have to strip them off here.\n        s = s.strip(\"'\")\n    return s.split(\",\")\n\n\ndef unglob_args(args: list[str]) -> list[str]:\n    \"\"\"Interpret shell wildcards for platforms that need it.\"\"\"\n    if env.WINDOWS:\n        globbed = []\n        for arg in args:\n            if \"?\" in arg or \"*\" in arg:\n                globbed.extend(glob.glob(arg))\n            else:\n                globbed.append(arg)\n        args = globbed\n    return args\n\n\nHELP_TOPICS = {\n    \"help\": \"\"\"\\\n        Coverage.py, version {__version__} {extension_modifier}\n        Measure, collect, and report on code coverage in Python programs.\n\n        usage: {program_name} <command> [options] [args]\n\n        Commands:\n            annotate    Annotate source files with execution information.\n            combine     Combine a number of data files.\n            debug       Display information about the internals of coverage.py\n            erase       Erase previously collected coverage data.\n            help        Get help on using coverage.py.\n            html        Create an HTML report.\n            json        Create a JSON report of coverage results.\n            lcov        Create an LCOV report of coverage results.\n            report      Report coverage stats on modules.\n            run         Run a Python program and measure code execution.\n            xml         Create an XML report of coverage results.\n\n        Use \"{program_name} help <command>\" for detailed help on any command.\n    \"\"\",\n\n    \"minimum_help\": (\n        \"Code coverage for Python, version {__version__} {extension_modifier}.  \" +\n        \"Use '{program_name} help' for help.\"\n    ),\n\n    \"version\": \"Coverage.py, version {__version__} {extension_modifier}\",\n}\n\n\ndef main(argv: list[str] | None = None) -> int | None:\n    \"\"\"The main entry point to coverage.py.\n\n    This is installed as the script entry point.\n\n    \"\"\"\n    if argv is None:\n        argv = sys.argv[1:]\n    try:\n        status = CoverageScript().command_line(argv)\n    except _ExceptionDuringRun as err:\n        # An exception was caught while running the product code.  The\n        # sys.exc_info() return tuple is packed into an _ExceptionDuringRun\n        # exception.\n        traceback.print_exception(*err.args)    # pylint: disable=no-value-for-parameter\n        status = ERR\n    except _BaseCoverageException as err:\n        # A controlled error inside coverage.py: print the message to the user.\n        msg = err.args[0]\n        print(msg)\n        status = ERR\n    except SystemExit as err:\n        # The user called `sys.exit()`.  Exit with their argument, if any.\n        if err.args:\n            status = err.args[0]\n        else:\n            status = None\n    return status\n\n# Profiling using ox_profile.  Install it from GitHub:\n#   pip install git+https://github.com/emin63/ox_profile.git\n#\n# $set_env.py: COVERAGE_PROFILE - Set to use ox_profile.\n_profile = os.getenv(\"COVERAGE_PROFILE\")\nif _profile:                                                # pragma: debugging\n    from ox_profile.core.launchers import SimpleLauncher    # pylint: disable=import-error\n    original_main = main\n\n    def main(                                               # pylint: disable=function-redefined\n        argv: list[str] | None = None,\n    ) -> int | None:\n        \"\"\"A wrapper around main that profiles.\"\"\"\n        profiler = SimpleLauncher.launch()\n        try:\n            return original_main(argv)\n        finally:\n            data, _ = profiler.query(re_filter=\"coverage\", max_records=100)\n            print(profiler.show(query=data, limit=100, sep=\"\", col=\"\"))\n            profiler.cancel()\n", "coverage/lcovreport.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"LCOV reporting for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport hashlib\nimport sys\n\nfrom typing import IO, Iterable, TYPE_CHECKING\n\nfrom coverage.plugin import FileReporter\nfrom coverage.report_core import get_analysis_to_report\nfrom coverage.results import Analysis, Numbers\nfrom coverage.types import TMorf\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n\n\ndef line_hash(line: str) -> str:\n    \"\"\"Produce a hash of a source line for use in the LCOV file.\"\"\"\n    # The LCOV file format requires MD5 as a fingerprint of the file. This is\n    # not a security use.  Some security scanners raise alarms about the use of\n    # MD5 here, but it is a false positive. This is not a security concern.\n    hashed = hashlib.md5(line.encode(\"utf-8\")).digest()\n    return base64.b64encode(hashed).decode(\"ascii\").rstrip(\"=\")\n\n\nclass LcovReporter:\n    \"\"\"A reporter for writing LCOV coverage reports.\"\"\"\n\n    report_type = \"LCOV report\"\n\n    def __init__(self, coverage: Coverage) -> None:\n        self.coverage = coverage\n        self.total = Numbers(self.coverage.config.precision)\n\n    def report(self, morfs: Iterable[TMorf] | None, outfile: IO[str]) -> float:\n        \"\"\"Renders the full lcov report.\n\n        `morfs` is a list of modules or filenames\n\n        outfile is the file object to write the file into.\n        \"\"\"\n\n        self.coverage.get_data()\n        outfile = outfile or sys.stdout\n\n        for fr, analysis in get_analysis_to_report(self.coverage, morfs):\n            self.get_lcov(fr, analysis, outfile)\n\n        return self.total.n_statements and self.total.pc_covered\n\n    def get_lcov(self, fr: FileReporter, analysis: Analysis, outfile: IO[str]) -> None:\n        \"\"\"Produces the lcov data for a single file.\n\n        This currently supports both line and branch coverage,\n        however function coverage is not supported.\n        \"\"\"\n        self.total += analysis.numbers\n\n        outfile.write(\"TN:\\n\")\n        outfile.write(f\"SF:{fr.relative_filename()}\\n\")\n        source_lines = fr.source().splitlines()\n        for covered in sorted(analysis.executed):\n            if covered in analysis.excluded:\n                # Do not report excluded as executed\n                continue\n            # Note: Coverage.py currently only supports checking *if* a line\n            # has been executed, not how many times, so we set this to 1 for\n            # nice output even if it's technically incorrect.\n\n            # The lines below calculate a 64-bit encoded md5 hash of the line\n            # corresponding to the DA lines in the lcov file, for either case\n            # of the line being covered or missed in coverage.py. The final two\n            # characters of the encoding (\"==\") are removed from the hash to\n            # allow genhtml to run on the resulting lcov file.\n            if source_lines:\n                if covered-1 >= len(source_lines):\n                    break\n                line = source_lines[covered-1]\n            else:\n                line = \"\"\n            outfile.write(f\"DA:{covered},1,{line_hash(line)}\\n\")\n\n        for missed in sorted(analysis.missing):\n            # We don't have to skip excluded lines here, because `missing`\n            # already doesn't have them.\n            assert source_lines\n            line = source_lines[missed-1]\n            outfile.write(f\"DA:{missed},0,{line_hash(line)}\\n\")\n\n        outfile.write(f\"LF:{analysis.numbers.n_statements}\\n\")\n        outfile.write(f\"LH:{analysis.numbers.n_executed}\\n\")\n\n        # More information dense branch coverage data.\n        missing_arcs = analysis.missing_branch_arcs()\n        executed_arcs = analysis.executed_branch_arcs()\n        for block_number, block_line_number in enumerate(\n            sorted(analysis.branch_stats().keys()),\n        ):\n            for branch_number, line_number in enumerate(\n                sorted(missing_arcs[block_line_number]),\n            ):\n                # The exit branches have a negative line number,\n                # this will not produce valid lcov. Setting\n                # the line number of the exit branch to 0 will allow\n                # for valid lcov, while preserving the data.\n                line_number = max(line_number, 0)\n                outfile.write(f\"BRDA:{line_number},{block_number},{branch_number},-\\n\")\n\n            # The start value below allows for the block number to be\n            # preserved between these two for loops (stopping the loop from\n            # resetting the value of the block number to 0).\n            for branch_number, line_number in enumerate(\n                sorted(executed_arcs[block_line_number]),\n                start=len(missing_arcs[block_line_number]),\n            ):\n                line_number = max(line_number, 0)\n                outfile.write(f\"BRDA:{line_number},{block_number},{branch_number},1\\n\")\n\n        # Summary of the branch coverage.\n        if analysis.has_arcs:\n            branch_stats = analysis.branch_stats()\n            brf = sum(t for t, k in branch_stats.values())\n            brh = brf - sum(t - k for t, k in branch_stats.values())\n            outfile.write(f\"BRF:{brf}\\n\")\n            outfile.write(f\"BRH:{brh}\\n\")\n\n        outfile.write(\"end_of_record\\n\")\n", "coverage/results.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Results of coverage measurement.\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport dataclasses\n\nfrom collections.abc import Container\nfrom typing import Iterable, TYPE_CHECKING\n\nfrom coverage.exceptions import ConfigError\nfrom coverage.misc import nice_pair\nfrom coverage.types import TArc, TLineNo\n\nif TYPE_CHECKING:\n    from coverage.data import CoverageData\n    from coverage.plugin import FileReporter\n\n\ndef analysis_from_file_reporter(\n    data: CoverageData,\n    precision: int,\n    file_reporter: FileReporter,\n    filename: str,\n) -> Analysis:\n    \"\"\"Create an Analysis from a FileReporter.\"\"\"\n    has_arcs = data.has_arcs()\n    statements = file_reporter.lines()\n    excluded = file_reporter.excluded_lines()\n    executed = file_reporter.translate_lines(data.lines(filename) or [])\n\n    if has_arcs:\n        _arc_possibilities_set = file_reporter.arcs()\n        _arcs_executed_set = file_reporter.translate_arcs(data.arcs(filename) or [])\n        exit_counts = file_reporter.exit_counts()\n        no_branch = file_reporter.no_branch_lines()\n    else:\n        _arc_possibilities_set = set()\n        _arcs_executed_set = set()\n        exit_counts = {}\n        no_branch = set()\n\n    return Analysis(\n        precision=precision,\n        filename=filename,\n        has_arcs=has_arcs,\n        statements=statements,\n        excluded=excluded,\n        executed=executed,\n        _arc_possibilities_set=_arc_possibilities_set,\n        _arcs_executed_set=_arcs_executed_set,\n        exit_counts=exit_counts,\n        no_branch=no_branch,\n    )\n\n\n@dataclasses.dataclass\nclass Analysis:\n    \"\"\"The results of analyzing a FileReporter.\"\"\"\n\n    precision: int\n    filename: str\n    has_arcs: bool\n    statements: set[TLineNo]\n    excluded: set[TLineNo]\n    executed: set[TLineNo]\n    _arc_possibilities_set: set[TArc]\n    _arcs_executed_set: set[TArc]\n    exit_counts: dict[TLineNo, int]\n    no_branch: set[TLineNo]\n\n    def __post_init__(self) -> None:\n        self.arc_possibilities = sorted(self._arc_possibilities_set)\n        self.arcs_executed = sorted(self._arcs_executed_set)\n        self.missing = self.statements - self.executed\n\n        if self.has_arcs:\n            n_branches = self._total_branches()\n            mba = self.missing_branch_arcs()\n            n_partial_branches = sum(len(v) for k,v in mba.items() if k not in self.missing)\n            n_missing_branches = sum(len(v) for k,v in mba.items())\n        else:\n            n_branches = n_partial_branches = n_missing_branches = 0\n\n        self.numbers = Numbers(\n            precision=self.precision,\n            n_files=1,\n            n_statements=len(self.statements),\n            n_excluded=len(self.excluded),\n            n_missing=len(self.missing),\n            n_branches=n_branches,\n            n_partial_branches=n_partial_branches,\n            n_missing_branches=n_missing_branches,\n        )\n\n    def narrow(self, lines: Container[TLineNo]) -> Analysis:\n        \"\"\"Create a narrowed Analysis.\n\n        The current analysis is copied to make a new one that only considers\n        the lines in `lines`.\n        \"\"\"\n\n        statements = {lno for lno in self.statements if lno in lines}\n        excluded = {lno for lno in self.excluded if lno in lines}\n        executed = {lno for lno in self.executed if lno in lines}\n\n        if self.has_arcs:\n            _arc_possibilities_set = {\n                (a, b) for a, b in self._arc_possibilities_set\n                if a in lines or b in lines\n            }\n            _arcs_executed_set = {\n                (a, b) for a, b in self._arcs_executed_set\n                if a in lines or b in lines\n            }\n            exit_counts = {\n                lno: num for lno, num in self.exit_counts.items()\n                if lno in lines\n            }\n            no_branch = {lno for lno in self.no_branch if lno in lines}\n        else:\n            _arc_possibilities_set = set()\n            _arcs_executed_set = set()\n            exit_counts = {}\n            no_branch = set()\n\n        return Analysis(\n            precision=self.precision,\n            filename=self.filename,\n            has_arcs=self.has_arcs,\n            statements=statements,\n            excluded=excluded,\n            executed=executed,\n            _arc_possibilities_set=_arc_possibilities_set,\n            _arcs_executed_set=_arcs_executed_set,\n            exit_counts=exit_counts,\n            no_branch=no_branch,\n        )\n\n    def missing_formatted(self, branches: bool = False) -> str:\n        \"\"\"The missing line numbers, formatted nicely.\n\n        Returns a string like \"1-2, 5-11, 13-14\".\n\n        If `branches` is true, includes the missing branch arcs also.\n\n        \"\"\"\n        if branches and self.has_arcs:\n            arcs = self.missing_branch_arcs().items()\n        else:\n            arcs = None\n\n        return format_lines(self.statements, self.missing, arcs=arcs)\n\n    def arcs_missing(self) -> list[TArc]:\n        \"\"\"Returns a sorted list of the un-executed arcs in the code.\"\"\"\n        missing = (\n            p for p in self.arc_possibilities\n                if p not in self.arcs_executed\n                    and p[0] not in self.no_branch\n                    and p[1] not in self.excluded\n        )\n        return sorted(missing)\n\n    def arcs_unpredicted(self) -> list[TArc]:\n        \"\"\"Returns a sorted list of the executed arcs missing from the code.\"\"\"\n        # Exclude arcs here which connect a line to itself.  They can occur\n        # in executed data in some cases.  This is where they can cause\n        # trouble, and here is where it's the least burden to remove them.\n        # Also, generators can somehow cause arcs from \"enter\" to \"exit\", so\n        # make sure we have at least one positive value.\n        unpredicted = (\n            e for e in self.arcs_executed\n                if e not in self.arc_possibilities\n                    and e[0] != e[1]\n                    and (e[0] > 0 or e[1] > 0)\n        )\n        return sorted(unpredicted)\n\n    def _branch_lines(self) -> list[TLineNo]:\n        \"\"\"Returns a list of line numbers that have more than one exit.\"\"\"\n        return [l1 for l1,count in self.exit_counts.items() if count > 1]\n\n    def _total_branches(self) -> int:\n        \"\"\"How many total branches are there?\"\"\"\n        return sum(count for count in self.exit_counts.values() if count > 1)\n\n    def missing_branch_arcs(self) -> dict[TLineNo, list[TLineNo]]:\n        \"\"\"Return arcs that weren't executed from branch lines.\n\n        Returns {l1:[l2a,l2b,...], ...}\n\n        \"\"\"\n        missing = self.arcs_missing()\n        branch_lines = set(self._branch_lines())\n        mba = collections.defaultdict(list)\n        for l1, l2 in missing:\n            if l1 in branch_lines:\n                mba[l1].append(l2)\n        return mba\n\n    def executed_branch_arcs(self) -> dict[TLineNo, list[TLineNo]]:\n        \"\"\"Return arcs that were executed from branch lines.\n\n        Returns {l1:[l2a,l2b,...], ...}\n\n        \"\"\"\n        branch_lines = set(self._branch_lines())\n        eba = collections.defaultdict(list)\n        for l1, l2 in self.arcs_executed:\n            if l1 in branch_lines:\n                eba[l1].append(l2)\n        return eba\n\n    def branch_stats(self) -> dict[TLineNo, tuple[int, int]]:\n        \"\"\"Get stats about branches.\n\n        Returns a dict mapping line numbers to a tuple:\n        (total_exits, taken_exits).\n        \"\"\"\n\n        missing_arcs = self.missing_branch_arcs()\n        stats = {}\n        for lnum in self._branch_lines():\n            exits = self.exit_counts[lnum]\n            missing = len(missing_arcs[lnum])\n            stats[lnum] = (exits, exits - missing)\n        return stats\n\n\n@dataclasses.dataclass\nclass Numbers:\n    \"\"\"The numerical results of measuring coverage.\n\n    This holds the basic statistics from `Analysis`, and is used to roll\n    up statistics across files.\n\n    \"\"\"\n\n    precision: int = 0\n    n_files: int = 0\n    n_statements: int = 0\n    n_excluded: int = 0\n    n_missing: int = 0\n    n_branches: int = 0\n    n_partial_branches: int = 0\n    n_missing_branches: int = 0\n\n    @property\n    def n_executed(self) -> int:\n        \"\"\"Returns the number of executed statements.\"\"\"\n        return self.n_statements - self.n_missing\n\n    @property\n    def n_executed_branches(self) -> int:\n        \"\"\"Returns the number of executed branches.\"\"\"\n        return self.n_branches - self.n_missing_branches\n\n    @property\n    def pc_covered(self) -> float:\n        \"\"\"Returns a single percentage value for coverage.\"\"\"\n        if self.n_statements > 0:\n            numerator, denominator = self.ratio_covered\n            pc_cov = (100.0 * numerator) / denominator\n        else:\n            pc_cov = 100.0\n        return pc_cov\n\n    @property\n    def pc_covered_str(self) -> str:\n        \"\"\"Returns the percent covered, as a string, without a percent sign.\n\n        Note that \"0\" is only returned when the value is truly zero, and \"100\"\n        is only returned when the value is truly 100.  Rounding can never\n        result in either \"0\" or \"100\".\n\n        \"\"\"\n        return display_covered(self.pc_covered, self.precision)\n\n    @property\n    def ratio_covered(self) -> tuple[int, int]:\n        \"\"\"Return a numerator and denominator for the coverage ratio.\"\"\"\n        numerator = self.n_executed + self.n_executed_branches\n        denominator = self.n_statements + self.n_branches\n        return numerator, denominator\n\n    def __add__(self, other: Numbers) -> Numbers:\n        return Numbers(\n            self.precision,\n            self.n_files + other.n_files,\n            self.n_statements + other.n_statements,\n            self.n_excluded + other.n_excluded,\n            self.n_missing + other.n_missing,\n            self.n_branches + other.n_branches,\n            self.n_partial_branches + other.n_partial_branches,\n            self.n_missing_branches + other.n_missing_branches,\n        )\n\n    def __radd__(self, other: int) -> Numbers:\n        # Implementing 0+Numbers allows us to sum() a list of Numbers.\n        assert other == 0   # we only ever call it this way.\n        return self\n\n\ndef display_covered(pc: float, precision: int) -> str:\n    \"\"\"Return a displayable total percentage, as a string.\n\n    Note that \"0\" is only returned when the value is truly zero, and \"100\"\n    is only returned when the value is truly 100.  Rounding can never\n    result in either \"0\" or \"100\".\n\n    \"\"\"\n    near0 = 1.0 / 10 ** precision\n    if 0 < pc < near0:\n        pc = near0\n    elif (100.0 - near0) < pc < 100:\n        pc = 100.0 - near0\n    else:\n        pc = round(pc, precision)\n    return \"%.*f\" % (precision, pc)\n\n\ndef _line_ranges(\n    statements: Iterable[TLineNo],\n    lines: Iterable[TLineNo],\n) -> list[tuple[TLineNo, TLineNo]]:\n    \"\"\"Produce a list of ranges for `format_lines`.\"\"\"\n    statements = sorted(statements)\n    lines = sorted(lines)\n\n    pairs = []\n    start = None\n    lidx = 0\n    for stmt in statements:\n        if lidx >= len(lines):\n            break\n        if stmt == lines[lidx]:\n            lidx += 1\n            if not start:\n                start = stmt\n            end = stmt\n        elif start:\n            pairs.append((start, end))\n            start = None\n    if start:\n        pairs.append((start, end))\n    return pairs\n\n\ndef format_lines(\n    statements: Iterable[TLineNo],\n    lines: Iterable[TLineNo],\n    arcs: Iterable[tuple[TLineNo, list[TLineNo]]] | None = None,\n) -> str:\n    \"\"\"Nicely format a list of line numbers.\n\n    Format a list of line numbers for printing by coalescing groups of lines as\n    long as the lines represent consecutive statements.  This will coalesce\n    even if there are gaps between statements.\n\n    For example, if `statements` is [1,2,3,4,5,10,11,12,13,14] and\n    `lines` is [1,2,5,10,11,13,14] then the result will be \"1-2, 5-11, 13-14\".\n\n    Both `lines` and `statements` can be any iterable. All of the elements of\n    `lines` must be in `statements`, and all of the values must be positive\n    integers.\n\n    If `arcs` is provided, they are (start,[end,end,end]) pairs that will be\n    included in the output as long as start isn't in `lines`.\n\n    \"\"\"\n    line_items = [(pair[0], nice_pair(pair)) for pair in _line_ranges(statements, lines)]\n    if arcs is not None:\n        line_exits = sorted(arcs)\n        for line, exits in line_exits:\n            for ex in sorted(exits):\n                if line not in lines and ex not in lines:\n                    dest = (ex if ex > 0 else \"exit\")\n                    line_items.append((line, f\"{line}->{dest}\"))\n\n    ret = \", \".join(t[-1] for t in sorted(line_items))\n    return ret\n\n\ndef should_fail_under(total: float, fail_under: float, precision: int) -> bool:\n    \"\"\"Determine if a total should fail due to fail-under.\n\n    `total` is a float, the coverage measurement total. `fail_under` is the\n    fail_under setting to compare with. `precision` is the number of digits\n    to consider after the decimal point.\n\n    Returns True if the total should fail.\n\n    \"\"\"\n    # We can never achieve higher than 100% coverage, or less than zero.\n    if not (0 <= fail_under <= 100.0):\n        msg = f\"fail_under={fail_under} is invalid. Must be between 0 and 100.\"\n        raise ConfigError(msg)\n\n    # Special case for fail_under=100, it must really be 100.\n    if fail_under == 100.0 and total != 100.0:\n        return True\n\n    return round(total, precision) < fail_under\n", "coverage/python.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Python source expertise for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport os.path\nimport types\nimport zipimport\n\nfrom typing import Iterable, TYPE_CHECKING\n\nfrom coverage import env\nfrom coverage.exceptions import CoverageException, NoSource\nfrom coverage.files import canonical_filename, relative_filename, zip_location\nfrom coverage.misc import isolate_module, join_regex\nfrom coverage.parser import PythonParser\nfrom coverage.phystokens import source_token_lines, source_encoding\nfrom coverage.plugin import CodeRegion, FileReporter\nfrom coverage.regions import code_regions\nfrom coverage.types import TArc, TLineNo, TMorf, TSourceTokenLines\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n\nos = isolate_module(os)\n\n\ndef read_python_source(filename: str) -> bytes:\n    \"\"\"Read the Python source text from `filename`.\n\n    Returns bytes.\n\n    \"\"\"\n    with open(filename, \"rb\") as f:\n        source = f.read()\n\n    return source.replace(b\"\\r\\n\", b\"\\n\").replace(b\"\\r\", b\"\\n\")\n\n\ndef get_python_source(filename: str) -> str:\n    \"\"\"Return the source code, as unicode.\"\"\"\n    base, ext = os.path.splitext(filename)\n    if ext == \".py\" and env.WINDOWS:\n        exts = [\".py\", \".pyw\"]\n    else:\n        exts = [ext]\n\n    source_bytes: bytes | None\n    for ext in exts:\n        try_filename = base + ext\n        if os.path.exists(try_filename):\n            # A regular text file: open it.\n            source_bytes = read_python_source(try_filename)\n            break\n\n        # Maybe it's in a zip file?\n        source_bytes = get_zip_bytes(try_filename)\n        if source_bytes is not None:\n            break\n    else:\n        # Couldn't find source.\n        raise NoSource(f\"No source for code: '{filename}'.\")\n\n    # Replace \\f because of http://bugs.python.org/issue19035\n    source_bytes = source_bytes.replace(b\"\\f\", b\" \")\n    source = source_bytes.decode(source_encoding(source_bytes), \"replace\")\n\n    # Python code should always end with a line with a newline.\n    if source and source[-1] != \"\\n\":\n        source += \"\\n\"\n\n    return source\n\n\ndef get_zip_bytes(filename: str) -> bytes | None:\n    \"\"\"Get data from `filename` if it is a zip file path.\n\n    Returns the bytestring data read from the zip file, or None if no zip file\n    could be found or `filename` isn't in it.  The data returned will be\n    an empty string if the file is empty.\n\n    \"\"\"\n    zipfile_inner = zip_location(filename)\n    if zipfile_inner is not None:\n        zipfile, inner = zipfile_inner\n        try:\n            zi = zipimport.zipimporter(zipfile)\n        except zipimport.ZipImportError:\n            return None\n        try:\n            data = zi.get_data(inner)\n        except OSError:\n            return None\n        return data\n    return None\n\n\ndef source_for_file(filename: str) -> str:\n    \"\"\"Return the source filename for `filename`.\n\n    Given a file name being traced, return the best guess as to the source\n    file to attribute it to.\n\n    \"\"\"\n    if filename.endswith(\".py\"):\n        # .py files are themselves source files.\n        return filename\n\n    elif filename.endswith((\".pyc\", \".pyo\")):\n        # Bytecode files probably have source files near them.\n        py_filename = filename[:-1]\n        if os.path.exists(py_filename):\n            # Found a .py file, use that.\n            return py_filename\n        if env.WINDOWS:\n            # On Windows, it could be a .pyw file.\n            pyw_filename = py_filename + \"w\"\n            if os.path.exists(pyw_filename):\n                return pyw_filename\n        # Didn't find source, but it's probably the .py file we want.\n        return py_filename\n\n    # No idea, just use the file name as-is.\n    return filename\n\n\ndef source_for_morf(morf: TMorf) -> str:\n    \"\"\"Get the source filename for the module-or-file `morf`.\"\"\"\n    if hasattr(morf, \"__file__\") and morf.__file__:\n        filename = morf.__file__\n    elif isinstance(morf, types.ModuleType):\n        # A module should have had .__file__, otherwise we can't use it.\n        # This could be a PEP-420 namespace package.\n        raise CoverageException(f\"Module {morf} has no file\")\n    else:\n        filename = morf\n\n    filename = source_for_file(filename)\n    return filename\n\n\nclass PythonFileReporter(FileReporter):\n    \"\"\"Report support for a Python file.\"\"\"\n\n    def __init__(self, morf: TMorf, coverage: Coverage | None = None) -> None:\n        self.coverage = coverage\n\n        filename = source_for_morf(morf)\n\n        fname = filename\n        canonicalize = True\n        if self.coverage is not None:\n            if self.coverage.config.relative_files:\n                canonicalize = False\n        if canonicalize:\n            fname = canonical_filename(filename)\n        super().__init__(fname)\n\n        if hasattr(morf, \"__name__\"):\n            name = morf.__name__.replace(\".\", os.sep)\n            if os.path.basename(filename).startswith(\"__init__.\"):\n                name += os.sep + \"__init__\"\n            name += \".py\"\n        else:\n            name = relative_filename(filename)\n        self.relname = name\n\n        self._source: str | None = None\n        self._parser: PythonParser | None = None\n        self._excluded = None\n\n    def __repr__(self) -> str:\n        return f\"<PythonFileReporter {self.filename!r}>\"\n\n    def relative_filename(self) -> str:\n        return self.relname\n\n    @property\n    def parser(self) -> PythonParser:\n        \"\"\"Lazily create a :class:`PythonParser`.\"\"\"\n        assert self.coverage is not None\n        if self._parser is None:\n            self._parser = PythonParser(\n                filename=self.filename,\n                exclude=self.coverage._exclude_regex(\"exclude\"),\n            )\n            self._parser.parse_source()\n        return self._parser\n\n    def lines(self) -> set[TLineNo]:\n        \"\"\"Return the line numbers of statements in the file.\"\"\"\n        return self.parser.statements\n\n    def excluded_lines(self) -> set[TLineNo]:\n        \"\"\"Return the line numbers of statements in the file.\"\"\"\n        return self.parser.excluded\n\n    def translate_lines(self, lines: Iterable[TLineNo]) -> set[TLineNo]:\n        return self.parser.translate_lines(lines)\n\n    def translate_arcs(self, arcs: Iterable[TArc]) -> set[TArc]:\n        return self.parser.translate_arcs(arcs)\n\n    def no_branch_lines(self) -> set[TLineNo]:\n        assert self.coverage is not None\n        no_branch = self.parser.lines_matching(\n            join_regex(\n                self.coverage.config.partial_list\n                + self.coverage.config.partial_always_list\n            )\n        )\n        return no_branch\n\n    def arcs(self) -> set[TArc]:\n        return self.parser.arcs()\n\n    def exit_counts(self) -> dict[TLineNo, int]:\n        return self.parser.exit_counts()\n\n    def missing_arc_description(\n        self,\n        start: TLineNo,\n        end: TLineNo,\n        executed_arcs: Iterable[TArc] | None = None,\n    ) -> str:\n        return self.parser.missing_arc_description(start, end, executed_arcs)\n\n    def source(self) -> str:\n        if self._source is None:\n            self._source = get_python_source(self.filename)\n        return self._source\n\n    def should_be_python(self) -> bool:\n        \"\"\"Does it seem like this file should contain Python?\n\n        This is used to decide if a file reported as part of the execution of\n        a program was really likely to have contained Python in the first\n        place.\n\n        \"\"\"\n        # Get the file extension.\n        _, ext = os.path.splitext(self.filename)\n\n        # Anything named *.py* should be Python.\n        if ext.startswith(\".py\"):\n            return True\n        # A file with no extension should be Python.\n        if not ext:\n            return True\n        # Everything else is probably not Python.\n        return False\n\n    def source_token_lines(self) -> TSourceTokenLines:\n        return source_token_lines(self.source())\n\n    def code_regions(self) -> Iterable[CodeRegion]:\n        return code_regions(self.source())\n\n    def code_region_kinds(self) -> Iterable[tuple[str, str]]:\n        return [\n            (\"function\", \"functions\"),\n            (\"class\", \"classes\"),\n        ]\n", "coverage/files.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"File wrangling.\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport ntpath\nimport os\nimport os.path\nimport posixpath\nimport re\nimport sys\n\nfrom typing import Callable, Iterable\n\nfrom coverage import env\nfrom coverage.exceptions import ConfigError\nfrom coverage.misc import human_sorted, isolate_module, join_regex\n\n\nos = isolate_module(os)\n\n\nRELATIVE_DIR: str = \"\"\nCANONICAL_FILENAME_CACHE: dict[str, str] = {}\n\ndef set_relative_directory() -> None:\n    \"\"\"Set the directory that `relative_filename` will be relative to.\"\"\"\n    global RELATIVE_DIR, CANONICAL_FILENAME_CACHE\n\n    # The current directory\n    abs_curdir = abs_file(os.curdir)\n    if not abs_curdir.endswith(os.sep):\n        # Suffix with separator only if not at the system root\n        abs_curdir = abs_curdir + os.sep\n\n    # The absolute path to our current directory.\n    RELATIVE_DIR = os.path.normcase(abs_curdir)\n\n    # Cache of results of calling the canonical_filename() method, to\n    # avoid duplicating work.\n    CANONICAL_FILENAME_CACHE = {}\n\n\ndef relative_directory() -> str:\n    \"\"\"Return the directory that `relative_filename` is relative to.\"\"\"\n    return RELATIVE_DIR\n\n\ndef relative_filename(filename: str) -> str:\n    \"\"\"Return the relative form of `filename`.\n\n    The file name will be relative to the current directory when the\n    `set_relative_directory` was called.\n\n    \"\"\"\n    fnorm = os.path.normcase(filename)\n    if fnorm.startswith(RELATIVE_DIR):\n        filename = filename[len(RELATIVE_DIR):]\n    return filename\n\n\ndef canonical_filename(filename: str) -> str:\n    \"\"\"Return a canonical file name for `filename`.\n\n    An absolute path with no redundant components and normalized case.\n\n    \"\"\"\n    if filename not in CANONICAL_FILENAME_CACHE:\n        cf = filename\n        if not os.path.isabs(filename):\n            for path in [os.curdir] + sys.path:\n                if path is None:\n                    continue # type: ignore[unreachable]\n                f = os.path.join(path, filename)\n                try:\n                    exists = os.path.exists(f)\n                except UnicodeError:\n                    exists = False\n                if exists:\n                    cf = f\n                    break\n        cf = abs_file(cf)\n        CANONICAL_FILENAME_CACHE[filename] = cf\n    return CANONICAL_FILENAME_CACHE[filename]\n\n\ndef flat_rootname(filename: str) -> str:\n    \"\"\"A base for a flat file name to correspond to this file.\n\n    Useful for writing files about the code where you want all the files in\n    the same directory, but need to differentiate same-named files from\n    different directories.\n\n    For example, the file a/b/c.py will return 'z_86bbcbe134d28fd2_c_py'\n\n    \"\"\"\n    dirname, basename = ntpath.split(filename)\n    if dirname:\n        fp = hashlib.new(\"sha3_256\", dirname.encode(\"UTF-8\")).hexdigest()[:16]\n        prefix = f\"z_{fp}_\"\n    else:\n        prefix = \"\"\n    return prefix + basename.replace(\".\", \"_\")\n\n\nif env.WINDOWS:\n\n    _ACTUAL_PATH_CACHE: dict[str, str] = {}\n    _ACTUAL_PATH_LIST_CACHE: dict[str, list[str]] = {}\n\n    def actual_path(path: str) -> str:\n        \"\"\"Get the actual path of `path`, including the correct case.\"\"\"\n        if path in _ACTUAL_PATH_CACHE:\n            return _ACTUAL_PATH_CACHE[path]\n\n        head, tail = os.path.split(path)\n        if not tail:\n            # This means head is the drive spec: normalize it.\n            actpath = head.upper()\n        elif not head:\n            actpath = tail\n        else:\n            head = actual_path(head)\n            if head in _ACTUAL_PATH_LIST_CACHE:\n                files = _ACTUAL_PATH_LIST_CACHE[head]\n            else:\n                try:\n                    files = os.listdir(head)\n                except Exception:\n                    # This will raise OSError, or this bizarre TypeError:\n                    # https://bugs.python.org/issue1776160\n                    files = []\n                _ACTUAL_PATH_LIST_CACHE[head] = files\n            normtail = os.path.normcase(tail)\n            for f in files:\n                if os.path.normcase(f) == normtail:\n                    tail = f\n                    break\n            actpath = os.path.join(head, tail)\n        _ACTUAL_PATH_CACHE[path] = actpath\n        return actpath\n\nelse:\n    def actual_path(path: str) -> str:\n        \"\"\"The actual path for non-Windows platforms.\"\"\"\n        return path\n\n\ndef abs_file(path: str) -> str:\n    \"\"\"Return the absolute normalized form of `path`.\"\"\"\n    return actual_path(os.path.abspath(os.path.realpath(path)))\n\n\ndef zip_location(filename: str) -> tuple[str, str] | None:\n    \"\"\"Split a filename into a zipfile / inner name pair.\n\n    Only return a pair if the zipfile exists.  No check is made if the inner\n    name is in the zipfile.\n\n    \"\"\"\n    for ext in [\".zip\", \".whl\", \".egg\", \".pex\"]:\n        zipbase, extension, inner = filename.partition(ext + sep(filename))\n        if extension:\n            zipfile = zipbase + ext\n            if os.path.exists(zipfile):\n                return zipfile, inner\n    return None\n\n\ndef source_exists(path: str) -> bool:\n    \"\"\"Determine if a source file path exists.\"\"\"\n    if os.path.exists(path):\n        return True\n\n    if zip_location(path):\n        # If zip_location returns anything, then it's a zipfile that\n        # exists. That's good enough for us.\n        return True\n\n    return False\n\n\ndef python_reported_file(filename: str) -> str:\n    \"\"\"Return the string as Python would describe this file name.\"\"\"\n    if env.PYBEHAVIOR.report_absolute_files:\n        filename = os.path.abspath(filename)\n    return filename\n\n\ndef isabs_anywhere(filename: str) -> bool:\n    \"\"\"Is `filename` an absolute path on any OS?\"\"\"\n    return ntpath.isabs(filename) or posixpath.isabs(filename)\n\n\ndef prep_patterns(patterns: Iterable[str]) -> list[str]:\n    \"\"\"Prepare the file patterns for use in a `GlobMatcher`.\n\n    If a pattern starts with a wildcard, it is used as a pattern\n    as-is.  If it does not start with a wildcard, then it is made\n    absolute with the current directory.\n\n    If `patterns` is None, an empty list is returned.\n\n    \"\"\"\n    prepped = []\n    for p in patterns or []:\n        prepped.append(p)\n        if not p.startswith((\"*\", \"?\")):\n            prepped.append(abs_file(p))\n    return prepped\n\n\nclass TreeMatcher:\n    \"\"\"A matcher for files in a tree.\n\n    Construct with a list of paths, either files or directories. Paths match\n    with the `match` method if they are one of the files, or if they are\n    somewhere in a subtree rooted at one of the directories.\n\n    \"\"\"\n    def __init__(self, paths: Iterable[str], name: str = \"unknown\") -> None:\n        self.original_paths: list[str] = human_sorted(paths)\n        #self.paths = list(map(os.path.normcase, paths))\n        self.paths = [os.path.normcase(p) for p in paths]\n        self.name = name\n\n    def __repr__(self) -> str:\n        return f\"<TreeMatcher {self.name} {self.original_paths!r}>\"\n\n    def info(self) -> list[str]:\n        \"\"\"A list of strings for displaying when dumping state.\"\"\"\n        return self.original_paths\n\n    def match(self, fpath: str) -> bool:\n        \"\"\"Does `fpath` indicate a file in one of our trees?\"\"\"\n        fpath = os.path.normcase(fpath)\n        for p in self.paths:\n            if fpath.startswith(p):\n                if fpath == p:\n                    # This is the same file!\n                    return True\n                if fpath[len(p)] == os.sep:\n                    # This is a file in the directory\n                    return True\n        return False\n\n\nclass ModuleMatcher:\n    \"\"\"A matcher for modules in a tree.\"\"\"\n    def __init__(self, module_names: Iterable[str], name:str = \"unknown\") -> None:\n        self.modules = list(module_names)\n        self.name = name\n\n    def __repr__(self) -> str:\n        return f\"<ModuleMatcher {self.name} {self.modules!r}>\"\n\n    def info(self) -> list[str]:\n        \"\"\"A list of strings for displaying when dumping state.\"\"\"\n        return self.modules\n\n    def match(self, module_name: str) -> bool:\n        \"\"\"Does `module_name` indicate a module in one of our packages?\"\"\"\n        if not module_name:\n            return False\n\n        for m in self.modules:\n            if module_name.startswith(m):\n                if module_name == m:\n                    return True\n                if module_name[len(m)] == \".\":\n                    # This is a module in the package\n                    return True\n\n        return False\n\n\nclass GlobMatcher:\n    \"\"\"A matcher for files by file name pattern.\"\"\"\n    def __init__(self, pats: Iterable[str], name: str = \"unknown\") -> None:\n        self.pats = list(pats)\n        self.re = globs_to_regex(self.pats, case_insensitive=env.WINDOWS)\n        self.name = name\n\n    def __repr__(self) -> str:\n        return f\"<GlobMatcher {self.name} {self.pats!r}>\"\n\n    def info(self) -> list[str]:\n        \"\"\"A list of strings for displaying when dumping state.\"\"\"\n        return self.pats\n\n    def match(self, fpath: str) -> bool:\n        \"\"\"Does `fpath` match one of our file name patterns?\"\"\"\n        return self.re.match(fpath) is not None\n\n\ndef sep(s: str) -> str:\n    \"\"\"Find the path separator used in this string, or os.sep if none.\"\"\"\n    if sep_match := re.search(r\"[\\\\/]\", s):\n        the_sep = sep_match[0]\n    else:\n        the_sep = os.sep\n    return the_sep\n\n\n# Tokenizer for _glob_to_regex.\n# None as a sub means disallowed.\nG2RX_TOKENS = [(re.compile(rx), sub) for rx, sub in [\n    (r\"\\*\\*\\*+\", None),             # Can't have ***\n    (r\"[^/]+\\*\\*+\", None),          # Can't have x**\n    (r\"\\*\\*+[^/]+\", None),          # Can't have **x\n    (r\"\\*\\*/\\*\\*\", None),           # Can't have **/**\n    (r\"^\\*+/\", r\"(.*[/\\\\\\\\])?\"),    # ^*/ matches any prefix-slash, or nothing.\n    (r\"/\\*+$\", r\"[/\\\\\\\\].*\"),       # /*$ matches any slash-suffix.\n    (r\"\\*\\*/\", r\"(.*[/\\\\\\\\])?\"),    # **/ matches any subdirs, including none\n    (r\"/\", r\"[/\\\\\\\\]\"),             # / matches either slash or backslash\n    (r\"\\*\", r\"[^/\\\\\\\\]*\"),          # * matches any number of non slash-likes\n    (r\"\\?\", r\"[^/\\\\\\\\]\"),           # ? matches one non slash-like\n    (r\"\\[.*?\\]\", r\"\\g<0>\"),         # [a-f] matches [a-f]\n    (r\"[a-zA-Z0-9_-]+\", r\"\\g<0>\"),  # word chars match themselves\n    (r\"[\\[\\]]\", None),              # Can't have single square brackets\n    (r\".\", r\"\\\\\\g<0>\"),             # Anything else is escaped to be safe\n]]\n\ndef _glob_to_regex(pattern: str) -> str:\n    \"\"\"Convert a file-path glob pattern into a regex.\"\"\"\n    # Turn all backslashes into slashes to simplify the tokenizer.\n    pattern = pattern.replace(\"\\\\\", \"/\")\n    if \"/\" not in pattern:\n        pattern = \"**/\" + pattern\n    path_rx = []\n    pos = 0\n    while pos < len(pattern):\n        for rx, sub in G2RX_TOKENS:                     # pragma: always breaks\n            if m := rx.match(pattern, pos=pos):\n                if sub is None:\n                    raise ConfigError(f\"File pattern can't include {m[0]!r}\")\n                path_rx.append(m.expand(sub))\n                pos = m.end()\n                break\n    return \"\".join(path_rx)\n\n\ndef globs_to_regex(\n    patterns: Iterable[str],\n    case_insensitive: bool = False,\n    partial: bool = False,\n) -> re.Pattern[str]:\n    \"\"\"Convert glob patterns to a compiled regex that matches any of them.\n\n    Slashes are always converted to match either slash or backslash, for\n    Windows support, even when running elsewhere.\n\n    If the pattern has no slash or backslash, then it is interpreted as\n    matching a file name anywhere it appears in the tree.  Otherwise, the glob\n    pattern must match the whole file path.\n\n    If `partial` is true, then the pattern will match if the target string\n    starts with the pattern. Otherwise, it must match the entire string.\n\n    Returns: a compiled regex object.  Use the .match method to compare target\n    strings.\n\n    \"\"\"\n    flags = 0\n    if case_insensitive:\n        flags |= re.IGNORECASE\n    rx = join_regex(map(_glob_to_regex, patterns))\n    if not partial:\n        rx = fr\"(?:{rx})\\Z\"\n    compiled = re.compile(rx, flags=flags)\n    return compiled\n\n\nclass PathAliases:\n    \"\"\"A collection of aliases for paths.\n\n    When combining data files from remote machines, often the paths to source\n    code are different, for example, due to OS differences, or because of\n    serialized checkouts on continuous integration machines.\n\n    A `PathAliases` object tracks a list of pattern/result pairs, and can\n    map a path through those aliases to produce a unified path.\n\n    \"\"\"\n    def __init__(\n        self,\n        debugfn: Callable[[str], None] | None = None,\n        relative: bool = False,\n    ) -> None:\n        # A list of (original_pattern, regex, result)\n        self.aliases: list[tuple[str, re.Pattern[str], str]] = []\n        self.debugfn = debugfn or (lambda msg: 0)\n        self.relative = relative\n        self.pprinted = False\n\n    def pprint(self) -> None:\n        \"\"\"Dump the important parts of the PathAliases, for debugging.\"\"\"\n        self.debugfn(f\"Aliases (relative={self.relative}):\")\n        for original_pattern, regex, result in self.aliases:\n            self.debugfn(f\" Rule: {original_pattern!r} -> {result!r} using regex {regex.pattern!r}\")\n\n    def add(self, pattern: str, result: str) -> None:\n        \"\"\"Add the `pattern`/`result` pair to the list of aliases.\n\n        `pattern` is an `glob`-style pattern.  `result` is a simple\n        string.  When mapping paths, if a path starts with a match against\n        `pattern`, then that match is replaced with `result`.  This models\n        isomorphic source trees being rooted at different places on two\n        different machines.\n\n        `pattern` can't end with a wildcard component, since that would\n        match an entire tree, and not just its root.\n\n        \"\"\"\n        original_pattern = pattern\n        pattern_sep = sep(pattern)\n\n        if len(pattern) > 1:\n            pattern = pattern.rstrip(r\"\\/\")\n\n        # The pattern can't end with a wildcard component.\n        if pattern.endswith(\"*\"):\n            raise ConfigError(\"Pattern must not end with wildcards.\")\n\n        # The pattern is meant to match a file path.  Let's make it absolute\n        # unless it already is, or is meant to match any prefix.\n        if not self.relative:\n            if not pattern.startswith(\"*\") and not isabs_anywhere(pattern + pattern_sep):\n                pattern = abs_file(pattern)\n        if not pattern.endswith(pattern_sep):\n            pattern += pattern_sep\n\n        # Make a regex from the pattern.\n        regex = globs_to_regex([pattern], case_insensitive=True, partial=True)\n\n        # Normalize the result: it must end with a path separator.\n        result_sep = sep(result)\n        result = result.rstrip(r\"\\/\") + result_sep\n        self.aliases.append((original_pattern, regex, result))\n\n    def map(self, path: str, exists:Callable[[str], bool] = source_exists) -> str:\n        \"\"\"Map `path` through the aliases.\n\n        `path` is checked against all of the patterns.  The first pattern to\n        match is used to replace the root of the path with the result root.\n        Only one pattern is ever used.  If no patterns match, `path` is\n        returned unchanged.\n\n        The separator style in the result is made to match that of the result\n        in the alias.\n\n        `exists` is a function to determine if the resulting path actually\n        exists.\n\n        Returns the mapped path.  If a mapping has happened, this is a\n        canonical path.  If no mapping has happened, it is the original value\n        of `path` unchanged.\n\n        \"\"\"\n        if not self.pprinted:\n            self.pprint()\n            self.pprinted = True\n\n        for original_pattern, regex, result in self.aliases:\n            if m := regex.match(path):\n                new = path.replace(m[0], result)\n                new = new.replace(sep(path), sep(result))\n                if not self.relative:\n                    new = canonical_filename(new)\n                dot_start = result.startswith((\"./\", \".\\\\\")) and len(result) > 2\n                if new.startswith((\"./\", \".\\\\\")) and not dot_start:\n                    new = new[2:]\n                if not exists(new):\n                    self.debugfn(\n                        f\"Rule {original_pattern!r} changed {path!r} to {new!r} \" +\n                        \"which doesn't exist, continuing\",\n                    )\n                    continue\n                self.debugfn(\n                    f\"Matched path {path!r} to rule {original_pattern!r} -> {result!r}, \" +\n                    f\"producing {new!r}\",\n                )\n                return new\n\n        # If we get here, no pattern matched.\n\n        if self.relative:\n            path = relative_filename(path)\n\n        if self.relative and not isabs_anywhere(path):\n            # Auto-generate a pattern to implicitly match relative files\n            parts = re.split(r\"[/\\\\]\", path)\n            if len(parts) > 1:\n                dir1 = parts[0]\n                pattern = f\"*/{dir1}\"\n                regex_pat = fr\"^(.*[\\\\/])?{re.escape(dir1)}[\\\\/]\"\n                result = f\"{dir1}{os.sep}\"\n                # Only add a new pattern if we don't already have this pattern.\n                if not any(p == pattern for p, _, _ in self.aliases):\n                    self.debugfn(\n                        f\"Generating rule: {pattern!r} -> {result!r} using regex {regex_pat!r}\",\n                    )\n                    self.aliases.append((pattern, re.compile(regex_pat), result))\n                    return self.map(path, exists=exists)\n\n        self.debugfn(f\"No rules match, path {path!r} is unchanged\")\n        return path\n\n\ndef find_python_files(dirname: str, include_namespace_packages: bool) -> Iterable[str]:\n    \"\"\"Yield all of the importable Python files in `dirname`, recursively.\n\n    To be importable, the files have to be in a directory with a __init__.py,\n    except for `dirname` itself, which isn't required to have one.  The\n    assumption is that `dirname` was specified directly, so the user knows\n    best, but sub-directories are checked for a __init__.py to be sure we only\n    find the importable files.\n\n    If `include_namespace_packages` is True, then the check for __init__.py\n    files is skipped.\n\n    Files with strange characters are skipped, since they couldn't have been\n    imported, and are probably editor side-files.\n\n    \"\"\"\n    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dirname)):\n        if not include_namespace_packages:\n            if i > 0 and \"__init__.py\" not in filenames:\n                # If a directory doesn't have __init__.py, then it isn't\n                # importable and neither are its files\n                del dirnames[:]\n                continue\n        for filename in filenames:\n            # We're only interested in files that look like reasonable Python\n            # files: Must end with .py or .pyw, and must not have certain funny\n            # characters that probably mean they are editor junk.\n            if re.match(r\"^[^.#~!$@%^&*()+=,]+\\.pyw?$\", filename):\n                yield os.path.join(dirpath, filename)\n\n\n# Globally set the relative directory.\nset_relative_directory()\n", "coverage/execfile.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Execute files of Python code.\"\"\"\n\nfrom __future__ import annotations\n\nimport importlib.machinery\nimport importlib.util\nimport inspect\nimport marshal\nimport os\nimport struct\nimport sys\n\nfrom importlib.machinery import ModuleSpec\nfrom types import CodeType, ModuleType\nfrom typing import Any\n\nfrom coverage import env\nfrom coverage.exceptions import CoverageException, _ExceptionDuringRun, NoCode, NoSource\nfrom coverage.files import canonical_filename, python_reported_file\nfrom coverage.misc import isolate_module\nfrom coverage.python import get_python_source\n\nos = isolate_module(os)\n\n\nPYC_MAGIC_NUMBER = importlib.util.MAGIC_NUMBER\n\nclass DummyLoader:\n    \"\"\"A shim for the pep302 __loader__, emulating pkgutil.ImpLoader.\n\n    Currently only implements the .fullname attribute\n    \"\"\"\n    def __init__(self, fullname: str, *_args: Any) -> None:\n        self.fullname = fullname\n\n\ndef find_module(\n    modulename: str,\n) -> tuple[str | None, str, ModuleSpec]:\n    \"\"\"Find the module named `modulename`.\n\n    Returns the file path of the module, the name of the enclosing\n    package, and the spec.\n    \"\"\"\n    try:\n        spec = importlib.util.find_spec(modulename)\n    except ImportError as err:\n        raise NoSource(str(err)) from err\n    if not spec:\n        raise NoSource(f\"No module named {modulename!r}\")\n    pathname = spec.origin\n    packagename = spec.name\n    if spec.submodule_search_locations:\n        mod_main = modulename + \".__main__\"\n        spec = importlib.util.find_spec(mod_main)\n        if not spec:\n            raise NoSource(\n                f\"No module named {mod_main}; \" +\n                f\"{modulename!r} is a package and cannot be directly executed\",\n            )\n        pathname = spec.origin\n        packagename = spec.name\n    packagename = packagename.rpartition(\".\")[0]\n    return pathname, packagename, spec\n\n\nclass PyRunner:\n    \"\"\"Multi-stage execution of Python code.\n\n    This is meant to emulate real Python execution as closely as possible.\n\n    \"\"\"\n    def __init__(self, args: list[str], as_module: bool = False) -> None:\n        self.args = args\n        self.as_module = as_module\n\n        self.arg0 = args[0]\n        self.package: str | None = None\n        self.modulename: str | None = None\n        self.pathname: str | None = None\n        self.loader: DummyLoader | None = None\n        self.spec: ModuleSpec | None = None\n\n    def prepare(self) -> None:\n        \"\"\"Set sys.path properly.\n\n        This needs to happen before any importing, and without importing anything.\n        \"\"\"\n        path0: str | None\n        if self.as_module:\n            path0 = os.getcwd()\n        elif os.path.isdir(self.arg0):\n            # Running a directory means running the __main__.py file in that\n            # directory.\n            path0 = self.arg0\n        else:\n            path0 = os.path.abspath(os.path.dirname(self.arg0))\n\n        if os.path.isdir(sys.path[0]):\n            # sys.path fakery.  If we are being run as a command, then sys.path[0]\n            # is the directory of the \"coverage\" script.  If this is so, replace\n            # sys.path[0] with the directory of the file we're running, or the\n            # current directory when running modules.  If it isn't so, then we\n            # don't know what's going on, and just leave it alone.\n            top_file = inspect.stack()[-1][0].f_code.co_filename\n            sys_path_0_abs = os.path.abspath(sys.path[0])\n            top_file_dir_abs = os.path.abspath(os.path.dirname(top_file))\n            sys_path_0_abs = canonical_filename(sys_path_0_abs)\n            top_file_dir_abs = canonical_filename(top_file_dir_abs)\n            if sys_path_0_abs != top_file_dir_abs:\n                path0 = None\n\n        else:\n            # sys.path[0] is a file. Is the next entry the directory containing\n            # that file?\n            if sys.path[1] == os.path.dirname(sys.path[0]):\n                # Can it be right to always remove that?\n                del sys.path[1]\n\n        if path0 is not None:\n            sys.path[0] = python_reported_file(path0)\n\n    def _prepare2(self) -> None:\n        \"\"\"Do more preparation to run Python code.\n\n        Includes finding the module to run and adjusting sys.argv[0].\n        This method is allowed to import code.\n\n        \"\"\"\n        if self.as_module:\n            self.modulename = self.arg0\n            pathname, self.package, self.spec = find_module(self.modulename)\n            if self.spec is not None:\n                self.modulename = self.spec.name\n            self.loader = DummyLoader(self.modulename)\n            assert pathname is not None\n            self.pathname = os.path.abspath(pathname)\n            self.args[0] = self.arg0 = self.pathname\n        elif os.path.isdir(self.arg0):\n            # Running a directory means running the __main__.py file in that\n            # directory.\n            for ext in [\".py\", \".pyc\", \".pyo\"]:\n                try_filename = os.path.join(self.arg0, \"__main__\" + ext)\n                # 3.8.10 changed how files are reported when running a\n                # directory.  But I'm not sure how far this change is going to\n                # spread, so I'll just hard-code it here for now.\n                if env.PYVERSION >= (3, 8, 10):\n                    try_filename = os.path.abspath(try_filename)\n                if os.path.exists(try_filename):\n                    self.arg0 = try_filename\n                    break\n            else:\n                raise NoSource(f\"Can't find '__main__' module in '{self.arg0}'\")\n\n            # Make a spec. I don't know if this is the right way to do it.\n            try_filename = python_reported_file(try_filename)\n            self.spec = importlib.machinery.ModuleSpec(\"__main__\", None, origin=try_filename)\n            self.spec.has_location = True\n            self.package = \"\"\n            self.loader = DummyLoader(\"__main__\")\n        else:\n            self.loader = DummyLoader(\"__main__\")\n\n        self.arg0 = python_reported_file(self.arg0)\n\n    def run(self) -> None:\n        \"\"\"Run the Python code!\"\"\"\n\n        self._prepare2()\n\n        # Create a module to serve as __main__\n        main_mod = ModuleType(\"__main__\")\n\n        from_pyc = self.arg0.endswith((\".pyc\", \".pyo\"))\n        main_mod.__file__ = self.arg0\n        if from_pyc:\n            main_mod.__file__ = main_mod.__file__[:-1]\n        if self.package is not None:\n            main_mod.__package__ = self.package\n        main_mod.__loader__ = self.loader   # type: ignore[assignment]\n        if self.spec is not None:\n            main_mod.__spec__ = self.spec\n\n        main_mod.__builtins__ = sys.modules[\"builtins\"]     # type: ignore[attr-defined]\n\n        sys.modules[\"__main__\"] = main_mod\n\n        # Set sys.argv properly.\n        sys.argv = self.args\n\n        try:\n            # Make a code object somehow.\n            if from_pyc:\n                code = make_code_from_pyc(self.arg0)\n            else:\n                code = make_code_from_py(self.arg0)\n        except CoverageException:\n            raise\n        except Exception as exc:\n            msg = f\"Couldn't run '{self.arg0}' as Python code: {exc.__class__.__name__}: {exc}\"\n            raise CoverageException(msg) from exc\n\n        # Execute the code object.\n        # Return to the original directory in case the test code exits in\n        # a non-existent directory.\n        cwd = os.getcwd()\n        try:\n            exec(code, main_mod.__dict__)\n        except SystemExit:                          # pylint: disable=try-except-raise\n            # The user called sys.exit().  Just pass it along to the upper\n            # layers, where it will be handled.\n            raise\n        except Exception:\n            # Something went wrong while executing the user code.\n            # Get the exc_info, and pack them into an exception that we can\n            # throw up to the outer loop.  We peel one layer off the traceback\n            # so that the coverage.py code doesn't appear in the final printed\n            # traceback.\n            typ, err, tb = sys.exc_info()\n            assert typ is not None\n            assert err is not None\n            assert tb is not None\n\n            # PyPy3 weirdness.  If I don't access __context__, then somehow it\n            # is non-None when the exception is reported at the upper layer,\n            # and a nested exception is shown to the user.  This getattr fixes\n            # it somehow? https://bitbucket.org/pypy/pypy/issue/1903\n            getattr(err, \"__context__\", None)\n\n            # Call the excepthook.\n            try:\n                assert err.__traceback__ is not None\n                err.__traceback__ = err.__traceback__.tb_next\n                sys.excepthook(typ, err, tb.tb_next)\n            except SystemExit:                      # pylint: disable=try-except-raise\n                raise\n            except Exception as exc:\n                # Getting the output right in the case of excepthook\n                # shenanigans is kind of involved.\n                sys.stderr.write(\"Error in sys.excepthook:\\n\")\n                typ2, err2, tb2 = sys.exc_info()\n                assert typ2 is not None\n                assert err2 is not None\n                assert tb2 is not None\n                err2.__suppress_context__ = True\n                assert err2.__traceback__ is not None\n                err2.__traceback__ = err2.__traceback__.tb_next\n                sys.__excepthook__(typ2, err2, tb2.tb_next)\n                sys.stderr.write(\"\\nOriginal exception was:\\n\")\n                raise _ExceptionDuringRun(typ, err, tb.tb_next) from exc\n            else:\n                sys.exit(1)\n        finally:\n            os.chdir(cwd)\n\n\ndef run_python_module(args: list[str]) -> None:\n    \"\"\"Run a Python module, as though with ``python -m name args...``.\n\n    `args` is the argument array to present as sys.argv, including the first\n    element naming the module being executed.\n\n    This is a helper for tests, to encapsulate how to use PyRunner.\n\n    \"\"\"\n    runner = PyRunner(args, as_module=True)\n    runner.prepare()\n    runner.run()\n\n\ndef run_python_file(args: list[str]) -> None:\n    \"\"\"Run a Python file as if it were the main program on the command line.\n\n    `args` is the argument array to present as sys.argv, including the first\n    element naming the file being executed.  `package` is the name of the\n    enclosing package, if any.\n\n    This is a helper for tests, to encapsulate how to use PyRunner.\n\n    \"\"\"\n    runner = PyRunner(args, as_module=False)\n    runner.prepare()\n    runner.run()\n\n\ndef make_code_from_py(filename: str) -> CodeType:\n    \"\"\"Get source from `filename` and make a code object of it.\"\"\"\n    # Open the source file.\n    try:\n        source = get_python_source(filename)\n    except (OSError, NoSource) as exc:\n        raise NoSource(f\"No file to run: '{filename}'\") from exc\n\n    return compile(source, filename, \"exec\", dont_inherit=True)\n\n\ndef make_code_from_pyc(filename: str) -> CodeType:\n    \"\"\"Get a code object from a .pyc file.\"\"\"\n    try:\n        fpyc = open(filename, \"rb\")\n    except OSError as exc:\n        raise NoCode(f\"No file to run: '{filename}'\") from exc\n\n    with fpyc:\n        # First four bytes are a version-specific magic number.  It has to\n        # match or we won't run the file.\n        magic = fpyc.read(4)\n        if magic != PYC_MAGIC_NUMBER:\n            raise NoCode(f\"Bad magic number in .pyc file: {magic!r} != {PYC_MAGIC_NUMBER!r}\")\n\n        flags = struct.unpack(\"<L\", fpyc.read(4))[0]\n        hash_based = flags & 0x01\n        if hash_based:\n            fpyc.read(8)    # Skip the hash.\n        else:\n            # Skip the junk in the header that we don't need.\n            fpyc.read(4)    # Skip the moddate.\n            fpyc.read(4)    # Skip the size.\n\n        # The rest of the file is the code object we want.\n        code = marshal.load(fpyc)\n        assert isinstance(code, CodeType)\n\n    return code\n", "coverage/misc.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Miscellaneous stuff for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport datetime\nimport errno\nimport functools\nimport hashlib\nimport importlib\nimport importlib.util\nimport inspect\nimport os\nimport os.path\nimport re\nimport sys\nimport types\n\nfrom types import ModuleType\nfrom typing import (\n    Any, Iterable, Iterator, Mapping, NoReturn, Sequence, TypeVar,\n)\n\nfrom coverage.exceptions import CoverageException\nfrom coverage.types import TArc\n\n# In 6.0, the exceptions moved from misc.py to exceptions.py.  But a number of\n# other packages were importing the exceptions from misc, so import them here.\n# pylint: disable=unused-wildcard-import\nfrom coverage.exceptions import *   # pylint: disable=wildcard-import\n\nISOLATED_MODULES: dict[ModuleType, ModuleType] = {}\n\n\ndef isolate_module(mod: ModuleType) -> ModuleType:\n    \"\"\"Copy a module so that we are isolated from aggressive mocking.\n\n    If a test suite mocks os.path.exists (for example), and then we need to use\n    it during the test, everything will get tangled up if we use their mock.\n    Making a copy of the module when we import it will isolate coverage.py from\n    those complications.\n    \"\"\"\n    if mod not in ISOLATED_MODULES:\n        new_mod = types.ModuleType(mod.__name__)\n        ISOLATED_MODULES[mod] = new_mod\n        for name in dir(mod):\n            value = getattr(mod, name)\n            if isinstance(value, types.ModuleType):\n                value = isolate_module(value)\n            setattr(new_mod, name, value)\n    return ISOLATED_MODULES[mod]\n\nos = isolate_module(os)\n\n\nclass SysModuleSaver:\n    \"\"\"Saves the contents of sys.modules, and removes new modules later.\"\"\"\n    def __init__(self) -> None:\n        self.old_modules = set(sys.modules)\n\n    def restore(self) -> None:\n        \"\"\"Remove any modules imported since this object started.\"\"\"\n        new_modules = set(sys.modules) - self.old_modules\n        for m in new_modules:\n            del sys.modules[m]\n\n\n@contextlib.contextmanager\ndef sys_modules_saved() -> Iterator[None]:\n    \"\"\"A context manager to remove any modules imported during a block.\"\"\"\n    saver = SysModuleSaver()\n    try:\n        yield\n    finally:\n        saver.restore()\n\n\ndef import_third_party(modname: str) -> tuple[ModuleType, bool]:\n    \"\"\"Import a third-party module we need, but might not be installed.\n\n    This also cleans out the module after the import, so that coverage won't\n    appear to have imported it.  This lets the third party use coverage for\n    their own tests.\n\n    Arguments:\n        modname (str): the name of the module to import.\n\n    Returns:\n        The imported module, and a boolean indicating if the module could be imported.\n\n    If the boolean is False, the module returned is not the one you want: don't use it.\n\n    \"\"\"\n    with sys_modules_saved():\n        try:\n            return importlib.import_module(modname), True\n        except ImportError:\n            return sys, False\n\n\ndef nice_pair(pair: TArc) -> str:\n    \"\"\"Make a nice string representation of a pair of numbers.\n\n    If the numbers are equal, just return the number, otherwise return the pair\n    with a dash between them, indicating the range.\n\n    \"\"\"\n    start, end = pair\n    if start == end:\n        return \"%d\" % start\n    else:\n        return \"%d-%d\" % (start, end)\n\n\ndef bool_or_none(b: Any) -> bool | None:\n    \"\"\"Return bool(b), but preserve None.\"\"\"\n    if b is None:\n        return None\n    else:\n        return bool(b)\n\n\ndef join_regex(regexes: Iterable[str]) -> str:\n    \"\"\"Combine a series of regex strings into one that matches any of them.\"\"\"\n    regexes = list(regexes)\n    if len(regexes) == 1:\n        return regexes[0]\n    else:\n        return \"|\".join(f\"(?:{r})\" for r in regexes)\n\n\ndef file_be_gone(path: str) -> None:\n    \"\"\"Remove a file, and don't get annoyed if it doesn't exist.\"\"\"\n    try:\n        os.remove(path)\n    except OSError as e:\n        if e.errno != errno.ENOENT:\n            raise\n\n\ndef ensure_dir(directory: str) -> None:\n    \"\"\"Make sure the directory exists.\n\n    If `directory` is None or empty, do nothing.\n    \"\"\"\n    if directory:\n        os.makedirs(directory, exist_ok=True)\n\n\ndef ensure_dir_for_file(path: str) -> None:\n    \"\"\"Make sure the directory for the path exists.\"\"\"\n    ensure_dir(os.path.dirname(path))\n\n\nclass Hasher:\n    \"\"\"Hashes Python data for fingerprinting.\"\"\"\n    def __init__(self) -> None:\n        self.hash = hashlib.new(\"sha3_256\")\n\n    def update(self, v: Any) -> None:\n        \"\"\"Add `v` to the hash, recursively if needed.\"\"\"\n        self.hash.update(str(type(v)).encode(\"utf-8\"))\n        if isinstance(v, str):\n            self.hash.update(v.encode(\"utf-8\"))\n        elif isinstance(v, bytes):\n            self.hash.update(v)\n        elif v is None:\n            pass\n        elif isinstance(v, (int, float)):\n            self.hash.update(str(v).encode(\"utf-8\"))\n        elif isinstance(v, (tuple, list)):\n            for e in v:\n                self.update(e)\n        elif isinstance(v, dict):\n            keys = v.keys()\n            for k in sorted(keys):\n                self.update(k)\n                self.update(v[k])\n        else:\n            for k in dir(v):\n                if k.startswith(\"__\"):\n                    continue\n                a = getattr(v, k)\n                if inspect.isroutine(a):\n                    continue\n                self.update(k)\n                self.update(a)\n        self.hash.update(b\".\")\n\n    def hexdigest(self) -> str:\n        \"\"\"Retrieve the hex digest of the hash.\"\"\"\n        return self.hash.hexdigest()[:32]\n\n\ndef _needs_to_implement(that: Any, func_name: str) -> NoReturn:\n    \"\"\"Helper to raise NotImplementedError in interface stubs.\"\"\"\n    if hasattr(that, \"_coverage_plugin_name\"):\n        thing = \"Plugin\"\n        name = that._coverage_plugin_name\n    else:\n        thing = \"Class\"\n        klass = that.__class__\n        name = f\"{klass.__module__}.{klass.__name__}\"\n\n    raise NotImplementedError(\n        f\"{thing} {name!r} needs to implement {func_name}()\",\n    )\n\n\nclass DefaultValue:\n    \"\"\"A sentinel object to use for unusual default-value needs.\n\n    Construct with a string that will be used as the repr, for display in help\n    and Sphinx output.\n\n    \"\"\"\n    def __init__(self, display_as: str) -> None:\n        self.display_as = display_as\n\n    def __repr__(self) -> str:\n        return self.display_as\n\n\ndef substitute_variables(text: str, variables: Mapping[str, str]) -> str:\n    \"\"\"Substitute ``${VAR}`` variables in `text` with their values.\n\n    Variables in the text can take a number of shell-inspired forms::\n\n        $VAR\n        ${VAR}\n        ${VAR?}             strict: an error if VAR isn't defined.\n        ${VAR-missing}      defaulted: \"missing\" if VAR isn't defined.\n        $$                  just a dollar sign.\n\n    `variables` is a dictionary of variable values.\n\n    Returns the resulting text with values substituted.\n\n    \"\"\"\n    dollar_pattern = r\"\"\"(?x)   # Use extended regex syntax\n        \\$                      # A dollar sign,\n        (?:                     # then\n            (?P<dollar>\\$) |        # a dollar sign, or\n            (?P<word1>\\w+) |        # a plain word, or\n            {                       # a {-wrapped\n                (?P<word2>\\w+)          # word,\n                (?:\n                    (?P<strict>\\?) |        # with a strict marker\n                    -(?P<defval>[^}]*)      # or a default value\n                )?                      # maybe.\n            }\n        )\n        \"\"\"\n\n    dollar_groups = (\"dollar\", \"word1\", \"word2\")\n\n    def dollar_replace(match: re.Match[str]) -> str:\n        \"\"\"Called for each $replacement.\"\"\"\n        # Only one of the dollar_groups will have matched, just get its text.\n        word = next(g for g in match.group(*dollar_groups) if g)    # pragma: always breaks\n        if word == \"$\":\n            return \"$\"\n        elif word in variables:\n            return variables[word]\n        elif match[\"strict\"]:\n            msg = f\"Variable {word} is undefined: {text!r}\"\n            raise CoverageException(msg)\n        else:\n            return match[\"defval\"]\n\n    text = re.sub(dollar_pattern, dollar_replace, text)\n    return text\n\n\ndef format_local_datetime(dt: datetime.datetime) -> str:\n    \"\"\"Return a string with local timezone representing the date.\n    \"\"\"\n    return dt.astimezone().strftime(\"%Y-%m-%d %H:%M %z\")\n\n\ndef import_local_file(modname: str, modfile: str | None = None) -> ModuleType:\n    \"\"\"Import a local file as a module.\n\n    Opens a file in the current directory named `modname`.py, imports it\n    as `modname`, and returns the module object.  `modfile` is the file to\n    import if it isn't in the current directory.\n\n    \"\"\"\n    if modfile is None:\n        modfile = modname + \".py\"\n    spec = importlib.util.spec_from_file_location(modname, modfile)\n    assert spec is not None\n    mod = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = mod\n    assert spec.loader is not None\n    spec.loader.exec_module(mod)\n\n    return mod\n\n\n@functools.lru_cache(maxsize=None)\ndef _human_key(s: str) -> tuple[list[str | int], str]:\n    \"\"\"Turn a string into a list of string and number chunks.\n\n    \"z23a\" -> ([\"z\", 23, \"a\"], \"z23a\")\n\n    The original string is appended as a last value to ensure the\n    key is unique enough so that \"x1y\" and \"x001y\" can be distinguished.\n    \"\"\"\n    def tryint(s: str) -> str | int:\n        \"\"\"If `s` is a number, return an int, else `s` unchanged.\"\"\"\n        try:\n            return int(s)\n        except ValueError:\n            return s\n\n    return ([tryint(c) for c in re.split(r\"(\\d+)\", s)], s)\n\ndef human_sorted(strings: Iterable[str]) -> list[str]:\n    \"\"\"Sort the given iterable of strings the way that humans expect.\n\n    Numeric components in the strings are sorted as numbers.\n\n    Returns the sorted list.\n\n    \"\"\"\n    return sorted(strings, key=_human_key)\n\nSortableItem = TypeVar(\"SortableItem\", bound=Sequence[Any])\n\ndef human_sorted_items(\n    items: Iterable[SortableItem],\n    reverse: bool = False,\n) -> list[SortableItem]:\n    \"\"\"Sort (string, ...) items the way humans expect.\n\n    The elements of `items` can be any tuple/list. They'll be sorted by the\n    first element (a string), with ties broken by the remaining elements.\n\n    Returns the sorted list of items.\n    \"\"\"\n    return sorted(items, key=lambda item: (_human_key(item[0]), *item[1:]), reverse=reverse)\n\n\ndef plural(n: int, thing: str = \"\", things: str = \"\") -> str:\n    \"\"\"Pluralize a word.\n\n    If n is 1, return thing.  Otherwise return things, or thing+s.\n    \"\"\"\n    if n == 1:\n        return thing\n    else:\n        return things or (thing + \"s\")\n\n\ndef stdout_link(text: str, url: str) -> str:\n    \"\"\"Format text+url as a clickable link for stdout.\n\n    If attached to a terminal, use escape sequences. Otherwise, just return\n    the text.\n    \"\"\"\n    if hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty():\n        return f\"\\033]8;;{url}\\a{text}\\033]8;;\\a\"\n    else:\n        return text\n", "coverage/types.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nTypes for use throughout coverage.py.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport pathlib\n\nfrom types import FrameType, ModuleType\nfrom typing import (\n    Any, Callable, Dict, Iterable, List, Mapping, Optional, Protocol,\n    Set, Tuple, Type, Union, TYPE_CHECKING,\n)\n\nif TYPE_CHECKING:\n    from coverage.plugin import FileTracer\n\n\nAnyCallable = Callable[..., Any]\n\n## File paths\n\n# For arguments that are file paths:\nif TYPE_CHECKING:\n    FilePath = Union[str, os.PathLike[str]]\nelse:\n    # PathLike < python3.9 doesn't support subscription\n    FilePath = Union[str, os.PathLike]\n# For testing FilePath arguments\nFilePathClasses = [str, pathlib.Path]\nFilePathType = Union[Type[str], Type[pathlib.Path]]\n\n## Python tracing\n\nclass TTraceFn(Protocol):\n    \"\"\"A Python trace function.\"\"\"\n    def __call__(\n        self,\n        frame: FrameType,\n        event: str,\n        arg: Any,\n        lineno: TLineNo | None = None,  # Our own twist, see collector.py\n    ) -> TTraceFn | None:\n        ...\n\n## Coverage.py tracing\n\n# Line numbers are pervasive enough that they deserve their own type.\nTLineNo = int\n\nTArc = Tuple[TLineNo, TLineNo]\n\nclass TFileDisposition(Protocol):\n    \"\"\"A simple value type for recording what to do with a file.\"\"\"\n\n    original_filename: str\n    canonical_filename: str\n    source_filename: str | None\n    trace: bool\n    reason: str\n    file_tracer: FileTracer | None\n    has_dynamic_filename: bool\n\n\n# When collecting data, we use a dictionary with a few possible shapes. The\n# keys are always file names.\n# - If measuring line coverage, the values are sets of line numbers.\n# - If measuring arcs in the Python tracer, the values are sets of arcs (pairs\n#   of line numbers).\n# - If measuring arcs in the C tracer, the values are sets of packed arcs (two\n#   line numbers combined into one integer).\n\nTTraceFileData = Union[Set[TLineNo], Set[TArc], Set[int]]\n\nTTraceData = Dict[str, TTraceFileData]\n\nclass TracerCore(Protocol):\n    \"\"\"Anything that can report on Python execution.\"\"\"\n\n    data: TTraceData\n    trace_arcs: bool\n    should_trace: Callable[[str, FrameType], TFileDisposition]\n    should_trace_cache: Mapping[str, TFileDisposition | None]\n    should_start_context: Callable[[FrameType], str | None] | None\n    switch_context: Callable[[str | None], None] | None\n    lock_data: Callable[[], None]\n    unlock_data: Callable[[], None]\n    warn: TWarnFn\n\n    def __init__(self) -> None:\n        ...\n\n    def start(self) -> TTraceFn | None:\n        \"\"\"Start this tracer, return a trace function if based on sys.settrace.\"\"\"\n\n    def stop(self) -> None:\n        \"\"\"Stop this tracer.\"\"\"\n\n    def activity(self) -> bool:\n        \"\"\"Has there been any activity?\"\"\"\n\n    def reset_activity(self) -> None:\n        \"\"\"Reset the activity() flag.\"\"\"\n\n    def get_stats(self) -> dict[str, int] | None:\n        \"\"\"Return a dictionary of statistics, or None.\"\"\"\n\n\n## Coverage\n\n# Many places use kwargs as Coverage kwargs.\nTCovKwargs = Any\n\n\n## Configuration\n\n# One value read from a config file.\nTConfigValueIn = Optional[Union[bool, int, float, str, Iterable[str]]]\nTConfigValueOut = Optional[Union[bool, int, float, str, List[str]]]\n# An entire config section, mapping option names to values.\nTConfigSectionIn = Mapping[str, TConfigValueIn]\nTConfigSectionOut = Mapping[str, TConfigValueOut]\n\nclass TConfigurable(Protocol):\n    \"\"\"Something that can proxy to the coverage configuration settings.\"\"\"\n\n    def get_option(self, option_name: str) -> TConfigValueOut | None:\n        \"\"\"Get an option from the configuration.\n\n        `option_name` is a colon-separated string indicating the section and\n        option name.  For example, the ``branch`` option in the ``[run]``\n        section of the config file would be indicated with `\"run:branch\"`.\n\n        Returns the value of the option.\n\n        \"\"\"\n\n    def set_option(self, option_name: str, value: TConfigValueIn | TConfigSectionIn) -> None:\n        \"\"\"Set an option in the configuration.\n\n        `option_name` is a colon-separated string indicating the section and\n        option name.  For example, the ``branch`` option in the ``[run]``\n        section of the config file would be indicated with `\"run:branch\"`.\n\n        `value` is the new value for the option.\n\n        \"\"\"\n\nclass TPluginConfig(Protocol):\n    \"\"\"Something that can provide options to a plugin.\"\"\"\n\n    def get_plugin_options(self, plugin: str) -> TConfigSectionOut:\n        \"\"\"Get the options for a plugin.\"\"\"\n\n\n## Parsing\n\nTMorf = Union[ModuleType, str]\n\nTSourceTokenLines = Iterable[List[Tuple[str, str]]]\n\n\n## Plugins\n\nclass TPlugin(Protocol):\n    \"\"\"What all plugins have in common.\"\"\"\n    _coverage_plugin_name: str\n    _coverage_enabled: bool\n\n\n## Debugging\n\nclass TWarnFn(Protocol):\n    \"\"\"A callable warn() function.\"\"\"\n    def __call__(self, msg: str, slug: str | None = None, once: bool = False) -> None:\n        ...\n\n\nclass TDebugCtl(Protocol):\n    \"\"\"A DebugControl object, or something like it.\"\"\"\n\n    def should(self, option: str) -> bool:\n        \"\"\"Decide whether to output debug information in category `option`.\"\"\"\n\n    def write(self, msg: str) -> None:\n        \"\"\"Write a line of debug output.\"\"\"\n\n\nclass TWritable(Protocol):\n    \"\"\"Anything that can be written to.\"\"\"\n\n    def write(self, msg: str) -> None:\n        \"\"\"Write a message.\"\"\"\n", "coverage/disposition.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Simple value objects for tracking what to do with files.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom coverage.types import TFileDisposition\n\nif TYPE_CHECKING:\n    from coverage.plugin import FileTracer\n\n\nclass FileDisposition:\n    \"\"\"A simple value type for recording what to do with a file.\"\"\"\n\n    original_filename: str\n    canonical_filename: str\n    source_filename: str | None\n    trace: bool\n    reason: str\n    file_tracer: FileTracer | None\n    has_dynamic_filename: bool\n\n    def __repr__(self) -> str:\n        return f\"<FileDisposition {self.canonical_filename!r}: trace={self.trace}>\"\n\n\n# FileDisposition \"methods\": FileDisposition is a pure value object, so it can\n# be implemented in either C or Python.  Acting on them is done with these\n# functions.\n\ndef disposition_init(cls: type[TFileDisposition], original_filename: str) -> TFileDisposition:\n    \"\"\"Construct and initialize a new FileDisposition object.\"\"\"\n    disp = cls()\n    disp.original_filename = original_filename\n    disp.canonical_filename = original_filename\n    disp.source_filename = None\n    disp.trace = False\n    disp.reason = \"\"\n    disp.file_tracer = None\n    disp.has_dynamic_filename = False\n    return disp\n\n\ndef disposition_debug_msg(disp: TFileDisposition) -> str:\n    \"\"\"Make a nice debug message of what the FileDisposition is doing.\"\"\"\n    if disp.trace:\n        msg = f\"Tracing {disp.original_filename!r}\"\n        if disp.original_filename != disp.source_filename:\n            msg += f\" as {disp.source_filename!r}\"\n        if disp.file_tracer:\n            msg += f\": will be traced by {disp.file_tracer!r}\"\n    else:\n        msg = f\"Not tracing {disp.original_filename!r}: {disp.reason}\"\n    return msg\n", "coverage/sysmon.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Callback functions and support for sys.monitoring data collection.\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport inspect\nimport os\nimport os.path\nimport sys\nimport threading\nimport traceback\n\nfrom dataclasses import dataclass\nfrom types import CodeType, FrameType\nfrom typing import (\n    Any,\n    Callable,\n    Set,\n    TYPE_CHECKING,\n    cast,\n)\n\nfrom coverage.debug import short_filename, short_stack\nfrom coverage.types import (\n    AnyCallable,\n    TArc,\n    TFileDisposition,\n    TLineNo,\n    TTraceData,\n    TTraceFileData,\n    TracerCore,\n    TWarnFn,\n)\n\n# pylint: disable=unused-argument\n\nLOG = False\n\n# This module will be imported in all versions of Python, but only used in 3.12+\n# It will be type-checked for 3.12, but not for earlier versions.\nsys_monitoring = getattr(sys, \"monitoring\", None)\n\nif TYPE_CHECKING:\n    assert sys_monitoring is not None\n    # I want to say this but it's not allowed:\n    #   MonitorReturn = Literal[sys.monitoring.DISABLE] | None\n    MonitorReturn = Any\n\n\nif LOG:  # pragma: debugging\n\n    class LoggingWrapper:\n        \"\"\"Wrap a namespace to log all its functions.\"\"\"\n\n        def __init__(self, wrapped: Any, namespace: str) -> None:\n            self.wrapped = wrapped\n            self.namespace = namespace\n\n        def __getattr__(self, name: str) -> Callable[..., Any]:\n            def _wrapped(*args: Any, **kwargs: Any) -> Any:\n                log(f\"{self.namespace}.{name}{args}{kwargs}\")\n                return getattr(self.wrapped, name)(*args, **kwargs)\n\n            return _wrapped\n\n    sys_monitoring = LoggingWrapper(sys_monitoring, \"sys.monitoring\")\n    assert sys_monitoring is not None\n\n    short_stack = functools.partial(\n        short_stack, full=True, short_filenames=True, frame_ids=True,\n    )\n    seen_threads: set[int] = set()\n\n    def log(msg: str) -> None:\n        \"\"\"Write a message to our detailed debugging log(s).\"\"\"\n        # Thread ids are reused across processes?\n        # Make a shorter number more likely to be unique.\n        pid = os.getpid()\n        tid = cast(int, threading.current_thread().ident)\n        tslug = f\"{(pid * tid) % 9_999_991:07d}\"\n        if tid not in seen_threads:\n            seen_threads.add(tid)\n            log(f\"New thread {tid} {tslug}:\\n{short_stack()}\")\n        # log_seq = int(os.getenv(\"PANSEQ\", \"0\"))\n        # root = f\"/tmp/pan.{log_seq:03d}\"\n        for filename in [\n            \"/tmp/foo.out\",\n            # f\"{root}.out\",\n            # f\"{root}-{pid}.out\",\n            # f\"{root}-{pid}-{tslug}.out\",\n        ]:\n            with open(filename, \"a\") as f:\n                print(f\"{pid}:{tslug}: {msg}\", file=f, flush=True)\n\n    def arg_repr(arg: Any) -> str:\n        \"\"\"Make a customized repr for logged values.\"\"\"\n        if isinstance(arg, CodeType):\n            return (\n                f\"<code @{id(arg):#x}\"\n                + f\" name={arg.co_name},\"\n                + f\" file={short_filename(arg.co_filename)!r}#{arg.co_firstlineno}>\"\n            )\n        return repr(arg)\n\n    def panopticon(*names: str | None) -> AnyCallable:\n        \"\"\"Decorate a function to log its calls.\"\"\"\n\n        def _decorator(method: AnyCallable) -> AnyCallable:\n            @functools.wraps(method)\n            def _wrapped(self: Any, *args: Any) -> Any:\n                try:\n                    # log(f\"{method.__name__}() stack:\\n{short_stack()}\")\n                    args_reprs = []\n                    for name, arg in zip(names, args):\n                        if name is None:\n                            continue\n                        args_reprs.append(f\"{name}={arg_repr(arg)}\")\n                    log(f\"{id(self):#x}:{method.__name__}({', '.join(args_reprs)})\")\n                    ret = method(self, *args)\n                    # log(f\" end {id(self):#x}:{method.__name__}({', '.join(args_reprs)})\")\n                    return ret\n                except Exception as exc:\n                    log(f\"!!{exc.__class__.__name__}: {exc}\")\n                    log(\"\".join(traceback.format_exception(exc))) # pylint: disable=[no-value-for-parameter]\n                    try:\n                        assert sys_monitoring is not None\n                        sys_monitoring.set_events(sys.monitoring.COVERAGE_ID, 0)\n                    except ValueError:\n                        # We might have already shut off monitoring.\n                        log(\"oops, shutting off events with disabled tool id\")\n                    raise\n\n            return _wrapped\n\n        return _decorator\n\nelse:\n\n    def log(msg: str) -> None:\n        \"\"\"Write a message to our detailed debugging log(s), but not really.\"\"\"\n\n    def panopticon(*names: str | None) -> AnyCallable:\n        \"\"\"Decorate a function to log its calls, but not really.\"\"\"\n\n        def _decorator(meth: AnyCallable) -> AnyCallable:\n            return meth\n\n        return _decorator\n\n\n@dataclass\nclass CodeInfo:\n    \"\"\"The information we want about each code object.\"\"\"\n\n    tracing: bool\n    file_data: TTraceFileData | None\n    # TODO: what is byte_to_line for?\n    byte_to_line: dict[int, int] | None\n\n\ndef bytes_to_lines(code: CodeType) -> dict[int, int]:\n    \"\"\"Make a dict mapping byte code offsets to line numbers.\"\"\"\n    b2l = {}\n    for bstart, bend, lineno in code.co_lines():\n        if lineno is not None:\n            for boffset in range(bstart, bend, 2):\n                b2l[boffset] = lineno\n    return b2l\n\n\nclass SysMonitor(TracerCore):\n    \"\"\"Python implementation of the raw data tracer for PEP669 implementations.\"\"\"\n\n    # One of these will be used across threads. Be careful.\n\n    def __init__(self, tool_id: int) -> None:\n        # Attributes set from the collector:\n        self.data: TTraceData\n        self.trace_arcs = False\n        self.should_trace: Callable[[str, FrameType], TFileDisposition]\n        self.should_trace_cache: dict[str, TFileDisposition | None]\n        # TODO: should_start_context and switch_context are unused!\n        # Change tests/testenv.py:DYN_CONTEXTS when this is updated.\n        self.should_start_context: Callable[[FrameType], str | None] | None = None\n        self.switch_context: Callable[[str | None], None] | None = None\n        self.lock_data: Callable[[], None]\n        self.unlock_data: Callable[[], None]\n        # TODO: warn is unused.\n        self.warn: TWarnFn\n\n        self.myid = tool_id\n\n        # Map id(code_object) -> CodeInfo\n        self.code_infos: dict[int, CodeInfo] = {}\n        # A list of code_objects, just to keep them alive so that id's are\n        # useful as identity.\n        self.code_objects: list[CodeType] = []\n        self.last_lines: dict[FrameType, int] = {}\n        # Map id(code_object) -> code_object\n        self.local_event_codes: dict[int, CodeType] = {}\n        self.sysmon_on = False\n        self.lock = threading.Lock()\n\n        self.stats = {\n            \"starts\": 0,\n        }\n\n        self.stopped = False\n        self._activity = False\n\n    def __repr__(self) -> str:\n        points = sum(len(v) for v in self.data.values())\n        files = len(self.data)\n        return f\"<SysMonitor at {id(self):#x}: {points} data points in {files} files>\"\n\n    @panopticon()\n    def start(self) -> None:\n        \"\"\"Start this Tracer.\"\"\"\n        self.stopped = False\n\n        assert sys_monitoring is not None\n        sys_monitoring.use_tool_id(self.myid, \"coverage.py\")\n        register = functools.partial(sys_monitoring.register_callback, self.myid)\n        events = sys_monitoring.events\n        if self.trace_arcs:\n            sys_monitoring.set_events(\n                self.myid,\n                events.PY_START | events.PY_UNWIND,\n            )\n            register(events.PY_START, self.sysmon_py_start)\n            register(events.PY_RESUME, self.sysmon_py_resume_arcs)\n            register(events.PY_RETURN, self.sysmon_py_return_arcs)\n            register(events.PY_UNWIND, self.sysmon_py_unwind_arcs)\n            register(events.LINE, self.sysmon_line_arcs)\n        else:\n            sys_monitoring.set_events(self.myid, events.PY_START)\n            register(events.PY_START, self.sysmon_py_start)\n            register(events.LINE, self.sysmon_line_lines)\n        sys_monitoring.restart_events()\n        self.sysmon_on = True\n\n    @panopticon()\n    def stop(self) -> None:\n        \"\"\"Stop this Tracer.\"\"\"\n        if not self.sysmon_on:\n            # In forking situations, we might try to stop when we are not\n            # started.  Do nothing in that case.\n            return\n        assert sys_monitoring is not None\n        sys_monitoring.set_events(self.myid, 0)\n        with self.lock:\n            self.sysmon_on = False\n            for code in self.local_event_codes.values():\n                sys_monitoring.set_local_events(self.myid, code, 0)\n            self.local_event_codes = {}\n        sys_monitoring.free_tool_id(self.myid)\n\n    @panopticon()\n    def post_fork(self) -> None:\n        \"\"\"The process has forked, clean up as needed.\"\"\"\n        self.stop()\n\n    def activity(self) -> bool:\n        \"\"\"Has there been any activity?\"\"\"\n        return self._activity\n\n    def reset_activity(self) -> None:\n        \"\"\"Reset the activity() flag.\"\"\"\n        self._activity = False\n\n    def get_stats(self) -> dict[str, int] | None:\n        \"\"\"Return a dictionary of statistics, or None.\"\"\"\n        return None\n\n    # The number of frames in callers_frame takes @panopticon into account.\n    if LOG:\n\n        def callers_frame(self) -> FrameType:\n            \"\"\"Get the frame of the Python code we're monitoring.\"\"\"\n            return (\n                inspect.currentframe().f_back.f_back.f_back  # type: ignore[union-attr,return-value]\n            )\n\n    else:\n\n        def callers_frame(self) -> FrameType:\n            \"\"\"Get the frame of the Python code we're monitoring.\"\"\"\n            return inspect.currentframe().f_back.f_back  # type: ignore[union-attr,return-value]\n\n    @panopticon(\"code\", \"@\")\n    def sysmon_py_start(self, code: CodeType, instruction_offset: int) -> MonitorReturn:\n        \"\"\"Handle sys.monitoring.events.PY_START events.\"\"\"\n        # Entering a new frame.  Decide if we should trace in this file.\n        self._activity = True\n        self.stats[\"starts\"] += 1\n\n        code_info = self.code_infos.get(id(code))\n        tracing_code: bool | None = None\n        file_data: TTraceFileData | None = None\n        if code_info is not None:\n            tracing_code = code_info.tracing\n            file_data = code_info.file_data\n\n        if tracing_code is None:\n            filename = code.co_filename\n            disp = self.should_trace_cache.get(filename)\n            if disp is None:\n                frame = inspect.currentframe().f_back  # type: ignore[union-attr]\n                if LOG:\n                    # @panopticon adds a frame.\n                    frame = frame.f_back  # type: ignore[union-attr]\n                disp = self.should_trace(filename, frame)  # type: ignore[arg-type]\n                self.should_trace_cache[filename] = disp\n\n            tracing_code = disp.trace\n            if tracing_code:\n                tracename = disp.source_filename\n                assert tracename is not None\n                self.lock_data()\n                try:\n                    if tracename not in self.data:\n                        self.data[tracename] = set()\n                finally:\n                    self.unlock_data()\n                file_data = self.data[tracename]\n                b2l = bytes_to_lines(code)\n            else:\n                file_data = None\n                b2l = None\n\n            self.code_infos[id(code)] = CodeInfo(\n                tracing=tracing_code,\n                file_data=file_data,\n                byte_to_line=b2l,\n            )\n            self.code_objects.append(code)\n\n            if tracing_code:\n                events = sys.monitoring.events\n                with self.lock:\n                    if self.sysmon_on:\n                        assert sys_monitoring is not None\n                        sys_monitoring.set_local_events(\n                            self.myid,\n                            code,\n                            events.PY_RETURN\n                            #\n                            | events.PY_RESUME\n                            # | events.PY_YIELD\n                            | events.LINE,\n                            # | events.BRANCH\n                            # | events.JUMP\n                        )\n                        self.local_event_codes[id(code)] = code\n\n        if tracing_code and self.trace_arcs:\n            frame = self.callers_frame()\n            self.last_lines[frame] = -code.co_firstlineno\n            return None\n        else:\n            return sys.monitoring.DISABLE\n\n    @panopticon(\"code\", \"@\")\n    def sysmon_py_resume_arcs(\n        self, code: CodeType, instruction_offset: int,\n    ) -> MonitorReturn:\n        \"\"\"Handle sys.monitoring.events.PY_RESUME events for branch coverage.\"\"\"\n        frame = self.callers_frame()\n        self.last_lines[frame] = frame.f_lineno\n\n    @panopticon(\"code\", \"@\", None)\n    def sysmon_py_return_arcs(\n        self, code: CodeType, instruction_offset: int, retval: object,\n    ) -> MonitorReturn:\n        \"\"\"Handle sys.monitoring.events.PY_RETURN events for branch coverage.\"\"\"\n        frame = self.callers_frame()\n        code_info = self.code_infos.get(id(code))\n        if code_info is not None and code_info.file_data is not None:\n            last_line = self.last_lines.get(frame)\n            if last_line is not None:\n                arc = (last_line, -code.co_firstlineno)\n                # log(f\"adding {arc=}\")\n                cast(Set[TArc], code_info.file_data).add(arc)\n\n        # Leaving this function, no need for the frame any more.\n        self.last_lines.pop(frame, None)\n\n    @panopticon(\"code\", \"@\", \"exc\")\n    def sysmon_py_unwind_arcs(\n        self, code: CodeType, instruction_offset: int, exception: BaseException,\n    ) -> MonitorReturn:\n        \"\"\"Handle sys.monitoring.events.PY_UNWIND events for branch coverage.\"\"\"\n        frame = self.callers_frame()\n        # Leaving this function.\n        last_line = self.last_lines.pop(frame, None)\n        if isinstance(exception, GeneratorExit):\n            # We don't want to count generator exits as arcs.\n            return\n        code_info = self.code_infos.get(id(code))\n        if code_info is not None and code_info.file_data is not None:\n            if last_line is not None:\n                arc = (last_line, -code.co_firstlineno)\n                # log(f\"adding {arc=}\")\n                cast(Set[TArc], code_info.file_data).add(arc)\n\n\n    @panopticon(\"code\", \"line\")\n    def sysmon_line_lines(self, code: CodeType, line_number: int) -> MonitorReturn:\n        \"\"\"Handle sys.monitoring.events.LINE events for line coverage.\"\"\"\n        code_info = self.code_infos[id(code)]\n        if code_info.file_data is not None:\n            cast(Set[TLineNo], code_info.file_data).add(line_number)\n            # log(f\"adding {line_number=}\")\n        return sys.monitoring.DISABLE\n\n    @panopticon(\"code\", \"line\")\n    def sysmon_line_arcs(self, code: CodeType, line_number: int) -> MonitorReturn:\n        \"\"\"Handle sys.monitoring.events.LINE events for branch coverage.\"\"\"\n        code_info = self.code_infos[id(code)]\n        ret = None\n        if code_info.file_data is not None:\n            frame = self.callers_frame()\n            last_line = self.last_lines.get(frame)\n            if last_line is not None:\n                arc = (last_line, line_number)\n                cast(Set[TArc], code_info.file_data).add(arc)\n            # log(f\"adding {arc=}\")\n            self.last_lines[frame] = line_number\n        return ret\n", "coverage/control.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Central control stuff for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport collections\nimport contextlib\nimport functools\nimport os\nimport os.path\nimport platform\nimport signal\nimport sys\nimport threading\nimport time\nimport warnings\n\nfrom types import FrameType\nfrom typing import (\n    cast,\n    Any, Callable, IO, Iterable, Iterator, List,\n)\n\nfrom coverage import env\nfrom coverage.annotate import AnnotateReporter\nfrom coverage.collector import Collector, HAS_CTRACER\nfrom coverage.config import CoverageConfig, read_coverage_config\nfrom coverage.context import should_start_context_test_function, combine_context_switchers\nfrom coverage.data import CoverageData, combine_parallel_data\nfrom coverage.debug import (\n    DebugControl, NoDebugging, short_stack, write_formatted_info, relevant_environment_display,\n)\nfrom coverage.disposition import disposition_debug_msg\nfrom coverage.exceptions import ConfigError, CoverageException, CoverageWarning, PluginError\nfrom coverage.files import PathAliases, abs_file, relative_filename, set_relative_directory\nfrom coverage.html import HtmlReporter\nfrom coverage.inorout import InOrOut\nfrom coverage.jsonreport import JsonReporter\nfrom coverage.lcovreport import LcovReporter\nfrom coverage.misc import bool_or_none, join_regex\nfrom coverage.misc import DefaultValue, ensure_dir_for_file, isolate_module\nfrom coverage.multiproc import patch_multiprocessing\nfrom coverage.plugin import FileReporter\nfrom coverage.plugin_support import Plugins\nfrom coverage.python import PythonFileReporter\nfrom coverage.report import SummaryReporter\nfrom coverage.report_core import render_report\nfrom coverage.results import Analysis, analysis_from_file_reporter\nfrom coverage.types import (\n    FilePath, TConfigurable, TConfigSectionIn, TConfigValueIn, TConfigValueOut,\n    TFileDisposition, TLineNo, TMorf,\n)\nfrom coverage.xmlreport import XmlReporter\n\nos = isolate_module(os)\n\n@contextlib.contextmanager\ndef override_config(cov: Coverage, **kwargs: TConfigValueIn) -> Iterator[None]:\n    \"\"\"Temporarily tweak the configuration of `cov`.\n\n    The arguments are applied to `cov.config` with the `from_args` method.\n    At the end of the with-statement, the old configuration is restored.\n    \"\"\"\n    original_config = cov.config\n    cov.config = cov.config.copy()\n    try:\n        cov.config.from_args(**kwargs)\n        yield\n    finally:\n        cov.config = original_config\n\n\nDEFAULT_DATAFILE = DefaultValue(\"MISSING\")\n_DEFAULT_DATAFILE = DEFAULT_DATAFILE  # Just in case, for backwards compatibility\n\nclass Coverage(TConfigurable):\n    \"\"\"Programmatic access to coverage.py.\n\n    To use::\n\n        from coverage import Coverage\n\n        cov = Coverage()\n        cov.start()\n        #.. call your code ..\n        cov.stop()\n        cov.html_report(directory=\"covhtml\")\n\n    A context manager is available to do the same thing::\n\n        cov = Coverage()\n        with cov.collect():\n            #.. call your code ..\n        cov.html_report(directory=\"covhtml\")\n\n    Note: in keeping with Python custom, names starting with underscore are\n    not part of the public API. They might stop working at any point.  Please\n    limit yourself to documented methods to avoid problems.\n\n    Methods can raise any of the exceptions described in :ref:`api_exceptions`.\n\n    \"\"\"\n\n    # The stack of started Coverage instances.\n    _instances: list[Coverage] = []\n\n    @classmethod\n    def current(cls) -> Coverage | None:\n        \"\"\"Get the latest started `Coverage` instance, if any.\n\n        Returns: a `Coverage` instance, or None.\n\n        .. versionadded:: 5.0\n\n        \"\"\"\n        if cls._instances:\n            return cls._instances[-1]\n        else:\n            return None\n\n    def __init__(                       # pylint: disable=too-many-arguments\n        self,\n        data_file: FilePath | DefaultValue | None = DEFAULT_DATAFILE,\n        data_suffix: str | bool | None = None,\n        cover_pylib: bool | None = None,\n        auto_data: bool = False,\n        timid: bool | None = None,\n        branch: bool | None = None,\n        config_file: FilePath | bool = True,\n        source: Iterable[str] | None = None,\n        source_pkgs: Iterable[str] | None = None,\n        omit: str | Iterable[str] | None = None,\n        include: str | Iterable[str] | None = None,\n        debug: Iterable[str] | None = None,\n        concurrency: str | Iterable[str] | None = None,\n        check_preimported: bool = False,\n        context: str | None = None,\n        messages: bool = False,\n    ) -> None:\n        \"\"\"\n        Many of these arguments duplicate and override values that can be\n        provided in a configuration file.  Parameters that are missing here\n        will use values from the config file.\n\n        `data_file` is the base name of the data file to use. The config value\n        defaults to \".coverage\".  None can be provided to prevent writing a data\n        file.  `data_suffix` is appended (with a dot) to `data_file` to create\n        the final file name.  If `data_suffix` is simply True, then a suffix is\n        created with the machine and process identity included.\n\n        `cover_pylib` is a boolean determining whether Python code installed\n        with the Python interpreter is measured.  This includes the Python\n        standard library and any packages installed with the interpreter.\n\n        If `auto_data` is true, then any existing data file will be read when\n        coverage measurement starts, and data will be saved automatically when\n        measurement stops.\n\n        If `timid` is true, then a slower and simpler trace function will be\n        used.  This is important for some environments where manipulation of\n        tracing functions breaks the faster trace function.\n\n        If `branch` is true, then branch coverage will be measured in addition\n        to the usual statement coverage.\n\n        `config_file` determines what configuration file to read:\n\n            * If it is \".coveragerc\", it is interpreted as if it were True,\n              for backward compatibility.\n\n            * If it is a string, it is the name of the file to read.  If the\n              file can't be read, it is an error.\n\n            * If it is True, then a few standard files names are tried\n              (\".coveragerc\", \"setup.cfg\", \"tox.ini\").  It is not an error for\n              these files to not be found.\n\n            * If it is False, then no configuration file is read.\n\n        `source` is a list of file paths or package names.  Only code located\n        in the trees indicated by the file paths or package names will be\n        measured.\n\n        `source_pkgs` is a list of package names. It works the same as\n        `source`, but can be used to name packages where the name can also be\n        interpreted as a file path.\n\n        `include` and `omit` are lists of file name patterns. Files that match\n        `include` will be measured, files that match `omit` will not.  Each\n        will also accept a single string argument.\n\n        `debug` is a list of strings indicating what debugging information is\n        desired.\n\n        `concurrency` is a string indicating the concurrency library being used\n        in the measured code.  Without this, coverage.py will get incorrect\n        results if these libraries are in use.  Valid strings are \"greenlet\",\n        \"eventlet\", \"gevent\", \"multiprocessing\", or \"thread\" (the default).\n        This can also be a list of these strings.\n\n        If `check_preimported` is true, then when coverage is started, the\n        already-imported files will be checked to see if they should be\n        measured by coverage.  Importing measured files before coverage is\n        started can mean that code is missed.\n\n        `context` is a string to use as the :ref:`static context\n        <static_contexts>` label for collected data.\n\n        If `messages` is true, some messages will be printed to stdout\n        indicating what is happening.\n\n        .. versionadded:: 4.0\n            The `concurrency` parameter.\n\n        .. versionadded:: 4.2\n            The `concurrency` parameter can now be a list of strings.\n\n        .. versionadded:: 5.0\n            The `check_preimported` and `context` parameters.\n\n        .. versionadded:: 5.3\n            The `source_pkgs` parameter.\n\n        .. versionadded:: 6.0\n            The `messages` parameter.\n\n        \"\"\"\n        # Start self.config as a usable default configuration. It will soon be\n        # replaced with the real configuration.\n        self.config = CoverageConfig()\n\n        # data_file=None means no disk file at all. data_file missing means\n        # use the value from the config file.\n        self._no_disk = data_file is None\n        if isinstance(data_file, DefaultValue):\n            data_file = None\n        if data_file is not None:\n            data_file = os.fspath(data_file)\n\n        # This is injectable by tests.\n        self._debug_file: IO[str] | None = None\n\n        self._auto_load = self._auto_save = auto_data\n        self._data_suffix_specified = data_suffix\n\n        # Is it ok for no data to be collected?\n        self._warn_no_data = True\n        self._warn_unimported_source = True\n        self._warn_preimported_source = check_preimported\n        self._no_warn_slugs: list[str] = []\n        self._messages = messages\n\n        # A record of all the warnings that have been issued.\n        self._warnings: list[str] = []\n\n        # Other instance attributes, set with placebos or placeholders.\n        # More useful objects will be created later.\n        self._debug: DebugControl = NoDebugging()\n        self._inorout: InOrOut | None = None\n        self._plugins: Plugins = Plugins()\n        self._data: CoverageData | None = None\n        self._collector: Collector | None = None\n        self._metacov = False\n\n        self._file_mapper: Callable[[str], str] = abs_file\n        self._data_suffix = self._run_suffix = None\n        self._exclude_re: dict[str, str] = {}\n        self._old_sigterm: Callable[[int, FrameType | None], Any] | None = None\n\n        # State machine variables:\n        # Have we initialized everything?\n        self._inited = False\n        self._inited_for_start = False\n        # Have we started collecting and not stopped it?\n        self._started = False\n        # Should we write the debug output?\n        self._should_write_debug = True\n\n        # Build our configuration from a number of sources.\n        if not isinstance(config_file, bool):\n            config_file = os.fspath(config_file)\n        self.config = read_coverage_config(\n            config_file=config_file,\n            warn=self._warn,\n            data_file=data_file,\n            cover_pylib=cover_pylib,\n            timid=timid,\n            branch=branch,\n            parallel=bool_or_none(data_suffix),\n            source=source,\n            source_pkgs=source_pkgs,\n            run_omit=omit,\n            run_include=include,\n            debug=debug,\n            report_omit=omit,\n            report_include=include,\n            concurrency=concurrency,\n            context=context,\n        )\n\n        # If we have sub-process measurement happening automatically, then we\n        # want any explicit creation of a Coverage object to mean, this process\n        # is already coverage-aware, so don't auto-measure it.  By now, the\n        # auto-creation of a Coverage object has already happened.  But we can\n        # find it and tell it not to save its data.\n        if not env.METACOV:\n            _prevent_sub_process_measurement()\n\n    def _init(self) -> None:\n        \"\"\"Set all the initial state.\n\n        This is called by the public methods to initialize state. This lets us\n        construct a :class:`Coverage` object, then tweak its state before this\n        function is called.\n\n        \"\"\"\n        if self._inited:\n            return\n\n        self._inited = True\n\n        # Create and configure the debugging controller.\n        self._debug = DebugControl(self.config.debug, self._debug_file, self.config.debug_file)\n        if self._debug.should(\"process\"):\n            self._debug.write(\"Coverage._init\")\n\n        if \"multiprocessing\" in (self.config.concurrency or ()):\n            # Multi-processing uses parallel for the subprocesses, so also use\n            # it for the main process.\n            self.config.parallel = True\n\n        # _exclude_re is a dict that maps exclusion list names to compiled regexes.\n        self._exclude_re = {}\n\n        set_relative_directory()\n        if self.config.relative_files:\n            self._file_mapper = relative_filename\n\n        # Load plugins\n        self._plugins = Plugins.load_plugins(self.config.plugins, self.config, self._debug)\n\n        # Run configuring plugins.\n        for plugin in self._plugins.configurers:\n            # We need an object with set_option and get_option. Either self or\n            # self.config will do. Choosing randomly stops people from doing\n            # other things with those objects, against the public API.  Yes,\n            # this is a bit childish. :)\n            plugin.configure([self, self.config][int(time.time()) % 2])\n\n    def _post_init(self) -> None:\n        \"\"\"Stuff to do after everything is initialized.\"\"\"\n        if self._should_write_debug:\n            self._should_write_debug = False\n            self._write_startup_debug()\n\n        # \"[run] _crash\" will raise an exception if the value is close by in\n        # the call stack, for testing error handling.\n        if self.config._crash and self.config._crash in short_stack():\n            raise RuntimeError(f\"Crashing because called by {self.config._crash}\")\n\n    def _write_startup_debug(self) -> None:\n        \"\"\"Write out debug info at startup if needed.\"\"\"\n        wrote_any = False\n        with self._debug.without_callers():\n            if self._debug.should(\"config\"):\n                config_info = self.config.debug_info()\n                write_formatted_info(self._debug.write, \"config\", config_info)\n                wrote_any = True\n\n            if self._debug.should(\"sys\"):\n                write_formatted_info(self._debug.write, \"sys\", self.sys_info())\n                for plugin in self._plugins:\n                    header = \"sys: \" + plugin._coverage_plugin_name\n                    info = plugin.sys_info()\n                    write_formatted_info(self._debug.write, header, info)\n                wrote_any = True\n\n            if self._debug.should(\"pybehave\"):\n                write_formatted_info(self._debug.write, \"pybehave\", env.debug_info())\n                wrote_any = True\n\n        if wrote_any:\n            write_formatted_info(self._debug.write, \"end\", ())\n\n    def _should_trace(self, filename: str, frame: FrameType) -> TFileDisposition:\n        \"\"\"Decide whether to trace execution in `filename`.\n\n        Calls `_should_trace_internal`, and returns the FileDisposition.\n\n        \"\"\"\n        assert self._inorout is not None\n        disp = self._inorout.should_trace(filename, frame)\n        if self._debug.should(\"trace\"):\n            self._debug.write(disposition_debug_msg(disp))\n        return disp\n\n    def _check_include_omit_etc(self, filename: str, frame: FrameType) -> bool:\n        \"\"\"Check a file name against the include/omit/etc, rules, verbosely.\n\n        Returns a boolean: True if the file should be traced, False if not.\n\n        \"\"\"\n        assert self._inorout is not None\n        reason = self._inorout.check_include_omit_etc(filename, frame)\n        if self._debug.should(\"trace\"):\n            if not reason:\n                msg = f\"Including {filename!r}\"\n            else:\n                msg = f\"Not including {filename!r}: {reason}\"\n            self._debug.write(msg)\n\n        return not reason\n\n    def _warn(self, msg: str, slug: str | None = None, once: bool = False) -> None:\n        \"\"\"Use `msg` as a warning.\n\n        For warning suppression, use `slug` as the shorthand.\n\n        If `once` is true, only show this warning once (determined by the\n        slug.)\n\n        \"\"\"\n        if not self._no_warn_slugs:\n            self._no_warn_slugs = list(self.config.disable_warnings)\n\n        if slug in self._no_warn_slugs:\n            # Don't issue the warning\n            return\n\n        self._warnings.append(msg)\n        if slug:\n            msg = f\"{msg} ({slug})\"\n        if self._debug.should(\"pid\"):\n            msg = f\"[{os.getpid()}] {msg}\"\n        warnings.warn(msg, category=CoverageWarning, stacklevel=2)\n\n        if once:\n            assert slug is not None\n            self._no_warn_slugs.append(slug)\n\n    def _message(self, msg: str) -> None:\n        \"\"\"Write a message to the user, if configured to do so.\"\"\"\n        if self._messages:\n            print(msg)\n\n    def get_option(self, option_name: str) -> TConfigValueOut | None:\n        \"\"\"Get an option from the configuration.\n\n        `option_name` is a colon-separated string indicating the section and\n        option name.  For example, the ``branch`` option in the ``[run]``\n        section of the config file would be indicated with `\"run:branch\"`.\n\n        Returns the value of the option.  The type depends on the option\n        selected.\n\n        As a special case, an `option_name` of ``\"paths\"`` will return an\n        dictionary with the entire ``[paths]`` section value.\n\n        .. versionadded:: 4.0\n\n        \"\"\"\n        return self.config.get_option(option_name)\n\n    def set_option(self, option_name: str, value: TConfigValueIn | TConfigSectionIn) -> None:\n        \"\"\"Set an option in the configuration.\n\n        `option_name` is a colon-separated string indicating the section and\n        option name.  For example, the ``branch`` option in the ``[run]``\n        section of the config file would be indicated with ``\"run:branch\"``.\n\n        `value` is the new value for the option.  This should be an\n        appropriate Python value.  For example, use True for booleans, not the\n        string ``\"True\"``.\n\n        As an example, calling:\n\n        .. code-block:: python\n\n            cov.set_option(\"run:branch\", True)\n\n        has the same effect as this configuration file:\n\n        .. code-block:: ini\n\n            [run]\n            branch = True\n\n        As a special case, an `option_name` of ``\"paths\"`` will replace the\n        entire ``[paths]`` section.  The value should be a dictionary.\n\n        .. versionadded:: 4.0\n\n        \"\"\"\n        self.config.set_option(option_name, value)\n\n    def load(self) -> None:\n        \"\"\"Load previously-collected coverage data from the data file.\"\"\"\n        self._init()\n        if self._collector is not None:\n            self._collector.reset()\n        should_skip = self.config.parallel and not os.path.exists(self.config.data_file)\n        if not should_skip:\n            self._init_data(suffix=None)\n        self._post_init()\n        if not should_skip:\n            assert self._data is not None\n            self._data.read()\n\n    def _init_for_start(self) -> None:\n        \"\"\"Initialization for start()\"\"\"\n        # Construct the collector.\n        concurrency: list[str] = self.config.concurrency or []\n        if \"multiprocessing\" in concurrency:\n            if self.config.config_file is None:\n                raise ConfigError(\"multiprocessing requires a configuration file\")\n            patch_multiprocessing(rcfile=self.config.config_file)\n\n        dycon = self.config.dynamic_context\n        if not dycon or dycon == \"none\":\n            context_switchers = []\n        elif dycon == \"test_function\":\n            context_switchers = [should_start_context_test_function]\n        else:\n            raise ConfigError(f\"Don't understand dynamic_context setting: {dycon!r}\")\n\n        context_switchers.extend(\n            plugin.dynamic_context for plugin in self._plugins.context_switchers\n        )\n\n        should_start_context = combine_context_switchers(context_switchers)\n\n        self._collector = Collector(\n            should_trace=self._should_trace,\n            check_include=self._check_include_omit_etc,\n            should_start_context=should_start_context,\n            file_mapper=self._file_mapper,\n            timid=self.config.timid,\n            branch=self.config.branch,\n            warn=self._warn,\n            concurrency=concurrency,\n            metacov=self._metacov,\n        )\n\n        suffix = self._data_suffix_specified\n        if suffix:\n            if not isinstance(suffix, str):\n                # if data_suffix=True, use .machinename.pid.random\n                suffix = True\n        elif self.config.parallel:\n            if suffix is None:\n                suffix = True\n            elif not isinstance(suffix, str):\n                suffix = bool(suffix)\n        else:\n            suffix = None\n\n        self._init_data(suffix)\n\n        assert self._data is not None\n        self._collector.use_data(self._data, self.config.context)\n\n        # Early warning if we aren't going to be able to support plugins.\n        if self._plugins.file_tracers and not self._collector.supports_plugins:\n            self._warn(\n                \"Plugin file tracers ({}) aren't supported with {}\".format(\n                    \", \".join(\n                        plugin._coverage_plugin_name\n                            for plugin in self._plugins.file_tracers\n                    ),\n                    self._collector.tracer_name(),\n                ),\n            )\n            for plugin in self._plugins.file_tracers:\n                plugin._coverage_enabled = False\n\n        # Create the file classifying substructure.\n        self._inorout = InOrOut(\n            config=self.config,\n            warn=self._warn,\n            debug=(self._debug if self._debug.should(\"trace\") else None),\n            include_namespace_packages=self.config.include_namespace_packages,\n        )\n        self._inorout.plugins = self._plugins\n        self._inorout.disp_class = self._collector.file_disposition_class\n\n        # It's useful to write debug info after initing for start.\n        self._should_write_debug = True\n\n        # Register our clean-up handlers.\n        atexit.register(self._atexit)\n        if self.config.sigterm:\n            is_main = (threading.current_thread() == threading.main_thread())\n            if is_main and not env.WINDOWS:\n                # The Python docs seem to imply that SIGTERM works uniformly even\n                # on Windows, but that's not my experience, and this agrees:\n                # https://stackoverflow.com/questions/35772001/x/35792192#35792192\n                self._old_sigterm = signal.signal(      # type: ignore[assignment]\n                    signal.SIGTERM, self._on_sigterm,\n                )\n\n    def _init_data(self, suffix: str | bool | None) -> None:\n        \"\"\"Create a data file if we don't have one yet.\"\"\"\n        if self._data is None:\n            # Create the data file.  We do this at construction time so that the\n            # data file will be written into the directory where the process\n            # started rather than wherever the process eventually chdir'd to.\n            ensure_dir_for_file(self.config.data_file)\n            self._data = CoverageData(\n                basename=self.config.data_file,\n                suffix=suffix,\n                warn=self._warn,\n                debug=self._debug,\n                no_disk=self._no_disk,\n            )\n\n    def start(self) -> None:\n        \"\"\"Start measuring code coverage.\n\n        Coverage measurement is only collected in functions called after\n        :meth:`start` is invoked.  Statements in the same scope as\n        :meth:`start` won't be measured.\n\n        Once you invoke :meth:`start`, you must also call :meth:`stop`\n        eventually, or your process might not shut down cleanly.\n\n        The :meth:`collect` method is a context manager to handle both\n        starting and stopping collection.\n\n        \"\"\"\n        self._init()\n        if not self._inited_for_start:\n            self._inited_for_start = True\n            self._init_for_start()\n        self._post_init()\n\n        assert self._collector is not None\n        assert self._inorout is not None\n\n        # Issue warnings for possible problems.\n        self._inorout.warn_conflicting_settings()\n\n        # See if we think some code that would eventually be measured has\n        # already been imported.\n        if self._warn_preimported_source:\n            self._inorout.warn_already_imported_files()\n\n        if self._auto_load:\n            self.load()\n\n        self._collector.start()\n        self._started = True\n        self._instances.append(self)\n\n    def stop(self) -> None:\n        \"\"\"Stop measuring code coverage.\"\"\"\n        if self._instances:\n            if self._instances[-1] is self:\n                self._instances.pop()\n        if self._started:\n            assert self._collector is not None\n            self._collector.stop()\n        self._started = False\n\n    @contextlib.contextmanager\n    def collect(self) -> Iterator[None]:\n        \"\"\"A context manager to start/stop coverage measurement collection.\n\n        .. versionadded:: 7.3\n\n        \"\"\"\n        self.start()\n        try:\n            yield\n        finally:\n            self.stop()\n\n    def _atexit(self, event: str = \"atexit\") -> None:\n        \"\"\"Clean up on process shutdown.\"\"\"\n        if self._debug.should(\"process\"):\n            self._debug.write(f\"{event}: pid: {os.getpid()}, instance: {self!r}\")\n        if self._started:\n            self.stop()\n        if self._auto_save or event == \"sigterm\":\n            self.save()\n\n    def _on_sigterm(self, signum_unused: int, frame_unused: FrameType | None) -> None:\n        \"\"\"A handler for signal.SIGTERM.\"\"\"\n        self._atexit(\"sigterm\")\n        # Statements after here won't be seen by metacov because we just wrote\n        # the data, and are about to kill the process.\n        signal.signal(signal.SIGTERM, self._old_sigterm)    # pragma: not covered\n        os.kill(os.getpid(), signal.SIGTERM)                # pragma: not covered\n\n    def erase(self) -> None:\n        \"\"\"Erase previously collected coverage data.\n\n        This removes the in-memory data collected in this session as well as\n        discarding the data file.\n\n        \"\"\"\n        self._init()\n        self._post_init()\n        if self._collector is not None:\n            self._collector.reset()\n        self._init_data(suffix=None)\n        assert self._data is not None\n        self._data.erase(parallel=self.config.parallel)\n        self._data = None\n        self._inited_for_start = False\n\n    def switch_context(self, new_context: str) -> None:\n        \"\"\"Switch to a new dynamic context.\n\n        `new_context` is a string to use as the :ref:`dynamic context\n        <dynamic_contexts>` label for collected data.  If a :ref:`static\n        context <static_contexts>` is in use, the static and dynamic context\n        labels will be joined together with a pipe character.\n\n        Coverage collection must be started already.\n\n        .. versionadded:: 5.0\n\n        \"\"\"\n        if not self._started:                           # pragma: part started\n            raise CoverageException(\"Cannot switch context, coverage is not started\")\n\n        assert self._collector is not None\n        if self._collector.should_start_context:\n            self._warn(\"Conflicting dynamic contexts\", slug=\"dynamic-conflict\", once=True)\n\n        self._collector.switch_context(new_context)\n\n    def clear_exclude(self, which: str = \"exclude\") -> None:\n        \"\"\"Clear the exclude list.\"\"\"\n        self._init()\n        setattr(self.config, which + \"_list\", [])\n        self._exclude_regex_stale()\n\n    def exclude(self, regex: str, which: str = \"exclude\") -> None:\n        \"\"\"Exclude source lines from execution consideration.\n\n        A number of lists of regular expressions are maintained.  Each list\n        selects lines that are treated differently during reporting.\n\n        `which` determines which list is modified.  The \"exclude\" list selects\n        lines that are not considered executable at all.  The \"partial\" list\n        indicates lines with branches that are not taken.\n\n        `regex` is a regular expression.  The regex is added to the specified\n        list.  If any of the regexes in the list is found in a line, the line\n        is marked for special treatment during reporting.\n\n        \"\"\"\n        self._init()\n        excl_list = getattr(self.config, which + \"_list\")\n        excl_list.append(regex)\n        self._exclude_regex_stale()\n\n    def _exclude_regex_stale(self) -> None:\n        \"\"\"Drop all the compiled exclusion regexes, a list was modified.\"\"\"\n        self._exclude_re.clear()\n\n    def _exclude_regex(self, which: str) -> str:\n        \"\"\"Return a regex string for the given exclusion list.\"\"\"\n        if which not in self._exclude_re:\n            excl_list = getattr(self.config, which + \"_list\")\n            self._exclude_re[which] = join_regex(excl_list)\n        return self._exclude_re[which]\n\n    def get_exclude_list(self, which: str = \"exclude\") -> list[str]:\n        \"\"\"Return a list of excluded regex strings.\n\n        `which` indicates which list is desired.  See :meth:`exclude` for the\n        lists that are available, and their meaning.\n\n        \"\"\"\n        self._init()\n        return cast(List[str], getattr(self.config, which + \"_list\"))\n\n    def save(self) -> None:\n        \"\"\"Save the collected coverage data to the data file.\"\"\"\n        data = self.get_data()\n        data.write()\n\n    def _make_aliases(self) -> PathAliases:\n        \"\"\"Create a PathAliases from our configuration.\"\"\"\n        aliases = PathAliases(\n            debugfn=(self._debug.write if self._debug.should(\"pathmap\") else None),\n            relative=self.config.relative_files,\n        )\n        for paths in self.config.paths.values():\n            result = paths[0]\n            for pattern in paths[1:]:\n                aliases.add(pattern, result)\n        return aliases\n\n    def combine(\n        self,\n        data_paths: Iterable[str] | None = None,\n        strict: bool = False,\n        keep: bool = False,\n    ) -> None:\n        \"\"\"Combine together a number of similarly-named coverage data files.\n\n        All coverage data files whose name starts with `data_file` (from the\n        coverage() constructor) will be read, and combined together into the\n        current measurements.\n\n        `data_paths` is a list of files or directories from which data should\n        be combined. If no list is passed, then the data files from the\n        directory indicated by the current data file (probably the current\n        directory) will be combined.\n\n        If `strict` is true, then it is an error to attempt to combine when\n        there are no data files to combine.\n\n        If `keep` is true, then original input data files won't be deleted.\n\n        .. versionadded:: 4.0\n            The `data_paths` parameter.\n\n        .. versionadded:: 4.3\n            The `strict` parameter.\n\n        .. versionadded: 5.5\n            The `keep` parameter.\n        \"\"\"\n        self._init()\n        self._init_data(suffix=None)\n        self._post_init()\n        self.get_data()\n\n        assert self._data is not None\n        combine_parallel_data(\n            self._data,\n            aliases=self._make_aliases(),\n            data_paths=data_paths,\n            strict=strict,\n            keep=keep,\n            message=self._message,\n        )\n\n    def get_data(self) -> CoverageData:\n        \"\"\"Get the collected data.\n\n        Also warn about various problems collecting data.\n\n        Returns a :class:`coverage.CoverageData`, the collected coverage data.\n\n        .. versionadded:: 4.0\n\n        \"\"\"\n        self._init()\n        self._init_data(suffix=None)\n        self._post_init()\n\n        if self._collector is not None:\n            for plugin in self._plugins:\n                if not plugin._coverage_enabled:\n                    self._collector.plugin_was_disabled(plugin)\n\n            if self._collector.flush_data():\n                self._post_save_work()\n\n        assert self._data is not None\n        return self._data\n\n    def _post_save_work(self) -> None:\n        \"\"\"After saving data, look for warnings, post-work, etc.\n\n        Warn about things that should have happened but didn't.\n        Look for un-executed files.\n\n        \"\"\"\n        assert self._data is not None\n        assert self._inorout is not None\n\n        # If there are still entries in the source_pkgs_unmatched list,\n        # then we never encountered those packages.\n        if self._warn_unimported_source:\n            self._inorout.warn_unimported_source()\n\n        # Find out if we got any data.\n        if not self._data and self._warn_no_data:\n            self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n\n        # Touch all the files that could have executed, so that we can\n        # mark completely un-executed files as 0% covered.\n        file_paths = collections.defaultdict(list)\n        for file_path, plugin_name in self._inorout.find_possibly_unexecuted_files():\n            file_path = self._file_mapper(file_path)\n            file_paths[plugin_name].append(file_path)\n        for plugin_name, paths in file_paths.items():\n            self._data.touch_files(paths, plugin_name)\n\n    # Backward compatibility with version 1.\n    def analysis(self, morf: TMorf) -> tuple[str, list[TLineNo], list[TLineNo], str]:\n        \"\"\"Like `analysis2` but doesn't return excluded line numbers.\"\"\"\n        f, s, _, m, mf = self.analysis2(morf)\n        return f, s, m, mf\n\n    def analysis2(\n        self,\n        morf: TMorf,\n    ) -> tuple[str, list[TLineNo], list[TLineNo], list[TLineNo], str]:\n        \"\"\"Analyze a module.\n\n        `morf` is a module or a file name.  It will be analyzed to determine\n        its coverage statistics.  The return value is a 5-tuple:\n\n        * The file name for the module.\n        * A list of line numbers of executable statements.\n        * A list of line numbers of excluded statements.\n        * A list of line numbers of statements not run (missing from\n          execution).\n        * A readable formatted string of the missing line numbers.\n\n        The analysis uses the source file itself and the current measured\n        coverage data.\n\n        \"\"\"\n        analysis = self._analyze(morf)\n        return (\n            analysis.filename,\n            sorted(analysis.statements),\n            sorted(analysis.excluded),\n            sorted(analysis.missing),\n            analysis.missing_formatted(),\n        )\n\n    def _analyze(self, morf: TMorf) -> Analysis:\n        \"\"\"Analyze a module or file.  Private for now.\"\"\"\n        self._init()\n        self._post_init()\n\n        data = self.get_data()\n        file_reporter = self._get_file_reporter(morf)\n        filename = self._file_mapper(file_reporter.filename)\n        return analysis_from_file_reporter(data, self.config.precision, file_reporter, filename)\n\n    @functools.lru_cache(maxsize=1)\n    def _get_file_reporter(self, morf: TMorf) -> FileReporter:\n        \"\"\"Get a FileReporter for a module or file name.\"\"\"\n        assert self._data is not None\n        plugin = None\n        file_reporter: str | FileReporter = \"python\"\n\n        if isinstance(morf, str):\n            mapped_morf = self._file_mapper(morf)\n            plugin_name = self._data.file_tracer(mapped_morf)\n            if plugin_name:\n                plugin = self._plugins.get(plugin_name)\n\n                if plugin:\n                    file_reporter = plugin.file_reporter(mapped_morf)\n                    if file_reporter is None:\n                        raise PluginError(\n                            \"Plugin {!r} did not provide a file reporter for {!r}.\".format(\n                                plugin._coverage_plugin_name, morf,\n                            ),\n                        )\n\n        if file_reporter == \"python\":\n            file_reporter = PythonFileReporter(morf, self)\n\n        assert isinstance(file_reporter, FileReporter)\n        return file_reporter\n\n    def _get_file_reporters(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n    ) -> list[tuple[FileReporter, TMorf]]:\n        \"\"\"Get FileReporters for a list of modules or file names.\n\n        For each module or file name in `morfs`, find a FileReporter.  Return\n        a list pairing FileReporters with the morfs.\n\n        If `morfs` is a single module or file name, this returns a list of one\n        FileReporter.  If `morfs` is empty or None, then the list of all files\n        measured is used to find the FileReporters.\n\n        \"\"\"\n        assert self._data is not None\n        if not morfs:\n            morfs = self._data.measured_files()\n\n        # Be sure we have a collection.\n        if not isinstance(morfs, (list, tuple, set)):\n            morfs = [morfs]     # type: ignore[list-item]\n\n        return [(self._get_file_reporter(morf), morf) for morf in morfs]\n\n    def _prepare_data_for_reporting(self) -> None:\n        \"\"\"Re-map data before reporting, to get implicit \"combine\" behavior.\"\"\"\n        if self.config.paths:\n            mapped_data = CoverageData(warn=self._warn, debug=self._debug, no_disk=True)\n            if self._data is not None:\n                mapped_data.update(self._data, map_path=self._make_aliases().map)\n            self._data = mapped_data\n\n    def report(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n        show_missing: bool | None = None,\n        ignore_errors: bool | None = None,\n        file: IO[str] | None = None,\n        omit: str | list[str] | None = None,\n        include: str | list[str] | None = None,\n        skip_covered: bool | None = None,\n        contexts: list[str] | None = None,\n        skip_empty: bool | None = None,\n        precision: int | None = None,\n        sort: str | None = None,\n        output_format: str | None = None,\n    ) -> float:\n        \"\"\"Write a textual summary report to `file`.\n\n        Each module in `morfs` is listed, with counts of statements, executed\n        statements, missing statements, and a list of lines missed.\n\n        If `show_missing` is true, then details of which lines or branches are\n        missing will be included in the report.  If `ignore_errors` is true,\n        then a failure while reporting a single file will not stop the entire\n        report.\n\n        `file` is a file-like object, suitable for writing.\n\n        `output_format` determines the format, either \"text\" (the default),\n        \"markdown\", or \"total\".\n\n        `include` is a list of file name patterns.  Files that match will be\n        included in the report. Files matching `omit` will not be included in\n        the report.\n\n        If `skip_covered` is true, don't report on files with 100% coverage.\n\n        If `skip_empty` is true, don't report on empty files (those that have\n        no statements).\n\n        `contexts` is a list of regular expression strings.  Only data from\n        :ref:`dynamic contexts <dynamic_contexts>` that match one of those\n        expressions (using :func:`re.search <python:re.search>`) will be\n        included in the report.\n\n        `precision` is the number of digits to display after the decimal\n        point for percentages.\n\n        All of the arguments default to the settings read from the\n        :ref:`configuration file <config>`.\n\n        Returns a float, the total percentage covered.\n\n        .. versionadded:: 4.0\n            The `skip_covered` parameter.\n\n        .. versionadded:: 5.0\n            The `contexts` and `skip_empty` parameters.\n\n        .. versionadded:: 5.2\n            The `precision` parameter.\n\n        .. versionadded:: 7.0\n            The `format` parameter.\n\n        \"\"\"\n        self._prepare_data_for_reporting()\n        with override_config(\n            self,\n            ignore_errors=ignore_errors,\n            report_omit=omit,\n            report_include=include,\n            show_missing=show_missing,\n            skip_covered=skip_covered,\n            report_contexts=contexts,\n            skip_empty=skip_empty,\n            precision=precision,\n            sort=sort,\n            format=output_format,\n        ):\n            reporter = SummaryReporter(self)\n            return reporter.report(morfs, outfile=file)\n\n    def annotate(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n        directory: str | None = None,\n        ignore_errors: bool | None = None,\n        omit: str | list[str] | None = None,\n        include: str | list[str] | None = None,\n        contexts: list[str] | None = None,\n    ) -> None:\n        \"\"\"Annotate a list of modules.\n\n        Each module in `morfs` is annotated.  The source is written to a new\n        file, named with a \",cover\" suffix, with each line prefixed with a\n        marker to indicate the coverage of the line.  Covered lines have \">\",\n        excluded lines have \"-\", and missing lines have \"!\".\n\n        See :meth:`report` for other arguments.\n\n        \"\"\"\n        self._prepare_data_for_reporting()\n        with override_config(\n            self,\n            ignore_errors=ignore_errors,\n            report_omit=omit,\n            report_include=include,\n            report_contexts=contexts,\n        ):\n            reporter = AnnotateReporter(self)\n            reporter.report(morfs, directory=directory)\n\n    def html_report(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n        directory: str | None = None,\n        ignore_errors: bool | None = None,\n        omit: str | list[str] | None = None,\n        include: str | list[str] | None = None,\n        extra_css: str | None = None,\n        title: str | None = None,\n        skip_covered: bool | None = None,\n        show_contexts: bool | None = None,\n        contexts: list[str] | None = None,\n        skip_empty: bool | None = None,\n        precision: int | None = None,\n    ) -> float:\n        \"\"\"Generate an HTML report.\n\n        The HTML is written to `directory`.  The file \"index.html\" is the\n        overview starting point, with links to more detailed pages for\n        individual modules.\n\n        `extra_css` is a path to a file of other CSS to apply on the page.\n        It will be copied into the HTML directory.\n\n        `title` is a text string (not HTML) to use as the title of the HTML\n        report.\n\n        See :meth:`report` for other arguments.\n\n        Returns a float, the total percentage covered.\n\n        .. note::\n\n            The HTML report files are generated incrementally based on the\n            source files and coverage results. If you modify the report files,\n            the changes will not be considered.  You should be careful about\n            changing the files in the report folder.\n\n        \"\"\"\n        self._prepare_data_for_reporting()\n        with override_config(\n            self,\n            ignore_errors=ignore_errors,\n            report_omit=omit,\n            report_include=include,\n            html_dir=directory,\n            extra_css=extra_css,\n            html_title=title,\n            html_skip_covered=skip_covered,\n            show_contexts=show_contexts,\n            report_contexts=contexts,\n            html_skip_empty=skip_empty,\n            precision=precision,\n        ):\n            reporter = HtmlReporter(self)\n            ret = reporter.report(morfs)\n            return ret\n\n    def xml_report(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n        outfile: str | None = None,\n        ignore_errors: bool | None = None,\n        omit: str | list[str] | None = None,\n        include: str | list[str] | None = None,\n        contexts: list[str] | None = None,\n        skip_empty: bool | None = None,\n    ) -> float:\n        \"\"\"Generate an XML report of coverage results.\n\n        The report is compatible with Cobertura reports.\n\n        Each module in `morfs` is included in the report.  `outfile` is the\n        path to write the file to, \"-\" will write to stdout.\n\n        See :meth:`report` for other arguments.\n\n        Returns a float, the total percentage covered.\n\n        \"\"\"\n        self._prepare_data_for_reporting()\n        with override_config(\n            self,\n            ignore_errors=ignore_errors,\n            report_omit=omit,\n            report_include=include,\n            xml_output=outfile,\n            report_contexts=contexts,\n            skip_empty=skip_empty,\n        ):\n            return render_report(self.config.xml_output, XmlReporter(self), morfs, self._message)\n\n    def json_report(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n        outfile: str | None = None,\n        ignore_errors: bool | None = None,\n        omit: str | list[str] | None = None,\n        include: str | list[str] | None = None,\n        contexts: list[str] | None = None,\n        pretty_print: bool | None = None,\n        show_contexts: bool | None = None,\n    ) -> float:\n        \"\"\"Generate a JSON report of coverage results.\n\n        Each module in `morfs` is included in the report.  `outfile` is the\n        path to write the file to, \"-\" will write to stdout.\n\n        `pretty_print` is a boolean, whether to pretty-print the JSON output or not.\n\n        See :meth:`report` for other arguments.\n\n        Returns a float, the total percentage covered.\n\n        .. versionadded:: 5.0\n\n        \"\"\"\n        self._prepare_data_for_reporting()\n        with override_config(\n            self,\n            ignore_errors=ignore_errors,\n            report_omit=omit,\n            report_include=include,\n            json_output=outfile,\n            report_contexts=contexts,\n            json_pretty_print=pretty_print,\n            json_show_contexts=show_contexts,\n        ):\n            return render_report(self.config.json_output, JsonReporter(self), morfs, self._message)\n\n    def lcov_report(\n        self,\n        morfs: Iterable[TMorf] | None = None,\n        outfile: str | None = None,\n        ignore_errors: bool | None = None,\n        omit: str | list[str] | None = None,\n        include: str | list[str] | None = None,\n        contexts: list[str] | None = None,\n    ) -> float:\n        \"\"\"Generate an LCOV report of coverage results.\n\n        Each module in `morfs` is included in the report. `outfile` is the\n        path to write the file to, \"-\" will write to stdout.\n\n        See :meth:`report` for other arguments.\n\n        .. versionadded:: 6.3\n        \"\"\"\n        self._prepare_data_for_reporting()\n        with override_config(\n            self,\n            ignore_errors=ignore_errors,\n            report_omit=omit,\n            report_include=include,\n            lcov_output=outfile,\n            report_contexts=contexts,\n        ):\n            return render_report(self.config.lcov_output, LcovReporter(self), morfs, self._message)\n\n    def sys_info(self) -> Iterable[tuple[str, Any]]:\n        \"\"\"Return a list of (key, value) pairs showing internal information.\"\"\"\n\n        import coverage as covmod\n\n        self._init()\n        self._post_init()\n\n        def plugin_info(plugins: list[Any]) -> list[str]:\n            \"\"\"Make an entry for the sys_info from a list of plug-ins.\"\"\"\n            entries = []\n            for plugin in plugins:\n                entry = plugin._coverage_plugin_name\n                if not plugin._coverage_enabled:\n                    entry += \" (disabled)\"\n                entries.append(entry)\n            return entries\n\n        info = [\n            (\"coverage_version\", covmod.__version__),\n            (\"coverage_module\", covmod.__file__),\n            (\"core\", self._collector.tracer_name() if self._collector is not None else \"-none-\"),\n            (\"CTracer\", \"available\" if HAS_CTRACER else \"unavailable\"),\n            (\"plugins.file_tracers\", plugin_info(self._plugins.file_tracers)),\n            (\"plugins.configurers\", plugin_info(self._plugins.configurers)),\n            (\"plugins.context_switchers\", plugin_info(self._plugins.context_switchers)),\n            (\"configs_attempted\", self.config.config_files_attempted),\n            (\"configs_read\", self.config.config_files_read),\n            (\"config_file\", self.config.config_file),\n            (\"config_contents\",\n                repr(self.config._config_contents) if self.config._config_contents else \"-none-\",\n            ),\n            (\"data_file\", self._data.data_filename() if self._data is not None else \"-none-\"),\n            (\"python\", sys.version.replace(\"\\n\", \"\")),\n            (\"platform\", platform.platform()),\n            (\"implementation\", platform.python_implementation()),\n            (\"gil_enabled\", getattr(sys, '_is_gil_enabled', lambda: True)()),\n            (\"executable\", sys.executable),\n            (\"def_encoding\", sys.getdefaultencoding()),\n            (\"fs_encoding\", sys.getfilesystemencoding()),\n            (\"pid\", os.getpid()),\n            (\"cwd\", os.getcwd()),\n            (\"path\", sys.path),\n            (\"environment\", [f\"{k} = {v}\" for k, v in relevant_environment_display(os.environ)]),\n            (\"command_line\", \" \".join(getattr(sys, \"argv\", [\"-none-\"]))),\n        ]\n\n        if self._inorout is not None:\n            info.extend(self._inorout.sys_info())\n\n        info.extend(CoverageData.sys_info())\n\n        return info\n\n\n# Mega debugging...\n# $set_env.py: COVERAGE_DEBUG_CALLS - Lots and lots of output about calls to Coverage.\nif int(os.getenv(\"COVERAGE_DEBUG_CALLS\", 0)):               # pragma: debugging\n    from coverage.debug import decorate_methods, show_calls\n\n    Coverage = decorate_methods(        # type: ignore[misc]\n        show_calls(show_args=True),\n        butnot=[\"get_data\"],\n    )(Coverage)\n\n\ndef process_startup() -> Coverage | None:\n    \"\"\"Call this at Python start-up to perhaps measure coverage.\n\n    If the environment variable COVERAGE_PROCESS_START is defined, coverage\n    measurement is started.  The value of the variable is the config file\n    to use.\n\n    There are two ways to configure your Python installation to invoke this\n    function when Python starts:\n\n    #. Create or append to sitecustomize.py to add these lines::\n\n        import coverage\n        coverage.process_startup()\n\n    #. Create a .pth file in your Python installation containing::\n\n        import coverage; coverage.process_startup()\n\n    Returns the :class:`Coverage` instance that was started, or None if it was\n    not started by this call.\n\n    \"\"\"\n    cps = os.getenv(\"COVERAGE_PROCESS_START\")\n    if not cps:\n        # No request for coverage, nothing to do.\n        return None\n\n    # This function can be called more than once in a process. This happens\n    # because some virtualenv configurations make the same directory visible\n    # twice in sys.path.  This means that the .pth file will be found twice,\n    # and executed twice, executing this function twice.  We set a global\n    # flag (an attribute on this function) to indicate that coverage.py has\n    # already been started, so we can avoid doing it twice.\n    #\n    # https://github.com/nedbat/coveragepy/issues/340 has more details.\n\n    if hasattr(process_startup, \"coverage\"):\n        # We've annotated this function before, so we must have already\n        # started coverage.py in this process.  Nothing to do.\n        return None\n\n    cov = Coverage(config_file=cps)\n    process_startup.coverage = cov      # type: ignore[attr-defined]\n    cov._warn_no_data = False\n    cov._warn_unimported_source = False\n    cov._warn_preimported_source = False\n    cov._auto_save = True\n    cov.start()\n\n    return cov\n\n\ndef _prevent_sub_process_measurement() -> None:\n    \"\"\"Stop any subprocess auto-measurement from writing data.\"\"\"\n    auto_created_coverage = getattr(process_startup, \"coverage\", None)\n    if auto_created_coverage is not None:\n        auto_created_coverage._auto_save = False\n", "coverage/__main__.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Coverage.py's main entry point.\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\nfrom coverage.cmdline import main\nsys.exit(main())\n", "coverage/multiproc.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Monkey-patching to add multiprocessing support for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport multiprocessing\nimport multiprocessing.process\nimport os\nimport os.path\nimport sys\nimport traceback\n\nfrom typing import Any\n\nfrom coverage.debug import DebugControl\n\n# An attribute that will be set on the module to indicate that it has been\n# monkey-patched.\nPATCHED_MARKER = \"_coverage$patched\"\n\n\nOriginalProcess = multiprocessing.process.BaseProcess\noriginal_bootstrap = OriginalProcess._bootstrap     # type: ignore[attr-defined]\n\nclass ProcessWithCoverage(OriginalProcess):         # pylint: disable=abstract-method\n    \"\"\"A replacement for multiprocess.Process that starts coverage.\"\"\"\n\n    def _bootstrap(self, *args, **kwargs):          # type: ignore[no-untyped-def]\n        \"\"\"Wrapper around _bootstrap to start coverage.\"\"\"\n        debug: DebugControl | None = None\n        try:\n            from coverage import Coverage       # avoid circular import\n            cov = Coverage(data_suffix=True, auto_data=True)\n            cov._warn_preimported_source = False\n            cov.start()\n            _debug = cov._debug\n            assert _debug is not None\n            if _debug.should(\"multiproc\"):\n                debug = _debug\n            if debug:\n                debug.write(\"Calling multiprocessing bootstrap\")\n        except Exception:\n            print(\"Exception during multiprocessing bootstrap init:\", file=sys.stderr)\n            traceback.print_exc(file=sys.stderr)\n            sys.stderr.flush()\n            raise\n        try:\n            return original_bootstrap(self, *args, **kwargs)\n        finally:\n            if debug:\n                debug.write(\"Finished multiprocessing bootstrap\")\n            try:\n                cov.stop()\n                cov.save()\n            except Exception as exc:\n                if debug:\n                    debug.write(\"Exception during multiprocessing bootstrap cleanup\", exc=exc)\n                raise\n            if debug:\n                debug.write(\"Saved multiprocessing data\")\n\nclass Stowaway:\n    \"\"\"An object to pickle, so when it is unpickled, it can apply the monkey-patch.\"\"\"\n    def __init__(self, rcfile: str) -> None:\n        self.rcfile = rcfile\n\n    def __getstate__(self) -> dict[str, str]:\n        return {\"rcfile\": self.rcfile}\n\n    def __setstate__(self, state: dict[str, str]) -> None:\n        patch_multiprocessing(state[\"rcfile\"])\n\n\ndef patch_multiprocessing(rcfile: str) -> None:\n    \"\"\"Monkey-patch the multiprocessing module.\n\n    This enables coverage measurement of processes started by multiprocessing.\n    This involves aggressive monkey-patching.\n\n    `rcfile` is the path to the rcfile being used.\n\n    \"\"\"\n\n    if hasattr(multiprocessing, PATCHED_MARKER):\n        return\n\n    OriginalProcess._bootstrap = ProcessWithCoverage._bootstrap     # type: ignore[attr-defined]\n\n    # Set the value in ProcessWithCoverage that will be pickled into the child\n    # process.\n    os.environ[\"COVERAGE_RCFILE\"] = os.path.abspath(rcfile)\n\n    # When spawning processes rather than forking them, we have no state in the\n    # new process.  We sneak in there with a Stowaway: we stuff one of our own\n    # objects into the data that gets pickled and sent to the sub-process. When\n    # the Stowaway is unpickled, its __setstate__ method is called, which\n    # re-applies the monkey-patch.\n    # Windows only spawns, so this is needed to keep Windows working.\n    try:\n        from multiprocessing import spawn\n        original_get_preparation_data = spawn.get_preparation_data\n    except (ImportError, AttributeError):\n        pass\n    else:\n        def get_preparation_data_with_stowaway(name: str) -> dict[str, Any]:\n            \"\"\"Get the original preparation data, and also insert our stowaway.\"\"\"\n            d = original_get_preparation_data(name)\n            d[\"stowaway\"] = Stowaway(rcfile)\n            return d\n\n        spawn.get_preparation_data = get_preparation_data_with_stowaway\n\n    setattr(multiprocessing, PATCHED_MARKER, True)\n", "coverage/env.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Determine facts about the environment.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport platform\nimport sys\n\nfrom typing import Any, Iterable\n\n# debug_info() at the bottom wants to show all the globals, but not imports.\n# Grab the global names here to know which names to not show. Nothing defined\n# above this line will be in the output.\n_UNINTERESTING_GLOBALS = list(globals())\n# These names also shouldn't be shown.\n_UNINTERESTING_GLOBALS += [\"PYBEHAVIOR\", \"debug_info\"]\n\n# Operating systems.\nWINDOWS = sys.platform == \"win32\"\nLINUX = sys.platform.startswith(\"linux\")\nOSX = sys.platform == \"darwin\"\n\n# Python implementations.\nCPYTHON = (platform.python_implementation() == \"CPython\")\nPYPY = (platform.python_implementation() == \"PyPy\")\n\n# Python versions. We amend version_info with one more value, a zero if an\n# official version, or 1 if built from source beyond an official version.\n# Only use sys.version_info directly where tools like mypy need it to understand\n# version-specfic code, otherwise use PYVERSION.\nPYVERSION = sys.version_info + (int(platform.python_version()[-1] == \"+\"),)\n\nif PYPY:\n    PYPYVERSION = sys.pypy_version_info         # type: ignore[attr-defined]\nelse:\n    PYPYVERSION = (0,)\n\n# Python behavior.\nclass PYBEHAVIOR:\n    \"\"\"Flags indicating this Python's behavior.\"\"\"\n\n    # Does Python conform to PEP626, Precise line numbers for debugging and other tools.\n    # https://www.python.org/dev/peps/pep-0626\n    pep626 = (PYVERSION > (3, 10, 0, \"alpha\", 4))\n\n    # Is \"if __debug__\" optimized away?\n    optimize_if_debug = not pep626\n\n    # Is \"if not __debug__\" optimized away? The exact details have changed\n    # across versions.\n    if pep626:\n        optimize_if_not_debug = 1\n    elif PYPY:\n        if PYVERSION >= (3, 9):\n            optimize_if_not_debug = 2\n        else:\n            optimize_if_not_debug = 3\n    else:\n        optimize_if_not_debug = 2\n\n    # 3.7 changed how functions with only docstrings are numbered.\n    docstring_only_function = (not PYPY) and (PYVERSION <= (3, 10))\n\n    # When a break/continue/return statement in a try block jumps to a finally\n    # block, does the finally jump back to the break/continue/return (pre-3.10)\n    # to do the work?\n    finally_jumps_back = (PYVERSION < (3, 10))\n\n    # CPython 3.11 now jumps to the decorator line again while executing\n    # the decorator.\n    trace_decorator_line_again = (CPYTHON and PYVERSION > (3, 11, 0, \"alpha\", 3, 0))\n\n    # CPython 3.9a1 made sys.argv[0] and other reported files absolute paths.\n    report_absolute_files = (\n        (CPYTHON or (PYPY and PYPYVERSION >= (7, 3, 10)))\n        and PYVERSION >= (3, 9)\n    )\n\n    # Lines after break/continue/return/raise are no longer compiled into the\n    # bytecode.  They used to be marked as missing, now they aren't executable.\n    omit_after_jump = (\n        pep626\n        or (PYPY and PYVERSION >= (3, 9) and PYPYVERSION >= (7, 3, 12))\n    )\n\n    # PyPy has always omitted statements after return.\n    omit_after_return = omit_after_jump or PYPY\n\n    # Optimize away unreachable try-else clauses.\n    optimize_unreachable_try_else = pep626\n\n    # Modules used to have firstlineno equal to the line number of the first\n    # real line of code.  Now they always start at 1.\n    module_firstline_1 = pep626\n\n    # Are \"if 0:\" lines (and similar) kept in the compiled code?\n    keep_constant_test = pep626\n\n    # When leaving a with-block, do we visit the with-line again for the exit?\n    exit_through_with = (PYVERSION >= (3, 10, 0, \"beta\"))\n\n    # Match-case construct.\n    match_case = (PYVERSION >= (3, 10))\n\n    # Some words are keywords in some places, identifiers in other places.\n    soft_keywords = (PYVERSION >= (3, 10))\n\n    # Modules start with a line numbered zero. This means empty modules have\n    # only a 0-number line, which is ignored, giving a truly empty module.\n    empty_is_empty = (PYVERSION >= (3, 11, 0, \"beta\", 4))\n\n    # Are comprehensions inlined (new) or compiled as called functions (old)?\n    # Changed in https://github.com/python/cpython/pull/101441\n    comprehensions_are_functions = (PYVERSION <= (3, 12, 0, \"alpha\", 7, 0))\n\n    # PEP669 Low Impact Monitoring: https://peps.python.org/pep-0669/\n    pep669 = bool(getattr(sys, \"monitoring\", None))\n\n    # Where does frame.f_lasti point when yielding from a generator?\n    # It used to point at the YIELD, now it points at the RESUME.\n    # https://github.com/python/cpython/issues/113728\n    lasti_is_yield = (PYVERSION < (3, 13))\n\n\n# Coverage.py specifics, about testing scenarios. See tests/testenv.py also.\n\n# Are we coverage-measuring ourselves?\nMETACOV = os.getenv(\"COVERAGE_COVERAGE\") is not None\n\n# Are we running our test suite?\n# Even when running tests, you can use COVERAGE_TESTING=0 to disable the\n# test-specific behavior like AST checking.\nTESTING = os.getenv(\"COVERAGE_TESTING\") == \"True\"\n\n\ndef debug_info() -> Iterable[tuple[str, Any]]:\n    \"\"\"Return a list of (name, value) pairs for printing debug information.\"\"\"\n    info = [\n        (name, value) for name, value in globals().items()\n        if not name.startswith(\"_\") and name not in _UNINTERESTING_GLOBALS\n    ]\n    info += [\n        (name, value) for name, value in PYBEHAVIOR.__dict__.items()\n        if not name.startswith(\"_\")\n    ]\n    return sorted(info)\n", "coverage/__init__.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nCode coverage measurement for Python.\n\nNed Batchelder\nhttps://coverage.readthedocs.io\n\n\"\"\"\n\nfrom __future__ import annotations\n\n# mypy's convention is that \"import as\" names are public from the module.\n# We import names as themselves to indicate that. Pylint sees it as pointless,\n# so disable its warning.\n# pylint: disable=useless-import-alias\n\nfrom coverage.version import (\n    __version__ as __version__,\n    version_info as version_info,\n)\n\nfrom coverage.control import (\n    Coverage as Coverage,\n    process_startup as process_startup,\n)\nfrom coverage.data import CoverageData as CoverageData\nfrom coverage.exceptions import CoverageException as CoverageException\nfrom coverage.plugin import (\n    CodeRegion as CodeRegion,\n    CoveragePlugin as CoveragePlugin,\n    FileReporter as FileReporter,\n    FileTracer as FileTracer,\n)\n\n# Backward compatibility.\ncoverage = Coverage\n", "coverage/context.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Determine contexts for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nfrom types import FrameType\nfrom typing import cast, Callable, Sequence\n\n\ndef combine_context_switchers(\n    context_switchers: Sequence[Callable[[FrameType], str | None]],\n) -> Callable[[FrameType], str | None] | None:\n    \"\"\"Create a single context switcher from multiple switchers.\n\n    `context_switchers` is a list of functions that take a frame as an\n    argument and return a string to use as the new context label.\n\n    Returns a function that composites `context_switchers` functions, or None\n    if `context_switchers` is an empty list.\n\n    When invoked, the combined switcher calls `context_switchers` one-by-one\n    until a string is returned.  The combined switcher returns None if all\n    `context_switchers` return None.\n    \"\"\"\n    if not context_switchers:\n        return None\n\n    if len(context_switchers) == 1:\n        return context_switchers[0]\n\n    def should_start_context(frame: FrameType) -> str | None:\n        \"\"\"The combiner for multiple context switchers.\"\"\"\n        for switcher in context_switchers:\n            new_context = switcher(frame)\n            if new_context is not None:\n                return new_context\n        return None\n\n    return should_start_context\n\n\ndef should_start_context_test_function(frame: FrameType) -> str | None:\n    \"\"\"Is this frame calling a test_* function?\"\"\"\n    co_name = frame.f_code.co_name\n    if co_name.startswith(\"test\") or co_name == \"runTest\":\n        return qualname_from_frame(frame)\n    return None\n\n\ndef qualname_from_frame(frame: FrameType) -> str | None:\n    \"\"\"Get a qualified name for the code running in `frame`.\"\"\"\n    co = frame.f_code\n    fname = co.co_name\n    method = None\n    if co.co_argcount and co.co_varnames[0] == \"self\":\n        self = frame.f_locals.get(\"self\", None)\n        method = getattr(self, fname, None)\n\n    if method is None:\n        func = frame.f_globals.get(fname)\n        if func is None:\n            return None\n        return cast(str, func.__module__ + \".\" + fname)\n\n    func = getattr(method, \"__func__\", None)\n    if func is None:\n        cls = self.__class__\n        return cast(str, cls.__module__ + \".\" + cls.__name__ + \".\" + fname)\n\n    return cast(str, func.__module__ + \".\" + func.__qualname__)\n", "coverage/parser.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Code parsing for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport functools\nimport collections\nimport os\nimport re\nimport sys\nimport token\nimport tokenize\n\nfrom dataclasses import dataclass\nfrom types import CodeType\nfrom typing import (\n    cast, Any, Callable, Dict, Iterable, List, Optional, Protocol, Sequence,\n    Set, Tuple,\n)\n\nfrom coverage import env\nfrom coverage.bytecode import code_objects\nfrom coverage.debug import short_stack\nfrom coverage.exceptions import NoSource, NotPython\nfrom coverage.misc import nice_pair\nfrom coverage.phystokens import generate_tokens\nfrom coverage.types import TArc, TLineNo\n\n\nclass PythonParser:\n    \"\"\"Parse code to find executable lines, excluded lines, etc.\n\n    This information is all based on static analysis: no code execution is\n    involved.\n\n    \"\"\"\n    def __init__(\n        self,\n        text: str | None = None,\n        filename: str | None = None,\n        exclude: str | None = None,\n    ) -> None:\n        \"\"\"\n        Source can be provided as `text`, the text itself, or `filename`, from\n        which the text will be read.  Excluded lines are those that match\n        `exclude`, a regex string.\n\n        \"\"\"\n        assert text or filename, \"PythonParser needs either text or filename\"\n        self.filename = filename or \"<code>\"\n        if text is not None:\n            self.text: str = text\n        else:\n            from coverage.python import get_python_source\n            try:\n                self.text = get_python_source(self.filename)\n            except OSError as err:\n                raise NoSource(f\"No source for code: '{self.filename}': {err}\") from err\n\n        self.exclude = exclude\n\n        # The parsed AST of the text.\n        self._ast_root: ast.AST | None = None\n\n        # The normalized line numbers of the statements in the code. Exclusions\n        # are taken into account, and statements are adjusted to their first\n        # lines.\n        self.statements: set[TLineNo] = set()\n\n        # The normalized line numbers of the excluded lines in the code,\n        # adjusted to their first lines.\n        self.excluded: set[TLineNo] = set()\n\n        # The raw_* attributes are only used in this class, and in\n        # lab/parser.py to show how this class is working.\n\n        # The line numbers that start statements, as reported by the line\n        # number table in the bytecode.\n        self.raw_statements: set[TLineNo] = set()\n\n        # The raw line numbers of excluded lines of code, as marked by pragmas.\n        self.raw_excluded: set[TLineNo] = set()\n\n        # The line numbers of class definitions.\n        self.raw_classdefs: set[TLineNo] = set()\n\n        # The line numbers of docstring lines.\n        self.raw_docstrings: set[TLineNo] = set()\n\n        # Internal detail, used by lab/parser.py.\n        self.show_tokens = False\n\n        # A dict mapping line numbers to lexical statement starts for\n        # multi-line statements.\n        self._multiline: dict[TLineNo, TLineNo] = {}\n\n        # Lazily-created arc data, and missing arc descriptions.\n        self._all_arcs: set[TArc] | None = None\n        self._missing_arc_fragments: TArcFragments | None = None\n\n    def lines_matching(self, regex: str) -> set[TLineNo]:\n        \"\"\"Find the lines matching a regex.\n\n        Returns a set of line numbers, the lines that contain a match for\n        `regex`.  The entire line needn't match, just a part of it.\n\n        \"\"\"\n        regex_c = re.compile(regex)\n        matches = set()\n        for i, ltext in enumerate(self.text.split(\"\\n\"), start=1):\n            if regex_c.search(ltext):\n                matches.add(self._multiline.get(i, i))\n        return matches\n\n    def _raw_parse(self) -> None:\n        \"\"\"Parse the source to find the interesting facts about its lines.\n\n        A handful of attributes are updated.\n\n        \"\"\"\n        # Find lines which match an exclusion pattern.\n        if self.exclude:\n            self.raw_excluded = self.lines_matching(self.exclude)\n            self.excluded = set(self.raw_excluded)\n\n        # The current number of indents.\n        indent: int = 0\n        # An exclusion comment will exclude an entire clause at this indent.\n        exclude_indent: int = 0\n        # Are we currently excluding lines?\n        excluding: bool = False\n        # The line number of the first line in a multi-line statement.\n        first_line: int = 0\n        # Is the file empty?\n        empty: bool = True\n        # Parenthesis (and bracket) nesting level.\n        nesting: int = 0\n\n        assert self.text is not None\n        tokgen = generate_tokens(self.text)\n        for toktype, ttext, (slineno, _), (elineno, _), ltext in tokgen:\n            if self.show_tokens:                # pragma: debugging\n                print(\"%10s %5s %-20r %r\" % (\n                    tokenize.tok_name.get(toktype, toktype),\n                    nice_pair((slineno, elineno)), ttext, ltext,\n                ))\n            if toktype == token.INDENT:\n                indent += 1\n            elif toktype == token.DEDENT:\n                indent -= 1\n            elif toktype == token.OP:\n                if ttext == \":\" and nesting == 0:\n                    should_exclude = (\n                        self.excluded.intersection(range(first_line, elineno + 1))\n                    )\n                    if not excluding and should_exclude:\n                        # Start excluding a suite.  We trigger off of the colon\n                        # token so that the #pragma comment will be recognized on\n                        # the same line as the colon.\n                        self.excluded.add(elineno)\n                        exclude_indent = indent\n                        excluding = True\n                elif ttext in \"([{\":\n                    nesting += 1\n                elif ttext in \")]}\":\n                    nesting -= 1\n            elif toktype == token.NEWLINE:\n                if first_line and elineno != first_line:\n                    # We're at the end of a line, and we've ended on a\n                    # different line than the first line of the statement,\n                    # so record a multi-line range.\n                    for l in range(first_line, elineno+1):\n                        self._multiline[l] = first_line\n                first_line = 0\n\n            if ttext.strip() and toktype != tokenize.COMMENT:\n                # A non-white-space token.\n                empty = False\n                if not first_line:\n                    # The token is not white space, and is the first in a statement.\n                    first_line = slineno\n                    # Check whether to end an excluded suite.\n                    if excluding and indent <= exclude_indent:\n                        excluding = False\n                    if excluding:\n                        self.excluded.add(elineno)\n\n        # Find the starts of the executable statements.\n        if not empty:\n            byte_parser = ByteParser(self.text, filename=self.filename)\n            self.raw_statements.update(byte_parser._find_statements())\n\n        # The first line of modules can lie and say 1 always, even if the first\n        # line of code is later. If so, map 1 to the actual first line of the\n        # module.\n        if env.PYBEHAVIOR.module_firstline_1 and self._multiline:\n            self._multiline[1] = min(self.raw_statements)\n\n        self.excluded = self.first_lines(self.excluded)\n\n        # AST lets us find classes, docstrings, and decorator-affected\n        # functions and classes.\n        assert self._ast_root is not None\n        for node in ast.walk(self._ast_root):\n            # Find class definitions.\n            if isinstance(node, ast.ClassDef):\n                self.raw_classdefs.add(node.lineno)\n            # Find docstrings.\n            if isinstance(node, (ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef, ast.Module)):\n                if node.body:\n                    first = node.body[0]\n                    if (\n                        isinstance(first, ast.Expr)\n                        and isinstance(first.value, ast.Constant)\n                        and isinstance(first.value.value, str)\n                    ):\n                        self.raw_docstrings.update(\n                            range(first.lineno, cast(int, first.end_lineno) + 1)\n                        )\n            # Exclusions carry from decorators and signatures to the bodies of\n            # functions and classes.\n            if isinstance(node, (ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef)):\n                first_line = min((d.lineno for d in node.decorator_list), default=node.lineno)\n                if self.excluded.intersection(range(first_line, node.lineno + 1)):\n                    self.excluded.update(range(first_line, cast(int, node.end_lineno) + 1))\n\n    @functools.lru_cache(maxsize=1000)\n    def first_line(self, lineno: TLineNo) -> TLineNo:\n        \"\"\"Return the first line number of the statement including `lineno`.\"\"\"\n        if lineno < 0:\n            lineno = -self._multiline.get(-lineno, -lineno)\n        else:\n            lineno = self._multiline.get(lineno, lineno)\n        return lineno\n\n    def first_lines(self, linenos: Iterable[TLineNo]) -> set[TLineNo]:\n        \"\"\"Map the line numbers in `linenos` to the correct first line of the\n        statement.\n\n        Returns a set of the first lines.\n\n        \"\"\"\n        return {self.first_line(l) for l in linenos}\n\n    def translate_lines(self, lines: Iterable[TLineNo]) -> set[TLineNo]:\n        \"\"\"Implement `FileReporter.translate_lines`.\"\"\"\n        return self.first_lines(lines)\n\n    def translate_arcs(self, arcs: Iterable[TArc]) -> set[TArc]:\n        \"\"\"Implement `FileReporter.translate_arcs`.\"\"\"\n        return {(self.first_line(a), self.first_line(b)) for (a, b) in arcs}\n\n    def parse_source(self) -> None:\n        \"\"\"Parse source text to find executable lines, excluded lines, etc.\n\n        Sets the .excluded and .statements attributes, normalized to the first\n        line of multi-line statements.\n\n        \"\"\"\n        try:\n            self._ast_root = ast.parse(self.text)\n            self._raw_parse()\n        except (tokenize.TokenError, IndentationError, SyntaxError) as err:\n            if hasattr(err, \"lineno\"):\n                lineno = err.lineno         # IndentationError\n            else:\n                lineno = err.args[1][0]     # TokenError\n            raise NotPython(\n                f\"Couldn't parse '{self.filename}' as Python source: \" +\n                f\"{err.args[0]!r} at line {lineno}\",\n            ) from err\n\n        ignore = self.excluded | self.raw_docstrings\n        starts = self.raw_statements - ignore\n        self.statements = self.first_lines(starts) - ignore\n\n    def arcs(self) -> set[TArc]:\n        \"\"\"Get information about the arcs available in the code.\n\n        Returns a set of line number pairs.  Line numbers have been normalized\n        to the first line of multi-line statements.\n\n        \"\"\"\n        if self._all_arcs is None:\n            self._analyze_ast()\n        assert self._all_arcs is not None\n        return self._all_arcs\n\n    def _analyze_ast(self) -> None:\n        \"\"\"Run the AstArcAnalyzer and save its results.\n\n        `_all_arcs` is the set of arcs in the code.\n\n        \"\"\"\n        assert self._ast_root is not None\n        aaa = AstArcAnalyzer(self._ast_root, self.raw_statements, self._multiline)\n        aaa.analyze()\n\n        self._all_arcs = set()\n        for l1, l2 in aaa.arcs:\n            fl1 = self.first_line(l1)\n            fl2 = self.first_line(l2)\n            if fl1 != fl2:\n                self._all_arcs.add((fl1, fl2))\n\n        self._missing_arc_fragments = aaa.missing_arc_fragments\n\n    @functools.lru_cache()\n    def exit_counts(self) -> dict[TLineNo, int]:\n        \"\"\"Get a count of exits from that each line.\n\n        Excluded lines are excluded.\n\n        \"\"\"\n        exit_counts: dict[TLineNo, int] = collections.defaultdict(int)\n        for l1, l2 in self.arcs():\n            if l1 < 0:\n                # Don't ever report -1 as a line number\n                continue\n            if l1 in self.excluded:\n                # Don't report excluded lines as line numbers.\n                continue\n            if l2 in self.excluded:\n                # Arcs to excluded lines shouldn't count.\n                continue\n            exit_counts[l1] += 1\n\n        # Class definitions have one extra exit, so remove one for each:\n        for l in self.raw_classdefs:\n            # Ensure key is there: class definitions can include excluded lines.\n            if l in exit_counts:\n                exit_counts[l] -= 1\n\n        return exit_counts\n\n    def missing_arc_description(\n        self,\n        start: TLineNo,\n        end: TLineNo,\n        executed_arcs: Iterable[TArc] | None = None,\n    ) -> str:\n        \"\"\"Provide an English sentence describing a missing arc.\"\"\"\n        if self._missing_arc_fragments is None:\n            self._analyze_ast()\n            assert self._missing_arc_fragments is not None\n\n        actual_start = start\n\n        if (\n            executed_arcs and\n            end < 0 and end == -start and\n            (end, start) not in executed_arcs and\n            (end, start) in self._missing_arc_fragments\n        ):\n            # It's a one-line callable, and we never even started it,\n            # and we have a message about not starting it.\n            start, end = end, start\n\n        fragment_pairs = self._missing_arc_fragments.get((start, end), [(None, None)])\n\n        msgs = []\n        for smsg, emsg in fragment_pairs:\n            if emsg is None:\n                if end < 0:\n                    # Hmm, maybe we have a one-line callable, let's check.\n                    if (-end, end) in self._missing_arc_fragments:\n                        return self.missing_arc_description(-end, end)\n                    emsg = \"didn't jump to the function exit\"\n                else:\n                    emsg = \"didn't jump to line {lineno}\"\n            emsg = emsg.format(lineno=end)\n\n            msg = f\"line {actual_start} {emsg}\"\n            if smsg is not None:\n                msg += f\" because {smsg.format(lineno=actual_start)}\"\n\n            msgs.append(msg)\n\n        return \" or \".join(msgs)\n\n\nclass ByteParser:\n    \"\"\"Parse bytecode to understand the structure of code.\"\"\"\n\n    def __init__(\n        self,\n        text: str,\n        code: CodeType | None = None,\n        filename: str | None = None,\n    ) -> None:\n        self.text = text\n        if code is not None:\n            self.code = code\n        else:\n            assert filename is not None\n            # We only get here if earlier ast parsing succeeded, so no need to\n            # catch errors.\n            self.code = compile(text, filename, \"exec\", dont_inherit=True)\n\n    def child_parsers(self) -> Iterable[ByteParser]:\n        \"\"\"Iterate over all the code objects nested within this one.\n\n        The iteration includes `self` as its first value.\n\n        \"\"\"\n        return (ByteParser(self.text, code=c) for c in code_objects(self.code))\n\n    def _line_numbers(self) -> Iterable[TLineNo]:\n        \"\"\"Yield the line numbers possible in this code object.\n\n        Uses co_lnotab described in Python/compile.c to find the\n        line numbers.  Produces a sequence: l0, l1, ...\n        \"\"\"\n        if hasattr(self.code, \"co_lines\"):\n            # PYVERSIONS: new in 3.10\n            for _, _, line in self.code.co_lines():\n                if line:\n                    yield line\n        else:\n            # Adapted from dis.py in the standard library.\n            byte_increments = self.code.co_lnotab[0::2]\n            line_increments = self.code.co_lnotab[1::2]\n\n            last_line_num = None\n            line_num = self.code.co_firstlineno\n            byte_num = 0\n            for byte_incr, line_incr in zip(byte_increments, line_increments):\n                if byte_incr:\n                    if line_num != last_line_num:\n                        yield line_num\n                        last_line_num = line_num\n                    byte_num += byte_incr\n                if line_incr >= 0x80:\n                    line_incr -= 0x100\n                line_num += line_incr\n            if line_num != last_line_num:\n                yield line_num\n\n    def _find_statements(self) -> Iterable[TLineNo]:\n        \"\"\"Find the statements in `self.code`.\n\n        Produce a sequence of line numbers that start statements.  Recurses\n        into all code objects reachable from `self.code`.\n\n        \"\"\"\n        for bp in self.child_parsers():\n            # Get all of the lineno information from this code.\n            yield from bp._line_numbers()\n\n\n#\n# AST analysis\n#\n\n@dataclass(frozen=True, order=True)\nclass ArcStart:\n    \"\"\"The information needed to start an arc.\n\n    `lineno` is the line number the arc starts from.\n\n    `cause` is an English text fragment used as the `startmsg` for\n    AstArcAnalyzer.missing_arc_fragments.  It will be used to describe why an\n    arc wasn't executed, so should fit well into a sentence of the form,\n    \"Line 17 didn't run because {cause}.\"  The fragment can include \"{lineno}\"\n    to have `lineno` interpolated into it.\n\n    As an example, this code::\n\n        if something(x):        # line 1\n            func(x)             # line 2\n        more_stuff()            # line 3\n\n    would have two ArcStarts:\n\n    - ArcStart(1, \"the condition on line 1 was always true\")\n    - ArcStart(1, \"the condition on line 1 was never true\")\n\n    The first would be used to create an arc from 1 to 3, creating a message like\n    \"line 1 didn't jump to line 3 because the condition on line 1 was always true.\"\n\n    The second would be used for the arc from 1 to 2, creating a message like\n    \"line 1 didn't jump to line 2 because the condition on line 1 was never true.\"\n\n    \"\"\"\n    lineno: TLineNo\n    cause: str = \"\"\n\n\nclass TAddArcFn(Protocol):\n    \"\"\"The type for AstArcAnalyzer.add_arc().\"\"\"\n    def __call__(\n        self,\n        start: TLineNo,\n        end: TLineNo,\n        smsg: str | None = None,\n        emsg: str | None = None,\n    ) -> None:\n        ...\n\nTArcFragments = Dict[TArc, List[Tuple[Optional[str], Optional[str]]]]\n\nclass Block:\n    \"\"\"\n    Blocks need to handle various exiting statements in their own ways.\n\n    All of these methods take a list of exits, and a callable `add_arc`\n    function that they can use to add arcs if needed.  They return True if the\n    exits are handled, or False if the search should continue up the block\n    stack.\n    \"\"\"\n    # pylint: disable=unused-argument\n    def process_break_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        \"\"\"Process break exits.\"\"\"\n        # Because break can only appear in loops, and most subclasses\n        # implement process_break_exits, this function is never reached.\n        raise AssertionError\n\n    def process_continue_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        \"\"\"Process continue exits.\"\"\"\n        # Because continue can only appear in loops, and most subclasses\n        # implement process_continue_exits, this function is never reached.\n        raise AssertionError\n\n    def process_raise_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        \"\"\"Process raise exits.\"\"\"\n        return False\n\n    def process_return_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        \"\"\"Process return exits.\"\"\"\n        return False\n\n\nclass LoopBlock(Block):\n    \"\"\"A block on the block stack representing a `for` or `while` loop.\"\"\"\n    def __init__(self, start: TLineNo) -> None:\n        # The line number where the loop starts.\n        self.start = start\n        # A set of ArcStarts, the arcs from break statements exiting this loop.\n        self.break_exits: set[ArcStart] = set()\n\n    def process_break_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        self.break_exits.update(exits)\n        return True\n\n    def process_continue_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        for xit in exits:\n            add_arc(xit.lineno, self.start, xit.cause)\n        return True\n\n\nclass FunctionBlock(Block):\n    \"\"\"A block on the block stack representing a function definition.\"\"\"\n    def __init__(self, start: TLineNo, name: str) -> None:\n        # The line number where the function starts.\n        self.start = start\n        # The name of the function.\n        self.name = name\n\n    def process_raise_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        for xit in exits:\n            add_arc(\n                xit.lineno, -self.start, xit.cause,\n                f\"didn't except from function {self.name!r}\",\n            )\n        return True\n\n    def process_return_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        for xit in exits:\n            add_arc(\n                xit.lineno, -self.start, xit.cause,\n                f\"didn't return from function {self.name!r}\",\n            )\n        return True\n\n\nclass TryBlock(Block):\n    \"\"\"A block on the block stack representing a `try` block.\"\"\"\n    def __init__(self, handler_start: TLineNo | None, final_start: TLineNo | None) -> None:\n        # The line number of the first \"except\" handler, if any.\n        self.handler_start = handler_start\n        # The line number of the \"finally:\" clause, if any.\n        self.final_start = final_start\n\n        # The ArcStarts for breaks/continues/returns/raises inside the \"try:\"\n        # that need to route through the \"finally:\" clause.\n        self.break_from: set[ArcStart] = set()\n        self.continue_from: set[ArcStart] = set()\n        self.raise_from: set[ArcStart] = set()\n        self.return_from: set[ArcStart] = set()\n\n    def process_break_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        if self.final_start is not None:\n            self.break_from.update(exits)\n            return True\n        return False\n\n    def process_continue_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        if self.final_start is not None:\n            self.continue_from.update(exits)\n            return True\n        return False\n\n    def process_raise_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        if self.handler_start is not None:\n            for xit in exits:\n                add_arc(xit.lineno, self.handler_start, xit.cause)\n        else:\n            assert self.final_start is not None\n            self.raise_from.update(exits)\n        return True\n\n    def process_return_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        if self.final_start is not None:\n            self.return_from.update(exits)\n            return True\n        return False\n\n\nclass WithBlock(Block):\n    \"\"\"A block on the block stack representing a `with` block.\"\"\"\n    def __init__(self, start: TLineNo) -> None:\n        # We only ever use this block if it is needed, so that we don't have to\n        # check this setting in all the methods.\n        assert env.PYBEHAVIOR.exit_through_with\n\n        # The line number of the with statement.\n        self.start = start\n\n        # The ArcStarts for breaks/continues/returns/raises inside the \"with:\"\n        # that need to go through the with-statement while exiting.\n        self.break_from: set[ArcStart] = set()\n        self.continue_from: set[ArcStart] = set()\n        self.return_from: set[ArcStart] = set()\n\n    def _process_exits(\n        self,\n        exits: set[ArcStart],\n        add_arc: TAddArcFn,\n        from_set: set[ArcStart] | None = None,\n    ) -> bool:\n        \"\"\"Helper to process the four kinds of exits.\"\"\"\n        for xit in exits:\n            add_arc(xit.lineno, self.start, xit.cause)\n        if from_set is not None:\n            from_set.update(exits)\n        return True\n\n    def process_break_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        return self._process_exits(exits, add_arc, self.break_from)\n\n    def process_continue_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        return self._process_exits(exits, add_arc, self.continue_from)\n\n    def process_raise_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        return self._process_exits(exits, add_arc)\n\n    def process_return_exits(self, exits: set[ArcStart], add_arc: TAddArcFn) -> bool:\n        return self._process_exits(exits, add_arc, self.return_from)\n\n\nclass NodeList(ast.AST):\n    \"\"\"A synthetic fictitious node, containing a sequence of nodes.\n\n    This is used when collapsing optimized if-statements, to represent the\n    unconditional execution of one of the clauses.\n\n    \"\"\"\n    def __init__(self, body: Sequence[ast.AST]) -> None:\n        self.body = body\n        self.lineno = body[0].lineno\n\n# TODO: Shouldn't the cause messages join with \"and\" instead of \"or\"?\n\ndef _make_expression_code_method(noun: str) -> Callable[[AstArcAnalyzer, ast.AST], None]:\n    \"\"\"A function to make methods for expression-based callable _code_object__ methods.\"\"\"\n    def _code_object__expression_callable(self: AstArcAnalyzer, node: ast.AST) -> None:\n        start = self.line_for_node(node)\n        self.add_arc(-start, start, None, f\"didn't run the {noun} on line {start}\")\n        self.add_arc(start, -start, None, f\"didn't finish the {noun} on line {start}\")\n    return _code_object__expression_callable\n\n\nclass AstArcAnalyzer:\n    \"\"\"Analyze source text with an AST to find executable code paths.\n\n    The .analyze() method does the work, and populates these attributes:\n\n    `arcs`: a set of (from, to) pairs of the the arcs possible in the code.\n\n    `missing_arc_fragments`: a dict mapping (from, to) arcs to lists of\n    message fragments explaining why the arc is missing from execution::\n\n        { (start, end): [(startmsg, endmsg), ...], }\n\n    For an arc starting from line 17, they should be usable to form complete\n    sentences like: \"Line 17 {endmsg} because {startmsg}\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        root_node: ast.AST,\n        statements: set[TLineNo],\n        multiline: dict[TLineNo, TLineNo],\n    ) -> None:\n        self.root_node = root_node\n        # TODO: I think this is happening in too many places.\n        self.statements = {multiline.get(l, l) for l in statements}\n        self.multiline = multiline\n\n        # Turn on AST dumps with an environment variable.\n        # $set_env.py: COVERAGE_AST_DUMP - Dump the AST nodes when parsing code.\n        dump_ast = bool(int(os.getenv(\"COVERAGE_AST_DUMP\", \"0\")))\n\n        if dump_ast:                                # pragma: debugging\n            # Dump the AST so that failing tests have helpful output.\n            print(f\"Statements: {self.statements}\")\n            print(f\"Multiline map: {self.multiline}\")\n            dumpkw: dict[str, Any] = {}\n            if sys.version_info >= (3, 9):\n                dumpkw[\"indent\"] = 4\n            print(ast.dump(self.root_node, include_attributes=True, **dumpkw))\n\n        self.arcs: set[TArc] = set()\n        self.missing_arc_fragments: TArcFragments = collections.defaultdict(list)\n        self.block_stack: list[Block] = []\n\n        # $set_env.py: COVERAGE_TRACK_ARCS - Trace possible arcs added while parsing code.\n        self.debug = bool(int(os.getenv(\"COVERAGE_TRACK_ARCS\", \"0\")))\n\n    def analyze(self) -> None:\n        \"\"\"Examine the AST tree from `self.root_node` to determine possible arcs.\"\"\"\n        for node in ast.walk(self.root_node):\n            node_name = node.__class__.__name__\n            code_object_handler = getattr(self, \"_code_object__\" + node_name, None)\n            if code_object_handler is not None:\n                code_object_handler(node)\n\n    # Code object dispatchers: _code_object__*\n    #\n    # These methods are used by analyze() as the start of the analysis.\n    # There is one for each construct with a code object.\n\n    def _code_object__Module(self, node: ast.Module) -> None:\n        start = self.line_for_node(node)\n        if node.body:\n            exits = self.body_exits(node.body, from_start=ArcStart(-start))\n            for xit in exits:\n                self.add_arc(xit.lineno, -start, xit.cause, \"didn't exit the module\")\n        else:\n            # Empty module.\n            self.add_arc(-start, start)\n            self.add_arc(start, -start)\n\n    def _code_object__FunctionDef(self, node: ast.FunctionDef) -> None:\n        start = self.line_for_node(node)\n        self.block_stack.append(FunctionBlock(start=start, name=node.name))\n        exits = self.body_exits(node.body, from_start=ArcStart(-start))\n        self.process_return_exits(exits)\n        self.block_stack.pop()\n\n    _code_object__AsyncFunctionDef = _code_object__FunctionDef\n\n    def _code_object__ClassDef(self, node: ast.ClassDef) -> None:\n        start = self.line_for_node(node)\n        self.add_arc(-start, start)\n        exits = self.body_exits(node.body, from_start=ArcStart(start))\n        for xit in exits:\n            self.add_arc(\n                xit.lineno, -start, xit.cause,\n                f\"didn't exit the body of class {node.name!r}\",\n            )\n\n    _code_object__Lambda = _make_expression_code_method(\"lambda\")\n    _code_object__GeneratorExp = _make_expression_code_method(\"generator expression\")\n    if env.PYBEHAVIOR.comprehensions_are_functions:\n        _code_object__DictComp = _make_expression_code_method(\"dictionary comprehension\")\n        _code_object__SetComp = _make_expression_code_method(\"set comprehension\")\n        _code_object__ListComp = _make_expression_code_method(\"list comprehension\")\n\n\n    def add_arc(\n        self,\n        start: TLineNo,\n        end: TLineNo,\n        smsg: str | None = None,\n        emsg: str | None = None,\n    ) -> None:\n        \"\"\"Add an arc, including message fragments to use if it is missing.\"\"\"\n        if self.debug:                      # pragma: debugging\n            print(f\"\\nAdding possible arc: ({start}, {end}): {smsg!r}, {emsg!r}\")\n            print(short_stack())\n        self.arcs.add((start, end))\n\n        if smsg is not None or emsg is not None:\n            self.missing_arc_fragments[(start, end)].append((smsg, emsg))\n\n    def nearest_blocks(self) -> Iterable[Block]:\n        \"\"\"Yield the blocks in nearest-to-farthest order.\"\"\"\n        return reversed(self.block_stack)\n\n    def line_for_node(self, node: ast.AST) -> TLineNo:\n        \"\"\"What is the right line number to use for this node?\n\n        This dispatches to _line__Node functions where needed.\n\n        \"\"\"\n        node_name = node.__class__.__name__\n        handler = cast(\n            Optional[Callable[[ast.AST], TLineNo]],\n            getattr(self, \"_line__\" + node_name, None),\n        )\n        if handler is not None:\n            return handler(node)\n        else:\n            return node.lineno\n\n    # First lines: _line__*\n    #\n    # Dispatched by line_for_node, each method knows how to identify the first\n    # line number in the node, as Python will report it.\n\n    def _line_decorated(self, node: ast.FunctionDef) -> TLineNo:\n        \"\"\"Compute first line number for things that can be decorated (classes and functions).\"\"\"\n        if node.decorator_list:\n            lineno = node.decorator_list[0].lineno\n        else:\n            lineno = node.lineno\n        return lineno\n\n    def _line__Assign(self, node: ast.Assign) -> TLineNo:\n        return self.line_for_node(node.value)\n\n    _line__ClassDef = _line_decorated\n\n    def _line__Dict(self, node: ast.Dict) -> TLineNo:\n        if node.keys:\n            if node.keys[0] is not None:\n                return node.keys[0].lineno\n            else:\n                # Unpacked dict literals `{**{\"a\":1}}` have None as the key,\n                # use the value in that case.\n                return node.values[0].lineno\n        else:\n            return node.lineno\n\n    _line__FunctionDef = _line_decorated\n    _line__AsyncFunctionDef = _line_decorated\n\n    def _line__List(self, node: ast.List) -> TLineNo:\n        if node.elts:\n            return self.line_for_node(node.elts[0])\n        else:\n            return node.lineno\n\n    def _line__Module(self, node: ast.Module) -> TLineNo:\n        if env.PYBEHAVIOR.module_firstline_1:\n            return 1\n        elif node.body:\n            return self.line_for_node(node.body[0])\n        else:\n            # Empty modules have no line number, they always start at 1.\n            return 1\n\n    # The node types that just flow to the next node with no complications.\n    OK_TO_DEFAULT = {\n        \"AnnAssign\", \"Assign\", \"Assert\", \"AugAssign\", \"Delete\", \"Expr\", \"Global\",\n        \"Import\", \"ImportFrom\", \"Nonlocal\", \"Pass\",\n    }\n\n    def node_exits(self, node: ast.AST) -> set[ArcStart]:\n        \"\"\"Find the set of arc starts that exit this node.\n\n        Return a set of ArcStarts, exits from this node to the next. Because a\n        node represents an entire sub-tree (including its children), the exits\n        from a node can be arbitrarily complex::\n\n            if something(1):\n                if other(2):\n                    doit(3)\n                else:\n                    doit(5)\n\n        There are three exits from line 1: they start at lines 1, 3 and 5.\n        There are two exits from line 2: lines 3 and 5.\n\n        \"\"\"\n        node_name = node.__class__.__name__\n        handler = cast(\n            Optional[Callable[[ast.AST], Set[ArcStart]]],\n            getattr(self, \"_handle__\" + node_name, None),\n        )\n        if handler is not None:\n            arc_starts = handler(node)\n        else:\n            # No handler: either it's something that's ok to default (a simple\n            # statement), or it's something we overlooked.\n            if env.TESTING:\n                if node_name not in self.OK_TO_DEFAULT:\n                    raise RuntimeError(f\"*** Unhandled: {node}\")        # pragma: only failure\n\n            # Default for simple statements: one exit from this node.\n            arc_starts = {ArcStart(self.line_for_node(node))}\n        return arc_starts\n\n    def body_exits(\n        self,\n        body: Sequence[ast.AST],\n        from_start: ArcStart | None = None,\n        prev_starts: set[ArcStart] | None = None,\n    ) -> set[ArcStart]:\n        \"\"\"Find arc starts that exit the body of a compound statement.\n\n        `body` is the body node.  `from_start` is a single `ArcStart` that can\n        be the previous line in flow before this body.  `prev_starts` is a set\n        of ArcStarts that can be the previous line.  Only one of them should be\n        given.\n\n        Also records arcs (using `add_arc`) within the body.\n\n        Returns a set of ArcStarts, the exits from this body.\n\n        \"\"\"\n        if prev_starts is None:\n            assert from_start is not None\n            prev_starts = {from_start}\n        else:\n            assert from_start is None\n\n        # Loop over the nodes in the body, making arcs from each one's exits to\n        # the next node.\n        for body_node in body:\n            lineno = self.line_for_node(body_node)\n            first_line = self.multiline.get(lineno, lineno)\n            if first_line not in self.statements:\n                maybe_body_node = self.find_non_missing_node(body_node)\n                if maybe_body_node is None:\n                    continue\n                body_node = maybe_body_node\n                lineno = self.line_for_node(body_node)\n            for prev_start in prev_starts:\n                self.add_arc(prev_start.lineno, lineno, prev_start.cause)\n            prev_starts = self.node_exits(body_node)\n        return prev_starts\n\n    def find_non_missing_node(self, node: ast.AST) -> ast.AST | None:\n        \"\"\"Search `node` looking for a child that has not been optimized away.\n\n        This might return the node you started with, or it will work recursively\n        to find a child node in self.statements.\n\n        Returns a node, or None if none of the node remains.\n\n        \"\"\"\n        # This repeats work just done in body_exits, but this duplication\n        # means we can avoid a function call in the 99.9999% case of not\n        # optimizing away statements.\n        lineno = self.line_for_node(node)\n        first_line = self.multiline.get(lineno, lineno)\n        if first_line in self.statements:\n            return node\n\n        missing_fn = cast(\n            Optional[Callable[[ast.AST], Optional[ast.AST]]],\n            getattr(self, \"_missing__\" + node.__class__.__name__, None),\n        )\n        if missing_fn is not None:\n            ret_node = missing_fn(node)\n        else:\n            ret_node = None\n        return ret_node\n\n    # Missing nodes: _missing__*\n    #\n    # Entire statements can be optimized away by Python. They will appear in\n    # the AST, but not the bytecode.  These functions are called (by\n    # find_non_missing_node) to find a node to use instead of the missing\n    # node.  They can return None if the node should truly be gone.\n\n    def _missing__If(self, node: ast.If) -> ast.AST | None:\n        # If the if-node is missing, then one of its children might still be\n        # here, but not both. So return the first of the two that isn't missing.\n        # Use a NodeList to hold the clauses as a single node.\n        non_missing = self.find_non_missing_node(NodeList(node.body))\n        if non_missing:\n            return non_missing\n        if node.orelse:\n            return self.find_non_missing_node(NodeList(node.orelse))\n        return None\n\n    def _missing__NodeList(self, node: NodeList) -> ast.AST | None:\n        # A NodeList might be a mixture of missing and present nodes. Find the\n        # ones that are present.\n        non_missing_children = []\n        for child in node.body:\n            maybe_child = self.find_non_missing_node(child)\n            if maybe_child is not None:\n                non_missing_children.append(maybe_child)\n\n        # Return the simplest representation of the present children.\n        if not non_missing_children:\n            return None\n        if len(non_missing_children) == 1:\n            return non_missing_children[0]\n        return NodeList(non_missing_children)\n\n    def _missing__While(self, node: ast.While) -> ast.AST | None:\n        body_nodes = self.find_non_missing_node(NodeList(node.body))\n        if not body_nodes:\n            return None\n        # Make a synthetic While-true node.\n        new_while = ast.While()\n        new_while.lineno = body_nodes.lineno\n        new_while.test = ast.Name()\n        new_while.test.lineno = body_nodes.lineno\n        new_while.test.id = \"True\"\n        assert hasattr(body_nodes, \"body\")\n        new_while.body = body_nodes.body\n        new_while.orelse = []\n        return new_while\n\n    def is_constant_expr(self, node: ast.AST) -> str | None:\n        \"\"\"Is this a compile-time constant?\"\"\"\n        node_name = node.__class__.__name__\n        if node_name in [\"Constant\", \"NameConstant\", \"Num\"]:\n            return \"Num\"\n        elif isinstance(node, ast.Name):\n            if node.id in [\"True\", \"False\", \"None\", \"__debug__\"]:\n                return \"Name\"\n        return None\n\n    # In the fullness of time, these might be good tests to write:\n    #   while EXPR:\n    #   while False:\n    #   listcomps hidden deep in other expressions\n    #   listcomps hidden in lists: x = [[i for i in range(10)]]\n    #   nested function definitions\n\n    # Exit processing: process_*_exits\n    #\n    # These functions process the four kinds of jump exits: break, continue,\n    # raise, and return.  To figure out where an exit goes, we have to look at\n    # the block stack context.  For example, a break will jump to the nearest\n    # enclosing loop block, or the nearest enclosing finally block, whichever\n    # is nearer.\n\n    def process_break_exits(self, exits: set[ArcStart]) -> None:\n        \"\"\"Add arcs due to jumps from `exits` being breaks.\"\"\"\n        for block in self.nearest_blocks():                         # pragma: always breaks\n            if block.process_break_exits(exits, self.add_arc):\n                break\n\n    def process_continue_exits(self, exits: set[ArcStart]) -> None:\n        \"\"\"Add arcs due to jumps from `exits` being continues.\"\"\"\n        for block in self.nearest_blocks():                         # pragma: always breaks\n            if block.process_continue_exits(exits, self.add_arc):\n                break\n\n    def process_raise_exits(self, exits: set[ArcStart]) -> None:\n        \"\"\"Add arcs due to jumps from `exits` being raises.\"\"\"\n        for block in self.nearest_blocks():\n            if block.process_raise_exits(exits, self.add_arc):\n                break\n\n    def process_return_exits(self, exits: set[ArcStart]) -> None:\n        \"\"\"Add arcs due to jumps from `exits` being returns.\"\"\"\n        for block in self.nearest_blocks():                         # pragma: always breaks\n            if block.process_return_exits(exits, self.add_arc):\n                break\n\n    # Node handlers: _handle__*\n    #\n    # Each handler deals with a specific AST node type, dispatched from\n    # node_exits.  Handlers return the set of exits from that node, and can\n    # also call self.add_arc to record arcs they find.  These functions mirror\n    # the Python semantics of each syntactic construct.  See the docstring\n    # for node_exits to understand the concept of exits from a node.\n    #\n    # Every node type that represents a statement should have a handler, or it\n    # should be listed in OK_TO_DEFAULT.\n\n    def _handle__Break(self, node: ast.Break) -> set[ArcStart]:\n        here = self.line_for_node(node)\n        break_start = ArcStart(here, cause=\"the break on line {lineno} wasn't executed\")\n        self.process_break_exits({break_start})\n        return set()\n\n    def _handle_decorated(self, node: ast.FunctionDef) -> set[ArcStart]:\n        \"\"\"Add arcs for things that can be decorated (classes and functions).\"\"\"\n        main_line: TLineNo = node.lineno\n        last: TLineNo | None = node.lineno\n        decs = node.decorator_list\n        if decs:\n            last = None\n            for dec_node in decs:\n                dec_start = self.line_for_node(dec_node)\n                if last is not None and dec_start != last:  # type: ignore[unreachable]\n                    self.add_arc(last, dec_start)           # type: ignore[unreachable]\n                last = dec_start\n            assert last is not None\n            self.add_arc(last, main_line)\n            last = main_line\n            if env.PYBEHAVIOR.trace_decorator_line_again:\n                for top, bot in zip(decs, decs[1:]):\n                    self.add_arc(self.line_for_node(bot), self.line_for_node(top))\n                self.add_arc(self.line_for_node(decs[0]), main_line)\n                self.add_arc(main_line, self.line_for_node(decs[-1]))\n            # The definition line may have been missed, but we should have it\n            # in `self.statements`.  For some constructs, `line_for_node` is\n            # not what we'd think of as the first line in the statement, so map\n            # it to the first one.\n            if node.body:\n                body_start = self.line_for_node(node.body[0])\n                body_start = self.multiline.get(body_start, body_start)\n        # The body is handled in collect_arcs.\n        assert last is not None\n        return {ArcStart(last)}\n\n    _handle__ClassDef = _handle_decorated\n\n    def _handle__Continue(self, node: ast.Continue) -> set[ArcStart]:\n        here = self.line_for_node(node)\n        continue_start = ArcStart(here, cause=\"the continue on line {lineno} wasn't executed\")\n        self.process_continue_exits({continue_start})\n        return set()\n\n    def _handle__For(self, node: ast.For) -> set[ArcStart]:\n        start = self.line_for_node(node.iter)\n        self.block_stack.append(LoopBlock(start=start))\n        from_start = ArcStart(start, cause=\"the loop on line {lineno} never started\")\n        exits = self.body_exits(node.body, from_start=from_start)\n        # Any exit from the body will go back to the top of the loop.\n        for xit in exits:\n            self.add_arc(xit.lineno, start, xit.cause)\n        my_block = self.block_stack.pop()\n        assert isinstance(my_block, LoopBlock)\n        exits = my_block.break_exits\n        from_start = ArcStart(start, cause=\"the loop on line {lineno} didn't complete\")\n        if node.orelse:\n            else_exits = self.body_exits(node.orelse, from_start=from_start)\n            exits |= else_exits\n        else:\n            # No else clause: exit from the for line.\n            exits.add(from_start)\n        return exits\n\n    _handle__AsyncFor = _handle__For\n\n    _handle__FunctionDef = _handle_decorated\n    _handle__AsyncFunctionDef = _handle_decorated\n\n    def _handle__If(self, node: ast.If) -> set[ArcStart]:\n        start = self.line_for_node(node.test)\n        from_start = ArcStart(start, cause=\"the condition on line {lineno} was never true\")\n        exits = self.body_exits(node.body, from_start=from_start)\n        from_start = ArcStart(start, cause=\"the condition on line {lineno} was always true\")\n        exits |= self.body_exits(node.orelse, from_start=from_start)\n        return exits\n\n    if sys.version_info >= (3, 10):\n        def _handle__Match(self, node: ast.Match) -> set[ArcStart]:\n            start = self.line_for_node(node)\n            last_start = start\n            exits = set()\n            for case in node.cases:\n                case_start = self.line_for_node(case.pattern)\n                self.add_arc(last_start, case_start, \"the pattern on line {lineno} always matched\")\n                from_start = ArcStart(\n                    case_start,\n                    cause=\"the pattern on line {lineno} never matched\",\n                )\n                exits |= self.body_exits(case.body, from_start=from_start)\n                last_start = case_start\n\n            # case is now the last case, check for wildcard match.\n            pattern = case.pattern      # pylint: disable=undefined-loop-variable\n            while isinstance(pattern, ast.MatchOr):\n                pattern = pattern.patterns[-1]\n            had_wildcard = (\n                isinstance(pattern, ast.MatchAs)\n                and pattern.pattern is None\n                and case.guard is None  # pylint: disable=undefined-loop-variable\n            )\n\n            if not had_wildcard:\n                exits.add(\n                    ArcStart(case_start, cause=\"the pattern on line {lineno} always matched\"),\n                )\n            return exits\n\n    def _handle__NodeList(self, node: NodeList) -> set[ArcStart]:\n        start = self.line_for_node(node)\n        exits = self.body_exits(node.body, from_start=ArcStart(start))\n        return exits\n\n    def _handle__Raise(self, node: ast.Raise) -> set[ArcStart]:\n        here = self.line_for_node(node)\n        raise_start = ArcStart(here, cause=\"the raise on line {lineno} wasn't executed\")\n        self.process_raise_exits({raise_start})\n        # `raise` statement jumps away, no exits from here.\n        return set()\n\n    def _handle__Return(self, node: ast.Return) -> set[ArcStart]:\n        here = self.line_for_node(node)\n        return_start = ArcStart(here, cause=\"the return on line {lineno} wasn't executed\")\n        self.process_return_exits({return_start})\n        # `return` statement jumps away, no exits from here.\n        return set()\n\n    def _handle__Try(self, node: ast.Try) -> set[ArcStart]:\n        if node.handlers:\n            handler_start = self.line_for_node(node.handlers[0])\n        else:\n            handler_start = None\n\n        if node.finalbody:\n            final_start = self.line_for_node(node.finalbody[0])\n        else:\n            final_start = None\n\n        # This is true by virtue of Python syntax: have to have either except\n        # or finally, or both.\n        assert handler_start is not None or final_start is not None\n        try_block = TryBlock(handler_start, final_start)\n        self.block_stack.append(try_block)\n\n        start = self.line_for_node(node)\n        exits = self.body_exits(node.body, from_start=ArcStart(start))\n\n        # We're done with the `try` body, so this block no longer handles\n        # exceptions. We keep the block so the `finally` clause can pick up\n        # flows from the handlers and `else` clause.\n        if node.finalbody:\n            try_block.handler_start = None\n            if node.handlers:\n                # If there are `except` clauses, then raises in the try body\n                # will already jump to them.  Start this set over for raises in\n                # `except` and `else`.\n                try_block.raise_from = set()\n        else:\n            self.block_stack.pop()\n\n        handler_exits: set[ArcStart] = set()\n\n        if node.handlers:\n            last_handler_start: TLineNo | None = None\n            for handler_node in node.handlers:\n                handler_start = self.line_for_node(handler_node)\n                if last_handler_start is not None:\n                    self.add_arc(last_handler_start, handler_start)\n                last_handler_start = handler_start\n                from_cause = \"the exception caught by line {lineno} didn't happen\"\n                from_start = ArcStart(handler_start, cause=from_cause)\n                handler_exits |= self.body_exits(handler_node.body, from_start=from_start)\n\n        if node.orelse:\n            exits = self.body_exits(node.orelse, prev_starts=exits)\n\n        exits |= handler_exits\n\n        if node.finalbody:\n            self.block_stack.pop()\n            final_from = (                  # You can get to the `finally` clause from:\n                exits |                         # the exits of the body or `else` clause,\n                try_block.break_from |          # or a `break`,\n                try_block.continue_from |       # or a `continue`,\n                try_block.raise_from |          # or a `raise`,\n                try_block.return_from           # or a `return`.\n            )\n\n            final_exits = self.body_exits(node.finalbody, prev_starts=final_from)\n\n            if try_block.break_from:\n                if env.PYBEHAVIOR.finally_jumps_back:\n                    for break_line in try_block.break_from:\n                        lineno = break_line.lineno\n                        cause = break_line.cause.format(lineno=lineno)\n                        for final_exit in final_exits:\n                            self.add_arc(final_exit.lineno, lineno, cause)\n                    breaks = try_block.break_from\n                else:\n                    breaks = self._combine_finally_starts(try_block.break_from, final_exits)\n                self.process_break_exits(breaks)\n\n            if try_block.continue_from:\n                if env.PYBEHAVIOR.finally_jumps_back:\n                    for continue_line in try_block.continue_from:\n                        lineno = continue_line.lineno\n                        cause = continue_line.cause.format(lineno=lineno)\n                        for final_exit in final_exits:\n                            self.add_arc(final_exit.lineno, lineno, cause)\n                    continues = try_block.continue_from\n                else:\n                    continues = self._combine_finally_starts(try_block.continue_from, final_exits)\n                self.process_continue_exits(continues)\n\n            if try_block.raise_from:\n                self.process_raise_exits(\n                    self._combine_finally_starts(try_block.raise_from, final_exits),\n                )\n\n            if try_block.return_from:\n                if env.PYBEHAVIOR.finally_jumps_back:\n                    for return_line in try_block.return_from:\n                        lineno = return_line.lineno\n                        cause = return_line.cause.format(lineno=lineno)\n                        for final_exit in final_exits:\n                            self.add_arc(final_exit.lineno, lineno, cause)\n                    returns = try_block.return_from\n                else:\n                    returns = self._combine_finally_starts(try_block.return_from, final_exits)\n                self.process_return_exits(returns)\n\n            if exits:\n                # The finally clause's exits are only exits for the try block\n                # as a whole if the try block had some exits to begin with.\n                exits = final_exits\n\n        return exits\n\n    def _combine_finally_starts(self, starts: set[ArcStart], exits: set[ArcStart]) -> set[ArcStart]:\n        \"\"\"Helper for building the cause of `finally` branches.\n\n        \"finally\" clauses might not execute their exits, and the causes could\n        be due to a failure to execute any of the exits in the try block. So\n        we use the causes from `starts` as the causes for `exits`.\n        \"\"\"\n        causes = []\n        for start in sorted(starts):\n            if start.cause:\n                causes.append(start.cause.format(lineno=start.lineno))\n        cause = \" or \".join(causes)\n        exits = {ArcStart(xit.lineno, cause) for xit in exits}\n        return exits\n\n    def _handle__While(self, node: ast.While) -> set[ArcStart]:\n        start = to_top = self.line_for_node(node.test)\n        constant_test = self.is_constant_expr(node.test)\n        top_is_body0 = False\n        if constant_test:\n            top_is_body0 = True\n        if env.PYBEHAVIOR.keep_constant_test:\n            top_is_body0 = False\n        if top_is_body0:\n            to_top = self.line_for_node(node.body[0])\n        self.block_stack.append(LoopBlock(start=to_top))\n        from_start = ArcStart(start, cause=\"the condition on line {lineno} was never true\")\n        exits = self.body_exits(node.body, from_start=from_start)\n        for xit in exits:\n            self.add_arc(xit.lineno, to_top, xit.cause)\n        exits = set()\n        my_block = self.block_stack.pop()\n        assert isinstance(my_block, LoopBlock)\n        exits.update(my_block.break_exits)\n        from_start = ArcStart(start, cause=\"the condition on line {lineno} was always true\")\n        if node.orelse:\n            else_exits = self.body_exits(node.orelse, from_start=from_start)\n            exits |= else_exits\n        else:\n            # No `else` clause: you can exit from the start.\n            if not constant_test:\n                exits.add(from_start)\n        return exits\n\n    def _handle__With(self, node: ast.With) -> set[ArcStart]:\n        start = self.line_for_node(node)\n        if env.PYBEHAVIOR.exit_through_with:\n            self.block_stack.append(WithBlock(start=start))\n        exits = self.body_exits(node.body, from_start=ArcStart(start))\n        if env.PYBEHAVIOR.exit_through_with:\n            with_block = self.block_stack.pop()\n            assert isinstance(with_block, WithBlock)\n            with_exit = {ArcStart(start)}\n            if exits:\n                for xit in exits:\n                    self.add_arc(xit.lineno, start)\n                exits = with_exit\n            if with_block.break_from:\n                self.process_break_exits(\n                    self._combine_finally_starts(with_block.break_from, with_exit),\n                )\n            if with_block.continue_from:\n                self.process_continue_exits(\n                    self._combine_finally_starts(with_block.continue_from, with_exit),\n                )\n            if with_block.return_from:\n                self.process_return_exits(\n                    self._combine_finally_starts(with_block.return_from, with_exit),\n                )\n        return exits\n\n    _handle__AsyncWith = _handle__With\n", "coverage/report_core.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Reporter foundation for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\nfrom typing import (\n    Callable, Iterable, Iterator, IO, Protocol, TYPE_CHECKING,\n)\n\nfrom coverage.exceptions import NoDataError, NotPython\nfrom coverage.files import prep_patterns, GlobMatcher\nfrom coverage.misc import ensure_dir_for_file, file_be_gone\nfrom coverage.plugin import FileReporter\nfrom coverage.results import Analysis\nfrom coverage.types import TMorf\n\nif TYPE_CHECKING:\n    from coverage import Coverage\n\n\nclass Reporter(Protocol):\n    \"\"\"What we expect of reporters.\"\"\"\n\n    report_type: str\n\n    def report(self, morfs: Iterable[TMorf] | None, outfile: IO[str]) -> float:\n        \"\"\"Generate a report of `morfs`, written to `outfile`.\"\"\"\n\n\ndef render_report(\n    output_path: str,\n    reporter: Reporter,\n    morfs: Iterable[TMorf] | None,\n    msgfn: Callable[[str], None],\n) -> float:\n    \"\"\"Run a one-file report generator, managing the output file.\n\n    This function ensures the output file is ready to be written to. Then writes\n    the report to it. Then closes the file and cleans up.\n\n    \"\"\"\n    file_to_close = None\n    delete_file = False\n\n    if output_path == \"-\":\n        outfile = sys.stdout\n    else:\n        # Ensure that the output directory is created; done here because this\n        # report pre-opens the output file.  HtmlReporter does this on its own\n        # because its task is more complex, being multiple files.\n        ensure_dir_for_file(output_path)\n        outfile = open(output_path, \"w\", encoding=\"utf-8\")\n        file_to_close = outfile\n        delete_file = True\n\n    try:\n        ret = reporter.report(morfs, outfile=outfile)\n        if file_to_close is not None:\n            msgfn(f\"Wrote {reporter.report_type} to {output_path}\")\n        delete_file = False\n        return ret\n    finally:\n        if file_to_close is not None:\n            file_to_close.close()\n            if delete_file:\n                file_be_gone(output_path)           # pragma: part covered (doesn't return)\n\n\ndef get_analysis_to_report(\n    coverage: Coverage,\n    morfs: Iterable[TMorf] | None,\n) -> Iterator[tuple[FileReporter, Analysis]]:\n    \"\"\"Get the files to report on.\n\n    For each morf in `morfs`, if it should be reported on (based on the omit\n    and include configuration options), yield a pair, the `FileReporter` and\n    `Analysis` for the morf.\n\n    \"\"\"\n    fr_morfs = coverage._get_file_reporters(morfs)\n    config = coverage.config\n\n    if config.report_include:\n        matcher = GlobMatcher(prep_patterns(config.report_include), \"report_include\")\n        fr_morfs = [(fr, morf) for (fr, morf) in fr_morfs if matcher.match(fr.filename)]\n\n    if config.report_omit:\n        matcher = GlobMatcher(prep_patterns(config.report_omit), \"report_omit\")\n        fr_morfs = [(fr, morf) for (fr, morf) in fr_morfs if not matcher.match(fr.filename)]\n\n    if not fr_morfs:\n        raise NoDataError(\"No data to report.\")\n\n    for fr, morf in sorted(fr_morfs):\n        try:\n            analysis = coverage._analyze(morf)\n        except NotPython:\n            # Only report errors for .py files, and only if we didn't\n            # explicitly suppress those errors.\n            # NotPython is only raised by PythonFileReporter, which has a\n            # should_be_python() method.\n            if fr.should_be_python():       # type: ignore[attr-defined]\n                if config.ignore_errors:\n                    msg = f\"Couldn't parse Python file '{fr.filename}'\"\n                    coverage._warn(msg, slug=\"couldnt-parse\")\n                else:\n                    raise\n        except Exception as exc:\n            if config.ignore_errors:\n                msg = f\"Couldn't parse '{fr.filename}': {exc}\".rstrip()\n                coverage._warn(msg, slug=\"couldnt-parse\")\n            else:\n                raise\n        else:\n            yield (fr, analysis)\n", "coverage/phystokens.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Better tokenizing for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport io\nimport keyword\nimport re\nimport sys\nimport token\nimport tokenize\n\nfrom typing import Iterable\n\nfrom coverage import env\nfrom coverage.types import TLineNo, TSourceTokenLines\n\n\nTokenInfos = Iterable[tokenize.TokenInfo]\n\n\ndef _phys_tokens(toks: TokenInfos) -> TokenInfos:\n    \"\"\"Return all physical tokens, even line continuations.\n\n    tokenize.generate_tokens() doesn't return a token for the backslash that\n    continues lines.  This wrapper provides those tokens so that we can\n    re-create a faithful representation of the original source.\n\n    Returns the same values as generate_tokens()\n\n    \"\"\"\n    last_line: str | None = None\n    last_lineno = -1\n    last_ttext: str = \"\"\n    for ttype, ttext, (slineno, scol), (elineno, ecol), ltext in toks:\n        if last_lineno != elineno:\n            if last_line and last_line.endswith(\"\\\\\\n\"):\n                # We are at the beginning of a new line, and the last line\n                # ended with a backslash.  We probably have to inject a\n                # backslash token into the stream. Unfortunately, there's more\n                # to figure out.  This code::\n                #\n                #   usage = \"\"\"\\\n                #   HEY THERE\n                #   \"\"\"\n                #\n                # triggers this condition, but the token text is::\n                #\n                #   '\"\"\"\\\\\\nHEY THERE\\n\"\"\"'\n                #\n                # so we need to figure out if the backslash is already in the\n                # string token or not.\n                inject_backslash = True\n                if last_ttext.endswith(\"\\\\\"):\n                    inject_backslash = False\n                elif ttype == token.STRING:\n                    if \"\\n\" in ttext and ttext.split(\"\\n\", 1)[0][-1] == \"\\\\\":\n                        # It's a multi-line string and the first line ends with\n                        # a backslash, so we don't need to inject another.\n                        inject_backslash = False\n                if inject_backslash:\n                    # Figure out what column the backslash is in.\n                    ccol = len(last_line.split(\"\\n\")[-2]) - 1\n                    # Yield the token, with a fake token type.\n                    yield tokenize.TokenInfo(\n                        99999, \"\\\\\\n\",\n                        (slineno, ccol), (slineno, ccol+2),\n                        last_line,\n                    )\n            last_line = ltext\n        if ttype not in (tokenize.NEWLINE, tokenize.NL):\n            last_ttext = ttext\n        yield tokenize.TokenInfo(ttype, ttext, (slineno, scol), (elineno, ecol), ltext)\n        last_lineno = elineno\n\n\ndef find_soft_key_lines(source: str) -> set[TLineNo]:\n    \"\"\"Helper for finding lines with soft keywords, like match/case lines.\"\"\"\n    soft_key_lines: set[TLineNo] = set()\n\n    for node in ast.walk(ast.parse(source)):\n        if sys.version_info >= (3, 10) and isinstance(node, ast.Match):\n            soft_key_lines.add(node.lineno)\n            for case in node.cases:\n                soft_key_lines.add(case.pattern.lineno)\n        elif sys.version_info >= (3, 12) and isinstance(node, ast.TypeAlias):\n            soft_key_lines.add(node.lineno)\n\n    return soft_key_lines\n\n\ndef source_token_lines(source: str) -> TSourceTokenLines:\n    \"\"\"Generate a series of lines, one for each line in `source`.\n\n    Each line is a list of pairs, each pair is a token::\n\n        [('key', 'def'), ('ws', ' '), ('nam', 'hello'), ('op', '('), ... ]\n\n    Each pair has a token class, and the token text.\n\n    If you concatenate all the token texts, and then join them with newlines,\n    you should have your original `source` back, with two differences:\n    trailing white space is not preserved, and a final line with no newline\n    is indistinguishable from a final line with a newline.\n\n    \"\"\"\n\n    ws_tokens = {token.INDENT, token.DEDENT, token.NEWLINE, tokenize.NL}\n    line: list[tuple[str, str]] = []\n    col = 0\n\n    source = source.expandtabs(8).replace(\"\\r\\n\", \"\\n\")\n    tokgen = generate_tokens(source)\n\n    if env.PYBEHAVIOR.soft_keywords:\n        soft_key_lines = find_soft_key_lines(source)\n    else:\n        soft_key_lines = set()\n\n    for ttype, ttext, (sline, scol), (_, ecol), _ in _phys_tokens(tokgen):\n        mark_start = True\n        for part in re.split(\"(\\n)\", ttext):\n            if part == \"\\n\":\n                yield line\n                line = []\n                col = 0\n                mark_end = False\n            elif part == \"\":\n                mark_end = False\n            elif ttype in ws_tokens:\n                mark_end = False\n            else:\n                if mark_start and scol > col:\n                    line.append((\"ws\", \" \" * (scol - col)))\n                    mark_start = False\n                tok_class = tokenize.tok_name.get(ttype, \"xx\").lower()[:3]\n                if ttype == token.NAME:\n                    if keyword.iskeyword(ttext):\n                        # Hard keywords are always keywords.\n                        tok_class = \"key\"\n                    elif sys.version_info >= (3, 10):   # PYVERSIONS\n                        # Need the version_info check to keep mypy from borking\n                        # on issoftkeyword here.\n                        if env.PYBEHAVIOR.soft_keywords and keyword.issoftkeyword(ttext):\n                            # Soft keywords appear at the start of their line.\n                            if len(line) == 0:\n                                is_start_of_line = True\n                            elif (len(line) == 1) and line[0][0] == \"ws\":\n                                is_start_of_line = True\n                            else:\n                                is_start_of_line = False\n                            if is_start_of_line and sline in soft_key_lines:\n                                tok_class = \"key\"\n                line.append((tok_class, part))\n                mark_end = True\n            scol = 0\n        if mark_end:\n            col = ecol\n\n    if line:\n        yield line\n\n\ndef generate_tokens(text: str) -> TokenInfos:\n    \"\"\"A helper around `tokenize.generate_tokens`.\n\n    Originally this was used to cache the results, but it didn't seem to make\n    reporting go faster, and caused issues with using too much memory.\n\n    \"\"\"\n    readline = io.StringIO(text).readline\n    return tokenize.generate_tokens(readline)\n\n\ndef source_encoding(source: bytes) -> str:\n    \"\"\"Determine the encoding for `source`, according to PEP 263.\n\n    `source` is a byte string: the text of the program.\n\n    Returns a string, the name of the encoding.\n\n    \"\"\"\n    readline = iter(source.splitlines(True)).__next__\n    return tokenize.detect_encoding(readline)[0]\n", "coverage/collector.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Raw data collector for coverage.py.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport functools\nimport os\nimport sys\n\nfrom types import FrameType\nfrom typing import (\n    cast, Any, Callable, Dict, List, Mapping, Set, TypeVar,\n)\n\nfrom coverage import env\nfrom coverage.config import CoverageConfig\nfrom coverage.data import CoverageData\nfrom coverage.debug import short_stack\nfrom coverage.disposition import FileDisposition\nfrom coverage.exceptions import ConfigError\nfrom coverage.misc import human_sorted_items, isolate_module\nfrom coverage.plugin import CoveragePlugin\nfrom coverage.pytracer import PyTracer\nfrom coverage.sysmon import SysMonitor\nfrom coverage.types import (\n    TArc, TFileDisposition, TTraceData, TTraceFn, TracerCore, TWarnFn,\n)\n\nos = isolate_module(os)\n\n\ntry:\n    # Use the C extension code when we can, for speed.\n    from coverage.tracer import CTracer, CFileDisposition\n    HAS_CTRACER = True\nexcept ImportError:\n    # Couldn't import the C extension, maybe it isn't built.\n    if os.getenv(\"COVERAGE_CORE\") == \"ctrace\":      # pragma: part covered\n        # During testing, we use the COVERAGE_CORE environment variable\n        # to indicate that we've fiddled with the environment to test this\n        # fallback code.  If we thought we had a C tracer, but couldn't import\n        # it, then exit quickly and clearly instead of dribbling confusing\n        # errors. I'm using sys.exit here instead of an exception because an\n        # exception here causes all sorts of other noise in unittest.\n        sys.stderr.write(\"*** COVERAGE_CORE is 'ctrace' but can't import CTracer!\\n\")\n        sys.exit(1)\n    HAS_CTRACER = False\n\nT = TypeVar(\"T\")\n\n\nclass Collector:\n    \"\"\"Collects trace data.\n\n    Creates a Tracer object for each thread, since they track stack\n    information.  Each Tracer points to the same shared data, contributing\n    traced data points.\n\n    When the Collector is started, it creates a Tracer for the current thread,\n    and installs a function to create Tracers for each new thread started.\n    When the Collector is stopped, all active Tracers are stopped.\n\n    Threads started while the Collector is stopped will never have Tracers\n    associated with them.\n\n    \"\"\"\n\n    # The stack of active Collectors.  Collectors are added here when started,\n    # and popped when stopped.  Collectors on the stack are paused when not\n    # the top, and resumed when they become the top again.\n    _collectors: list[Collector] = []\n\n    # The concurrency settings we support here.\n    LIGHT_THREADS = {\"greenlet\", \"eventlet\", \"gevent\"}\n\n    def __init__(\n        self,\n        should_trace: Callable[[str, FrameType], TFileDisposition],\n        check_include: Callable[[str, FrameType], bool],\n        should_start_context: Callable[[FrameType], str | None] | None,\n        file_mapper: Callable[[str], str],\n        timid: bool,\n        branch: bool,\n        warn: TWarnFn,\n        concurrency: list[str],\n        metacov: bool,\n    ) -> None:\n        \"\"\"Create a collector.\n\n        `should_trace` is a function, taking a file name and a frame, and\n        returning a `coverage.FileDisposition object`.\n\n        `check_include` is a function taking a file name and a frame. It returns\n        a boolean: True if the file should be traced, False if not.\n\n        `should_start_context` is a function taking a frame, and returning a\n        string. If the frame should be the start of a new context, the string\n        is the new context. If the frame should not be the start of a new\n        context, return None.\n\n        `file_mapper` is a function taking a filename, and returning a Unicode\n        filename.  The result is the name that will be recorded in the data\n        file.\n\n        If `timid` is true, then a slower simpler trace function will be\n        used.  This is important for some environments where manipulation of\n        tracing functions make the faster more sophisticated trace function not\n        operate properly.\n\n        If `branch` is true, then branches will be measured.  This involves\n        collecting data on which statements followed each other (arcs).  Use\n        `get_arc_data` to get the arc data.\n\n        `warn` is a warning function, taking a single string message argument\n        and an optional slug argument which will be a string or None, to be\n        used if a warning needs to be issued.\n\n        `concurrency` is a list of strings indicating the concurrency libraries\n        in use.  Valid values are \"greenlet\", \"eventlet\", \"gevent\", or \"thread\"\n        (the default).  \"thread\" can be combined with one of the other three.\n        Other values are ignored.\n\n        \"\"\"\n        self.should_trace = should_trace\n        self.check_include = check_include\n        self.should_start_context = should_start_context\n        self.file_mapper = file_mapper\n        self.branch = branch\n        self.warn = warn\n        self.concurrency = concurrency\n        assert isinstance(self.concurrency, list), f\"Expected a list: {self.concurrency!r}\"\n\n        self.pid = os.getpid()\n\n        self.covdata: CoverageData\n        self.threading = None\n        self.static_context: str | None = None\n\n        self.origin = short_stack()\n\n        self.concur_id_func = None\n\n        self._trace_class: type[TracerCore]\n        self.file_disposition_class: type[TFileDisposition]\n\n        core: str | None\n        if timid:\n            core = \"pytrace\"\n        else:\n            core = os.getenv(\"COVERAGE_CORE\")\n\n            if core == \"sysmon\" and not env.PYBEHAVIOR.pep669:\n                self.warn(\"sys.monitoring isn't available, using default core\", slug=\"no-sysmon\")\n                core = None\n\n            if not core:\n                # Once we're comfortable with sysmon as a default:\n                # if env.PYBEHAVIOR.pep669 and self.should_start_context is None:\n                #     core = \"sysmon\"\n                if HAS_CTRACER:\n                    core = \"ctrace\"\n                else:\n                    core = \"pytrace\"\n\n        if core == \"sysmon\":\n            self._trace_class = SysMonitor\n            self._core_kwargs = {\"tool_id\": 3 if metacov else 1}\n            self.file_disposition_class = FileDisposition\n            self.supports_plugins = False\n            self.packed_arcs = False\n            self.systrace = False\n        elif core == \"ctrace\":\n            self._trace_class = CTracer\n            self._core_kwargs = {}\n            self.file_disposition_class = CFileDisposition\n            self.supports_plugins = True\n            self.packed_arcs = True\n            self.systrace = True\n        elif core == \"pytrace\":\n            self._trace_class = PyTracer\n            self._core_kwargs = {}\n            self.file_disposition_class = FileDisposition\n            self.supports_plugins = False\n            self.packed_arcs = False\n            self.systrace = True\n        else:\n            raise ConfigError(f\"Unknown core value: {core!r}\")\n\n        # We can handle a few concurrency options here, but only one at a time.\n        concurrencies = set(self.concurrency)\n        unknown = concurrencies - CoverageConfig.CONCURRENCY_CHOICES\n        if unknown:\n            show = \", \".join(sorted(unknown))\n            raise ConfigError(f\"Unknown concurrency choices: {show}\")\n        light_threads = concurrencies & self.LIGHT_THREADS\n        if len(light_threads) > 1:\n            show = \", \".join(sorted(light_threads))\n            raise ConfigError(f\"Conflicting concurrency settings: {show}\")\n        do_threading = False\n\n        tried = \"nothing\"  # to satisfy pylint\n        try:\n            if \"greenlet\" in concurrencies:\n                tried = \"greenlet\"\n                import greenlet\n                self.concur_id_func = greenlet.getcurrent\n            elif \"eventlet\" in concurrencies:\n                tried = \"eventlet\"\n                import eventlet.greenthread     # pylint: disable=import-error,useless-suppression\n                self.concur_id_func = eventlet.greenthread.getcurrent\n            elif \"gevent\" in concurrencies:\n                tried = \"gevent\"\n                import gevent                   # pylint: disable=import-error,useless-suppression\n                self.concur_id_func = gevent.getcurrent\n\n            if \"thread\" in concurrencies:\n                do_threading = True\n        except ImportError as ex:\n            msg = f\"Couldn't trace with concurrency={tried}, the module isn't installed.\"\n            raise ConfigError(msg) from ex\n\n        if self.concur_id_func and not hasattr(self._trace_class, \"concur_id_func\"):\n            raise ConfigError(\n                \"Can't support concurrency={} with {}, only threads are supported.\".format(\n                    tried, self.tracer_name(),\n                ),\n            )\n\n        if do_threading or not concurrencies:\n            # It's important to import threading only if we need it.  If\n            # it's imported early, and the program being measured uses\n            # gevent, then gevent's monkey-patching won't work properly.\n            import threading\n            self.threading = threading\n\n        self.reset()\n\n    def __repr__(self) -> str:\n        return f\"<Collector at {id(self):#x}: {self.tracer_name()}>\"\n\n    def use_data(self, covdata: CoverageData, context: str | None) -> None:\n        \"\"\"Use `covdata` for recording data.\"\"\"\n        self.covdata = covdata\n        self.static_context = context\n        self.covdata.set_context(self.static_context)\n\n    def tracer_name(self) -> str:\n        \"\"\"Return the class name of the tracer we're using.\"\"\"\n        return self._trace_class.__name__\n\n    def _clear_data(self) -> None:\n        \"\"\"Clear out existing data, but stay ready for more collection.\"\"\"\n        # We used to use self.data.clear(), but that would remove filename\n        # keys and data values that were still in use higher up the stack\n        # when we are called as part of switch_context.\n        with self.data_lock or contextlib.nullcontext():\n            for d in self.data.values():\n                d.clear()\n\n        for tracer in self.tracers:\n            tracer.reset_activity()\n\n    def reset(self) -> None:\n        \"\"\"Clear collected data, and prepare to collect more.\"\"\"\n        self.data_lock = self.threading.Lock() if self.threading else None\n\n        # The trace data we are collecting.\n        self.data: TTraceData = {}\n\n        # A dictionary mapping file names to file tracer plugin names that will\n        # handle them.\n        self.file_tracers: dict[str, str] = {}\n\n        self.disabled_plugins: set[str] = set()\n\n        # The .should_trace_cache attribute is a cache from file names to\n        # coverage.FileDisposition objects, or None.  When a file is first\n        # considered for tracing, a FileDisposition is obtained from\n        # Coverage.should_trace.  Its .trace attribute indicates whether the\n        # file should be traced or not.  If it should be, a plugin with dynamic\n        # file names can decide not to trace it based on the dynamic file name\n        # being excluded by the inclusion rules, in which case the\n        # FileDisposition will be replaced by None in the cache.\n        if env.PYPY:\n            import __pypy__                     # pylint: disable=import-error\n            # Alex Gaynor said:\n            # should_trace_cache is a strictly growing key: once a key is in\n            # it, it never changes.  Further, the keys used to access it are\n            # generally constant, given sufficient context. That is to say, at\n            # any given point _trace() is called, pypy is able to know the key.\n            # This is because the key is determined by the physical source code\n            # line, and that's invariant with the call site.\n            #\n            # This property of a dict with immutable keys, combined with\n            # call-site-constant keys is a match for PyPy's module dict,\n            # which is optimized for such workloads.\n            #\n            # This gives a 20% benefit on the workload described at\n            # https://bitbucket.org/pypy/pypy/issue/1871/10x-slower-than-cpython-under-coverage\n            self.should_trace_cache = __pypy__.newdict(\"module\")\n        else:\n            self.should_trace_cache = {}\n\n        # Our active Tracers.\n        self.tracers: list[TracerCore] = []\n\n        self._clear_data()\n\n    def lock_data(self) -> None:\n        \"\"\"Lock self.data_lock, for use by the C tracer.\"\"\"\n        if self.data_lock is not None:\n            self.data_lock.acquire()\n\n    def unlock_data(self) -> None:\n        \"\"\"Unlock self.data_lock, for use by the C tracer.\"\"\"\n        if self.data_lock is not None:\n            self.data_lock.release()\n\n    def _start_tracer(self) -> TTraceFn | None:\n        \"\"\"Start a new Tracer object, and store it in self.tracers.\"\"\"\n        tracer = self._trace_class(**self._core_kwargs)\n        tracer.data = self.data\n        tracer.lock_data = self.lock_data\n        tracer.unlock_data = self.unlock_data\n        tracer.trace_arcs = self.branch\n        tracer.should_trace = self.should_trace\n        tracer.should_trace_cache = self.should_trace_cache\n        tracer.warn = self.warn\n\n        if hasattr(tracer, 'concur_id_func'):\n            tracer.concur_id_func = self.concur_id_func\n        if hasattr(tracer, 'file_tracers'):\n            tracer.file_tracers = self.file_tracers\n        if hasattr(tracer, 'threading'):\n            tracer.threading = self.threading\n        if hasattr(tracer, 'check_include'):\n            tracer.check_include = self.check_include\n        if hasattr(tracer, 'should_start_context'):\n            tracer.should_start_context = self.should_start_context\n        if hasattr(tracer, 'switch_context'):\n            tracer.switch_context = self.switch_context\n        if hasattr(tracer, 'disable_plugin'):\n            tracer.disable_plugin = self.disable_plugin\n\n        fn = tracer.start()\n        self.tracers.append(tracer)\n\n        return fn\n\n    # The trace function has to be set individually on each thread before\n    # execution begins.  Ironically, the only support the threading module has\n    # for running code before the thread main is the tracing function.  So we\n    # install this as a trace function, and the first time it's called, it does\n    # the real trace installation.\n    #\n    # New in 3.12: threading.settrace_all_threads: https://github.com/python/cpython/pull/96681\n\n    def _installation_trace(self, frame: FrameType, event: str, arg: Any) -> TTraceFn | None:\n        \"\"\"Called on new threads, installs the real tracer.\"\"\"\n        # Remove ourselves as the trace function.\n        sys.settrace(None)\n        # Install the real tracer.\n        fn: TTraceFn | None = self._start_tracer()\n        # Invoke the real trace function with the current event, to be sure\n        # not to lose an event.\n        if fn:\n            fn = fn(frame, event, arg)\n        # Return the new trace function to continue tracing in this scope.\n        return fn\n\n    def start(self) -> None:\n        \"\"\"Start collecting trace information.\"\"\"\n        # We may be a new collector in a forked process.  The old process'\n        # collectors will be in self._collectors, but they won't be usable.\n        # Find them and discard them.\n        keep_collectors = []\n        for c in self._collectors:\n            if c.pid == self.pid:\n                keep_collectors.append(c)\n            else:\n                c.post_fork()\n        self._collectors[:] = keep_collectors\n\n        if self._collectors:\n            self._collectors[-1].pause()\n\n        self.tracers = []\n\n        try:\n            # Install the tracer on this thread.\n            self._start_tracer()\n        except:\n            if self._collectors:\n                self._collectors[-1].resume()\n            raise\n\n        # If _start_tracer succeeded, then we add ourselves to the global\n        # stack of collectors.\n        self._collectors.append(self)\n\n        # Install our installation tracer in threading, to jump-start other\n        # threads.\n        if self.systrace and self.threading:\n            self.threading.settrace(self._installation_trace)\n\n    def stop(self) -> None:\n        \"\"\"Stop collecting trace information.\"\"\"\n        assert self._collectors\n        if self._collectors[-1] is not self:\n            print(\"self._collectors:\")\n            for c in self._collectors:\n                print(f\"  {c!r}\\n{c.origin}\")\n        assert self._collectors[-1] is self, (\n            f\"Expected current collector to be {self!r}, but it's {self._collectors[-1]!r}\"\n        )\n\n        self.pause()\n\n        # Remove this Collector from the stack, and resume the one underneath (if any).\n        self._collectors.pop()\n        if self._collectors:\n            self._collectors[-1].resume()\n\n    def pause(self) -> None:\n        \"\"\"Pause tracing, but be prepared to `resume`.\"\"\"\n        for tracer in self.tracers:\n            tracer.stop()\n            stats = tracer.get_stats()\n            if stats:\n                print(\"\\nCoverage.py tracer stats:\")\n                for k, v in human_sorted_items(stats.items()):\n                    print(f\"{k:>20}: {v}\")\n        if self.threading:\n            self.threading.settrace(None)\n\n    def resume(self) -> None:\n        \"\"\"Resume tracing after a `pause`.\"\"\"\n        for tracer in self.tracers:\n            tracer.start()\n        if self.systrace:\n            if self.threading:\n                self.threading.settrace(self._installation_trace)\n            else:\n                self._start_tracer()\n\n    def post_fork(self) -> None:\n        \"\"\"After a fork, tracers might need to adjust.\"\"\"\n        for tracer in self.tracers:\n            if hasattr(tracer, \"post_fork\"):\n                tracer.post_fork()\n\n    def _activity(self) -> bool:\n        \"\"\"Has any activity been traced?\n\n        Returns a boolean, True if any trace function was invoked.\n\n        \"\"\"\n        return any(tracer.activity() for tracer in self.tracers)\n\n    def switch_context(self, new_context: str | None) -> None:\n        \"\"\"Switch to a new dynamic context.\"\"\"\n        context: str | None\n        self.flush_data()\n        if self.static_context:\n            context = self.static_context\n            if new_context:\n                context += \"|\" + new_context\n        else:\n            context = new_context\n        self.covdata.set_context(context)\n\n    def disable_plugin(self, disposition: TFileDisposition) -> None:\n        \"\"\"Disable the plugin mentioned in `disposition`.\"\"\"\n        file_tracer = disposition.file_tracer\n        assert file_tracer is not None\n        plugin = file_tracer._coverage_plugin\n        plugin_name = plugin._coverage_plugin_name\n        self.warn(f\"Disabling plug-in {plugin_name!r} due to previous exception\")\n        plugin._coverage_enabled = False\n        disposition.trace = False\n\n    @functools.lru_cache(maxsize=None)          # pylint: disable=method-cache-max-size-none\n    def cached_mapped_file(self, filename: str) -> str:\n        \"\"\"A locally cached version of file names mapped through file_mapper.\"\"\"\n        return self.file_mapper(filename)\n\n    def mapped_file_dict(self, d: Mapping[str, T]) -> dict[str, T]:\n        \"\"\"Return a dict like d, but with keys modified by file_mapper.\"\"\"\n        # The call to list(items()) ensures that the GIL protects the dictionary\n        # iterator against concurrent modifications by tracers running\n        # in other threads. We try three times in case of concurrent\n        # access, hoping to get a clean copy.\n        runtime_err = None\n        for _ in range(3):                      # pragma: part covered\n            try:\n                items = list(d.items())\n            except RuntimeError as ex:          # pragma: cant happen\n                runtime_err = ex\n            else:\n                break\n        else:                                   # pragma: cant happen\n            assert isinstance(runtime_err, Exception)\n            raise runtime_err\n\n        return {self.cached_mapped_file(k): v for k, v in items if v}\n\n    def plugin_was_disabled(self, plugin: CoveragePlugin) -> None:\n        \"\"\"Record that `plugin` was disabled during the run.\"\"\"\n        self.disabled_plugins.add(plugin._coverage_plugin_name)\n\n    def flush_data(self) -> bool:\n        \"\"\"Save the collected data to our associated `CoverageData`.\n\n        Data may have also been saved along the way. This forces the\n        last of the data to be saved.\n\n        Returns True if there was data to save, False if not.\n        \"\"\"\n        if not self._activity():\n            return False\n\n        if self.branch:\n            if self.packed_arcs:\n                # Unpack the line number pairs packed into integers.  See\n                # tracer.c:CTracer_record_pair for the C code that creates\n                # these packed ints.\n                arc_data: dict[str, list[TArc]] = {}\n                packed_data = cast(Dict[str, Set[int]], self.data)\n\n                # The list() here and in the inner loop are to get a clean copy\n                # even as tracers are continuing to add data.\n                for fname, packeds in list(packed_data.items()):\n                    tuples = []\n                    for packed in list(packeds):\n                        l1 = packed & 0xFFFFF\n                        l2 = (packed & (0xFFFFF << 20)) >> 20\n                        if packed & (1 << 40):\n                            l1 *= -1\n                        if packed & (1 << 41):\n                            l2 *= -1\n                        tuples.append((l1, l2))\n                    arc_data[fname] = tuples\n            else:\n                arc_data = cast(Dict[str, List[TArc]], self.data)\n            self.covdata.add_arcs(self.mapped_file_dict(arc_data))\n        else:\n            line_data = cast(Dict[str, Set[int]], self.data)\n            self.covdata.add_lines(self.mapped_file_dict(line_data))\n\n        file_tracers = {\n            k: v for k, v in self.file_tracers.items()\n            if v not in self.disabled_plugins\n        }\n        self.covdata.add_file_tracers(self.mapped_file_dict(file_tracers))\n\n        self._clear_data()\n        return True\n", "coverage/tomlconfig.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"TOML configuration support for coverage.py\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\n\nfrom typing import Any, Callable, Iterable, TypeVar\n\nfrom coverage import env\nfrom coverage.exceptions import ConfigError\nfrom coverage.misc import import_third_party, substitute_variables\nfrom coverage.types import TConfigSectionOut, TConfigValueOut\n\n\nif env.PYVERSION >= (3, 11, 0, \"alpha\", 7):\n    import tomllib      # pylint: disable=import-error\n    has_tomllib = True\nelse:\n    # TOML support on Python 3.10 and below is an install-time extra option.\n    tomllib, has_tomllib = import_third_party(\"tomli\")\n\n\nclass TomlDecodeError(Exception):\n    \"\"\"An exception class that exists even when toml isn't installed.\"\"\"\n    pass\n\n\nTWant = TypeVar(\"TWant\")\n\nclass TomlConfigParser:\n    \"\"\"TOML file reading with the interface of HandyConfigParser.\"\"\"\n\n    # This class has the same interface as config.HandyConfigParser, no\n    # need for docstrings.\n    # pylint: disable=missing-function-docstring\n\n    def __init__(self, our_file: bool) -> None:\n        self.our_file = our_file\n        self.data: dict[str, Any] = {}\n\n    def read(self, filenames: Iterable[str]) -> list[str]:\n        # RawConfigParser takes a filename or list of filenames, but we only\n        # ever call this with a single filename.\n        assert isinstance(filenames, (bytes, str, os.PathLike))\n        filename = os.fspath(filenames)\n\n        try:\n            with open(filename, encoding='utf-8') as fp:\n                toml_text = fp.read()\n        except OSError:\n            return []\n        if has_tomllib:\n            try:\n                self.data = tomllib.loads(toml_text)\n            except tomllib.TOMLDecodeError as err:\n                raise TomlDecodeError(str(err)) from err\n            return [filename]\n        else:\n            has_toml = re.search(r\"^\\[tool\\.coverage(\\.|])\", toml_text, flags=re.MULTILINE)\n            if self.our_file or has_toml:\n                # Looks like they meant to read TOML, but we can't read it.\n                msg = \"Can't read {!r} without TOML support. Install with [toml] extra\"\n                raise ConfigError(msg.format(filename))\n            return []\n\n    def _get_section(self, section: str) -> tuple[str | None, TConfigSectionOut | None]:\n        \"\"\"Get a section from the data.\n\n        Arguments:\n            section (str): A section name, which can be dotted.\n\n        Returns:\n            name (str): the actual name of the section that was found, if any,\n                or None.\n            data (str): the dict of data in the section, or None if not found.\n\n        \"\"\"\n        prefixes = [\"tool.coverage.\"]\n        for prefix in prefixes:\n            real_section = prefix + section\n            parts = real_section.split(\".\")\n            try:\n                data = self.data[parts[0]]\n                for part in parts[1:]:\n                    data = data[part]\n            except KeyError:\n                continue\n            break\n        else:\n            return None, None\n        return real_section, data\n\n    def _get(self, section: str, option: str) -> tuple[str, TConfigValueOut]:\n        \"\"\"Like .get, but returns the real section name and the value.\"\"\"\n        name, data = self._get_section(section)\n        if data is None:\n            raise ConfigError(f\"No section: {section!r}\")\n        assert name is not None\n        try:\n            value = data[option]\n        except KeyError:\n            raise ConfigError(f\"No option {option!r} in section: {name!r}\") from None\n        return name, value\n\n    def _get_single(self, section: str, option: str) -> Any:\n        \"\"\"Get a single-valued option.\n\n        Performs environment substitution if the value is a string. Other types\n        will be converted later as needed.\n        \"\"\"\n        name, value = self._get(section, option)\n        if isinstance(value, str):\n            value = substitute_variables(value, os.environ)\n        return name, value\n\n    def has_option(self, section: str, option: str) -> bool:\n        _, data = self._get_section(section)\n        if data is None:\n            return False\n        return option in data\n\n    def real_section(self, section: str) -> str | None:\n        name, _ = self._get_section(section)\n        return name\n\n    def has_section(self, section: str) -> bool:\n        name, _ = self._get_section(section)\n        return bool(name)\n\n    def options(self, section: str) -> list[str]:\n        _, data = self._get_section(section)\n        if data is None:\n            raise ConfigError(f\"No section: {section!r}\")\n        return list(data.keys())\n\n    def get_section(self, section: str) -> TConfigSectionOut:\n        _, data = self._get_section(section)\n        return data or {}\n\n    def get(self, section: str, option: str) -> Any:\n        _, value = self._get_single(section, option)\n        return value\n\n    def _check_type(\n        self,\n        section: str,\n        option: str,\n        value: Any,\n        type_: type[TWant],\n        converter: Callable[[Any], TWant] | None,\n        type_desc: str,\n    ) -> TWant:\n        \"\"\"Check that `value` has the type we want, converting if needed.\n\n        Returns the resulting value of the desired type.\n        \"\"\"\n        if isinstance(value, type_):\n            return value\n        if isinstance(value, str) and converter is not None:\n            try:\n                return converter(value)\n            except Exception as e:\n                raise ValueError(\n                    f\"Option [{section}]{option} couldn't convert to {type_desc}: {value!r}\",\n                ) from e\n        raise ValueError(\n            f\"Option [{section}]{option} is not {type_desc}: {value!r}\",\n        )\n\n    def getboolean(self, section: str, option: str) -> bool:\n        name, value = self._get_single(section, option)\n        bool_strings = {\"true\": True, \"false\": False}\n        return self._check_type(name, option, value, bool, bool_strings.__getitem__, \"a boolean\")\n\n    def _get_list(self, section: str, option: str) -> tuple[str, list[str]]:\n        \"\"\"Get a list of strings, substituting environment variables in the elements.\"\"\"\n        name, values = self._get(section, option)\n        values = self._check_type(name, option, values, list, None, \"a list\")\n        values = [substitute_variables(value, os.environ) for value in values]\n        return name, values\n\n    def getlist(self, section: str, option: str) -> list[str]:\n        _, values = self._get_list(section, option)\n        return values\n\n    def getregexlist(self, section: str, option: str) -> list[str]:\n        name, values = self._get_list(section, option)\n        for value in values:\n            value = value.strip()\n            try:\n                re.compile(value)\n            except re.error as e:\n                raise ConfigError(f\"Invalid [{name}].{option} value {value!r}: {e}\") from e\n        return values\n\n    def getint(self, section: str, option: str) -> int:\n        name, value = self._get_single(section, option)\n        return self._check_type(name, option, value, int, int, \"an integer\")\n\n    def getfloat(self, section: str, option: str) -> float:\n        name, value = self._get_single(section, option)\n        if isinstance(value, int):\n            value = float(value)\n        return self._check_type(name, option, value, float, float, \"a float\")\n", "lab/parse_all.py": "\"\"\"Parse every Python file in a tree.\"\"\"\n\nimport os\nimport sys\n\nfrom coverage.parser import PythonParser\n\nfor root, dirnames, filenames in os.walk(sys.argv[1]):\n    for filename in filenames:\n        if filename.endswith(\".py\"):\n            filename = os.path.join(root, filename)\n            print(f\":: {filename}\")\n            try:\n                par = PythonParser(filename=filename)\n                par.parse_source()\n                par.arcs()\n            except Exception as exc:\n                print(f\"  ** {exc}\")\n", "lab/run_sysmon.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Run sys.monitoring on a file of Python code.\"\"\"\n\nimport functools\nimport sys\n\nprint(sys.version)\nthe_program = sys.argv[1]\n\ncode = open(the_program).read()\n\nmy_id = sys.monitoring.COVERAGE_ID\nsys.monitoring.use_tool_id(my_id, \"run_sysmon.py\")\nregister = functools.partial(sys.monitoring.register_callback, my_id)\nevents = sys.monitoring.events\n\n\ndef bytes_to_lines(code):\n    \"\"\"Make a dict mapping byte code offsets to line numbers.\"\"\"\n    b2l = {}\n    cur_line = 0\n    for bstart, bend, lineno in code.co_lines():\n        for boffset in range(bstart, bend, 2):\n            b2l[boffset] = lineno\n    return b2l\n\n\ndef sysmon_py_start(code, instruction_offset):\n    print(f\"PY_START: {code.co_filename}@{instruction_offset}\")\n    sys.monitoring.set_local_events(\n        my_id,\n        code,\n        events.PY_RETURN | events.PY_RESUME | events.LINE | events.BRANCH | events.JUMP,\n    )\n\n\ndef sysmon_py_resume(code, instruction_offset):\n    b2l = bytes_to_lines(code)\n    print(\n        f\"PY_RESUME: {code.co_filename}@{instruction_offset}, \"\n        + f\"{b2l[instruction_offset]}\"\n    )\n\n\ndef sysmon_py_return(code, instruction_offset, retval):\n    b2l = bytes_to_lines(code)\n    print(\n        f\"PY_RETURN: {code.co_filename}@{instruction_offset}, \"\n        + f\"{b2l[instruction_offset]}\"\n    )\n\n\ndef sysmon_line(code, line_number):\n    print(f\"LINE: {code.co_filename}@{line_number}\")\n    return sys.monitoring.DISABLE\n\n\ndef sysmon_branch(code, instruction_offset, destination_offset):\n    b2l = bytes_to_lines(code)\n    print(\n        f\"BRANCH: {code.co_filename}@{instruction_offset}->{destination_offset}, \"\n        + f\"{b2l[instruction_offset]}->{b2l[destination_offset]}\"\n    )\n\n\ndef sysmon_jump(code, instruction_offset, destination_offset):\n    b2l = bytes_to_lines(code)\n    print(\n        f\"JUMP: {code.co_filename}@{instruction_offset}->{destination_offset}, \"\n        + f\"{b2l[instruction_offset]}->{b2l[destination_offset]}\"\n    )\n\n\nsys.monitoring.set_events(\n    my_id,\n    events.PY_START | events.PY_UNWIND,\n)\nregister(events.PY_START, sysmon_py_start)\nregister(events.PY_RESUME, sysmon_py_resume)\nregister(events.PY_RETURN, sysmon_py_return)\n# register(events.PY_UNWIND, sysmon_py_unwind_arcs)\nregister(events.LINE, sysmon_line)\nregister(events.BRANCH, sysmon_branch)\nregister(events.JUMP, sysmon_jump)\n\nexec(code)\n", "lab/run_trace.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Run a simple trace function on a file of Python code.\"\"\"\n\nimport os, sys\n\nnest = 0\n\ndef trace(frame, event, arg):\n    global nest\n\n    if nest is None:\n        # This can happen when Python is shutting down.\n        return None\n\n    print(\"%s%s %s %d @%d\" % (\n        \"    \" * nest,\n        event,\n        os.path.basename(frame.f_code.co_filename),\n        frame.f_lineno,\n        frame.f_lasti,\n    ))\n\n    if event == 'call':\n        nest += 1\n    if event == 'return':\n        nest -= 1\n\n    return trace\n\nprint(sys.version)\nthe_program = sys.argv[1]\n\ncode = open(the_program).read()\nsys.settrace(trace)\nexec(code)\n", "lab/goals.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\\\nCheck coverage goals.\n\nUse `coverage json` to get a coverage.json file, then run this tool\nto check goals for subsets of files.\n\nPatterns can use '**/foo*.py' to find files anywhere in the project,\nand '!**/something.py' to exclude files matching a pattern.\n\n--file will check each file individually for the required coverage.\n--group checks the entire group collectively.\n\n\"\"\"\n\nimport argparse\nimport json\nimport sys\n\nfrom wcmatch import fnmatch as wcfnmatch    # python -m pip install wcmatch\n\nfrom coverage.results import Numbers        # Note: an internal class!\n\n\ndef select_files(files, pat):\n    flags = wcfnmatch.NEGATE | wcfnmatch.NEGATEALL\n    selected = [f for f in files if wcfnmatch.fnmatch(f, pat, flags=flags)]\n    return selected\n\ndef total_for_files(data, files):\n    total = Numbers(precision=3)\n    for f in files:\n        sel_summ = data[\"files\"][f][\"summary\"]\n        total += Numbers(\n            n_statements=sel_summ[\"num_statements\"],\n            n_excluded=sel_summ[\"excluded_lines\"],\n            n_missing=sel_summ[\"missing_lines\"],\n            n_branches=sel_summ.get(\"num_branches\", 0),\n            n_partial_branches=sel_summ.get(\"num_partial_branches\", 0),\n            n_missing_branches=sel_summ.get(\"missing_branches\", 0),\n        )\n\n    return total\n\ndef main(argv):\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--file\", \"-f\", action=\"store_true\", help=\"Check each file individually\")\n    parser.add_argument(\"--group\", \"-g\", action=\"store_true\", help=\"Check a group of files\")\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Be chatty about what's happening\")\n    parser.add_argument(\"goal\", type=float, help=\"Coverage goal\")\n    parser.add_argument(\"pattern\", type=str, nargs=\"+\", help=\"Patterns to check\")\n    args = parser.parse_args(argv)\n\n    print(\"** Note: this is a proof-of-concept. Support is not promised. **\")\n    print(\"Read more: https://nedbatchelder.com/blog/202111/coverage_goals.html\")\n    print(\"Feedback is appreciated: https://github.com/nedbat/coveragepy/issues/691\")\n\n    if args.file and args.group:\n        print(\"Can't use --file and --group together\")\n        return 1\n    if not (args.file or args.group):\n        print(\"Need either --file or --group\")\n        return 1\n\n    with open(\"coverage.json\") as j:\n        data = json.load(j)\n    all_files = list(data[\"files\"].keys())\n    selected = select_files(all_files, args.pattern)\n\n    ok = True\n    if args.group:\n        total = total_for_files(data, selected)\n        pat_nice = \",\".join(args.pattern)\n        result = f\"Coverage for {pat_nice} is {total.pc_covered_str}\"\n        if total.pc_covered < args.goal:\n            print(f\"{result}, below {args.goal}\")\n            ok = False\n        elif args.verbose:\n            print(result)\n    else:\n        for fname in selected:\n            total = total_for_files(data, [fname])\n            result = f\"Coverage for {fname} is {total.pc_covered_str}\"\n            if total.pc_covered < args.goal:\n                print(f\"{result}, below {args.goal}\")\n                ok = False\n            elif args.verbose:\n                print(result)\n\n    return 0 if ok else 2\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv[1:]))\n", "lab/genpy.py": "\"\"\"Generate random Python for testing.\"\"\"\n\nimport collections\nfrom itertools import cycle, product\nimport random\nimport re\n\nfrom coverage.parser import PythonParser\n\n\nclass PythonSpinner:\n    \"\"\"Spin Python source from a simple AST.\"\"\"\n\n    def __init__(self):\n        self.lines = []\n        self.lines.append(\"async def func():\")\n        self.indent = 4\n\n    @property\n    def lineno(self):\n        return len(self.lines) + 1\n\n    @classmethod\n    def generate_python(cls, ast):\n        spinner = cls()\n        spinner.gen_python_internal(ast)\n        return \"\\n\".join(spinner.lines)\n\n    def add_line(self, line):\n        g = f\"g{self.lineno}\"\n        self.lines.append(' ' * self.indent + line.format(g=g, lineno=self.lineno))\n\n    def add_block(self, node):\n        self.indent += 4\n        self.gen_python_internal(node)\n        self.indent -= 4\n\n    def maybe_block(self, node, nodei, keyword):\n        if len(node) > nodei and node[nodei] is not None:\n            self.add_line(keyword + \":\")\n            self.add_block(node[nodei])\n\n    def gen_python_internal(self, ast):\n        for node in ast:\n            if isinstance(node, list):\n                op = node[0]\n                if op == \"if\":\n                    self.add_line(\"if {g}:\")\n                    self.add_block(node[1])\n                    self.maybe_block(node, 2, \"else\")\n                elif op == \"for\":\n                    self.add_line(\"for x in {g}:\")\n                    self.add_block(node[1])\n                    self.maybe_block(node, 2, \"else\")\n                elif op == \"while\":\n                    self.add_line(\"while {g}:\")\n                    self.add_block(node[1])\n                    self.maybe_block(node, 2, \"else\")\n                elif op == \"try\":\n                    self.add_line(\"try:\")\n                    self.add_block(node[1])\n                    # 'except' clauses are different, because there can be any\n                    # number.\n                    if len(node) > 2 and node[2] is not None:\n                        for except_node in node[2]:\n                            self.add_line(f\"except Exception{self.lineno}:\")\n                            self.add_block(except_node)\n                    self.maybe_block(node, 3, \"else\")\n                    self.maybe_block(node, 4, \"finally\")\n                elif op == \"with\":\n                    self.add_line(\"with {g} as x:\")\n                    self.add_block(node[1])\n                else:\n                    raise Exception(f\"Bad list node: {node!r}\")\n            else:\n                op = node\n                if op == \"assign\":\n                    self.add_line(\"x = {lineno}\")\n                elif op in [\"break\", \"continue\"]:\n                    self.add_line(op)\n                elif op == \"return\":\n                    self.add_line(\"return\")\n                elif op == \"yield\":\n                    self.add_line(\"yield {lineno}\")\n                else:\n                    raise Exception(f\"Bad atom node: {node!r}\")\n\n\ndef weighted_choice(rand, choices):\n    \"\"\"Choose from a list of [(choice, weight), ...] options, randomly.\"\"\"\n    total = sum(w for c, w in choices)\n    r = rand.uniform(0, total)\n    upto = 0\n    for c, w in choices:\n        if upto + w >= r:\n            return c\n        upto += w\n    assert False, \"Shouldn't get here\"\n\n\nclass RandomAstMaker:\n    def __init__(self, seed=None):\n        self.r = random.Random()\n        if seed is not None:\n            self.r.seed(seed)\n        self.depth = 0\n        self.bc_allowed = set()\n\n    def roll(self, prob=0.5):\n        return self.r.random() <= prob\n\n    def choose(self, choices):\n        \"\"\"Roll the dice to choose an option.\"\"\"\n        return weighted_choice(self.r, choices)\n\n    STMT_CHOICES = [\n        [(\"if\", 10), (\"for\", 10), (\"try\", 10), (\"while\", 3), (\"with\", 10), (\"assign\", 20), (\"return\", 1), (\"yield\", 0)],\n        [(\"if\", 10), (\"for\", 10), (\"try\", 10), (\"while\", 3), (\"with\", 10), (\"assign\", 40), (\"return\", 1), (\"yield\", 0), (\"break\", 10), (\"continue\", 10)],\n        [(\"if\", 10), (\"for\", 10), (\"try\", 10), (\"while\", 3), (\"with\", 10), (\"assign\", 40), (\"return\", 1), (\"yield\", 0), (\"break\", 10), (\"continue\", 10)],\n        [(\"if\", 10), (\"for\", 10), (\"try\", 10), (\"while\", 3), (\"with\", 10), (\"assign\", 40), (\"return\", 1), (\"yield\", 0), (\"break\", 10), (\"continue\", 10)],\n        [(\"if\", 10), (\"for\", 10), (\"try\", 10), (\"while\", 3), (\"with\", 10), (\"assign\", 40), (\"return\", 1), (\"yield\", 0), (\"break\", 10), (\"continue\", 10)],\n        # Last element has to have no compound statements, to limit depth.\n        [(\"assign\", 10), (\"return\", 1), (\"yield\", 0), (\"break\", 10), (\"continue\", 10)],\n    ]\n\n    def make_body(self, parent):\n        body = []\n        choices = self.STMT_CHOICES[self.depth]\n\n        self.depth += 1\n        nstmts = self.choose([(1, 10), (2, 25), (3, 10), (4, 10), (5, 5)])\n        for _ in range(nstmts):\n            stmt = self.choose(choices)\n            if stmt == \"if\":\n                body.append([\"if\", self.make_body(\"if\")])\n                if self.roll():\n                    body[-1].append(self.make_body(\"ifelse\"))\n            elif stmt == \"for\":\n                old_allowed = self.bc_allowed\n                self.bc_allowed = self.bc_allowed | {\"break\", \"continue\"}\n                body.append([\"for\", self.make_body(\"for\")])\n                self.bc_allowed = old_allowed\n                if self.roll():\n                    body[-1].append(self.make_body(\"forelse\"))\n            elif stmt == \"while\":\n                old_allowed = self.bc_allowed\n                self.bc_allowed = self.bc_allowed | {\"break\", \"continue\"}\n                body.append([\"while\", self.make_body(\"while\")])\n                self.bc_allowed = old_allowed\n                if self.roll():\n                    body[-1].append(self.make_body(\"whileelse\"))\n            elif stmt == \"try\":\n                else_clause = self.make_body(\"try\") if self.roll() else None\n                old_allowed = self.bc_allowed\n                self.bc_allowed = self.bc_allowed - {\"continue\"}\n                finally_clause = self.make_body(\"finally\") if self.roll() else None\n                self.bc_allowed = old_allowed\n                if else_clause:\n                    with_exceptions = True\n                elif not else_clause and not finally_clause:\n                    with_exceptions = True\n                else:\n                    with_exceptions = self.roll()\n                if with_exceptions:\n                    num_exceptions = self.choose([(1, 50), (2, 50)])\n                    exceptions = [self.make_body(\"except\") for _ in range(num_exceptions)]\n                else:\n                    exceptions = None\n                body.append(\n                    [\"try\", self.make_body(\"tryelse\"), exceptions, else_clause, finally_clause]\n                )\n            elif stmt == \"with\":\n                body.append([\"with\", self.make_body(\"with\")])\n            elif stmt == \"return\":\n                body.append(stmt)\n                break\n            elif stmt == \"yield\":\n                body.append(\"yield\")\n            elif stmt in [\"break\", \"continue\"]:\n                if stmt in self.bc_allowed:\n                    # A break or continue immediately after a loop is not\n                    # interesting.  So if we are immediately after a loop, then\n                    # insert an assignment.\n                    if not body and (parent in [\"for\", \"while\"]):\n                        body.append(\"assign\")\n                    body.append(stmt)\n                    break\n                else:\n                    stmt = \"assign\"\n\n            if stmt == \"assign\":\n                # Don't put two assignments in a row, there's no point.\n                if not body or body[-1] != \"assign\":\n                    body.append(\"assign\")\n\n        self.depth -= 1\n        return body\n\n\ndef async_alternatives(source):\n    parts = re.split(r\"(for |with )\", source)\n    nchoices = len(parts) // 2\n    #print(\"{} choices\".format(nchoices))\n\n    def constant(s):\n        return [s]\n\n    def maybe_async(s):\n        return [s, \"async \"+s]\n\n    choices = [f(x) for f, x in zip(cycle([constant, maybe_async]), parts)]\n    for result in product(*choices):\n        source = \"\".join(result)\n        yield source\n\n\ndef compare_alternatives(source):\n    all_all_arcs = collections.defaultdict(list)\n    for i, alternate_source in enumerate(async_alternatives(source)):\n        parser = PythonParser(alternate_source)\n        arcs = parser.arcs()\n        all_all_arcs[tuple(arcs)].append((i, alternate_source))\n\n    return len(all_all_arcs)\n\n\ndef show_a_bunch():\n    longest = \"\"\n    for i in range(100):\n        maker = RandomAstMaker(i)\n        source = PythonSpinner.generate_python(maker.make_body(\"def\"))\n        try:\n            print(\"-\"*80, \"\\n\", source, sep=\"\")\n            compile(source, \"<string>\", \"exec\", dont_inherit=True)\n        except Exception as ex:\n            print(f\"Oops: {ex}\\n{source}\")\n        if len(source) > len(longest):\n            longest = source\n\n\ndef show_alternatives():\n    for i in range(1000):\n        maker = RandomAstMaker(i)\n        source = PythonSpinner.generate_python(maker.make_body(\"def\"))\n        nlines = len(source.splitlines())\n        if nlines < 15:\n            nalt = compare_alternatives(source)\n            if nalt > 1:\n                print(f\"--- {nlines:3} lines, {nalt:2} alternatives ---------\")\n                print(source)\n\n\n\ndef show_one():\n    maker = RandomAstMaker()\n    source = PythonSpinner.generate_python(maker.make_body(\"def\"))\n    print(source)\n\nif __name__ == \"__main__\":\n    show_one()\n    #show_alternatives()\n", "lab/pick.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nPick lines from the standard input.  Blank or commented lines are ignored.\n\nUsed to subset lists of tests to run.  Use with the --select-cmd pytest plugin\noption.\n\nThe first command line argument is a mode for selection. Other arguments depend\non the mode.  Only one mode is currently implemented: sample.\n\nModes:\n\n    - ``sample``: randomly sample N lines from the input.\n\n        - the first argument is N, the number of lines you want.\n\n        - the second argument is optional: a seed for the randomizer.\n          Using the same seed will produce the same output.\n\nExamples:\n\nGet a list of test nodes::\n\n    pytest --collect-only | grep :: > tests.txt\n\nUse like this::\n\n    pytest --cache-clear --select-cmd=\"python pick.py sample 10 < tests.txt\"\n\nFor coverage.py specifically::\n\n    tox -q -e py311 -- -n 0 --cache-clear --select-cmd=\"python lab/pick.py sample 10 < tests.txt\"\n\nor::\n\n    for n in $(seq 1 100); do \\\n        echo seed=$n; \\\n        tox -q -e py311 -- -n 0 --cache-clear --select-cmd=\"python lab/pick.py sample 3 $n < tests.txt\"; \\\n    done\n\nMore about this: https://nedbatchelder.com/blog/202401/randomly_subsetting_test_suites.html\n\n\"\"\"\n\nimport random\nimport sys\n\nargs = sys.argv[1:][::-1]\nnext_arg = args.pop\n\nlines = []\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    if line.startswith(\"#\"):\n        continue\n    lines.append(line)\n\nmode = next_arg()\nif mode == \"sample\":\n    number = int(next_arg())\n    if args:\n        random.seed(next_arg())\n    lines = random.sample(lines, number)\nelse:\n    raise ValueError(f\"Don't know {mode=}\")\n\nfor line in lines:\n    print(line)\n", "lab/extract_code.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nUse this to copy some indented code from the coverage.py test suite into a\nstandalone file for deeper testing, or writing bug reports.\n\nGive it a file name and a line number, and it will find the indented\nmulti-line string containing that line number, and output the dedented\ncontents of the string.\n\nIf tests/test_arcs.py has this (partial) content::\n\n    1630\t    def test_partial_generators(self):\n    1631\t        # https://github.com/nedbat/coveragepy/issues/475\n    1632\t        # Line 2 is executed completely.\n    1633\t        # Line 3 is started but not finished, because zip ends before it finishes.\n    1634\t        # Line 4 is never started.\n    1635\t        cov = self.check_coverage('''\\\n    1636\t            def f(a, b):\n    1637\t                c = (i for i in a)          # 2\n    1638\t                d = (j for j in b)          # 3\n    1639\t                e = (k for k in b)          # 4\n    1640\t                return dict(zip(c, d))\n    1641\n    1642\t            f(['a', 'b'], [1, 2, 3])\n    1643\t            ''',\n    1644\t            arcz=\".1 17 7.  .2 23 34 45 5.  -22 2-2  -33 3-3  -44 4-4\",\n    1645\t            arcz_missing=\"3-3 -44 4-4\",\n    1646\t        )\n\nthen you can do::\n\n    % python lab/extract_code.py tests/test_arcs.py 1637\n    def f(a, b):\n        c = (i for i in a)          # 2\n        d = (j for j in b)          # 3\n        e = (k for k in b)          # 4\n        return dict(zip(c, d))\n\n    f(['a', 'b'], [1, 2, 3])\n    %\n\n\"\"\"\n\nimport sys\nimport textwrap\n\nif len(sys.argv) == 2:\n    fname, lineno = sys.argv[1].split(\":\")\nelse:\n    fname, lineno = sys.argv[1:]\nlineno = int(lineno)\n\nwith open(fname) as code_file:\n    lines = [\"\", *code_file]\n\n# Find opening triple-quote\nfor start in range(lineno, 0, -1):\n    line = lines[start]\n    if \"'''\" in line or '\"\"\"' in line:\n        break\n\nfor end in range(lineno+1, len(lines)):\n    line = lines[end]\n    if \"'''\" in line or '\"\"\"' in line:\n        break\n\ncode = \"\".join(lines[start+1: end])\ncode = textwrap.dedent(code)\n\nprint(code, end=\"\")\n", "lab/bpo_prelude.py": "import linecache, sys\n\ndef trace(frame, event, arg):\n    # The weird globals here is to avoid a NameError on shutdown...\n    if frame.f_code.co_filename == globals().get(\"__file__\"):\n        lineno = frame.f_lineno\n        line = linecache.getline(__file__, lineno).rstrip()\n        print(\"{} {}: {}\".format(event[:4], lineno, line))\n    return trace\n\nprint(sys.version)\nsys.settrace(trace)\n\n", "lab/show_platform.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\nimport platform\nimport types\n\nfor n in dir(platform):\n    if n.startswith(\"_\"):\n        continue\n    v = getattr(platform, n)\n    if isinstance(v, types.ModuleType):\n        continue\n    if callable(v):\n        try:\n            v = v()\n            n += \"()\"\n        except:\n            continue\n    print(f\"{n:>30}: {v!r}\")\n", "lab/hack_pyc.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\" Wicked hack to get .pyc files to do bytecode tracing instead of\n    line tracing.\n\"\"\"\n\nimport marshal, new, opcode, sys, types\n\nfrom lnotab import lnotab_numbers, lnotab_string\n\nclass PycFile:\n    def read(self, f):\n        if isinstance(f, basestring):\n            f = open(f, \"rb\")\n        self.magic = f.read(4)\n        self.modtime = f.read(4)\n        self.code = marshal.load(f)\n\n    def write(self, f):\n        if isinstance(f, basestring):\n            f = open(f, \"wb\")\n        f.write(self.magic)\n        f.write(self.modtime)\n        marshal.dump(self.code, f)\n\n    def hack_line_numbers(self):\n        self.code = hack_line_numbers(self.code)\n\ndef hack_line_numbers(code):\n    \"\"\" Replace a code object's line number information to claim that every\n        byte of the bytecode is a new source line.  Returns a new code\n        object.  Also recurses to hack the line numbers in nested code objects.\n    \"\"\"\n\n    # Create a new lnotab table.  Each opcode is claimed to be at\n    # 1000*lineno + (opcode number within line), so for example, the opcodes on\n    # source line 12 will be given new line numbers 12000, 12001, 12002, etc.\n    old_num = list(lnotab_numbers(code.co_lnotab, code.co_firstlineno))\n    n_bytes = len(code.co_code)\n    new_num = []\n    line = 0\n    opnum_in_line = 0\n    i_byte = 0\n    while i_byte < n_bytes:\n        if old_num and i_byte == old_num[0][0]:\n            line = old_num.pop(0)[1]\n            opnum_in_line = 0\n        new_num.append((i_byte, 100000000 + 1000*line + opnum_in_line))\n        if ord(code.co_code[i_byte]) >= opcode.HAVE_ARGUMENT:\n            i_byte += 3\n        else:\n            i_byte += 1\n        opnum_in_line += 1\n\n    # new_num is a list of pairs, (byteoff, lineoff).  Turn it into an lnotab.\n    new_firstlineno = new_num[0][1]-1\n    new_lnotab = lnotab_string(new_num, new_firstlineno)\n\n    # Recurse into code constants in this code object.\n    new_consts = []\n    for const in code.co_consts:\n        if type(const) == types.CodeType:\n            new_consts.append(hack_line_numbers(const))\n        else:\n            new_consts.append(const)\n\n    # Create a new code object, just like the old one, except with new\n    # line numbers.\n    new_code = new.code(\n        code.co_argcount, code.co_nlocals, code.co_stacksize, code.co_flags,\n        code.co_code, tuple(new_consts), code.co_names, code.co_varnames,\n        code.co_filename, code.co_name, new_firstlineno, new_lnotab\n    )\n\n    return new_code\n\ndef hack_file(f):\n    pyc = PycFile()\n    pyc.read(f)\n    pyc.hack_line_numbers()\n    pyc.write(f)\n\nif __name__ == '__main__':\n    hack_file(sys.argv[1])\n", "lab/find_class.py": "class Parent:\n    def meth(self):\n        print(\"METH\")\n\nclass Child(Parent):\n    pass\n\ndef trace(frame, event, args):\n    # Thanks to Aleksi Torhamo for code and idea.\n    co = frame.f_code\n    fname = co.co_name\n    if not co.co_varnames:\n        return\n    locs = frame.f_locals\n    first_arg = co.co_varnames[0]\n    if co.co_argcount:\n        self = locs[first_arg]\n    elif co.co_flags & 0x04:    # *args syntax\n        self = locs[first_arg][0]\n    else:\n        return\n\n    func = getattr(self, fname).__func__\n    if hasattr(func, '__qualname__'):\n        qname = func.__qualname__\n    else:\n        for cls in self.__class__.__mro__:\n            f = cls.__dict__.get(fname, None)\n            if f is None:\n                continue\n            if f is func:\n                qname = cls.__name__ + \".\" + fname\n                break\n    print(f\"{event}: {self}.{fname} {qname}\")\n    return trace\n\nimport sys\nsys.settrace(trace)\n\nChild().meth()\n", "lab/branches.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n# Demonstrate some issues with coverage.py branch testing.\n\ndef my_function(x):\n    \"\"\"This isn't real code, just snippets...\"\"\"\n\n    # An infinite loop is structurally still a branch: it can next execute the\n    # first line of the loop, or the first line after the loop.  But\n    # \"while True\" will never jump to the line after the loop, so the line\n    # is shown as a partial branch:\n\n    i = 0\n    while True:\n        print(\"In while True\")\n        if i > 0:\n            break\n        i += 1\n    print(\"Left the True loop\")\n\n    # Notice that \"while 1\" also has this problem.  Even though the compiler\n    # knows there's no computation at the top of the loop, it's still expressed\n    # in bytecode as a branch with two possibilities.\n\n    i = 0\n    while 1:\n        print(\"In while 1\")\n        if i > 0:\n            break\n        i += 1\n    print(\"Left the 1 loop\")\n\n    # Coverage.py lets developers exclude lines that they know will not be\n    # executed.  So far, the branch coverage doesn't use all that information\n    # when deciding which lines are partially executed.\n    #\n    # Here, even though the else line is explicitly marked as never executed,\n    # the if line complains that it never branched to the else:\n\n    if x < 1000:\n        # This branch is always taken\n        print(\"x is reasonable\")\n    else:   # pragma: nocover\n        print(\"this never happens\")\n\n    # try-except structures are complex branches.  An except clause with a\n    # type is a three-way branch: there could be no exception, there could be\n    # a matching exception, and there could be a non-matching exception.\n    #\n    # Here we run the code twice: once with no exception, and once with a\n    # matching exception.  The \"except\" line is marked as partial because we\n    # never executed its third case: a non-matching exception.\n\n    for y in (1, 2):\n        try:\n            if y % 2:\n                raise ValueError(\"y is odd!\")\n        except ValueError:\n            print(\"y must have been odd\")\n        print(\"done with y\")\n    print(\"done with 1, 2\")\n\n    # Another except clause, but this time all three cases are executed.  No\n    # partial lines are shown:\n\n    for y in (0, 1, 2):\n        try:\n            if y % 2:\n                raise ValueError(\"y is odd!\")\n            if y == 0:\n                raise Exception(\"zero!\")\n        except ValueError:\n            print(\"y must have been odd\")\n        except:\n            print(\"y is something else\")\n        print(\"done with y\")\n    print(\"done with 0, 1, 2\")\n\n\nmy_function(1)\n", "lab/parser.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Parser.py: a main for invoking code in coverage/parser.py\"\"\"\n\n\nimport collections\nimport dis\nimport glob\nimport optparse\nimport os\nimport re\nimport sys\nimport textwrap\nimport types\n\nfrom coverage.parser import PythonParser\nfrom coverage.python import get_python_source\n\n\nclass ParserMain:\n    \"\"\"A main for code parsing experiments.\"\"\"\n\n    def main(self, args):\n        \"\"\"A main function for trying the code from the command line.\"\"\"\n\n        parser = optparse.OptionParser()\n        parser.add_option(\n            \"-d\", action=\"store_true\", dest=\"dis\",\n            help=\"Disassemble\"\n        )\n        parser.add_option(\n            \"-R\", action=\"store_true\", dest=\"recursive\",\n            help=\"Recurse to find source files\"\n        )\n        parser.add_option(\n            \"-s\", action=\"store_true\", dest=\"source\",\n            help=\"Show analyzed source\"\n        )\n        parser.add_option(\n            \"-t\", action=\"store_true\", dest=\"tokens\",\n            help=\"Show tokens\"\n        )\n\n        options, args = parser.parse_args()\n        if options.recursive:\n            if args:\n                root = args[0]\n            else:\n                root = \".\"\n            for root, _, _ in os.walk(root):\n                for f in glob.glob(root + \"/*.py\"):\n                    self.one_file(options, f)\n        elif not args:\n            parser.print_help()\n        else:\n            self.one_file(options, args[0])\n\n    def one_file(self, options, filename):\n        \"\"\"Process just one file.\"\"\"\n        # `filename` can have a line number suffix. In that case, extract those\n        # lines, dedent them, and use that.  This is for trying test cases\n        # embedded in the test files.\n        if match := re.search(r\"^(.*):(\\d+)-(\\d+)$\", filename):\n            filename, start, end = match.groups()\n            start, end = int(start), int(end)\n        else:\n            start = end = None\n\n        try:\n            text = get_python_source(filename)\n            if start is not None:\n                lines = text.splitlines(True)\n                text = textwrap.dedent(\"\".join(lines[start-1:end]).replace(\"\\\\\\\\\", \"\\\\\"))\n            pyparser = PythonParser(text, filename=filename, exclude=r\"no\\s*cover\")\n            pyparser.parse_source()\n        except Exception as err:\n            print(f\"{err}\")\n            return\n\n        if options.dis:\n            print(\"Main code:\")\n            disassemble(pyparser.text)\n\n        arcs = pyparser.arcs()\n\n        if options.source or options.tokens:\n            pyparser.show_tokens = options.tokens\n            pyparser.parse_source()\n\n            if options.source:\n                arc_chars = self.arc_ascii_art(arcs)\n                if arc_chars:\n                    arc_width = max(len(a) for a in arc_chars.values())\n\n                exit_counts = pyparser.exit_counts()\n\n                for lineno, ltext in enumerate(pyparser.text.splitlines(), start=1):\n                    marks = [' '] * 6\n                    a = ' '\n                    if lineno in pyparser.raw_statements:\n                        marks[0] = '-'\n                    if lineno in pyparser.statements:\n                        marks[1] = '='\n                    exits = exit_counts.get(lineno, 0)\n                    if exits > 1:\n                        marks[2] = str(exits)\n                    if lineno in pyparser.raw_docstrings:\n                        marks[3] = '\"'\n                    if lineno in pyparser.raw_classdefs:\n                        marks[3] = 'C'\n                    if lineno in pyparser.raw_excluded:\n                        marks[4] = 'X'\n                    elif lineno in pyparser.excluded:\n                        marks[4] = '\u00d7'\n                    if lineno in pyparser._multiline.values():\n                        marks[5] = 'o'\n                    elif lineno in pyparser._multiline.keys():\n                        marks[5] = '.'\n\n                    if arc_chars:\n                        a = arc_chars[lineno].ljust(arc_width)\n                    else:\n                        a = \"\"\n\n                    print(\"%4d %s%s %s\" % (lineno, \"\".join(marks), a, ltext))\n\n    def arc_ascii_art(self, arcs):\n        \"\"\"Draw arcs as ascii art.\n\n        Returns a dictionary mapping line numbers to ascii strings to draw for\n        that line.\n\n        \"\"\"\n        plus_ones = set()\n        arc_chars = collections.defaultdict(str)\n        for lfrom, lto in sorted(arcs):\n            if lfrom < 0:\n                arc_chars[lto] += 'v'\n            elif lto < 0:\n                arc_chars[lfrom] += '^'\n            else:\n                if lfrom == lto - 1:\n                    plus_ones.add(lfrom)\n                    arc_chars[lfrom] += \"\"      # ensure this line is in arc_chars\n                    continue\n                if lfrom < lto:\n                    l1, l2 = lfrom, lto\n                else:\n                    l1, l2 = lto, lfrom\n                w = first_all_blanks(arc_chars[l] for l in range(l1, l2+1))\n                for l in range(l1, l2+1):\n                    if l == lfrom:\n                        ch = '<'\n                    elif l == lto:\n                        ch = '>'\n                    else:\n                        ch = '|'\n                    arc_chars[l] = set_char(arc_chars[l], w, ch)\n\n        # Add the plusses as the first character\n        for lineno, arcs in arc_chars.items():\n            arc_chars[lineno] = (\n                (\"+\" if lineno in plus_ones else \" \") +\n                arcs\n            )\n\n        return arc_chars\n\n\ndef all_code_objects(code):\n    \"\"\"Iterate over all the code objects in `code`.\"\"\"\n    stack = [code]\n    while stack:\n        # We're going to return the code object on the stack, but first\n        # push its children for later returning.\n        code = stack.pop()\n        stack.extend(c for c in code.co_consts if isinstance(c, types.CodeType))\n        yield code\n\n\ndef disassemble(text):\n    \"\"\"Disassemble code, for ad-hoc experimenting.\"\"\"\n\n    code = compile(text, \"\", \"exec\", dont_inherit=True)\n    for code_obj in all_code_objects(code):\n        if text:\n            srclines = text.splitlines()\n        else:\n            srclines = None\n        print(\"\\n%s: \" % code_obj)\n        upto = None\n        for inst in dis.get_instructions(code_obj):\n            if inst.starts_line is not None:\n                if srclines:\n                    upto = upto or inst.starts_line - 1\n                    while upto <= inst.starts_line - 1:\n                        print(\"{:>100}{}\".format(\"\", srclines[upto]))\n                        upto += 1\n                elif inst.offset > 0:\n                    print(\"\")\n            line = inst._disassemble()\n            print(f\"{line:<70}\")\n\n    print(\"\")\n\n\ndef set_char(s, n, c):\n    \"\"\"Set the nth char of s to be c, extending s if needed.\"\"\"\n    s = s.ljust(n)\n    return s[:n] + c + s[n+1:]\n\n\ndef blanks(s):\n    \"\"\"Return the set of positions where s is blank.\"\"\"\n    return {i for i, c in enumerate(s) if c == \" \"}\n\n\ndef first_all_blanks(ss):\n    \"\"\"Find the first position that is all blank in the strings ss.\"\"\"\n    ss = list(ss)\n    blankss = blanks(ss[0])\n    for s in ss[1:]:\n        blankss &= blanks(s)\n    if blankss:\n        return min(blankss)\n    else:\n        return max(len(s) for s in ss)\n\n\nif __name__ == '__main__':\n    ParserMain().main(sys.argv[1:])\n", "lab/branch_trace.py": "import sys\n\npairs = set()\nlast = -1\n\ndef trace(frame, event, arg):\n    global last\n    if event == \"line\":\n        this = frame.f_lineno\n        pairs.add((last, this))\n        last = this\n    return trace\n\ncode = open(sys.argv[1]).read()\nsys.settrace(trace)\nexec(code)\nprint(sorted(pairs))\n", "lab/show_pyc.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nDump the contents of a .pyc file.\n\nThe output will only be correct if run with the same version of Python that\nproduced the .pyc.\n\n\"\"\"\n\nimport binascii\nimport dis\nimport marshal\nimport struct\nimport sys\nimport time\nimport types\n\n\ndef show_pyc_file(fname):\n    f = open(fname, \"rb\")\n    magic = f.read(4)\n    print(\"magic %s\" % (binascii.hexlify(magic)))\n    read_date_and_size = True\n    flags = struct.unpack('<L', f.read(4))[0]\n    hash_based = bool(flags & 0x01)\n    check_source = bool(flags & 0x02)\n    print(f\"flags {flags:#08x}\")\n    if hash_based:\n        source_hash = f.read(8)\n        read_date_and_size = False\n        print(f\"hash {binascii.hexlify(source_hash)}\")\n        print(f\"check_source {check_source}\")\n    if read_date_and_size:\n        moddate = f.read(4)\n        modtime = time.asctime(time.localtime(struct.unpack('<L', moddate)[0]))\n        print(f\"moddate {binascii.hexlify(moddate)} ({modtime})\")\n        size = f.read(4)\n        print(\"pysize %s (%d)\" % (binascii.hexlify(size), struct.unpack('<L', size)[0]))\n    code = marshal.load(f)\n    show_code(code)\n\ndef show_py_file(fname):\n    text = open(fname).read().replace('\\r\\n', '\\n')\n    show_py_text(text, fname=fname)\n\ndef show_py_text(text, fname=\"<string>\"):\n    code = compile(text, fname, \"exec\", dont_inherit=True)\n    show_code(code)\n\nCO_FLAGS = [\n    ('CO_OPTIMIZED',                0x00001),\n    ('CO_NEWLOCALS',                0x00002),\n    ('CO_VARARGS',                  0x00004),\n    ('CO_VARKEYWORDS',              0x00008),\n    ('CO_NESTED',                   0x00010),\n    ('CO_GENERATOR',                0x00020),\n    ('CO_NOFREE',                   0x00040),\n    ('CO_COROUTINE',                0x00080),\n    ('CO_ITERABLE_COROUTINE',       0x00100),\n    ('CO_ASYNC_GENERATOR',          0x00200),\n    ('CO_GENERATOR_ALLOWED',        0x01000),\n]\n\nif sys.version_info < (3, 9):\n    CO_FLAGS += [\n        ('CO_FUTURE_DIVISION',          0x02000),\n        ('CO_FUTURE_ABSOLUTE_IMPORT',   0x04000),\n        ('CO_FUTURE_WITH_STATEMENT',    0x08000),\n        ('CO_FUTURE_PRINT_FUNCTION',    0x10000),\n        ('CO_FUTURE_UNICODE_LITERALS',  0x20000),\n        ('CO_FUTURE_BARRY_AS_BDFL',     0x40000),\n        ('CO_FUTURE_GENERATOR_STOP',    0x80000),\n    ]\nelse:\n    CO_FLAGS += [\n        ('CO_FUTURE_DIVISION',          0x0020000),\n        ('CO_FUTURE_ABSOLUTE_IMPORT',   0x0040000),\n        ('CO_FUTURE_WITH_STATEMENT',    0x0080000),\n        ('CO_FUTURE_PRINT_FUNCTION',    0x0100000),\n        ('CO_FUTURE_UNICODE_LITERALS',  0x0200000),\n        ('CO_FUTURE_BARRY_AS_BDFL',     0x0400000),\n        ('CO_FUTURE_GENERATOR_STOP',    0x0800000),\n        ('CO_FUTURE_ANNOTATIONS',       0x1000000),\n    ]\n\n\ndef show_code(code, indent='', number=None):\n    label = \"\"\n    if number is not None:\n        label = \"%d: \" % number\n    print(f\"{indent}{label}code\")\n    indent += \"    \"\n    print(f\"{indent}name {code.co_name!r}\")\n    print(\"%sargcount %d\" % (indent, code.co_argcount))\n    print(\"%snlocals %d\" % (indent, code.co_nlocals))\n    print(\"%sstacksize %d\" % (indent, code.co_stacksize))\n    print(f\"{indent}flags {code.co_flags:04x}: {flag_words(code.co_flags, CO_FLAGS)}\")\n    show_hex(\"code\", code.co_code, indent=indent)\n    dis.disassemble(code)\n    print(\"%sconsts\" % indent)\n    for i, const in enumerate(code.co_consts):\n        if type(const) == types.CodeType:\n            show_code(const, indent+\"    \", number=i)\n        else:\n            print(\"    %s%d: %r\" % (indent, i, const))\n    print(f\"{indent}names {code.co_names!r}\")\n    print(f\"{indent}varnames {code.co_varnames!r}\")\n    print(f\"{indent}freevars {code.co_freevars!r}\")\n    print(f\"{indent}cellvars {code.co_cellvars!r}\")\n    print(f\"{indent}filename {code.co_filename!r}\")\n    print(\"%sfirstlineno %d\" % (indent, code.co_firstlineno))\n    show_hex(\"lnotab\", code.co_lnotab, indent=indent)\n    print(\"    {}{}\".format(indent, \", \".join(f\"{line!r}:{byte!r}\" for byte, line in lnotab_interpreted(code))))\n    if hasattr(code, \"co_linetable\"):\n        show_hex(\"linetable\", code.co_linetable, indent=indent)\n    if hasattr(code, \"co_lines\"):\n        print(\"    {}co_lines {}\".format(\n            indent,\n            \", \".join(f\"{line!r}:{start!r}-{end!r}\" for start, end, line in code.co_lines())\n        ))\n\ndef show_hex(label, h, indent):\n    h = binascii.hexlify(h)\n    if len(h) < 60:\n        print(\"{}{} {}\".format(indent, label, h.decode('ascii')))\n    else:\n        print(f\"{indent}{label}\")\n        for i in range(0, len(h), 60):\n            print(\"{}   {}\".format(indent, h[i:i+60].decode('ascii')))\n\n\ndef lnotab_interpreted(code):\n    # Adapted from dis.py in the standard library.\n    byte_increments = code.co_lnotab[0::2]\n    line_increments = code.co_lnotab[1::2]\n\n    last_line_num = None\n    line_num = code.co_firstlineno\n    byte_num = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if line_num != last_line_num:\n                yield (byte_num, line_num)\n                last_line_num = line_num\n            byte_num += byte_incr\n        if line_incr >= 0x80:\n            line_incr -= 0x100\n        line_num += line_incr\n    if line_num != last_line_num:\n        yield (byte_num, line_num)\n\ndef flag_words(flags, flag_defs):\n    words = []\n    for word, flag in flag_defs:\n        if flag & flags:\n            words.append(word)\n    return \", \".join(words)\n\ndef show_file(fname):\n    if fname.endswith('pyc'):\n        show_pyc_file(fname)\n    elif fname.endswith('py'):\n        show_py_file(fname)\n    else:\n        print(\"Odd file:\", fname)\n\ndef main(args):\n    if args[0] == '-c':\n        show_py_text(\" \".join(args[1:]).replace(\";\", \"\\n\"))\n    else:\n        for a in args:\n            show_file(a)\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n", "lab/platform_info.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Dump information so we can get a quick look at what's available.\"\"\"\n\nimport platform\nimport sys\n\n\ndef whatever(f):\n    try:\n        return f()\n    except:\n        return f\n\n\ndef dump_module(mod):\n    print(f\"\\n###  {mod.__name__} ---------------------------\")\n    for name in dir(mod):\n        if name.startswith(\"_\"):\n            continue\n        print(f\"{name:30s}: {whatever(getattr(mod, name))!r:.100}\")\n\n\nfor mod in [platform, sys]:\n    dump_module(mod)\n", "lab/select_contexts.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\\\nSelect certain contexts from a coverage.py data file.\n\"\"\"\n\nimport argparse\nimport re\nimport sys\n\nimport coverage\n\n\ndef main(argv):\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--include\", type=str, help=\"Regex for contexts to keep\")\n    parser.add_argument(\"--exclude\", type=str, help=\"Regex for contexts to discard\")\n    args = parser.parse_args(argv)\n\n    print(\"** Note: this is a proof-of-concept. Support is not promised. **\")\n    print(\"Feedback is appreciated: https://github.com/nedbat/coveragepy/issues/668\")\n\n    cov_in = coverage.Coverage()\n    cov_in.load()\n    data_in = cov_in.get_data()\n    print(f\"Contexts in {data_in.data_filename()}:\")\n    for ctx in sorted(data_in.measured_contexts()):\n        print(f\"    {ctx}\")\n\n    if args.include is None and args.exclude is None:\n        print(\"Nothing to do, no output written.\")\n        return\n\n    out_file = \"output.data\"\n    file_names = data_in.measured_files()\n    print(f\"{len(file_names)} measured files\")\n    print(f\"Writing to {out_file}\")\n    cov_out = coverage.Coverage(data_file=out_file)\n    data_out = cov_out.get_data()\n\n    for ctx in sorted(data_in.measured_contexts()):\n        if args.include is not None:\n            if not re.search(args.include, ctx):\n                print(f\"Skipping context {ctx}, not included\")\n                continue\n        if args.exclude is not None:\n            if re.search(args.exclude, ctx):\n                print(f\"Skipping context {ctx}, excluded\")\n                continue\n        print(f\"Keeping context {ctx}\")\n        data_in.set_query_context(ctx)\n        data_out.set_context(ctx)\n        if data_in.has_arcs():\n            data_out.add_arcs({f: data_in.arcs(f) for f in file_names})\n        else:\n            data_out.add_lines({f: data_in.lines(f) for f in file_names})\n\n    for fname in file_names:\n        data_out.touch_file(fname, data_in.file_tracer(fname))\n\n    cov_out.save()\n\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv[1:]))\n", "benchmark/run.py": "import optparse\nfrom pathlib import Path\n\nfrom benchmark import *\n\nparser = optparse.OptionParser()\nparser.add_option(\n    \"--clean\",\n    action=\"store_true\",\n    dest=\"clean\",\n    default=False,\n    help=\"Delete the results.json file before running benchmarks\"\n)\noptions, args = parser.parse_args()\n\nif options.clean:\n    results_file = Path(\"results.json\")\n    if results_file.exists():\n        results_file.unlink()\n        print(\"Deleted results.json\")\n\nif 0:\n    run_experiment(\n        py_versions=[\n            # Python(3, 11),\n            AdHocPython(\"/usr/local/cpython/v3.10.5\", \"v3.10.5\"),\n            AdHocPython(\"/usr/local/cpython/v3.11.0b3\", \"v3.11.0b3\"),\n            AdHocPython(\"/usr/local/cpython/94231\", \"94231\"),\n        ],\n        cov_versions=[\n            Coverage(\"6.4.1\", \"coverage==6.4.1\"),\n        ],\n        projects=[\n            AdHocProject(\"/src/bugs/bug1339/bug1339.py\"),\n            SlipcoverBenchmark(\"bm_sudoku.py\"),\n            SlipcoverBenchmark(\"bm_spectral_norm.py\"),\n        ],\n        rows=[\"cov\", \"proj\"],\n        column=\"pyver\",\n        ratios=[\n            (\"3.11b3 vs 3.10\", \"v3.11.0b3\", \"v3.10.5\"),\n            (\"94231 vs 3.10\", \"94231\", \"v3.10.5\"),\n        ],\n    )\n\n\nif 0:\n    run_experiment(\n        py_versions=[\n            Python(3, 9),\n            Python(3, 11),\n        ],\n        cov_versions=[\n            Coverage(\"701\", \"coverage==7.0.1\"),\n            Coverage(\n                \"701.dynctx\", \"coverage==7.0.1\", [(\"dynamic_context\", \"test_function\")]\n            ),\n            Coverage(\"702\", \"coverage==7.0.2\"),\n            Coverage(\n                \"702.dynctx\", \"coverage==7.0.2\", [(\"dynamic_context\", \"test_function\")]\n            ),\n        ],\n        projects=[\n            ProjectAttrs(),\n        ],\n        rows=[\"proj\", \"pyver\"],\n        column=\"cov\",\n        ratios=[\n            (\".2 vs .1\", \"702\", \"701\"),\n            (\".1 dynctx cost\", \"701.dynctx\", \"701\"),\n            (\".2 dynctx cost\", \"702.dynctx\", \"702\"),\n        ],\n    )\n\n\nif 0:\n    # Compare two Python versions\n    v1 = 10\n    v2 = 11\n    run_experiment(\n        py_versions=[\n            Python(3, v1),\n            Python(3, v2),\n        ],\n        cov_versions=[\n            Coverage(\"753\", \"coverage==7.5.3\"),\n        ],\n        projects=[\n            ProjectMashumaro(),\n        ],\n        rows=[\"cov\", \"proj\"],\n        column=\"pyver\",\n        ratios=[\n            (f\"3.{v2} vs 3.{v1}\", f\"python3.{v2}\", f\"python3.{v1}\"),\n        ],\n    )\n\nif 1:\n    # Compare sysmon on many projects\n\n    run_experiment(\n        py_versions=[\n            Python(3, 12),\n        ],\n        cov_versions=[\n            NoCoverage(\"nocov\"),\n            CoverageSource(slug=\"ctrace\", env_vars={\"COVERAGE_CORE\": \"ctrace\"}),\n            CoverageSource(slug=\"sysmon\", env_vars={\"COVERAGE_CORE\": \"sysmon\"}),\n        ],\n        projects=[\n            # ProjectSphinx(),  # Works, slow\n            ProjectPygments(),  # Works\n            # ProjectRich(),  # Doesn't work\n            # ProjectTornado(),  # Works, tests fail\n            # ProjectDulwich(),  # Works\n            # ProjectBlack(),  # Works, slow\n            # ProjectMpmath(),  # Works, slow\n            ProjectMypy(),  # Works, slow\n            # ProjectHtml5lib(),  # Works\n            # ProjectUrllib3(),  # Works\n        ],\n        rows=[\"pyver\", \"proj\"],\n        column=\"cov\",\n        ratios=[\n            (f\"ctrace%\", \"ctrace\", \"nocov\"),\n            (f\"sysmon%\", \"sysmon\", \"nocov\"),\n        ],\n        load=True,\n    )\n\nif 0:\n    # Compare current Coverage source against shipped version\n    run_experiment(\n        py_versions=[\n            Python(3, 11),\n        ],\n        cov_versions=[\n            Coverage(\"pip\", \"coverage\"),\n            CoverageSource(slug=\"latest\"),\n        ],\n        projects=[\n            ProjectMashumaro(),\n            ProjectOperator(),\n        ],\n        rows=[\"pyver\", \"proj\"],\n        column=\"cov\",\n        ratios=[\n            (f\"Latest vs shipped\", \"latest\", \"pip\"),\n        ],\n    )\n\nif 0:\n    # Compare 3.12 coverage vs no coverage\n    run_experiment(\n        py_versions=[\n            Python(3, 12),\n        ],\n        cov_versions=[\n            NoCoverage(\"nocov\"),\n            Coverage(\"732\", \"coverage==7.3.2\"),\n            CoverageSource(\n                slug=\"sysmon\",\n                env_vars={\"COVERAGE_CORE\": \"sysmon\"},\n            ),\n        ],\n        projects=[\n            ProjectMashumaro(),     # small: \"-k ck\"\n            ProjectOperator(),      # small: \"-k irk\"\n        ],\n        rows=[\"pyver\", \"proj\"],\n        column=\"cov\",\n        ratios=[\n            (f\"732%\", \"732\", \"nocov\"),\n            (f\"sysmon%\", \"sysmon\", \"nocov\"),\n        ],\n    )\n\nif 0:\n    # Compare 3.12 coverage vs no coverage\n    run_experiment(\n        py_versions=[\n            Python(3, 12),\n        ],\n        cov_versions=[\n            NoCoverage(\"nocov\"),\n            Coverage(\"732\", \"coverage==7.3.2\"),\n            CoverageSource(\n                slug=\"sysmon\",\n                env_vars={\"COVERAGE_CORE\": \"sysmon\"},\n            ),\n        ],\n        projects=[\n            ProjectMashumaro(),         # small: \"-k ck\"\n            ProjectMashumaroBranch(),   # small: \"-k ck\"\n        ],\n        rows=[\"pyver\", \"proj\"],\n        column=\"cov\",\n        ratios=[\n            (f\"732%\", \"732\", \"nocov\"),\n            (f\"sysmon%\", \"sysmon\", \"nocov\"),\n        ],\n    )\n", "benchmark/benchmark.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Run performance comparisons for versions of coverage\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport contextlib\nimport itertools\nimport json\nimport os\nimport random\nimport shutil\nimport statistics\nimport subprocess\nimport sys\nimport time\nimport traceback\n\nfrom pathlib import Path\n\nfrom dataclasses import dataclass\nfrom io import TextIOWrapper\nfrom types import TracebackType\nfrom typing import Any, Dict, Iterable, Iterator, Mapping, Optional, Tuple, Type, cast\n\nimport requests\nimport tabulate\n\nTweaksType = Optional[Iterable[Tuple[str, Any]]]\nEnv_VarsType = Optional[Dict[str, str]]\n\n\nclass ShellSession:\n    \"\"\"A logged shell session.\n\n    The duration of the last command is available as .last_duration.\n    \"\"\"\n\n    def __init__(self, output_filename: str):\n        self.output_filename = output_filename\n        self.last_duration: float = 0\n        self.foutput: TextIOWrapper | None = None\n        self.env_vars = {\"PATH\": os.getenv(\"PATH\")}\n\n    def __enter__(self) -> ShellSession:\n        self.foutput = open(self.output_filename, \"a\", encoding=\"utf-8\")\n        print(f\"Logging output to {os.path.abspath(self.output_filename)}\")\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -> None:\n        if self.foutput is not None:\n            self.foutput.close()\n\n    @contextlib.contextmanager\n    def set_env(self, *env_varss: dict[str, str] | None) -> Iterator[None]:\n        \"\"\"Set environment variables.\n\n        All the arguments are dicts of name:value, or None.  All are applied\n        to the environment variables.\n        \"\"\"\n        old_env_vars = self.env_vars\n        self.env_vars = dict(old_env_vars)\n        for env_vars in env_varss:\n            self.env_vars.update(env_vars or {})\n        try:\n            yield\n        finally:\n            self.env_vars = old_env_vars\n\n    def print(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Print a message to this shell's log.\"\"\"\n        print(*args, **kwargs, file=self.foutput)\n\n    def print_banner(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Print a distinguished banner to the log.\"\"\"\n        self.print(\"\\n######> \", end=\"\")\n        self.print(*args, **kwargs)\n\n    def run_command(self, cmd: str) -> str:\n        \"\"\"\n        Run a command line (with a shell).\n\n        Returns:\n            str: the output of the command.\n\n        \"\"\"\n        self.print(f\"\\n### ========================\\n$ {cmd}\")\n        start = time.perf_counter()\n        proc = subprocess.run(\n            cmd,\n            shell=True,\n            check=False,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            env=cast(Mapping[str, str], self.env_vars),\n        )\n        output = proc.stdout.decode(\"utf-8\")\n        self.last_duration = time.perf_counter() - start\n        self.print(output, end=\"\")\n        self.print(f\"(was: {cmd})\")\n        self.print(f\"(in {os.getcwd()}, duration: {self.last_duration:.3f}s)\")\n\n        if proc.returncode != 0:\n            self.print(f\"ERROR: command returned {proc.returncode}\")\n            raise Exception(\n                f\"Command failed ({proc.returncode}): {cmd!r}, output was:\\n{output}\"\n            )\n\n        return output.strip()\n\n\ndef rmrf(path: Path) -> None:\n    \"\"\"\n    Remove a directory tree.  It's OK if it doesn't exist.\n    \"\"\"\n    if path.exists():\n        shutil.rmtree(path)\n\n\n@contextlib.contextmanager\ndef change_dir(newdir: Path) -> Iterator[Path]:\n    \"\"\"\n    Change to a new directory, and then change back.\n\n    Will make the directory if needed.\n    \"\"\"\n    old_dir = os.getcwd()\n    newdir.mkdir(parents=True, exist_ok=True)\n    os.chdir(newdir)\n    try:\n        yield newdir\n    finally:\n        os.chdir(old_dir)\n\n\n@contextlib.contextmanager\ndef file_replace(file_name: Path, old_text: str, new_text: str) -> Iterator[None]:\n    \"\"\"\n    Replace some text in `file_name`, and change it back.\n    \"\"\"\n    file_text = \"\"\n    if old_text:\n        file_text = file_name.read_text()\n        if old_text not in file_text:\n            raise Exception(\"Old text {old_text!r} not found in {file_name}\")\n        updated_text = file_text.replace(old_text, new_text)\n        file_name.write_text(updated_text)\n    try:\n        yield\n    finally:\n        if old_text:\n            file_name.write_text(file_text)\n\n\ndef file_must_exist(file_name: str, kind: str = \"file\") -> Path:\n    \"\"\"\n    Check that a file exists, for early validation of pip (etc) arguments.\n\n    Raises an exception if it doesn't exist.  Returns the resolved path if it\n    does exist so we can use relative paths and they'll still work once we've\n    cd'd to the temporary workspace.\n    \"\"\"\n    path = Path(file_name).expanduser().resolve()\n    if not path.exists():\n        kind = kind[0].upper() + kind[1:]\n        raise RuntimeError(f\"{kind} {file_name!r} doesn't exist\")\n    return path\n\n\ndef url_must_exist(url: str) -> bool:\n    \"\"\"\n    Check that a URL exists, for early validation of pip (etc) arguments.\n\n    Raises an exception if it doesn't exist.\n    \"\"\"\n    resp = requests.head(url)\n    resp.raise_for_status()\n    return True\n\n\nclass ProjectToTest:\n    \"\"\"Information about a project to use as a test case.\"\"\"\n\n    # Where can we clone the project from?\n    git_url: str = \"\"\n    slug: str = \"\"\n    env_vars: Env_VarsType = {}\n\n    def __init__(self) -> None:\n        url_must_exist(self.git_url)\n        if not self.slug:\n            if self.git_url:\n                self.slug = self.git_url.split(\"/\")[-1]\n\n    def shell(self) -> ShellSession:\n        return ShellSession(f\"output_{self.slug}.log\")\n\n    def make_dir(self) -> None:\n        self.dir = Path(f\"work_{self.slug}\")\n        if self.dir.exists():\n            rmrf(self.dir)\n\n    def get_source(self, shell: ShellSession, retries: int = 5) -> None:\n        \"\"\"Get the source of the project.\"\"\"\n        for retry in range(retries):\n            try:\n                shell.run_command(f\"git clone {self.git_url} {self.dir}\")\n                return\n            except Exception as e:\n                print(f\"Retrying to clone {self.git_url} due to error:\\n{e}\")\n                if retry == retries - 1:\n                    raise e\n\n    def prep_environment(self, env: Env) -> None:\n        \"\"\"Prepare the environment to run the test suite.\n\n        This is not timed.\n        \"\"\"\n\n    @contextlib.contextmanager\n    def tweak_coverage_settings(self, settings: TweaksType) -> Iterator[None]:\n        \"\"\"Tweak the coverage settings.\n\n        NOTE: This is not properly factored, and is only used by ToxProject now!!!\n        \"\"\"\n        yield\n\n    def pre_check(self, env: Env) -> None:\n        pass\n\n    def post_check(self, env: Env) -> None:\n        pass\n\n    def run_no_coverage(self, env: Env) -> float:\n        \"\"\"Run the test suite with no coverage measurement.\n\n        Returns the duration of the run.\n        \"\"\"\n        return 0.0\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        \"\"\"Run the test suite with coverage measurement.\n\n        Must install a particular version of coverage using `cov_ver.pip_args`.\n\n        Returns the duration of the run.\n        \"\"\"\n        return 0.0\n\n\nclass EmptyProject(ProjectToTest):\n    \"\"\"A dummy project for testing other parts of this code.\"\"\"\n\n    def __init__(self, slug: str = \"empty\", fake_durations: Iterable[float] = (1.23,)):\n        self.slug = slug\n        self.durations = iter(itertools.cycle(fake_durations))\n\n    def get_source(self, shell: ShellSession, retries: int = 5) -> None:\n        pass\n\n    def run_no_coverage(self, env: Env) -> float:\n        \"\"\"Run the test suite with coverage measurement.\"\"\"\n        return next(self.durations)\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        \"\"\"Run the test suite with coverage measurement.\"\"\"\n        return next(self.durations)\n\n\nclass ToxProject(ProjectToTest):\n    \"\"\"A project using tox to run the test suite.\"\"\"\n\n    env_vars: Env_VarsType = {\n        **(ProjectToTest.env_vars or {}),\n        # Allow some environment variables into the tox execution.\n        \"TOX_OVERRIDE\": \"testenv.pass_env+=COVERAGE_DEBUG,COVERAGE_CORE,COVERAGE_FORCE_CONFIG\",\n        \"COVERAGE_DEBUG\": \"config,sys\",\n    }\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install tox\")\n        self.run_tox(env, env.pyver.toxenv, \"--notest\")\n\n    def run_tox(self, env: Env, toxenv: str, toxargs: str = \"\") -> float:\n        \"\"\"Run a tox command. Return the duration.\"\"\"\n        env.shell.run_command(f\"{env.python} -m tox -v -e {toxenv} {toxargs}\")\n        return env.shell.last_duration\n\n    def run_no_coverage(self, env: Env) -> float:\n        return self.run_tox(env, env.pyver.toxenv, \"--skip-pkg-install\")\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        self.run_tox(env, env.pyver.toxenv, \"--notest\")\n        env.shell.run_command(\n            f\".tox/{env.pyver.toxenv}/bin/python -m pip install {cov_ver.pip_args}\"\n        )\n        with self.tweak_coverage_settings(cov_ver.tweaks):\n            self.pre_check(env)  # NOTE: Not properly factored, and only used from here.\n            duration = self.run_tox(env, env.pyver.toxenv, \"--skip-pkg-install\")\n            self.post_check(\n                env\n            )  # NOTE: Not properly factored, and only used from here.\n        return duration\n\n\nclass ProjectPytestHtml(ToxProject):\n    \"\"\"pytest-dev/pytest-html\"\"\"\n\n    git_url = \"https://github.com/pytest-dev/pytest-html\"\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        raise Exception(\"This doesn't work because options changed to tweaks\")\n        covenv = env.pyver.toxenv + \"-cov\"  # type: ignore[unreachable]\n        self.run_tox(env, covenv, \"--notest\")\n        env.shell.run_command(\n            f\".tox/{covenv}/bin/python -m pip install {cov_ver.pip_args}\"\n        )\n        if cov_ver.tweaks:\n            replace = (\"# reference: https\", f\"[run]\\n{cov_ver.tweaks}\\n#\")\n        else:\n            replace = (\"\", \"\")\n        with file_replace(Path(\".coveragerc\"), *replace):\n            env.shell.run_command(\"cat .coveragerc\")\n            env.shell.run_command(f\".tox/{covenv}/bin/python -m coverage debug sys\")\n            return self.run_tox(env, covenv, \"--skip-pkg-install\")\n\n\nclass ProjectDateutil(ToxProject):\n    \"\"\"dateutil/dateutil\"\"\"\n\n    git_url = \"https://github.com/dateutil/dateutil\"\n\n    def prep_environment(self, env: Env) -> None:\n        super().prep_environment(env)\n        env.shell.run_command(f\"{env.python} updatezinfo.py\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(\"echo No option to run without coverage\")\n        return 0\n\n\nclass ProjectAttrs(ToxProject):\n    \"\"\"python-attrs/attrs\"\"\"\n\n    git_url = \"https://github.com/python-attrs/attrs\"\n\n    @contextlib.contextmanager\n    def tweak_coverage_settings(self, tweaks: TweaksType) -> Iterator[None]:\n        return tweak_toml_coverage_settings(\"pyproject.toml\", tweaks)\n\n    def pre_check(self, env: Env) -> None:\n        env.shell.run_command(\"cat pyproject.toml\")\n\n    def post_check(self, env: Env) -> None:\n        env.shell.run_command(\"ls -al\")\n\n\nclass ProjectDjangoAuthToolkit(ToxProject):\n    \"\"\"jazzband/django-oauth-toolkit\"\"\"\n\n    git_url = \"https://github.com/jazzband/django-oauth-toolkit\"\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(\"echo No option to run without coverage\")\n        return 0\n\n\nclass ProjectDjango(ToxProject):\n    \"\"\"django/django\"\"\"\n\n    # brew install libmemcached\n    # pip install -e .\n    # coverage run tests/runtests.py --settings=test_sqlite\n    # coverage report --format=total --precision=6\n    # 32.848540\n\n\nclass ProjectMashumaro(ProjectToTest):\n    git_url = \"https://github.com/Fatal1ty/mashumaro\"\n\n    def __init__(self, more_pytest_args: str = \"\"):\n        super().__init__()\n        self.more_pytest_args = more_pytest_args\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install .\")\n        env.shell.run_command(f\"{env.python} -m pip install -r requirements-dev.txt\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m pytest {self.more_pytest_args}\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(\n            f\"{env.python} -m pytest --cov=mashumaro --cov=tests {self.more_pytest_args}\"\n        )\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectMashumaroBranch(ProjectMashumaro):\n    def __init__(self, more_pytest_args: str = \"\"):\n        super().__init__(more_pytest_args=\"--cov-branch \" + more_pytest_args)\n        self.slug = \"mashbranch\"\n\n\nclass ProjectOperator(ProjectToTest):\n    git_url = \"https://github.com/nedbat/operator\"\n\n    def __init__(self, more_pytest_args: str = \"\"):\n        super().__init__()\n        self.more_pytest_args = more_pytest_args\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install tox\")\n        Path(\"/tmp/operator_tmp\").mkdir(exist_ok=True)\n        env.shell.run_command(f\"{env.python} -m tox -e unit --notest\")\n        env.shell.run_command(f\"{env.python} -m tox -e unitnocov --notest\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(\n            f\"TMPDIR=/tmp/operator_tmp {env.python} -m tox -e unitnocov --skip-pkg-install\"\n            + f\" -- {self.more_pytest_args}\"\n        )\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(\n            f\"TMPDIR=/tmp/operator_tmp {env.python} -m tox -e unit --skip-pkg-install\"\n            + f\" -- {self.more_pytest_args}\"\n        )\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectPygments(ToxProject):\n    git_url = \"https://github.com/pygments/pygments\"\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        self.run_tox(env, env.pyver.toxenv, \"--notest\")\n        env.shell.run_command(\n            f\".tox/{env.pyver.toxenv}/bin/python -m pip install {cov_ver.pip_args}\"\n        )\n        with self.tweak_coverage_settings(cov_ver.tweaks):\n            self.pre_check(env)  # NOTE: Not properly factored, and only used here.\n            duration = self.run_tox(\n                env, env.pyver.toxenv, \"--skip-pkg-install -- --cov\"\n            )\n            self.post_check(env)  # NOTE: Not properly factored, and only used here.\n        return duration\n\n\nclass ProjectRich(ToxProject):\n    git_url = \"https://github.com/Textualize/rich\"\n\n    def prep_environment(self, env: Env) -> None:\n        raise Exception(\"Doesn't work due to poetry install error.\")\n\n\nclass ProjectTornado(ToxProject):\n    git_url = \"https://github.com/tornadoweb/tornado\"\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m tornado.test\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(f\"{env.python} -m coverage run -m tornado.test\")\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectDulwich(ToxProject):\n    git_url = \"https://github.com/jelmer/dulwich\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install -r requirements.txt\")\n        env.shell.run_command(f\"{env.python} -m pip install .\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m unittest tests.test_suite\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(\n            f\"{env.python} -m coverage run -m unittest tests.test_suite\"\n        )\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectBlack(ToxProject):\n    git_url = \"https://github.com/psf/black\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install -r test_requirements.txt\")\n        env.shell.run_command(f\"{env.python} -m pip install -e .[d]\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(\n            f\"{env.python} -m pytest tests --run-optional no_jupyter --no-cov --numprocesses 1\"\n        )\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(\n            f\"{env.python} -m pytest tests --run-optional no_jupyter --cov --numprocesses 1\"\n        )\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectMpmath(ProjectToTest):\n    git_url = \"https://github.com/mpmath/mpmath\"\n    select = \"-k 'not (torture or extra or functions2 or calculus or cli or elliptic or quad)'\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install .[develop]\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m pytest {self.select} --no-cov\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(f\"{env.python} -m pytest {self.select} --cov=mpmath\")\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectMypy(ToxProject):\n    git_url = \"https://github.com/python/mypy\"\n\n    SLOW_TESTS = \" or \".join([\n        \"PythonCmdline\",\n        \"PEP561Suite\",\n        \"PythonEvaluation\",\n        \"testdaemon\",\n        \"StubgenCmdLine\",\n        \"StubgenPythonSuite\",\n        \"TestRun\",\n        \"TestRunMultiFile\",\n        \"TestExternal\",\n        \"TestCommandLine\",\n        \"ErrorStreamSuite\",\n    ])\n\n    FAST = f\"-k 'not ({SLOW_TESTS})'\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install -r test-requirements.txt\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m pytest {self.FAST} --no-cov\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        pforce = Path(\"force.ini\")\n        pforce.write_text(\"[run]\\nbranch=false\\n\")\n        with env.shell.set_env({\"COVERAGE_FORCE_CONFIG\": str(pforce.resolve())}):\n            env.shell.run_command(f\"{env.python} -m pytest {self.FAST} --cov\")\n            duration = env.shell.last_duration\n            report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectHtml5lib(ToxProject):\n    git_url = \"https://github.com/html5lib/html5lib-python\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install -r requirements-test.txt\")\n        env.shell.run_command(f\"{env.python} -m pip install .\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m pytest\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(f\"{env.python} -m coverage run -m pytest\")\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectSphinx(ToxProject):\n    git_url = \"https://github.com/sphinx-doc/sphinx\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install .[test]\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m pytest\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(f\"{env.python} -m coverage run -m pytest\")\n        duration = env.shell.last_duration\n        env.shell.run_command(f\"{env.python} -m coverage combine\")\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\nclass ProjectUrllib3(ProjectToTest):\n    git_url = \"https://github.com/urllib3/urllib3\"\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install -r dev-requirements.txt\")\n        env.shell.run_command(f\"{env.python} -m pip install .\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        env.shell.run_command(f\"{env.python} -m pytest\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        env.shell.run_command(f\"{env.python} -m coverage run -m pytest\")\n        duration = env.shell.last_duration\n        report = env.shell.run_command(f\"{env.python} -m coverage report --precision=6\")\n        print(\"Results:\", report.splitlines()[-1])\n        return duration\n\n\ndef tweak_toml_coverage_settings(toml_file: str, tweaks: TweaksType) -> Iterator[None]:\n    if tweaks:\n        toml_inserts = []\n        for name, value in tweaks:\n            if isinstance(value, bool):\n                toml_inserts.append(f\"{name} = {str(value).lower()}\")\n            elif isinstance(value, str):\n                toml_inserts.append(f\"{name} = '{value}'\")\n            else:\n                raise Exception(f\"Can't tweak toml setting: {name} = {value!r}\")\n        header = \"[tool.coverage.run]\\n\"\n        insert = header + \"\\n\".join(toml_inserts) + \"\\n\"\n    else:\n        header = insert = \"\"\n    return file_replace(Path(toml_file), header, insert)  # type: ignore\n\n\nclass AdHocProject(ProjectToTest):\n    \"\"\"A standalone program to run locally.\"\"\"\n\n    def __init__(\n        self, python_file: str, cur_dir: str | None = None, pip_args: str = \"\"\n    ):\n        super().__init__()\n        self.python_file = Path(python_file)\n        if not self.python_file.exists():\n            raise ValueError(f\"Couldn't find {self.python_file} to run ad-hoc.\")\n        self.cur_dir = Path(cur_dir or self.python_file.parent)\n        if not self.cur_dir.exists():\n            raise ValueError(f\"Couldn't find {self.cur_dir} to run in.\")\n        self.pip_args = pip_args\n        self.slug = self.python_file.name\n\n    def get_source(self, shell: ShellSession, retries: int = 5) -> None:\n        pass\n\n    def prep_environment(self, env: Env) -> None:\n        env.shell.run_command(f\"{env.python} -m pip install {self.pip_args}\")\n\n    def run_no_coverage(self, env: Env) -> float:\n        with change_dir(self.cur_dir):\n            env.shell.run_command(f\"{env.python} {self.python_file}\")\n        return env.shell.last_duration\n\n    def run_with_coverage(self, env: Env, cov_ver: Coverage) -> float:\n        env.shell.run_command(f\"{env.python} -m pip install {cov_ver.pip_args}\")\n        with change_dir(self.cur_dir):\n            env.shell.run_command(f\"{env.python} -m coverage run {self.python_file}\")\n        return env.shell.last_duration\n\n\nclass SlipcoverBenchmark(AdHocProject):\n    \"\"\"\n    For running code from the Slipcover benchmarks.\n\n    Clone https://github.com/plasma-umass/slipcover to /src/slipcover\n\n    \"\"\"\n\n    def __init__(self, python_file: str):\n        super().__init__(\n            python_file=f\"/src/slipcover/benchmarks/{python_file}\",\n            cur_dir=\"/src/slipcover\",\n            pip_args=\"six pyperf\",\n        )\n\n\nclass PyVersion:\n    \"\"\"A version of Python to use.\"\"\"\n\n    # The command to run this Python\n    command: str\n    # Short word for messages, directories, etc\n    slug: str\n    # The tox environment to run this Python\n    toxenv: str\n\n\nclass Python(PyVersion):\n    \"\"\"A version of CPython to use.\"\"\"\n\n    def __init__(self, major: int, minor: int):\n        self.command = self.slug = f\"python{major}.{minor}\"\n        self.toxenv = f\"py{major}{minor}\"\n\n\nclass PyPy(PyVersion):\n    \"\"\"A version of PyPy to use.\"\"\"\n\n    def __init__(self, major: int, minor: int):\n        self.command = self.slug = f\"pypy{major}.{minor}\"\n        self.toxenv = f\"pypy{major}{minor}\"\n\n\nclass AdHocPython(PyVersion):\n    \"\"\"A custom build of Python to use.\"\"\"\n\n    def __init__(self, path: str, slug: str):\n        self.command = f\"{path}/bin/python3\"\n        file_must_exist(self.command, \"python command\")\n        self.slug = slug\n        self.toxenv = \"\"\n\n\n@dataclass\nclass Coverage:\n    \"\"\"A version of coverage.py to use, maybe None.\"\"\"\n\n    # Short word for messages, directories, etc\n    slug: str\n    # Arguments for \"pip install ...\"\n    pip_args: str | None = None\n    # Tweaks to the .coveragerc file\n    tweaks: TweaksType = None\n    # Environment variables to set\n    env_vars: Env_VarsType = None\n\n\nclass NoCoverage(Coverage):\n    \"\"\"Run without coverage at all.\"\"\"\n\n    def __init__(self, slug: str = \"nocov\"):\n        super().__init__(slug=slug, pip_args=None)\n\n\nclass CoveragePR(Coverage):\n    \"\"\"A version of coverage.py from a pull request.\"\"\"\n\n    def __init__(\n        self, number: int, tweaks: TweaksType = None, env_vars: Env_VarsType = None\n    ):\n        url = f\"https://github.com/nedbat/coveragepy.git@refs/pull/{number}/merge\"\n        url_must_exist(url)\n        super().__init__(\n            slug=f\"#{number}\",\n            pip_args=f\"git+{url}\",\n            tweaks=tweaks,\n            env_vars=env_vars,\n        )\n\n\nclass CoverageCommit(Coverage):\n    \"\"\"A version of coverage.py from a specific commit.\"\"\"\n\n    def __init__(\n        self, sha: str, tweaks: TweaksType = None, env_vars: Env_VarsType = None\n    ):\n        url = f\"https://github.com/nedbat/coveragepy.git@{sha}\"\n        url_must_exist(url)\n        super().__init__(\n            slug=sha,\n            pip_args=f\"git+{url}\",\n            tweaks=tweaks,\n            env_vars=env_vars,\n        )\n\n\nclass CoverageSource(Coverage):\n    \"\"\"The coverage.py in a working tree.\"\"\"\n\n    def __init__(\n        self,\n        directory_name: str = \"..\",\n        slug: str = \"source\",\n        tweaks: TweaksType = None,\n        env_vars: Env_VarsType = None,\n    ):\n        directory = file_must_exist(directory_name, \"coverage directory\")\n        super().__init__(\n            slug=slug,\n            pip_args=str(directory),\n            tweaks=tweaks,\n            env_vars=env_vars,\n        )\n\n\n@dataclass\nclass Env:\n    \"\"\"An environment to run a test suite in.\"\"\"\n\n    pyver: PyVersion\n    python: Path\n    shell: ShellSession\n\n\nResultKey = Tuple[str, str, str]\n\nDIMENSION_NAMES = [\"proj\", \"pyver\", \"cov\"]\n\n\nclass Experiment:\n    \"\"\"A particular time experiment to run.\"\"\"\n\n    def __init__(\n        self,\n        py_versions: list[PyVersion],\n        cov_versions: list[Coverage],\n        projects: list[ProjectToTest],\n        results_file: str = \"results.json\",\n        load: bool = False,\n        cwd: str = \"\",\n    ):\n        self.py_versions = py_versions\n        self.cov_versions = cov_versions\n        self.projects = projects\n        self.results_file = Path(cwd) / Path(results_file)\n        self.result_data: dict[ResultKey, list[float]] = (\n            self.load_results() if load else {}\n        )\n        self.summary_data: dict[ResultKey, float] = {}\n\n    def save_results(self) -> None:\n        \"\"\"Save current results to the JSON file.\"\"\"\n        with self.results_file.open(\"w\") as f:\n            json.dump({\" \".join(k): v for k, v in self.result_data.items()}, f)\n\n    def load_results(self) -> dict[ResultKey, list[float]]:\n        \"\"\"Load results from the JSON file if it exists.\"\"\"\n        if self.results_file.exists():\n            with self.results_file.open(\"r\") as f:\n                data: dict[str, list[float]] = json.load(f)\n            return {\n                (k.split()[0], k.split()[1], k.split()[2]): v for k, v in data.items()\n            }\n        return {}\n\n    def run(self, num_runs: int = 3) -> None:\n        total_runs = (\n            len(self.projects)\n            * len(self.py_versions)\n            * len(self.cov_versions)\n            * num_runs\n        )\n        total_run_nums = iter(itertools.count(start=1))\n\n        all_runs = []\n\n        for proj in self.projects:\n            with proj.shell() as shell:\n                print(f\"Prepping project {proj.slug}\")\n                shell.print_banner(f\"Prepping project {proj.slug}\")\n                proj.make_dir()\n                proj.get_source(shell)\n\n                for pyver in self.py_versions:\n                    print(f\"Making venv for {proj.slug} {pyver.slug}\")\n                    venv_dir = f\"venv_{proj.slug}_{pyver.slug}\"\n                    shell.run_command(f\"{pyver.command} -m venv {venv_dir}\")\n                    python = Path.cwd() / f\"{venv_dir}/bin/python\"\n                    shell.run_command(f\"{python} -V\")\n                    shell.run_command(f\"{python} -m pip install -U pip\")\n                    env = Env(pyver, python, shell)\n\n                    with change_dir(proj.dir):\n                        print(f\"Prepping for {proj.slug} {pyver.slug}\")\n                        proj.prep_environment(env)\n                        for cov_ver in self.cov_versions:\n                            all_runs.append((proj, pyver, cov_ver, env))\n\n        all_runs *= num_runs\n        random.shuffle(all_runs)\n\n        run_data: dict[ResultKey, list[float]] = collections.defaultdict(list)\n        run_data.update(self.result_data)\n\n        for proj, pyver, cov_ver, env in all_runs:\n            result_key = (proj.slug, pyver.slug, cov_ver.slug)\n            total_run_num = next(total_run_nums)\n            if (\n                result_key in self.result_data\n                and len(self.result_data[result_key]) >= num_runs\n            ):\n                print(f\"Skipping {result_key} as results already exist.\")\n                continue\n\n            with env.shell:\n                banner = (\n                    \"Running tests: \"\n                    + f\"proj={proj.slug}, py={pyver.slug}, cov={cov_ver.slug}, \"\n                    + f\"{total_run_num} of {total_runs}\"\n                )\n                print(banner)\n                env.shell.print_banner(banner)\n                with change_dir(proj.dir):\n                    with env.shell.set_env(proj.env_vars, cov_ver.env_vars):\n                        try:\n                            if cov_ver.pip_args is None:\n                                dur = proj.run_no_coverage(env)\n                            else:\n                                dur = proj.run_with_coverage(env, cov_ver)\n                        except Exception as exc:\n                            print(f\"!!! {exc = }\")\n                            traceback.print_exc(file=env.shell.foutput)\n                            dur = float(\"NaN\")\n            print(f\"Tests took {dur:.3f}s\")\n            if result_key not in self.result_data:\n                self.result_data[result_key] = []\n            self.result_data[result_key].append(dur)\n            run_data[result_key].append(dur)\n            self.save_results()\n\n        # Summarize and collect the data.\n        print(\"# Results\")\n        for proj in self.projects:\n            for pyver in self.py_versions:\n                for cov_ver in self.cov_versions:\n                    result_key = (proj.slug, pyver.slug, cov_ver.slug)\n                    data = run_data[result_key]\n                    med = statistics.median(data)\n                    self.summary_data[result_key] = med\n                    stdev = statistics.stdev(data) if len(data) > 1 else 0.0\n                    summary = (\n                        f\"Median for {proj.slug}, {pyver.slug}, {cov_ver.slug}: \"\n                        + f\"{med:.3f}s, \"\n                        + f\"stdev={stdev:.3f}\"\n                    )\n                    if 1:\n                        data_sum = \", \".join(f\"{d:.3f}\" for d in data)\n                        summary += f\", data={data_sum}\"\n                    print(summary)\n\n    def show_results(\n        self,\n        rows: list[str],\n        column: str,\n        ratios: Iterable[tuple[str, str, str]] = (),\n    ) -> None:\n        dimensions = {\n            \"cov\": [cov_ver.slug for cov_ver in self.cov_versions],\n            \"pyver\": [pyver.slug for pyver in self.py_versions],\n            \"proj\": [proj.slug for proj in self.projects],\n        }\n\n        table_axes = [dimensions[rowname] for rowname in rows]\n        data_order = [*rows, column]\n        remap = [data_order.index(datum) for datum in DIMENSION_NAMES]\n\n        header = []\n        header.extend(rows)\n        header.extend(dimensions[column])\n        header.extend(slug for slug, _, _ in ratios)\n\n        aligns = [\"left\"] * len(rows) + [\"right\"] * (len(header) - len(rows))\n        data = []\n\n        for tup in itertools.product(*table_axes):\n            row: list[str] = []\n            row.extend(tup)\n            col_data = {}\n            for col in dimensions[column]:\n                key = (*tup, col)\n                key = tuple(key[i] for i in remap)\n                key = cast(ResultKey, key)\n                result_time = self.summary_data[key]\n                row.append(f\"{result_time:.1f}s\")\n                col_data[col] = result_time\n            for _, num, denom in ratios:\n                ratio = col_data[num] / col_data[denom]\n                row.append(f\"{ratio * 100:.0f}%\")\n            data.append(row)\n\n        print()\n        print(tabulate.tabulate(data, headers=header, colalign=aligns, tablefmt=\"pipe\"))\n\n\nPERF_DIR = Path(\"/tmp/covperf\")\n\n\ndef run_experiment(\n    py_versions: list[PyVersion],\n    cov_versions: list[Coverage],\n    projects: list[ProjectToTest],\n    rows: list[str],\n    column: str,\n    ratios: Iterable[tuple[str, str, str]] = (),\n    num_runs: int = int(sys.argv[1]),\n    load: bool = False,\n) -> None:\n    \"\"\"\n    Run a benchmarking experiment and print a table of results.\n\n    Arguments:\n\n        py_versions: The Python versions to test.\n        cov_versions: The coverage versions to test.\n        projects: The projects to run.\n        rows: A list of strings chosen from `\"pyver\"`, `\"cov\"`, and `\"proj\"`.\n        column: The remaining dimension not used in `rows`.\n        ratios: A list of triples: (title, slug1, slug2).\n        num_runs: The number of times to run each matrix element.\n\n    \"\"\"\n    slugs = [v.slug for v in py_versions + cov_versions + projects]\n    if len(set(slugs)) != len(slugs):\n        raise Exception(f\"Slugs must be unique: {slugs}\")\n    if any(\" \" in slug for slug in slugs):\n        raise Exception(f\"No spaces in slugs please: {slugs}\")\n    ratio_slugs = [rslug for ratio in ratios for rslug in ratio[1:]]\n    if any(rslug not in slugs for rslug in ratio_slugs):\n        raise Exception(f\"Ratio slug doesn't match a slug: {ratio_slugs}, {slugs}\")\n    if set(rows + [column]) != set(DIMENSION_NAMES):\n        raise Exception(\n            f\"All of these must be in rows or column: {', '.join(DIMENSION_NAMES)}\"\n        )\n\n    print(f\"Removing and re-making {PERF_DIR}\")\n    rmrf(PERF_DIR)\n\n    cwd = str(Path.cwd())\n    with change_dir(PERF_DIR):\n        exp = Experiment(\n            py_versions=py_versions,\n            cov_versions=cov_versions,\n            projects=projects,\n            load=load,\n            cwd=cwd,\n        )\n        exp.run(num_runs=int(num_runs))\n        exp.show_results(rows=rows, column=column, ratios=ratios)\n", "benchmark/empty.py": "from benchmark import *\n\nrun_experiment(\n    py_versions=[\n        Python(3, 9),\n        Python(3, 11),\n    ],\n    cov_versions=[\n        Coverage(\"701\", \"coverage==7.0.1\"),\n        Coverage(\n            \"701.dynctx\", \"coverage==7.0.1\", [(\"dynamic_context\", \"test_function\")]\n        ),\n        Coverage(\"702\", \"coverage==7.0.2\"),\n        Coverage(\n            \"702.dynctx\", \"coverage==7.0.2\", [(\"dynamic_context\", \"test_function\")]\n        ),\n    ],\n    projects=[\n        EmptyProject(\"empty\", [1.2, 3.4]),\n        EmptyProject(\"dummy\", [6.9, 7.1]),\n    ],\n    rows=[\"proj\", \"pyver\"],\n    column=\"cov\",\n    ratios=[\n        (\".2 vs .1\", \"702\", \"701\"),\n        (\".1 dynctx cost\", \"701.dynctx\", \"701\"),\n        (\".2 dynctx cost\", \"702.dynctx\", \"702\"),\n    ],\n)\n", "benchmark/fake.py": "from benchmark import *\n\nclass ProjectSlow(EmptyProject):\n    def __init__(self):\n        super().__init__(slug=\"slow\", fake_durations=[23.9, 24.2])\n\nclass ProjectOdd(EmptyProject):\n    def __init__(self):\n        super().__init__(slug=\"odd\", fake_durations=[10.1, 10.5, 9.9])\n\n\nrun_experiment(\n    py_versions=[\n        Python(3, 10),\n        Python(3, 11),\n    #    Python(3, 12),\n    ],\n    cov_versions=[\n        Coverage(\"753\", \"coverage==7.5.3\"),\n        CoverageSource(\"~/coverage\"),\n    ],\n    projects=[\n        ProjectSlow(),\n        ProjectOdd(),\n    ],\n    rows=[\"cov\", \"proj\"],\n    column=\"pyver\",\n    ratios=[\n        (\"11 vs 10\", \"python3.11\", \"python3.10\"),\n    #    (\"12 vs 11\", \"python3.12\", \"python3.11\"),\n    ],\n)\n", "ci/parse_relnotes.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"\nParse CHANGES.md into a JSON structure.\n\nRun with two arguments: the .md file to parse, and the JSON file to write:\n\n\tpython parse_relnotes.py CHANGES.md relnotes.json\n\nEvery section that has something that looks like a version number in it will\nbe recorded as the release notes for that version.\n\n\"\"\"\n\nimport json\nimport re\nimport sys\n\n\nclass TextChunkBuffer:\n    \"\"\"Hold onto text chunks until needed.\"\"\"\n    def __init__(self):\n        self.buffer = []\n\n    def append(self, text):\n        \"\"\"Add `text` to the buffer.\"\"\"\n        self.buffer.append(text)\n\n    def clear(self):\n        \"\"\"Clear the buffer.\"\"\"\n        self.buffer = []\n\n    def flush(self):\n        \"\"\"Produce a (\"text\", text) tuple if there's anything here.\"\"\"\n        buffered = \"\".join(self.buffer).strip()\n        if buffered:\n            yield (\"text\", buffered)\n        self.clear()\n\n\ndef parse_md(lines):\n    \"\"\"Parse markdown lines, producing (type, text) chunks.\"\"\"\n    buffer = TextChunkBuffer()\n\n    for line in lines:\n        if header_match := re.search(r\"^(#+) (.+)$\", line):\n            yield from buffer.flush()\n            hashes, text = header_match.groups()\n            yield (f\"h{len(hashes)}\", text)\n        else:\n            buffer.append(line)\n\n    yield from buffer.flush()\n\n\ndef sections(parsed_data):\n    \"\"\"Convert a stream of parsed tokens into sections with text and notes.\n\n    Yields a stream of:\n        ('h-level', 'header text', 'text')\n\n    \"\"\"\n    header = None\n    text = []\n    for ttype, ttext in parsed_data:\n        if ttype.startswith('h'):\n            if header:\n                yield (*header, \"\\n\".join(text))\n            text = []\n            header = (ttype, ttext)\n        elif ttype == \"text\":\n            text.append(ttext)\n        else:\n            raise RuntimeError(f\"Don't know ttype {ttype!r}\")\n    yield (*header, \"\\n\".join(text))\n\n\ndef refind(regex, text):\n    \"\"\"Find a regex in some text, and return the matched text, or None.\"\"\"\n    if m := re.search(regex, text):\n        return m.group()\n    else:\n        return None\n\n\ndef fix_ref_links(text, version):\n    \"\"\"Find links to .rst files, and make them full RTFD links.\"\"\"\n    def new_link(m):\n        return f\"](https://coverage.readthedocs.io/en/{version}/{m[1]}.html{m[2]})\"\n    return re.sub(r\"\\]\\((\\w+)\\.rst(#.*?)\\)\", new_link, text)\n\n\ndef relnotes(mdlines):\n    r\"\"\"Yield (version, text) pairs from markdown lines.\n\n    Each tuple is a separate version mentioned in the release notes.\n\n    A version is any section with \\d\\.\\d in the header text.\n\n    \"\"\"\n    for _, htext, text in sections(parse_md(mdlines)):\n        version = refind(r\"\\d+\\.\\d[^ ]*\", htext)\n        if version:\n            prerelease = any(c in version for c in \"abc\")\n            when = refind(r\"\\d+-\\d+-\\d+\", htext)\n            text = fix_ref_links(text, version)\n            yield {\n                \"version\": version,\n                \"text\": text,\n                \"prerelease\": prerelease,\n                \"when\": when,\n            }\n\ndef parse(md_filename, json_filename):\n    \"\"\"Main function: parse markdown and write JSON.\"\"\"\n    with open(md_filename) as mf:\n        markdown = mf.read()\n    with open(json_filename, \"w\") as jf:\n        json.dump(list(relnotes(markdown.splitlines(True))), jf, indent=4)\n\nif __name__ == \"__main__\":\n    parse(*sys.argv[1:3])\n", "ci/trigger_action.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Trigger a repository_dispatch GitHub action.\"\"\"\n\nimport sys\n\nfrom session import get_session\n\nrepo_owner, event_type = sys.argv[1:]\n\n# The GitHub URL makes no mention of which workflow to use. It's found based on\n# the event_type, which matches the types in the workflow:\n#\n#   on:\n#     repository_dispatch:\n#       types:\n#         - build-kits\n#\n\nurl = f\"https://api.github.com/repos/{repo_owner}/dispatches\"\ndata = {\"event_type\": event_type}\n\nresp = get_session().post(url, json=data)\nif resp.status_code // 100 == 2:\n    print(\"Success\")\nelse:\n    print(f\"Status: {resp.status_code}\")\n    print(resp.text)\n", "ci/comment_on_fixes.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Add a release comment to all the issues mentioned in the latest release.\"\"\"\n\nimport json\nimport re\nimport sys\n\nfrom session import get_session\n\nwith open(\"tmp/relnotes.json\") as frn:\n    relnotes = json.load(frn)\n\nlatest = relnotes[0]\nversion = latest[\"version\"]\ncomment = (\n    f\"This is now released as part of [coverage {version}]\" +\n    f\"(https://pypi.org/project/coverage/{version}).\"\n)\nprint(f\"Comment will be:\\n\\n{comment}\\n\")\n\nrepo_owner = sys.argv[1]\nfor m in re.finditer(fr\"https://github.com/{repo_owner}/(issues|pull)/(\\d+)\", latest[\"text\"]):\n    kind, number = m.groups()\n    do_comment = False\n\n    if kind == \"issues\":\n        url = f\"https://api.github.com/repos/{repo_owner}/issues/{number}\"\n        issue_data = get_session().get(url).json()\n        if issue_data[\"state\"] == \"closed\":\n            do_comment = True\n        else:\n            print(f\"Still open, comment manually: {m[0]}\")\n    else:\n        url = f\"https://api.github.com/repos/{repo_owner}/pulls/{number}\"\n        pull_data = get_session().get(url).json()\n        if pull_data[\"state\"] == \"closed\":\n            if pull_data[\"merged\"]:\n                do_comment = True\n            else:\n                print(f\"Not merged, comment manually: {m[0]}\")\n        else:\n            print(f\"Still open, comment manually: {m[0]}\")\n\n    if do_comment:\n        print(f\"Commenting on {m[0]}\")\n        url = f\"https://api.github.com/repos/{repo_owner}/issues/{number}/comments\"\n        resp = get_session().post(url, json={\"body\": comment})\n        print(resp)\n", "ci/session.py": "# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0\n# For details: https://github.com/nedbat/coveragepy/blob/master/NOTICE.txt\n\n\"\"\"Help make a requests Session with proper authentication.\"\"\"\n\nimport os\nimport sys\n\nimport requests\n\n_SESSION = None\n\ndef get_session():\n    \"\"\"Get a properly authenticated requests Session.\"\"\"\n\n    global _SESSION\n\n    if _SESSION is None:\n        # If GITHUB_TOKEN is in the environment, use it.\n        token = os.environ.get(\"GITHUB_TOKEN\")\n        if token is None:\n            sys.exit(\"!! Must have a GITHUB_TOKEN\")\n\n        _SESSION = requests.session()\n        _SESSION.headers[\"Authorization\"] = f\"token {token}\"\n        # requests.get() will always prefer the .netrc file even if a header\n        # is already set.  This tells it to ignore the .netrc file.\n        _SESSION.trust_env = False\n\n    return _SESSION\n"}