{"profiler/profiler_script.py": "\"\"\"A script for profiling.\n\nTo generate and read results:\n  - `tox -e profile`\n  - `firefox .tox/prof/output.svg`\n\"\"\"\nfrom pathlib import Path\n\nimport tomli\n\nbenchmark_toml = (\n    (Path(__file__).parent.parent / \"benchmark\" / \"data.toml\").read_bytes().decode()\n)\n\n# Run this a few times to emphasize over imports and other overhead above.\nfor _ in range(1000):\n    tomli.loads(benchmark_toml)\n", "tests/test_data.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\nimport json\nfrom pathlib import Path\nimport unittest\n\nfrom . import burntsushi, tomllib\n\n\nclass MissingFile:\n    def __init__(self, path: Path):\n        self.path = path\n\n\nDATA_DIR = Path(__file__).parent / \"data\"\n\nVALID_FILES = tuple((DATA_DIR / \"valid\").glob(\"**/*.toml\"))\nassert VALID_FILES, \"Valid TOML test files not found\"\n\n_expected_files = []\nfor p in VALID_FILES:\n    json_path = p.with_suffix(\".json\")\n    try:\n        text = json.loads(json_path.read_bytes().decode())\n    except FileNotFoundError:\n        text = MissingFile(json_path)\n    _expected_files.append(text)\nVALID_FILES_EXPECTED = tuple(_expected_files)\n\nINVALID_FILES = tuple((DATA_DIR / \"invalid\").glob(\"**/*.toml\"))\nassert INVALID_FILES, \"Invalid TOML test files not found\"\n\n\nclass TestData(unittest.TestCase):\n    def test_invalid(self):\n        for invalid in INVALID_FILES:\n            with self.subTest(msg=invalid.stem):\n                toml_bytes = invalid.read_bytes()\n                try:\n                    toml_str = toml_bytes.decode()\n                except UnicodeDecodeError:\n                    # Some BurntSushi tests are not valid UTF-8. Skip those.\n                    continue\n                with self.assertRaises(tomllib.TOMLDecodeError):\n                    tomllib.loads(toml_str)\n\n    def test_valid(self):\n        for valid, expected in zip(VALID_FILES, VALID_FILES_EXPECTED):\n            with self.subTest(msg=valid.stem):\n                if isinstance(expected, MissingFile):\n                    # For a poor man's xfail, assert that this is one of the\n                    # test cases where expected data is known to be missing.\n                    assert valid.stem in {\n                        \"qa-array-inline-nested-1000\",\n                        \"qa-table-inline-nested-1000\",\n                    }\n                    continue\n                toml_str = valid.read_bytes().decode()\n                actual = tomllib.loads(toml_str)\n                actual = burntsushi.convert(actual)\n                expected = burntsushi.normalize(expected)\n                self.assertEqual(actual, expected)\n", "tests/test_misc.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\nimport copy\nimport datetime\nfrom decimal import Decimal as D\nfrom pathlib import Path\nimport tempfile\nimport unittest\n\nfrom . import tomllib\n\n\nclass TestMiscellaneous(unittest.TestCase):\n    def test_load(self):\n        content = \"one=1 \\n two='two' \\n arr=[]\"\n        expected = {\"one\": 1, \"two\": \"two\", \"arr\": []}\n        with tempfile.TemporaryDirectory() as tmp_dir_path:\n            file_path = Path(tmp_dir_path) / \"test.toml\"\n            file_path.write_text(content)\n\n            with open(file_path, \"rb\") as bin_f:\n                actual = tomllib.load(bin_f)\n        self.assertEqual(actual, expected)\n\n    def test_incorrect_load(self):\n        content = \"one=1\"\n        with tempfile.TemporaryDirectory() as tmp_dir_path:\n            file_path = Path(tmp_dir_path) / \"test.toml\"\n            file_path.write_text(content)\n\n            with open(file_path, \"r\") as txt_f:\n                with self.assertRaises(TypeError):\n                    tomllib.load(txt_f)  # type: ignore[arg-type]\n\n    def test_parse_float(self):\n        doc = \"\"\"\n              val=0.1\n              biggest1=inf\n              biggest2=+inf\n              smallest=-inf\n              notnum1=nan\n              notnum2=-nan\n              notnum3=+nan\n              \"\"\"\n        obj = tomllib.loads(doc, parse_float=D)\n        expected = {\n            \"val\": D(\"0.1\"),\n            \"biggest1\": D(\"inf\"),\n            \"biggest2\": D(\"inf\"),\n            \"smallest\": D(\"-inf\"),\n            \"notnum1\": D(\"nan\"),\n            \"notnum2\": D(\"-nan\"),\n            \"notnum3\": D(\"nan\"),\n        }\n        for k, expected_val in expected.items():\n            actual_val = obj[k]\n            self.assertIsInstance(actual_val, D)\n            if actual_val.is_nan():\n                self.assertTrue(expected_val.is_nan())\n            else:\n                self.assertEqual(actual_val, expected_val)\n\n    def test_deepcopy(self):\n        doc = \"\"\"\n              [bliibaa.diibaa]\n              offsettime=[1979-05-27T00:32:00.999999-07:00]\n              \"\"\"\n        obj = tomllib.loads(doc)\n        obj_copy = copy.deepcopy(obj)\n        self.assertEqual(obj_copy, obj)\n        expected_obj = {\n            \"bliibaa\": {\n                \"diibaa\": {\n                    \"offsettime\": [\n                        datetime.datetime(\n                            1979,\n                            5,\n                            27,\n                            0,\n                            32,\n                            0,\n                            999999,\n                            tzinfo=datetime.timezone(datetime.timedelta(hours=-7)),\n                        )\n                    ]\n                }\n            }\n        }\n        self.assertEqual(obj_copy, expected_obj)\n\n    def test_inline_array_recursion_limit(self):\n        nest_count = 470\n        recursive_array_toml = \"arr = \" + nest_count * \"[\" + nest_count * \"]\"\n        tomllib.loads(recursive_array_toml)\n\n    def test_inline_table_recursion_limit(self):\n        nest_count = 310\n        recursive_table_toml = nest_count * \"key = {\" + nest_count * \"}\"\n        tomllib.loads(recursive_table_toml)\n", "tests/burntsushi.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Utilities for tests that are in the \"burntsushi\" format.\"\"\"\n\nimport datetime\nfrom typing import Any\n\n# Aliases for converting TOML compliance format [1] to BurntSushi format [2]\n# [1] https://github.com/toml-lang/compliance/blob/db7c3211fda30ff9ddb10292f4aeda7e2e10abc4/docs/json-encoding.md  # noqa: E501\n# [2] https://github.com/BurntSushi/toml-test/blob/4634fdf3a6ecd6aaea5f4cdcd98b2733c2694993/README.md  # noqa: E501\n_aliases = {\n    \"boolean\": \"bool\",\n    \"offset datetime\": \"datetime\",\n    \"local datetime\": \"datetime-local\",\n    \"local date\": \"date-local\",\n    \"local time\": \"time-local\",\n}\n\n\ndef convert(obj):  # noqa: C901\n    if isinstance(obj, str):\n        return {\"type\": \"string\", \"value\": obj}\n    elif isinstance(obj, bool):\n        return {\"type\": \"bool\", \"value\": str(obj).lower()}\n    elif isinstance(obj, int):\n        return {\"type\": \"integer\", \"value\": str(obj)}\n    elif isinstance(obj, float):\n        return {\"type\": \"float\", \"value\": _normalize_float_str(str(obj))}\n    elif isinstance(obj, datetime.datetime):\n        val = _normalize_datetime_str(obj.isoformat())\n        if obj.tzinfo:\n            return {\"type\": \"datetime\", \"value\": val}\n        return {\"type\": \"datetime-local\", \"value\": val}\n    elif isinstance(obj, datetime.time):\n        return {\n            \"type\": \"time-local\",\n            \"value\": _normalize_localtime_str(str(obj)),\n        }\n    elif isinstance(obj, datetime.date):\n        return {\n            \"type\": \"date-local\",\n            \"value\": str(obj),\n        }\n    elif isinstance(obj, list):\n        return [convert(i) for i in obj]\n    elif isinstance(obj, dict):\n        return {k: convert(v) for k, v in obj.items()}\n    raise Exception(\"unsupported type\")\n\n\ndef normalize(obj: Any) -> Any:\n    \"\"\"Normalize test objects.\n\n    This normalizes primitive values (e.g. floats), and also converts from\n    TOML compliance format [1] to BurntSushi format [2].\n\n    [1] https://github.com/toml-lang/compliance/blob/db7c3211fda30ff9ddb10292f4aeda7e2e10abc4/docs/json-encoding.md  # noqa: E501\n    [2] https://github.com/BurntSushi/toml-test/blob/4634fdf3a6ecd6aaea5f4cdcd98b2733c2694993/README.md  # noqa: E501\n    \"\"\"\n    if isinstance(obj, list):\n        return [normalize(item) for item in obj]\n    if isinstance(obj, dict):\n        if \"type\" in obj and \"value\" in obj:\n            type_ = obj[\"type\"]\n            norm_type = _aliases.get(type_, type_)\n            value = obj[\"value\"]\n            if norm_type == \"float\":\n                norm_value = _normalize_float_str(value)\n            elif norm_type in {\"datetime\", \"datetime-local\"}:\n                norm_value = _normalize_datetime_str(value)\n            elif norm_type == \"time-local\":\n                norm_value = _normalize_localtime_str(value)\n            else:\n                norm_value = value\n\n            if norm_type == \"array\":\n                return [normalize(item) for item in value]\n            return {\"type\": norm_type, \"value\": norm_value}\n        return {k: normalize(v) for k, v in obj.items()}\n    raise AssertionError(\"Burntsushi fixtures should be dicts/lists only\")\n\n\ndef _normalize_datetime_str(dt_str: str) -> str:\n    if dt_str[-1].lower() == \"z\":\n        dt_str = dt_str[:-1] + \"+00:00\"\n\n    date = dt_str[:10]\n    rest = dt_str[11:]\n\n    if \"+\" in rest:\n        sign = \"+\"\n    elif \"-\" in rest:\n        sign = \"-\"\n    else:\n        sign = \"\"\n\n    if sign:\n        time, _, offset = rest.partition(sign)\n    else:\n        time = rest\n        offset = \"\"\n\n    time = time.rstrip(\"0\") if \".\" in time else time\n    return date + \"T\" + time + sign + offset\n\n\ndef _normalize_localtime_str(lt_str: str) -> str:\n    return lt_str.rstrip(\"0\") if \".\" in lt_str else lt_str\n\n\ndef _normalize_float_str(float_str: str) -> str:\n    as_float = float(float_str)\n\n    # Normalize \"-0.0\" and \"+0.0\"\n    if as_float == 0:\n        return \"0\"\n\n    return str(as_float)\n", "tests/test_error.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\nimport unittest\n\nfrom . import tomllib\n\n\nclass TestError(unittest.TestCase):\n    def test_line_and_col(self):\n        with self.assertRaises(tomllib.TOMLDecodeError) as exc_info:\n            tomllib.loads(\"val=.\")\n        self.assertEqual(str(exc_info.exception), \"Invalid value (at line 1, column 5)\")\n\n        with self.assertRaises(tomllib.TOMLDecodeError) as exc_info:\n            tomllib.loads(\".\")\n        self.assertEqual(\n            str(exc_info.exception), \"Invalid statement (at line 1, column 1)\"\n        )\n\n        with self.assertRaises(tomllib.TOMLDecodeError) as exc_info:\n            tomllib.loads(\"\\n\\nval=.\")\n        self.assertEqual(str(exc_info.exception), \"Invalid value (at line 3, column 5)\")\n\n        with self.assertRaises(tomllib.TOMLDecodeError) as exc_info:\n            tomllib.loads(\"\\n\\n.\")\n        self.assertEqual(\n            str(exc_info.exception), \"Invalid statement (at line 3, column 1)\"\n        )\n\n    def test_missing_value(self):\n        with self.assertRaises(tomllib.TOMLDecodeError) as exc_info:\n            tomllib.loads(\"\\n\\nfwfw=\")\n        self.assertEqual(str(exc_info.exception), \"Invalid value (at end of document)\")\n\n    def test_invalid_char_quotes(self):\n        with self.assertRaises(tomllib.TOMLDecodeError) as exc_info:\n            tomllib.loads(\"v = '\\n'\")\n        self.assertTrue(\" '\\\\n' \" in str(exc_info.exception))\n\n    def test_module_name(self):\n        self.assertEqual(tomllib.TOMLDecodeError().__module__, tomllib.__name__)\n\n    def test_invalid_parse_float(self):\n        def dict_returner(s: str) -> dict:\n            return {}\n\n        def list_returner(s: str) -> list:\n            return []\n\n        for invalid_parse_float in (dict_returner, list_returner):\n            with self.assertRaises(ValueError) as exc_info:\n                tomllib.loads(\"f=0.1\", parse_float=invalid_parse_float)\n            self.assertEqual(\n                str(exc_info.exception), \"parse_float must not return dicts or lists\"\n            )\n", "tests/__init__.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\n__all__ = (\"tomllib\",)\n\n# By changing this one line, we can run the tests against\n# a different module name.\nimport tomli as tomllib\n", "benchmark/run.py": "from __future__ import annotations\n\nfrom collections.abc import Callable\nfrom pathlib import Path\nimport timeit\n\nimport pytomlpp\nimport qtoml\nimport rtoml\nimport toml\nimport tomlkit\n\nimport tomli\n\n\ndef benchmark(\n    name: str,\n    run_count: int,\n    func: Callable,\n    col_width: tuple,\n    compare_to: float | None = None,\n) -> float:\n    placeholder = \"Running...\"\n    print(f\"{name:>{col_width[0]}} | {placeholder}\", end=\"\", flush=True)\n    time_taken = timeit.timeit(func, number=run_count)\n    print(\"\\b\" * len(placeholder), end=\"\")\n    time_suffix = \" s\"\n    print(f\"{time_taken:{col_width[1]-len(time_suffix)}.3g}{time_suffix}\", end=\"\")\n    if compare_to is None:\n        print(\" | baseline (100%)\", end=\"\")\n    else:\n        delta = compare_to / time_taken\n        print(f\" | {delta:.2%}\", end=\"\")\n    print()\n    return time_taken\n\n\ndef run(run_count: int) -> None:\n    data_path = Path(__file__).parent / \"data.toml\"\n    test_data = data_path.read_bytes().decode()\n\n    # qtoml has a bug making it crash without this newline normalization\n    test_data = test_data.replace(\"\\r\\n\", \"\\n\")\n\n    col_width = (10, 10, 28)\n    col_head = (\"parser\", \"exec time\", \"performance (more is better)\")\n    print(f\"Parsing data.toml {run_count} times:\")\n    print(\"-\" * col_width[0] + \"---\" + \"-\" * col_width[1] + \"---\" + col_width[2] * \"-\")\n    print(\n        f\"{col_head[0]:>{col_width[0]}} | {col_head[1]:>{col_width[1]}} | {col_head[2]}\"\n    )\n    print(\"-\" * col_width[0] + \"-+-\" + \"-\" * col_width[1] + \"-+-\" + col_width[2] * \"-\")\n    # fmt: off\n    baseline = benchmark(\"rtoml\", run_count, lambda: rtoml.loads(test_data), col_width)  # noqa: E501\n    benchmark(\"pytomlpp\", run_count, lambda: pytomlpp.loads(test_data), col_width, compare_to=baseline)  # noqa: E501\n    benchmark(\"tomli\", run_count, lambda: tomli.loads(test_data), col_width, compare_to=baseline)  # noqa: E501\n    benchmark(\"toml\", run_count, lambda: toml.loads(test_data), col_width, compare_to=baseline)  # noqa: E501\n    benchmark(\"qtoml\", run_count, lambda: qtoml.loads(test_data), col_width, compare_to=baseline)  # noqa: E501\n    benchmark(\"tomlkit\", run_count, lambda: tomlkit.parse(test_data), col_width, compare_to=baseline)  # noqa: E501\n    # fmt: on\n\n\nif __name__ == \"__main__\":\n    run(5000)\n", "src/tomli/_re.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\nfrom __future__ import annotations\n\nfrom datetime import date, datetime, time, timedelta, timezone, tzinfo\nfrom functools import lru_cache\nimport re\nfrom typing import Any\n\nfrom ._types import ParseFloat\n\n# E.g.\n# - 00:32:00.999999\n# - 00:32:00\n_TIME_RE_STR = r\"([01][0-9]|2[0-3]):([0-5][0-9]):([0-5][0-9])(?:\\.([0-9]{1,6})[0-9]*)?\"\n\nRE_NUMBER = re.compile(\n    r\"\"\"\n0\n(?:\n    x[0-9A-Fa-f](?:_?[0-9A-Fa-f])*   # hex\n    |\n    b[01](?:_?[01])*                 # bin\n    |\n    o[0-7](?:_?[0-7])*               # oct\n)\n|\n[+-]?(?:0|[1-9](?:_?[0-9])*)         # dec, integer part\n(?P<floatpart>\n    (?:\\.[0-9](?:_?[0-9])*)?         # optional fractional part\n    (?:[eE][+-]?[0-9](?:_?[0-9])*)?  # optional exponent part\n)\n\"\"\",\n    flags=re.VERBOSE,\n)\nRE_LOCALTIME = re.compile(_TIME_RE_STR)\nRE_DATETIME = re.compile(\n    rf\"\"\"\n([0-9]{{4}})-(0[1-9]|1[0-2])-(0[1-9]|[12][0-9]|3[01])  # date, e.g. 1988-10-27\n(?:\n    [Tt ]\n    {_TIME_RE_STR}\n    (?:([Zz])|([+-])([01][0-9]|2[0-3]):([0-5][0-9]))?  # optional time offset\n)?\n\"\"\",\n    flags=re.VERBOSE,\n)\n\n\ndef match_to_datetime(match: re.Match) -> datetime | date:\n    \"\"\"Convert a `RE_DATETIME` match to `datetime.datetime` or `datetime.date`.\n\n    Raises ValueError if the match does not correspond to a valid date\n    or datetime.\n    \"\"\"\n    (\n        year_str,\n        month_str,\n        day_str,\n        hour_str,\n        minute_str,\n        sec_str,\n        micros_str,\n        zulu_time,\n        offset_sign_str,\n        offset_hour_str,\n        offset_minute_str,\n    ) = match.groups()\n    year, month, day = int(year_str), int(month_str), int(day_str)\n    if hour_str is None:\n        return date(year, month, day)\n    hour, minute, sec = int(hour_str), int(minute_str), int(sec_str)\n    micros = int(micros_str.ljust(6, \"0\")) if micros_str else 0\n    if offset_sign_str:\n        tz: tzinfo | None = cached_tz(\n            offset_hour_str, offset_minute_str, offset_sign_str\n        )\n    elif zulu_time:\n        tz = timezone.utc\n    else:  # local date-time\n        tz = None\n    return datetime(year, month, day, hour, minute, sec, micros, tzinfo=tz)\n\n\n@lru_cache(maxsize=None)\ndef cached_tz(hour_str: str, minute_str: str, sign_str: str) -> timezone:\n    sign = 1 if sign_str == \"+\" else -1\n    return timezone(\n        timedelta(\n            hours=sign * int(hour_str),\n            minutes=sign * int(minute_str),\n        )\n    )\n\n\ndef match_to_localtime(match: re.Match) -> time:\n    hour_str, minute_str, sec_str, micros_str = match.groups()\n    micros = int(micros_str.ljust(6, \"0\")) if micros_str else 0\n    return time(int(hour_str), int(minute_str), int(sec_str), micros)\n\n\ndef match_to_number(match: re.Match, parse_float: ParseFloat) -> Any:\n    if match.group(\"floatpart\"):\n        return parse_float(match.group())\n    return int(match.group(), 0)\n", "src/tomli/_types.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\nfrom typing import Any, Callable, Tuple\n\n# Type annotations\nParseFloat = Callable[[str], Any]\nKey = Tuple[str, ...]\nPos = int\n", "src/tomli/_parser.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\nimport string\nfrom types import MappingProxyType\nfrom typing import IO, Any, NamedTuple\n\nfrom ._re import (\n    RE_DATETIME,\n    RE_LOCALTIME,\n    RE_NUMBER,\n    match_to_datetime,\n    match_to_localtime,\n    match_to_number,\n)\nfrom ._types import Key, ParseFloat, Pos\n\nASCII_CTRL = frozenset(chr(i) for i in range(32)) | frozenset(chr(127))\n\n# Neither of these sets include quotation mark or backslash. They are\n# currently handled as separate cases in the parser functions.\nILLEGAL_BASIC_STR_CHARS = ASCII_CTRL - frozenset(\"\\t\")\nILLEGAL_MULTILINE_BASIC_STR_CHARS = ASCII_CTRL - frozenset(\"\\t\\n\")\n\nILLEGAL_LITERAL_STR_CHARS = ILLEGAL_BASIC_STR_CHARS\nILLEGAL_MULTILINE_LITERAL_STR_CHARS = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n\nILLEGAL_COMMENT_CHARS = ILLEGAL_BASIC_STR_CHARS\n\nTOML_WS = frozenset(\" \\t\")\nTOML_WS_AND_NEWLINE = TOML_WS | frozenset(\"\\n\")\nBARE_KEY_CHARS = frozenset(string.ascii_letters + string.digits + \"-_\")\nKEY_INITIAL_CHARS = BARE_KEY_CHARS | frozenset(\"\\\"'\")\nHEXDIGIT_CHARS = frozenset(string.hexdigits)\n\nBASIC_STR_ESCAPE_REPLACEMENTS = MappingProxyType(\n    {\n        \"\\\\b\": \"\\u0008\",  # backspace\n        \"\\\\t\": \"\\u0009\",  # tab\n        \"\\\\n\": \"\\u000A\",  # linefeed\n        \"\\\\f\": \"\\u000C\",  # form feed\n        \"\\\\r\": \"\\u000D\",  # carriage return\n        '\\\\\"': \"\\u0022\",  # quote\n        \"\\\\\\\\\": \"\\u005C\",  # backslash\n    }\n)\n\n\nclass TOMLDecodeError(ValueError):\n    \"\"\"An error raised if a document is not valid TOML.\"\"\"\n\n\ndef load(__fp: IO[bytes], *, parse_float: ParseFloat = float) -> dict[str, Any]:\n    \"\"\"Parse TOML from a binary file object.\"\"\"\n    b = __fp.read()\n    try:\n        s = b.decode()\n    except AttributeError:\n        raise TypeError(\n            \"File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`\"\n        ) from None\n    return loads(s, parse_float=parse_float)\n\n\ndef loads(__s: str, *, parse_float: ParseFloat = float) -> dict[str, Any]:  # noqa: C901\n    \"\"\"Parse TOML from a string.\"\"\"\n\n    # The spec allows converting \"\\r\\n\" to \"\\n\", even in string\n    # literals. Let's do so to simplify parsing.\n    src = __s.replace(\"\\r\\n\", \"\\n\")\n    pos = 0\n    out = Output(NestedDict(), Flags())\n    header: Key = ()\n    parse_float = make_safe_parse_float(parse_float)\n\n    # Parse one statement at a time\n    # (typically means one line in TOML source)\n    while True:\n        # 1. Skip line leading whitespace\n        pos = skip_chars(src, pos, TOML_WS)\n\n        # 2. Parse rules. Expect one of the following:\n        #    - end of file\n        #    - end of line\n        #    - comment\n        #    - key/value pair\n        #    - append dict to list (and move to its namespace)\n        #    - create dict (and move to its namespace)\n        # Skip trailing whitespace when applicable.\n        try:\n            char = src[pos]\n        except IndexError:\n            break\n        if char == \"\\n\":\n            pos += 1\n            continue\n        if char in KEY_INITIAL_CHARS:\n            pos = key_value_rule(src, pos, out, header, parse_float)\n            pos = skip_chars(src, pos, TOML_WS)\n        elif char == \"[\":\n            try:\n                second_char: str | None = src[pos + 1]\n            except IndexError:\n                second_char = None\n            out.flags.finalize_pending()\n            if second_char == \"[\":\n                pos, header = create_list_rule(src, pos, out)\n            else:\n                pos, header = create_dict_rule(src, pos, out)\n            pos = skip_chars(src, pos, TOML_WS)\n        elif char != \"#\":\n            raise suffixed_err(src, pos, \"Invalid statement\")\n\n        # 3. Skip comment\n        pos = skip_comment(src, pos)\n\n        # 4. Expect end of line or end of file\n        try:\n            char = src[pos]\n        except IndexError:\n            break\n        if char != \"\\n\":\n            raise suffixed_err(\n                src, pos, \"Expected newline or end of document after a statement\"\n            )\n        pos += 1\n\n    return out.data.dict\n\n\nclass Flags:\n    \"\"\"Flags that map to parsed keys/namespaces.\"\"\"\n\n    # Marks an immutable namespace (inline array or inline table).\n    FROZEN = 0\n    # Marks a nest that has been explicitly created and can no longer\n    # be opened using the \"[table]\" syntax.\n    EXPLICIT_NEST = 1\n\n    def __init__(self) -> None:\n        self._flags: dict[str, dict] = {}\n        self._pending_flags: set[tuple[Key, int]] = set()\n\n    def add_pending(self, key: Key, flag: int) -> None:\n        self._pending_flags.add((key, flag))\n\n    def finalize_pending(self) -> None:\n        for key, flag in self._pending_flags:\n            self.set(key, flag, recursive=False)\n        self._pending_flags.clear()\n\n    def unset_all(self, key: Key) -> None:\n        cont = self._flags\n        for k in key[:-1]:\n            if k not in cont:\n                return\n            cont = cont[k][\"nested\"]\n        cont.pop(key[-1], None)\n\n    def set(self, key: Key, flag: int, *, recursive: bool) -> None:  # noqa: A003\n        cont = self._flags\n        key_parent, key_stem = key[:-1], key[-1]\n        for k in key_parent:\n            if k not in cont:\n                cont[k] = {\"flags\": set(), \"recursive_flags\": set(), \"nested\": {}}\n            cont = cont[k][\"nested\"]\n        if key_stem not in cont:\n            cont[key_stem] = {\"flags\": set(), \"recursive_flags\": set(), \"nested\": {}}\n        cont[key_stem][\"recursive_flags\" if recursive else \"flags\"].add(flag)\n\n    def is_(self, key: Key, flag: int) -> bool:\n        if not key:\n            return False  # document root has no flags\n        cont = self._flags\n        for k in key[:-1]:\n            if k not in cont:\n                return False\n            inner_cont = cont[k]\n            if flag in inner_cont[\"recursive_flags\"]:\n                return True\n            cont = inner_cont[\"nested\"]\n        key_stem = key[-1]\n        if key_stem in cont:\n            cont = cont[key_stem]\n            return flag in cont[\"flags\"] or flag in cont[\"recursive_flags\"]\n        return False\n\n\nclass NestedDict:\n    def __init__(self) -> None:\n        # The parsed content of the TOML document\n        self.dict: dict[str, Any] = {}\n\n    def get_or_create_nest(\n        self,\n        key: Key,\n        *,\n        access_lists: bool = True,\n    ) -> dict:\n        cont: Any = self.dict\n        for k in key:\n            if k not in cont:\n                cont[k] = {}\n            cont = cont[k]\n            if access_lists and isinstance(cont, list):\n                cont = cont[-1]\n            if not isinstance(cont, dict):\n                raise KeyError(\"There is no nest behind this key\")\n        return cont\n\n    def append_nest_to_list(self, key: Key) -> None:\n        cont = self.get_or_create_nest(key[:-1])\n        last_key = key[-1]\n        if last_key in cont:\n            list_ = cont[last_key]\n            if not isinstance(list_, list):\n                raise KeyError(\"An object other than list found behind this key\")\n            list_.append({})\n        else:\n            cont[last_key] = [{}]\n\n\nclass Output(NamedTuple):\n    data: NestedDict\n    flags: Flags\n\n\ndef skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:\n    try:\n        while src[pos] in chars:\n            pos += 1\n    except IndexError:\n        pass\n    return pos\n\n\ndef skip_until(\n    src: str,\n    pos: Pos,\n    expect: str,\n    *,\n    error_on: frozenset[str],\n    error_on_eof: bool,\n) -> Pos:\n    try:\n        new_pos = src.index(expect, pos)\n    except ValueError:\n        new_pos = len(src)\n        if error_on_eof:\n            raise suffixed_err(src, new_pos, f\"Expected {expect!r}\") from None\n\n    if not error_on.isdisjoint(src[pos:new_pos]):\n        while src[pos] not in error_on:\n            pos += 1\n        raise suffixed_err(src, pos, f\"Found invalid character {src[pos]!r}\")\n    return new_pos\n\n\ndef skip_comment(src: str, pos: Pos) -> Pos:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char == \"#\":\n        return skip_until(\n            src, pos + 1, \"\\n\", error_on=ILLEGAL_COMMENT_CHARS, error_on_eof=False\n        )\n    return pos\n\n\ndef skip_comments_and_array_ws(src: str, pos: Pos) -> Pos:\n    while True:\n        pos_before_skip = pos\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        pos = skip_comment(src, pos)\n        if pos == pos_before_skip:\n            return pos\n\n\ndef create_dict_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 1  # Skip \"[\"\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n\n    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f\"Cannot declare {key} twice\")\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.get_or_create_nest(key)\n    except KeyError:\n        raise suffixed_err(src, pos, \"Cannot overwrite a value\") from None\n\n    if not src.startswith(\"]\", pos):\n        raise suffixed_err(src, pos, \"Expected ']' at the end of a table declaration\")\n    return pos + 1, key\n\n\ndef create_list_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 2  # Skip \"[[\"\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n\n    if out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f\"Cannot mutate immutable namespace {key}\")\n    # Free the namespace now that it points to another empty list item...\n    out.flags.unset_all(key)\n    # ...but this key precisely is still prohibited from table declaration\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.append_nest_to_list(key)\n    except KeyError:\n        raise suffixed_err(src, pos, \"Cannot overwrite a value\") from None\n\n    if not src.startswith(\"]]\", pos):\n        raise suffixed_err(src, pos, \"Expected ']]' at the end of an array declaration\")\n    return pos + 2, key\n\n\ndef key_value_rule(\n    src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat\n) -> Pos:\n    pos, key, value = parse_key_value_pair(src, pos, parse_float)\n    key_parent, key_stem = key[:-1], key[-1]\n    abs_key_parent = header + key_parent\n\n    relative_path_cont_keys = (header + key[:i] for i in range(1, len(key)))\n    for cont_key in relative_path_cont_keys:\n        # Check that dotted key syntax does not redefine an existing table\n        if out.flags.is_(cont_key, Flags.EXPLICIT_NEST):\n            raise suffixed_err(src, pos, f\"Cannot redefine namespace {cont_key}\")\n        # Containers in the relative path can't be opened with the table syntax or\n        # dotted key/value syntax in following table sections.\n        out.flags.add_pending(cont_key, Flags.EXPLICIT_NEST)\n\n    if out.flags.is_(abs_key_parent, Flags.FROZEN):\n        raise suffixed_err(\n            src, pos, f\"Cannot mutate immutable namespace {abs_key_parent}\"\n        )\n\n    try:\n        nest = out.data.get_or_create_nest(abs_key_parent)\n    except KeyError:\n        raise suffixed_err(src, pos, \"Cannot overwrite a value\") from None\n    if key_stem in nest:\n        raise suffixed_err(src, pos, \"Cannot overwrite a value\")\n    # Mark inline table and array namespaces recursively immutable\n    if isinstance(value, (dict, list)):\n        out.flags.set(header + key, Flags.FROZEN, recursive=True)\n    nest[key_stem] = value\n    return pos\n\n\ndef parse_key_value_pair(\n    src: str, pos: Pos, parse_float: ParseFloat\n) -> tuple[Pos, Key, Any]:\n    pos, key = parse_key(src, pos)\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char != \"=\":\n        raise suffixed_err(src, pos, \"Expected '=' after a key in a key/value pair\")\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, value = parse_value(src, pos, parse_float)\n    return pos, key, value\n\n\ndef parse_key(src: str, pos: Pos) -> tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: str | None = src[pos]\n        except IndexError:\n            char = None\n        if char != \".\":\n            return pos, key\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)\n\n\ndef parse_key_part(src: str, pos: Pos) -> tuple[Pos, str]:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char in BARE_KEY_CHARS:\n        start_pos = pos\n        pos = skip_chars(src, pos, BARE_KEY_CHARS)\n        return pos, src[start_pos:pos]\n    if char == \"'\":\n        return parse_literal_str(src, pos)\n    if char == '\"':\n        return parse_one_line_basic_str(src, pos)\n    raise suffixed_err(src, pos, \"Invalid initial character for a key part\")\n\n\ndef parse_one_line_basic_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1\n    return parse_basic_str(src, pos, multiline=False)\n\n\ndef parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, list]:\n    pos += 1\n    array: list = []\n\n    pos = skip_comments_and_array_ws(src, pos)\n    if src.startswith(\"]\", pos):\n        return pos + 1, array\n    while True:\n        pos, val = parse_value(src, pos, parse_float)\n        array.append(val)\n        pos = skip_comments_and_array_ws(src, pos)\n\n        c = src[pos : pos + 1]\n        if c == \"]\":\n            return pos + 1, array\n        if c != \",\":\n            raise suffixed_err(src, pos, \"Unclosed array\")\n        pos += 1\n\n        pos = skip_comments_and_array_ws(src, pos)\n        if src.startswith(\"]\", pos):\n            return pos + 1, array\n\n\ndef parse_inline_table(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, dict]:\n    pos += 1\n    nested_dict = NestedDict()\n    flags = Flags()\n\n    pos = skip_chars(src, pos, TOML_WS)\n    if src.startswith(\"}\", pos):\n        return pos + 1, nested_dict.dict\n    while True:\n        pos, key, value = parse_key_value_pair(src, pos, parse_float)\n        key_parent, key_stem = key[:-1], key[-1]\n        if flags.is_(key, Flags.FROZEN):\n            raise suffixed_err(src, pos, f\"Cannot mutate immutable namespace {key}\")\n        try:\n            nest = nested_dict.get_or_create_nest(key_parent, access_lists=False)\n        except KeyError:\n            raise suffixed_err(src, pos, \"Cannot overwrite a value\") from None\n        if key_stem in nest:\n            raise suffixed_err(src, pos, f\"Duplicate inline table key {key_stem!r}\")\n        nest[key_stem] = value\n        pos = skip_chars(src, pos, TOML_WS)\n        c = src[pos : pos + 1]\n        if c == \"}\":\n            return pos + 1, nested_dict.dict\n        if c != \",\":\n            raise suffixed_err(src, pos, \"Unclosed inline table\")\n        if isinstance(value, (dict, list)):\n            flags.set(key, Flags.FROZEN, recursive=True)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n\n\ndef parse_basic_str_escape(\n    src: str, pos: Pos, *, multiline: bool = False\n) -> tuple[Pos, str]:\n    escape_id = src[pos : pos + 2]\n    pos += 2\n    if multiline and escape_id in {\"\\\\ \", \"\\\\\\t\", \"\\\\\\n\"}:\n        # Skip whitespace until next non-whitespace character or end of\n        # the doc. Error if non-whitespace is found before newline.\n        if escape_id != \"\\\\\\n\":\n            pos = skip_chars(src, pos, TOML_WS)\n            try:\n                char = src[pos]\n            except IndexError:\n                return pos, \"\"\n            if char != \"\\n\":\n                raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\")\n            pos += 1\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        return pos, \"\"\n    if escape_id == \"\\\\u\":\n        return parse_hex_char(src, pos, 4)\n    if escape_id == \"\\\\U\":\n        return parse_hex_char(src, pos, 8)\n    try:\n        return pos, BASIC_STR_ESCAPE_REPLACEMENTS[escape_id]\n    except KeyError:\n        raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\") from None\n\n\ndef parse_basic_str_escape_multiline(src: str, pos: Pos) -> tuple[Pos, str]:\n    return parse_basic_str_escape(src, pos, multiline=True)\n\n\ndef parse_hex_char(src: str, pos: Pos, hex_len: int) -> tuple[Pos, str]:\n    hex_str = src[pos : pos + hex_len]\n    if len(hex_str) != hex_len or not HEXDIGIT_CHARS.issuperset(hex_str):\n        raise suffixed_err(src, pos, \"Invalid hex value\")\n    pos += hex_len\n    hex_int = int(hex_str, 16)\n    if not is_unicode_scalar_value(hex_int):\n        raise suffixed_err(src, pos, \"Escaped character is not a Unicode scalar value\")\n    return pos, chr(hex_int)\n\n\ndef parse_literal_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1  # Skip starting apostrophe\n    start_pos = pos\n    pos = skip_until(\n        src, pos, \"'\", error_on=ILLEGAL_LITERAL_STR_CHARS, error_on_eof=True\n    )\n    return pos + 1, src[start_pos:pos]  # Skip ending apostrophe\n\n\ndef parse_multiline_str(src: str, pos: Pos, *, literal: bool) -> tuple[Pos, str]:\n    pos += 3\n    if src.startswith(\"\\n\", pos):\n        pos += 1\n\n    if literal:\n        delim = \"'\"\n        end_pos = skip_until(\n            src,\n            pos,\n            \"'''\",\n            error_on=ILLEGAL_MULTILINE_LITERAL_STR_CHARS,\n            error_on_eof=True,\n        )\n        result = src[pos:end_pos]\n        pos = end_pos + 3\n    else:\n        delim = '\"'\n        pos, result = parse_basic_str(src, pos, multiline=True)\n\n    # Add at maximum two extra apostrophes/quotes if the end sequence\n    # is 4 or 5 chars long instead of just 3.\n    if not src.startswith(delim, pos):\n        return pos, result\n    pos += 1\n    if not src.startswith(delim, pos):\n        return pos, result + delim\n    pos += 1\n    return pos, result + (delim * 2)\n\n\ndef parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:\n    if multiline:\n        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape_multiline\n    else:\n        error_on = ILLEGAL_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape\n    result = \"\"\n    start_pos = pos\n    while True:\n        try:\n            char = src[pos]\n        except IndexError:\n            raise suffixed_err(src, pos, \"Unterminated string\") from None\n        if char == '\"':\n            if not multiline:\n                return pos + 1, result + src[start_pos:pos]\n            if src.startswith('\"\"\"', pos):\n                return pos + 3, result + src[start_pos:pos]\n            pos += 1\n            continue\n        if char == \"\\\\\":\n            result += src[start_pos:pos]\n            pos, parsed_escape = parse_escapes(src, pos)\n            result += parsed_escape\n            start_pos = pos\n            continue\n        if char in error_on:\n            raise suffixed_err(src, pos, f\"Illegal character {char!r}\")\n        pos += 1\n\n\ndef parse_value(  # noqa: C901\n    src: str, pos: Pos, parse_float: ParseFloat\n) -> tuple[Pos, Any]:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n\n    # IMPORTANT: order conditions based on speed of checking and likelihood\n\n    # Basic strings\n    if char == '\"':\n        if src.startswith('\"\"\"', pos):\n            return parse_multiline_str(src, pos, literal=False)\n        return parse_one_line_basic_str(src, pos)\n\n    # Literal strings\n    if char == \"'\":\n        if src.startswith(\"'''\", pos):\n            return parse_multiline_str(src, pos, literal=True)\n        return parse_literal_str(src, pos)\n\n    # Booleans\n    if char == \"t\":\n        if src.startswith(\"true\", pos):\n            return pos + 4, True\n    if char == \"f\":\n        if src.startswith(\"false\", pos):\n            return pos + 5, False\n\n    # Arrays\n    if char == \"[\":\n        return parse_array(src, pos, parse_float)\n\n    # Inline tables\n    if char == \"{\":\n        return parse_inline_table(src, pos, parse_float)\n\n    # Dates and times\n    datetime_match = RE_DATETIME.match(src, pos)\n    if datetime_match:\n        try:\n            datetime_obj = match_to_datetime(datetime_match)\n        except ValueError as e:\n            raise suffixed_err(src, pos, \"Invalid date or datetime\") from e\n        return datetime_match.end(), datetime_obj\n    localtime_match = RE_LOCALTIME.match(src, pos)\n    if localtime_match:\n        return localtime_match.end(), match_to_localtime(localtime_match)\n\n    # Integers and \"normal\" floats.\n    # The regex will greedily match any type starting with a decimal\n    # char, so needs to be located after handling of dates and times.\n    number_match = RE_NUMBER.match(src, pos)\n    if number_match:\n        return number_match.end(), match_to_number(number_match, parse_float)\n\n    # Special floats\n    first_three = src[pos : pos + 3]\n    if first_three in {\"inf\", \"nan\"}:\n        return pos + 3, parse_float(first_three)\n    first_four = src[pos : pos + 4]\n    if first_four in {\"-inf\", \"+inf\", \"-nan\", \"+nan\"}:\n        return pos + 4, parse_float(first_four)\n\n    raise suffixed_err(src, pos, \"Invalid value\")\n\n\ndef suffixed_err(src: str, pos: Pos, msg: str) -> TOMLDecodeError:\n    \"\"\"Return a `TOMLDecodeError` where error message is suffixed with\n    coordinates in source.\"\"\"\n\n    def coord_repr(src: str, pos: Pos) -> str:\n        if pos >= len(src):\n            return \"end of document\"\n        line = src.count(\"\\n\", 0, pos) + 1\n        if line == 1:\n            column = pos + 1\n        else:\n            column = pos - src.rindex(\"\\n\", 0, pos)\n        return f\"line {line}, column {column}\"\n\n    return TOMLDecodeError(f\"{msg} (at {coord_repr(src, pos)})\")\n\n\ndef is_unicode_scalar_value(codepoint: int) -> bool:\n    return (0 <= codepoint <= 55295) or (57344 <= codepoint <= 1114111)\n\n\ndef make_safe_parse_float(parse_float: ParseFloat) -> ParseFloat:\n    \"\"\"A decorator to make `parse_float` safe.\n\n    `parse_float` must not return dicts or lists, because these types\n    would be mixed with parsed TOML tables and arrays, thus confusing\n    the parser. The returned decorated callable raises `ValueError`\n    instead of returning illegal types.\n    \"\"\"\n    # The default `float` callable never returns illegal types. Optimize it.\n    if parse_float is float:\n        return float\n\n    def safe_parse_float(float_str: str) -> Any:\n        float_value = parse_float(float_str)\n        if isinstance(float_value, (dict, list)):\n            raise ValueError(\"parse_float must not return dicts or lists\")\n        return float_value\n\n    return safe_parse_float\n", "src/tomli/__init__.py": "# SPDX-License-Identifier: MIT\n# SPDX-FileCopyrightText: 2021 Taneli Hukkinen\n# Licensed to PSF under a Contributor Agreement.\n\n__all__ = (\"loads\", \"load\", \"TOMLDecodeError\")\n__version__ = \"2.0.1\"  # DO NOT EDIT THIS LINE MANUALLY. LET bump2version UTILITY DO IT\n\nfrom ._parser import TOMLDecodeError, load, loads\n\n# Pretend this exception was created here.\nTOMLDecodeError.__module__ = __name__\n", "fuzzer/fuzz.py": "import atheris\n\nwith atheris.instrument_imports():\n    from math import isnan\n    import sys\n    import warnings\n\n    import tomli_w\n\n    import tomli\n\n# Disable any caching used so that the same lines of code run\n# on a given input consistently.\ntomli._re.cached_tz = tomli._re.cached_tz.__wrapped__\n\n# Suppress all warnings.\nwarnings.simplefilter(\"ignore\")\n\n\ndef test_one_input(input_bytes: bytes) -> None:\n    # We need a Unicode string, not bytes\n    fdp = atheris.FuzzedDataProvider(input_bytes)\n    data = fdp.ConsumeUnicode(sys.maxsize)\n\n    try:\n        toml_obj = tomli.loads(data)\n    except (tomli.TOMLDecodeError, RecursionError):\n        return\n    except BaseException:\n        print_err(data)\n        raise\n\n    try:\n        recovered_data = tomli_w.dumps(toml_obj)\n    except RecursionError:\n        return\n    except BaseException:\n        print_err(data)\n        raise\n\n    roundtripped_obj = tomli.loads(recovered_data)\n    normalize_toml_obj(roundtripped_obj)\n    normalize_toml_obj(toml_obj)\n    if roundtripped_obj != toml_obj:\n        sys.stderr.write(\n            f\"Original dict:\\n{toml_obj}\\nRoundtripped dict:\\n{roundtripped_obj}\\n\"\n        )\n        sys.stderr.flush()\n        print_err(data)\n        raise Exception(\"Dicts not equal after roundtrip\")\n\n\ndef print_err(data):\n    codepoints = [hex(ord(x)) for x in data]\n    sys.stderr.write(f\"Input was {type(data)}:\\n{data}\\nCodepoints:\\n{codepoints}\\n\")\n    sys.stderr.flush()\n\n\ndef normalize_toml_obj(toml_obj: dict) -> None:\n    \"\"\"Make NaNs equal when compared (without using recursion).\"\"\"\n    to_process = [toml_obj]\n    while to_process:\n        cont = to_process.pop()\n        for k, v in cont.items() if isinstance(cont, dict) else enumerate(cont):\n            if isinstance(v, float) and isnan(v):\n                cont[k] = \"nan\"\n            elif isinstance(v, (dict, list)):\n                to_process.append(v)\n\n\ndef main():\n    # For possible options, see https://llvm.org/docs/LibFuzzer.html#options\n    fuzzer_options = sys.argv\n    atheris.Setup(fuzzer_options, test_one_input)\n    atheris.Fuzz()\n\n\nif __name__ == \"__main__\":\n    main()\n"}