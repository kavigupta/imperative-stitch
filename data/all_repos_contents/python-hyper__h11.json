{"setup.py": "from setuptools import setup, find_packages\n\n# defines __version__\nexec(open(\"h11/_version.py\").read())\n\nsetup(\n    name=\"h11\",\n    version=__version__,\n    description=\n        \"A pure-Python, bring-your-own-I/O implementation of HTTP/1.1\",\n    long_description=open(\"README.rst\").read(),\n    author=\"Nathaniel J. Smith\",\n    author_email=\"njs@pobox.com\",\n    license=\"MIT\",\n    packages=find_packages(exclude=[\"h11.tests\"]),\n    package_data={'h11': ['py.typed']},\n    url=\"https://github.com/python-hyper/h11\",\n    python_requires=\">=3.8\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: System :: Networking\",\n    ],\n)\n", "bench/benchmarks/benchmarks.py": "# Write the benchmarking functions here.\n# See \"Writing benchmarks\" in the asv docs for more information.\n\nimport h11\n\n\n# Basic ASV benchmark of core functionality\ndef time_server_basic_get_with_realistic_headers():\n    c = h11.Connection(h11.SERVER)\n    c.receive_data(\n        b\"GET / HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"User-Agent: Mozilla/5.0 (X11; Linux x86_64; \"\n        b\"rv:45.0) Gecko/20100101 Firefox/45.0\\r\\n\"\n        b\"Accept: text/html,application/xhtml+xml,\"\n        b\"application/xml;q=0.9,*/*;q=0.8\\r\\n\"\n        b\"Accept-Language: en-US,en;q=0.5\\r\\n\"\n        b\"Accept-Encoding: gzip, deflate, br\\r\\n\"\n        b\"DNT: 1\\r\\n\"\n        b\"Cookie: ID=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\\r\\n\"\n        b\"Connection: keep-alive\\r\\n\\r\\n\"\n    )\n    while True:\n        event = c.next_event()\n        if event is h11.NEED_DATA:\n            break\n\n    c.send(\n        h11.Response(\n            status_code=200,\n            headers=[\n                (b\"Cache-Control\", b\"private, max-age=0\"),\n                (b\"Content-Encoding\", b\"gzip\"),\n                (b\"Content-Type\", b\"text/html; charset=UTF-8\"),\n                (b\"Date\", b\"Fri, 20 May 2016 09:23:41 GMT\"),\n                (b\"Expires\", b\"-1\"),\n                (b\"Server\", b\"gws\"),\n                (b\"X-Frame-Options\", b\"SAMEORIGIN\"),\n                (b\"X-XSS-Protection\", b\"1; mode=block\"),\n                (b\"Content-Length\", b\"1000\"),\n            ],\n        )\n    )\n    c.send(h11.Data(data=b\"x\" * 1000))\n    c.send(h11.EndOfMessage())\n\n\n# Useful for manual benchmarking, e.g. with vmprof or on PyPy\ndef _run_basic_get_repeatedly():\n    from timeit import default_timer\n\n    REPEAT = 10000\n    # while True:\n    for _ in range(7):\n        start = default_timer()\n        for _ in range(REPEAT):\n            time_server_basic_get_with_realistic_headers()\n        finish = default_timer()\n        print(f\"{REPEAT / (finish - start):.1f} requests/sec\")\n\n\nif __name__ == \"__main__\":\n    _run_basic_get_repeatedly()\n", "bench/benchmarks/__init__.py": "", "h11/_readers.py": "# Code to read HTTP data\n#\n# Strategy: each reader is a callable which takes a ReceiveBuffer object, and\n# either:\n# 1) consumes some of it and returns an Event\n# 2) raises a LocalProtocolError (for consistency -- e.g. we call validate()\n#    and it might raise a LocalProtocolError, so simpler just to always use\n#    this)\n# 3) returns None, meaning \"I need more data\"\n#\n# If they have a .read_eof attribute, then this will be called if an EOF is\n# received -- but this is optional. Either way, the actual ConnectionClosed\n# event will be generated afterwards.\n#\n# READERS is a dict describing how to pick a reader. It maps states to either:\n# - a reader\n# - or, for body readers, a dict of per-framing reader factories\n\nimport re\nfrom typing import Any, Callable, Dict, Iterable, NoReturn, Optional, Tuple, Type, Union\n\nfrom ._abnf import chunk_header, header_field, request_line, status_line\nfrom ._events import Data, EndOfMessage, InformationalResponse, Request, Response\nfrom ._receivebuffer import ReceiveBuffer\nfrom ._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    IDLE,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n)\nfrom ._util import LocalProtocolError, RemoteProtocolError, Sentinel, validate\n\n__all__ = [\"READERS\"]\n\nheader_field_re = re.compile(header_field.encode(\"ascii\"))\nobs_fold_re = re.compile(rb\"[ \\t]+\")\n\n\ndef _obsolete_line_fold(lines: Iterable[bytes]) -> Iterable[bytes]:\n    it = iter(lines)\n    last: Optional[bytes] = None\n    for line in it:\n        match = obs_fold_re.match(line)\n        if match:\n            if last is None:\n                raise LocalProtocolError(\"continuation line at start of headers\")\n            if not isinstance(last, bytearray):\n                # Cast to a mutable type, avoiding copy on append to ensure O(n) time\n                last = bytearray(last)\n            last += b\" \"\n            last += line[match.end() :]\n        else:\n            if last is not None:\n                yield last\n            last = line\n    if last is not None:\n        yield last\n\n\ndef _decode_header_lines(\n    lines: Iterable[bytes],\n) -> Iterable[Tuple[bytes, bytes]]:\n    for line in _obsolete_line_fold(lines):\n        matches = validate(header_field_re, line, \"illegal header line: {!r}\", line)\n        yield (matches[\"field_name\"], matches[\"field_value\"])\n\n\nrequest_line_re = re.compile(request_line.encode(\"ascii\"))\n\n\ndef maybe_read_from_IDLE_client(buf: ReceiveBuffer) -> Optional[Request]:\n    lines = buf.maybe_extract_lines()\n    if lines is None:\n        if buf.is_next_line_obviously_invalid_request_line():\n            raise LocalProtocolError(\"illegal request line\")\n        return None\n    if not lines:\n        raise LocalProtocolError(\"no request line received\")\n    matches = validate(\n        request_line_re, lines[0], \"illegal request line: {!r}\", lines[0]\n    )\n    return Request(\n        headers=list(_decode_header_lines(lines[1:])), _parsed=True, **matches\n    )\n\n\nstatus_line_re = re.compile(status_line.encode(\"ascii\"))\n\n\ndef maybe_read_from_SEND_RESPONSE_server(\n    buf: ReceiveBuffer,\n) -> Union[InformationalResponse, Response, None]:\n    lines = buf.maybe_extract_lines()\n    if lines is None:\n        if buf.is_next_line_obviously_invalid_request_line():\n            raise LocalProtocolError(\"illegal request line\")\n        return None\n    if not lines:\n        raise LocalProtocolError(\"no response line received\")\n    matches = validate(status_line_re, lines[0], \"illegal status line: {!r}\", lines[0])\n    http_version = (\n        b\"1.1\" if matches[\"http_version\"] is None else matches[\"http_version\"]\n    )\n    reason = b\"\" if matches[\"reason\"] is None else matches[\"reason\"]\n    status_code = int(matches[\"status_code\"])\n    class_: Union[Type[InformationalResponse], Type[Response]] = (\n        InformationalResponse if status_code < 200 else Response\n    )\n    return class_(\n        headers=list(_decode_header_lines(lines[1:])),\n        _parsed=True,\n        status_code=status_code,\n        reason=reason,\n        http_version=http_version,\n    )\n\n\nclass ContentLengthReader:\n    def __init__(self, length: int) -> None:\n        self._length = length\n        self._remaining = length\n\n    def __call__(self, buf: ReceiveBuffer) -> Union[Data, EndOfMessage, None]:\n        if self._remaining == 0:\n            return EndOfMessage()\n        data = buf.maybe_extract_at_most(self._remaining)\n        if data is None:\n            return None\n        self._remaining -= len(data)\n        return Data(data=data)\n\n    def read_eof(self) -> NoReturn:\n        raise RemoteProtocolError(\n            \"peer closed connection without sending complete message body \"\n            \"(received {} bytes, expected {})\".format(\n                self._length - self._remaining, self._length\n            )\n        )\n\n\nchunk_header_re = re.compile(chunk_header.encode(\"ascii\"))\n\n\nclass ChunkedReader:\n    def __init__(self) -> None:\n        self._bytes_in_chunk = 0\n        # After reading a chunk, we have to throw away the trailing \\r\\n; if\n        # this is >0 then we discard that many bytes before resuming regular\n        # de-chunkification.\n        self._bytes_to_discard = 0\n        self._reading_trailer = False\n\n    def __call__(self, buf: ReceiveBuffer) -> Union[Data, EndOfMessage, None]:\n        if self._reading_trailer:\n            lines = buf.maybe_extract_lines()\n            if lines is None:\n                return None\n            return EndOfMessage(headers=list(_decode_header_lines(lines)))\n        if self._bytes_to_discard > 0:\n            data = buf.maybe_extract_at_most(self._bytes_to_discard)\n            if data is None:\n                return None\n            self._bytes_to_discard -= len(data)\n            if self._bytes_to_discard > 0:\n                return None\n            # else, fall through and read some more\n        assert self._bytes_to_discard == 0\n        if self._bytes_in_chunk == 0:\n            # We need to refill our chunk count\n            chunk_header = buf.maybe_extract_next_line()\n            if chunk_header is None:\n                return None\n            matches = validate(\n                chunk_header_re,\n                chunk_header,\n                \"illegal chunk header: {!r}\",\n                chunk_header,\n            )\n            # XX FIXME: we discard chunk extensions. Does anyone care?\n            self._bytes_in_chunk = int(matches[\"chunk_size\"], base=16)\n            if self._bytes_in_chunk == 0:\n                self._reading_trailer = True\n                return self(buf)\n            chunk_start = True\n        else:\n            chunk_start = False\n        assert self._bytes_in_chunk > 0\n        data = buf.maybe_extract_at_most(self._bytes_in_chunk)\n        if data is None:\n            return None\n        self._bytes_in_chunk -= len(data)\n        if self._bytes_in_chunk == 0:\n            self._bytes_to_discard = 2\n            chunk_end = True\n        else:\n            chunk_end = False\n        return Data(data=data, chunk_start=chunk_start, chunk_end=chunk_end)\n\n    def read_eof(self) -> NoReturn:\n        raise RemoteProtocolError(\n            \"peer closed connection without sending complete message body \"\n            \"(incomplete chunked read)\"\n        )\n\n\nclass Http10Reader:\n    def __call__(self, buf: ReceiveBuffer) -> Optional[Data]:\n        data = buf.maybe_extract_at_most(999999999)\n        if data is None:\n            return None\n        return Data(data=data)\n\n    def read_eof(self) -> EndOfMessage:\n        return EndOfMessage()\n\n\ndef expect_nothing(buf: ReceiveBuffer) -> None:\n    if buf:\n        raise LocalProtocolError(\"Got data when expecting EOF\")\n    return None\n\n\nReadersType = Dict[\n    Union[Type[Sentinel], Tuple[Type[Sentinel], Type[Sentinel]]],\n    Union[Callable[..., Any], Dict[str, Callable[..., Any]]],\n]\n\nREADERS: ReadersType = {\n    (CLIENT, IDLE): maybe_read_from_IDLE_client,\n    (SERVER, IDLE): maybe_read_from_SEND_RESPONSE_server,\n    (SERVER, SEND_RESPONSE): maybe_read_from_SEND_RESPONSE_server,\n    (CLIENT, DONE): expect_nothing,\n    (CLIENT, MUST_CLOSE): expect_nothing,\n    (CLIENT, CLOSED): expect_nothing,\n    (SERVER, DONE): expect_nothing,\n    (SERVER, MUST_CLOSE): expect_nothing,\n    (SERVER, CLOSED): expect_nothing,\n    SEND_BODY: {\n        \"chunked\": ChunkedReader,\n        \"content-length\": ContentLengthReader,\n        \"http/1.0\": Http10Reader,\n    },\n}\n", "h11/_abnf.py": "# We use native strings for all the re patterns, to take advantage of string\n# formatting, and then convert to bytestrings when compiling the final re\n# objects.\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#whitespace\n#  OWS            = *( SP / HTAB )\n#                 ; optional whitespace\nOWS = r\"[ \\t]*\"\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#rule.token.separators\n#   token          = 1*tchar\n#\n#   tchar          = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\"\n#                  / \"+\" / \"-\" / \".\" / \"^\" / \"_\" / \"`\" / \"|\" / \"~\"\n#                  / DIGIT / ALPHA\n#                  ; any VCHAR, except delimiters\ntoken = r\"[-!#$%&'*+.^_`|~0-9a-zA-Z]+\"\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#header.fields\n#  field-name     = token\nfield_name = token\n\n# The standard says:\n#\n#  field-value    = *( field-content / obs-fold )\n#  field-content  = field-vchar [ 1*( SP / HTAB ) field-vchar ]\n#  field-vchar    = VCHAR / obs-text\n#  obs-fold       = CRLF 1*( SP / HTAB )\n#                 ; obsolete line folding\n#                 ; see Section 3.2.4\n#\n# https://tools.ietf.org/html/rfc5234#appendix-B.1\n#\n#   VCHAR          =  %x21-7E\n#                  ; visible (printing) characters\n#\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#rule.quoted-string\n#   obs-text       = %x80-FF\n#\n# However, the standard definition of field-content is WRONG! It disallows\n# fields containing a single visible character surrounded by whitespace,\n# e.g. \"foo a bar\".\n#\n# See: https://www.rfc-editor.org/errata_search.php?rfc=7230&eid=4189\n#\n# So our definition of field_content attempts to fix it up...\n#\n# Also, we allow lots of control characters, because apparently people assume\n# that they're legal in practice (e.g., google analytics makes cookies with\n# \\x01 in them!):\n#   https://github.com/python-hyper/h11/issues/57\n# We still don't allow NUL or whitespace, because those are often treated as\n# meta-characters and letting them through can lead to nasty issues like SSRF.\nvchar = r\"[\\x21-\\x7e]\"\nvchar_or_obs_text = r\"[^\\x00\\s]\"\nfield_vchar = vchar_or_obs_text\nfield_content = r\"{field_vchar}+(?:[ \\t]+{field_vchar}+)*\".format(**globals())\n\n# We handle obs-fold at a different level, and our fixed-up field_content\n# already grows to swallow the whole value, so ? instead of *\nfield_value = r\"({field_content})?\".format(**globals())\n\n#  header-field   = field-name \":\" OWS field-value OWS\nheader_field = (\n    r\"(?P<field_name>{field_name})\"\n    r\":\"\n    r\"{OWS}\"\n    r\"(?P<field_value>{field_value})\"\n    r\"{OWS}\".format(**globals())\n)\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#request.line\n#\n#   request-line   = method SP request-target SP HTTP-version CRLF\n#   method         = token\n#   HTTP-version   = HTTP-name \"/\" DIGIT \".\" DIGIT\n#   HTTP-name      = %x48.54.54.50 ; \"HTTP\", case-sensitive\n#\n# request-target is complicated (see RFC 7230 sec 5.3) -- could be path, full\n# URL, host+port (for connect), or even \"*\", but in any case we are guaranteed\n# that it contists of the visible printing characters.\nmethod = token\nrequest_target = r\"{vchar}+\".format(**globals())\nhttp_version = r\"HTTP/(?P<http_version>[0-9]\\.[0-9])\"\nrequest_line = (\n    r\"(?P<method>{method})\"\n    r\" \"\n    r\"(?P<target>{request_target})\"\n    r\" \"\n    r\"{http_version}\".format(**globals())\n)\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#status.line\n#\n#   status-line = HTTP-version SP status-code SP reason-phrase CRLF\n#   status-code    = 3DIGIT\n#   reason-phrase  = *( HTAB / SP / VCHAR / obs-text )\nstatus_code = r\"[0-9]{3}\"\nreason_phrase = r\"([ \\t]|{vchar_or_obs_text})*\".format(**globals())\nstatus_line = (\n    r\"{http_version}\"\n    r\" \"\n    r\"(?P<status_code>{status_code})\"\n    # However, there are apparently a few too many servers out there that just\n    # leave out the reason phrase:\n    #   https://github.com/scrapy/scrapy/issues/345#issuecomment-281756036\n    #   https://github.com/seanmonstar/httparse/issues/29\n    # so make it optional. ?: is a non-capturing group.\n    r\"(?: (?P<reason>{reason_phrase}))?\".format(**globals())\n)\n\nHEXDIG = r\"[0-9A-Fa-f]\"\n# Actually\n#\n#      chunk-size     = 1*HEXDIG\n#\n# but we impose an upper-limit to avoid ridiculosity. len(str(2**64)) == 20\nchunk_size = r\"({HEXDIG}){{1,20}}\".format(**globals())\n# Actually\n#\n#     chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-val ] )\n#\n# but we aren't parsing the things so we don't really care.\nchunk_ext = r\";.*\"\nchunk_header = (\n    r\"(?P<chunk_size>{chunk_size})\"\n    r\"(?P<chunk_ext>{chunk_ext})?\"\n    r\"{OWS}\\r\\n\".format(\n        **globals()\n    )  # Even though the specification does not allow for extra whitespaces,\n    # we are lenient with trailing whitespaces because some servers on the wild use it.\n)\n", "h11/_receivebuffer.py": "import re\nimport sys\nfrom typing import List, Optional, Union\n\n__all__ = [\"ReceiveBuffer\"]\n\n\n# Operations we want to support:\n# - find next \\r\\n or \\r\\n\\r\\n (\\n or \\n\\n are also acceptable),\n#   or wait until there is one\n# - read at-most-N bytes\n# Goals:\n# - on average, do this fast\n# - worst case, do this in O(n) where n is the number of bytes processed\n# Plan:\n# - store bytearray, offset, how far we've searched for a separator token\n# - use the how-far-we've-searched data to avoid rescanning\n# - while doing a stream of uninterrupted processing, advance offset instead\n#   of constantly copying\n# WARNING:\n# - I haven't benchmarked or profiled any of this yet.\n#\n# Note that starting in Python 3.4, deleting the initial n bytes from a\n# bytearray is amortized O(n), thanks to some excellent work by Antoine\n# Martin:\n#\n#     https://bugs.python.org/issue19087\n#\n# This means that if we only supported 3.4+, we could get rid of the code here\n# involving self._start and self.compress, because it's doing exactly the same\n# thing that bytearray now does internally.\n#\n# BUT unfortunately, we still support 2.7, and reading short segments out of a\n# long buffer MUST be O(bytes read) to avoid DoS issues, so we can't actually\n# delete this code. Yet:\n#\n#     https://pythonclock.org/\n#\n# (Two things to double-check first though: make sure PyPy also has the\n# optimization, and benchmark to make sure it's a win, since we do have a\n# slightly clever thing where we delay calling compress() until we've\n# processed a whole event, which could in theory be slightly more efficient\n# than the internal bytearray support.)\nblank_line_regex = re.compile(b\"\\n\\r?\\n\", re.MULTILINE)\n\n\nclass ReceiveBuffer:\n    def __init__(self) -> None:\n        self._data = bytearray()\n        self._next_line_search = 0\n        self._multiple_lines_search = 0\n\n    def __iadd__(self, byteslike: Union[bytes, bytearray]) -> \"ReceiveBuffer\":\n        self._data += byteslike\n        return self\n\n    def __bool__(self) -> bool:\n        return bool(len(self))\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    # for @property unprocessed_data\n    def __bytes__(self) -> bytes:\n        return bytes(self._data)\n\n    def _extract(self, count: int) -> bytearray:\n        # extracting an initial slice of the data buffer and return it\n        out = self._data[:count]\n        del self._data[:count]\n\n        self._next_line_search = 0\n        self._multiple_lines_search = 0\n\n        return out\n\n    def maybe_extract_at_most(self, count: int) -> Optional[bytearray]:\n        \"\"\"\n        Extract a fixed number of bytes from the buffer.\n        \"\"\"\n        out = self._data[:count]\n        if not out:\n            return None\n\n        return self._extract(count)\n\n    def maybe_extract_next_line(self) -> Optional[bytearray]:\n        \"\"\"\n        Extract the first line, if it is completed in the buffer.\n        \"\"\"\n        # Only search in buffer space that we've not already looked at.\n        search_start_index = max(0, self._next_line_search - 1)\n        partial_idx = self._data.find(b\"\\r\\n\", search_start_index)\n\n        if partial_idx == -1:\n            self._next_line_search = len(self._data)\n            return None\n\n        # + 2 is to compensate len(b\"\\r\\n\")\n        idx = partial_idx + 2\n\n        return self._extract(idx)\n\n    def maybe_extract_lines(self) -> Optional[List[bytearray]]:\n        \"\"\"\n        Extract everything up to the first blank line, and return a list of lines.\n        \"\"\"\n        # Handle the case where we have an immediate empty line.\n        if self._data[:1] == b\"\\n\":\n            self._extract(1)\n            return []\n\n        if self._data[:2] == b\"\\r\\n\":\n            self._extract(2)\n            return []\n\n        # Only search in buffer space that we've not already looked at.\n        match = blank_line_regex.search(self._data, self._multiple_lines_search)\n        if match is None:\n            self._multiple_lines_search = max(0, len(self._data) - 2)\n            return None\n\n        # Truncate the buffer and return it.\n        idx = match.span(0)[-1]\n        out = self._extract(idx)\n        lines = out.split(b\"\\n\")\n\n        for line in lines:\n            if line.endswith(b\"\\r\"):\n                del line[-1]\n\n        assert lines[-2] == lines[-1] == b\"\"\n\n        del lines[-2:]\n\n        return lines\n\n    # In theory we should wait until `\\r\\n` before starting to validate\n    # incoming data. However it's interesting to detect (very) invalid data\n    # early given they might not even contain `\\r\\n` at all (hence only\n    # timeout will get rid of them).\n    # This is not a 100% effective detection but more of a cheap sanity check\n    # allowing for early abort in some useful cases.\n    # This is especially interesting when peer is messing up with HTTPS and\n    # sent us a TLS stream where we were expecting plain HTTP given all\n    # versions of TLS so far start handshake with a 0x16 message type code.\n    def is_next_line_obviously_invalid_request_line(self) -> bool:\n        try:\n            # HTTP header line must not contain non-printable characters\n            # and should not start with a space\n            return self._data[0] < 0x21\n        except IndexError:\n            return False\n", "h11/_state.py": "################################################################\n# The core state machine\n################################################################\n#\n# Rule 1: everything that affects the state machine and state transitions must\n# live here in this file. As much as possible goes into the table-based\n# representation, but for the bits that don't quite fit, the actual code and\n# state must nonetheless live here.\n#\n# Rule 2: this file does not know about what role we're playing; it only knows\n# about HTTP request/response cycles in the abstract. This ensures that we\n# don't cheat and apply different rules to local and remote parties.\n#\n#\n# Theory of operation\n# ===================\n#\n# Possibly the simplest way to think about this is that we actually have 5\n# different state machines here. Yes, 5. These are:\n#\n# 1) The client state, with its complicated automaton (see the docs)\n# 2) The server state, with its complicated automaton (see the docs)\n# 3) The keep-alive state, with possible states {True, False}\n# 4) The SWITCH_CONNECT state, with possible states {False, True}\n# 5) The SWITCH_UPGRADE state, with possible states {False, True}\n#\n# For (3)-(5), the first state listed is the initial state.\n#\n# (1)-(3) are stored explicitly in member variables. The last\n# two are stored implicitly in the pending_switch_proposals set as:\n#   (state of 4) == (_SWITCH_CONNECT in pending_switch_proposals)\n#   (state of 5) == (_SWITCH_UPGRADE in pending_switch_proposals)\n#\n# And each of these machines has two different kinds of transitions:\n#\n# a) Event-triggered\n# b) State-triggered\n#\n# Event triggered is the obvious thing that you'd think it is: some event\n# happens, and if it's the right event at the right time then a transition\n# happens. But there are somewhat complicated rules for which machines can\n# \"see\" which events. (As a rule of thumb, if a machine \"sees\" an event, this\n# means two things: the event can affect the machine, and if the machine is\n# not in a state where it expects that event then it's an error.) These rules\n# are:\n#\n# 1) The client machine sees all h11.events objects emitted by the client.\n#\n# 2) The server machine sees all h11.events objects emitted by the server.\n#\n#    It also sees the client's Request event.\n#\n#    And sometimes, server events are annotated with a _SWITCH_* event. For\n#    example, we can have a (Response, _SWITCH_CONNECT) event, which is\n#    different from a regular Response event.\n#\n# 3) The keep-alive machine sees the process_keep_alive_disabled() event\n#    (which is derived from Request/Response events), and this event\n#    transitions it from True -> False, or from False -> False. There's no way\n#    to transition back.\n#\n# 4&5) The _SWITCH_* machines transition from False->True when we get a\n#    Request that proposes the relevant type of switch (via\n#    process_client_switch_proposals), and they go from True->False when we\n#    get a Response that has no _SWITCH_* annotation.\n#\n# So that's event-triggered transitions.\n#\n# State-triggered transitions are less standard. What they do here is couple\n# the machines together. The way this works is, when certain *joint*\n# configurations of states are achieved, then we automatically transition to a\n# new *joint* state. So, for example, if we're ever in a joint state with\n#\n#   client: DONE\n#   keep-alive: False\n#\n# then the client state immediately transitions to:\n#\n#   client: MUST_CLOSE\n#\n# This is fundamentally different from an event-based transition, because it\n# doesn't matter how we arrived at the {client: DONE, keep-alive: False} state\n# -- maybe the client transitioned SEND_BODY -> DONE, or keep-alive\n# transitioned True -> False. Either way, once this precondition is satisfied,\n# this transition is immediately triggered.\n#\n# What if two conflicting state-based transitions get enabled at the same\n# time?  In practice there's only one case where this arises (client DONE ->\n# MIGHT_SWITCH_PROTOCOL versus DONE -> MUST_CLOSE), and we resolve it by\n# explicitly prioritizing the DONE -> MIGHT_SWITCH_PROTOCOL transition.\n#\n# Implementation\n# --------------\n#\n# The event-triggered transitions for the server and client machines are all\n# stored explicitly in a table. Ditto for the state-triggered transitions that\n# involve just the server and client state.\n#\n# The transitions for the other machines, and the state-triggered transitions\n# that involve the other machines, are written out as explicit Python code.\n#\n# It'd be nice if there were some cleaner way to do all this. This isn't\n# *too* terrible, but I feel like it could probably be better.\n#\n# WARNING\n# -------\n#\n# The script that generates the state machine diagrams for the docs knows how\n# to read out the EVENT_TRIGGERED_TRANSITIONS and STATE_TRIGGERED_TRANSITIONS\n# tables. But it can't automatically read the transitions that are written\n# directly in Python code. So if you touch those, you need to also update the\n# script to keep it in sync!\nfrom typing import cast, Dict, Optional, Set, Tuple, Type, Union\n\nfrom ._events import *\nfrom ._util import LocalProtocolError, Sentinel\n\n# Everything in __all__ gets re-exported as part of the h11 public API.\n__all__ = [\n    \"CLIENT\",\n    \"SERVER\",\n    \"IDLE\",\n    \"SEND_RESPONSE\",\n    \"SEND_BODY\",\n    \"DONE\",\n    \"MUST_CLOSE\",\n    \"CLOSED\",\n    \"MIGHT_SWITCH_PROTOCOL\",\n    \"SWITCHED_PROTOCOL\",\n    \"ERROR\",\n]\n\n\nclass CLIENT(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SERVER(Sentinel, metaclass=Sentinel):\n    pass\n\n\n# States\nclass IDLE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SEND_RESPONSE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SEND_BODY(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass DONE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass MUST_CLOSE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass CLOSED(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass ERROR(Sentinel, metaclass=Sentinel):\n    pass\n\n\n# Switch types\nclass MIGHT_SWITCH_PROTOCOL(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SWITCHED_PROTOCOL(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass _SWITCH_UPGRADE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass _SWITCH_CONNECT(Sentinel, metaclass=Sentinel):\n    pass\n\n\nEventTransitionType = Dict[\n    Type[Sentinel],\n    Dict[\n        Type[Sentinel],\n        Dict[Union[Type[Event], Tuple[Type[Event], Type[Sentinel]]], Type[Sentinel]],\n    ],\n]\n\nEVENT_TRIGGERED_TRANSITIONS: EventTransitionType = {\n    CLIENT: {\n        IDLE: {Request: SEND_BODY, ConnectionClosed: CLOSED},\n        SEND_BODY: {Data: SEND_BODY, EndOfMessage: DONE},\n        DONE: {ConnectionClosed: CLOSED},\n        MUST_CLOSE: {ConnectionClosed: CLOSED},\n        CLOSED: {ConnectionClosed: CLOSED},\n        MIGHT_SWITCH_PROTOCOL: {},\n        SWITCHED_PROTOCOL: {},\n        ERROR: {},\n    },\n    SERVER: {\n        IDLE: {\n            ConnectionClosed: CLOSED,\n            Response: SEND_BODY,\n            # Special case: server sees client Request events, in this form\n            (Request, CLIENT): SEND_RESPONSE,\n        },\n        SEND_RESPONSE: {\n            InformationalResponse: SEND_RESPONSE,\n            Response: SEND_BODY,\n            (InformationalResponse, _SWITCH_UPGRADE): SWITCHED_PROTOCOL,\n            (Response, _SWITCH_CONNECT): SWITCHED_PROTOCOL,\n        },\n        SEND_BODY: {Data: SEND_BODY, EndOfMessage: DONE},\n        DONE: {ConnectionClosed: CLOSED},\n        MUST_CLOSE: {ConnectionClosed: CLOSED},\n        CLOSED: {ConnectionClosed: CLOSED},\n        SWITCHED_PROTOCOL: {},\n        ERROR: {},\n    },\n}\n\nStateTransitionType = Dict[\n    Tuple[Type[Sentinel], Type[Sentinel]], Dict[Type[Sentinel], Type[Sentinel]]\n]\n\n# NB: there are also some special-case state-triggered transitions hard-coded\n# into _fire_state_triggered_transitions below.\nSTATE_TRIGGERED_TRANSITIONS: StateTransitionType = {\n    # (Client state, Server state) -> new states\n    # Protocol negotiation\n    (MIGHT_SWITCH_PROTOCOL, SWITCHED_PROTOCOL): {CLIENT: SWITCHED_PROTOCOL},\n    # Socket shutdown\n    (CLOSED, DONE): {SERVER: MUST_CLOSE},\n    (CLOSED, IDLE): {SERVER: MUST_CLOSE},\n    (ERROR, DONE): {SERVER: MUST_CLOSE},\n    (DONE, CLOSED): {CLIENT: MUST_CLOSE},\n    (IDLE, CLOSED): {CLIENT: MUST_CLOSE},\n    (DONE, ERROR): {CLIENT: MUST_CLOSE},\n}\n\n\nclass ConnectionState:\n    def __init__(self) -> None:\n        # Extra bits of state that don't quite fit into the state model.\n\n        # If this is False then it enables the automatic DONE -> MUST_CLOSE\n        # transition. Don't set this directly; call .keep_alive_disabled()\n        self.keep_alive = True\n\n        # This is a subset of {UPGRADE, CONNECT}, containing the proposals\n        # made by the client for switching protocols.\n        self.pending_switch_proposals: Set[Type[Sentinel]] = set()\n\n        self.states: Dict[Type[Sentinel], Type[Sentinel]] = {CLIENT: IDLE, SERVER: IDLE}\n\n    def process_error(self, role: Type[Sentinel]) -> None:\n        self.states[role] = ERROR\n        self._fire_state_triggered_transitions()\n\n    def process_keep_alive_disabled(self) -> None:\n        self.keep_alive = False\n        self._fire_state_triggered_transitions()\n\n    def process_client_switch_proposal(self, switch_event: Type[Sentinel]) -> None:\n        self.pending_switch_proposals.add(switch_event)\n        self._fire_state_triggered_transitions()\n\n    def process_event(\n        self,\n        role: Type[Sentinel],\n        event_type: Type[Event],\n        server_switch_event: Optional[Type[Sentinel]] = None,\n    ) -> None:\n        _event_type: Union[Type[Event], Tuple[Type[Event], Type[Sentinel]]] = event_type\n        if server_switch_event is not None:\n            assert role is SERVER\n            if server_switch_event not in self.pending_switch_proposals:\n                raise LocalProtocolError(\n                    \"Received server _SWITCH_UPGRADE event without a pending proposal\"\n                )\n            _event_type = (event_type, server_switch_event)\n        if server_switch_event is None and _event_type is Response:\n            self.pending_switch_proposals = set()\n        self._fire_event_triggered_transitions(role, _event_type)\n        # Special case: the server state does get to see Request\n        # events.\n        if _event_type is Request:\n            assert role is CLIENT\n            self._fire_event_triggered_transitions(SERVER, (Request, CLIENT))\n        self._fire_state_triggered_transitions()\n\n    def _fire_event_triggered_transitions(\n        self,\n        role: Type[Sentinel],\n        event_type: Union[Type[Event], Tuple[Type[Event], Type[Sentinel]]],\n    ) -> None:\n        state = self.states[role]\n        try:\n            new_state = EVENT_TRIGGERED_TRANSITIONS[role][state][event_type]\n        except KeyError:\n            event_type = cast(Type[Event], event_type)\n            raise LocalProtocolError(\n                \"can't handle event type {} when role={} and state={}\".format(\n                    event_type.__name__, role, self.states[role]\n                )\n            ) from None\n        self.states[role] = new_state\n\n    def _fire_state_triggered_transitions(self) -> None:\n        # We apply these rules repeatedly until converging on a fixed point\n        while True:\n            start_states = dict(self.states)\n\n            # It could happen that both these special-case transitions are\n            # enabled at the same time:\n            #\n            #    DONE -> MIGHT_SWITCH_PROTOCOL\n            #    DONE -> MUST_CLOSE\n            #\n            # For example, this will always be true of a HTTP/1.0 client\n            # requesting CONNECT.  If this happens, the protocol switch takes\n            # priority. From there the client will either go to\n            # SWITCHED_PROTOCOL, in which case it's none of our business when\n            # they close the connection, or else the server will deny the\n            # request, in which case the client will go back to DONE and then\n            # from there to MUST_CLOSE.\n            if self.pending_switch_proposals:\n                if self.states[CLIENT] is DONE:\n                    self.states[CLIENT] = MIGHT_SWITCH_PROTOCOL\n\n            if not self.pending_switch_proposals:\n                if self.states[CLIENT] is MIGHT_SWITCH_PROTOCOL:\n                    self.states[CLIENT] = DONE\n\n            if not self.keep_alive:\n                for role in (CLIENT, SERVER):\n                    if self.states[role] is DONE:\n                        self.states[role] = MUST_CLOSE\n\n            # Tabular state-triggered transitions\n            joint_state = (self.states[CLIENT], self.states[SERVER])\n            changes = STATE_TRIGGERED_TRANSITIONS.get(joint_state, {})\n            self.states.update(changes)\n\n            if self.states == start_states:\n                # Fixed point reached\n                return\n\n    def start_next_cycle(self) -> None:\n        if self.states != {CLIENT: DONE, SERVER: DONE}:\n            raise LocalProtocolError(\n                f\"not in a reusable state. self.states={self.states}\"\n            )\n        # Can't reach DONE/DONE with any of these active, but still, let's be\n        # sure.\n        assert self.keep_alive\n        assert not self.pending_switch_proposals\n        self.states = {CLIENT: IDLE, SERVER: IDLE}\n", "h11/_headers.py": "import re\nfrom typing import AnyStr, cast, List, overload, Sequence, Tuple, TYPE_CHECKING, Union\n\nfrom ._abnf import field_name, field_value\nfrom ._util import bytesify, LocalProtocolError, validate\n\nif TYPE_CHECKING:\n    from ._events import Request\n\ntry:\n    from typing import Literal\nexcept ImportError:\n    from typing_extensions import Literal  # type: ignore\n\n\n# Facts\n# -----\n#\n# Headers are:\n#   keys: case-insensitive ascii\n#   values: mixture of ascii and raw bytes\n#\n# \"Historically, HTTP has allowed field content with text in the ISO-8859-1\n# charset [ISO-8859-1], supporting other charsets only through use of\n# [RFC2047] encoding.  In practice, most HTTP header field values use only a\n# subset of the US-ASCII charset [USASCII]. Newly defined header fields SHOULD\n# limit their field values to US-ASCII octets.  A recipient SHOULD treat other\n# octets in field content (obs-text) as opaque data.\"\n# And it deprecates all non-ascii values\n#\n# Leading/trailing whitespace in header names is forbidden\n#\n# Values get leading/trailing whitespace stripped\n#\n# Content-Disposition actually needs to contain unicode semantically; to\n# accomplish this it has a terrifically weird way of encoding the filename\n# itself as ascii (and even this still has lots of cross-browser\n# incompatibilities)\n#\n# Order is important:\n# \"a proxy MUST NOT change the order of these field values when forwarding a\n# message\"\n# (and there are several headers where the order indicates a preference)\n#\n# Multiple occurences of the same header:\n# \"A sender MUST NOT generate multiple header fields with the same field name\n# in a message unless either the entire field value for that header field is\n# defined as a comma-separated list [or the header is Set-Cookie which gets a\n# special exception]\" - RFC 7230. (cookies are in RFC 6265)\n#\n# So every header aside from Set-Cookie can be merged by b\", \".join if it\n# occurs repeatedly. But, of course, they can't necessarily be split by\n# .split(b\",\"), because quoting.\n#\n# Given all this mess (case insensitive, duplicates allowed, order is\n# important, ...), there doesn't appear to be any standard way to handle\n# headers in Python -- they're almost like dicts, but... actually just\n# aren't. For now we punt and just use a super simple representation: headers\n# are a list of pairs\n#\n#   [(name1, value1), (name2, value2), ...]\n#\n# where all entries are bytestrings, names are lowercase and have no\n# leading/trailing whitespace, and values are bytestrings with no\n# leading/trailing whitespace. Searching and updating are done via naive O(n)\n# methods.\n#\n# Maybe a dict-of-lists would be better?\n\n_content_length_re = re.compile(rb\"[0-9]+\")\n_field_name_re = re.compile(field_name.encode(\"ascii\"))\n_field_value_re = re.compile(field_value.encode(\"ascii\"))\n\n\nclass Headers(Sequence[Tuple[bytes, bytes]]):\n    \"\"\"\n    A list-like interface that allows iterating over headers as byte-pairs\n    of (lowercased-name, value).\n\n    Internally we actually store the representation as three-tuples,\n    including both the raw original casing, in order to preserve casing\n    over-the-wire, and the lowercased name, for case-insensitive comparisions.\n\n    r = Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"Host\", \"example.org\"), (\"Connection\", \"keep-alive\")],\n        http_version=\"1.1\",\n    )\n    assert r.headers == [\n        (b\"host\", b\"example.org\"),\n        (b\"connection\", b\"keep-alive\")\n    ]\n    assert r.headers.raw_items() == [\n        (b\"Host\", b\"example.org\"),\n        (b\"Connection\", b\"keep-alive\")\n    ]\n    \"\"\"\n\n    __slots__ = \"_full_items\"\n\n    def __init__(self, full_items: List[Tuple[bytes, bytes, bytes]]) -> None:\n        self._full_items = full_items\n\n    def __bool__(self) -> bool:\n        return bool(self._full_items)\n\n    def __eq__(self, other: object) -> bool:\n        return list(self) == list(other)  # type: ignore\n\n    def __len__(self) -> int:\n        return len(self._full_items)\n\n    def __repr__(self) -> str:\n        return \"<Headers(%s)>\" % repr(list(self))\n\n    def __getitem__(self, idx: int) -> Tuple[bytes, bytes]:  # type: ignore[override]\n        _, name, value = self._full_items[idx]\n        return (name, value)\n\n    def raw_items(self) -> List[Tuple[bytes, bytes]]:\n        return [(raw_name, value) for raw_name, _, value in self._full_items]\n\n\nHeaderTypes = Union[\n    List[Tuple[bytes, bytes]],\n    List[Tuple[bytes, str]],\n    List[Tuple[str, bytes]],\n    List[Tuple[str, str]],\n]\n\n\n@overload\ndef normalize_and_validate(headers: Headers, _parsed: Literal[True]) -> Headers:\n    ...\n\n\n@overload\ndef normalize_and_validate(headers: HeaderTypes, _parsed: Literal[False]) -> Headers:\n    ...\n\n\n@overload\ndef normalize_and_validate(\n    headers: Union[Headers, HeaderTypes], _parsed: bool = False\n) -> Headers:\n    ...\n\n\ndef normalize_and_validate(\n    headers: Union[Headers, HeaderTypes], _parsed: bool = False\n) -> Headers:\n    new_headers = []\n    seen_content_length = None\n    saw_transfer_encoding = False\n    for name, value in headers:\n        # For headers coming out of the parser, we can safely skip some steps,\n        # because it always returns bytes and has already run these regexes\n        # over the data:\n        if not _parsed:\n            name = bytesify(name)\n            value = bytesify(value)\n            validate(_field_name_re, name, \"Illegal header name {!r}\", name)\n            validate(_field_value_re, value, \"Illegal header value {!r}\", value)\n        assert isinstance(name, bytes)\n        assert isinstance(value, bytes)\n\n        raw_name = name\n        name = name.lower()\n        if name == b\"content-length\":\n            lengths = {length.strip() for length in value.split(b\",\")}\n            if len(lengths) != 1:\n                raise LocalProtocolError(\"conflicting Content-Length headers\")\n            value = lengths.pop()\n            validate(_content_length_re, value, \"bad Content-Length\")\n            if seen_content_length is None:\n                seen_content_length = value\n                new_headers.append((raw_name, name, value))\n            elif seen_content_length != value:\n                raise LocalProtocolError(\"conflicting Content-Length headers\")\n        elif name == b\"transfer-encoding\":\n            # \"A server that receives a request message with a transfer coding\n            # it does not understand SHOULD respond with 501 (Not\n            # Implemented).\"\n            # https://tools.ietf.org/html/rfc7230#section-3.3.1\n            if saw_transfer_encoding:\n                raise LocalProtocolError(\n                    \"multiple Transfer-Encoding headers\", error_status_hint=501\n                )\n            # \"All transfer-coding names are case-insensitive\"\n            # -- https://tools.ietf.org/html/rfc7230#section-4\n            value = value.lower()\n            if value != b\"chunked\":\n                raise LocalProtocolError(\n                    \"Only Transfer-Encoding: chunked is supported\",\n                    error_status_hint=501,\n                )\n            saw_transfer_encoding = True\n            new_headers.append((raw_name, name, value))\n        else:\n            new_headers.append((raw_name, name, value))\n    return Headers(new_headers)\n\n\ndef get_comma_header(headers: Headers, name: bytes) -> List[bytes]:\n    # Should only be used for headers whose value is a list of\n    # comma-separated, case-insensitive values.\n    #\n    # The header name `name` is expected to be lower-case bytes.\n    #\n    # Connection: meets these criteria (including cast insensitivity).\n    #\n    # Content-Length: technically is just a single value (1*DIGIT), but the\n    # standard makes reference to implementations that do multiple values, and\n    # using this doesn't hurt. Ditto, case insensitivity doesn't things either\n    # way.\n    #\n    # Transfer-Encoding: is more complex (allows for quoted strings), so\n    # splitting on , is actually wrong. For example, this is legal:\n    #\n    #    Transfer-Encoding: foo; options=\"1,2\", chunked\n    #\n    # and should be parsed as\n    #\n    #    foo; options=\"1,2\"\n    #    chunked\n    #\n    # but this naive function will parse it as\n    #\n    #    foo; options=\"1\n    #    2\"\n    #    chunked\n    #\n    # However, this is okay because the only thing we are going to do with\n    # any Transfer-Encoding is reject ones that aren't just \"chunked\", so\n    # both of these will be treated the same anyway.\n    #\n    # Expect: the only legal value is the literal string\n    # \"100-continue\". Splitting on commas is harmless. Case insensitive.\n    #\n    out: List[bytes] = []\n    for _, found_name, found_raw_value in headers._full_items:\n        if found_name == name:\n            found_raw_value = found_raw_value.lower()\n            for found_split_value in found_raw_value.split(b\",\"):\n                found_split_value = found_split_value.strip()\n                if found_split_value:\n                    out.append(found_split_value)\n    return out\n\n\ndef set_comma_header(headers: Headers, name: bytes, new_values: List[bytes]) -> Headers:\n    # The header name `name` is expected to be lower-case bytes.\n    #\n    # Note that when we store the header we use title casing for the header\n    # names, in order to match the conventional HTTP header style.\n    #\n    # Simply calling `.title()` is a blunt approach, but it's correct\n    # here given the cases where we're using `set_comma_header`...\n    #\n    # Connection, Content-Length, Transfer-Encoding.\n    new_headers: List[Tuple[bytes, bytes]] = []\n    for found_raw_name, found_name, found_raw_value in headers._full_items:\n        if found_name != name:\n            new_headers.append((found_raw_name, found_raw_value))\n    for new_value in new_values:\n        new_headers.append((name.title(), new_value))\n    return normalize_and_validate(new_headers)\n\n\ndef has_expect_100_continue(request: \"Request\") -> bool:\n    # https://tools.ietf.org/html/rfc7231#section-5.1.1\n    # \"A server that receives a 100-continue expectation in an HTTP/1.0 request\n    # MUST ignore that expectation.\"\n    if request.http_version < b\"1.1\":\n        return False\n    expect = get_comma_header(request.headers, b\"expect\")\n    return b\"100-continue\" in expect\n", "h11/_util.py": "from typing import Any, Dict, NoReturn, Pattern, Tuple, Type, TypeVar, Union\n\n__all__ = [\n    \"ProtocolError\",\n    \"LocalProtocolError\",\n    \"RemoteProtocolError\",\n    \"validate\",\n    \"bytesify\",\n]\n\n\nclass ProtocolError(Exception):\n    \"\"\"Exception indicating a violation of the HTTP/1.1 protocol.\n\n    This as an abstract base class, with two concrete base classes:\n    :exc:`LocalProtocolError`, which indicates that you tried to do something\n    that HTTP/1.1 says is illegal, and :exc:`RemoteProtocolError`, which\n    indicates that the remote peer tried to do something that HTTP/1.1 says is\n    illegal. See :ref:`error-handling` for details.\n\n    In addition to the normal :exc:`Exception` features, it has one attribute:\n\n    .. attribute:: error_status_hint\n\n       This gives a suggestion as to what status code a server might use if\n       this error occurred as part of a request.\n\n       For a :exc:`RemoteProtocolError`, this is useful as a suggestion for\n       how you might want to respond to a misbehaving peer, if you're\n       implementing a server.\n\n       For a :exc:`LocalProtocolError`, this can be taken as a suggestion for\n       how your peer might have responded to *you* if h11 had allowed you to\n       continue.\n\n       The default is 400 Bad Request, a generic catch-all for protocol\n       violations.\n\n    \"\"\"\n\n    def __init__(self, msg: str, error_status_hint: int = 400) -> None:\n        if type(self) is ProtocolError:\n            raise TypeError(\"tried to directly instantiate ProtocolError\")\n        Exception.__init__(self, msg)\n        self.error_status_hint = error_status_hint\n\n\n# Strategy: there are a number of public APIs where a LocalProtocolError can\n# be raised (send(), all the different event constructors, ...), and only one\n# public API where RemoteProtocolError can be raised\n# (receive_data()). Therefore we always raise LocalProtocolError internally,\n# and then receive_data will translate this into a RemoteProtocolError.\n#\n# Internally:\n#   LocalProtocolError is the generic \"ProtocolError\".\n# Externally:\n#   LocalProtocolError is for local errors and RemoteProtocolError is for\n#   remote errors.\nclass LocalProtocolError(ProtocolError):\n    def _reraise_as_remote_protocol_error(self) -> NoReturn:\n        # After catching a LocalProtocolError, use this method to re-raise it\n        # as a RemoteProtocolError. This method must be called from inside an\n        # except: block.\n        #\n        # An easy way to get an equivalent RemoteProtocolError is just to\n        # modify 'self' in place.\n        self.__class__ = RemoteProtocolError  # type: ignore\n        # But the re-raising is somewhat non-trivial -- you might think that\n        # now that we've modified the in-flight exception object, that just\n        # doing 'raise' to re-raise it would be enough. But it turns out that\n        # this doesn't work, because Python tracks the exception type\n        # (exc_info[0]) separately from the exception object (exc_info[1]),\n        # and we only modified the latter. So we really do need to re-raise\n        # the new type explicitly.\n        # On py3, the traceback is part of the exception object, so our\n        # in-place modification preserved it and we can just re-raise:\n        raise self\n\n\nclass RemoteProtocolError(ProtocolError):\n    pass\n\n\ndef validate(\n    regex: Pattern[bytes], data: bytes, msg: str = \"malformed data\", *format_args: Any\n) -> Dict[str, bytes]:\n    match = regex.fullmatch(data)\n    if not match:\n        if format_args:\n            msg = msg.format(*format_args)\n        raise LocalProtocolError(msg)\n    return match.groupdict()\n\n\n# Sentinel values\n#\n# - Inherit identity-based comparison and hashing from object\n# - Have a nice repr\n# - Have a *bonus property*: type(sentinel) is sentinel\n#\n# The bonus property is useful if you want to take the return value from\n# next_event() and do some sort of dispatch based on type(event).\n\n_T_Sentinel = TypeVar(\"_T_Sentinel\", bound=\"Sentinel\")\n\n\nclass Sentinel(type):\n    def __new__(\n        cls: Type[_T_Sentinel],\n        name: str,\n        bases: Tuple[type, ...],\n        namespace: Dict[str, Any],\n        **kwds: Any\n    ) -> _T_Sentinel:\n        assert bases == (Sentinel,)\n        v = super().__new__(cls, name, bases, namespace, **kwds)\n        v.__class__ = v  # type: ignore\n        return v\n\n    def __repr__(self) -> str:\n        return self.__name__\n\n\n# Used for methods, request targets, HTTP versions, header names, and header\n# values. Accepts ascii-strings, or bytes/bytearray/memoryview/..., and always\n# returns bytes.\ndef bytesify(s: Union[bytes, bytearray, memoryview, int, str]) -> bytes:\n    # Fast-path:\n    if type(s) is bytes:\n        return s\n    if isinstance(s, str):\n        s = s.encode(\"ascii\")\n    if isinstance(s, int):\n        raise TypeError(\"expected bytes-like object, not int\")\n    return bytes(s)\n", "h11/_events.py": "# High level events that make up HTTP/1.1 conversations. Loosely inspired by\n# the corresponding events in hyper-h2:\n#\n#     http://python-hyper.org/h2/en/stable/api.html#events\n#\n# Don't subclass these. Stuff will break.\n\nimport re\nfrom abc import ABC\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Union\n\nfrom ._abnf import method, request_target\nfrom ._headers import Headers, normalize_and_validate\nfrom ._util import bytesify, LocalProtocolError, validate\n\n# Everything in __all__ gets re-exported as part of the h11 public API.\n__all__ = [\n    \"Event\",\n    \"Request\",\n    \"InformationalResponse\",\n    \"Response\",\n    \"Data\",\n    \"EndOfMessage\",\n    \"ConnectionClosed\",\n]\n\nmethod_re = re.compile(method.encode(\"ascii\"))\nrequest_target_re = re.compile(request_target.encode(\"ascii\"))\n\n\nclass Event(ABC):\n    \"\"\"\n    Base class for h11 events.\n    \"\"\"\n\n    __slots__ = ()\n\n\n@dataclass(init=False, frozen=True)\nclass Request(Event):\n    \"\"\"The beginning of an HTTP request.\n\n    Fields:\n\n    .. attribute:: method\n\n       An HTTP method, e.g. ``b\"GET\"`` or ``b\"POST\"``. Always a byte\n       string. :term:`Bytes-like objects <bytes-like object>` and native\n       strings containing only ascii characters will be automatically\n       converted to byte strings.\n\n    .. attribute:: target\n\n       The target of an HTTP request, e.g. ``b\"/index.html\"``, or one of the\n       more exotic formats described in `RFC 7320, section 5.3\n       <https://tools.ietf.org/html/rfc7230#section-5.3>`_. Always a byte\n       string. :term:`Bytes-like objects <bytes-like object>` and native\n       strings containing only ascii characters will be automatically\n       converted to byte strings.\n\n    .. attribute:: headers\n\n       Request headers, represented as a list of (name, value) pairs. See\n       :ref:`the header normalization rules <headers-format>` for details.\n\n    .. attribute:: http_version\n\n       The HTTP protocol version, represented as a byte string like\n       ``b\"1.1\"``. See :ref:`the HTTP version normalization rules\n       <http_version-format>` for details.\n\n    \"\"\"\n\n    __slots__ = (\"method\", \"headers\", \"target\", \"http_version\")\n\n    method: bytes\n    headers: Headers\n    target: bytes\n    http_version: bytes\n\n    def __init__(\n        self,\n        *,\n        method: Union[bytes, str],\n        headers: Union[Headers, List[Tuple[bytes, bytes]], List[Tuple[str, str]]],\n        target: Union[bytes, str],\n        http_version: Union[bytes, str] = b\"1.1\",\n        _parsed: bool = False,\n    ) -> None:\n        super().__init__()\n        if isinstance(headers, Headers):\n            object.__setattr__(self, \"headers\", headers)\n        else:\n            object.__setattr__(\n                self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\n            )\n        if not _parsed:\n            object.__setattr__(self, \"method\", bytesify(method))\n            object.__setattr__(self, \"target\", bytesify(target))\n            object.__setattr__(self, \"http_version\", bytesify(http_version))\n        else:\n            object.__setattr__(self, \"method\", method)\n            object.__setattr__(self, \"target\", target)\n            object.__setattr__(self, \"http_version\", http_version)\n\n        # \"A server MUST respond with a 400 (Bad Request) status code to any\n        # HTTP/1.1 request message that lacks a Host header field and to any\n        # request message that contains more than one Host header field or a\n        # Host header field with an invalid field-value.\"\n        # -- https://tools.ietf.org/html/rfc7230#section-5.4\n        host_count = 0\n        for name, value in self.headers:\n            if name == b\"host\":\n                host_count += 1\n        if self.http_version == b\"1.1\" and host_count == 0:\n            raise LocalProtocolError(\"Missing mandatory Host: header\")\n        if host_count > 1:\n            raise LocalProtocolError(\"Found multiple Host: headers\")\n\n        validate(method_re, self.method, \"Illegal method characters\")\n        validate(request_target_re, self.target, \"Illegal target characters\")\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass _ResponseBase(Event):\n    __slots__ = (\"headers\", \"http_version\", \"reason\", \"status_code\")\n\n    headers: Headers\n    http_version: bytes\n    reason: bytes\n    status_code: int\n\n    def __init__(\n        self,\n        *,\n        headers: Union[Headers, List[Tuple[bytes, bytes]], List[Tuple[str, str]]],\n        status_code: int,\n        http_version: Union[bytes, str] = b\"1.1\",\n        reason: Union[bytes, str] = b\"\",\n        _parsed: bool = False,\n    ) -> None:\n        super().__init__()\n        if isinstance(headers, Headers):\n            object.__setattr__(self, \"headers\", headers)\n        else:\n            object.__setattr__(\n                self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\n            )\n        if not _parsed:\n            object.__setattr__(self, \"reason\", bytesify(reason))\n            object.__setattr__(self, \"http_version\", bytesify(http_version))\n            if not isinstance(status_code, int):\n                raise LocalProtocolError(\"status code must be integer\")\n            # Because IntEnum objects are instances of int, but aren't\n            # duck-compatible (sigh), see gh-72.\n            object.__setattr__(self, \"status_code\", int(status_code))\n        else:\n            object.__setattr__(self, \"reason\", reason)\n            object.__setattr__(self, \"http_version\", http_version)\n            object.__setattr__(self, \"status_code\", status_code)\n\n        self.__post_init__()\n\n    def __post_init__(self) -> None:\n        pass\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass InformationalResponse(_ResponseBase):\n    \"\"\"An HTTP informational response.\n\n    Fields:\n\n    .. attribute:: status_code\n\n       The status code of this response, as an integer. For an\n       :class:`InformationalResponse`, this is always in the range [100,\n       200).\n\n    .. attribute:: headers\n\n       Request headers, represented as a list of (name, value) pairs. See\n       :ref:`the header normalization rules <headers-format>` for\n       details.\n\n    .. attribute:: http_version\n\n       The HTTP protocol version, represented as a byte string like\n       ``b\"1.1\"``. See :ref:`the HTTP version normalization rules\n       <http_version-format>` for details.\n\n    .. attribute:: reason\n\n       The reason phrase of this response, as a byte string. For example:\n       ``b\"OK\"``, or ``b\"Not Found\"``.\n\n    \"\"\"\n\n    def __post_init__(self) -> None:\n        if not (100 <= self.status_code < 200):\n            raise LocalProtocolError(\n                \"InformationalResponse status_code should be in range \"\n                \"[100, 200), not {}\".format(self.status_code)\n            )\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass Response(_ResponseBase):\n    \"\"\"The beginning of an HTTP response.\n\n    Fields:\n\n    .. attribute:: status_code\n\n       The status code of this response, as an integer. For an\n       :class:`Response`, this is always in the range [200,\n       1000).\n\n    .. attribute:: headers\n\n       Request headers, represented as a list of (name, value) pairs. See\n       :ref:`the header normalization rules <headers-format>` for details.\n\n    .. attribute:: http_version\n\n       The HTTP protocol version, represented as a byte string like\n       ``b\"1.1\"``. See :ref:`the HTTP version normalization rules\n       <http_version-format>` for details.\n\n    .. attribute:: reason\n\n       The reason phrase of this response, as a byte string. For example:\n       ``b\"OK\"``, or ``b\"Not Found\"``.\n\n    \"\"\"\n\n    def __post_init__(self) -> None:\n        if not (200 <= self.status_code < 1000):\n            raise LocalProtocolError(\n                \"Response status_code should be in range [200, 1000), not {}\".format(\n                    self.status_code\n                )\n            )\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass Data(Event):\n    \"\"\"Part of an HTTP message body.\n\n    Fields:\n\n    .. attribute:: data\n\n       A :term:`bytes-like object` containing part of a message body. Or, if\n       using the ``combine=False`` argument to :meth:`Connection.send`, then\n       any object that your socket writing code knows what to do with, and for\n       which calling :func:`len` returns the number of bytes that will be\n       written -- see :ref:`sendfile` for details.\n\n    .. attribute:: chunk_start\n\n       A marker that indicates whether this data object is from the start of a\n       chunked transfer encoding chunk. This field is ignored when when a Data\n       event is provided to :meth:`Connection.send`: it is only valid on\n       events emitted from :meth:`Connection.next_event`. You probably\n       shouldn't use this attribute at all; see\n       :ref:`chunk-delimiters-are-bad` for details.\n\n    .. attribute:: chunk_end\n\n       A marker that indicates whether this data object is the last for a\n       given chunked transfer encoding chunk. This field is ignored when when\n       a Data event is provided to :meth:`Connection.send`: it is only valid\n       on events emitted from :meth:`Connection.next_event`. You probably\n       shouldn't use this attribute at all; see\n       :ref:`chunk-delimiters-are-bad` for details.\n\n    \"\"\"\n\n    __slots__ = (\"data\", \"chunk_start\", \"chunk_end\")\n\n    data: bytes\n    chunk_start: bool\n    chunk_end: bool\n\n    def __init__(\n        self, data: bytes, chunk_start: bool = False, chunk_end: bool = False\n    ) -> None:\n        object.__setattr__(self, \"data\", data)\n        object.__setattr__(self, \"chunk_start\", chunk_start)\n        object.__setattr__(self, \"chunk_end\", chunk_end)\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n# XX FIXME: \"A recipient MUST ignore (or consider as an error) any fields that\n# are forbidden to be sent in a trailer, since processing them as if they were\n# present in the header section might bypass external security filters.\"\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#chunked.trailer.part\n# Unfortunately, the list of forbidden fields is long and vague :-/\n@dataclass(init=False, frozen=True)\nclass EndOfMessage(Event):\n    \"\"\"The end of an HTTP message.\n\n    Fields:\n\n    .. attribute:: headers\n\n       Default value: ``[]``\n\n       Any trailing headers attached to this message, represented as a list of\n       (name, value) pairs. See :ref:`the header normalization rules\n       <headers-format>` for details.\n\n       Must be empty unless ``Transfer-Encoding: chunked`` is in use.\n\n    \"\"\"\n\n    __slots__ = (\"headers\",)\n\n    headers: Headers\n\n    def __init__(\n        self,\n        *,\n        headers: Union[\n            Headers, List[Tuple[bytes, bytes]], List[Tuple[str, str]], None\n        ] = None,\n        _parsed: bool = False,\n    ) -> None:\n        super().__init__()\n        if headers is None:\n            headers = Headers([])\n        elif not isinstance(headers, Headers):\n            headers = normalize_and_validate(headers, _parsed=_parsed)\n\n        object.__setattr__(self, \"headers\", headers)\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(frozen=True)\nclass ConnectionClosed(Event):\n    \"\"\"This event indicates that the sender has closed their outgoing\n    connection.\n\n    Note that this does not necessarily mean that they can't *receive* further\n    data, because TCP connections are composed to two one-way channels which\n    can be closed independently. See :ref:`closing` for details.\n\n    No fields.\n    \"\"\"\n\n    pass\n", "h11/__init__.py": "# A highish-level implementation of the HTTP/1.1 wire protocol (RFC 7230),\n# containing no networking code at all, loosely modelled on hyper-h2's generic\n# implementation of HTTP/2 (and in particular the h2.connection.H2Connection\n# class). There's still a bunch of subtle details you need to get right if you\n# want to make this actually useful, because it doesn't implement all the\n# semantics to check that what you're asking to write to the wire is sensible,\n# but at least it gets you out of dealing with the wire itself.\n\nfrom h11._connection import Connection, NEED_DATA, PAUSED\nfrom h11._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom h11._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    ERROR,\n    IDLE,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom h11._util import LocalProtocolError, ProtocolError, RemoteProtocolError\nfrom h11._version import __version__\n\nPRODUCT_ID = \"python-h11/\" + __version__\n\n\n__all__ = (\n    \"Connection\",\n    \"NEED_DATA\",\n    \"PAUSED\",\n    \"ConnectionClosed\",\n    \"Data\",\n    \"EndOfMessage\",\n    \"Event\",\n    \"InformationalResponse\",\n    \"Request\",\n    \"Response\",\n    \"CLIENT\",\n    \"CLOSED\",\n    \"DONE\",\n    \"ERROR\",\n    \"IDLE\",\n    \"MUST_CLOSE\",\n    \"SEND_BODY\",\n    \"SEND_RESPONSE\",\n    \"SERVER\",\n    \"SWITCHED_PROTOCOL\",\n    \"ProtocolError\",\n    \"LocalProtocolError\",\n    \"RemoteProtocolError\",\n)\n", "h11/_writers.py": "# Code to read HTTP data\n#\n# Strategy: each writer takes an event + a write-some-bytes function, which is\n# calls.\n#\n# WRITERS is a dict describing how to pick a reader. It maps states to either:\n# - a writer\n# - or, for body writers, a dict of framin-dependent writer factories\n\nfrom typing import Any, Callable, Dict, List, Tuple, Type, Union\n\nfrom ._events import Data, EndOfMessage, Event, InformationalResponse, Request, Response\nfrom ._headers import Headers\nfrom ._state import CLIENT, IDLE, SEND_BODY, SEND_RESPONSE, SERVER\nfrom ._util import LocalProtocolError, Sentinel\n\n__all__ = [\"WRITERS\"]\n\nWriter = Callable[[bytes], Any]\n\n\ndef write_headers(headers: Headers, write: Writer) -> None:\n    # \"Since the Host field-value is critical information for handling a\n    # request, a user agent SHOULD generate Host as the first header field\n    # following the request-line.\" - RFC 7230\n    raw_items = headers._full_items\n    for raw_name, name, value in raw_items:\n        if name == b\"host\":\n            write(b\"%s: %s\\r\\n\" % (raw_name, value))\n    for raw_name, name, value in raw_items:\n        if name != b\"host\":\n            write(b\"%s: %s\\r\\n\" % (raw_name, value))\n    write(b\"\\r\\n\")\n\n\ndef write_request(request: Request, write: Writer) -> None:\n    if request.http_version != b\"1.1\":\n        raise LocalProtocolError(\"I only send HTTP/1.1\")\n    write(b\"%s %s HTTP/1.1\\r\\n\" % (request.method, request.target))\n    write_headers(request.headers, write)\n\n\n# Shared between InformationalResponse and Response\ndef write_any_response(\n    response: Union[InformationalResponse, Response], write: Writer\n) -> None:\n    if response.http_version != b\"1.1\":\n        raise LocalProtocolError(\"I only send HTTP/1.1\")\n    status_bytes = str(response.status_code).encode(\"ascii\")\n    # We don't bother sending ascii status messages like \"OK\"; they're\n    # optional and ignored by the protocol. (But the space after the numeric\n    # status code is mandatory.)\n    #\n    # XX FIXME: could at least make an effort to pull out the status message\n    # from stdlib's http.HTTPStatus table. Or maybe just steal their enums\n    # (either by import or copy/paste). We already accept them as status codes\n    # since they're of type IntEnum < int.\n    write(b\"HTTP/1.1 %s %s\\r\\n\" % (status_bytes, response.reason))\n    write_headers(response.headers, write)\n\n\nclass BodyWriter:\n    def __call__(self, event: Event, write: Writer) -> None:\n        if type(event) is Data:\n            self.send_data(event.data, write)\n        elif type(event) is EndOfMessage:\n            self.send_eom(event.headers, write)\n        else:  # pragma: no cover\n            assert False\n\n    def send_data(self, data: bytes, write: Writer) -> None:\n        pass\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        pass\n\n\n#\n# These are all careful not to do anything to 'data' except call len(data) and\n# write(data). This allows us to transparently pass-through funny objects,\n# like placeholder objects referring to files on disk that will be sent via\n# sendfile(2).\n#\nclass ContentLengthWriter(BodyWriter):\n    def __init__(self, length: int) -> None:\n        self._length = length\n\n    def send_data(self, data: bytes, write: Writer) -> None:\n        self._length -= len(data)\n        if self._length < 0:\n            raise LocalProtocolError(\"Too much data for declared Content-Length\")\n        write(data)\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        if self._length != 0:\n            raise LocalProtocolError(\"Too little data for declared Content-Length\")\n        if headers:\n            raise LocalProtocolError(\"Content-Length and trailers don't mix\")\n\n\nclass ChunkedWriter(BodyWriter):\n    def send_data(self, data: bytes, write: Writer) -> None:\n        # if we encoded 0-length data in the naive way, it would look like an\n        # end-of-message.\n        if not data:\n            return\n        write(b\"%x\\r\\n\" % len(data))\n        write(data)\n        write(b\"\\r\\n\")\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        write(b\"0\\r\\n\")\n        write_headers(headers, write)\n\n\nclass Http10Writer(BodyWriter):\n    def send_data(self, data: bytes, write: Writer) -> None:\n        write(data)\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        if headers:\n            raise LocalProtocolError(\"can't send trailers to HTTP/1.0 client\")\n        # no need to close the socket ourselves, that will be taken care of by\n        # Connection: close machinery\n\n\nWritersType = Dict[\n    Union[Tuple[Type[Sentinel], Type[Sentinel]], Type[Sentinel]],\n    Union[\n        Dict[str, Type[BodyWriter]],\n        Callable[[Union[InformationalResponse, Response], Writer], None],\n        Callable[[Request, Writer], None],\n    ],\n]\n\nWRITERS: WritersType = {\n    (CLIENT, IDLE): write_request,\n    (SERVER, IDLE): write_any_response,\n    (SERVER, SEND_RESPONSE): write_any_response,\n    SEND_BODY: {\n        \"chunked\": ChunkedWriter,\n        \"content-length\": ContentLengthWriter,\n        \"http/1.0\": Http10Writer,\n    },\n}\n", "h11/_connection.py": "# This contains the main Connection class. Everything in h11 revolves around\n# this.\nfrom typing import (\n    Any,\n    Callable,\n    cast,\n    Dict,\n    List,\n    Optional,\n    overload,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom ._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom ._headers import get_comma_header, has_expect_100_continue, set_comma_header\nfrom ._readers import READERS, ReadersType\nfrom ._receivebuffer import ReceiveBuffer\nfrom ._state import (\n    _SWITCH_CONNECT,\n    _SWITCH_UPGRADE,\n    CLIENT,\n    ConnectionState,\n    DONE,\n    ERROR,\n    MIGHT_SWITCH_PROTOCOL,\n    SEND_BODY,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom ._util import (  # Import the internal things we need\n    LocalProtocolError,\n    RemoteProtocolError,\n    Sentinel,\n)\nfrom ._writers import WRITERS, WritersType\n\n# Everything in __all__ gets re-exported as part of the h11 public API.\n__all__ = [\"Connection\", \"NEED_DATA\", \"PAUSED\"]\n\n\nclass NEED_DATA(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass PAUSED(Sentinel, metaclass=Sentinel):\n    pass\n\n\n# If we ever have this much buffered without it making a complete parseable\n# event, we error out. The only time we really buffer is when reading the\n# request/response line + headers together, so this is effectively the limit on\n# the size of that.\n#\n# Some precedents for defaults:\n# - node.js: 80 * 1024\n# - tomcat: 8 * 1024\n# - IIS: 16 * 1024\n# - Apache: <8 KiB per line>\nDEFAULT_MAX_INCOMPLETE_EVENT_SIZE = 16 * 1024\n\n\n# RFC 7230's rules for connection lifecycles:\n# - If either side says they want to close the connection, then the connection\n#   must close.\n# - HTTP/1.1 defaults to keep-alive unless someone says Connection: close\n# - HTTP/1.0 defaults to close unless both sides say Connection: keep-alive\n#   (and even this is a mess -- e.g. if you're implementing a proxy then\n#   sending Connection: keep-alive is forbidden).\n#\n# We simplify life by simply not supporting keep-alive with HTTP/1.0 peers. So\n# our rule is:\n# - If someone says Connection: close, we will close\n# - If someone uses HTTP/1.0, we will close.\ndef _keep_alive(event: Union[Request, Response]) -> bool:\n    connection = get_comma_header(event.headers, b\"connection\")\n    if b\"close\" in connection:\n        return False\n    if getattr(event, \"http_version\", b\"1.1\") < b\"1.1\":\n        return False\n    return True\n\n\ndef _body_framing(\n    request_method: bytes, event: Union[Request, Response]\n) -> Tuple[str, Union[Tuple[()], Tuple[int]]]:\n    # Called when we enter SEND_BODY to figure out framing information for\n    # this body.\n    #\n    # These are the only two events that can trigger a SEND_BODY state:\n    assert type(event) in (Request, Response)\n    # Returns one of:\n    #\n    #    (\"content-length\", count)\n    #    (\"chunked\", ())\n    #    (\"http/1.0\", ())\n    #\n    # which are (lookup key, *args) for constructing body reader/writer\n    # objects.\n    #\n    # Reference: https://tools.ietf.org/html/rfc7230#section-3.3.3\n    #\n    # Step 1: some responses always have an empty body, regardless of what the\n    # headers say.\n    if type(event) is Response:\n        if (\n            event.status_code in (204, 304)\n            or request_method == b\"HEAD\"\n            or (request_method == b\"CONNECT\" and 200 <= event.status_code < 300)\n        ):\n            return (\"content-length\", (0,))\n        # Section 3.3.3 also lists another case -- responses with status_code\n        # < 200. For us these are InformationalResponses, not Responses, so\n        # they can't get into this function in the first place.\n        assert event.status_code >= 200\n\n    # Step 2: check for Transfer-Encoding (T-E beats C-L):\n    transfer_encodings = get_comma_header(event.headers, b\"transfer-encoding\")\n    if transfer_encodings:\n        assert transfer_encodings == [b\"chunked\"]\n        return (\"chunked\", ())\n\n    # Step 3: check for Content-Length\n    content_lengths = get_comma_header(event.headers, b\"content-length\")\n    if content_lengths:\n        return (\"content-length\", (int(content_lengths[0]),))\n\n    # Step 4: no applicable headers; fallback/default depends on type\n    if type(event) is Request:\n        return (\"content-length\", (0,))\n    else:\n        return (\"http/1.0\", ())\n\n\n################################################################\n#\n# The main Connection class\n#\n################################################################\n\n\nclass Connection:\n    \"\"\"An object encapsulating the state of an HTTP connection.\n\n    Args:\n        our_role: If you're implementing a client, pass :data:`h11.CLIENT`. If\n            you're implementing a server, pass :data:`h11.SERVER`.\n\n        max_incomplete_event_size (int):\n            The maximum number of bytes we're willing to buffer of an\n            incomplete event. In practice this mostly sets a limit on the\n            maximum size of the request/response line + headers. If this is\n            exceeded, then :meth:`next_event` will raise\n            :exc:`RemoteProtocolError`.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        our_role: Type[Sentinel],\n        max_incomplete_event_size: int = DEFAULT_MAX_INCOMPLETE_EVENT_SIZE,\n    ) -> None:\n        self._max_incomplete_event_size = max_incomplete_event_size\n        # State and role tracking\n        if our_role not in (CLIENT, SERVER):\n            raise ValueError(f\"expected CLIENT or SERVER, not {our_role!r}\")\n        self.our_role = our_role\n        self.their_role: Type[Sentinel]\n        if our_role is CLIENT:\n            self.their_role = SERVER\n        else:\n            self.their_role = CLIENT\n        self._cstate = ConnectionState()\n\n        # Callables for converting data->events or vice-versa given the\n        # current state\n        self._writer = self._get_io_object(self.our_role, None, WRITERS)\n        self._reader = self._get_io_object(self.their_role, None, READERS)\n\n        # Holds any unprocessed received data\n        self._receive_buffer = ReceiveBuffer()\n        # If this is true, then it indicates that the incoming connection was\n        # closed *after* the end of whatever's in self._receive_buffer:\n        self._receive_buffer_closed = False\n\n        # Extra bits of state that don't fit into the state machine.\n        #\n        # These two are only used to interpret framing headers for figuring\n        # out how to read/write response bodies. their_http_version is also\n        # made available as a convenient public API.\n        self.their_http_version: Optional[bytes] = None\n        self._request_method: Optional[bytes] = None\n        # This is pure flow-control and doesn't at all affect the set of legal\n        # transitions, so no need to bother ConnectionState with it:\n        self.client_is_waiting_for_100_continue = False\n\n    @property\n    def states(self) -> Dict[Type[Sentinel], Type[Sentinel]]:\n        \"\"\"A dictionary like::\n\n           {CLIENT: <client state>, SERVER: <server state>}\n\n        See :ref:`state-machine` for details.\n\n        \"\"\"\n        return dict(self._cstate.states)\n\n    @property\n    def our_state(self) -> Type[Sentinel]:\n        \"\"\"The current state of whichever role we are playing. See\n        :ref:`state-machine` for details.\n        \"\"\"\n        return self._cstate.states[self.our_role]\n\n    @property\n    def their_state(self) -> Type[Sentinel]:\n        \"\"\"The current state of whichever role we are NOT playing. See\n        :ref:`state-machine` for details.\n        \"\"\"\n        return self._cstate.states[self.their_role]\n\n    @property\n    def they_are_waiting_for_100_continue(self) -> bool:\n        return self.their_role is CLIENT and self.client_is_waiting_for_100_continue\n\n    def start_next_cycle(self) -> None:\n        \"\"\"Attempt to reset our connection state for a new request/response\n        cycle.\n\n        If both client and server are in :data:`DONE` state, then resets them\n        both to :data:`IDLE` state in preparation for a new request/response\n        cycle on this same connection. Otherwise, raises a\n        :exc:`LocalProtocolError`.\n\n        See :ref:`keepalive-and-pipelining`.\n\n        \"\"\"\n        old_states = dict(self._cstate.states)\n        self._cstate.start_next_cycle()\n        self._request_method = None\n        # self.their_http_version gets left alone, since it presumably lasts\n        # beyond a single request/response cycle\n        assert not self.client_is_waiting_for_100_continue\n        self._respond_to_state_changes(old_states)\n\n    def _process_error(self, role: Type[Sentinel]) -> None:\n        old_states = dict(self._cstate.states)\n        self._cstate.process_error(role)\n        self._respond_to_state_changes(old_states)\n\n    def _server_switch_event(self, event: Event) -> Optional[Type[Sentinel]]:\n        if type(event) is InformationalResponse and event.status_code == 101:\n            return _SWITCH_UPGRADE\n        if type(event) is Response:\n            if (\n                _SWITCH_CONNECT in self._cstate.pending_switch_proposals\n                and 200 <= event.status_code < 300\n            ):\n                return _SWITCH_CONNECT\n        return None\n\n    # All events go through here\n    def _process_event(self, role: Type[Sentinel], event: Event) -> None:\n        # First, pass the event through the state machine to make sure it\n        # succeeds.\n        old_states = dict(self._cstate.states)\n        if role is CLIENT and type(event) is Request:\n            if event.method == b\"CONNECT\":\n                self._cstate.process_client_switch_proposal(_SWITCH_CONNECT)\n            if get_comma_header(event.headers, b\"upgrade\"):\n                self._cstate.process_client_switch_proposal(_SWITCH_UPGRADE)\n        server_switch_event = None\n        if role is SERVER:\n            server_switch_event = self._server_switch_event(event)\n        self._cstate.process_event(role, type(event), server_switch_event)\n\n        # Then perform the updates triggered by it.\n\n        if type(event) is Request:\n            self._request_method = event.method\n\n        if role is self.their_role and type(event) in (\n            Request,\n            Response,\n            InformationalResponse,\n        ):\n            event = cast(Union[Request, Response, InformationalResponse], event)\n            self.their_http_version = event.http_version\n\n        # Keep alive handling\n        #\n        # RFC 7230 doesn't really say what one should do if Connection: close\n        # shows up on a 1xx InformationalResponse. I think the idea is that\n        # this is not supposed to happen. In any case, if it does happen, we\n        # ignore it.\n        if type(event) in (Request, Response) and not _keep_alive(\n            cast(Union[Request, Response], event)\n        ):\n            self._cstate.process_keep_alive_disabled()\n\n        # 100-continue\n        if type(event) is Request and has_expect_100_continue(event):\n            self.client_is_waiting_for_100_continue = True\n        if type(event) in (InformationalResponse, Response):\n            self.client_is_waiting_for_100_continue = False\n        if role is CLIENT and type(event) in (Data, EndOfMessage):\n            self.client_is_waiting_for_100_continue = False\n\n        self._respond_to_state_changes(old_states, event)\n\n    def _get_io_object(\n        self,\n        role: Type[Sentinel],\n        event: Optional[Event],\n        io_dict: Union[ReadersType, WritersType],\n    ) -> Optional[Callable[..., Any]]:\n        # event may be None; it's only used when entering SEND_BODY\n        state = self._cstate.states[role]\n        if state is SEND_BODY:\n            # Special case: the io_dict has a dict of reader/writer factories\n            # that depend on the request/response framing.\n            framing_type, args = _body_framing(\n                cast(bytes, self._request_method), cast(Union[Request, Response], event)\n            )\n            return io_dict[SEND_BODY][framing_type](*args)  # type: ignore[index]\n        else:\n            # General case: the io_dict just has the appropriate reader/writer\n            # for this state\n            return io_dict.get((role, state))  # type: ignore[return-value]\n\n    # This must be called after any action that might have caused\n    # self._cstate.states to change.\n    def _respond_to_state_changes(\n        self,\n        old_states: Dict[Type[Sentinel], Type[Sentinel]],\n        event: Optional[Event] = None,\n    ) -> None:\n        # Update reader/writer\n        if self.our_state != old_states[self.our_role]:\n            self._writer = self._get_io_object(self.our_role, event, WRITERS)\n        if self.their_state != old_states[self.their_role]:\n            self._reader = self._get_io_object(self.their_role, event, READERS)\n\n    @property\n    def trailing_data(self) -> Tuple[bytes, bool]:\n        \"\"\"Data that has been received, but not yet processed, represented as\n        a tuple with two elements, where the first is a byte-string containing\n        the unprocessed data itself, and the second is a bool that is True if\n        the receive connection was closed.\n\n        See :ref:`switching-protocols` for discussion of why you'd want this.\n        \"\"\"\n        return (bytes(self._receive_buffer), self._receive_buffer_closed)\n\n    def receive_data(self, data: bytes) -> None:\n        \"\"\"Add data to our internal receive buffer.\n\n        This does not actually do any processing on the data, just stores\n        it. To trigger processing, you have to call :meth:`next_event`.\n\n        Args:\n            data (:term:`bytes-like object`):\n                The new data that was just received.\n\n                Special case: If *data* is an empty byte-string like ``b\"\"``,\n                then this indicates that the remote side has closed the\n                connection (end of file). Normally this is convenient, because\n                standard Python APIs like :meth:`file.read` or\n                :meth:`socket.recv` use ``b\"\"`` to indicate end-of-file, while\n                other failures to read are indicated using other mechanisms\n                like raising :exc:`TimeoutError`. When using such an API you\n                can just blindly pass through whatever you get from ``read``\n                to :meth:`receive_data`, and everything will work.\n\n                But, if you have an API where reading an empty string is a\n                valid non-EOF condition, then you need to be aware of this and\n                make sure to check for such strings and avoid passing them to\n                :meth:`receive_data`.\n\n        Returns:\n            Nothing, but after calling this you should call :meth:`next_event`\n            to parse the newly received data.\n\n        Raises:\n            RuntimeError:\n                Raised if you pass an empty *data*, indicating EOF, and then\n                pass a non-empty *data*, indicating more data that somehow\n                arrived after the EOF.\n\n                (Calling ``receive_data(b\"\")`` multiple times is fine,\n                and equivalent to calling it once.)\n\n        \"\"\"\n        if data:\n            if self._receive_buffer_closed:\n                raise RuntimeError(\"received close, then received more data?\")\n            self._receive_buffer += data\n        else:\n            self._receive_buffer_closed = True\n\n    def _extract_next_receive_event(\n        self,\n    ) -> Union[Event, Type[NEED_DATA], Type[PAUSED]]:\n        state = self.their_state\n        # We don't pause immediately when they enter DONE, because even in\n        # DONE state we can still process a ConnectionClosed() event. But\n        # if we have data in our buffer, then we definitely aren't getting\n        # a ConnectionClosed() immediately and we need to pause.\n        if state is DONE and self._receive_buffer:\n            return PAUSED\n        if state is MIGHT_SWITCH_PROTOCOL or state is SWITCHED_PROTOCOL:\n            return PAUSED\n        assert self._reader is not None\n        event = self._reader(self._receive_buffer)\n        if event is None:\n            if not self._receive_buffer and self._receive_buffer_closed:\n                # In some unusual cases (basically just HTTP/1.0 bodies), EOF\n                # triggers an actual protocol event; in that case, we want to\n                # return that event, and then the state will change and we'll\n                # get called again to generate the actual ConnectionClosed().\n                if hasattr(self._reader, \"read_eof\"):\n                    event = self._reader.read_eof()\n                else:\n                    event = ConnectionClosed()\n        if event is None:\n            event = NEED_DATA\n        return event  # type: ignore[no-any-return]\n\n    def next_event(self) -> Union[Event, Type[NEED_DATA], Type[PAUSED]]:\n        \"\"\"Parse the next event out of our receive buffer, update our internal\n        state, and return it.\n\n        This is a mutating operation -- think of it like calling :func:`next`\n        on an iterator.\n\n        Returns:\n            : One of three things:\n\n            1) An event object -- see :ref:`events`.\n\n            2) The special constant :data:`NEED_DATA`, which indicates that\n               you need to read more data from your socket and pass it to\n               :meth:`receive_data` before this method will be able to return\n               any more events.\n\n            3) The special constant :data:`PAUSED`, which indicates that we\n               are not in a state where we can process incoming data (usually\n               because the peer has finished their part of the current\n               request/response cycle, and you have not yet called\n               :meth:`start_next_cycle`). See :ref:`flow-control` for details.\n\n        Raises:\n            RemoteProtocolError:\n                The peer has misbehaved. You should close the connection\n                (possibly after sending some kind of 4xx response).\n\n        Once this method returns :class:`ConnectionClosed` once, then all\n        subsequent calls will also return :class:`ConnectionClosed`.\n\n        If this method raises any exception besides :exc:`RemoteProtocolError`\n        then that's a bug -- if it happens please file a bug report!\n\n        If this method raises any exception then it also sets\n        :attr:`Connection.their_state` to :data:`ERROR` -- see\n        :ref:`error-handling` for discussion.\n\n        \"\"\"\n\n        if self.their_state is ERROR:\n            raise RemoteProtocolError(\"Can't receive data when peer state is ERROR\")\n        try:\n            event = self._extract_next_receive_event()\n            if event not in [NEED_DATA, PAUSED]:\n                self._process_event(self.their_role, cast(Event, event))\n            if event is NEED_DATA:\n                if len(self._receive_buffer) > self._max_incomplete_event_size:\n                    # 431 is \"Request header fields too large\" which is pretty\n                    # much the only situation where we can get here\n                    raise RemoteProtocolError(\n                        \"Receive buffer too long\", error_status_hint=431\n                    )\n                if self._receive_buffer_closed:\n                    # We're still trying to complete some event, but that's\n                    # never going to happen because no more data is coming\n                    raise RemoteProtocolError(\"peer unexpectedly closed connection\")\n            return event\n        except BaseException as exc:\n            self._process_error(self.their_role)\n            if isinstance(exc, LocalProtocolError):\n                exc._reraise_as_remote_protocol_error()\n            else:\n                raise\n\n    @overload\n    def send(self, event: ConnectionClosed) -> None:\n        ...\n\n    @overload\n    def send(\n        self, event: Union[Request, InformationalResponse, Response, Data, EndOfMessage]\n    ) -> bytes:\n        ...\n\n    @overload\n    def send(self, event: Event) -> Optional[bytes]:\n        ...\n\n    def send(self, event: Event) -> Optional[bytes]:\n        \"\"\"Convert a high-level event into bytes that can be sent to the peer,\n        while updating our internal state machine.\n\n        Args:\n            event: The :ref:`event <events>` to send.\n\n        Returns:\n            If ``type(event) is ConnectionClosed``, then returns\n            ``None``. Otherwise, returns a :term:`bytes-like object`.\n\n        Raises:\n            LocalProtocolError:\n                Sending this event at this time would violate our\n                understanding of the HTTP/1.1 protocol.\n\n        If this method raises any exception then it also sets\n        :attr:`Connection.our_state` to :data:`ERROR` -- see\n        :ref:`error-handling` for discussion.\n\n        \"\"\"\n        data_list = self.send_with_data_passthrough(event)\n        if data_list is None:\n            return None\n        else:\n            return b\"\".join(data_list)\n\n    def send_with_data_passthrough(self, event: Event) -> Optional[List[bytes]]:\n        \"\"\"Identical to :meth:`send`, except that in situations where\n        :meth:`send` returns a single :term:`bytes-like object`, this instead\n        returns a list of them -- and when sending a :class:`Data` event, this\n        list is guaranteed to contain the exact object you passed in as\n        :attr:`Data.data`. See :ref:`sendfile` for discussion.\n\n        \"\"\"\n        if self.our_state is ERROR:\n            raise LocalProtocolError(\"Can't send data when our state is ERROR\")\n        try:\n            if type(event) is Response:\n                event = self._clean_up_response_headers_for_sending(event)\n            # We want to call _process_event before calling the writer,\n            # because if someone tries to do something invalid then this will\n            # give a sensible error message, while our writers all just assume\n            # they will only receive valid events. But, _process_event might\n            # change self._writer. So we have to do a little dance:\n            writer = self._writer\n            self._process_event(self.our_role, event)\n            if type(event) is ConnectionClosed:\n                return None\n            else:\n                # In any situation where writer is None, process_event should\n                # have raised ProtocolError\n                assert writer is not None\n                data_list: List[bytes] = []\n                writer(event, data_list.append)\n                return data_list\n        except:\n            self._process_error(self.our_role)\n            raise\n\n    def send_failed(self) -> None:\n        \"\"\"Notify the state machine that we failed to send the data it gave\n        us.\n\n        This causes :attr:`Connection.our_state` to immediately become\n        :data:`ERROR` -- see :ref:`error-handling` for discussion.\n\n        \"\"\"\n        self._process_error(self.our_role)\n\n    # When sending a Response, we take responsibility for a few things:\n    #\n    # - Sometimes you MUST set Connection: close. We take care of those\n    #   times. (You can also set it yourself if you want, and if you do then\n    #   we'll respect that and close the connection at the right time. But you\n    #   don't have to worry about that unless you want to.)\n    #\n    # - The user has to set Content-Length if they want it. Otherwise, for\n    #   responses that have bodies (e.g. not HEAD), then we will automatically\n    #   select the right mechanism for streaming a body of unknown length,\n    #   which depends on depending on the peer's HTTP version.\n    #\n    # This function's *only* responsibility is making sure headers are set up\n    # right -- everything downstream just looks at the headers. There are no\n    # side channels.\n    def _clean_up_response_headers_for_sending(self, response: Response) -> Response:\n        assert type(response) is Response\n\n        headers = response.headers\n        need_close = False\n\n        # HEAD requests need some special handling: they always act like they\n        # have Content-Length: 0, and that's how _body_framing treats\n        # them. But their headers are supposed to match what we would send if\n        # the request was a GET. (Technically there is one deviation allowed:\n        # we're allowed to leave out the framing headers -- see\n        # https://tools.ietf.org/html/rfc7231#section-4.3.2 . But it's just as\n        # easy to get them right.)\n        method_for_choosing_headers = cast(bytes, self._request_method)\n        if method_for_choosing_headers == b\"HEAD\":\n            method_for_choosing_headers = b\"GET\"\n        framing_type, _ = _body_framing(method_for_choosing_headers, response)\n        if framing_type in (\"chunked\", \"http/1.0\"):\n            # This response has a body of unknown length.\n            # If our peer is HTTP/1.1, we use Transfer-Encoding: chunked\n            # If our peer is HTTP/1.0, we use no framing headers, and close the\n            # connection afterwards.\n            #\n            # Make sure to clear Content-Length (in principle user could have\n            # set both and then we ignored Content-Length b/c\n            # Transfer-Encoding overwrote it -- this would be naughty of them,\n            # but the HTTP spec says that if our peer does this then we have\n            # to fix it instead of erroring out, so we'll accord the user the\n            # same respect).\n            headers = set_comma_header(headers, b\"content-length\", [])\n            if self.their_http_version is None or self.their_http_version < b\"1.1\":\n                # Either we never got a valid request and are sending back an\n                # error (their_http_version is None), so we assume the worst;\n                # or else we did get a valid HTTP/1.0 request, so we know that\n                # they don't understand chunked encoding.\n                headers = set_comma_header(headers, b\"transfer-encoding\", [])\n                # This is actually redundant ATM, since currently we\n                # unconditionally disable keep-alive when talking to HTTP/1.0\n                # peers. But let's be defensive just in case we add\n                # Connection: keep-alive support later:\n                if self._request_method != b\"HEAD\":\n                    need_close = True\n            else:\n                headers = set_comma_header(headers, b\"transfer-encoding\", [b\"chunked\"])\n\n        if not self._cstate.keep_alive or need_close:\n            # Make sure Connection: close is set\n            connection = set(get_comma_header(headers, b\"connection\"))\n            connection.discard(b\"keep-alive\")\n            connection.add(b\"close\")\n            headers = set_comma_header(headers, b\"connection\", sorted(connection))\n\n        return Response(\n            headers=headers,\n            status_code=response.status_code,\n            http_version=response.http_version,\n            reason=response.reason,\n        )\n", "h11/_version.py": "# This file must be kept very simple, because it is consumed from several\n# places -- it is imported by h11/__init__.py, execfile'd by setup.py, etc.\n\n# We use a simple scheme:\n#   1.0.0 -> 1.0.0+dev -> 1.1.0 -> 1.1.0+dev\n# where the +dev versions are never released into the wild, they're just what\n# we stick into the VCS in between releases.\n#\n# This is compatible with PEP 440:\n#   http://legacy.python.org/dev/peps/pep-0440/\n# via the use of the \"local suffix\" \"+dev\", which is disallowed on index\n# servers and causes 1.0.0+dev to sort after plain 1.0.0, which is what we\n# want. (Contrast with the special suffix 1.0.0.dev, which sorts *before*\n# 1.0.0.)\n\n__version__ = \"0.14.0+dev\"\n", "h11/tests/test_headers.py": "import pytest\n\nfrom .._events import Request\nfrom .._headers import (\n    get_comma_header,\n    has_expect_100_continue,\n    Headers,\n    normalize_and_validate,\n    set_comma_header,\n)\nfrom .._util import LocalProtocolError\n\n\ndef test_normalize_and_validate() -> None:\n    assert normalize_and_validate([(\"foo\", \"bar\")]) == [(b\"foo\", b\"bar\")]\n    assert normalize_and_validate([(b\"foo\", b\"bar\")]) == [(b\"foo\", b\"bar\")]\n\n    # no leading/trailing whitespace in names\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo \", \"bar\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\" foo\", \"bar\")])\n\n    # no weird characters in names\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate([(b\"foo bar\", b\"baz\")])\n    assert \"foo bar\" in str(excinfo.value)\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo\\x00bar\", b\"baz\")])\n    # Not even 8-bit characters:\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo\\xffbar\", b\"baz\")])\n    # And not even the control characters we allow in values:\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo\\x01bar\", b\"baz\")])\n\n    # no return or NUL characters in values\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate([(\"foo\", \"bar\\rbaz\")])\n    assert \"bar\\\\rbaz\" in str(excinfo.value)\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"bar\\nbaz\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"bar\\x00baz\")])\n    # no leading/trailing whitespace\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"barbaz  \")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"  barbaz\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"barbaz\\t\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"\\tbarbaz\")])\n\n    # content-length\n    assert normalize_and_validate([(\"Content-Length\", \"1\")]) == [\n        (b\"content-length\", b\"1\")\n    ]\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"asdf\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"1x\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"1\"), (\"Content-Length\", \"2\")])\n    assert normalize_and_validate(\n        [(\"Content-Length\", \"0\"), (\"Content-Length\", \"0\")]\n    ) == [(b\"content-length\", b\"0\")]\n    assert normalize_and_validate([(\"Content-Length\", \"0 , 0\")]) == [\n        (b\"content-length\", b\"0\")\n    ]\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate(\n            [(\"Content-Length\", \"1\"), (\"Content-Length\", \"1\"), (\"Content-Length\", \"2\")]\n        )\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"1 , 1,2\")])\n\n    # transfer-encoding\n    assert normalize_and_validate([(\"Transfer-Encoding\", \"chunked\")]) == [\n        (b\"transfer-encoding\", b\"chunked\")\n    ]\n    assert normalize_and_validate([(\"Transfer-Encoding\", \"cHuNkEd\")]) == [\n        (b\"transfer-encoding\", b\"chunked\")\n    ]\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate([(\"Transfer-Encoding\", \"gzip\")])\n    assert excinfo.value.error_status_hint == 501  # Not Implemented\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate(\n            [(\"Transfer-Encoding\", \"chunked\"), (\"Transfer-Encoding\", \"gzip\")]\n        )\n    assert excinfo.value.error_status_hint == 501  # Not Implemented\n\n\ndef test_get_set_comma_header() -> None:\n    headers = normalize_and_validate(\n        [\n            (\"Connection\", \"close\"),\n            (\"whatever\", \"something\"),\n            (\"connectiON\", \"fOo,, , BAR\"),\n        ]\n    )\n\n    assert get_comma_header(headers, b\"connection\") == [b\"close\", b\"foo\", b\"bar\"]\n\n    headers = set_comma_header(headers, b\"newthing\", [\"a\", \"b\"])  # type: ignore\n\n    with pytest.raises(LocalProtocolError):\n        set_comma_header(headers, b\"newthing\", [\"  a\", \"b\"])  # type: ignore\n\n    assert headers == [\n        (b\"connection\", b\"close\"),\n        (b\"whatever\", b\"something\"),\n        (b\"connection\", b\"fOo,, , BAR\"),\n        (b\"newthing\", b\"a\"),\n        (b\"newthing\", b\"b\"),\n    ]\n\n    headers = set_comma_header(headers, b\"whatever\", [\"different thing\"])  # type: ignore\n\n    assert headers == [\n        (b\"connection\", b\"close\"),\n        (b\"connection\", b\"fOo,, , BAR\"),\n        (b\"newthing\", b\"a\"),\n        (b\"newthing\", b\"b\"),\n        (b\"whatever\", b\"different thing\"),\n    ]\n\n\ndef test_has_100_continue() -> None:\n    assert has_expect_100_continue(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Expect\", \"100-continue\")],\n        )\n    )\n    assert not has_expect_100_continue(\n        Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")])\n    )\n    # Case insensitive\n    assert has_expect_100_continue(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Expect\", \"100-Continue\")],\n        )\n    )\n    # Doesn't work in HTTP/1.0\n    assert not has_expect_100_continue(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Expect\", \"100-continue\")],\n            http_version=\"1.0\",\n        )\n    )\n", "h11/tests/test_io.py": "from typing import Any, Callable, Generator, List\n\nimport pytest\n\nfrom .._events import (\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._headers import Headers, normalize_and_validate\nfrom .._readers import (\n    _obsolete_line_fold,\n    ChunkedReader,\n    ContentLengthReader,\n    Http10Reader,\n    READERS,\n)\nfrom .._receivebuffer import ReceiveBuffer\nfrom .._state import CLIENT, IDLE, SEND_RESPONSE, SERVER\nfrom .._util import LocalProtocolError\nfrom .._writers import (\n    ChunkedWriter,\n    ContentLengthWriter,\n    Http10Writer,\n    write_any_response,\n    write_headers,\n    write_request,\n    WRITERS,\n)\nfrom .helpers import normalize_data_events\n\nSIMPLE_CASES = [\n    (\n        (CLIENT, IDLE),\n        Request(\n            method=\"GET\",\n            target=\"/a\",\n            headers=[(\"Host\", \"foo\"), (\"Connection\", \"close\")],\n        ),\n        b\"GET /a HTTP/1.1\\r\\nHost: foo\\r\\nConnection: close\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        Response(status_code=200, headers=[(\"Connection\", \"close\")], reason=b\"OK\"),\n        b\"HTTP/1.1 200 OK\\r\\nConnection: close\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        Response(status_code=200, headers=[], reason=b\"OK\"),\n        b\"HTTP/1.1 200 OK\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        InformationalResponse(\n            status_code=101, headers=[(\"Upgrade\", \"websocket\")], reason=b\"Upgrade\"\n        ),\n        b\"HTTP/1.1 101 Upgrade\\r\\nUpgrade: websocket\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        InformationalResponse(status_code=101, headers=[], reason=b\"Upgrade\"),\n        b\"HTTP/1.1 101 Upgrade\\r\\n\\r\\n\",\n    ),\n]\n\n\ndef dowrite(writer: Callable[..., None], obj: Any) -> bytes:\n    got_list: List[bytes] = []\n    writer(obj, got_list.append)\n    return b\"\".join(got_list)\n\n\ndef tw(writer: Any, obj: Any, expected: Any) -> None:\n    got = dowrite(writer, obj)\n    assert got == expected\n\n\ndef makebuf(data: bytes) -> ReceiveBuffer:\n    buf = ReceiveBuffer()\n    buf += data\n    return buf\n\n\ndef tr(reader: Any, data: bytes, expected: Any) -> None:\n    def check(got: Any) -> None:\n        assert got == expected\n        # Headers should always be returned as bytes, not e.g. bytearray\n        # https://github.com/python-hyper/wsproto/pull/54#issuecomment-377709478\n        for name, value in getattr(got, \"headers\", []):\n            assert type(name) is bytes\n            assert type(value) is bytes\n\n    # Simple: consume whole thing\n    buf = makebuf(data)\n    check(reader(buf))\n    assert not buf\n\n    # Incrementally growing buffer\n    buf = ReceiveBuffer()\n    for i in range(len(data)):\n        assert reader(buf) is None\n        buf += data[i : i + 1]\n    check(reader(buf))\n\n    # Trailing data\n    buf = makebuf(data)\n    buf += b\"trailing\"\n    check(reader(buf))\n    assert bytes(buf) == b\"trailing\"\n\n\ndef test_writers_simple() -> None:\n    for (role, state), event, binary in SIMPLE_CASES:\n        tw(WRITERS[role, state], event, binary)\n\n\ndef test_readers_simple() -> None:\n    for (role, state), event, binary in SIMPLE_CASES:\n        tr(READERS[role, state], binary, event)\n\n\ndef test_writers_unusual() -> None:\n    # Simple test of the write_headers utility routine\n    tw(\n        write_headers,\n        normalize_and_validate([(\"foo\", \"bar\"), (\"baz\", \"quux\")]),\n        b\"foo: bar\\r\\nbaz: quux\\r\\n\\r\\n\",\n    )\n    tw(write_headers, Headers([]), b\"\\r\\n\")\n\n    # We understand HTTP/1.0, but we don't speak it\n    with pytest.raises(LocalProtocolError):\n        tw(\n            write_request,\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"foo\"), (\"Connection\", \"close\")],\n                http_version=\"1.0\",\n            ),\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tw(\n            write_any_response,\n            Response(\n                status_code=200, headers=[(\"Connection\", \"close\")], http_version=\"1.0\"\n            ),\n            None,\n        )\n\n\ndef test_readers_unusual() -> None:\n    # Reading HTTP/1.0\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.0\\r\\nSome: header\\r\\n\\r\\n\",\n        Request(\n            method=\"HEAD\",\n            target=\"/foo\",\n            headers=[(\"Some\", \"header\")],\n            http_version=\"1.0\",\n        ),\n    )\n\n    # check no-headers, since it's only legal with HTTP/1.0\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.0\\r\\n\\r\\n\",\n        Request(method=\"HEAD\", target=\"/foo\", headers=[], http_version=\"1.0\"),\n    )\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\nSome: header\\r\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"Some\", \"header\")],\n            http_version=\"1.0\",\n            reason=b\"OK\",\n        ),\n    )\n\n    # single-character header values (actually disallowed by the ABNF in RFC\n    # 7230 -- this is a bug in the standard that we originally copied...)\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\n\" b\"Foo: a a a a a \\r\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"Foo\", \"a a a a a\")],\n            http_version=\"1.0\",\n            reason=b\"OK\",\n        ),\n    )\n\n    # Empty headers -- also legal\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\n\" b\"Foo:\\r\\n\\r\\n\",\n        Response(\n            status_code=200, headers=[(\"Foo\", \"\")], http_version=\"1.0\", reason=b\"OK\"\n        ),\n    )\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\n\" b\"Foo: \\t \\t \\r\\n\\r\\n\",\n        Response(\n            status_code=200, headers=[(\"Foo\", \"\")], http_version=\"1.0\", reason=b\"OK\"\n        ),\n    )\n\n    # Tolerate broken servers that leave off the response code\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200\\r\\n\" b\"Foo: bar\\r\\n\\r\\n\",\n        Response(\n            status_code=200, headers=[(\"Foo\", \"bar\")], http_version=\"1.0\", reason=b\"\"\n        ),\n    )\n\n    # Tolerate headers line endings (\\r\\n and \\n)\n    #    \\n\\r\\b between headers and body\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.1 200 OK\\r\\nSomeHeader: val\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"SomeHeader\", \"val\")],\n            http_version=\"1.1\",\n            reason=\"OK\",\n        ),\n    )\n\n    #   delimited only with \\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.1 200 OK\\nSomeHeader1: val1\\nSomeHeader2: val2\\n\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"SomeHeader1\", \"val1\"), (\"SomeHeader2\", \"val2\")],\n            http_version=\"1.1\",\n            reason=\"OK\",\n        ),\n    )\n\n    #   mixed \\r\\n and \\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.1 200 OK\\r\\nSomeHeader1: val1\\nSomeHeader2: val2\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"SomeHeader1\", \"val1\"), (\"SomeHeader2\", \"val2\")],\n            http_version=\"1.1\",\n            reason=\"OK\",\n        ),\n    )\n\n    # obsolete line folding\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"Some: multi-line\\r\\n\"\n        b\" header\\r\\n\"\n        b\"\\tnonsense\\r\\n\"\n        b\"    \\t   \\t\\tI guess\\r\\n\"\n        b\"Connection: close\\r\\n\"\n        b\"More-nonsense: in the\\r\\n\"\n        b\"    last header  \\r\\n\\r\\n\",\n        Request(\n            method=\"HEAD\",\n            target=\"/foo\",\n            headers=[\n                (\"Host\", \"example.com\"),\n                (\"Some\", \"multi-line header nonsense I guess\"),\n                (\"Connection\", \"close\"),\n                (\"More-nonsense\", \"in the last header\"),\n            ],\n        ),\n    )\n\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"  folded: line\\r\\n\\r\\n\",\n            None,\n        )\n\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"foo  : line\\r\\n\\r\\n\",\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"foo\\t: line\\r\\n\\r\\n\",\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"foo\\t: line\\r\\n\\r\\n\",\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tr(READERS[CLIENT, IDLE], b\"HEAD /foo HTTP/1.1\\r\\n\" b\": line\\r\\n\\r\\n\", None)\n\n\ndef test__obsolete_line_fold_bytes() -> None:\n    # _obsolete_line_fold has a defensive cast to bytearray, which is\n    # necessary to protect against O(n^2) behavior in case anyone ever passes\n    # in regular bytestrings... but right now we never pass in regular\n    # bytestrings. so this test just exists to get some coverage on that\n    # defensive cast.\n    assert list(_obsolete_line_fold([b\"aaa\", b\"bbb\", b\"  ccc\", b\"ddd\"])) == [\n        b\"aaa\",\n        bytearray(b\"bbb ccc\"),\n        b\"ddd\",\n    ]\n\n\ndef _run_reader_iter(\n    reader: Any, buf: bytes, do_eof: bool\n) -> Generator[Any, None, None]:\n    while True:\n        event = reader(buf)\n        if event is None:\n            break\n        yield event\n        # body readers have undefined behavior after returning EndOfMessage,\n        # because this changes the state so they don't get called again\n        if type(event) is EndOfMessage:\n            break\n    if do_eof:\n        assert not buf\n        yield reader.read_eof()\n\n\ndef _run_reader(*args: Any) -> List[Event]:\n    events = list(_run_reader_iter(*args))\n    return normalize_data_events(events)\n\n\ndef t_body_reader(thunk: Any, data: bytes, expected: Any, do_eof: bool = False) -> None:\n    # Simple: consume whole thing\n    print(\"Test 1\")\n    buf = makebuf(data)\n    assert _run_reader(thunk(), buf, do_eof) == expected\n\n    # Incrementally growing buffer\n    print(\"Test 2\")\n    reader = thunk()\n    buf = ReceiveBuffer()\n    events = []\n    for i in range(len(data)):\n        events += _run_reader(reader, buf, False)\n        buf += data[i : i + 1]\n    events += _run_reader(reader, buf, do_eof)\n    assert normalize_data_events(events) == expected\n\n    is_complete = any(type(event) is EndOfMessage for event in expected)\n    if is_complete and not do_eof:\n        buf = makebuf(data + b\"trailing\")\n        assert _run_reader(thunk(), buf, False) == expected\n\n\ndef test_ContentLengthReader() -> None:\n    t_body_reader(lambda: ContentLengthReader(0), b\"\", [EndOfMessage()])\n\n    t_body_reader(\n        lambda: ContentLengthReader(10),\n        b\"0123456789\",\n        [Data(data=b\"0123456789\"), EndOfMessage()],\n    )\n\n\ndef test_Http10Reader() -> None:\n    t_body_reader(Http10Reader, b\"\", [EndOfMessage()], do_eof=True)\n    t_body_reader(Http10Reader, b\"asdf\", [Data(data=b\"asdf\")], do_eof=False)\n    t_body_reader(\n        Http10Reader, b\"asdf\", [Data(data=b\"asdf\"), EndOfMessage()], do_eof=True\n    )\n\n\ndef test_ChunkedReader() -> None:\n    t_body_reader(ChunkedReader, b\"0\\r\\n\\r\\n\", [EndOfMessage()])\n\n    t_body_reader(\n        ChunkedReader,\n        b\"0\\r\\nSome: header\\r\\n\\r\\n\",\n        [EndOfMessage(headers=[(\"Some\", \"header\")])],\n    )\n\n    t_body_reader(\n        ChunkedReader,\n        b\"5\\r\\n01234\\r\\n\"\n        + b\"10\\r\\n0123456789abcdef\\r\\n\"\n        + b\"0\\r\\n\"\n        + b\"Some: header\\r\\n\\r\\n\",\n        [\n            Data(data=b\"012340123456789abcdef\"),\n            EndOfMessage(headers=[(\"Some\", \"header\")]),\n        ],\n    )\n\n    t_body_reader(\n        ChunkedReader,\n        b\"5\\r\\n01234\\r\\n\" + b\"10\\r\\n0123456789abcdef\\r\\n\" + b\"0\\r\\n\\r\\n\",\n        [Data(data=b\"012340123456789abcdef\"), EndOfMessage()],\n    )\n\n    # handles upper and lowercase hex\n    t_body_reader(\n        ChunkedReader,\n        b\"aA\\r\\n\" + b\"x\" * 0xAA + b\"\\r\\n\" + b\"0\\r\\n\\r\\n\",\n        [Data(data=b\"x\" * 0xAA), EndOfMessage()],\n    )\n\n    # refuses arbitrarily long chunk integers\n    with pytest.raises(LocalProtocolError):\n        # Technically this is legal HTTP/1.1, but we refuse to process chunk\n        # sizes that don't fit into 20 characters of hex\n        t_body_reader(ChunkedReader, b\"9\" * 100 + b\"\\r\\nxxx\", [Data(data=b\"xxx\")])\n\n    # refuses garbage in the chunk count\n    with pytest.raises(LocalProtocolError):\n        t_body_reader(ChunkedReader, b\"10\\x00\\r\\nxxx\", None)\n\n    # handles (and discards) \"chunk extensions\" omg wtf\n    t_body_reader(\n        ChunkedReader,\n        b\"5; hello=there\\r\\n\"\n        + b\"xxxxx\"\n        + b\"\\r\\n\"\n        + b'0; random=\"junk\"; some=more; canbe=lonnnnngg\\r\\n\\r\\n',\n        [Data(data=b\"xxxxx\"), EndOfMessage()],\n    )\n\n    t_body_reader(\n        ChunkedReader,\n        b\"5   \t \\r\\n01234\\r\\n\" + b\"0\\r\\n\\r\\n\",\n        [Data(data=b\"01234\"), EndOfMessage()],\n    )\n\n\ndef test_ContentLengthWriter() -> None:\n    w = ContentLengthWriter(5)\n    assert dowrite(w, Data(data=b\"123\")) == b\"123\"\n    assert dowrite(w, Data(data=b\"45\")) == b\"45\"\n    assert dowrite(w, EndOfMessage()) == b\"\"\n\n    w = ContentLengthWriter(5)\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, Data(data=b\"123456\"))\n\n    w = ContentLengthWriter(5)\n    dowrite(w, Data(data=b\"123\"))\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, Data(data=b\"456\"))\n\n    w = ContentLengthWriter(5)\n    dowrite(w, Data(data=b\"123\"))\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, EndOfMessage())\n\n    w = ContentLengthWriter(5)\n    dowrite(w, Data(data=b\"123\")) == b\"123\"\n    dowrite(w, Data(data=b\"45\")) == b\"45\"\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, EndOfMessage(headers=[(\"Etag\", \"asdf\")]))\n\n\ndef test_ChunkedWriter() -> None:\n    w = ChunkedWriter()\n    assert dowrite(w, Data(data=b\"aaa\")) == b\"3\\r\\naaa\\r\\n\"\n    assert dowrite(w, Data(data=b\"a\" * 20)) == b\"14\\r\\n\" + b\"a\" * 20 + b\"\\r\\n\"\n\n    assert dowrite(w, Data(data=b\"\")) == b\"\"\n\n    assert dowrite(w, EndOfMessage()) == b\"0\\r\\n\\r\\n\"\n\n    assert (\n        dowrite(w, EndOfMessage(headers=[(\"Etag\", \"asdf\"), (\"a\", \"b\")]))\n        == b\"0\\r\\nEtag: asdf\\r\\na: b\\r\\n\\r\\n\"\n    )\n\n\ndef test_Http10Writer() -> None:\n    w = Http10Writer()\n    assert dowrite(w, Data(data=b\"1234\")) == b\"1234\"\n    assert dowrite(w, EndOfMessage()) == b\"\"\n\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, EndOfMessage(headers=[(\"Etag\", \"asdf\")]))\n\n\ndef test_reject_garbage_after_request_line() -> None:\n    with pytest.raises(LocalProtocolError):\n        tr(READERS[SERVER, SEND_RESPONSE], b\"HTTP/1.0 200 OK\\x00xxxx\\r\\n\\r\\n\", None)\n\n\ndef test_reject_garbage_after_response_line() -> None:\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1 xxxxxx\\r\\n\" b\"Host: a\\r\\n\\r\\n\",\n            None,\n        )\n\n\ndef test_reject_garbage_in_header_line() -> None:\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"Host: foo\\x00bar\\r\\n\\r\\n\",\n            None,\n        )\n\n\ndef test_reject_non_vchar_in_path() -> None:\n    for bad_char in b\"\\x00\\x20\\x7f\\xee\":\n        message = bytearray(b\"HEAD /\")\n        message.append(bad_char)\n        message.extend(b\" HTTP/1.1\\r\\nHost: foobar\\r\\n\\r\\n\")\n        with pytest.raises(LocalProtocolError):\n            tr(READERS[CLIENT, IDLE], message, None)\n\n\n# https://github.com/python-hyper/h11/issues/57\ndef test_allow_some_garbage_in_cookies() -> None:\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.1\\r\\n\"\n        b\"Host: foo\\r\\n\"\n        b\"Set-Cookie: ___utmvafIumyLc=kUd\\x01UpAt; path=/; Max-Age=900\\r\\n\"\n        b\"\\r\\n\",\n        Request(\n            method=\"HEAD\",\n            target=\"/foo\",\n            headers=[\n                (\"Host\", \"foo\"),\n                (\"Set-Cookie\", \"___utmvafIumyLc=kUd\\x01UpAt; path=/; Max-Age=900\"),\n            ],\n        ),\n    )\n\n\ndef test_host_comes_first() -> None:\n    tw(\n        write_headers,\n        normalize_and_validate([(\"foo\", \"bar\"), (\"Host\", \"example.com\")]),\n        b\"Host: example.com\\r\\nfoo: bar\\r\\n\\r\\n\",\n    )\n", "h11/tests/test_util.py": "import re\nimport sys\nimport traceback\nfrom typing import NoReturn\n\nimport pytest\n\nfrom .._util import (\n    bytesify,\n    LocalProtocolError,\n    ProtocolError,\n    RemoteProtocolError,\n    Sentinel,\n    validate,\n)\n\n\ndef test_ProtocolError() -> None:\n    with pytest.raises(TypeError):\n        ProtocolError(\"abstract base class\")\n\n\ndef test_LocalProtocolError() -> None:\n    try:\n        raise LocalProtocolError(\"foo\")\n    except LocalProtocolError as e:\n        assert str(e) == \"foo\"\n        assert e.error_status_hint == 400\n\n    try:\n        raise LocalProtocolError(\"foo\", error_status_hint=418)\n    except LocalProtocolError as e:\n        assert str(e) == \"foo\"\n        assert e.error_status_hint == 418\n\n    def thunk() -> NoReturn:\n        raise LocalProtocolError(\"a\", error_status_hint=420)\n\n    try:\n        try:\n            thunk()\n        except LocalProtocolError as exc1:\n            orig_traceback = \"\".join(traceback.format_tb(sys.exc_info()[2]))\n            exc1._reraise_as_remote_protocol_error()\n    except RemoteProtocolError as exc2:\n        assert type(exc2) is RemoteProtocolError\n        assert exc2.args == (\"a\",)\n        assert exc2.error_status_hint == 420\n        new_traceback = \"\".join(traceback.format_tb(sys.exc_info()[2]))\n        assert new_traceback.endswith(orig_traceback)\n\n\ndef test_validate() -> None:\n    my_re = re.compile(rb\"(?P<group1>[0-9]+)\\.(?P<group2>[0-9]+)\")\n    with pytest.raises(LocalProtocolError):\n        validate(my_re, b\"0.\")\n\n    groups = validate(my_re, b\"0.1\")\n    assert groups == {\"group1\": b\"0\", \"group2\": b\"1\"}\n\n    # successful partial matches are an error - must match whole string\n    with pytest.raises(LocalProtocolError):\n        validate(my_re, b\"0.1xx\")\n    with pytest.raises(LocalProtocolError):\n        validate(my_re, b\"0.1\\n\")\n\n\ndef test_validate_formatting() -> None:\n    my_re = re.compile(rb\"foo\")\n\n    with pytest.raises(LocalProtocolError) as excinfo:\n        validate(my_re, b\"\", \"oops\")\n    assert \"oops\" in str(excinfo.value)\n\n    with pytest.raises(LocalProtocolError) as excinfo:\n        validate(my_re, b\"\", \"oops {}\")\n    assert \"oops {}\" in str(excinfo.value)\n\n    with pytest.raises(LocalProtocolError) as excinfo:\n        validate(my_re, b\"\", \"oops {} xx\", 10)\n    assert \"oops 10 xx\" in str(excinfo.value)\n\n\ndef test_make_sentinel() -> None:\n    class S(Sentinel, metaclass=Sentinel):\n        pass\n\n    assert repr(S) == \"S\"\n    assert S == S\n    assert type(S).__name__ == \"S\"\n    assert S in {S}\n    assert type(S) is S\n\n    class S2(Sentinel, metaclass=Sentinel):\n        pass\n\n    assert repr(S2) == \"S2\"\n    assert S != S2\n    assert S not in {S2}\n    assert type(S) is not type(S2)\n\n\ndef test_bytesify() -> None:\n    assert bytesify(b\"123\") == b\"123\"\n    assert bytesify(bytearray(b\"123\")) == b\"123\"\n    assert bytesify(\"123\") == b\"123\"\n\n    with pytest.raises(UnicodeEncodeError):\n        bytesify(\"\\u1234\")\n\n    with pytest.raises(TypeError):\n        bytesify(10)\n", "h11/tests/test_helpers.py": "from .._events import Data, EndOfMessage, Response\nfrom .helpers import normalize_data_events\n\n\ndef test_normalize_data_events() -> None:\n    assert normalize_data_events(\n        [\n            Data(data=bytearray(b\"1\")),\n            Data(data=b\"2\"),\n            Response(status_code=200, headers=[]),\n            Data(data=b\"3\"),\n            Data(data=b\"4\"),\n            EndOfMessage(),\n            Data(data=b\"5\"),\n            Data(data=b\"6\"),\n            Data(data=b\"7\"),\n        ]\n    ) == [\n        Data(data=b\"12\"),\n        Response(status_code=200, headers=[]),\n        Data(data=b\"34\"),\n        EndOfMessage(),\n        Data(data=b\"567\"),\n    ]\n", "h11/tests/test_state.py": "import pytest\n\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._state import (\n    _SWITCH_CONNECT,\n    _SWITCH_UPGRADE,\n    CLIENT,\n    CLOSED,\n    ConnectionState,\n    DONE,\n    IDLE,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom .._util import LocalProtocolError\n\n\ndef test_ConnectionState() -> None:\n    cs = ConnectionState()\n\n    # Basic event-triggered transitions\n\n    assert cs.states == {CLIENT: IDLE, SERVER: IDLE}\n\n    cs.process_event(CLIENT, Request)\n    # The SERVER-Request special case:\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    # Illegal transitions raise an error and nothing happens\n    with pytest.raises(LocalProtocolError):\n        cs.process_event(CLIENT, Request)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    cs.process_event(SERVER, InformationalResponse)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    cs.process_event(SERVER, Response)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_BODY}\n\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, EndOfMessage)\n    assert cs.states == {CLIENT: DONE, SERVER: DONE}\n\n    # State-triggered transition\n\n    cs.process_event(SERVER, ConnectionClosed)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: CLOSED}\n\n\ndef test_ConnectionState_keep_alive() -> None:\n    # keep_alive = False\n    cs = ConnectionState()\n    cs.process_event(CLIENT, Request)\n    cs.process_keep_alive_disabled()\n    cs.process_event(CLIENT, EndOfMessage)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: SEND_RESPONSE}\n\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: MUST_CLOSE}\n\n\ndef test_ConnectionState_keep_alive_in_DONE() -> None:\n    # Check that if keep_alive is disabled when the CLIENT is already in DONE,\n    # then this is sufficient to immediately trigger the DONE -> MUST_CLOSE\n    # transition\n    cs = ConnectionState()\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    assert cs.states[CLIENT] is DONE\n    cs.process_keep_alive_disabled()\n    assert cs.states[CLIENT] is MUST_CLOSE\n\n\ndef test_ConnectionState_switch_denied() -> None:\n    for switch_type in (_SWITCH_CONNECT, _SWITCH_UPGRADE):\n        for deny_early in (True, False):\n            cs = ConnectionState()\n            cs.process_client_switch_proposal(switch_type)\n            cs.process_event(CLIENT, Request)\n            cs.process_event(CLIENT, Data)\n            assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n            assert switch_type in cs.pending_switch_proposals\n\n            if deny_early:\n                # before client reaches DONE\n                cs.process_event(SERVER, Response)\n                assert not cs.pending_switch_proposals\n\n            cs.process_event(CLIENT, EndOfMessage)\n\n            if deny_early:\n                assert cs.states == {CLIENT: DONE, SERVER: SEND_BODY}\n            else:\n                assert cs.states == {\n                    CLIENT: MIGHT_SWITCH_PROTOCOL,\n                    SERVER: SEND_RESPONSE,\n                }\n\n                cs.process_event(SERVER, InformationalResponse)\n                assert cs.states == {\n                    CLIENT: MIGHT_SWITCH_PROTOCOL,\n                    SERVER: SEND_RESPONSE,\n                }\n\n                cs.process_event(SERVER, Response)\n                assert cs.states == {CLIENT: DONE, SERVER: SEND_BODY}\n                assert not cs.pending_switch_proposals\n\n\n_response_type_for_switch = {\n    _SWITCH_UPGRADE: InformationalResponse,\n    _SWITCH_CONNECT: Response,\n    None: Response,\n}\n\n\ndef test_ConnectionState_protocol_switch_accepted() -> None:\n    for switch_event in [_SWITCH_UPGRADE, _SWITCH_CONNECT]:\n        cs = ConnectionState()\n        cs.process_client_switch_proposal(switch_event)\n        cs.process_event(CLIENT, Request)\n        cs.process_event(CLIENT, Data)\n        assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n        cs.process_event(CLIENT, EndOfMessage)\n        assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n\n        cs.process_event(SERVER, InformationalResponse)\n        assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n\n        cs.process_event(SERVER, _response_type_for_switch[switch_event], switch_event)\n        assert cs.states == {CLIENT: SWITCHED_PROTOCOL, SERVER: SWITCHED_PROTOCOL}\n\n\ndef test_ConnectionState_double_protocol_switch() -> None:\n    # CONNECT + Upgrade is legal! Very silly, but legal. So we support\n    # it. Because sometimes doing the silly thing is easier than not.\n    for server_switch in [None, _SWITCH_UPGRADE, _SWITCH_CONNECT]:\n        cs = ConnectionState()\n        cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n        cs.process_client_switch_proposal(_SWITCH_CONNECT)\n        cs.process_event(CLIENT, Request)\n        cs.process_event(CLIENT, EndOfMessage)\n        assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n        cs.process_event(\n            SERVER, _response_type_for_switch[server_switch], server_switch\n        )\n        if server_switch is None:\n            assert cs.states == {CLIENT: DONE, SERVER: SEND_BODY}\n        else:\n            assert cs.states == {CLIENT: SWITCHED_PROTOCOL, SERVER: SWITCHED_PROTOCOL}\n\n\ndef test_ConnectionState_inconsistent_protocol_switch() -> None:\n    for client_switches, server_switch in [\n        ([], _SWITCH_CONNECT),\n        ([], _SWITCH_UPGRADE),\n        ([_SWITCH_UPGRADE], _SWITCH_CONNECT),\n        ([_SWITCH_CONNECT], _SWITCH_UPGRADE),\n    ]:\n        cs = ConnectionState()\n        for client_switch in client_switches:  # type: ignore[attr-defined]\n            cs.process_client_switch_proposal(client_switch)\n        cs.process_event(CLIENT, Request)\n        with pytest.raises(LocalProtocolError):\n            cs.process_event(SERVER, Response, server_switch)\n\n\ndef test_ConnectionState_keepalive_protocol_switch_interaction() -> None:\n    # keep_alive=False + pending_switch_proposals\n    cs = ConnectionState()\n    cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n    cs.process_event(CLIENT, Request)\n    cs.process_keep_alive_disabled()\n    cs.process_event(CLIENT, Data)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    # the protocol switch \"wins\"\n    cs.process_event(CLIENT, EndOfMessage)\n    assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n\n    # but when the server denies the request, keep_alive comes back into play\n    cs.process_event(SERVER, Response)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: SEND_BODY}\n\n\ndef test_ConnectionState_reuse() -> None:\n    cs = ConnectionState()\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    cs.start_next_cycle()\n    assert cs.states == {CLIENT: IDLE, SERVER: IDLE}\n\n    # No keepalive\n\n    cs.process_event(CLIENT, Request)\n    cs.process_keep_alive_disabled()\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    # One side closed\n\n    cs = ConnectionState()\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(CLIENT, ConnectionClosed)\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    # Succesful protocol switch\n\n    cs = ConnectionState()\n    cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, InformationalResponse, _SWITCH_UPGRADE)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    # Failed protocol switch\n\n    cs = ConnectionState()\n    cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    cs.start_next_cycle()\n    assert cs.states == {CLIENT: IDLE, SERVER: IDLE}\n\n\ndef test_server_request_is_illegal() -> None:\n    # There used to be a bug in how we handled the Request special case that\n    # made this allowed...\n    cs = ConnectionState()\n    with pytest.raises(LocalProtocolError):\n        cs.process_event(SERVER, Request)\n", "h11/tests/helpers.py": "from typing import cast, List, Type, Union, ValuesView\n\nfrom .._connection import Connection, NEED_DATA, PAUSED\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._state import CLIENT, CLOSED, DONE, MUST_CLOSE, SERVER\nfrom .._util import Sentinel\n\ntry:\n    from typing import Literal\nexcept ImportError:\n    from typing_extensions import Literal  # type: ignore\n\n\ndef get_all_events(conn: Connection) -> List[Event]:\n    got_events = []\n    while True:\n        event = conn.next_event()\n        if event in (NEED_DATA, PAUSED):\n            break\n        event = cast(Event, event)\n        got_events.append(event)\n        if type(event) is ConnectionClosed:\n            break\n    return got_events\n\n\ndef receive_and_get(conn: Connection, data: bytes) -> List[Event]:\n    conn.receive_data(data)\n    return get_all_events(conn)\n\n\n# Merges adjacent Data events, converts payloads to bytestrings, and removes\n# chunk boundaries.\ndef normalize_data_events(in_events: List[Event]) -> List[Event]:\n    out_events: List[Event] = []\n    for event in in_events:\n        if type(event) is Data:\n            event = Data(data=bytes(event.data), chunk_start=False, chunk_end=False)\n        if out_events and type(out_events[-1]) is type(event) is Data:\n            out_events[-1] = Data(\n                data=out_events[-1].data + event.data,\n                chunk_start=out_events[-1].chunk_start,\n                chunk_end=out_events[-1].chunk_end,\n            )\n        else:\n            out_events.append(event)\n    return out_events\n\n\n# Given that we want to write tests that push some events through a Connection\n# and check that its state updates appropriately... we might as make a habit\n# of pushing them through two Connections with a fake network link in\n# between.\nclass ConnectionPair:\n    def __init__(self) -> None:\n        self.conn = {CLIENT: Connection(CLIENT), SERVER: Connection(SERVER)}\n        self.other = {CLIENT: SERVER, SERVER: CLIENT}\n\n    @property\n    def conns(self) -> ValuesView[Connection]:\n        return self.conn.values()\n\n    # expect=\"match\" if expect=send_events; expect=[...] to say what expected\n    def send(\n        self,\n        role: Type[Sentinel],\n        send_events: Union[List[Event], Event],\n        expect: Union[List[Event], Event, Literal[\"match\"]] = \"match\",\n    ) -> bytes:\n        if not isinstance(send_events, list):\n            send_events = [send_events]\n        data = b\"\"\n        closed = False\n        for send_event in send_events:\n            new_data = self.conn[role].send(send_event)\n            if new_data is None:\n                closed = True\n            else:\n                data += new_data\n        # send uses b\"\" to mean b\"\", and None to mean closed\n        # receive uses b\"\" to mean closed, and None to mean \"try again\"\n        # so we have to translate between the two conventions\n        if data:\n            self.conn[self.other[role]].receive_data(data)\n        if closed:\n            self.conn[self.other[role]].receive_data(b\"\")\n        got_events = get_all_events(self.conn[self.other[role]])\n        if expect == \"match\":\n            expect = send_events\n        if not isinstance(expect, list):\n            expect = [expect]\n        assert got_events == expect\n        return data\n", "h11/tests/test_events.py": "from http import HTTPStatus\n\nimport pytest\n\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._util import LocalProtocolError\n\n\ndef test_events() -> None:\n    with pytest.raises(LocalProtocolError):\n        # Missing Host:\n        req = Request(\n            method=\"GET\", target=\"/\", headers=[(\"a\", \"b\")], http_version=\"1.1\"\n        )\n    # But this is okay (HTTP/1.0)\n    req = Request(method=\"GET\", target=\"/\", headers=[(\"a\", \"b\")], http_version=\"1.0\")\n    # fields are normalized\n    assert req.method == b\"GET\"\n    assert req.target == b\"/\"\n    assert req.headers == [(b\"a\", b\"b\")]\n    assert req.http_version == b\"1.0\"\n\n    # This is also okay -- has a Host (with weird capitalization, which is ok)\n    req = Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"a\", \"b\"), (\"hOSt\", \"example.com\")],\n        http_version=\"1.1\",\n    )\n    # we normalize header capitalization\n    assert req.headers == [(b\"a\", b\"b\"), (b\"host\", b\"example.com\")]\n\n    # Multiple host is bad too\n    with pytest.raises(LocalProtocolError):\n        req = Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"a\"), (\"Host\", \"a\")],\n            http_version=\"1.1\",\n        )\n    # Even for HTTP/1.0\n    with pytest.raises(LocalProtocolError):\n        req = Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"a\"), (\"Host\", \"a\")],\n            http_version=\"1.0\",\n        )\n\n    # Header values are validated\n    for bad_char in \"\\x00\\r\\n\\f\\v\":\n        with pytest.raises(LocalProtocolError):\n            req = Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"a\"), (\"Foo\", \"asd\" + bad_char)],\n                http_version=\"1.0\",\n            )\n\n    # But for compatibility we allow non-whitespace control characters, even\n    # though they're forbidden by the spec.\n    Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"Host\", \"a\"), (\"Foo\", \"asd\\x01\\x02\\x7f\")],\n        http_version=\"1.0\",\n    )\n\n    # Request target is validated\n    for bad_byte in b\"\\x00\\x20\\x7f\\xee\":\n        target = bytearray(b\"/\")\n        target.append(bad_byte)\n        with pytest.raises(LocalProtocolError):\n            Request(\n                method=\"GET\", target=target, headers=[(\"Host\", \"a\")], http_version=\"1.1\"\n            )\n\n    # Request method is validated\n    with pytest.raises(LocalProtocolError):\n        Request(\n            method=\"GET / HTTP/1.1\",\n            target=target,\n            headers=[(\"Host\", \"a\")],\n            http_version=\"1.1\",\n        )\n\n    ir = InformationalResponse(status_code=100, headers=[(\"Host\", \"a\")])\n    assert ir.status_code == 100\n    assert ir.headers == [(b\"host\", b\"a\")]\n    assert ir.http_version == b\"1.1\"\n\n    with pytest.raises(LocalProtocolError):\n        InformationalResponse(status_code=200, headers=[(\"Host\", \"a\")])\n\n    resp = Response(status_code=204, headers=[], http_version=\"1.0\")\n    assert resp.status_code == 204\n    assert resp.headers == []\n    assert resp.http_version == b\"1.0\"\n\n    with pytest.raises(LocalProtocolError):\n        resp = Response(status_code=100, headers=[], http_version=\"1.0\")\n\n    with pytest.raises(LocalProtocolError):\n        Response(status_code=\"100\", headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n\n    with pytest.raises(LocalProtocolError):\n        InformationalResponse(status_code=b\"100\", headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n\n    d = Data(data=b\"asdf\")\n    assert d.data == b\"asdf\"\n\n    eom = EndOfMessage()\n    assert eom.headers == []\n\n    cc = ConnectionClosed()\n    assert repr(cc) == \"ConnectionClosed()\"\n\n\ndef test_intenum_status_code() -> None:\n    # https://github.com/python-hyper/h11/issues/72\n\n    r = Response(status_code=HTTPStatus.OK, headers=[], http_version=\"1.0\")\n    assert r.status_code == HTTPStatus.OK\n    assert type(r.status_code) is not type(HTTPStatus.OK)\n    assert type(r.status_code) is int\n\n\ndef test_header_casing() -> None:\n    r = Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"Host\", \"example.org\"), (\"Connection\", \"keep-alive\")],\n        http_version=\"1.1\",\n    )\n    assert len(r.headers) == 2\n    assert r.headers[0] == (b\"host\", b\"example.org\")\n    assert r.headers == [(b\"host\", b\"example.org\"), (b\"connection\", b\"keep-alive\")]\n    assert r.headers.raw_items() == [\n        (b\"Host\", b\"example.org\"),\n        (b\"Connection\", b\"keep-alive\"),\n    ]\n", "h11/tests/test_against_stdlib_http.py": "import json\nimport os.path\nimport socket\nimport socketserver\nimport threading\nfrom contextlib import closing, contextmanager\nfrom http.server import SimpleHTTPRequestHandler\nfrom typing import Callable, Generator\nfrom urllib.request import urlopen\n\nimport h11\n\n\n@contextmanager\ndef socket_server(\n    handler: Callable[..., socketserver.BaseRequestHandler],\n) -> Generator[socketserver.TCPServer, None, None]:\n    httpd = socketserver.TCPServer((\"127.0.0.1\", 0), handler)\n    thread = threading.Thread(\n        target=httpd.serve_forever, kwargs={\"poll_interval\": 0.01}\n    )\n    thread.daemon = True\n    try:\n        thread.start()\n        yield httpd\n    finally:\n        httpd.shutdown()\n\n\ntest_file_path = os.path.join(os.path.dirname(__file__), \"data/test-file\")\nwith open(test_file_path, \"rb\") as f:\n    test_file_data = f.read()\n\n\nclass SingleMindedRequestHandler(SimpleHTTPRequestHandler):\n    def translate_path(self, path: str) -> str:\n        return test_file_path\n\n\ndef test_h11_as_client() -> None:\n    with socket_server(SingleMindedRequestHandler) as httpd:\n        with closing(socket.create_connection(httpd.server_address)) as s:  # type: ignore[arg-type]\n            c = h11.Connection(h11.CLIENT)\n\n            s.sendall(\n                c.send(\n                    h11.Request(\n                        method=\"GET\", target=\"/foo\", headers=[(\"Host\", \"localhost\")]\n                    )\n                )\n            )\n            s.sendall(c.send(h11.EndOfMessage()))\n\n            data = bytearray()\n            while True:\n                event = c.next_event()\n                print(event)\n                if event is h11.NEED_DATA:\n                    # Use a small read buffer to make things more challenging\n                    # and exercise more paths :-)\n                    c.receive_data(s.recv(10))\n                    continue\n                if type(event) is h11.Response:\n                    assert event.status_code == 200\n                if type(event) is h11.Data:\n                    data += event.data\n                if type(event) is h11.EndOfMessage:\n                    break\n            assert bytes(data) == test_file_data\n\n\nclass H11RequestHandler(socketserver.BaseRequestHandler):\n    def handle(self) -> None:\n        with closing(self.request) as s:\n            c = h11.Connection(h11.SERVER)\n            request = None\n            while True:\n                event = c.next_event()\n                if event is h11.NEED_DATA:\n                    # Use a small read buffer to make things more challenging\n                    # and exercise more paths :-)\n                    c.receive_data(s.recv(10))\n                    continue\n                if type(event) is h11.Request:\n                    request = event\n                if type(event) is h11.EndOfMessage:\n                    break\n            assert request is not None\n            info = json.dumps(\n                {\n                    \"method\": request.method.decode(\"ascii\"),\n                    \"target\": request.target.decode(\"ascii\"),\n                    \"headers\": {\n                        name.decode(\"ascii\"): value.decode(\"ascii\")\n                        for (name, value) in request.headers\n                    },\n                }\n            )\n            s.sendall(c.send(h11.Response(status_code=200, headers=[])))\n            s.sendall(c.send(h11.Data(data=info.encode(\"ascii\"))))\n            s.sendall(c.send(h11.EndOfMessage()))\n\n\ndef test_h11_as_server() -> None:\n    with socket_server(H11RequestHandler) as httpd:\n        host, port = httpd.server_address\n        url = f\"http://{host}:{port}/some-path\"  # type: ignore[str-bytes-safe]\n        with closing(urlopen(url)) as f:\n            assert f.getcode() == 200\n            data = f.read()\n    info = json.loads(data.decode(\"ascii\"))\n    print(info)\n    assert info[\"method\"] == \"GET\"\n    assert info[\"target\"] == \"/some-path\"\n    assert \"urllib\" in info[\"headers\"][\"user-agent\"]\n", "h11/tests/__init__.py": "", "h11/tests/test_connection.py": "from typing import Any, cast, Dict, List, Optional, Tuple, Type\n\nimport pytest\n\nfrom .._connection import _body_framing, _keep_alive, Connection, NEED_DATA, PAUSED\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    ERROR,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom .._util import LocalProtocolError, RemoteProtocolError, Sentinel\nfrom .helpers import ConnectionPair, get_all_events, receive_and_get\n\n\ndef test__keep_alive() -> None:\n    assert _keep_alive(\n        Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"Example.com\")])\n    )\n    assert not _keep_alive(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"Example.com\"), (\"Connection\", \"close\")],\n        )\n    )\n    assert not _keep_alive(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"Example.com\"), (\"Connection\", \"a, b, cLOse, foo\")],\n        )\n    )\n    assert not _keep_alive(\n        Request(method=\"GET\", target=\"/\", headers=[], http_version=\"1.0\")\n    )\n\n    assert _keep_alive(Response(status_code=200, headers=[]))\n    assert not _keep_alive(Response(status_code=200, headers=[(\"Connection\", \"close\")]))\n    assert not _keep_alive(\n        Response(status_code=200, headers=[(\"Connection\", \"a, b, cLOse, foo\")])\n    )\n    assert not _keep_alive(Response(status_code=200, headers=[], http_version=\"1.0\"))\n\n\ndef test__body_framing() -> None:\n    def headers(cl: Optional[int], te: bool) -> List[Tuple[str, str]]:\n        headers = []\n        if cl is not None:\n            headers.append((\"Content-Length\", str(cl)))\n        if te:\n            headers.append((\"Transfer-Encoding\", \"chunked\"))\n        return headers\n\n    def resp(\n        status_code: int = 200, cl: Optional[int] = None, te: bool = False\n    ) -> Response:\n        return Response(status_code=status_code, headers=headers(cl, te))\n\n    def req(cl: Optional[int] = None, te: bool = False) -> Request:\n        h = headers(cl, te)\n        h += [(\"Host\", \"example.com\")]\n        return Request(method=\"GET\", target=\"/\", headers=h)\n\n    # Special cases where the headers are ignored:\n    for kwargs in [{}, {\"cl\": 100}, {\"te\": True}, {\"cl\": 100, \"te\": True}]:\n        kwargs = cast(Dict[str, Any], kwargs)\n        for meth, r in [\n            (b\"HEAD\", resp(**kwargs)),\n            (b\"GET\", resp(status_code=204, **kwargs)),\n            (b\"GET\", resp(status_code=304, **kwargs)),\n        ]:\n            assert _body_framing(meth, r) == (\"content-length\", (0,))\n\n    # Transfer-encoding\n    for kwargs in [{\"te\": True}, {\"cl\": 100, \"te\": True}]:\n        kwargs = cast(Dict[str, Any], kwargs)\n        for meth, r in [(None, req(**kwargs)), (b\"GET\", resp(**kwargs))]:  # type: ignore\n            assert _body_framing(meth, r) == (\"chunked\", ())\n\n    # Content-Length\n    for meth, r in [(None, req(cl=100)), (b\"GET\", resp(cl=100))]:  # type: ignore\n        assert _body_framing(meth, r) == (\"content-length\", (100,))\n\n    # No headers\n    assert _body_framing(None, req()) == (\"content-length\", (0,))  # type: ignore\n    assert _body_framing(b\"GET\", resp()) == (\"http/1.0\", ())\n\n\ndef test_Connection_basics_and_content_length() -> None:\n    with pytest.raises(ValueError):\n        Connection(\"CLIENT\")  # type: ignore\n\n    p = ConnectionPair()\n    assert p.conn[CLIENT].our_role is CLIENT\n    assert p.conn[CLIENT].their_role is SERVER\n    assert p.conn[SERVER].our_role is SERVER\n    assert p.conn[SERVER].their_role is CLIENT\n\n    data = p.send(\n        CLIENT,\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Content-Length\", \"10\")],\n        ),\n    )\n    assert data == (\n        b\"GET / HTTP/1.1\\r\\n\" b\"Host: example.com\\r\\n\" b\"Content-Length: 10\\r\\n\\r\\n\"\n    )\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n    assert p.conn[CLIENT].our_state is SEND_BODY\n    assert p.conn[CLIENT].their_state is SEND_RESPONSE\n    assert p.conn[SERVER].our_state is SEND_RESPONSE\n    assert p.conn[SERVER].their_state is SEND_BODY\n\n    assert p.conn[CLIENT].their_http_version is None\n    assert p.conn[SERVER].their_http_version == b\"1.1\"\n\n    data = p.send(SERVER, InformationalResponse(status_code=100, headers=[]))\n    assert data == b\"HTTP/1.1 100 \\r\\n\\r\\n\"\n\n    data = p.send(SERVER, Response(status_code=200, headers=[(\"Content-Length\", \"11\")]))\n    assert data == b\"HTTP/1.1 200 \\r\\nContent-Length: 11\\r\\n\\r\\n\"\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: SEND_BODY, SERVER: SEND_BODY}\n\n    assert p.conn[CLIENT].their_http_version == b\"1.1\"\n    assert p.conn[SERVER].their_http_version == b\"1.1\"\n\n    data = p.send(CLIENT, Data(data=b\"12345\"))\n    assert data == b\"12345\"\n    data = p.send(\n        CLIENT, Data(data=b\"67890\"), expect=[Data(data=b\"67890\"), EndOfMessage()]\n    )\n    assert data == b\"67890\"\n    data = p.send(CLIENT, EndOfMessage(), expect=[])\n    assert data == b\"\"\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: SEND_BODY}\n\n    data = p.send(SERVER, Data(data=b\"1234567890\"))\n    assert data == b\"1234567890\"\n    data = p.send(SERVER, Data(data=b\"1\"), expect=[Data(data=b\"1\"), EndOfMessage()])\n    assert data == b\"1\"\n    data = p.send(SERVER, EndOfMessage(), expect=[])\n    assert data == b\"\"\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: DONE}\n\n\ndef test_chunked() -> None:\n    p = ConnectionPair()\n\n    p.send(\n        CLIENT,\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Transfer-Encoding\", \"chunked\")],\n        ),\n    )\n    data = p.send(CLIENT, Data(data=b\"1234567890\", chunk_start=True, chunk_end=True))\n    assert data == b\"a\\r\\n1234567890\\r\\n\"\n    data = p.send(CLIENT, Data(data=b\"abcde\", chunk_start=True, chunk_end=True))\n    assert data == b\"5\\r\\nabcde\\r\\n\"\n    data = p.send(CLIENT, Data(data=b\"\"), expect=[])\n    assert data == b\"\"\n    data = p.send(CLIENT, EndOfMessage(headers=[(\"hello\", \"there\")]))\n    assert data == b\"0\\r\\nhello: there\\r\\n\\r\\n\"\n\n    p.send(\n        SERVER, Response(status_code=200, headers=[(\"Transfer-Encoding\", \"chunked\")])\n    )\n    p.send(SERVER, Data(data=b\"54321\", chunk_start=True, chunk_end=True))\n    p.send(SERVER, Data(data=b\"12345\", chunk_start=True, chunk_end=True))\n    p.send(SERVER, EndOfMessage())\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: DONE}\n\n\ndef test_chunk_boundaries() -> None:\n    conn = Connection(our_role=SERVER)\n\n    request = (\n        b\"POST / HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"Transfer-Encoding: chunked\\r\\n\"\n        b\"\\r\\n\"\n    )\n    conn.receive_data(request)\n    assert conn.next_event() == Request(\n        method=\"POST\",\n        target=\"/\",\n        headers=[(\"Host\", \"example.com\"), (\"Transfer-Encoding\", \"chunked\")],\n    )\n    assert conn.next_event() is NEED_DATA\n\n    conn.receive_data(b\"5\\r\\nhello\\r\\n\")\n    assert conn.next_event() == Data(data=b\"hello\", chunk_start=True, chunk_end=True)\n\n    conn.receive_data(b\"5\\r\\nhel\")\n    assert conn.next_event() == Data(data=b\"hel\", chunk_start=True, chunk_end=False)\n\n    conn.receive_data(b\"l\")\n    assert conn.next_event() == Data(data=b\"l\", chunk_start=False, chunk_end=False)\n\n    conn.receive_data(b\"o\\r\\n\")\n    assert conn.next_event() == Data(data=b\"o\", chunk_start=False, chunk_end=True)\n\n    conn.receive_data(b\"5\\r\\nhello\")\n    assert conn.next_event() == Data(data=b\"hello\", chunk_start=True, chunk_end=True)\n\n    conn.receive_data(b\"\\r\\n\")\n    assert conn.next_event() == NEED_DATA\n\n    conn.receive_data(b\"0\\r\\n\\r\\n\")\n    assert conn.next_event() == EndOfMessage()\n\n\ndef test_client_talking_to_http10_server() -> None:\n    c = Connection(CLIENT)\n    c.send(Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")]))\n    c.send(EndOfMessage())\n    assert c.our_state is DONE\n    # No content-length, so Http10 framing for body\n    assert receive_and_get(c, b\"HTTP/1.0 200 OK\\r\\n\\r\\n\") == [\n        Response(status_code=200, headers=[], http_version=\"1.0\", reason=b\"OK\")\n    ]\n    assert c.our_state is MUST_CLOSE\n    assert receive_and_get(c, b\"12345\") == [Data(data=b\"12345\")]\n    assert receive_and_get(c, b\"67890\") == [Data(data=b\"67890\")]\n    assert receive_and_get(c, b\"\") == [EndOfMessage(), ConnectionClosed()]\n    assert c.their_state is CLOSED\n\n\ndef test_server_talking_to_http10_client() -> None:\n    c = Connection(SERVER)\n    # No content-length, so no body\n    # NB: no host header\n    assert receive_and_get(c, b\"GET / HTTP/1.0\\r\\n\\r\\n\") == [\n        Request(method=\"GET\", target=\"/\", headers=[], http_version=\"1.0\"),\n        EndOfMessage(),\n    ]\n    assert c.their_state is MUST_CLOSE\n\n    # We automatically Connection: close back at them\n    assert (\n        c.send(Response(status_code=200, headers=[]))\n        == b\"HTTP/1.1 200 \\r\\nConnection: close\\r\\n\\r\\n\"\n    )\n\n    assert c.send(Data(data=b\"12345\")) == b\"12345\"\n    assert c.send(EndOfMessage()) == b\"\"\n    assert c.our_state is MUST_CLOSE\n\n    # Check that it works if they do send Content-Length\n    c = Connection(SERVER)\n    # NB: no host header\n    assert receive_and_get(c, b\"POST / HTTP/1.0\\r\\nContent-Length: 10\\r\\n\\r\\n1\") == [\n        Request(\n            method=\"POST\",\n            target=\"/\",\n            headers=[(\"Content-Length\", \"10\")],\n            http_version=\"1.0\",\n        ),\n        Data(data=b\"1\"),\n    ]\n    assert receive_and_get(c, b\"234567890\") == [Data(data=b\"234567890\"), EndOfMessage()]\n    assert c.their_state is MUST_CLOSE\n    assert receive_and_get(c, b\"\") == [ConnectionClosed()]\n\n\ndef test_automatic_transfer_encoding_in_response() -> None:\n    # Check that in responses, the user can specify either Transfer-Encoding:\n    # chunked or no framing at all, and in both cases we automatically select\n    # the right option depending on whether the peer speaks HTTP/1.0 or\n    # HTTP/1.1\n    for user_headers in [\n        [(\"Transfer-Encoding\", \"chunked\")],\n        [],\n        # In fact, this even works if Content-Length is set,\n        # because if both are set then Transfer-Encoding wins\n        [(\"Transfer-Encoding\", \"chunked\"), (\"Content-Length\", \"100\")],\n    ]:\n        user_headers = cast(List[Tuple[str, str]], user_headers)\n        p = ConnectionPair()\n        p.send(\n            CLIENT,\n            [\n                Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")]),\n                EndOfMessage(),\n            ],\n        )\n        # When speaking to HTTP/1.1 client, all of the above cases get\n        # normalized to Transfer-Encoding: chunked\n        p.send(\n            SERVER,\n            Response(status_code=200, headers=user_headers),\n            expect=Response(\n                status_code=200, headers=[(\"Transfer-Encoding\", \"chunked\")]\n            ),\n        )\n\n        # When speaking to HTTP/1.0 client, all of the above cases get\n        # normalized to no-framing-headers\n        c = Connection(SERVER)\n        receive_and_get(c, b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n        assert (\n            c.send(Response(status_code=200, headers=user_headers))\n            == b\"HTTP/1.1 200 \\r\\nConnection: close\\r\\n\\r\\n\"\n        )\n        assert c.send(Data(data=b\"12345\")) == b\"12345\"\n\n\ndef test_automagic_connection_close_handling() -> None:\n    p = ConnectionPair()\n    # If the user explicitly sets Connection: close, then we notice and\n    # respect it\n    p.send(\n        CLIENT,\n        [\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"example.com\"), (\"Connection\", \"close\")],\n            ),\n            EndOfMessage(),\n        ],\n    )\n    for conn in p.conns:\n        assert conn.states[CLIENT] is MUST_CLOSE\n    # And if the client sets it, the server automatically echoes it back\n    p.send(\n        SERVER,\n        # no header here...\n        [Response(status_code=204, headers=[]), EndOfMessage()],\n        # ...but oh look, it arrived anyway\n        expect=[\n            Response(status_code=204, headers=[(\"connection\", \"close\")]),\n            EndOfMessage(),\n        ],\n    )\n    for conn in p.conns:\n        assert conn.states == {CLIENT: MUST_CLOSE, SERVER: MUST_CLOSE}\n\n\ndef test_100_continue() -> None:\n    def setup() -> ConnectionPair:\n        p = ConnectionPair()\n        p.send(\n            CLIENT,\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[\n                    (\"Host\", \"example.com\"),\n                    (\"Content-Length\", \"100\"),\n                    (\"Expect\", \"100-continue\"),\n                ],\n            ),\n        )\n        for conn in p.conns:\n            assert conn.client_is_waiting_for_100_continue\n        assert not p.conn[CLIENT].they_are_waiting_for_100_continue\n        assert p.conn[SERVER].they_are_waiting_for_100_continue\n        return p\n\n    # Disabled by 100 Continue\n    p = setup()\n    p.send(SERVER, InformationalResponse(status_code=100, headers=[]))\n    for conn in p.conns:\n        assert not conn.client_is_waiting_for_100_continue\n        assert not conn.they_are_waiting_for_100_continue\n\n    # Disabled by a real response\n    p = setup()\n    p.send(\n        SERVER, Response(status_code=200, headers=[(\"Transfer-Encoding\", \"chunked\")])\n    )\n    for conn in p.conns:\n        assert not conn.client_is_waiting_for_100_continue\n        assert not conn.they_are_waiting_for_100_continue\n\n    # Disabled by the client going ahead and sending stuff anyway\n    p = setup()\n    p.send(CLIENT, Data(data=b\"12345\"))\n    for conn in p.conns:\n        assert not conn.client_is_waiting_for_100_continue\n        assert not conn.they_are_waiting_for_100_continue\n\n\ndef test_max_incomplete_event_size_countermeasure() -> None:\n    # Infinitely long headers are definitely not okay\n    c = Connection(SERVER)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nEndless: \")\n    assert c.next_event() is NEED_DATA\n    with pytest.raises(RemoteProtocolError):\n        while True:\n            c.receive_data(b\"a\" * 1024)\n            c.next_event()\n\n    # Checking that the same header is accepted / rejected depending on the\n    # max_incomplete_event_size setting:\n    c = Connection(SERVER, max_incomplete_event_size=5000)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nBig: \")\n    c.receive_data(b\"a\" * 4000)\n    c.receive_data(b\"\\r\\n\\r\\n\")\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\", target=\"/\", http_version=\"1.0\", headers=[(\"big\", \"a\" * 4000)]\n        ),\n        EndOfMessage(),\n    ]\n\n    c = Connection(SERVER, max_incomplete_event_size=4000)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nBig: \")\n    c.receive_data(b\"a\" * 4000)\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n    # Temporarily exceeding the size limit is fine, as long as its done with\n    # complete events:\n    c = Connection(SERVER, max_incomplete_event_size=5000)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nContent-Length: 10000\")\n    c.receive_data(b\"\\r\\n\\r\\n\" + b\"a\" * 10000)\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            http_version=\"1.0\",\n            headers=[(\"Content-Length\", \"10000\")],\n        ),\n        Data(data=b\"a\" * 10000),\n        EndOfMessage(),\n    ]\n\n    c = Connection(SERVER, max_incomplete_event_size=100)\n    # Two pipelined requests to create a way-too-big receive buffer... but\n    # it's fine because we're not checking\n    c.receive_data(\n        b\"GET /1 HTTP/1.1\\r\\nHost: a\\r\\n\\r\\n\"\n        b\"GET /2 HTTP/1.1\\r\\nHost: b\\r\\n\\r\\n\" + b\"X\" * 1000\n    )\n    assert get_all_events(c) == [\n        Request(method=\"GET\", target=\"/1\", headers=[(\"host\", \"a\")]),\n        EndOfMessage(),\n    ]\n    # Even more data comes in, still no problem\n    c.receive_data(b\"X\" * 1000)\n    # We can respond and reuse to get the second pipelined request\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n    c.start_next_cycle()\n    assert get_all_events(c) == [\n        Request(method=\"GET\", target=\"/2\", headers=[(\"host\", \"b\")]),\n        EndOfMessage(),\n    ]\n    # But once we unpause and try to read the next message, and find that it's\n    # incomplete and the buffer is *still* way too large, then *that's* a\n    # problem:\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n    c.start_next_cycle()\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\ndef test_reuse_simple() -> None:\n    p = ConnectionPair()\n    p.send(\n        CLIENT,\n        [Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\")]), EndOfMessage()],\n    )\n    p.send(\n        SERVER,\n        [\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            EndOfMessage(),\n        ],\n    )\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: DONE}\n        conn.start_next_cycle()\n\n    p.send(\n        CLIENT,\n        [\n            Request(method=\"DELETE\", target=\"/foo\", headers=[(\"Host\", \"a\")]),\n            EndOfMessage(),\n        ],\n    )\n    p.send(\n        SERVER,\n        [\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            EndOfMessage(),\n        ],\n    )\n\n\ndef test_pipelining() -> None:\n    # Client doesn't support pipelining, so we have to do this by hand\n    c = Connection(SERVER)\n    assert c.next_event() is NEED_DATA\n    # 3 requests all bunched up\n    c.receive_data(\n        b\"GET /1 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"12345\"\n        b\"GET /2 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"67890\"\n        b\"GET /3 HTTP/1.1\\r\\nHost: a.com\\r\\n\\r\\n\"\n    )\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/1\",\n            headers=[(\"Host\", \"a.com\"), (\"Content-Length\", \"5\")],\n        ),\n        Data(data=b\"12345\"),\n        EndOfMessage(),\n    ]\n    assert c.their_state is DONE\n    assert c.our_state is SEND_RESPONSE\n\n    assert c.next_event() is PAUSED\n\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n    assert c.their_state is DONE\n    assert c.our_state is DONE\n\n    c.start_next_cycle()\n\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/2\",\n            headers=[(\"Host\", \"a.com\"), (\"Content-Length\", \"5\")],\n        ),\n        Data(data=b\"67890\"),\n        EndOfMessage(),\n    ]\n    assert c.next_event() is PAUSED\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n    c.start_next_cycle()\n\n    assert get_all_events(c) == [\n        Request(method=\"GET\", target=\"/3\", headers=[(\"Host\", \"a.com\")]),\n        EndOfMessage(),\n    ]\n    # Doesn't pause this time, no trailing data\n    assert c.next_event() is NEED_DATA\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n\n    # Arrival of more data triggers pause\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"SADF\")\n    assert c.next_event() is PAUSED\n    assert c.trailing_data == (b\"SADF\", False)\n    # If EOF arrives while paused, we don't see that either:\n    c.receive_data(b\"\")\n    assert c.trailing_data == (b\"SADF\", True)\n    assert c.next_event() is PAUSED\n    c.receive_data(b\"\")\n    assert c.next_event() is PAUSED\n    # Can't call receive_data with non-empty buf after closing it\n    with pytest.raises(RuntimeError):\n        c.receive_data(b\"FDSA\")\n\n\ndef test_protocol_switch() -> None:\n    for req, deny, accept in [\n        (\n            Request(\n                method=\"CONNECT\",\n                target=\"example.com:443\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\")],\n            ),\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n        ),\n        (\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\"), (\"Upgrade\", \"a, b\")],\n            ),\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            InformationalResponse(status_code=101, headers=[(\"Upgrade\", \"a\")]),\n        ),\n        (\n            Request(\n                method=\"CONNECT\",\n                target=\"example.com:443\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\"), (\"Upgrade\", \"a, b\")],\n            ),\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            # Accept CONNECT, not upgrade\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n        ),\n        (\n            Request(\n                method=\"CONNECT\",\n                target=\"example.com:443\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\"), (\"Upgrade\", \"a, b\")],\n            ),\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            # Accept Upgrade, not CONNECT\n            InformationalResponse(status_code=101, headers=[(\"Upgrade\", \"b\")]),\n        ),\n    ]:\n\n        def setup() -> ConnectionPair:\n            p = ConnectionPair()\n            p.send(CLIENT, req)\n            # No switch-related state change stuff yet; the client has to\n            # finish the request before that kicks in\n            for conn in p.conns:\n                assert conn.states[CLIENT] is SEND_BODY\n            p.send(CLIENT, [Data(data=b\"1\"), EndOfMessage()])\n            for conn in p.conns:\n                assert conn.states[CLIENT] is MIGHT_SWITCH_PROTOCOL\n            assert p.conn[SERVER].next_event() is PAUSED\n            return p\n\n        # Test deny case\n        p = setup()\n        p.send(SERVER, deny)\n        for conn in p.conns:\n            assert conn.states == {CLIENT: DONE, SERVER: SEND_BODY}\n        p.send(SERVER, EndOfMessage())\n        # Check that re-use is still allowed after a denial\n        for conn in p.conns:\n            conn.start_next_cycle()\n\n        # Test accept case\n        p = setup()\n        p.send(SERVER, accept)\n        for conn in p.conns:\n            assert conn.states == {CLIENT: SWITCHED_PROTOCOL, SERVER: SWITCHED_PROTOCOL}\n            conn.receive_data(b\"123\")\n            assert conn.next_event() is PAUSED\n            conn.receive_data(b\"456\")\n            assert conn.next_event() is PAUSED\n            assert conn.trailing_data == (b\"123456\", False)\n\n        # Pausing in might-switch, then recovery\n        # (weird artificial case where the trailing data actually is valid\n        # HTTP for some reason, because this makes it easier to test the state\n        # logic)\n        p = setup()\n        sc = p.conn[SERVER]\n        sc.receive_data(b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n        assert sc.next_event() is PAUSED\n        assert sc.trailing_data == (b\"GET / HTTP/1.0\\r\\n\\r\\n\", False)\n        sc.send(deny)\n        assert sc.next_event() is PAUSED\n        sc.send(EndOfMessage())\n        sc.start_next_cycle()\n        assert get_all_events(sc) == [\n            Request(method=\"GET\", target=\"/\", headers=[], http_version=\"1.0\"),\n            EndOfMessage(),\n        ]\n\n        # When we're DONE, have no trailing data, and the connection gets\n        # closed, we report ConnectionClosed(). When we're in might-switch or\n        # switched, we don't.\n        p = setup()\n        sc = p.conn[SERVER]\n        sc.receive_data(b\"\")\n        assert sc.next_event() is PAUSED\n        assert sc.trailing_data == (b\"\", True)\n        p.send(SERVER, accept)\n        assert sc.next_event() is PAUSED\n\n        p = setup()\n        sc = p.conn[SERVER]\n        sc.receive_data(b\"\")\n        assert sc.next_event() is PAUSED\n        sc.send(deny)\n        assert sc.next_event() == ConnectionClosed()\n\n        # You can't send after switching protocols, or while waiting for a\n        # protocol switch\n        p = setup()\n        with pytest.raises(LocalProtocolError):\n            p.conn[CLIENT].send(\n                Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\")])\n            )\n        p = setup()\n        p.send(SERVER, accept)\n        with pytest.raises(LocalProtocolError):\n            p.conn[SERVER].send(Data(data=b\"123\"))\n\n\ndef test_close_simple() -> None:\n    # Just immediately closing a new connection without anything having\n    # happened yet.\n    for who_shot_first, who_shot_second in [(CLIENT, SERVER), (SERVER, CLIENT)]:\n\n        def setup() -> ConnectionPair:\n            p = ConnectionPair()\n            p.send(who_shot_first, ConnectionClosed())\n            for conn in p.conns:\n                assert conn.states == {\n                    who_shot_first: CLOSED,\n                    who_shot_second: MUST_CLOSE,\n                }\n            return p\n\n        # You can keep putting b\"\" into a closed connection, and you keep\n        # getting ConnectionClosed() out:\n        p = setup()\n        assert p.conn[who_shot_second].next_event() == ConnectionClosed()\n        assert p.conn[who_shot_second].next_event() == ConnectionClosed()\n        p.conn[who_shot_second].receive_data(b\"\")\n        assert p.conn[who_shot_second].next_event() == ConnectionClosed()\n        # Second party can close...\n        p = setup()\n        p.send(who_shot_second, ConnectionClosed())\n        for conn in p.conns:\n            assert conn.our_state is CLOSED\n            assert conn.their_state is CLOSED\n        # But trying to receive new data on a closed connection is a\n        # RuntimeError (not ProtocolError, because the problem here isn't\n        # violation of HTTP, it's violation of physics)\n        p = setup()\n        with pytest.raises(RuntimeError):\n            p.conn[who_shot_second].receive_data(b\"123\")\n        # And receiving new data on a MUST_CLOSE connection is a ProtocolError\n        p = setup()\n        p.conn[who_shot_first].receive_data(b\"GET\")\n        with pytest.raises(RemoteProtocolError):\n            p.conn[who_shot_first].next_event()\n\n\ndef test_close_different_states() -> None:\n    req = [\n        Request(method=\"GET\", target=\"/foo\", headers=[(\"Host\", \"a\")]),\n        EndOfMessage(),\n    ]\n    resp = [\n        Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n        EndOfMessage(),\n    ]\n\n    # Client before request\n    p = ConnectionPair()\n    p.send(CLIENT, ConnectionClosed())\n    for conn in p.conns:\n        assert conn.states == {CLIENT: CLOSED, SERVER: MUST_CLOSE}\n\n    # Client after request\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    p.send(CLIENT, ConnectionClosed())\n    for conn in p.conns:\n        assert conn.states == {CLIENT: CLOSED, SERVER: SEND_RESPONSE}\n\n    # Server after request -> not allowed\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    with pytest.raises(LocalProtocolError):\n        p.conn[SERVER].send(ConnectionClosed())\n    p.conn[CLIENT].receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError):\n        p.conn[CLIENT].next_event()\n\n    # Server after response\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    p.send(SERVER, resp)\n    p.send(SERVER, ConnectionClosed())\n    for conn in p.conns:\n        assert conn.states == {CLIENT: MUST_CLOSE, SERVER: CLOSED}\n\n    # Both after closing (ConnectionClosed() is idempotent)\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    p.send(SERVER, resp)\n    p.send(CLIENT, ConnectionClosed())\n    p.send(SERVER, ConnectionClosed())\n    p.send(CLIENT, ConnectionClosed())\n    p.send(SERVER, ConnectionClosed())\n\n    # In the middle of sending -> not allowed\n    p = ConnectionPair()\n    p.send(\n        CLIENT,\n        Request(\n            method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\"), (\"Content-Length\", \"10\")]\n        ),\n    )\n    with pytest.raises(LocalProtocolError):\n        p.conn[CLIENT].send(ConnectionClosed())\n    p.conn[SERVER].receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError):\n        p.conn[SERVER].next_event()\n\n\n# Receive several requests and then client shuts down their side of the\n# connection; we can respond to each\ndef test_pipelined_close() -> None:\n    c = Connection(SERVER)\n    # 2 requests then a close\n    c.receive_data(\n        b\"GET /1 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"12345\"\n        b\"GET /2 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"67890\"\n    )\n    c.receive_data(b\"\")\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/1\",\n            headers=[(\"host\", \"a.com\"), (\"content-length\", \"5\")],\n        ),\n        Data(data=b\"12345\"),\n        EndOfMessage(),\n    ]\n    assert c.states[CLIENT] is DONE\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n    assert c.states[SERVER] is DONE\n    c.start_next_cycle()\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/2\",\n            headers=[(\"host\", \"a.com\"), (\"content-length\", \"5\")],\n        ),\n        Data(data=b\"67890\"),\n        EndOfMessage(),\n        ConnectionClosed(),\n    ]\n    assert c.states == {CLIENT: CLOSED, SERVER: SEND_RESPONSE}\n    c.send(Response(status_code=200, headers=[]))\n    c.send(EndOfMessage())\n    assert c.states == {CLIENT: CLOSED, SERVER: MUST_CLOSE}\n    c.send(ConnectionClosed())\n    assert c.states == {CLIENT: CLOSED, SERVER: CLOSED}\n\n\ndef test_sendfile() -> None:\n    class SendfilePlaceholder:\n        def __len__(self) -> int:\n            return 10\n\n    placeholder = SendfilePlaceholder()\n\n    def setup(\n        header: Tuple[str, str], http_version: str\n    ) -> Tuple[Connection, Optional[List[bytes]]]:\n        c = Connection(SERVER)\n        receive_and_get(\n            c, f\"GET / HTTP/{http_version}\\r\\nHost: a\\r\\n\\r\\n\".encode(\"ascii\")\n        )\n        headers = []\n        if header:\n            headers.append(header)\n        c.send(Response(status_code=200, headers=headers))\n        return c, c.send_with_data_passthrough(Data(data=placeholder))  # type: ignore\n\n    c, data = setup((\"Content-Length\", \"10\"), \"1.1\")\n    assert data == [placeholder]  # type: ignore\n    # Raises an error if the connection object doesn't think we've sent\n    # exactly 10 bytes\n    c.send(EndOfMessage())\n\n    _, data = setup((\"Transfer-Encoding\", \"chunked\"), \"1.1\")\n    assert placeholder in data  # type: ignore\n    data[data.index(placeholder)] = b\"x\" * 10  # type: ignore\n    assert b\"\".join(data) == b\"a\\r\\nxxxxxxxxxx\\r\\n\"  # type: ignore\n\n    c, data = setup(None, \"1.0\")  # type: ignore\n    assert data == [placeholder]  # type: ignore\n    assert c.our_state is SEND_BODY\n\n\ndef test_errors() -> None:\n    # After a receive error, you can't receive\n    for role in [CLIENT, SERVER]:\n        c = Connection(our_role=role)\n        c.receive_data(b\"gibberish\\r\\n\\r\\n\")\n        with pytest.raises(RemoteProtocolError):\n            c.next_event()\n        # Now any attempt to receive continues to raise\n        assert c.their_state is ERROR\n        assert c.our_state is not ERROR\n        print(c._cstate.states)\n        with pytest.raises(RemoteProtocolError):\n            c.next_event()\n        # But we can still yell at the client for sending us gibberish\n        if role is SERVER:\n            assert (\n                c.send(Response(status_code=400, headers=[]))\n                == b\"HTTP/1.1 400 \\r\\nConnection: close\\r\\n\\r\\n\"\n            )\n\n    # After an error sending, you can no longer send\n    # (This is especially important for things like content-length errors,\n    # where there's complex internal state being modified)\n    def conn(role: Type[Sentinel]) -> Connection:\n        c = Connection(our_role=role)\n        if role is SERVER:\n            # Put it into the state where it *could* send a response...\n            receive_and_get(c, b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n            assert c.our_state is SEND_RESPONSE\n        return c\n\n    for role in [CLIENT, SERVER]:\n        if role is CLIENT:\n            # This HTTP/1.0 request won't be detected as bad until after we go\n            # through the state machine and hit the writing code\n            good = Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")])\n            bad = Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"example.com\")],\n                http_version=\"1.0\",\n            )\n        elif role is SERVER:\n            good = Response(status_code=200, headers=[])  # type: ignore[assignment]\n            bad = Response(status_code=200, headers=[], http_version=\"1.0\")  # type: ignore[assignment]\n        # Make sure 'good' actually is good\n        c = conn(role)\n        c.send(good)\n        assert c.our_state is not ERROR\n        # Do that again, but this time sending 'bad' first\n        c = conn(role)\n        with pytest.raises(LocalProtocolError):\n            c.send(bad)\n        assert c.our_state is ERROR\n        assert c.their_state is not ERROR\n        # Now 'good' is not so good\n        with pytest.raises(LocalProtocolError):\n            c.send(good)\n\n        # And check send_failed() too\n        c = conn(role)\n        c.send_failed()\n        assert c.our_state is ERROR\n        assert c.their_state is not ERROR\n        # This is idempotent\n        c.send_failed()\n        assert c.our_state is ERROR\n        assert c.their_state is not ERROR\n\n\ndef test_idle_receive_nothing() -> None:\n    # At one point this incorrectly raised an error\n    for role in [CLIENT, SERVER]:\n        c = Connection(role)\n        assert c.next_event() is NEED_DATA\n\n\ndef test_connection_drop() -> None:\n    c = Connection(SERVER)\n    c.receive_data(b\"GET /\")\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\ndef test_408_request_timeout() -> None:\n    # Should be able to send this spontaneously as a server without seeing\n    # anything from client\n    p = ConnectionPair()\n    p.send(SERVER, Response(status_code=408, headers=[(b\"connection\", b\"close\")]))\n\n\n# This used to raise IndexError\ndef test_empty_request() -> None:\n    c = Connection(SERVER)\n    c.receive_data(b\"\\r\\n\")\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n# This used to raise IndexError\ndef test_empty_response() -> None:\n    c = Connection(CLIENT)\n    c.send(Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\")]))\n    c.receive_data(b\"\\r\\n\")\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n@pytest.mark.parametrize(\n    \"data\",\n    [\n        b\"\\x00\",\n        b\"\\x20\",\n        b\"\\x16\\x03\\x01\\x00\\xa5\",  # Typical start of a TLS Client Hello\n    ],\n)\ndef test_early_detection_of_invalid_request(data: bytes) -> None:\n    c = Connection(SERVER)\n    # Early detection should occur before even receiving a `\\r\\n`\n    c.receive_data(data)\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n@pytest.mark.parametrize(\n    \"data\",\n    [\n        b\"\\x00\",\n        b\"\\x20\",\n        b\"\\x16\\x03\\x03\\x00\\x31\",  # Typical start of a TLS Server Hello\n    ],\n)\ndef test_early_detection_of_invalid_response(data: bytes) -> None:\n    c = Connection(CLIENT)\n    # Early detection should occur before even receiving a `\\r\\n`\n    c.receive_data(data)\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n# This used to give different headers for HEAD and GET.\n# The correct way to handle HEAD is to put whatever headers we *would* have\n# put if it were a GET -- even though we know that for HEAD, those headers\n# will be ignored.\ndef test_HEAD_framing_headers() -> None:\n    def setup(method: bytes, http_version: bytes) -> Connection:\n        c = Connection(SERVER)\n        c.receive_data(\n            method + b\" / HTTP/\" + http_version + b\"\\r\\n\" + b\"Host: example.com\\r\\n\\r\\n\"\n        )\n        assert type(c.next_event()) is Request\n        assert type(c.next_event()) is EndOfMessage\n        return c\n\n    for method in [b\"GET\", b\"HEAD\"]:\n        # No Content-Length, HTTP/1.1 peer, should use chunked\n        c = setup(method, b\"1.1\")\n        assert (\n            c.send(Response(status_code=200, headers=[])) == b\"HTTP/1.1 200 \\r\\n\"\n            b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n        )\n\n        # No Content-Length, HTTP/1.0 peer, frame with connection: close\n        c = setup(method, b\"1.0\")\n        assert (\n            c.send(Response(status_code=200, headers=[])) == b\"HTTP/1.1 200 \\r\\n\"\n            b\"Connection: close\\r\\n\\r\\n\"\n        )\n\n        # Content-Length + Transfer-Encoding, TE wins\n        c = setup(method, b\"1.1\")\n        assert (\n            c.send(\n                Response(\n                    status_code=200,\n                    headers=[\n                        (\"Content-Length\", \"100\"),\n                        (\"Transfer-Encoding\", \"chunked\"),\n                    ],\n                )\n            )\n            == b\"HTTP/1.1 200 \\r\\n\"\n            b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n        )\n\n\ndef test_special_exceptions_for_lost_connection_in_message_body() -> None:\n    c = Connection(SERVER)\n    c.receive_data(\n        b\"POST / HTTP/1.1\\r\\n\" b\"Host: example.com\\r\\n\" b\"Content-Length: 100\\r\\n\\r\\n\"\n    )\n    assert type(c.next_event()) is Request\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"12345\")\n    assert c.next_event() == Data(data=b\"12345\")\n    c.receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError) as excinfo:\n        c.next_event()\n    assert \"received 5 bytes\" in str(excinfo.value)\n    assert \"expected 100\" in str(excinfo.value)\n\n    c = Connection(SERVER)\n    c.receive_data(\n        b\"POST / HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n    )\n    assert type(c.next_event()) is Request\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"8\\r\\n012345\")\n    assert c.next_event().data == b\"012345\"  # type: ignore\n    c.receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError) as excinfo:\n        c.next_event()\n    assert \"incomplete chunked read\" in str(excinfo.value)\n", "h11/tests/test_receivebuffer.py": "import re\nfrom typing import Tuple\n\nimport pytest\n\nfrom .._receivebuffer import ReceiveBuffer\n\n\ndef test_receivebuffer() -> None:\n    b = ReceiveBuffer()\n    assert not b\n    assert len(b) == 0\n    assert bytes(b) == b\"\"\n\n    b += b\"123\"\n    assert b\n    assert len(b) == 3\n    assert bytes(b) == b\"123\"\n\n    assert bytes(b) == b\"123\"\n\n    assert b.maybe_extract_at_most(2) == b\"12\"\n    assert b\n    assert len(b) == 1\n    assert bytes(b) == b\"3\"\n\n    assert bytes(b) == b\"3\"\n\n    assert b.maybe_extract_at_most(10) == b\"3\"\n    assert bytes(b) == b\"\"\n\n    assert b.maybe_extract_at_most(10) is None\n    assert not b\n\n    ################################################################\n    # maybe_extract_until_next\n    ################################################################\n\n    b += b\"123\\n456\\r\\n789\\r\\n\"\n\n    assert b.maybe_extract_next_line() == b\"123\\n456\\r\\n\"\n    assert bytes(b) == b\"789\\r\\n\"\n\n    assert b.maybe_extract_next_line() == b\"789\\r\\n\"\n    assert bytes(b) == b\"\"\n\n    b += b\"12\\r\"\n    assert b.maybe_extract_next_line() is None\n    assert bytes(b) == b\"12\\r\"\n\n    b += b\"345\\n\\r\"\n    assert b.maybe_extract_next_line() is None\n    assert bytes(b) == b\"12\\r345\\n\\r\"\n\n    # here we stopped at the middle of b\"\\r\\n\" delimiter\n\n    b += b\"\\n6789aaa123\\r\\n\"\n    assert b.maybe_extract_next_line() == b\"12\\r345\\n\\r\\n\"\n    assert b.maybe_extract_next_line() == b\"6789aaa123\\r\\n\"\n    assert b.maybe_extract_next_line() is None\n    assert bytes(b) == b\"\"\n\n    ################################################################\n    # maybe_extract_lines\n    ################################################################\n\n    b += b\"123\\r\\na: b\\r\\nfoo:bar\\r\\n\\r\\ntrailing\"\n    lines = b.maybe_extract_lines()\n    assert lines == [b\"123\", b\"a: b\", b\"foo:bar\"]\n    assert bytes(b) == b\"trailing\"\n\n    assert b.maybe_extract_lines() is None\n\n    b += b\"\\r\\n\\r\"\n    assert b.maybe_extract_lines() is None\n\n    assert b.maybe_extract_at_most(100) == b\"trailing\\r\\n\\r\"\n    assert not b\n\n    # Empty body case (as happens at the end of chunked encoding if there are\n    # no trailing headers, e.g.)\n    b += b\"\\r\\ntrailing\"\n    assert b.maybe_extract_lines() == []\n    assert bytes(b) == b\"trailing\"\n\n\n@pytest.mark.parametrize(\n    \"data\",\n    [\n        pytest.param(\n            (\n                b\"HTTP/1.1 200 OK\\r\\n\",\n                b\"Content-type: text/plain\\r\\n\",\n                b\"Connection: close\\r\\n\",\n                b\"\\r\\n\",\n                b\"Some body\",\n            ),\n            id=\"with_crlf_delimiter\",\n        ),\n        pytest.param(\n            (\n                b\"HTTP/1.1 200 OK\\n\",\n                b\"Content-type: text/plain\\n\",\n                b\"Connection: close\\n\",\n                b\"\\n\",\n                b\"Some body\",\n            ),\n            id=\"with_lf_only_delimiter\",\n        ),\n        pytest.param(\n            (\n                b\"HTTP/1.1 200 OK\\n\",\n                b\"Content-type: text/plain\\r\\n\",\n                b\"Connection: close\\n\",\n                b\"\\n\",\n                b\"Some body\",\n            ),\n            id=\"with_mixed_crlf_and_lf\",\n        ),\n    ],\n)\ndef test_receivebuffer_for_invalid_delimiter(data: Tuple[bytes]) -> None:\n    b = ReceiveBuffer()\n\n    for line in data:\n        b += line\n\n    lines = b.maybe_extract_lines()\n\n    assert lines == [\n        b\"HTTP/1.1 200 OK\",\n        b\"Content-type: text/plain\",\n        b\"Connection: close\",\n    ]\n    assert bytes(b) == b\"Some body\"\n", "fuzz/afl-server.py": "# Invariant tested: No matter what random garbage a client throws at us, we\n# either successfully parse it, or else throw a RemoteProtocolError, never any\n# other error.\n\nimport os\nimport sys\n\nimport afl\n\nimport h11\n\n\ndef process_all(c):\n    while True:\n        event = c.next_event()\n        if event is h11.NEED_DATA or event is h11.PAUSED:\n            break\n        if type(event) is h11.ConnectionClosed:\n            break\n\n\nafl.init()\n\ndata = sys.stdin.detach().read()\n\n# one big chunk\nserver1 = h11.Connection(h11.SERVER)\ntry:\n    server1.receive_data(data)\n    process_all(server1)\n    server1.receive_data(b\"\")\n    process_all(server1)\nexcept h11.RemoteProtocolError:\n    pass\n\n# byte at a time\nserver2 = h11.Connection(h11.SERVER)\ntry:\n    for i in range(len(data)):\n        server2.receive_data(data[i : i + 1])\n        process_all(server2)\n    server2.receive_data(b\"\")\n    process_all(server2)\nexcept h11.RemoteProtocolError:\n    pass\n\n# Suggested by the afl-python docs -- this substantially speeds up fuzzing, at\n# the risk of missing bugs that would cause the interpreter to crash on\n# exit. h11 is pure python, so I'm pretty sure h11 doesn't have any bugs that\n# would cause the interpreter to crash on exit.\nos._exit(0)\n", "docs/source/make-state-diagrams.py": "#!python\n\nimport sys\nsys.path.append(\"../..\")\n\nimport os.path\nimport subprocess\n\nfrom h11._events import *\nfrom h11._state import *\nfrom h11._state import (\n    _SWITCH_UPGRADE, _SWITCH_CONNECT,\n    EVENT_TRIGGERED_TRANSITIONS, STATE_TRIGGERED_TRANSITIONS,\n)\n\n_EVENT_COLOR = \"#002092\"\n_STATE_COLOR = \"#017517\"\n_SPECIAL_COLOR = \"#7600a1\"\n\nHEADER = \"\"\"\ndigraph {\n  graph [fontname = \"Lato\" bgcolor=\"transparent\"]\n  node  [fontname = \"Lato\"]\n  edge  [fontname = \"Lato\"]\n\"\"\"\n\ndef finish(machine_name):\n    return (\"\"\"\n  labelloc=\"t\"\n  labeljust=\"l\"\n  label=<<FONT POINT-SIZE=\"20\">h11 state machine: {}</FONT>>\n}}\n\"\"\".format(machine_name))\n\nclass Edges:\n    def __init__(self):\n        self.edges = []\n\n    def e(self, source, target, label, color, italicize=False, weight=1):\n        if italicize:\n            quoted_label = f\"<<i>{label}</i>>\"\n        else:\n            quoted_label = f'<{label}>'\n        self.edges.append(\n            f'{source} -> {target} [\\n'\n            f'  label={quoted_label},\\n'\n            f'  color=\"{color}\", fontcolor=\"{color}\",\\n'\n            f'  weight={weight},\\n'\n            f']\\n'\n            )\n\n    def write(self, f):\n        self.edges.sort()\n        f.write(\"\".join(self.edges))\n\ndef make_dot_special_state(out_path):\n    with open(out_path, \"w\") as f:\n        f.write(HEADER)\n        f.write(\"\"\"\n  kaT [label=<<i>keep-alive is enabled<br/>initial state</i>>]\n  kaF [label=<<i>keep-alive is disabled</i>>]\n\n  upF [label=<<i>No potential Upgrade: pending<br/>initial state</i>>]\n  upT [label=<<i>Potential Upgrade: pending</i>>]\n\n  coF [label=<<i>No potential CONNECT pending<br/>initial state</i>>]\n  coT [label=<<i>Potential CONNECT pending</i>>]\n\"\"\")\n        edges = Edges()\n        for s in [\"kaT\", \"kaF\"]:\n            edges.e(s, \"kaF\",\n                    \"Request/response with<br/>HTTP/1.0 or Connection: close\",\n                    color=_EVENT_COLOR,\n                    italicize=True)\n\n        edges.e(\"upF\", \"upT\",\n                \"Request with Upgrade:\",\n                color=_EVENT_COLOR, italicize=True)\n        edges.e(\"upT\", \"upF\",\n                \"Response\",\n                color=_EVENT_COLOR, italicize=True)\n\n        edges.e(\"coF\", \"coT\",\n                \"Request with CONNECT\",\n                color=_EVENT_COLOR, italicize=True)\n        edges.e(\"coT\", \"coF\",\n                \"Response without 2xx status\",\n                color=_EVENT_COLOR, italicize=True)\n\n        edges.write(f)\n\n        f.write(finish(\"special states\"))\n\ndef make_dot(role, out_path):\n    with open(out_path, \"w\") as f:\n        f.write(HEADER)\n        f.write(\"\"\"\n  IDLE [label=<IDLE<BR/><i>start state</i>>]\n  // move ERROR down to the bottom\n  {rank=same CLOSED ERROR}\n\"\"\")\n\n        # Dot output is sensitive to the order in which the nodes and edges\n        # are listed.  We generate them in python's randomized dict iteration\n        # order.  So to normalize order, we accumulate and then sort.\n        # Fortunately, this order happens to be one that produces a nice\n        # layout... with other orders I've seen really terrible layouts, and\n        # had to do things like move the server's IDLE->MUST_CLOSE to the top\n        # of the file to fix them.\n        edges = Edges()\n\n        CORE_EVENTS = {Request, InformationalResponse,\n                       Response, Data, EndOfMessage}\n\n        for (source_state, t) in EVENT_TRIGGERED_TRANSITIONS[role].items():\n            for (event_type, target_state) in t.items():\n                weight = 1\n                color = _EVENT_COLOR\n                italicize = False\n                if (event_type in CORE_EVENTS\n                    and source_state is not target_state):\n                    weight = 10\n                # exception\n                if (event_type is Response and source_state is IDLE):\n                    weight = 1\n                if isinstance(event_type, tuple):\n                    # The weird special cases\n                    #color = _SPECIAL_COLOR\n                    if event_type == (Request, CLIENT):\n                        name = \"<i>client makes Request</i>\"\n                        weight = 10\n                    elif event_type[1] is _SWITCH_UPGRADE:\n                        name = \"<i>101 Switching Protocols</i>\"\n                        weight = 1\n                    elif event_type[1] is _SWITCH_CONNECT:\n                        name = \"<i>CONNECT accepted</i>\"\n                        weight = 1\n                    else:\n                        assert False\n                else:\n                    name = event_type.__name__\n                edges.e(source_state, target_state, name, color,\n                        weight=weight, italicize=italicize)\n\n        for state_pair, updates in STATE_TRIGGERED_TRANSITIONS.items():\n            if role not in updates:\n                continue\n            if role is CLIENT:\n                (our_state, their_state) = state_pair\n            else:\n                (their_state, our_state) = state_pair\n            edges.e(our_state, updates[role],\n                    f\"<i>peer in</i><BR/>{their_state}\",\n                    color=_STATE_COLOR)\n\n        if role is CLIENT:\n            edges.e(DONE, MIGHT_SWITCH_PROTOCOL,\n                    \"Potential Upgrade:<BR/>or CONNECT pending\",\n                    _STATE_COLOR,\n                    italicize=True)\n            edges.e(MIGHT_SWITCH_PROTOCOL, DONE,\n                    \"No potential Upgrade:<BR/>or CONNECT pending\",\n                    _STATE_COLOR,\n                    italicize=True)\n\n        edges.e(DONE, MUST_CLOSE, \"keep-alive<BR/>is disabled\", _STATE_COLOR,\n                italicize=True)\n        edges.e(DONE, IDLE, \"start_next_cycle()\", _SPECIAL_COLOR)\n\n        edges.write(f)\n\n        # For some reason labelfontsize doesn't seem to do anything, but this\n        # works\n        f.write(finish(role))\n\nmy_dir = os.path.dirname(__file__)\nout_dir = os.path.join(my_dir, \"_static\")\nif not os.path.exists(out_dir):\n    os.path.mkdir(out_dir)\nfor role in (CLIENT, SERVER):\n    dot_path = os.path.join(out_dir, str(role) + \".dot\")\n    svg_path = dot_path[:-3] + \"svg\"\n    make_dot(role, dot_path)\n    subprocess.check_call([\"dot\", \"-Tsvg\", dot_path, \"-o\", svg_path])\n\ndot_path = os.path.join(out_dir, \"special-states.dot\")\nsvg_path = dot_path[:-3] + \"svg\"\nmake_dot_special_state(dot_path)\nsubprocess.check_call([\"dot\", \"-Tsvg\", dot_path, \"-o\", svg_path])\n", "docs/source/conf.py": "#!/usr/bin/env python3\n#\n# h11 documentation build configuration file, created by\n# sphinx-quickstart on Tue May  3 00:20:14 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n################################################################\n# hack hack\n#\n# The live ipython examples want to know where the docs source/ directory is,\n# so that they can find files that live there.\n#\n# There's no guarantee that our CWD == the source directory, but conf.py\n# *does* know what directory it lives in, so it can stash that in a public\n# place where the later code can find it.\n#\n# (In particular, the sphinx Makefile runs sphinx-build from a different\n# directory -- but RTD runs sphinx-build directly from inside the source/\n# directory, so there's no single value of this that works for both.)\n#\nimport os.path\nsys._h11_hack_docs_source_path = os.path.dirname(__file__)\n################################################################\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.napoleon',\n    'IPython.sphinxext.ipython_directive',\n    'IPython.sphinxext.ipython_console_highlighting',\n]\n\n# Undocumented trick: if we def setup here in conf.py, it gets called just\n# like an extension's setup function.\ndef setup(app):\n    app.add_javascript(\"show-code.js\")\n    app.add_javascript(\"facebox.js\")\n    app.add_stylesheet(\"facebox.css\")\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'h11'\ncopyright = '2016, Nathaniel J. Smith'\nauthor = 'Nathaniel J. Smith'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nimport h11\nversion = h11.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = h11.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# html_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# \"<project> v<release> documentation\" by default.\n#html_title = 'h11 v0.0.1'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not None, a 'Last updated on:' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to '%b %d, %Y'.\n#html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   'da', 'de', 'en', 'es', 'fi', 'fr', 'h', 'it', 'ja'\n#   'nl', 'no', 'pt', 'ro', 'r', 'sv', 'tr', 'zh'\n#html_search_language = 'en'\n\n# A dictionary with options for the search language support, empty by default.\n# 'ja' uses this config value.\n# 'zh' user can custom change `jieba` dictionary path.\n#html_search_options = {'type': 'default'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = 'scorer.js'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'h11doc'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n\n# Latex figure (float) alignment\n#'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'h11.tex', 'h11 Documentation',\n     'Nathaniel J. Smith', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'h11', 'h11 Documentation',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'h11', 'h11 Documentation',\n     author, 'h11', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    'python': ('https://docs.python.org/3.5', None),\n}\n", "docs/source/_examples/myclient.py": "import socket, ssl\nimport h11\n\nclass MyHttpClient:\n    def __init__(self, host, port):\n        self.sock = socket.create_connection((host, port))\n        if port == 443:\n            ctx = ssl.create_default_context()\n            self.sock = ctx.wrap_socket(self.sock, server_hostname=host)\n        self.conn = h11.Connection(our_role=h11.CLIENT)\n\n    def send(self, *events):\n        for event in events:\n            data = self.conn.send(event)\n            if data is None:\n                # event was a ConnectionClosed(), meaning that we won't be\n                # sending any more data:\n                self.sock.shutdown(socket.SHUT_WR)\n            else:\n                self.sock.sendall(data)\n\n    # max_bytes_per_recv intentionally set low for pedagogical purposes\n    def next_event(self, max_bytes_per_recv=200):\n        while True:\n            # If we already have a complete event buffered internally, just\n            # return that. Otherwise, read some data, add it to the internal\n            # buffer, and then try again.\n            event = self.conn.next_event()\n            if event is h11.NEED_DATA:\n                self.conn.receive_data(self.sock.recv(max_bytes_per_recv))\n                continue\n            return event\n", "examples/basic-client.py": "import socket\nimport ssl\n\nimport h11\n\n################################################################\n# Setup\n################################################################\n\nconn = h11.Connection(our_role=h11.CLIENT)\nctx = ssl.create_default_context()\nsock = ctx.wrap_socket(\n    socket.create_connection((\"httpbin.org\", 443)), server_hostname=\"httpbin.org\"\n)\n\n################################################################\n# Sending a request\n################################################################\n\n\ndef send(event):\n    print(\"Sending event:\")\n    print(event)\n    print()\n    # Pass the event through h11's state machine and encoding machinery\n    data = conn.send(event)\n    # Send the resulting bytes on the wire\n    sock.sendall(data)\n\n\nsend(\n    h11.Request(\n        method=\"GET\",\n        target=\"/get\",\n        headers=[(\"Host\", \"httpbin.org\"), (\"Connection\", \"close\")],\n    )\n)\nsend(h11.EndOfMessage())\n\n################################################################\n# Receiving the response\n################################################################\n\n\ndef next_event():\n    while True:\n        # Check if an event is already available\n        event = conn.next_event()\n        if event is h11.NEED_DATA:\n            # Nope, so fetch some data from the socket...\n            data = sock.recv(2048)\n            # ...and give it to h11 to convert back into events...\n            conn.receive_data(data)\n            # ...and then loop around to try again.\n            continue\n        return event\n\n\nwhile True:\n    event = next_event()\n    print(\"Received event:\")\n    print(event)\n    print()\n    if type(event) is h11.EndOfMessage:\n        break\n\n################################################################\n# Clean up\n################################################################\n\nsock.close()\n", "examples/trio-server.py": "# A simple HTTP server implemented using h11 and Trio:\n#   http://trio.readthedocs.io/en/latest/index.html\n#\n# All requests get echoed back a JSON document containing information about\n# the request.\n#\n# This is a rather involved example, since it attempts to both be\n# fully-HTTP-compliant and also demonstrate error handling.\n#\n# The main difference between an HTTP client and an HTTP server is that in a\n# client, if something goes wrong, you can just throw away that connection and\n# make a new one. In a server, you're expected to handle all kinds of garbage\n# input and internal errors and recover with grace and dignity. And that's\n# what this code does.\n#\n# I recommend pushing on it to see how it works -- e.g. watch what happens if\n# you visit http://localhost:8080 in a webbrowser that supports keep-alive,\n# hit reload a few times, and then wait for the keep-alive to time out on the\n# server.\n#\n# Or try using curl to start a chunked upload and then hit control-C in the\n# middle of the upload:\n#\n#    (for CHUNK in $(seq 10); do echo $CHUNK; sleep 1; done) \\\n#      | curl -T - http://localhost:8080/foo\n#\n# (Note that curl will send Expect: 100-Continue, too.)\n#\n# Or, heck, try letting curl complete successfully ;-).\n\n# Some potential improvements, if you wanted to try and extend this to a real\n# general-purpose HTTP server (and to give you some hints about the many\n# considerations that go into making a robust HTTP server):\n#\n# - The timeout handling is rather crude -- we impose a flat 10 second timeout\n#   on each request (starting from the end of the previous\n#   response). Something finer-grained would be better. Also, if a timeout is\n#   triggered we unconditionally send a 500 Internal Server Error; it would be\n#   better to keep track of whether the timeout is the client's fault, and if\n#   so send a 408 Request Timeout.\n#\n# - The error handling policy here is somewhat crude as well. It handles a lot\n#   of cases perfectly, but there are corner cases where the ideal behavior is\n#   more debateable. For example, if a client starts uploading a large\n#   request, uses 100-Continue, and we send an error response, then we'll shut\n#   down the connection immediately (for well-behaved clients) or after\n#   spending TIMEOUT seconds reading and discarding their upload (for\n#   ill-behaved ones that go on and try to upload their request anyway). And\n#   for clients that do this without 100-Continue, we'll send the error\n#   response and then shut them down after TIMEOUT seconds. This might or\n#   might not be your preferred policy, though -- maybe you want to shut such\n#   clients down immediately (even if this risks their not seeing the\n#   response), or maybe you're happy to let them continue sending all the data\n#   and wasting your bandwidth if this is what it takes to guarantee that they\n#   see your error response. Up to you, really.\n#\n# - Another example of a debateable choice: if a response handler errors out\n#   without having done *anything* -- hasn't started responding, hasn't read\n#   the request body -- then this connection actually is salvagable, if the\n#   server sends an error response + reads and discards the request body. This\n#   code sends the error response, but it doesn't try to salvage the\n#   connection by reading the request body, it just closes the\n#   connection. This is quite possibly the best option, but again this is a\n#   policy decision.\n#\n# - Our error pages always include the exception text. In real life you might\n#   want to log the exception but not send that information to the client.\n#\n# - Our error responses perhaps should include Connection: close when we know\n#   we're going to close this connection.\n#\n# - We don't support the HEAD method, but ought to.\n#\n# - We should probably do something cleverer with buffering responses and\n#   TCP_CORK and suchlike.\n\nimport datetime\nimport email.utils\nimport json\nfrom itertools import count\n\nimport trio\n\nimport h11\n\nMAX_RECV = 2**16\nTIMEOUT = 10\n\n\n# We are using email.utils.format_datetime to generate the Date header.\n# It may sound weird, but it actually follows the RFC.\n# Please see: https://stackoverflow.com/a/59416334/14723771\n#\n# See also:\n# [1] https://www.rfc-editor.org/rfc/rfc9110#section-5.6.7\n# [2] https://www.rfc-editor.org/rfc/rfc7231#section-7.1.1.1\n# [3] https://www.rfc-editor.org/rfc/rfc5322#section-3.3\ndef format_date_time(dt=None):\n    \"\"\"Generate a RFC 7231 / RFC 9110 IMF-fixdate string\"\"\"\n    if dt is None:\n        dt = datetime.datetime.now(datetime.timezone.utc)\n    return email.utils.format_datetime(dt, usegmt=True)\n\n\n################################################################\n# I/O adapter: h11 <-> trio\n################################################################\n\n\n# The core of this could be factored out to be usable for trio-based clients\n# too, as well as servers. But as a simplified pedagogical example we don't\n# attempt this here.\nclass TrioHTTPWrapper:\n    _next_id = count()\n\n    def __init__(self, stream):\n        self.stream = stream\n        self.conn = h11.Connection(h11.SERVER)\n        # Our Server: header\n        self.ident = \" \".join(\n            [f\"h11-example-trio-server/{h11.__version__}\", h11.PRODUCT_ID]\n        ).encode(\"ascii\")\n        # A unique id for this connection, to include in debugging output\n        # (useful for understanding what's going on if there are multiple\n        # simultaneous clients).\n        self._obj_id = next(TrioHTTPWrapper._next_id)\n\n    async def send(self, event):\n        # The code below doesn't send ConnectionClosed, so we don't bother\n        # handling it here either -- it would require that we do something\n        # appropriate when 'data' is None.\n        assert type(event) is not h11.ConnectionClosed\n        data = self.conn.send(event)\n        try:\n            await self.stream.send_all(data)\n        except BaseException:\n            # If send_all raises an exception (especially trio.Cancelled),\n            # we have no choice but to give it up.\n            self.conn.send_failed()\n            raise\n\n    async def _read_from_peer(self):\n        if self.conn.they_are_waiting_for_100_continue:\n            self.info(\"Sending 100 Continue\")\n            go_ahead = h11.InformationalResponse(\n                status_code=100, headers=self.basic_headers()\n            )\n            await self.send(go_ahead)\n        try:\n            data = await self.stream.receive_some(MAX_RECV)\n        except ConnectionError:\n            # They've stopped listening. Not much we can do about it here.\n            data = b\"\"\n        self.conn.receive_data(data)\n\n    async def next_event(self):\n        while True:\n            event = self.conn.next_event()\n            if event is h11.NEED_DATA:\n                await self._read_from_peer()\n                continue\n            return event\n\n    async def shutdown_and_clean_up(self):\n        # When this method is called, it's because we definitely want to kill\n        # this connection, either as a clean shutdown or because of some kind\n        # of error or loss-of-sync bug, and we no longer care if that violates\n        # the protocol or not. So we ignore the state of self.conn, and just\n        # go ahead and do the shutdown on the socket directly. (If you're\n        # implementing a client you might prefer to send ConnectionClosed()\n        # and let it raise an exception if that violates the protocol.)\n        #\n        try:\n            await self.stream.send_eof()\n        except trio.BrokenResourceError:\n            # They're already gone, nothing to do\n            return\n        # Wait and read for a bit to give them a chance to see that we closed\n        # things, but eventually give up and just close the socket.\n        # XX FIXME: possibly we should set SO_LINGER to 0 here, so\n        # that in the case where the client has ignored our shutdown and\n        # declined to initiate the close themselves, we do a violent shutdown\n        # (RST) and avoid the TIME_WAIT?\n        # it looks like nginx never does this for keepalive timeouts, and only\n        # does it for regular timeouts (slow clients I guess?) if explicitly\n        # enabled (\"Default: reset_timedout_connection off\")\n        with trio.move_on_after(TIMEOUT):\n            try:\n                while True:\n                    # Attempt to read until EOF\n                    got = await self.stream.receive_some(MAX_RECV)\n                    if not got:\n                        break\n            except trio.BrokenResourceError:\n                pass\n            finally:\n                await self.stream.aclose()\n\n    def basic_headers(self):\n        # HTTP requires these headers in all responses (client would do\n        # something different here)\n        return [\n            (\"Date\", format_date_time().encode(\"ascii\")),\n            (\"Server\", self.ident),\n        ]\n\n    def info(self, *args):\n        # Little debugging method\n        print(f\"{self._obj_id}:\", *args)\n\n\n################################################################\n# Server main loop\n################################################################\n\n\n# General theory:\n#\n# If everything goes well:\n# - we'll get a Request\n# - our response handler will read the request body and send a full response\n# - that will either leave us in MUST_CLOSE (if the client doesn't\n#   support keepalive) or DONE/DONE (if the client does).\n#\n# But then there are many, many different ways that things can go wrong\n# here. For example:\n# - we don't actually get a Request, but rather a ConnectionClosed\n# - exception is raised from somewhere (naughty client, broken\n#   response handler, whatever)\n#   - depending on what went wrong and where, we might or might not be\n#     able to send an error response, and the connection might or\n#     might not be salvagable after that\n# - response handler doesn't fully read the request or doesn't send a\n#   full response\n#\n# But these all have one thing in common: they involve us leaving the\n# nice easy path up above. So we can just proceed on the assumption\n# that the nice easy thing is what's happening, and whenever something\n# goes wrong do our best to get back onto that path, and h11 will keep\n# track of how successful we were and raise new errors if things don't work\n# out.\nasync def http_serve(stream):\n    wrapper = TrioHTTPWrapper(stream)\n    wrapper.info(\"Got new connection\")\n    while True:\n        assert wrapper.conn.states == {h11.CLIENT: h11.IDLE, h11.SERVER: h11.IDLE}\n\n        try:\n            with trio.fail_after(TIMEOUT):\n                wrapper.info(\"Server main loop waiting for request\")\n                event = await wrapper.next_event()\n                wrapper.info(\"Server main loop got event:\", event)\n                if type(event) is h11.Request:\n                    await send_echo_response(wrapper, event)\n        except Exception as exc:\n            wrapper.info(f\"Error during response handler: {exc!r}\")\n            await maybe_send_error_response(wrapper, exc)\n\n        if wrapper.conn.our_state is h11.MUST_CLOSE:\n            wrapper.info(\"connection is not reusable, so shutting down\")\n            await wrapper.shutdown_and_clean_up()\n            return\n        else:\n            try:\n                wrapper.info(\"trying to re-use connection\")\n                wrapper.conn.start_next_cycle()\n            except h11.ProtocolError:\n                states = wrapper.conn.states\n                wrapper.info(\"unexpected state\", states, \"-- bailing out\")\n                await maybe_send_error_response(\n                    wrapper, RuntimeError(f\"unexpected state {states}\")\n                )\n                await wrapper.shutdown_and_clean_up()\n                return\n\n\n################################################################\n# Actual response handlers\n################################################################\n\n\n# Helper function\nasync def send_simple_response(wrapper, status_code, content_type, body):\n    wrapper.info(\"Sending\", status_code, \"response with\", len(body), \"bytes\")\n    headers = wrapper.basic_headers()\n    headers.append((\"Content-Type\", content_type))\n    headers.append((\"Content-Length\", str(len(body))))\n    res = h11.Response(status_code=status_code, headers=headers)\n    await wrapper.send(res)\n    await wrapper.send(h11.Data(data=body))\n    await wrapper.send(h11.EndOfMessage())\n\n\nasync def maybe_send_error_response(wrapper, exc):\n    # If we can't send an error, oh well, nothing to be done\n    wrapper.info(\"trying to send error response...\")\n    if wrapper.conn.our_state not in {h11.IDLE, h11.SEND_RESPONSE}:\n        wrapper.info(\"...but I can't, because our state is\", wrapper.conn.our_state)\n        return\n    try:\n        if isinstance(exc, h11.RemoteProtocolError):\n            status_code = exc.error_status_hint\n        elif isinstance(exc, trio.TooSlowError):\n            status_code = 408  # Request Timeout\n        else:\n            status_code = 500\n        body = str(exc).encode(\"utf-8\")\n        await send_simple_response(\n            wrapper, status_code, \"text/plain; charset=utf-8\", body\n        )\n    except Exception as exc:\n        wrapper.info(\"error while sending error response:\", exc)\n\n\nasync def send_echo_response(wrapper, request):\n    wrapper.info(\"Preparing echo response\")\n    if request.method not in {b\"GET\", b\"POST\"}:\n        # Laziness: we should send a proper 405 Method Not Allowed with the\n        # appropriate Accept: header, but we don't.\n        raise RuntimeError(\"unsupported method\")\n    response_json = {\n        \"method\": request.method.decode(\"ascii\"),\n        \"target\": request.target.decode(\"ascii\"),\n        \"headers\": [\n            (name.decode(\"ascii\"), value.decode(\"ascii\"))\n            for (name, value) in request.headers\n        ],\n        \"body\": \"\",\n    }\n    while True:\n        event = await wrapper.next_event()\n        if type(event) is h11.EndOfMessage:\n            break\n        assert type(event) is h11.Data\n        response_json[\"body\"] += event.data.decode(\"ascii\")\n    response_body_unicode = json.dumps(\n        response_json, sort_keys=True, indent=4, separators=(\",\", \": \")\n    )\n    response_body_bytes = response_body_unicode.encode(\"utf-8\")\n    await send_simple_response(\n        wrapper, 200, \"application/json; charset=utf-8\", response_body_bytes\n    )\n\n\nasync def serve(port):\n    print(f\"listening on http://localhost:{port}\")\n    try:\n        await trio.serve_tcp(http_serve, port)\n    except KeyboardInterrupt:\n        print(\"KeyboardInterrupt - shutting down\")\n\n\n################################################################\n# Run the server\n################################################################\n\nif __name__ == \"__main__\":\n    trio.run(serve, 8080)\n"}