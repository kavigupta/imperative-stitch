{"setup.py": "from setuptools import setup, find_packages\n\n# defines __version__\nexec(open(\"h11/_version.py\").read())\n\nsetup(\n    name=\"h11\",\n    version=__version__,\n    description=\n        \"A pure-Python, bring-your-own-I/O implementation of HTTP/1.1\",\n    long_description=open(\"README.rst\").read(),\n    author=\"Nathaniel J. Smith\",\n    author_email=\"njs@pobox.com\",\n    license=\"MIT\",\n    packages=find_packages(exclude=[\"h11.tests\"]),\n    package_data={'h11': ['py.typed']},\n    url=\"https://github.com/python-hyper/h11\",\n    python_requires=\">=3.8\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: System :: Networking\",\n    ],\n)\n", "bench/benchmarks/benchmarks.py": "# Write the benchmarking functions here.\n# See \"Writing benchmarks\" in the asv docs for more information.\n\nimport h11\n\n\n# Basic ASV benchmark of core functionality\ndef time_server_basic_get_with_realistic_headers():\n    c = h11.Connection(h11.SERVER)\n    c.receive_data(\n        b\"GET / HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"User-Agent: Mozilla/5.0 (X11; Linux x86_64; \"\n        b\"rv:45.0) Gecko/20100101 Firefox/45.0\\r\\n\"\n        b\"Accept: text/html,application/xhtml+xml,\"\n        b\"application/xml;q=0.9,*/*;q=0.8\\r\\n\"\n        b\"Accept-Language: en-US,en;q=0.5\\r\\n\"\n        b\"Accept-Encoding: gzip, deflate, br\\r\\n\"\n        b\"DNT: 1\\r\\n\"\n        b\"Cookie: ID=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n        b\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\\r\\n\"\n        b\"Connection: keep-alive\\r\\n\\r\\n\"\n    )\n    while True:\n        event = c.next_event()\n        if event is h11.NEED_DATA:\n            break\n\n    c.send(\n        h11.Response(\n            status_code=200,\n            headers=[\n                (b\"Cache-Control\", b\"private, max-age=0\"),\n                (b\"Content-Encoding\", b\"gzip\"),\n                (b\"Content-Type\", b\"text/html; charset=UTF-8\"),\n                (b\"Date\", b\"Fri, 20 May 2016 09:23:41 GMT\"),\n                (b\"Expires\", b\"-1\"),\n                (b\"Server\", b\"gws\"),\n                (b\"X-Frame-Options\", b\"SAMEORIGIN\"),\n                (b\"X-XSS-Protection\", b\"1; mode=block\"),\n                (b\"Content-Length\", b\"1000\"),\n            ],\n        )\n    )\n    c.send(h11.Data(data=b\"x\" * 1000))\n    c.send(h11.EndOfMessage())\n\n\n# Useful for manual benchmarking, e.g. with vmprof or on PyPy\ndef _run_basic_get_repeatedly():\n    from timeit import default_timer\n\n    REPEAT = 10000\n    # while True:\n    for _ in range(7):\n        start = default_timer()\n        for _ in range(REPEAT):\n            time_server_basic_get_with_realistic_headers()\n        finish = default_timer()\n        print(f\"{REPEAT / (finish - start):.1f} requests/sec\")\n\n\nif __name__ == \"__main__\":\n    _run_basic_get_repeatedly()\n", "bench/benchmarks/__init__.py": "", "h11/_readers.py": "# Code to read HTTP data\n#\n# Strategy: each reader is a callable which takes a ReceiveBuffer object, and\n# either:\n# 1) consumes some of it and returns an Event\n# 2) raises a LocalProtocolError (for consistency -- e.g. we call validate()\n#    and it might raise a LocalProtocolError, so simpler just to always use\n#    this)\n# 3) returns None, meaning \"I need more data\"\n#\n# If they have a .read_eof attribute, then this will be called if an EOF is\n# received -- but this is optional. Either way, the actual ConnectionClosed\n# event will be generated afterwards.\n#\n# READERS is a dict describing how to pick a reader. It maps states to either:\n# - a reader\n# - or, for body readers, a dict of per-framing reader factories\n\nimport re\nfrom typing import Any, Callable, Dict, Iterable, NoReturn, Optional, Tuple, Type, Union\n\nfrom ._abnf import chunk_header, header_field, request_line, status_line\nfrom ._events import Data, EndOfMessage, InformationalResponse, Request, Response\nfrom ._receivebuffer import ReceiveBuffer\nfrom ._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    IDLE,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n)\nfrom ._util import LocalProtocolError, RemoteProtocolError, Sentinel, validate\n\n__all__ = [\"READERS\"]\n\nheader_field_re = re.compile(header_field.encode(\"ascii\"))\nobs_fold_re = re.compile(rb\"[ \\t]+\")\n\n\ndef _obsolete_line_fold(lines: Iterable[bytes]) -> Iterable[bytes]:\n    it = iter(lines)\n    last: Optional[bytes] = None\n    for line in it:\n        match = obs_fold_re.match(line)\n        if match:\n            if last is None:\n                raise LocalProtocolError(\"continuation line at start of headers\")\n            if not isinstance(last, bytearray):\n                # Cast to a mutable type, avoiding copy on append to ensure O(n) time\n                last = bytearray(last)\n            last += b\" \"\n            last += line[match.end() :]\n        else:\n            if last is not None:\n                yield last\n            last = line\n    if last is not None:\n        yield last\n\n\ndef _decode_header_lines(\n    lines: Iterable[bytes],\n) -> Iterable[Tuple[bytes, bytes]]:\n    for line in _obsolete_line_fold(lines):\n        matches = validate(header_field_re, line, \"illegal header line: {!r}\", line)\n        yield (matches[\"field_name\"], matches[\"field_value\"])\n\n\nrequest_line_re = re.compile(request_line.encode(\"ascii\"))\n\n\ndef maybe_read_from_IDLE_client(buf: ReceiveBuffer) -> Optional[Request]:\n    lines = buf.maybe_extract_lines()\n    if lines is None:\n        if buf.is_next_line_obviously_invalid_request_line():\n            raise LocalProtocolError(\"illegal request line\")\n        return None\n    if not lines:\n        raise LocalProtocolError(\"no request line received\")\n    matches = validate(\n        request_line_re, lines[0], \"illegal request line: {!r}\", lines[0]\n    )\n    return Request(\n        headers=list(_decode_header_lines(lines[1:])), _parsed=True, **matches\n    )\n\n\nstatus_line_re = re.compile(status_line.encode(\"ascii\"))\n\n\ndef maybe_read_from_SEND_RESPONSE_server(\n    buf: ReceiveBuffer,\n) -> Union[InformationalResponse, Response, None]:\n    lines = buf.maybe_extract_lines()\n    if lines is None:\n        if buf.is_next_line_obviously_invalid_request_line():\n            raise LocalProtocolError(\"illegal request line\")\n        return None\n    if not lines:\n        raise LocalProtocolError(\"no response line received\")\n    matches = validate(status_line_re, lines[0], \"illegal status line: {!r}\", lines[0])\n    http_version = (\n        b\"1.1\" if matches[\"http_version\"] is None else matches[\"http_version\"]\n    )\n    reason = b\"\" if matches[\"reason\"] is None else matches[\"reason\"]\n    status_code = int(matches[\"status_code\"])\n    class_: Union[Type[InformationalResponse], Type[Response]] = (\n        InformationalResponse if status_code < 200 else Response\n    )\n    return class_(\n        headers=list(_decode_header_lines(lines[1:])),\n        _parsed=True,\n        status_code=status_code,\n        reason=reason,\n        http_version=http_version,\n    )\n\n\nclass ContentLengthReader:\n    def __init__(self, length: int) -> None:\n        self._length = length\n        self._remaining = length\n\n    def __call__(self, buf: ReceiveBuffer) -> Union[Data, EndOfMessage, None]:\n        if self._remaining == 0:\n            return EndOfMessage()\n        data = buf.maybe_extract_at_most(self._remaining)\n        if data is None:\n            return None\n        self._remaining -= len(data)\n        return Data(data=data)\n\n    def read_eof(self) -> NoReturn:\n        raise RemoteProtocolError(\n            \"peer closed connection without sending complete message body \"\n            \"(received {} bytes, expected {})\".format(\n                self._length - self._remaining, self._length\n            )\n        )\n\n\nchunk_header_re = re.compile(chunk_header.encode(\"ascii\"))\n\n\nclass ChunkedReader:\n    def __init__(self) -> None:\n        self._bytes_in_chunk = 0\n        # After reading a chunk, we have to throw away the trailing \\r\\n; if\n        # this is >0 then we discard that many bytes before resuming regular\n        # de-chunkification.\n        self._bytes_to_discard = 0\n        self._reading_trailer = False\n\n    def __call__(self, buf: ReceiveBuffer) -> Union[Data, EndOfMessage, None]:\n        if self._reading_trailer:\n            lines = buf.maybe_extract_lines()\n            if lines is None:\n                return None\n            return EndOfMessage(headers=list(_decode_header_lines(lines)))\n        if self._bytes_to_discard > 0:\n            data = buf.maybe_extract_at_most(self._bytes_to_discard)\n            if data is None:\n                return None\n            self._bytes_to_discard -= len(data)\n            if self._bytes_to_discard > 0:\n                return None\n            # else, fall through and read some more\n        assert self._bytes_to_discard == 0\n        if self._bytes_in_chunk == 0:\n            # We need to refill our chunk count\n            chunk_header = buf.maybe_extract_next_line()\n            if chunk_header is None:\n                return None\n            matches = validate(\n                chunk_header_re,\n                chunk_header,\n                \"illegal chunk header: {!r}\",\n                chunk_header,\n            )\n            # XX FIXME: we discard chunk extensions. Does anyone care?\n            self._bytes_in_chunk = int(matches[\"chunk_size\"], base=16)\n            if self._bytes_in_chunk == 0:\n                self._reading_trailer = True\n                return self(buf)\n            chunk_start = True\n        else:\n            chunk_start = False\n        assert self._bytes_in_chunk > 0\n        data = buf.maybe_extract_at_most(self._bytes_in_chunk)\n        if data is None:\n            return None\n        self._bytes_in_chunk -= len(data)\n        if self._bytes_in_chunk == 0:\n            self._bytes_to_discard = 2\n            chunk_end = True\n        else:\n            chunk_end = False\n        return Data(data=data, chunk_start=chunk_start, chunk_end=chunk_end)\n\n    def read_eof(self) -> NoReturn:\n        raise RemoteProtocolError(\n            \"peer closed connection without sending complete message body \"\n            \"(incomplete chunked read)\"\n        )\n\n\nclass Http10Reader:\n    def __call__(self, buf: ReceiveBuffer) -> Optional[Data]:\n        data = buf.maybe_extract_at_most(999999999)\n        if data is None:\n            return None\n        return Data(data=data)\n\n    def read_eof(self) -> EndOfMessage:\n        return EndOfMessage()\n\n\ndef expect_nothing(buf: ReceiveBuffer) -> None:\n    if buf:\n        raise LocalProtocolError(\"Got data when expecting EOF\")\n    return None\n\n\nReadersType = Dict[\n    Union[Type[Sentinel], Tuple[Type[Sentinel], Type[Sentinel]]],\n    Union[Callable[..., Any], Dict[str, Callable[..., Any]]],\n]\n\nREADERS: ReadersType = {\n    (CLIENT, IDLE): maybe_read_from_IDLE_client,\n    (SERVER, IDLE): maybe_read_from_SEND_RESPONSE_server,\n    (SERVER, SEND_RESPONSE): maybe_read_from_SEND_RESPONSE_server,\n    (CLIENT, DONE): expect_nothing,\n    (CLIENT, MUST_CLOSE): expect_nothing,\n    (CLIENT, CLOSED): expect_nothing,\n    (SERVER, DONE): expect_nothing,\n    (SERVER, MUST_CLOSE): expect_nothing,\n    (SERVER, CLOSED): expect_nothing,\n    SEND_BODY: {\n        \"chunked\": ChunkedReader,\n        \"content-length\": ContentLengthReader,\n        \"http/1.0\": Http10Reader,\n    },\n}\n", "h11/_abnf.py": "# We use native strings for all the re patterns, to take advantage of string\n# formatting, and then convert to bytestrings when compiling the final re\n# objects.\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#whitespace\n#  OWS            = *( SP / HTAB )\n#                 ; optional whitespace\nOWS = r\"[ \\t]*\"\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#rule.token.separators\n#   token          = 1*tchar\n#\n#   tchar          = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\"\n#                  / \"+\" / \"-\" / \".\" / \"^\" / \"_\" / \"`\" / \"|\" / \"~\"\n#                  / DIGIT / ALPHA\n#                  ; any VCHAR, except delimiters\ntoken = r\"[-!#$%&'*+.^_`|~0-9a-zA-Z]+\"\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#header.fields\n#  field-name     = token\nfield_name = token\n\n# The standard says:\n#\n#  field-value    = *( field-content / obs-fold )\n#  field-content  = field-vchar [ 1*( SP / HTAB ) field-vchar ]\n#  field-vchar    = VCHAR / obs-text\n#  obs-fold       = CRLF 1*( SP / HTAB )\n#                 ; obsolete line folding\n#                 ; see Section 3.2.4\n#\n# https://tools.ietf.org/html/rfc5234#appendix-B.1\n#\n#   VCHAR          =  %x21-7E\n#                  ; visible (printing) characters\n#\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#rule.quoted-string\n#   obs-text       = %x80-FF\n#\n# However, the standard definition of field-content is WRONG! It disallows\n# fields containing a single visible character surrounded by whitespace,\n# e.g. \"foo a bar\".\n#\n# See: https://www.rfc-editor.org/errata_search.php?rfc=7230&eid=4189\n#\n# So our definition of field_content attempts to fix it up...\n#\n# Also, we allow lots of control characters, because apparently people assume\n# that they're legal in practice (e.g., google analytics makes cookies with\n# \\x01 in them!):\n#   https://github.com/python-hyper/h11/issues/57\n# We still don't allow NUL or whitespace, because those are often treated as\n# meta-characters and letting them through can lead to nasty issues like SSRF.\nvchar = r\"[\\x21-\\x7e]\"\nvchar_or_obs_text = r\"[^\\x00\\s]\"\nfield_vchar = vchar_or_obs_text\nfield_content = r\"{field_vchar}+(?:[ \\t]+{field_vchar}+)*\".format(**globals())\n\n# We handle obs-fold at a different level, and our fixed-up field_content\n# already grows to swallow the whole value, so ? instead of *\nfield_value = r\"({field_content})?\".format(**globals())\n\n#  header-field   = field-name \":\" OWS field-value OWS\nheader_field = (\n    r\"(?P<field_name>{field_name})\"\n    r\":\"\n    r\"{OWS}\"\n    r\"(?P<field_value>{field_value})\"\n    r\"{OWS}\".format(**globals())\n)\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#request.line\n#\n#   request-line   = method SP request-target SP HTTP-version CRLF\n#   method         = token\n#   HTTP-version   = HTTP-name \"/\" DIGIT \".\" DIGIT\n#   HTTP-name      = %x48.54.54.50 ; \"HTTP\", case-sensitive\n#\n# request-target is complicated (see RFC 7230 sec 5.3) -- could be path, full\n# URL, host+port (for connect), or even \"*\", but in any case we are guaranteed\n# that it contists of the visible printing characters.\nmethod = token\nrequest_target = r\"{vchar}+\".format(**globals())\nhttp_version = r\"HTTP/(?P<http_version>[0-9]\\.[0-9])\"\nrequest_line = (\n    r\"(?P<method>{method})\"\n    r\" \"\n    r\"(?P<target>{request_target})\"\n    r\" \"\n    r\"{http_version}\".format(**globals())\n)\n\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#status.line\n#\n#   status-line = HTTP-version SP status-code SP reason-phrase CRLF\n#   status-code    = 3DIGIT\n#   reason-phrase  = *( HTAB / SP / VCHAR / obs-text )\nstatus_code = r\"[0-9]{3}\"\nreason_phrase = r\"([ \\t]|{vchar_or_obs_text})*\".format(**globals())\nstatus_line = (\n    r\"{http_version}\"\n    r\" \"\n    r\"(?P<status_code>{status_code})\"\n    # However, there are apparently a few too many servers out there that just\n    # leave out the reason phrase:\n    #   https://github.com/scrapy/scrapy/issues/345#issuecomment-281756036\n    #   https://github.com/seanmonstar/httparse/issues/29\n    # so make it optional. ?: is a non-capturing group.\n    r\"(?: (?P<reason>{reason_phrase}))?\".format(**globals())\n)\n\nHEXDIG = r\"[0-9A-Fa-f]\"\n# Actually\n#\n#      chunk-size     = 1*HEXDIG\n#\n# but we impose an upper-limit to avoid ridiculosity. len(str(2**64)) == 20\nchunk_size = r\"({HEXDIG}){{1,20}}\".format(**globals())\n# Actually\n#\n#     chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-val ] )\n#\n# but we aren't parsing the things so we don't really care.\nchunk_ext = r\";.*\"\nchunk_header = (\n    r\"(?P<chunk_size>{chunk_size})\"\n    r\"(?P<chunk_ext>{chunk_ext})?\"\n    r\"{OWS}\\r\\n\".format(\n        **globals()\n    )  # Even though the specification does not allow for extra whitespaces,\n    # we are lenient with trailing whitespaces because some servers on the wild use it.\n)\n", "h11/_receivebuffer.py": "import re\nimport sys\nfrom typing import List, Optional, Union\n\n__all__ = [\"ReceiveBuffer\"]\n\n\n# Operations we want to support:\n# - find next \\r\\n or \\r\\n\\r\\n (\\n or \\n\\n are also acceptable),\n#   or wait until there is one\n# - read at-most-N bytes\n# Goals:\n# - on average, do this fast\n# - worst case, do this in O(n) where n is the number of bytes processed\n# Plan:\n# - store bytearray, offset, how far we've searched for a separator token\n# - use the how-far-we've-searched data to avoid rescanning\n# - while doing a stream of uninterrupted processing, advance offset instead\n#   of constantly copying\n# WARNING:\n# - I haven't benchmarked or profiled any of this yet.\n#\n# Note that starting in Python 3.4, deleting the initial n bytes from a\n# bytearray is amortized O(n), thanks to some excellent work by Antoine\n# Martin:\n#\n#     https://bugs.python.org/issue19087\n#\n# This means that if we only supported 3.4+, we could get rid of the code here\n# involving self._start and self.compress, because it's doing exactly the same\n# thing that bytearray now does internally.\n#\n# BUT unfortunately, we still support 2.7, and reading short segments out of a\n# long buffer MUST be O(bytes read) to avoid DoS issues, so we can't actually\n# delete this code. Yet:\n#\n#     https://pythonclock.org/\n#\n# (Two things to double-check first though: make sure PyPy also has the\n# optimization, and benchmark to make sure it's a win, since we do have a\n# slightly clever thing where we delay calling compress() until we've\n# processed a whole event, which could in theory be slightly more efficient\n# than the internal bytearray support.)\nblank_line_regex = re.compile(b\"\\n\\r?\\n\", re.MULTILINE)\n\n\nclass ReceiveBuffer:\n    def __init__(self) -> None:\n        self._data = bytearray()\n        self._next_line_search = 0\n        self._multiple_lines_search = 0\n\n    def __iadd__(self, byteslike: Union[bytes, bytearray]) -> \"ReceiveBuffer\":\n        self._data += byteslike\n        return self\n\n    def __bool__(self) -> bool:\n        return bool(len(self))\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    # for @property unprocessed_data\n    def __bytes__(self) -> bytes:\n        return bytes(self._data)\n\n    def _extract(self, count: int) -> bytearray:\n        # extracting an initial slice of the data buffer and return it\n        out = self._data[:count]\n        del self._data[:count]\n\n        self._next_line_search = 0\n        self._multiple_lines_search = 0\n\n        return out\n\n    def maybe_extract_at_most(self, count: int) -> Optional[bytearray]:\n        \"\"\"\n        Extract a fixed number of bytes from the buffer.\n        \"\"\"\n        out = self._data[:count]\n        if not out:\n            return None\n\n        return self._extract(count)\n\n    def maybe_extract_next_line(self) -> Optional[bytearray]:\n        \"\"\"\n        Extract the first line, if it is completed in the buffer.\n        \"\"\"\n        # Only search in buffer space that we've not already looked at.\n        search_start_index = max(0, self._next_line_search - 1)\n        partial_idx = self._data.find(b\"\\r\\n\", search_start_index)\n\n        if partial_idx == -1:\n            self._next_line_search = len(self._data)\n            return None\n\n        # + 2 is to compensate len(b\"\\r\\n\")\n        idx = partial_idx + 2\n\n        return self._extract(idx)\n\n    def maybe_extract_lines(self) -> Optional[List[bytearray]]:\n        \"\"\"\n        Extract everything up to the first blank line, and return a list of lines.\n        \"\"\"\n        # Handle the case where we have an immediate empty line.\n        if self._data[:1] == b\"\\n\":\n            self._extract(1)\n            return []\n\n        if self._data[:2] == b\"\\r\\n\":\n            self._extract(2)\n            return []\n\n        # Only search in buffer space that we've not already looked at.\n        match = blank_line_regex.search(self._data, self._multiple_lines_search)\n        if match is None:\n            self._multiple_lines_search = max(0, len(self._data) - 2)\n            return None\n\n        # Truncate the buffer and return it.\n        idx = match.span(0)[-1]\n        out = self._extract(idx)\n        lines = out.split(b\"\\n\")\n\n        for line in lines:\n            if line.endswith(b\"\\r\"):\n                del line[-1]\n\n        assert lines[-2] == lines[-1] == b\"\"\n\n        del lines[-2:]\n\n        return lines\n\n    # In theory we should wait until `\\r\\n` before starting to validate\n    # incoming data. However it's interesting to detect (very) invalid data\n    # early given they might not even contain `\\r\\n` at all (hence only\n    # timeout will get rid of them).\n    # This is not a 100% effective detection but more of a cheap sanity check\n    # allowing for early abort in some useful cases.\n    # This is especially interesting when peer is messing up with HTTPS and\n    # sent us a TLS stream where we were expecting plain HTTP given all\n    # versions of TLS so far start handshake with a 0x16 message type code.\n    def is_next_line_obviously_invalid_request_line(self) -> bool:\n        try:\n            # HTTP header line must not contain non-printable characters\n            # and should not start with a space\n            return self._data[0] < 0x21\n        except IndexError:\n            return False\n", "h11/_state.py": "################################################################\n# The core state machine\n################################################################\n#\n# Rule 1: everything that affects the state machine and state transitions must\n# live here in this file. As much as possible goes into the table-based\n# representation, but for the bits that don't quite fit, the actual code and\n# state must nonetheless live here.\n#\n# Rule 2: this file does not know about what role we're playing; it only knows\n# about HTTP request/response cycles in the abstract. This ensures that we\n# don't cheat and apply different rules to local and remote parties.\n#\n#\n# Theory of operation\n# ===================\n#\n# Possibly the simplest way to think about this is that we actually have 5\n# different state machines here. Yes, 5. These are:\n#\n# 1) The client state, with its complicated automaton (see the docs)\n# 2) The server state, with its complicated automaton (see the docs)\n# 3) The keep-alive state, with possible states {True, False}\n# 4) The SWITCH_CONNECT state, with possible states {False, True}\n# 5) The SWITCH_UPGRADE state, with possible states {False, True}\n#\n# For (3)-(5), the first state listed is the initial state.\n#\n# (1)-(3) are stored explicitly in member variables. The last\n# two are stored implicitly in the pending_switch_proposals set as:\n#   (state of 4) == (_SWITCH_CONNECT in pending_switch_proposals)\n#   (state of 5) == (_SWITCH_UPGRADE in pending_switch_proposals)\n#\n# And each of these machines has two different kinds of transitions:\n#\n# a) Event-triggered\n# b) State-triggered\n#\n# Event triggered is the obvious thing that you'd think it is: some event\n# happens, and if it's the right event at the right time then a transition\n# happens. But there are somewhat complicated rules for which machines can\n# \"see\" which events. (As a rule of thumb, if a machine \"sees\" an event, this\n# means two things: the event can affect the machine, and if the machine is\n# not in a state where it expects that event then it's an error.) These rules\n# are:\n#\n# 1) The client machine sees all h11.events objects emitted by the client.\n#\n# 2) The server machine sees all h11.events objects emitted by the server.\n#\n#    It also sees the client's Request event.\n#\n#    And sometimes, server events are annotated with a _SWITCH_* event. For\n#    example, we can have a (Response, _SWITCH_CONNECT) event, which is\n#    different from a regular Response event.\n#\n# 3) The keep-alive machine sees the process_keep_alive_disabled() event\n#    (which is derived from Request/Response events), and this event\n#    transitions it from True -> False, or from False -> False. There's no way\n#    to transition back.\n#\n# 4&5) The _SWITCH_* machines transition from False->True when we get a\n#    Request that proposes the relevant type of switch (via\n#    process_client_switch_proposals), and they go from True->False when we\n#    get a Response that has no _SWITCH_* annotation.\n#\n# So that's event-triggered transitions.\n#\n# State-triggered transitions are less standard. What they do here is couple\n# the machines together. The way this works is, when certain *joint*\n# configurations of states are achieved, then we automatically transition to a\n# new *joint* state. So, for example, if we're ever in a joint state with\n#\n#   client: DONE\n#   keep-alive: False\n#\n# then the client state immediately transitions to:\n#\n#   client: MUST_CLOSE\n#\n# This is fundamentally different from an event-based transition, because it\n# doesn't matter how we arrived at the {client: DONE, keep-alive: False} state\n# -- maybe the client transitioned SEND_BODY -> DONE, or keep-alive\n# transitioned True -> False. Either way, once this precondition is satisfied,\n# this transition is immediately triggered.\n#\n# What if two conflicting state-based transitions get enabled at the same\n# time?  In practice there's only one case where this arises (client DONE ->\n# MIGHT_SWITCH_PROTOCOL versus DONE -> MUST_CLOSE), and we resolve it by\n# explicitly prioritizing the DONE -> MIGHT_SWITCH_PROTOCOL transition.\n#\n# Implementation\n# --------------\n#\n# The event-triggered transitions for the server and client machines are all\n# stored explicitly in a table. Ditto for the state-triggered transitions that\n# involve just the server and client state.\n#\n# The transitions for the other machines, and the state-triggered transitions\n# that involve the other machines, are written out as explicit Python code.\n#\n# It'd be nice if there were some cleaner way to do all this. This isn't\n# *too* terrible, but I feel like it could probably be better.\n#\n# WARNING\n# -------\n#\n# The script that generates the state machine diagrams for the docs knows how\n# to read out the EVENT_TRIGGERED_TRANSITIONS and STATE_TRIGGERED_TRANSITIONS\n# tables. But it can't automatically read the transitions that are written\n# directly in Python code. So if you touch those, you need to also update the\n# script to keep it in sync!\nfrom typing import cast, Dict, Optional, Set, Tuple, Type, Union\n\nfrom ._events import *\nfrom ._util import LocalProtocolError, Sentinel\n\n# Everything in __all__ gets re-exported as part of the h11 public API.\n__all__ = [\n    \"CLIENT\",\n    \"SERVER\",\n    \"IDLE\",\n    \"SEND_RESPONSE\",\n    \"SEND_BODY\",\n    \"DONE\",\n    \"MUST_CLOSE\",\n    \"CLOSED\",\n    \"MIGHT_SWITCH_PROTOCOL\",\n    \"SWITCHED_PROTOCOL\",\n    \"ERROR\",\n]\n\n\nclass CLIENT(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SERVER(Sentinel, metaclass=Sentinel):\n    pass\n\n\n# States\nclass IDLE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SEND_RESPONSE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SEND_BODY(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass DONE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass MUST_CLOSE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass CLOSED(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass ERROR(Sentinel, metaclass=Sentinel):\n    pass\n\n\n# Switch types\nclass MIGHT_SWITCH_PROTOCOL(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass SWITCHED_PROTOCOL(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass _SWITCH_UPGRADE(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass _SWITCH_CONNECT(Sentinel, metaclass=Sentinel):\n    pass\n\n\nEventTransitionType = Dict[\n    Type[Sentinel],\n    Dict[\n        Type[Sentinel],\n        Dict[Union[Type[Event], Tuple[Type[Event], Type[Sentinel]]], Type[Sentinel]],\n    ],\n]\n\nEVENT_TRIGGERED_TRANSITIONS: EventTransitionType = {\n    CLIENT: {\n        IDLE: {Request: SEND_BODY, ConnectionClosed: CLOSED},\n        SEND_BODY: {Data: SEND_BODY, EndOfMessage: DONE},\n        DONE: {ConnectionClosed: CLOSED},\n        MUST_CLOSE: {ConnectionClosed: CLOSED},\n        CLOSED: {ConnectionClosed: CLOSED},\n        MIGHT_SWITCH_PROTOCOL: {},\n        SWITCHED_PROTOCOL: {},\n        ERROR: {},\n    },\n    SERVER: {\n        IDLE: {\n            ConnectionClosed: CLOSED,\n            Response: SEND_BODY,\n            # Special case: server sees client Request events, in this form\n            (Request, CLIENT): SEND_RESPONSE,\n        },\n        SEND_RESPONSE: {\n            InformationalResponse: SEND_RESPONSE,\n            Response: SEND_BODY,\n            (InformationalResponse, _SWITCH_UPGRADE): SWITCHED_PROTOCOL,\n            (Response, _SWITCH_CONNECT): SWITCHED_PROTOCOL,\n        },\n        SEND_BODY: {Data: SEND_BODY, EndOfMessage: DONE},\n        DONE: {ConnectionClosed: CLOSED},\n        MUST_CLOSE: {ConnectionClosed: CLOSED},\n        CLOSED: {ConnectionClosed: CLOSED},\n        SWITCHED_PROTOCOL: {},\n        ERROR: {},\n    },\n}\n\nStateTransitionType = Dict[\n    Tuple[Type[Sentinel], Type[Sentinel]], Dict[Type[Sentinel], Type[Sentinel]]\n]\n\n# NB: there are also some special-case state-triggered transitions hard-coded\n# into _fire_state_triggered_transitions below.\nSTATE_TRIGGERED_TRANSITIONS: StateTransitionType = {\n    # (Client state, Server state) -> new states\n    # Protocol negotiation\n    (MIGHT_SWITCH_PROTOCOL, SWITCHED_PROTOCOL): {CLIENT: SWITCHED_PROTOCOL},\n    # Socket shutdown\n    (CLOSED, DONE): {SERVER: MUST_CLOSE},\n    (CLOSED, IDLE): {SERVER: MUST_CLOSE},\n    (ERROR, DONE): {SERVER: MUST_CLOSE},\n    (DONE, CLOSED): {CLIENT: MUST_CLOSE},\n    (IDLE, CLOSED): {CLIENT: MUST_CLOSE},\n    (DONE, ERROR): {CLIENT: MUST_CLOSE},\n}\n\n\nclass ConnectionState:\n    def __init__(self) -> None:\n        # Extra bits of state that don't quite fit into the state model.\n\n        # If this is False then it enables the automatic DONE -> MUST_CLOSE\n        # transition. Don't set this directly; call .keep_alive_disabled()\n        self.keep_alive = True\n\n        # This is a subset of {UPGRADE, CONNECT}, containing the proposals\n        # made by the client for switching protocols.\n        self.pending_switch_proposals: Set[Type[Sentinel]] = set()\n\n        self.states: Dict[Type[Sentinel], Type[Sentinel]] = {CLIENT: IDLE, SERVER: IDLE}\n\n    def process_error(self, role: Type[Sentinel]) -> None:\n        self.states[role] = ERROR\n        self._fire_state_triggered_transitions()\n\n    def process_keep_alive_disabled(self) -> None:\n        self.keep_alive = False\n        self._fire_state_triggered_transitions()\n\n    def process_client_switch_proposal(self, switch_event: Type[Sentinel]) -> None:\n        self.pending_switch_proposals.add(switch_event)\n        self._fire_state_triggered_transitions()\n\n    def process_event(\n        self,\n        role: Type[Sentinel],\n        event_type: Type[Event],\n        server_switch_event: Optional[Type[Sentinel]] = None,\n    ) -> None:\n        _event_type: Union[Type[Event], Tuple[Type[Event], Type[Sentinel]]] = event_type\n        if server_switch_event is not None:\n            assert role is SERVER\n            if server_switch_event not in self.pending_switch_proposals:\n                raise LocalProtocolError(\n                    \"Received server _SWITCH_UPGRADE event without a pending proposal\"\n                )\n            _event_type = (event_type, server_switch_event)\n        if server_switch_event is None and _event_type is Response:\n            self.pending_switch_proposals = set()\n        self._fire_event_triggered_transitions(role, _event_type)\n        # Special case: the server state does get to see Request\n        # events.\n        if _event_type is Request:\n            assert role is CLIENT\n            self._fire_event_triggered_transitions(SERVER, (Request, CLIENT))\n        self._fire_state_triggered_transitions()\n\n    def _fire_event_triggered_transitions(\n        self,\n        role: Type[Sentinel],\n        event_type: Union[Type[Event], Tuple[Type[Event], Type[Sentinel]]],\n    ) -> None:\n        state = self.states[role]\n        try:\n            new_state = EVENT_TRIGGERED_TRANSITIONS[role][state][event_type]\n        except KeyError:\n            event_type = cast(Type[Event], event_type)\n            raise LocalProtocolError(\n                \"can't handle event type {} when role={} and state={}\".format(\n                    event_type.__name__, role, self.states[role]\n                )\n            ) from None\n        self.states[role] = new_state\n\n    def _fire_state_triggered_transitions(self) -> None:\n        # We apply these rules repeatedly until converging on a fixed point\n        while True:\n            start_states = dict(self.states)\n\n            # It could happen that both these special-case transitions are\n            # enabled at the same time:\n            #\n            #    DONE -> MIGHT_SWITCH_PROTOCOL\n            #    DONE -> MUST_CLOSE\n            #\n            # For example, this will always be true of a HTTP/1.0 client\n            # requesting CONNECT.  If this happens, the protocol switch takes\n            # priority. From there the client will either go to\n            # SWITCHED_PROTOCOL, in which case it's none of our business when\n            # they close the connection, or else the server will deny the\n            # request, in which case the client will go back to DONE and then\n            # from there to MUST_CLOSE.\n            if self.pending_switch_proposals:\n                if self.states[CLIENT] is DONE:\n                    self.states[CLIENT] = MIGHT_SWITCH_PROTOCOL\n\n            if not self.pending_switch_proposals:\n                if self.states[CLIENT] is MIGHT_SWITCH_PROTOCOL:\n                    self.states[CLIENT] = DONE\n\n            if not self.keep_alive:\n                for role in (CLIENT, SERVER):\n                    if self.states[role] is DONE:\n                        self.states[role] = MUST_CLOSE\n\n            # Tabular state-triggered transitions\n            joint_state = (self.states[CLIENT], self.states[SERVER])\n            changes = STATE_TRIGGERED_TRANSITIONS.get(joint_state, {})\n            self.states.update(changes)\n\n            if self.states == start_states:\n                # Fixed point reached\n                return\n\n    def start_next_cycle(self) -> None:\n        if self.states != {CLIENT: DONE, SERVER: DONE}:\n            raise LocalProtocolError(\n                f\"not in a reusable state. self.states={self.states}\"\n            )\n        # Can't reach DONE/DONE with any of these active, but still, let's be\n        # sure.\n        assert self.keep_alive\n        assert not self.pending_switch_proposals\n        self.states = {CLIENT: IDLE, SERVER: IDLE}\n", "h11/_headers.py": "import re\nfrom typing import AnyStr, cast, List, overload, Sequence, Tuple, TYPE_CHECKING, Union\n\nfrom ._abnf import field_name, field_value\nfrom ._util import bytesify, LocalProtocolError, validate\n\nif TYPE_CHECKING:\n    from ._events import Request\n\ntry:\n    from typing import Literal\nexcept ImportError:\n    from typing_extensions import Literal  # type: ignore\n\n\n# Facts\n# -----\n#\n# Headers are:\n#   keys: case-insensitive ascii\n#   values: mixture of ascii and raw bytes\n#\n# \"Historically, HTTP has allowed field content with text in the ISO-8859-1\n# charset [ISO-8859-1], supporting other charsets only through use of\n# [RFC2047] encoding.  In practice, most HTTP header field values use only a\n# subset of the US-ASCII charset [USASCII]. Newly defined header fields SHOULD\n# limit their field values to US-ASCII octets.  A recipient SHOULD treat other\n# octets in field content (obs-text) as opaque data.\"\n# And it deprecates all non-ascii values\n#\n# Leading/trailing whitespace in header names is forbidden\n#\n# Values get leading/trailing whitespace stripped\n#\n# Content-Disposition actually needs to contain unicode semantically; to\n# accomplish this it has a terrifically weird way of encoding the filename\n# itself as ascii (and even this still has lots of cross-browser\n# incompatibilities)\n#\n# Order is important:\n# \"a proxy MUST NOT change the order of these field values when forwarding a\n# message\"\n# (and there are several headers where the order indicates a preference)\n#\n# Multiple occurences of the same header:\n# \"A sender MUST NOT generate multiple header fields with the same field name\n# in a message unless either the entire field value for that header field is\n# defined as a comma-separated list [or the header is Set-Cookie which gets a\n# special exception]\" - RFC 7230. (cookies are in RFC 6265)\n#\n# So every header aside from Set-Cookie can be merged by b\", \".join if it\n# occurs repeatedly. But, of course, they can't necessarily be split by\n# .split(b\",\"), because quoting.\n#\n# Given all this mess (case insensitive, duplicates allowed, order is\n# important, ...), there doesn't appear to be any standard way to handle\n# headers in Python -- they're almost like dicts, but... actually just\n# aren't. For now we punt and just use a super simple representation: headers\n# are a list of pairs\n#\n#   [(name1, value1), (name2, value2), ...]\n#\n# where all entries are bytestrings, names are lowercase and have no\n# leading/trailing whitespace, and values are bytestrings with no\n# leading/trailing whitespace. Searching and updating are done via naive O(n)\n# methods.\n#\n# Maybe a dict-of-lists would be better?\n\n_content_length_re = re.compile(rb\"[0-9]+\")\n_field_name_re = re.compile(field_name.encode(\"ascii\"))\n_field_value_re = re.compile(field_value.encode(\"ascii\"))\n\n\nclass Headers(Sequence[Tuple[bytes, bytes]]):\n    \"\"\"\n    A list-like interface that allows iterating over headers as byte-pairs\n    of (lowercased-name, value).\n\n    Internally we actually store the representation as three-tuples,\n    including both the raw original casing, in order to preserve casing\n    over-the-wire, and the lowercased name, for case-insensitive comparisions.\n\n    r = Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"Host\", \"example.org\"), (\"Connection\", \"keep-alive\")],\n        http_version=\"1.1\",\n    )\n    assert r.headers == [\n        (b\"host\", b\"example.org\"),\n        (b\"connection\", b\"keep-alive\")\n    ]\n    assert r.headers.raw_items() == [\n        (b\"Host\", b\"example.org\"),\n        (b\"Connection\", b\"keep-alive\")\n    ]\n    \"\"\"\n\n    __slots__ = \"_full_items\"\n\n    def __init__(self, full_items: List[Tuple[bytes, bytes, bytes]]) -> None:\n        self._full_items = full_items\n\n    def __bool__(self) -> bool:\n        return bool(self._full_items)\n\n    def __eq__(self, other: object) -> bool:\n        return list(self) == list(other)  # type: ignore\n\n    def __len__(self) -> int:\n        return len(self._full_items)\n\n    def __repr__(self) -> str:\n        return \"<Headers(%s)>\" % repr(list(self))\n\n    def __getitem__(self, idx: int) -> Tuple[bytes, bytes]:  # type: ignore[override]\n        _, name, value = self._full_items[idx]\n        return (name, value)\n\n    def raw_items(self) -> List[Tuple[bytes, bytes]]:\n        return [(raw_name, value) for raw_name, _, value in self._full_items]\n\n\nHeaderTypes = Union[\n    List[Tuple[bytes, bytes]],\n    List[Tuple[bytes, str]],\n    List[Tuple[str, bytes]],\n    List[Tuple[str, str]],\n]\n\n\n@overload\ndef normalize_and_validate(headers: Headers, _parsed: Literal[True]) -> Headers:\n    ...\n\n\n@overload\ndef normalize_and_validate(headers: HeaderTypes, _parsed: Literal[False]) -> Headers:\n    ...\n\n\n@overload\ndef normalize_and_validate(\n    headers: Union[Headers, HeaderTypes], _parsed: bool = False\n) -> Headers:\n    ...\n\n\ndef normalize_and_validate(\n    headers: Union[Headers, HeaderTypes], _parsed: bool = False\n) -> Headers:\n    new_headers = []\n    seen_content_length = None\n    saw_transfer_encoding = False\n    for name, value in headers:\n        # For headers coming out of the parser, we can safely skip some steps,\n        # because it always returns bytes and has already run these regexes\n        # over the data:\n        if not _parsed:\n            name = bytesify(name)\n            value = bytesify(value)\n            validate(_field_name_re, name, \"Illegal header name {!r}\", name)\n            validate(_field_value_re, value, \"Illegal header value {!r}\", value)\n        assert isinstance(name, bytes)\n        assert isinstance(value, bytes)\n\n        raw_name = name\n        name = name.lower()\n        if name == b\"content-length\":\n            lengths = {length.strip() for length in value.split(b\",\")}\n            if len(lengths) != 1:\n                raise LocalProtocolError(\"conflicting Content-Length headers\")\n            value = lengths.pop()\n            validate(_content_length_re, value, \"bad Content-Length\")\n            if seen_content_length is None:\n                seen_content_length = value\n                new_headers.append((raw_name, name, value))\n            elif seen_content_length != value:\n                raise LocalProtocolError(\"conflicting Content-Length headers\")\n        elif name == b\"transfer-encoding\":\n            # \"A server that receives a request message with a transfer coding\n            # it does not understand SHOULD respond with 501 (Not\n            # Implemented).\"\n            # https://tools.ietf.org/html/rfc7230#section-3.3.1\n            if saw_transfer_encoding:\n                raise LocalProtocolError(\n                    \"multiple Transfer-Encoding headers\", error_status_hint=501\n                )\n            # \"All transfer-coding names are case-insensitive\"\n            # -- https://tools.ietf.org/html/rfc7230#section-4\n            value = value.lower()\n            if value != b\"chunked\":\n                raise LocalProtocolError(\n                    \"Only Transfer-Encoding: chunked is supported\",\n                    error_status_hint=501,\n                )\n            saw_transfer_encoding = True\n            new_headers.append((raw_name, name, value))\n        else:\n            new_headers.append((raw_name, name, value))\n    return Headers(new_headers)\n\n\ndef get_comma_header(headers: Headers, name: bytes) -> List[bytes]:\n    # Should only be used for headers whose value is a list of\n    # comma-separated, case-insensitive values.\n    #\n    # The header name `name` is expected to be lower-case bytes.\n    #\n    # Connection: meets these criteria (including cast insensitivity).\n    #\n    # Content-Length: technically is just a single value (1*DIGIT), but the\n    # standard makes reference to implementations that do multiple values, and\n    # using this doesn't hurt. Ditto, case insensitivity doesn't things either\n    # way.\n    #\n    # Transfer-Encoding: is more complex (allows for quoted strings), so\n    # splitting on , is actually wrong. For example, this is legal:\n    #\n    #    Transfer-Encoding: foo; options=\"1,2\", chunked\n    #\n    # and should be parsed as\n    #\n    #    foo; options=\"1,2\"\n    #    chunked\n    #\n    # but this naive function will parse it as\n    #\n    #    foo; options=\"1\n    #    2\"\n    #    chunked\n    #\n    # However, this is okay because the only thing we are going to do with\n    # any Transfer-Encoding is reject ones that aren't just \"chunked\", so\n    # both of these will be treated the same anyway.\n    #\n    # Expect: the only legal value is the literal string\n    # \"100-continue\". Splitting on commas is harmless. Case insensitive.\n    #\n    out: List[bytes] = []\n    for _, found_name, found_raw_value in headers._full_items:\n        if found_name == name:\n            found_raw_value = found_raw_value.lower()\n            for found_split_value in found_raw_value.split(b\",\"):\n                found_split_value = found_split_value.strip()\n                if found_split_value:\n                    out.append(found_split_value)\n    return out\n\n\ndef set_comma_header(headers: Headers, name: bytes, new_values: List[bytes]) -> Headers:\n    # The header name `name` is expected to be lower-case bytes.\n    #\n    # Note that when we store the header we use title casing for the header\n    # names, in order to match the conventional HTTP header style.\n    #\n    # Simply calling `.title()` is a blunt approach, but it's correct\n    # here given the cases where we're using `set_comma_header`...\n    #\n    # Connection, Content-Length, Transfer-Encoding.\n    new_headers: List[Tuple[bytes, bytes]] = []\n    for found_raw_name, found_name, found_raw_value in headers._full_items:\n        if found_name != name:\n            new_headers.append((found_raw_name, found_raw_value))\n    for new_value in new_values:\n        new_headers.append((name.title(), new_value))\n    return normalize_and_validate(new_headers)\n\n\ndef has_expect_100_continue(request: \"Request\") -> bool:\n    # https://tools.ietf.org/html/rfc7231#section-5.1.1\n    # \"A server that receives a 100-continue expectation in an HTTP/1.0 request\n    # MUST ignore that expectation.\"\n    if request.http_version < b\"1.1\":\n        return False\n    expect = get_comma_header(request.headers, b\"expect\")\n    return b\"100-continue\" in expect\n", "h11/_util.py": "from typing import Any, Dict, NoReturn, Pattern, Tuple, Type, TypeVar, Union\n\n__all__ = [\n    \"ProtocolError\",\n    \"LocalProtocolError\",\n    \"RemoteProtocolError\",\n    \"validate\",\n    \"bytesify\",\n]\n\n\nclass ProtocolError(Exception):\n    \"\"\"Exception indicating a violation of the HTTP/1.1 protocol.\n\n    This as an abstract base class, with two concrete base classes:\n    :exc:`LocalProtocolError`, which indicates that you tried to do something\n    that HTTP/1.1 says is illegal, and :exc:`RemoteProtocolError`, which\n    indicates that the remote peer tried to do something that HTTP/1.1 says is\n    illegal. See :ref:`error-handling` for details.\n\n    In addition to the normal :exc:`Exception` features, it has one attribute:\n\n    .. attribute:: error_status_hint\n\n       This gives a suggestion as to what status code a server might use if\n       this error occurred as part of a request.\n\n       For a :exc:`RemoteProtocolError`, this is useful as a suggestion for\n       how you might want to respond to a misbehaving peer, if you're\n       implementing a server.\n\n       For a :exc:`LocalProtocolError`, this can be taken as a suggestion for\n       how your peer might have responded to *you* if h11 had allowed you to\n       continue.\n\n       The default is 400 Bad Request, a generic catch-all for protocol\n       violations.\n\n    \"\"\"\n\n    def __init__(self, msg: str, error_status_hint: int = 400) -> None:\n        if type(self) is ProtocolError:\n            raise TypeError(\"tried to directly instantiate ProtocolError\")\n        Exception.__init__(self, msg)\n        self.error_status_hint = error_status_hint\n\n\n# Strategy: there are a number of public APIs where a LocalProtocolError can\n# be raised (send(), all the different event constructors, ...), and only one\n# public API where RemoteProtocolError can be raised\n# (receive_data()). Therefore we always raise LocalProtocolError internally,\n# and then receive_data will translate this into a RemoteProtocolError.\n#\n# Internally:\n#   LocalProtocolError is the generic \"ProtocolError\".\n# Externally:\n#   LocalProtocolError is for local errors and RemoteProtocolError is for\n#   remote errors.\nclass LocalProtocolError(ProtocolError):\n    def _reraise_as_remote_protocol_error(self) -> NoReturn:\n        # After catching a LocalProtocolError, use this method to re-raise it\n        # as a RemoteProtocolError. This method must be called from inside an\n        # except: block.\n        #\n        # An easy way to get an equivalent RemoteProtocolError is just to\n        # modify 'self' in place.\n        self.__class__ = RemoteProtocolError  # type: ignore\n        # But the re-raising is somewhat non-trivial -- you might think that\n        # now that we've modified the in-flight exception object, that just\n        # doing 'raise' to re-raise it would be enough. But it turns out that\n        # this doesn't work, because Python tracks the exception type\n        # (exc_info[0]) separately from the exception object (exc_info[1]),\n        # and we only modified the latter. So we really do need to re-raise\n        # the new type explicitly.\n        # On py3, the traceback is part of the exception object, so our\n        # in-place modification preserved it and we can just re-raise:\n        raise self\n\n\nclass RemoteProtocolError(ProtocolError):\n    pass\n\n\ndef validate(\n    regex: Pattern[bytes], data: bytes, msg: str = \"malformed data\", *format_args: Any\n) -> Dict[str, bytes]:\n    match = regex.fullmatch(data)\n    if not match:\n        if format_args:\n            msg = msg.format(*format_args)\n        raise LocalProtocolError(msg)\n    return match.groupdict()\n\n\n# Sentinel values\n#\n# - Inherit identity-based comparison and hashing from object\n# - Have a nice repr\n# - Have a *bonus property*: type(sentinel) is sentinel\n#\n# The bonus property is useful if you want to take the return value from\n# next_event() and do some sort of dispatch based on type(event).\n\n_T_Sentinel = TypeVar(\"_T_Sentinel\", bound=\"Sentinel\")\n\n\nclass Sentinel(type):\n    def __new__(\n        cls: Type[_T_Sentinel],\n        name: str,\n        bases: Tuple[type, ...],\n        namespace: Dict[str, Any],\n        **kwds: Any\n    ) -> _T_Sentinel:\n        assert bases == (Sentinel,)\n        v = super().__new__(cls, name, bases, namespace, **kwds)\n        v.__class__ = v  # type: ignore\n        return v\n\n    def __repr__(self) -> str:\n        return self.__name__\n\n\n# Used for methods, request targets, HTTP versions, header names, and header\n# values. Accepts ascii-strings, or bytes/bytearray/memoryview/..., and always\n# returns bytes.\ndef bytesify(s: Union[bytes, bytearray, memoryview, int, str]) -> bytes:\n    # Fast-path:\n    if type(s) is bytes:\n        return s\n    if isinstance(s, str):\n        s = s.encode(\"ascii\")\n    if isinstance(s, int):\n        raise TypeError(\"expected bytes-like object, not int\")\n    return bytes(s)\n", "h11/_events.py": "# High level events that make up HTTP/1.1 conversations. Loosely inspired by\n# the corresponding events in hyper-h2:\n#\n#     http://python-hyper.org/h2/en/stable/api.html#events\n#\n# Don't subclass these. Stuff will break.\n\nimport re\nfrom abc import ABC\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Union\n\nfrom ._abnf import method, request_target\nfrom ._headers import Headers, normalize_and_validate\nfrom ._util import bytesify, LocalProtocolError, validate\n\n# Everything in __all__ gets re-exported as part of the h11 public API.\n__all__ = [\n    \"Event\",\n    \"Request\",\n    \"InformationalResponse\",\n    \"Response\",\n    \"Data\",\n    \"EndOfMessage\",\n    \"ConnectionClosed\",\n]\n\nmethod_re = re.compile(method.encode(\"ascii\"))\nrequest_target_re = re.compile(request_target.encode(\"ascii\"))\n\n\nclass Event(ABC):\n    \"\"\"\n    Base class for h11 events.\n    \"\"\"\n\n    __slots__ = ()\n\n\n@dataclass(init=False, frozen=True)\nclass Request(Event):\n    \"\"\"The beginning of an HTTP request.\n\n    Fields:\n\n    .. attribute:: method\n\n       An HTTP method, e.g. ``b\"GET\"`` or ``b\"POST\"``. Always a byte\n       string. :term:`Bytes-like objects <bytes-like object>` and native\n       strings containing only ascii characters will be automatically\n       converted to byte strings.\n\n    .. attribute:: target\n\n       The target of an HTTP request, e.g. ``b\"/index.html\"``, or one of the\n       more exotic formats described in `RFC 7320, section 5.3\n       <https://tools.ietf.org/html/rfc7230#section-5.3>`_. Always a byte\n       string. :term:`Bytes-like objects <bytes-like object>` and native\n       strings containing only ascii characters will be automatically\n       converted to byte strings.\n\n    .. attribute:: headers\n\n       Request headers, represented as a list of (name, value) pairs. See\n       :ref:`the header normalization rules <headers-format>` for details.\n\n    .. attribute:: http_version\n\n       The HTTP protocol version, represented as a byte string like\n       ``b\"1.1\"``. See :ref:`the HTTP version normalization rules\n       <http_version-format>` for details.\n\n    \"\"\"\n\n    __slots__ = (\"method\", \"headers\", \"target\", \"http_version\")\n\n    method: bytes\n    headers: Headers\n    target: bytes\n    http_version: bytes\n\n    def __init__(\n        self,\n        *,\n        method: Union[bytes, str],\n        headers: Union[Headers, List[Tuple[bytes, bytes]], List[Tuple[str, str]]],\n        target: Union[bytes, str],\n        http_version: Union[bytes, str] = b\"1.1\",\n        _parsed: bool = False,\n    ) -> None:\n        super().__init__()\n        if isinstance(headers, Headers):\n            object.__setattr__(self, \"headers\", headers)\n        else:\n            object.__setattr__(\n                self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\n            )\n        if not _parsed:\n            object.__setattr__(self, \"method\", bytesify(method))\n            object.__setattr__(self, \"target\", bytesify(target))\n            object.__setattr__(self, \"http_version\", bytesify(http_version))\n        else:\n            object.__setattr__(self, \"method\", method)\n            object.__setattr__(self, \"target\", target)\n            object.__setattr__(self, \"http_version\", http_version)\n\n        # \"A server MUST respond with a 400 (Bad Request) status code to any\n        # HTTP/1.1 request message that lacks a Host header field and to any\n        # request message that contains more than one Host header field or a\n        # Host header field with an invalid field-value.\"\n        # -- https://tools.ietf.org/html/rfc7230#section-5.4\n        host_count = 0\n        for name, value in self.headers:\n            if name == b\"host\":\n                host_count += 1\n        if self.http_version == b\"1.1\" and host_count == 0:\n            raise LocalProtocolError(\"Missing mandatory Host: header\")\n        if host_count > 1:\n            raise LocalProtocolError(\"Found multiple Host: headers\")\n\n        validate(method_re, self.method, \"Illegal method characters\")\n        validate(request_target_re, self.target, \"Illegal target characters\")\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass _ResponseBase(Event):\n    __slots__ = (\"headers\", \"http_version\", \"reason\", \"status_code\")\n\n    headers: Headers\n    http_version: bytes\n    reason: bytes\n    status_code: int\n\n    def __init__(\n        self,\n        *,\n        headers: Union[Headers, List[Tuple[bytes, bytes]], List[Tuple[str, str]]],\n        status_code: int,\n        http_version: Union[bytes, str] = b\"1.1\",\n        reason: Union[bytes, str] = b\"\",\n        _parsed: bool = False,\n    ) -> None:\n        super().__init__()\n        if isinstance(headers, Headers):\n            object.__setattr__(self, \"headers\", headers)\n        else:\n            object.__setattr__(\n                self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\n            )\n        if not _parsed:\n            object.__setattr__(self, \"reason\", bytesify(reason))\n            object.__setattr__(self, \"http_version\", bytesify(http_version))\n            if not isinstance(status_code, int):\n                raise LocalProtocolError(\"status code must be integer\")\n            # Because IntEnum objects are instances of int, but aren't\n            # duck-compatible (sigh), see gh-72.\n            object.__setattr__(self, \"status_code\", int(status_code))\n        else:\n            object.__setattr__(self, \"reason\", reason)\n            object.__setattr__(self, \"http_version\", http_version)\n            object.__setattr__(self, \"status_code\", status_code)\n\n        self.__post_init__()\n\n    def __post_init__(self) -> None:\n        pass\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass InformationalResponse(_ResponseBase):\n    \"\"\"An HTTP informational response.\n\n    Fields:\n\n    .. attribute:: status_code\n\n       The status code of this response, as an integer. For an\n       :class:`InformationalResponse`, this is always in the range [100,\n       200).\n\n    .. attribute:: headers\n\n       Request headers, represented as a list of (name, value) pairs. See\n       :ref:`the header normalization rules <headers-format>` for\n       details.\n\n    .. attribute:: http_version\n\n       The HTTP protocol version, represented as a byte string like\n       ``b\"1.1\"``. See :ref:`the HTTP version normalization rules\n       <http_version-format>` for details.\n\n    .. attribute:: reason\n\n       The reason phrase of this response, as a byte string. For example:\n       ``b\"OK\"``, or ``b\"Not Found\"``.\n\n    \"\"\"\n\n    def __post_init__(self) -> None:\n        if not (100 <= self.status_code < 200):\n            raise LocalProtocolError(\n                \"InformationalResponse status_code should be in range \"\n                \"[100, 200), not {}\".format(self.status_code)\n            )\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass Response(_ResponseBase):\n    \"\"\"The beginning of an HTTP response.\n\n    Fields:\n\n    .. attribute:: status_code\n\n       The status code of this response, as an integer. For an\n       :class:`Response`, this is always in the range [200,\n       1000).\n\n    .. attribute:: headers\n\n       Request headers, represented as a list of (name, value) pairs. See\n       :ref:`the header normalization rules <headers-format>` for details.\n\n    .. attribute:: http_version\n\n       The HTTP protocol version, represented as a byte string like\n       ``b\"1.1\"``. See :ref:`the HTTP version normalization rules\n       <http_version-format>` for details.\n\n    .. attribute:: reason\n\n       The reason phrase of this response, as a byte string. For example:\n       ``b\"OK\"``, or ``b\"Not Found\"``.\n\n    \"\"\"\n\n    def __post_init__(self) -> None:\n        if not (200 <= self.status_code < 1000):\n            raise LocalProtocolError(\n                \"Response status_code should be in range [200, 1000), not {}\".format(\n                    self.status_code\n                )\n            )\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(init=False, frozen=True)\nclass Data(Event):\n    \"\"\"Part of an HTTP message body.\n\n    Fields:\n\n    .. attribute:: data\n\n       A :term:`bytes-like object` containing part of a message body. Or, if\n       using the ``combine=False`` argument to :meth:`Connection.send`, then\n       any object that your socket writing code knows what to do with, and for\n       which calling :func:`len` returns the number of bytes that will be\n       written -- see :ref:`sendfile` for details.\n\n    .. attribute:: chunk_start\n\n       A marker that indicates whether this data object is from the start of a\n       chunked transfer encoding chunk. This field is ignored when when a Data\n       event is provided to :meth:`Connection.send`: it is only valid on\n       events emitted from :meth:`Connection.next_event`. You probably\n       shouldn't use this attribute at all; see\n       :ref:`chunk-delimiters-are-bad` for details.\n\n    .. attribute:: chunk_end\n\n       A marker that indicates whether this data object is the last for a\n       given chunked transfer encoding chunk. This field is ignored when when\n       a Data event is provided to :meth:`Connection.send`: it is only valid\n       on events emitted from :meth:`Connection.next_event`. You probably\n       shouldn't use this attribute at all; see\n       :ref:`chunk-delimiters-are-bad` for details.\n\n    \"\"\"\n\n    __slots__ = (\"data\", \"chunk_start\", \"chunk_end\")\n\n    data: bytes\n    chunk_start: bool\n    chunk_end: bool\n\n    def __init__(\n        self, data: bytes, chunk_start: bool = False, chunk_end: bool = False\n    ) -> None:\n        object.__setattr__(self, \"data\", data)\n        object.__setattr__(self, \"chunk_start\", chunk_start)\n        object.__setattr__(self, \"chunk_end\", chunk_end)\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n# XX FIXME: \"A recipient MUST ignore (or consider as an error) any fields that\n# are forbidden to be sent in a trailer, since processing them as if they were\n# present in the header section might bypass external security filters.\"\n# https://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7230.html#chunked.trailer.part\n# Unfortunately, the list of forbidden fields is long and vague :-/\n@dataclass(init=False, frozen=True)\nclass EndOfMessage(Event):\n    \"\"\"The end of an HTTP message.\n\n    Fields:\n\n    .. attribute:: headers\n\n       Default value: ``[]``\n\n       Any trailing headers attached to this message, represented as a list of\n       (name, value) pairs. See :ref:`the header normalization rules\n       <headers-format>` for details.\n\n       Must be empty unless ``Transfer-Encoding: chunked`` is in use.\n\n    \"\"\"\n\n    __slots__ = (\"headers\",)\n\n    headers: Headers\n\n    def __init__(\n        self,\n        *,\n        headers: Union[\n            Headers, List[Tuple[bytes, bytes]], List[Tuple[str, str]], None\n        ] = None,\n        _parsed: bool = False,\n    ) -> None:\n        super().__init__()\n        if headers is None:\n            headers = Headers([])\n        elif not isinstance(headers, Headers):\n            headers = normalize_and_validate(headers, _parsed=_parsed)\n\n        object.__setattr__(self, \"headers\", headers)\n\n    # This is an unhashable type.\n    __hash__ = None  # type: ignore\n\n\n@dataclass(frozen=True)\nclass ConnectionClosed(Event):\n    \"\"\"This event indicates that the sender has closed their outgoing\n    connection.\n\n    Note that this does not necessarily mean that they can't *receive* further\n    data, because TCP connections are composed to two one-way channels which\n    can be closed independently. See :ref:`closing` for details.\n\n    No fields.\n    \"\"\"\n\n    pass\n", "h11/__init__.py": "# A highish-level implementation of the HTTP/1.1 wire protocol (RFC 7230),\n# containing no networking code at all, loosely modelled on hyper-h2's generic\n# implementation of HTTP/2 (and in particular the h2.connection.H2Connection\n# class). There's still a bunch of subtle details you need to get right if you\n# want to make this actually useful, because it doesn't implement all the\n# semantics to check that what you're asking to write to the wire is sensible,\n# but at least it gets you out of dealing with the wire itself.\n\nfrom h11._connection import Connection, NEED_DATA, PAUSED\nfrom h11._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom h11._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    ERROR,\n    IDLE,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom h11._util import LocalProtocolError, ProtocolError, RemoteProtocolError\nfrom h11._version import __version__\n\nPRODUCT_ID = \"python-h11/\" + __version__\n\n\n__all__ = (\n    \"Connection\",\n    \"NEED_DATA\",\n    \"PAUSED\",\n    \"ConnectionClosed\",\n    \"Data\",\n    \"EndOfMessage\",\n    \"Event\",\n    \"InformationalResponse\",\n    \"Request\",\n    \"Response\",\n    \"CLIENT\",\n    \"CLOSED\",\n    \"DONE\",\n    \"ERROR\",\n    \"IDLE\",\n    \"MUST_CLOSE\",\n    \"SEND_BODY\",\n    \"SEND_RESPONSE\",\n    \"SERVER\",\n    \"SWITCHED_PROTOCOL\",\n    \"ProtocolError\",\n    \"LocalProtocolError\",\n    \"RemoteProtocolError\",\n)\n", "h11/_writers.py": "# Code to read HTTP data\n#\n# Strategy: each writer takes an event + a write-some-bytes function, which is\n# calls.\n#\n# WRITERS is a dict describing how to pick a reader. It maps states to either:\n# - a writer\n# - or, for body writers, a dict of framin-dependent writer factories\n\nfrom typing import Any, Callable, Dict, List, Tuple, Type, Union\n\nfrom ._events import Data, EndOfMessage, Event, InformationalResponse, Request, Response\nfrom ._headers import Headers\nfrom ._state import CLIENT, IDLE, SEND_BODY, SEND_RESPONSE, SERVER\nfrom ._util import LocalProtocolError, Sentinel\n\n__all__ = [\"WRITERS\"]\n\nWriter = Callable[[bytes], Any]\n\n\ndef write_headers(headers: Headers, write: Writer) -> None:\n    # \"Since the Host field-value is critical information for handling a\n    # request, a user agent SHOULD generate Host as the first header field\n    # following the request-line.\" - RFC 7230\n    raw_items = headers._full_items\n    for raw_name, name, value in raw_items:\n        if name == b\"host\":\n            write(b\"%s: %s\\r\\n\" % (raw_name, value))\n    for raw_name, name, value in raw_items:\n        if name != b\"host\":\n            write(b\"%s: %s\\r\\n\" % (raw_name, value))\n    write(b\"\\r\\n\")\n\n\ndef write_request(request: Request, write: Writer) -> None:\n    if request.http_version != b\"1.1\":\n        raise LocalProtocolError(\"I only send HTTP/1.1\")\n    write(b\"%s %s HTTP/1.1\\r\\n\" % (request.method, request.target))\n    write_headers(request.headers, write)\n\n\n# Shared between InformationalResponse and Response\ndef write_any_response(\n    response: Union[InformationalResponse, Response], write: Writer\n) -> None:\n    if response.http_version != b\"1.1\":\n        raise LocalProtocolError(\"I only send HTTP/1.1\")\n    status_bytes = str(response.status_code).encode(\"ascii\")\n    # We don't bother sending ascii status messages like \"OK\"; they're\n    # optional and ignored by the protocol. (But the space after the numeric\n    # status code is mandatory.)\n    #\n    # XX FIXME: could at least make an effort to pull out the status message\n    # from stdlib's http.HTTPStatus table. Or maybe just steal their enums\n    # (either by import or copy/paste). We already accept them as status codes\n    # since they're of type IntEnum < int.\n    write(b\"HTTP/1.1 %s %s\\r\\n\" % (status_bytes, response.reason))\n    write_headers(response.headers, write)\n\n\nclass BodyWriter:\n    def __call__(self, event: Event, write: Writer) -> None:\n        if type(event) is Data:\n            self.send_data(event.data, write)\n        elif type(event) is EndOfMessage:\n            self.send_eom(event.headers, write)\n        else:  # pragma: no cover\n            assert False\n\n    def send_data(self, data: bytes, write: Writer) -> None:\n        pass\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        pass\n\n\n#\n# These are all careful not to do anything to 'data' except call len(data) and\n# write(data). This allows us to transparently pass-through funny objects,\n# like placeholder objects referring to files on disk that will be sent via\n# sendfile(2).\n#\nclass ContentLengthWriter(BodyWriter):\n    def __init__(self, length: int) -> None:\n        self._length = length\n\n    def send_data(self, data: bytes, write: Writer) -> None:\n        self._length -= len(data)\n        if self._length < 0:\n            raise LocalProtocolError(\"Too much data for declared Content-Length\")\n        write(data)\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        if self._length != 0:\n            raise LocalProtocolError(\"Too little data for declared Content-Length\")\n        if headers:\n            raise LocalProtocolError(\"Content-Length and trailers don't mix\")\n\n\nclass ChunkedWriter(BodyWriter):\n    def send_data(self, data: bytes, write: Writer) -> None:\n        # if we encoded 0-length data in the naive way, it would look like an\n        # end-of-message.\n        if not data:\n            return\n        write(b\"%x\\r\\n\" % len(data))\n        write(data)\n        write(b\"\\r\\n\")\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        write(b\"0\\r\\n\")\n        write_headers(headers, write)\n\n\nclass Http10Writer(BodyWriter):\n    def send_data(self, data: bytes, write: Writer) -> None:\n        write(data)\n\n    def send_eom(self, headers: Headers, write: Writer) -> None:\n        if headers:\n            raise LocalProtocolError(\"can't send trailers to HTTP/1.0 client\")\n        # no need to close the socket ourselves, that will be taken care of by\n        # Connection: close machinery\n\n\nWritersType = Dict[\n    Union[Tuple[Type[Sentinel], Type[Sentinel]], Type[Sentinel]],\n    Union[\n        Dict[str, Type[BodyWriter]],\n        Callable[[Union[InformationalResponse, Response], Writer], None],\n        Callable[[Request, Writer], None],\n    ],\n]\n\nWRITERS: WritersType = {\n    (CLIENT, IDLE): write_request,\n    (SERVER, IDLE): write_any_response,\n    (SERVER, SEND_RESPONSE): write_any_response,\n    SEND_BODY: {\n        \"chunked\": ChunkedWriter,\n        \"content-length\": ContentLengthWriter,\n        \"http/1.0\": Http10Writer,\n    },\n}\n", "h11/_connection.py": "# This contains the main Connection class. Everything in h11 revolves around\n# this.\nfrom typing import (\n    Any,\n    Callable,\n    cast,\n    Dict,\n    List,\n    Optional,\n    overload,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom ._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom ._headers import get_comma_header, has_expect_100_continue, set_comma_header\nfrom ._readers import READERS, ReadersType\nfrom ._receivebuffer import ReceiveBuffer\nfrom ._state import (\n    _SWITCH_CONNECT,\n    _SWITCH_UPGRADE,\n    CLIENT,\n    ConnectionState,\n    DONE,\n    ERROR,\n    MIGHT_SWITCH_PROTOCOL,\n    SEND_BODY,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom ._util import (  # Import the internal things we need\n    LocalProtocolError,\n    RemoteProtocolError,\n    Sentinel,\n)\nfrom ._writers import WRITERS, WritersType\n\n# Everything in __all__ gets re-exported as part of the h11 public API.\n__all__ = [\"Connection\", \"NEED_DATA\", \"PAUSED\"]\n\n\nclass NEED_DATA(Sentinel, metaclass=Sentinel):\n    pass\n\n\nclass PAUSED(Sentinel, metaclass=Sentinel):\n    pass\n\n\n# If we ever have this much buffered without it making a complete parseable\n# event, we error out. The only time we really buffer is when reading the\n# request/response line + headers together, so this is effectively the limit on\n# the size of that.\n#\n# Some precedents for defaults:\n# - node.js: 80 * 1024\n# - tomcat: 8 * 1024\n# - IIS: 16 * 1024\n# - Apache: <8 KiB per line>\nDEFAULT_MAX_INCOMPLETE_EVENT_SIZE = 16 * 1024\n\n\n# RFC 7230's rules for connection lifecycles:\n# - If either side says they want to close the connection, then the connection\n#   must close.\n# - HTTP/1.1 defaults to keep-alive unless someone says Connection: close\n# - HTTP/1.0 defaults to close unless both sides say Connection: keep-alive\n#   (and even this is a mess -- e.g. if you're implementing a proxy then\n#   sending Connection: keep-alive is forbidden).\n#\n# We simplify life by simply not supporting keep-alive with HTTP/1.0 peers. So\n# our rule is:\n# - If someone says Connection: close, we will close\n# - If someone uses HTTP/1.0, we will close.\ndef _keep_alive(event: Union[Request, Response]) -> bool:\n    connection = get_comma_header(event.headers, b\"connection\")\n    if b\"close\" in connection:\n        return False\n    if getattr(event, \"http_version\", b\"1.1\") < b\"1.1\":\n        return False\n    return True\n\n\ndef _body_framing(\n    request_method: bytes, event: Union[Request, Response]\n) -> Tuple[str, Union[Tuple[()], Tuple[int]]]:\n    # Called when we enter SEND_BODY to figure out framing information for\n    # this body.\n    #\n    # These are the only two events that can trigger a SEND_BODY state:\n    assert type(event) in (Request, Response)\n    # Returns one of:\n    #\n    #    (\"content-length\", count)\n    #    (\"chunked\", ())\n    #    (\"http/1.0\", ())\n    #\n    # which are (lookup key, *args) for constructing body reader/writer\n    # objects.\n    #\n    # Reference: https://tools.ietf.org/html/rfc7230#section-3.3.3\n    #\n    # Step 1: some responses always have an empty body, regardless of what the\n    # headers say.\n    if type(event) is Response:\n        if (\n            event.status_code in (204, 304)\n            or request_method == b\"HEAD\"\n            or (request_method == b\"CONNECT\" and 200 <= event.status_code < 300)\n        ):\n            return (\"content-length\", (0,))\n        # Section 3.3.3 also lists another case -- responses with status_code\n        # < 200. For us these are InformationalResponses, not Responses, so\n        # they can't get into this function in the first place.\n        assert event.status_code >= 200\n\n    # Step 2: check for Transfer-Encoding (T-E beats C-L):\n    transfer_encodings = get_comma_header(event.headers, b\"transfer-encoding\")\n    if transfer_encodings:\n        assert transfer_encodings == [b\"chunked\"]\n        return (\"chunked\", ())\n\n    # Step 3: check for Content-Length\n    content_lengths = get_comma_header(event.headers, b\"content-length\")\n    if content_lengths:\n        return (\"content-length\", (int(content_lengths[0]),))\n\n    # Step 4: no applicable headers; fallback/default depends on type\n    if type(event) is Request:\n        return (\"content-length\", (0,))\n    else:\n        return (\"http/1.0\", ())\n\n\n################################################################\n#\n# The main Connection class\n#\n################################################################\n\n\nclass Connection:\n    \"\"\"An object encapsulating the state of an HTTP connection.\n\n    Args:\n        our_role: If you're implementing a client, pass :data:`h11.CLIENT`. If\n            you're implementing a server, pass :data:`h11.SERVER`.\n\n        max_incomplete_event_size (int):\n            The maximum number of bytes we're willing to buffer of an\n            incomplete event. In practice this mostly sets a limit on the\n            maximum size of the request/response line + headers. If this is\n            exceeded, then :meth:`next_event` will raise\n            :exc:`RemoteProtocolError`.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        our_role: Type[Sentinel],\n        max_incomplete_event_size: int = DEFAULT_MAX_INCOMPLETE_EVENT_SIZE,\n    ) -> None:\n        self._max_incomplete_event_size = max_incomplete_event_size\n        # State and role tracking\n        if our_role not in (CLIENT, SERVER):\n            raise ValueError(f\"expected CLIENT or SERVER, not {our_role!r}\")\n        self.our_role = our_role\n        self.their_role: Type[Sentinel]\n        if our_role is CLIENT:\n            self.their_role = SERVER\n        else:\n            self.their_role = CLIENT\n        self._cstate = ConnectionState()\n\n        # Callables for converting data->events or vice-versa given the\n        # current state\n        self._writer = self._get_io_object(self.our_role, None, WRITERS)\n        self._reader = self._get_io_object(self.their_role, None, READERS)\n\n        # Holds any unprocessed received data\n        self._receive_buffer = ReceiveBuffer()\n        # If this is true, then it indicates that the incoming connection was\n        # closed *after* the end of whatever's in self._receive_buffer:\n        self._receive_buffer_closed = False\n\n        # Extra bits of state that don't fit into the state machine.\n        #\n        # These two are only used to interpret framing headers for figuring\n        # out how to read/write response bodies. their_http_version is also\n        # made available as a convenient public API.\n        self.their_http_version: Optional[bytes] = None\n        self._request_method: Optional[bytes] = None\n        # This is pure flow-control and doesn't at all affect the set of legal\n        # transitions, so no need to bother ConnectionState with it:\n        self.client_is_waiting_for_100_continue = False\n\n    @property\n    def states(self) -> Dict[Type[Sentinel], Type[Sentinel]]:\n        \"\"\"A dictionary like::\n\n           {CLIENT: <client state>, SERVER: <server state>}\n\n        See :ref:`state-machine` for details.\n\n        \"\"\"\n        return dict(self._cstate.states)\n\n    @property\n    def our_state(self) -> Type[Sentinel]:\n        \"\"\"The current state of whichever role we are playing. See\n        :ref:`state-machine` for details.\n        \"\"\"\n        return self._cstate.states[self.our_role]\n\n    @property\n    def their_state(self) -> Type[Sentinel]:\n        \"\"\"The current state of whichever role we are NOT playing. See\n        :ref:`state-machine` for details.\n        \"\"\"\n        return self._cstate.states[self.their_role]\n\n    @property\n    def they_are_waiting_for_100_continue(self) -> bool:\n        return self.their_role is CLIENT and self.client_is_waiting_for_100_continue\n\n    def start_next_cycle(self) -> None:\n        \"\"\"Attempt to reset our connection state for a new request/response\n        cycle.\n\n        If both client and server are in :data:`DONE` state, then resets them\n        both to :data:`IDLE` state in preparation for a new request/response\n        cycle on this same connection. Otherwise, raises a\n        :exc:`LocalProtocolError`.\n\n        See :ref:`keepalive-and-pipelining`.\n\n        \"\"\"\n        old_states = dict(self._cstate.states)\n        self._cstate.start_next_cycle()\n        self._request_method = None\n        # self.their_http_version gets left alone, since it presumably lasts\n        # beyond a single request/response cycle\n        assert not self.client_is_waiting_for_100_continue\n        self._respond_to_state_changes(old_states)\n\n    def _process_error(self, role: Type[Sentinel]) -> None:\n        old_states = dict(self._cstate.states)\n        self._cstate.process_error(role)\n        self._respond_to_state_changes(old_states)\n\n    def _server_switch_event(self, event: Event) -> Optional[Type[Sentinel]]:\n        if type(event) is InformationalResponse and event.status_code == 101:\n            return _SWITCH_UPGRADE\n        if type(event) is Response:\n            if (\n                _SWITCH_CONNECT in self._cstate.pending_switch_proposals\n                and 200 <= event.status_code < 300\n            ):\n                return _SWITCH_CONNECT\n        return None\n\n    # All events go through here\n    def _process_event(self, role: Type[Sentinel], event: Event) -> None:\n        # First, pass the event through the state machine to make sure it\n        # succeeds.\n        old_states = dict(self._cstate.states)\n        if role is CLIENT and type(event) is Request:\n            if event.method == b\"CONNECT\":\n                self._cstate.process_client_switch_proposal(_SWITCH_CONNECT)\n            if get_comma_header(event.headers, b\"upgrade\"):\n                self._cstate.process_client_switch_proposal(_SWITCH_UPGRADE)\n        server_switch_event = None\n        if role is SERVER:\n            server_switch_event = self._server_switch_event(event)\n        self._cstate.process_event(role, type(event), server_switch_event)\n\n        # Then perform the updates triggered by it.\n\n        if type(event) is Request:\n            self._request_method = event.method\n\n        if role is self.their_role and type(event) in (\n            Request,\n            Response,\n            InformationalResponse,\n        ):\n            event = cast(Union[Request, Response, InformationalResponse], event)\n            self.their_http_version = event.http_version\n\n        # Keep alive handling\n        #\n        # RFC 7230 doesn't really say what one should do if Connection: close\n        # shows up on a 1xx InformationalResponse. I think the idea is that\n        # this is not supposed to happen. In any case, if it does happen, we\n        # ignore it.\n        if type(event) in (Request, Response) and not _keep_alive(\n            cast(Union[Request, Response], event)\n        ):\n            self._cstate.process_keep_alive_disabled()\n\n        # 100-continue\n        if type(event) is Request and has_expect_100_continue(event):\n            self.client_is_waiting_for_100_continue = True\n        if type(event) in (InformationalResponse, Response):\n            self.client_is_waiting_for_100_continue = False\n        if role is CLIENT and type(event) in (Data, EndOfMessage):\n            self.client_is_waiting_for_100_continue = False\n\n        self._respond_to_state_changes(old_states, event)\n\n    def _get_io_object(\n        self,\n        role: Type[Sentinel],\n        event: Optional[Event],\n        io_dict: Union[ReadersType, WritersType],\n    ) -> Optional[Callable[..., Any]]:\n        # event may be None; it's only used when entering SEND_BODY\n        state = self._cstate.states[role]\n        if state is SEND_BODY:\n            # Special case: the io_dict has a dict of reader/writer factories\n            # that depend on the request/response framing.\n            framing_type, args = _body_framing(\n                cast(bytes, self._request_method), cast(Union[Request, Response], event)\n            )\n            return io_dict[SEND_BODY][framing_type](*args)  # type: ignore[index]\n        else:\n            # General case: the io_dict just has the appropriate reader/writer\n            # for this state\n            return io_dict.get((role, state))  # type: ignore[return-value]\n\n    # This must be called after any action that might have caused\n    # self._cstate.states to change.\n    def _respond_to_state_changes(\n        self,\n        old_states: Dict[Type[Sentinel], Type[Sentinel]],\n        event: Optional[Event] = None,\n    ) -> None:\n        # Update reader/writer\n        if self.our_state != old_states[self.our_role]:\n            self._writer = self._get_io_object(self.our_role, event, WRITERS)\n        if self.their_state != old_states[self.their_role]:\n            self._reader = self._get_io_object(self.their_role, event, READERS)\n\n    @property\n    def trailing_data(self) -> Tuple[bytes, bool]:\n        \"\"\"Data that has been received, but not yet processed, represented as\n        a tuple with two elements, where the first is a byte-string containing\n        the unprocessed data itself, and the second is a bool that is True if\n        the receive connection was closed.\n\n        See :ref:`switching-protocols` for discussion of why you'd want this.\n        \"\"\"\n        return (bytes(self._receive_buffer), self._receive_buffer_closed)\n\n    def receive_data(self, data: bytes) -> None:\n        \"\"\"Add data to our internal receive buffer.\n\n        This does not actually do any processing on the data, just stores\n        it. To trigger processing, you have to call :meth:`next_event`.\n\n        Args:\n            data (:term:`bytes-like object`):\n                The new data that was just received.\n\n                Special case: If *data* is an empty byte-string like ``b\"\"``,\n                then this indicates that the remote side has closed the\n                connection (end of file). Normally this is convenient, because\n                standard Python APIs like :meth:`file.read` or\n                :meth:`socket.recv` use ``b\"\"`` to indicate end-of-file, while\n                other failures to read are indicated using other mechanisms\n                like raising :exc:`TimeoutError`. When using such an API you\n                can just blindly pass through whatever you get from ``read``\n                to :meth:`receive_data`, and everything will work.\n\n                But, if you have an API where reading an empty string is a\n                valid non-EOF condition, then you need to be aware of this and\n                make sure to check for such strings and avoid passing them to\n                :meth:`receive_data`.\n\n        Returns:\n            Nothing, but after calling this you should call :meth:`next_event`\n            to parse the newly received data.\n\n        Raises:\n            RuntimeError:\n                Raised if you pass an empty *data*, indicating EOF, and then\n                pass a non-empty *data*, indicating more data that somehow\n                arrived after the EOF.\n\n                (Calling ``receive_data(b\"\")`` multiple times is fine,\n                and equivalent to calling it once.)\n\n        \"\"\"\n        if data:\n            if self._receive_buffer_closed:\n                raise RuntimeError(\"received close, then received more data?\")\n            self._receive_buffer += data\n        else:\n            self._receive_buffer_closed = True\n\n    def _extract_next_receive_event(\n        self,\n    ) -> Union[Event, Type[NEED_DATA], Type[PAUSED]]:\n        state = self.their_state\n        # We don't pause immediately when they enter DONE, because even in\n        # DONE state we can still process a ConnectionClosed() event. But\n        # if we have data in our buffer, then we definitely aren't getting\n        # a ConnectionClosed() immediately and we need to pause.\n        if state is DONE and self._receive_buffer:\n            return PAUSED\n        if state is MIGHT_SWITCH_PROTOCOL or state is SWITCHED_PROTOCOL:\n            return PAUSED\n        assert self._reader is not None\n        event = self._reader(self._receive_buffer)\n        if event is None:\n            if not self._receive_buffer and self._receive_buffer_closed:\n                # In some unusual cases (basically just HTTP/1.0 bodies), EOF\n                # triggers an actual protocol event; in that case, we want to\n                # return that event, and then the state will change and we'll\n                # get called again to generate the actual ConnectionClosed().\n                if hasattr(self._reader, \"read_eof\"):\n                    event = self._reader.read_eof()\n                else:\n                    event = ConnectionClosed()\n        if event is None:\n            event = NEED_DATA\n        return event  # type: ignore[no-any-return]\n\n    def next_event(self) -> Union[Event, Type[NEED_DATA], Type[PAUSED]]:\n        \"\"\"Parse the next event out of our receive buffer, update our internal\n        state, and return it.\n\n        This is a mutating operation -- think of it like calling :func:`next`\n        on an iterator.\n\n        Returns:\n            : One of three things:\n\n            1) An event object -- see :ref:`events`.\n\n            2) The special constant :data:`NEED_DATA`, which indicates that\n               you need to read more data from your socket and pass it to\n               :meth:`receive_data` before this method will be able to return\n               any more events.\n\n            3) The special constant :data:`PAUSED`, which indicates that we\n               are not in a state where we can process incoming data (usually\n               because the peer has finished their part of the current\n               request/response cycle, and you have not yet called\n               :meth:`start_next_cycle`). See :ref:`flow-control` for details.\n\n        Raises:\n            RemoteProtocolError:\n                The peer has misbehaved. You should close the connection\n                (possibly after sending some kind of 4xx response).\n\n        Once this method returns :class:`ConnectionClosed` once, then all\n        subsequent calls will also return :class:`ConnectionClosed`.\n\n        If this method raises any exception besides :exc:`RemoteProtocolError`\n        then that's a bug -- if it happens please file a bug report!\n\n        If this method raises any exception then it also sets\n        :attr:`Connection.their_state` to :data:`ERROR` -- see\n        :ref:`error-handling` for discussion.\n\n        \"\"\"\n\n        if self.their_state is ERROR:\n            raise RemoteProtocolError(\"Can't receive data when peer state is ERROR\")\n        try:\n            event = self._extract_next_receive_event()\n            if event not in [NEED_DATA, PAUSED]:\n                self._process_event(self.their_role, cast(Event, event))\n            if event is NEED_DATA:\n                if len(self._receive_buffer) > self._max_incomplete_event_size:\n                    # 431 is \"Request header fields too large\" which is pretty\n                    # much the only situation where we can get here\n                    raise RemoteProtocolError(\n                        \"Receive buffer too long\", error_status_hint=431\n                    )\n                if self._receive_buffer_closed:\n                    # We're still trying to complete some event, but that's\n                    # never going to happen because no more data is coming\n                    raise RemoteProtocolError(\"peer unexpectedly closed connection\")\n            return event\n        except BaseException as exc:\n            self._process_error(self.their_role)\n            if isinstance(exc, LocalProtocolError):\n                exc._reraise_as_remote_protocol_error()\n            else:\n                raise\n\n    @overload\n    def send(self, event: ConnectionClosed) -> None:\n        ...\n\n    @overload\n    def send(\n        self, event: Union[Request, InformationalResponse, Response, Data, EndOfMessage]\n    ) -> bytes:\n        ...\n\n    @overload\n    def send(self, event: Event) -> Optional[bytes]:\n        ...\n\n    def send(self, event: Event) -> Optional[bytes]:\n        \"\"\"Convert a high-level event into bytes that can be sent to the peer,\n        while updating our internal state machine.\n\n        Args:\n            event: The :ref:`event <events>` to send.\n\n        Returns:\n            If ``type(event) is ConnectionClosed``, then returns\n            ``None``. Otherwise, returns a :term:`bytes-like object`.\n\n        Raises:\n            LocalProtocolError:\n                Sending this event at this time would violate our\n                understanding of the HTTP/1.1 protocol.\n\n        If this method raises any exception then it also sets\n        :attr:`Connection.our_state` to :data:`ERROR` -- see\n        :ref:`error-handling` for discussion.\n\n        \"\"\"\n        data_list = self.send_with_data_passthrough(event)\n        if data_list is None:\n            return None\n        else:\n            return b\"\".join(data_list)\n\n    def send_with_data_passthrough(self, event: Event) -> Optional[List[bytes]]:\n        \"\"\"Identical to :meth:`send`, except that in situations where\n        :meth:`send` returns a single :term:`bytes-like object`, this instead\n        returns a list of them -- and when sending a :class:`Data` event, this\n        list is guaranteed to contain the exact object you passed in as\n        :attr:`Data.data`. See :ref:`sendfile` for discussion.\n\n        \"\"\"\n        if self.our_state is ERROR:\n            raise LocalProtocolError(\"Can't send data when our state is ERROR\")\n        try:\n            if type(event) is Response:\n                event = self._clean_up_response_headers_for_sending(event)\n            # We want to call _process_event before calling the writer,\n            # because if someone tries to do something invalid then this will\n            # give a sensible error message, while our writers all just assume\n            # they will only receive valid events. But, _process_event might\n            # change self._writer. So we have to do a little dance:\n            writer = self._writer\n            self._process_event(self.our_role, event)\n            if type(event) is ConnectionClosed:\n                return None\n            else:\n                # In any situation where writer is None, process_event should\n                # have raised ProtocolError\n                assert writer is not None\n                data_list: List[bytes] = []\n                writer(event, data_list.append)\n                return data_list\n        except:\n            self._process_error(self.our_role)\n            raise\n\n    def send_failed(self) -> None:\n        \"\"\"Notify the state machine that we failed to send the data it gave\n        us.\n\n        This causes :attr:`Connection.our_state` to immediately become\n        :data:`ERROR` -- see :ref:`error-handling` for discussion.\n\n        \"\"\"\n        self._process_error(self.our_role)\n\n    # When sending a Response, we take responsibility for a few things:\n    #\n    # - Sometimes you MUST set Connection: close. We take care of those\n    #   times. (You can also set it yourself if you want, and if you do then\n    #   we'll respect that and close the connection at the right time. But you\n    #   don't have to worry about that unless you want to.)\n    #\n    # - The user has to set Content-Length if they want it. Otherwise, for\n    #   responses that have bodies (e.g. not HEAD), then we will automatically\n    #   select the right mechanism for streaming a body of unknown length,\n    #   which depends on depending on the peer's HTTP version.\n    #\n    # This function's *only* responsibility is making sure headers are set up\n    # right -- everything downstream just looks at the headers. There are no\n    # side channels.\n    def _clean_up_response_headers_for_sending(self, response: Response) -> Response:\n        assert type(response) is Response\n\n        headers = response.headers\n        need_close = False\n\n        # HEAD requests need some special handling: they always act like they\n        # have Content-Length: 0, and that's how _body_framing treats\n        # them. But their headers are supposed to match what we would send if\n        # the request was a GET. (Technically there is one deviation allowed:\n        # we're allowed to leave out the framing headers -- see\n        # https://tools.ietf.org/html/rfc7231#section-4.3.2 . But it's just as\n        # easy to get them right.)\n        method_for_choosing_headers = cast(bytes, self._request_method)\n        if method_for_choosing_headers == b\"HEAD\":\n            method_for_choosing_headers = b\"GET\"\n        framing_type, _ = _body_framing(method_for_choosing_headers, response)\n        if framing_type in (\"chunked\", \"http/1.0\"):\n            # This response has a body of unknown length.\n            # If our peer is HTTP/1.1, we use Transfer-Encoding: chunked\n            # If our peer is HTTP/1.0, we use no framing headers, and close the\n            # connection afterwards.\n            #\n            # Make sure to clear Content-Length (in principle user could have\n            # set both and then we ignored Content-Length b/c\n            # Transfer-Encoding overwrote it -- this would be naughty of them,\n            # but the HTTP spec says that if our peer does this then we have\n            # to fix it instead of erroring out, so we'll accord the user the\n            # same respect).\n            headers = set_comma_header(headers, b\"content-length\", [])\n            if self.their_http_version is None or self.their_http_version < b\"1.1\":\n                # Either we never got a valid request and are sending back an\n                # error (their_http_version is None), so we assume the worst;\n                # or else we did get a valid HTTP/1.0 request, so we know that\n                # they don't understand chunked encoding.\n                headers = set_comma_header(headers, b\"transfer-encoding\", [])\n                # This is actually redundant ATM, since currently we\n                # unconditionally disable keep-alive when talking to HTTP/1.0\n                # peers. But let's be defensive just in case we add\n                # Connection: keep-alive support later:\n                if self._request_method != b\"HEAD\":\n                    need_close = True\n            else:\n                headers = set_comma_header(headers, b\"transfer-encoding\", [b\"chunked\"])\n\n        if not self._cstate.keep_alive or need_close:\n            # Make sure Connection: close is set\n            connection = set(get_comma_header(headers, b\"connection\"))\n            connection.discard(b\"keep-alive\")\n            connection.add(b\"close\")\n            headers = set_comma_header(headers, b\"connection\", sorted(connection))\n\n        return Response(\n            headers=headers,\n            status_code=response.status_code,\n            http_version=response.http_version,\n            reason=response.reason,\n        )\n", "h11/_version.py": "# This file must be kept very simple, because it is consumed from several\n# places -- it is imported by h11/__init__.py, execfile'd by setup.py, etc.\n\n# We use a simple scheme:\n#   1.0.0 -> 1.0.0+dev -> 1.1.0 -> 1.1.0+dev\n# where the +dev versions are never released into the wild, they're just what\n# we stick into the VCS in between releases.\n#\n# This is compatible with PEP 440:\n#   http://legacy.python.org/dev/peps/pep-0440/\n# via the use of the \"local suffix\" \"+dev\", which is disallowed on index\n# servers and causes 1.0.0+dev to sort after plain 1.0.0, which is what we\n# want. (Contrast with the special suffix 1.0.0.dev, which sorts *before*\n# 1.0.0.)\n\n__version__ = \"0.14.0+dev\"\n", "fuzz/afl-server.py": "# Invariant tested: No matter what random garbage a client throws at us, we\n# either successfully parse it, or else throw a RemoteProtocolError, never any\n# other error.\n\nimport os\nimport sys\n\nimport afl\n\nimport h11\n\n\ndef process_all(c):\n    while True:\n        event = c.next_event()\n        if event is h11.NEED_DATA or event is h11.PAUSED:\n            break\n        if type(event) is h11.ConnectionClosed:\n            break\n\n\nafl.init()\n\ndata = sys.stdin.detach().read()\n\n# one big chunk\nserver1 = h11.Connection(h11.SERVER)\ntry:\n    server1.receive_data(data)\n    process_all(server1)\n    server1.receive_data(b\"\")\n    process_all(server1)\nexcept h11.RemoteProtocolError:\n    pass\n\n# byte at a time\nserver2 = h11.Connection(h11.SERVER)\ntry:\n    for i in range(len(data)):\n        server2.receive_data(data[i : i + 1])\n        process_all(server2)\n    server2.receive_data(b\"\")\n    process_all(server2)\nexcept h11.RemoteProtocolError:\n    pass\n\n# Suggested by the afl-python docs -- this substantially speeds up fuzzing, at\n# the risk of missing bugs that would cause the interpreter to crash on\n# exit. h11 is pure python, so I'm pretty sure h11 doesn't have any bugs that\n# would cause the interpreter to crash on exit.\nos._exit(0)\n", "examples/basic-client.py": "import socket\nimport ssl\n\nimport h11\n\n################################################################\n# Setup\n################################################################\n\nconn = h11.Connection(our_role=h11.CLIENT)\nctx = ssl.create_default_context()\nsock = ctx.wrap_socket(\n    socket.create_connection((\"httpbin.org\", 443)), server_hostname=\"httpbin.org\"\n)\n\n################################################################\n# Sending a request\n################################################################\n\n\ndef send(event):\n    print(\"Sending event:\")\n    print(event)\n    print()\n    # Pass the event through h11's state machine and encoding machinery\n    data = conn.send(event)\n    # Send the resulting bytes on the wire\n    sock.sendall(data)\n\n\nsend(\n    h11.Request(\n        method=\"GET\",\n        target=\"/get\",\n        headers=[(\"Host\", \"httpbin.org\"), (\"Connection\", \"close\")],\n    )\n)\nsend(h11.EndOfMessage())\n\n################################################################\n# Receiving the response\n################################################################\n\n\ndef next_event():\n    while True:\n        # Check if an event is already available\n        event = conn.next_event()\n        if event is h11.NEED_DATA:\n            # Nope, so fetch some data from the socket...\n            data = sock.recv(2048)\n            # ...and give it to h11 to convert back into events...\n            conn.receive_data(data)\n            # ...and then loop around to try again.\n            continue\n        return event\n\n\nwhile True:\n    event = next_event()\n    print(\"Received event:\")\n    print(event)\n    print()\n    if type(event) is h11.EndOfMessage:\n        break\n\n################################################################\n# Clean up\n################################################################\n\nsock.close()\n", "examples/trio-server.py": "# A simple HTTP server implemented using h11 and Trio:\n#   http://trio.readthedocs.io/en/latest/index.html\n#\n# All requests get echoed back a JSON document containing information about\n# the request.\n#\n# This is a rather involved example, since it attempts to both be\n# fully-HTTP-compliant and also demonstrate error handling.\n#\n# The main difference between an HTTP client and an HTTP server is that in a\n# client, if something goes wrong, you can just throw away that connection and\n# make a new one. In a server, you're expected to handle all kinds of garbage\n# input and internal errors and recover with grace and dignity. And that's\n# what this code does.\n#\n# I recommend pushing on it to see how it works -- e.g. watch what happens if\n# you visit http://localhost:8080 in a webbrowser that supports keep-alive,\n# hit reload a few times, and then wait for the keep-alive to time out on the\n# server.\n#\n# Or try using curl to start a chunked upload and then hit control-C in the\n# middle of the upload:\n#\n#    (for CHUNK in $(seq 10); do echo $CHUNK; sleep 1; done) \\\n#      | curl -T - http://localhost:8080/foo\n#\n# (Note that curl will send Expect: 100-Continue, too.)\n#\n# Or, heck, try letting curl complete successfully ;-).\n\n# Some potential improvements, if you wanted to try and extend this to a real\n# general-purpose HTTP server (and to give you some hints about the many\n# considerations that go into making a robust HTTP server):\n#\n# - The timeout handling is rather crude -- we impose a flat 10 second timeout\n#   on each request (starting from the end of the previous\n#   response). Something finer-grained would be better. Also, if a timeout is\n#   triggered we unconditionally send a 500 Internal Server Error; it would be\n#   better to keep track of whether the timeout is the client's fault, and if\n#   so send a 408 Request Timeout.\n#\n# - The error handling policy here is somewhat crude as well. It handles a lot\n#   of cases perfectly, but there are corner cases where the ideal behavior is\n#   more debateable. For example, if a client starts uploading a large\n#   request, uses 100-Continue, and we send an error response, then we'll shut\n#   down the connection immediately (for well-behaved clients) or after\n#   spending TIMEOUT seconds reading and discarding their upload (for\n#   ill-behaved ones that go on and try to upload their request anyway). And\n#   for clients that do this without 100-Continue, we'll send the error\n#   response and then shut them down after TIMEOUT seconds. This might or\n#   might not be your preferred policy, though -- maybe you want to shut such\n#   clients down immediately (even if this risks their not seeing the\n#   response), or maybe you're happy to let them continue sending all the data\n#   and wasting your bandwidth if this is what it takes to guarantee that they\n#   see your error response. Up to you, really.\n#\n# - Another example of a debateable choice: if a response handler errors out\n#   without having done *anything* -- hasn't started responding, hasn't read\n#   the request body -- then this connection actually is salvagable, if the\n#   server sends an error response + reads and discards the request body. This\n#   code sends the error response, but it doesn't try to salvage the\n#   connection by reading the request body, it just closes the\n#   connection. This is quite possibly the best option, but again this is a\n#   policy decision.\n#\n# - Our error pages always include the exception text. In real life you might\n#   want to log the exception but not send that information to the client.\n#\n# - Our error responses perhaps should include Connection: close when we know\n#   we're going to close this connection.\n#\n# - We don't support the HEAD method, but ought to.\n#\n# - We should probably do something cleverer with buffering responses and\n#   TCP_CORK and suchlike.\n\nimport datetime\nimport email.utils\nimport json\nfrom itertools import count\n\nimport trio\n\nimport h11\n\nMAX_RECV = 2**16\nTIMEOUT = 10\n\n\n# We are using email.utils.format_datetime to generate the Date header.\n# It may sound weird, but it actually follows the RFC.\n# Please see: https://stackoverflow.com/a/59416334/14723771\n#\n# See also:\n# [1] https://www.rfc-editor.org/rfc/rfc9110#section-5.6.7\n# [2] https://www.rfc-editor.org/rfc/rfc7231#section-7.1.1.1\n# [3] https://www.rfc-editor.org/rfc/rfc5322#section-3.3\ndef format_date_time(dt=None):\n    \"\"\"Generate a RFC 7231 / RFC 9110 IMF-fixdate string\"\"\"\n    if dt is None:\n        dt = datetime.datetime.now(datetime.timezone.utc)\n    return email.utils.format_datetime(dt, usegmt=True)\n\n\n################################################################\n# I/O adapter: h11 <-> trio\n################################################################\n\n\n# The core of this could be factored out to be usable for trio-based clients\n# too, as well as servers. But as a simplified pedagogical example we don't\n# attempt this here.\nclass TrioHTTPWrapper:\n    _next_id = count()\n\n    def __init__(self, stream):\n        self.stream = stream\n        self.conn = h11.Connection(h11.SERVER)\n        # Our Server: header\n        self.ident = \" \".join(\n            [f\"h11-example-trio-server/{h11.__version__}\", h11.PRODUCT_ID]\n        ).encode(\"ascii\")\n        # A unique id for this connection, to include in debugging output\n        # (useful for understanding what's going on if there are multiple\n        # simultaneous clients).\n        self._obj_id = next(TrioHTTPWrapper._next_id)\n\n    async def send(self, event):\n        # The code below doesn't send ConnectionClosed, so we don't bother\n        # handling it here either -- it would require that we do something\n        # appropriate when 'data' is None.\n        assert type(event) is not h11.ConnectionClosed\n        data = self.conn.send(event)\n        try:\n            await self.stream.send_all(data)\n        except BaseException:\n            # If send_all raises an exception (especially trio.Cancelled),\n            # we have no choice but to give it up.\n            self.conn.send_failed()\n            raise\n\n    async def _read_from_peer(self):\n        if self.conn.they_are_waiting_for_100_continue:\n            self.info(\"Sending 100 Continue\")\n            go_ahead = h11.InformationalResponse(\n                status_code=100, headers=self.basic_headers()\n            )\n            await self.send(go_ahead)\n        try:\n            data = await self.stream.receive_some(MAX_RECV)\n        except ConnectionError:\n            # They've stopped listening. Not much we can do about it here.\n            data = b\"\"\n        self.conn.receive_data(data)\n\n    async def next_event(self):\n        while True:\n            event = self.conn.next_event()\n            if event is h11.NEED_DATA:\n                await self._read_from_peer()\n                continue\n            return event\n\n    async def shutdown_and_clean_up(self):\n        # When this method is called, it's because we definitely want to kill\n        # this connection, either as a clean shutdown or because of some kind\n        # of error or loss-of-sync bug, and we no longer care if that violates\n        # the protocol or not. So we ignore the state of self.conn, and just\n        # go ahead and do the shutdown on the socket directly. (If you're\n        # implementing a client you might prefer to send ConnectionClosed()\n        # and let it raise an exception if that violates the protocol.)\n        #\n        try:\n            await self.stream.send_eof()\n        except trio.BrokenResourceError:\n            # They're already gone, nothing to do\n            return\n        # Wait and read for a bit to give them a chance to see that we closed\n        # things, but eventually give up and just close the socket.\n        # XX FIXME: possibly we should set SO_LINGER to 0 here, so\n        # that in the case where the client has ignored our shutdown and\n        # declined to initiate the close themselves, we do a violent shutdown\n        # (RST) and avoid the TIME_WAIT?\n        # it looks like nginx never does this for keepalive timeouts, and only\n        # does it for regular timeouts (slow clients I guess?) if explicitly\n        # enabled (\"Default: reset_timedout_connection off\")\n        with trio.move_on_after(TIMEOUT):\n            try:\n                while True:\n                    # Attempt to read until EOF\n                    got = await self.stream.receive_some(MAX_RECV)\n                    if not got:\n                        break\n            except trio.BrokenResourceError:\n                pass\n            finally:\n                await self.stream.aclose()\n\n    def basic_headers(self):\n        # HTTP requires these headers in all responses (client would do\n        # something different here)\n        return [\n            (\"Date\", format_date_time().encode(\"ascii\")),\n            (\"Server\", self.ident),\n        ]\n\n    def info(self, *args):\n        # Little debugging method\n        print(f\"{self._obj_id}:\", *args)\n\n\n################################################################\n# Server main loop\n################################################################\n\n\n# General theory:\n#\n# If everything goes well:\n# - we'll get a Request\n# - our response handler will read the request body and send a full response\n# - that will either leave us in MUST_CLOSE (if the client doesn't\n#   support keepalive) or DONE/DONE (if the client does).\n#\n# But then there are many, many different ways that things can go wrong\n# here. For example:\n# - we don't actually get a Request, but rather a ConnectionClosed\n# - exception is raised from somewhere (naughty client, broken\n#   response handler, whatever)\n#   - depending on what went wrong and where, we might or might not be\n#     able to send an error response, and the connection might or\n#     might not be salvagable after that\n# - response handler doesn't fully read the request or doesn't send a\n#   full response\n#\n# But these all have one thing in common: they involve us leaving the\n# nice easy path up above. So we can just proceed on the assumption\n# that the nice easy thing is what's happening, and whenever something\n# goes wrong do our best to get back onto that path, and h11 will keep\n# track of how successful we were and raise new errors if things don't work\n# out.\nasync def http_serve(stream):\n    wrapper = TrioHTTPWrapper(stream)\n    wrapper.info(\"Got new connection\")\n    while True:\n        assert wrapper.conn.states == {h11.CLIENT: h11.IDLE, h11.SERVER: h11.IDLE}\n\n        try:\n            with trio.fail_after(TIMEOUT):\n                wrapper.info(\"Server main loop waiting for request\")\n                event = await wrapper.next_event()\n                wrapper.info(\"Server main loop got event:\", event)\n                if type(event) is h11.Request:\n                    await send_echo_response(wrapper, event)\n        except Exception as exc:\n            wrapper.info(f\"Error during response handler: {exc!r}\")\n            await maybe_send_error_response(wrapper, exc)\n\n        if wrapper.conn.our_state is h11.MUST_CLOSE:\n            wrapper.info(\"connection is not reusable, so shutting down\")\n            await wrapper.shutdown_and_clean_up()\n            return\n        else:\n            try:\n                wrapper.info(\"trying to re-use connection\")\n                wrapper.conn.start_next_cycle()\n            except h11.ProtocolError:\n                states = wrapper.conn.states\n                wrapper.info(\"unexpected state\", states, \"-- bailing out\")\n                await maybe_send_error_response(\n                    wrapper, RuntimeError(f\"unexpected state {states}\")\n                )\n                await wrapper.shutdown_and_clean_up()\n                return\n\n\n################################################################\n# Actual response handlers\n################################################################\n\n\n# Helper function\nasync def send_simple_response(wrapper, status_code, content_type, body):\n    wrapper.info(\"Sending\", status_code, \"response with\", len(body), \"bytes\")\n    headers = wrapper.basic_headers()\n    headers.append((\"Content-Type\", content_type))\n    headers.append((\"Content-Length\", str(len(body))))\n    res = h11.Response(status_code=status_code, headers=headers)\n    await wrapper.send(res)\n    await wrapper.send(h11.Data(data=body))\n    await wrapper.send(h11.EndOfMessage())\n\n\nasync def maybe_send_error_response(wrapper, exc):\n    # If we can't send an error, oh well, nothing to be done\n    wrapper.info(\"trying to send error response...\")\n    if wrapper.conn.our_state not in {h11.IDLE, h11.SEND_RESPONSE}:\n        wrapper.info(\"...but I can't, because our state is\", wrapper.conn.our_state)\n        return\n    try:\n        if isinstance(exc, h11.RemoteProtocolError):\n            status_code = exc.error_status_hint\n        elif isinstance(exc, trio.TooSlowError):\n            status_code = 408  # Request Timeout\n        else:\n            status_code = 500\n        body = str(exc).encode(\"utf-8\")\n        await send_simple_response(\n            wrapper, status_code, \"text/plain; charset=utf-8\", body\n        )\n    except Exception as exc:\n        wrapper.info(\"error while sending error response:\", exc)\n\n\nasync def send_echo_response(wrapper, request):\n    wrapper.info(\"Preparing echo response\")\n    if request.method not in {b\"GET\", b\"POST\"}:\n        # Laziness: we should send a proper 405 Method Not Allowed with the\n        # appropriate Accept: header, but we don't.\n        raise RuntimeError(\"unsupported method\")\n    response_json = {\n        \"method\": request.method.decode(\"ascii\"),\n        \"target\": request.target.decode(\"ascii\"),\n        \"headers\": [\n            (name.decode(\"ascii\"), value.decode(\"ascii\"))\n            for (name, value) in request.headers\n        ],\n        \"body\": \"\",\n    }\n    while True:\n        event = await wrapper.next_event()\n        if type(event) is h11.EndOfMessage:\n            break\n        assert type(event) is h11.Data\n        response_json[\"body\"] += event.data.decode(\"ascii\")\n    response_body_unicode = json.dumps(\n        response_json, sort_keys=True, indent=4, separators=(\",\", \": \")\n    )\n    response_body_bytes = response_body_unicode.encode(\"utf-8\")\n    await send_simple_response(\n        wrapper, 200, \"application/json; charset=utf-8\", response_body_bytes\n    )\n\n\nasync def serve(port):\n    print(f\"listening on http://localhost:{port}\")\n    try:\n        await trio.serve_tcp(http_serve, port)\n    except KeyboardInterrupt:\n        print(\"KeyboardInterrupt - shutting down\")\n\n\n################################################################\n# Run the server\n################################################################\n\nif __name__ == \"__main__\":\n    trio.run(serve, 8080)\n"}