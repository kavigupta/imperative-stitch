{"backend/celery_worker.py": "import os\nfrom datetime import datetime, timedelta\nfrom tempfile import NamedTemporaryFile\nfrom uuid import UUID\n\nfrom celery.schedules import crontab\nfrom celery_config import celery\nfrom logger import get_logger\nfrom middlewares.auth.auth_bearer import AuthBearer\nfrom models.files import File\nfrom models.settings import get_supabase_client, get_supabase_db\nfrom modules.brain.integrations.Notion.Notion_connector import NotionConnector\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.brain.service.brain_vector_service import BrainVectorService\nfrom modules.notification.dto.inputs import NotificationUpdatableProperties\nfrom modules.notification.entity.notification import NotificationsStatusEnum\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.onboarding.service.onboarding_service import OnboardingService\nfrom packages.files.crawl.crawler import CrawlWebsite, slugify\nfrom packages.files.parsers.github import process_github\nfrom packages.files.processors import filter_file\nfrom packages.utils.telemetry import maybe_send_telemetry\nfrom pytz import timezone\n\nlogger = get_logger(__name__)\n\nonboardingService = OnboardingService()\nnotification_service = NotificationService()\nbrain_service = BrainService()\nauth_bearer = AuthBearer()\n\n\n@celery.task(name=\"process_file_and_notify\")\ndef process_file_and_notify(\n    file_name: str,\n    file_original_name: str,\n    brain_id,\n    notification_id=None,\n    integration=None,\n    delete_file=False,\n):\n    try:\n        supabase_client = get_supabase_client()\n        tmp_name = file_name.replace(\"/\", \"_\")\n        base_file_name = os.path.basename(file_name)\n        _, file_extension = os.path.splitext(base_file_name)\n\n        with NamedTemporaryFile(\n            suffix=\"_\" + tmp_name,  # pyright: ignore reportPrivateUsage=none\n        ) as tmp_file:\n            res = supabase_client.storage.from_(\"quivr\").download(file_name)\n            tmp_file.write(res)\n            tmp_file.flush()\n            file_instance = File(\n                file_name=base_file_name,\n                tmp_file_path=tmp_file.name,\n                bytes_content=res,\n                file_size=len(res),\n                file_extension=file_extension,\n            )\n            brain_vector_service = BrainVectorService(brain_id)\n            if delete_file:  # TODO fix bug\n                brain_vector_service.delete_file_from_brain(\n                    file_original_name, only_vectors=True\n                )\n\n            message = filter_file(\n                file=file_instance,\n                brain_id=brain_id,\n                original_file_name=file_original_name,\n            )\n\n            if notification_id:\n                notification_service.update_notification_by_id(\n                    notification_id,\n                    NotificationUpdatableProperties(\n                        status=NotificationsStatusEnum.SUCCESS,\n                        description=\"Your file has been properly uploaded!\",\n                    ),\n                )\n            brain_service.update_brain_last_update_time(brain_id)\n\n            return True\n\n    except TimeoutError:\n        logger.error(\"TimeoutError\")\n\n    except Exception as e:\n        logger.exception(e)\n        notification_service.update_notification_by_id(\n            notification_id,\n            NotificationUpdatableProperties(\n                status=NotificationsStatusEnum.ERROR,\n                description=f\"An error occurred while processing the file: {e}\",\n            ),\n        )\n\n\n@celery.task(name=\"process_crawl_and_notify\")\ndef process_crawl_and_notify(\n    crawl_website_url: str,\n    brain_id: UUID,\n    notification_id=None,\n):\n\n    crawl_website = CrawlWebsite(url=crawl_website_url)\n\n    if not crawl_website.checkGithub():\n        # Build file data\n        extracted_content = crawl_website.process()\n        extracted_content_bytes = extracted_content.encode(\"utf-8\")\n        file_name = slugify(crawl_website.url) + \".txt\"\n\n        with NamedTemporaryFile(\n            suffix=\"_\" + file_name,  # pyright: ignore reportPrivateUsage=none\n        ) as tmp_file:\n            tmp_file.write(extracted_content_bytes)\n            tmp_file.flush()\n            file_instance = File(\n                file_name=file_name,\n                tmp_file_path=tmp_file.name,\n                bytes_content=extracted_content_bytes,\n                file_size=len(extracted_content),\n                file_extension=\".txt\",\n            )\n            message = filter_file(\n                file=file_instance,\n                brain_id=brain_id,\n                original_file_name=crawl_website_url,\n            )\n            notification_service.update_notification_by_id(\n                notification_id,\n                NotificationUpdatableProperties(\n                    status=NotificationsStatusEnum.SUCCESS,\n                    description=\"Your URL has been properly crawled!\",\n                ),\n            )\n    else:\n        message = process_github(\n            repo=crawl_website.url,\n            brain_id=brain_id,\n        )\n\n    if notification_id:\n        notification_service.update_notification_by_id(\n            notification_id,\n            NotificationUpdatableProperties(\n                status=NotificationsStatusEnum.SUCCESS,\n                description=\"Your file has been properly uploaded!\",\n            ),\n        )\n\n    brain_service.update_brain_last_update_time(brain_id)\n    return True\n\n\n@celery.task\ndef remove_onboarding_more_than_x_days_task():\n    onboardingService.remove_onboarding_more_than_x_days(7)\n\n\n@celery.task(name=\"NotionConnectorLoad\")\ndef process_integration_brain_created_initial_load(brain_id, user_id):\n    notion_connector = NotionConnector(brain_id=brain_id, user_id=user_id)\n\n    pages = notion_connector.load()\n\n    print(\"pages: \", len(pages))\n\n\n@celery.task\ndef process_integration_brain_sync_user_brain(brain_id, user_id):\n    notion_connector = NotionConnector(brain_id=brain_id, user_id=user_id)\n\n    notion_connector.poll()\n\n\n@celery.task\ndef ping_telemetry():\n    maybe_send_telemetry(\"ping\", {\"ping\": \"pong\"})\n\n\n@celery.task(name=\"check_if_is_premium_user\")\ndef check_if_is_premium_user():\n    supabase = get_supabase_db()\n    supabase_db = supabase.db\n\n    paris_tz = timezone(\"Europe/Paris\")\n    current_time = datetime.now(paris_tz)\n    current_time_str = current_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    logger.debug(f\"Current time: {current_time_str}\")\n\n    # Define the memoization period (e.g., 1 hour)\n    memoization_period = timedelta(hours=1)\n    memoization_cutoff = current_time - memoization_period\n\n    # Fetch all necessary data in bulk\n    subscriptions = (\n        supabase_db.table(\"subscriptions\")\n        .select(\"*\")\n        .filter(\"current_period_end\", \"gt\", current_time_str)\n        .execute()\n    ).data\n\n    customers = (supabase_db.table(\"customers\").select(\"*\").execute()).data\n\n    customer_emails = [customer[\"email\"] for customer in customers]\n\n    # Split customer emails into batches of 50\n    email_batches = [\n        customer_emails[i : i + 20] for i in range(0, len(customer_emails), 20)\n    ]\n\n    users = []\n    for email_batch in email_batches:\n        batch_users = (\n            supabase_db.table(\"users\")\n            .select(\"id, email\")\n            .in_(\"email\", email_batch)\n            .execute()\n        ).data\n        users.extend(batch_users)\n\n    product_features = (\n        supabase_db.table(\"product_to_features\").select(\"*\").execute()\n    ).data\n\n    user_settings = (supabase_db.table(\"user_settings\").select(\"*\").execute()).data\n\n    # Create lookup dictionaries for faster access\n    user_dict = {user[\"email\"]: user[\"id\"] for user in users}\n    customer_dict = {customer[\"id\"]: customer for customer in customers}\n    product_dict = {\n        product[\"stripe_product_id\"]: product for product in product_features\n    }\n    settings_dict = {setting[\"user_id\"]: setting for setting in user_settings}\n\n    # Process subscriptions and update user settings\n    premium_user_ids = set()\n    settings_to_upsert = {}\n    for sub in subscriptions:\n        if sub[\"attrs\"][\"status\"] != \"active\":\n            continue\n\n        customer = customer_dict.get(sub[\"customer\"])\n        if not customer:\n            continue\n\n        user_id = user_dict.get(customer[\"email\"])\n        if not user_id:\n            continue\n\n        current_settings = settings_dict.get(user_id, {})\n        last_check = current_settings.get(\"last_stripe_check\")\n\n        # Skip if the user was checked recently\n        if last_check and datetime.fromisoformat(last_check) > memoization_cutoff:\n            premium_user_ids.add(user_id)\n            continue\n\n        user_id = str(user_id)  # Ensure user_id is a string\n        premium_user_ids.add(user_id)\n\n        product_id = sub[\"attrs\"][\"items\"][\"data\"][0][\"plan\"][\"product\"]\n        product = product_dict.get(product_id)\n        if not product:\n            logger.warning(f\"No matching product found for subscription: {sub['id']}\")\n            continue\n\n        settings_to_upsert[user_id] = {\n            \"user_id\": user_id,\n            \"max_brains\": product[\"max_brains\"],\n            \"max_brain_size\": product[\"max_brain_size\"],\n            \"monthly_chat_credit\": product[\"monthly_chat_credit\"],\n            \"api_access\": product[\"api_access\"],\n            \"models\": product[\"models\"],\n            \"is_premium\": True,\n            \"last_stripe_check\": current_time_str,\n        }\n\n    # Bulk upsert premium user settings in batches of 10\n    settings_list = list(settings_to_upsert.values())\n    for i in range(0, len(settings_list), 10):\n        batch = settings_list[i : i + 10]\n        supabase_db.table(\"user_settings\").upsert(batch).execute()\n\n    # Delete settings for non-premium users in batches of 10\n    settings_to_delete = [\n        setting[\"user_id\"]\n        for setting in user_settings\n        if setting[\"user_id\"] not in premium_user_ids and setting.get(\"is_premium\")\n    ]\n    for i in range(0, len(settings_to_delete), 10):\n        batch = settings_to_delete[i : i + 10]\n        supabase_db.table(\"user_settings\").delete().in_(\"user_id\", batch).execute()\n\n    logger.info(\n        f\"Updated {len(settings_to_upsert)} premium users, deleted settings for {len(settings_to_delete)} non-premium users\"\n    )\n    return True\n\n\ncelery.conf.beat_schedule = {\n    \"remove_onboarding_more_than_x_days_task\": {\n        \"task\": f\"{__name__}.remove_onboarding_more_than_x_days_task\",\n        \"schedule\": crontab(minute=\"0\", hour=\"0\"),\n    },\n    \"ping_telemetry\": {\n        \"task\": f\"{__name__}.ping_telemetry\",\n        \"schedule\": crontab(minute=\"*/30\", hour=\"*\"),\n    },\n    \"process_sync_active\": {\n        \"task\": \"process_sync_active\",\n        \"schedule\": crontab(minute=\"*/1\", hour=\"*\"),\n    },\n    \"process_premium_users\": {\n        \"task\": \"check_if_is_premium_user\",\n        \"schedule\": crontab(minute=\"*/1\", hour=\"*\"),\n    },\n}\n", "backend/celery_config.py": "# celery_config.py\nimport os\n\nimport dotenv\nfrom celery import Celery\n\ndotenv.load_dotenv()\n\nCELERY_BROKER_URL = os.getenv(\"CELERY_BROKER_URL\", \"\")\nCELERY_BROKER_QUEUE_NAME = os.getenv(\"CELERY_BROKER_QUEUE_NAME\", \"quivr\")\n\ncelery = Celery(__name__)\n\nif CELERY_BROKER_URL.startswith(\"sqs\"):\n    broker_transport_options = {\n        CELERY_BROKER_QUEUE_NAME: {\n            \"my-q\": {\n                \"url\": CELERY_BROKER_URL,\n            }\n        }\n    }\n    celery = Celery(\n        __name__,\n        broker=CELERY_BROKER_URL,\n        task_serializer=\"json\",\n        task_concurrency=4,\n        worker_prefetch_multiplier=1,\n        broker_transport_options=broker_transport_options,\n    )\n    celery.conf.task_default_queue = CELERY_BROKER_QUEUE_NAME\nelif CELERY_BROKER_URL.startswith(\"redis\"):\n    celery = Celery(\n        __name__,\n        broker=f\"{CELERY_BROKER_URL}\",\n        backend=f\"{CELERY_BROKER_URL}\",\n        task_concurrency=4,\n        worker_prefetch_multiplier=2,\n        task_serializer=\"json\",\n    )\nelse:\n    raise ValueError(f\"Unsupported broker URL: {CELERY_BROKER_URL}\")\n\n\ncelery.autodiscover_tasks([\"modules.sync\", \"modules\", \"middlewares\", \"packages\"])\n", "backend/main.py": "import logging\nimport os\n\nimport litellm\nimport sentry_sdk\nfrom dotenv import load_dotenv  # type: ignore\nfrom fastapi import FastAPI, HTTPException, Request\nfrom fastapi.responses import HTMLResponse, JSONResponse\nfrom logger import get_logger\nfrom middlewares.cors import add_cors_middleware\nfrom modules.analytics.controller.analytics_routes import analytics_router\nfrom modules.api_key.controller import api_key_router\nfrom modules.assistant.controller import assistant_router\nfrom modules.brain.controller import brain_router\nfrom modules.chat.controller import chat_router\nfrom modules.contact_support.controller import contact_router\nfrom modules.knowledge.controller import knowledge_router\nfrom modules.misc.controller import misc_router\nfrom modules.onboarding.controller import onboarding_router\nfrom modules.prompt.controller import prompt_router\nfrom modules.sync.controller import sync_router\nfrom modules.upload.controller import upload_router\nfrom modules.user.controller import user_router\nfrom packages.utils import handle_request_validation_error\nfrom packages.utils.telemetry import maybe_send_telemetry\nfrom pyinstrument import Profiler\nfrom routes.crawl_routes import crawl_router\nfrom routes.subscription_routes import subscription_router\nfrom sentry_sdk.integrations.fastapi import FastApiIntegration\nfrom sentry_sdk.integrations.starlette import StarletteIntegration\n\nload_dotenv()\n\n# Set the logging level for all loggers to WARNING\nlogging.basicConfig(level=logging.INFO)\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\nlogging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\nlogging.getLogger(\"litellm\").setLevel(logging.WARNING)\nlitellm.set_verbose = False\n\n\nlogger = get_logger(__name__)\n\n\ndef before_send(event, hint):\n    # If this is a transaction event\n    if event[\"type\"] == \"transaction\":\n        # And the transaction name contains 'healthz'\n        if \"healthz\" in event[\"transaction\"]:\n            # Drop the event by returning None\n            return None\n    # For other events, return them as is\n    return event\n\n\nsentry_dsn = os.getenv(\"SENTRY_DSN\")\nif sentry_dsn:\n    sentry_sdk.init(\n        dsn=sentry_dsn,\n        sample_rate=0.1,\n        enable_tracing=True,\n        traces_sample_rate=0.1,\n        integrations=[\n            StarletteIntegration(transaction_style=\"url\"),\n            FastApiIntegration(transaction_style=\"url\"),\n        ],\n        before_send=before_send,\n    )\n\napp = FastAPI()\n\nadd_cors_middleware(app)\n\napp.include_router(brain_router)\napp.include_router(chat_router)\napp.include_router(crawl_router)\napp.include_router(assistant_router)\napp.include_router(sync_router)\napp.include_router(onboarding_router)\napp.include_router(misc_router)\napp.include_router(analytics_router)\n\napp.include_router(upload_router)\napp.include_router(user_router)\napp.include_router(api_key_router)\napp.include_router(subscription_router)\napp.include_router(prompt_router)\napp.include_router(knowledge_router)\napp.include_router(contact_router)\n\nPROFILING = os.getenv(\"PROFILING\", \"false\").lower() == \"true\"\n\n\nif PROFILING:\n\n    @app.middleware(\"http\")\n    async def profile_request(request: Request, call_next):\n        profiling = request.query_params.get(\"profile\", False)\n        if profiling:\n            profiler = Profiler()\n            profiler.start()\n            await call_next(request)\n            profiler.stop()\n            return HTMLResponse(profiler.output_html())\n        else:\n            return await call_next(request)\n\n\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(_, exc):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\"detail\": exc.detail},\n    )\n\n\nhandle_request_validation_error(app)\n\nif os.getenv(\"TELEMETRY_ENABLED\") == \"true\":\n    logger.info(\"Telemetry enabled, we use telemetry to collect anonymous usage data.\")\n    logger.info(\n        \"To disable telemetry, set the TELEMETRY_ENABLED environment variable to false.\"\n    )\n    maybe_send_telemetry(\"booting\", {\"status\": \"ok\"})\n    maybe_send_telemetry(\"ping\", {\"ping\": \"pong\"})\n\n\nif __name__ == \"__main__\":\n    # run main.py to debug backend\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=5050, log_level=\"warning\", access_log=False)\n", "backend/__init__.py": "", "backend/logger.py": "import logging\nimport os\nfrom logging.handlers import RotatingFileHandler\n\nfrom colorlog import (\n    ColoredFormatter,\n)  # You need to install this package: pip install colorlog\n\n\ndef get_logger(logger_name, log_file=\"application.log\"):\n    log_level = os.getenv(\"LOG_LEVEL\", \"WARNING\").upper()\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(log_level)\n    logger.propagate = False  # Prevent log propagation to avoid double logging\n\n    formatter = logging.Formatter(\n        \"[%(levelname)s] %(name)s [%(filename)s:%(lineno)d]: %(message)s\"\n    )\n\n    color_formatter = ColoredFormatter(\n        \"%(log_color)s[%(levelname)s]%(reset)s %(name)s [%(filename)s:%(lineno)d]: %(message)s\",\n        log_colors={\n            \"DEBUG\": \"cyan\",\n            \"INFO\": \"green\",\n            \"WARNING\": \"yellow\",\n            \"ERROR\": \"red\",\n            \"CRITICAL\": \"red,bg_white\",\n        },\n        reset=True,\n        style=\"%\",\n    )\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(color_formatter)\n\n    file_handler = RotatingFileHandler(\n        log_file, maxBytes=5000000, backupCount=5\n    )  # 5MB file\n    file_handler.setFormatter(formatter)\n\n    if not logger.handlers:\n        logger.addHandler(console_handler)\n        logger.addHandler(file_handler)\n\n    return logger\n", "backend/routes/crawl_routes.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom celery_worker import process_crawl_and_notify\nfrom fastapi import APIRouter, Depends, Query, Request\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.brain.entity.brain_entity import RoleEnum\nfrom modules.brain.service.brain_authorization_service import (\n    validate_brain_authorization,\n)\nfrom modules.knowledge.dto.inputs import CreateKnowledgeProperties\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.notification.dto.inputs import CreateNotification\nfrom modules.notification.entity.notification import NotificationsStatusEnum\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_usage import UserUsage\nfrom packages.files.crawl.crawler import CrawlWebsite\nfrom packages.files.file import convert_bytes\n\nlogger = get_logger(__name__)\ncrawl_router = APIRouter()\n\nnotification_service = NotificationService()\nknowledge_service = KnowledgeService()\n\n\n@crawl_router.get(\"/crawl/healthz\", tags=[\"Health\"])\nasync def healthz():\n    return {\"status\": \"ok\"}\n\n\n@crawl_router.post(\"/crawl\", dependencies=[Depends(AuthBearer())], tags=[\"Crawl\"])\nasync def crawl_endpoint(\n    request: Request,\n    crawl_website: CrawlWebsite,\n    brain_id: UUID = Query(..., description=\"The ID of the brain\"),\n    chat_id: Optional[UUID] = Query(None, description=\"The ID of the chat\"),\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Crawl a website and process the crawled data.\n    \"\"\"\n\n    validate_brain_authorization(\n        brain_id, current_user.id, [RoleEnum.Editor, RoleEnum.Owner]\n    )\n\n    userDailyUsage = UserUsage(\n        id=current_user.id,\n        email=current_user.email,\n    )\n    userSettings = userDailyUsage.get_user_settings()\n\n    file_size = 1000000\n    remaining_free_space = userSettings.get(\"max_brain_size\", 1000000000)\n\n    if remaining_free_space - file_size < 0:\n        message = {\n            \"message\": f\"\u274c UserIdentity's brain will exceed maximum capacity with this upload. Maximum file allowed is : {convert_bytes(remaining_free_space)}\",\n            \"type\": \"error\",\n        }\n    else:\n        upload_notification = notification_service.add_notification(\n            CreateNotification(\n                user_id=current_user.id,\n                status=NotificationsStatusEnum.INFO,\n                title=f\"Processing Crawl {crawl_website.url}\",\n            )\n        )\n        knowledge_to_add = CreateKnowledgeProperties(\n            brain_id=brain_id,\n            url=crawl_website.url,\n            extension=\"html\",\n        )\n\n        added_knowledge = knowledge_service.add_knowledge(knowledge_to_add)\n        logger.info(f\"Knowledge {added_knowledge} added successfully\")\n\n        process_crawl_and_notify.delay(\n            crawl_website_url=crawl_website.url,\n            brain_id=brain_id,\n            notification_id=upload_notification.id,\n        )\n\n        return {\"message\": \"Crawl processing has started.\"}\n    return message\n", "backend/routes/__init__.py": "", "backend/routes/subscription_routes.py": "from typing import List\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom middlewares.auth.auth_bearer import AuthBearer, get_current_user\nfrom models import BrainSubscription\nfrom modules.brain.entity.brain_entity import RoleEnum\nfrom modules.brain.service.api_brain_definition_service import ApiBrainDefinitionService\nfrom modules.brain.service.brain_authorization_service import (\n    has_brain_authorization,\n    validate_brain_authorization,\n)\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.brain.service.brain_user_service import BrainUserService\nfrom modules.prompt.entity.prompt import PromptStatusEnum\nfrom modules.prompt.service.prompt_service import PromptService\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_service import UserService\nfrom pydantic import BaseModel\nfrom modules.brain.service.brain_subscription import (\n    SubscriptionInvitationService,\n    resend_invitation_email,\n)\nfrom modules.brain.repository import (\n    IntegrationBrain,\n)\nfrom routes.headers.get_origin_header import get_origin_header\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\nsubscription_router = APIRouter()\nsubscription_service = SubscriptionInvitationService()\nuser_service = UserService()\nprompt_service = PromptService()\nbrain_user_service = BrainUserService()\nbrain_service = BrainService()\napi_brain_definition_service = ApiBrainDefinitionService()\nintegration_brains_repository = IntegrationBrain()\n\n\n@subscription_router.post(\n    \"/brains/{brain_id}/subscription\",\n    dependencies=[\n        Depends(\n            AuthBearer(),\n        ),\n        Depends(has_brain_authorization([RoleEnum.Owner, RoleEnum.Editor])),\n        Depends(get_origin_header),\n    ],\n    tags=[\"BrainSubscription\"],\n)\ndef invite_users_to_brain(\n    brain_id: UUID,\n    users: List[dict],\n    origin: str = Depends(get_origin_header),\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Invite multiple users to a brain by their emails. This function creates\n    or updates a brain subscription invitation for each user and sends an\n    invitation email to each user.\n    \"\"\"\n    for user in users:\n        subscription = BrainSubscription(\n            brain_id=brain_id, email=user[\"email\"], rights=user[\"rights\"]\n        )\n        # check if user is an editor but trying to give high level permissions\n        if subscription.rights == \"Owner\":\n            try:\n                validate_brain_authorization(\n                    brain_id,\n                    current_user.id,\n                    RoleEnum.Owner,\n                )\n            except HTTPException:\n                raise HTTPException(\n                    status_code=403,\n                    detail=\"You don't have the rights to give owner permissions\",\n                )\n\n        try:\n            should_send_invitation_email = (\n                subscription_service.create_or_update_subscription_invitation(\n                    subscription\n                )\n            )\n            if should_send_invitation_email:\n                resend_invitation_email(\n                    subscription,\n                    inviter_email=current_user.email or \"Quivr\",\n                    user_id=current_user.id,\n                    origin=origin,\n                )\n        except Exception as e:\n            raise HTTPException(status_code=400, detail=f\"Error inviting user: {e}\")\n\n    return {\"message\": \"Invitations sent successfully\"}\n\n\n@subscription_router.get(\n    \"/brains/{brain_id}/users\",\n    dependencies=[\n        Depends(AuthBearer()),\n        Depends(has_brain_authorization([RoleEnum.Owner, RoleEnum.Editor])),\n    ],\n)\ndef get_users_with_brain_access(\n    brain_id: UUID,\n):\n    \"\"\"\n    Get all users for a brain\n    \"\"\"\n\n    brain_users = brain_user_service.get_brain_users(\n        brain_id=brain_id,\n    )\n\n    brain_access_list = []\n\n    for brain_user in brain_users:\n        brain_access = {}\n        # TODO: find a way to fetch user email concurrently\n        brain_access[\"email\"] = user_service.get_user_email_by_user_id(\n            brain_user.user_id\n        )\n        brain_access[\"rights\"] = brain_user.rights\n        brain_access_list.append(brain_access)\n\n    return brain_access_list\n\n\n@subscription_router.delete(\n    \"/brains/{brain_id}/subscription\",\n)\nasync def remove_user_subscription(\n    brain_id: UUID, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Remove a user's subscription to a brain\n    \"\"\"\n    targeted_brain = brain_service.get_brain_by_id(brain_id)\n\n    if targeted_brain is None:\n        raise HTTPException(\n            status_code=404,\n            detail=\"Brain not found while trying to delete\",\n        )\n\n    user_brain = brain_user_service.get_brain_for_user(current_user.id, brain_id)\n    if user_brain is None:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You don't have permission for this brain\",\n        )\n\n    if user_brain.rights != \"Owner\":\n        brain_user_service.delete_brain_user(current_user.id, brain_id)\n    else:\n        brain_users = brain_user_service.get_brain_users(\n            brain_id=brain_id,\n        )\n        brain_other_owners = [\n            brain\n            for brain in brain_users\n            if brain.rights == \"Owner\" and str(brain.user_id) != str(current_user.id)\n        ]\n\n        if len(brain_other_owners) == 0:\n            brain_service.delete_brain(\n                brain_id=brain_id,\n            )\n            if targeted_brain.prompt_id:\n                brain_to_delete_prompt = prompt_service.get_prompt_by_id(\n                    targeted_brain.prompt_id\n                )\n                if brain_to_delete_prompt is not None and (\n                    brain_to_delete_prompt.status == PromptStatusEnum.private\n                ):\n                    prompt_service.delete_prompt_by_id(targeted_brain.prompt_id)\n\n        else:\n            brain_user_service.delete_brain_user(\n                current_user.id,\n                brain_id,\n            )\n\n    return {\"message\": f\"Subscription removed successfully from brain {brain_id}\"}\n\n\n@subscription_router.get(\n    \"/brains/{brain_id}/subscription\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"BrainSubscription\"],\n)\ndef get_user_invitation(\n    brain_id: UUID, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Get an invitation to a brain for a user. This function checks if the user\n    has been invited to the brain and returns the invitation status.\n    \"\"\"\n    if not current_user.email:\n        raise HTTPException(status_code=400, detail=\"UserIdentity email is not defined\")\n\n    subscription = BrainSubscription(brain_id=brain_id, email=current_user.email)\n\n    invitation = subscription_service.fetch_invitation(subscription)\n\n    if invitation is None:\n        raise HTTPException(\n            status_code=404,\n            detail=\"You have not been invited to this brain\",\n        )\n\n    brain_details = brain_service.get_brain_details(brain_id, current_user.id)\n\n    if brain_details is None:\n        raise HTTPException(\n            status_code=404,\n            detail=\"Brain not found while trying to get invitation\",\n        )\n\n    return {\"name\": brain_details.name, \"rights\": invitation[\"rights\"]}\n\n\n@subscription_router.post(\n    \"/brains/{brain_id}/subscription/accept\",\n    tags=[\"Brain\"],\n)\nasync def accept_invitation(\n    brain_id: UUID, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Accept an invitation to a brain for a user. This function removes the\n    invitation from the subscription invitations and adds the user to the\n    brain users.\n    \"\"\"\n    if not current_user.email:\n        raise HTTPException(status_code=400, detail=\"UserIdentity email is not defined\")\n\n    subscription = BrainSubscription(brain_id=brain_id, email=current_user.email)\n\n    try:\n        invitation = subscription_service.fetch_invitation(subscription)\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Error fetching invitation: {e}\")\n\n    if not invitation:\n        raise HTTPException(status_code=404, detail=\"Invitation not found\")\n\n    try:\n        brain_user_service.create_brain_user(\n            user_id=current_user.id,\n            brain_id=brain_id,\n            rights=invitation[\"rights\"],\n            is_default_brain=False,\n        )\n        shared_brain = brain_service.get_brain_by_id(brain_id)\n        integration_brain = integration_brains_repository.get_integration_brain(\n            brain_id=brain_id,\n        )\n        integration_brains_repository.add_integration_brain(\n            brain_id=brain_id,\n            user_id=current_user.id,\n            integration_id=integration_brain.integration_id,\n            settings=integration_brain.settings,\n        )\n    except Exception as e:\n        logger.error(f\"Error adding user to brain: {e}\")\n        raise HTTPException(status_code=400, detail=f\"Error adding user to brain: {e}\")\n\n    try:\n        subscription_service.remove_invitation(subscription)\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Error removing invitation: {e}\")\n\n    return {\"message\": \"Invitation accepted successfully\"}\n\n\n@subscription_router.post(\n    \"/brains/{brain_id}/subscription/decline\",\n    tags=[\"Brain\"],\n)\nasync def decline_invitation(\n    brain_id: UUID, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Decline an invitation to a brain for a user. This function removes the\n    invitation from the subscription invitations.\n    \"\"\"\n    if not current_user.email:\n        raise HTTPException(status_code=400, detail=\"UserIdentity email is not defined\")\n\n    subscription = BrainSubscription(brain_id=brain_id, email=current_user.email)\n\n    try:\n        invitation = subscription_service.fetch_invitation(subscription)\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Error fetching invitation: {e}\")\n\n    if not invitation:\n        raise HTTPException(status_code=404, detail=\"Invitation not found\")\n\n    try:\n        subscription_service.remove_invitation(subscription)\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Error removing invitation: {e}\")\n\n    return {\"message\": \"Invitation declined successfully\"}\n\n\nclass BrainSubscriptionUpdatableProperties(BaseModel):\n    rights: str | None = None\n    email: str\n\n\n@subscription_router.put(\n    \"/brains/{brain_id}/subscription\",\n    dependencies=[\n        Depends(AuthBearer()),\n        Depends(has_brain_authorization([RoleEnum.Owner, RoleEnum.Editor])),\n    ],\n)\ndef update_brain_subscription(\n    brain_id: UUID,\n    subscription: BrainSubscriptionUpdatableProperties,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    user_email = subscription.email\n    if user_email == current_user.email:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You can't change your own permissions\",\n        )\n\n    user_id = user_service.get_user_id_by_email(user_email)\n\n    if user_id is None:\n        raise HTTPException(\n            status_code=404,\n            detail=\"User not found\",\n        )\n\n    # check if user is an editor but trying to give high level permissions\n    if subscription.rights == \"Owner\":\n        try:\n            validate_brain_authorization(\n                brain_id,\n                current_user.id,\n                RoleEnum.Owner,\n            )\n        except HTTPException:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You don't have the rights to give owner permissions\",\n            )\n\n    # check if user is not an editor trying to update an owner right which is not allowed\n    current_invitation = brain_user_service.get_brain_for_user(user_id, brain_id)\n    if current_invitation is not None and current_invitation.rights == \"Owner\":\n        try:\n            validate_brain_authorization(\n                brain_id,\n                current_user.id,\n                RoleEnum.Owner,\n            )\n        except HTTPException:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You can't change the permissions of an owner\",\n            )\n\n    # removing user access from brain\n    if subscription.rights is None:\n        try:\n            # only owners can remove user access to a brain\n            validate_brain_authorization(\n                brain_id,\n                current_user.id,\n                RoleEnum.Owner,\n            )\n            brain_user_service.delete_brain_user(user_id, brain_id)\n        except HTTPException:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You don't have the rights to remove user access\",\n            )\n    else:\n        brain_user_service.update_brain_user_rights(\n            brain_id, user_id, subscription.rights\n        )\n\n    return {\"message\": \"Brain subscription updated successfully\"}\n\n\n@subscription_router.post(\n    \"/brains/{brain_id}/subscribe\",\n    tags=[\"Subscription\"],\n)\nasync def subscribe_to_brain_handler(\n    brain_id: UUID,\n    secrets: dict = {},\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Subscribe to a public brain\n    \"\"\"\n    if not current_user.email:\n        raise HTTPException(status_code=400, detail=\"UserIdentity email is not defined\")\n\n    brain = brain_service.get_brain_by_id(brain_id)\n\n    if brain is None:\n        raise HTTPException(status_code=404, detail=\"Brain not found\")\n    if brain.status != \"public\":\n        raise HTTPException(\n            status_code=403,\n            detail=\"You cannot subscribe to this brain without invitation\",\n        )\n    # check if user is already subscribed to brain\n    user_brain = brain_user_service.get_brain_for_user(current_user.id, brain_id)\n    if user_brain is not None:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You are already subscribed to this brain\",\n        )\n    if brain.brain_type == \"api\":\n        brain_definition = api_brain_definition_service.get_api_brain_definition(\n            brain_id\n        )\n        brain_secrets = brain_definition.secrets if brain_definition != None else []\n\n        for secret in brain_secrets:\n            if not secrets[secret.name]:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Please provide the secret {secret}\",\n                )\n\n        for secret in brain_secrets:\n            brain_service.external_api_secrets_repository.create_secret(\n                user_id=current_user.id,\n                brain_id=brain_id,\n                secret_name=secret.name,\n                secret_value=secrets[secret.name],\n            )\n\n    try:\n        brain_user_service.create_brain_user(\n            user_id=current_user.id,\n            brain_id=brain_id,\n            rights=RoleEnum.Viewer,\n            is_default_brain=False,\n        )\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Error adding user to brain: {e}\")\n\n    return {\"message\": \"You have successfully subscribed to the brain\"}\n\n\n@subscription_router.post(\n    \"/brains/{brain_id}/unsubscribe\",\n    tags=[\"Subscription\"],\n)\nasync def unsubscribe_from_brain_handler(\n    brain_id: UUID, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Unsubscribe from a brain\n    \"\"\"\n    if not current_user.email:\n        raise HTTPException(status_code=400, detail=\"UserIdentity email is not defined\")\n\n    brain = brain_service.get_brain_by_id(brain_id)\n\n    if brain is None:\n        raise HTTPException(status_code=404, detail=\"Brain not found\")\n\n    # check if user is already subscribed to brain\n    user_brain = brain_user_service.get_brain_for_user(current_user.id, brain_id)\n    if user_brain is None:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You are not subscribed to this brain\",\n        )\n    brain_user_service.delete_brain_user(user_id=current_user.id, brain_id=brain_id)\n\n    return {\"message\": \"You have successfully unsubscribed from the brain\"}\n", "backend/routes/headers/get_origin_header.py": "from typing import Optional\n\nfrom fastapi import Header\n\n\ndef get_origin_header(origin: Optional[str] = Header(None)):\n    return origin\n", "backend/routes/headers/__init_.py": "", "backend/models/files_in_storage.py": "from uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass FileInStorage(BaseModel):\n    Id: UUID\n    Key: str\n\n    @property\n    def id(self) -> UUID:\n        return self.Id\n\n    @property\n    def key(self) -> str:\n        return self.Key\n", "backend/models/settings.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom langchain_community.embeddings.ollama import OllamaEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\nfrom logger import get_logger\nfrom models.databases.supabase.supabase import SupabaseDB\nfrom posthog import Posthog\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom sqlalchemy import Engine, create_engine\nfrom supabase.client import Client, create_client\nfrom langchain_community.vectorstores.supabase import SupabaseVectorStore\nlogger = get_logger(__name__)\n\n\nclass BrainRateLimiting(BaseSettings):\n    model_config = SettingsConfigDict(validate_default=False)\n    max_brain_per_user: int = 5\n\n\n# The `PostHogSettings` class is used to initialize and interact with the PostHog analytics service.\nclass PostHogSettings(BaseSettings):\n    model_config = SettingsConfigDict(validate_default=False)\n    posthog_api_key: str = None\n    posthog_api_url: str = None\n    posthog: Posthog = None\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        The function initializes the \"posthog\" attribute and calls the \"initialize_posthog\" method.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.posthog = None\n        self.initialize_posthog()\n\n    def initialize_posthog(self):\n        \"\"\"\n        The function initializes a PostHog client with an API key and URL.\n        \"\"\"\n        if self.posthog_api_key and self.posthog_api_url:\n            self.posthog = Posthog(\n                api_key=self.posthog_api_key, host=self.posthog_api_url\n            )\n\n    def log_event(self, user_id: UUID, event_name: str, event_properties: dict):\n        \"\"\"\n        The function logs an event with a user ID, event name, and event properties using the PostHog\n        analytics tool.\n\n        :param user_id: The user_id parameter is a UUID (Universally Unique Identifier) that uniquely\n        identifies a user. It is typically used to track and identify individual users in an application\n        or system\n        :type user_id: UUID\n        :param event_name: The event_name parameter is a string that represents the name or type of the\n        event that you want to log. It could be something like \"user_signed_up\", \"item_purchased\", or\n        \"page_viewed\"\n        :type event_name: str\n        :param event_properties: The event_properties parameter is a dictionary that contains additional\n        information or properties related to the event being logged. These properties provide more\n        context or details about the event and can be used for analysis or filtering purposes\n        :type event_properties: dict\n        \"\"\"\n        if self.posthog:\n            self.posthog.capture(user_id, event_name, event_properties)\n\n    def set_user_properties(self, user_id: UUID, event_name, properties: dict):\n        \"\"\"\n        The function sets user properties for a given user ID and event name using the PostHog analytics\n        tool.\n\n        :param user_id: The user_id parameter is a UUID (Universally Unique Identifier) that uniquely\n        identifies a user. It is used to associate the user with the event and properties being captured\n        :type user_id: UUID\n        :param event_name: The `event_name` parameter is a string that represents the name of the event\n        that you want to capture. It could be something like \"user_signed_up\" or \"item_purchased\"\n        :param properties: The `properties` parameter is a dictionary that contains the user properties\n        that you want to set. Each key-value pair in the dictionary represents a user property, where\n        the key is the name of the property and the value is the value you want to set for that property\n        :type properties: dict\n        \"\"\"\n        if self.posthog:\n            self.posthog.capture(\n                user_id, event=event_name, properties={\"$set\": properties}\n            )\n\n    def set_once_user_properties(self, user_id: UUID, event_name, properties: dict):\n        \"\"\"\n        The function sets user properties for a specific event, ensuring that the properties are only\n        set once.\n\n        :param user_id: The user_id parameter is a UUID (Universally Unique Identifier) that uniquely\n        identifies a user\n        :type user_id: UUID\n        :param event_name: The `event_name` parameter is a string that represents the name of the event\n        that you want to capture. It could be something like \"user_signed_up\" or \"item_purchased\"\n        :param properties: The `properties` parameter is a dictionary that contains the user properties\n        that you want to set. Each key-value pair in the dictionary represents a user property, where\n        the key is the property name and the value is the property value\n        :type properties: dict\n        \"\"\"\n        if self.posthog:\n            self.posthog.capture(\n                user_id, event=event_name, properties={\"$set_once\": properties}\n            )\n\n\nclass BrainSettings(BaseSettings):\n    model_config = SettingsConfigDict(validate_default=False)\n    openai_api_key: str = \"\"\n    supabase_url: str = \"\"\n    supabase_service_key: str = \"\"\n    resend_api_key: str = \"null\"\n    resend_email_address: str = \"brain@mail.quivr.app\"\n    ollama_api_base_url: str = None\n    langfuse_public_key: str = None\n    langfuse_secret_key: str = None\n    pg_database_url: str = None\n\n\nclass ResendSettings(BaseSettings):\n    model_config = SettingsConfigDict(validate_default=False)\n    resend_api_key: str = \"null\"\n\n\n# Global variables to store the Supabase client and database instances\n_supabase_client: Optional[Client] = None\n_supabase_db: Optional[SupabaseDB] = None\n_db_engine: Optional[Engine] = None\n\n\ndef get_pg_database_engine():\n    global _db_engine\n    if _db_engine is None:\n        logger.info(\"Creating Postgres DB engine\")\n        settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n        _db_engine = create_engine(settings.pg_database_url, pool_pre_ping=True)\n    return _db_engine\n\n\ndef get_supabase_client() -> Client:\n    global _supabase_client\n    if _supabase_client is None:\n        logger.info(\"Creating Supabase client\")\n        settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n        _supabase_client = create_client(\n            settings.supabase_url, settings.supabase_service_key\n        )\n    return _supabase_client\n\n\ndef get_supabase_db() -> SupabaseDB:\n    global _supabase_db\n    if _supabase_db is None:\n        logger.info(\"Creating Supabase DB\")\n        _supabase_db = SupabaseDB(get_supabase_client())\n    return _supabase_db\n\n\ndef get_embeddings():\n    settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n    if settings.ollama_api_base_url:\n        embeddings = OllamaEmbeddings(\n            base_url=settings.ollama_api_base_url,\n        )  # pyright: ignore reportPrivateUsage=none\n    else:\n        embeddings = OpenAIEmbeddings()  # pyright: ignore reportPrivateUsage=none\n    return embeddings\n\n\ndef get_documents_vector_store() -> SupabaseVectorStore:\n    settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n    embeddings = get_embeddings()\n    supabase_client: Client = get_supabase_client()\n    documents_vector_store = SupabaseVectorStore(\n        supabase_client, embeddings, table_name=\"vectors\"\n    )\n    return documents_vector_store\n", "backend/models/brains_subscription_invitations.py": "from uuid import UUID\n\nfrom logger import get_logger\nfrom pydantic import ConfigDict, BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass BrainSubscription(BaseModel):\n    brain_id: UUID\n    email: str\n    rights: str = \"Viewer\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n", "backend/models/files.py": "from pathlib import Path\nfrom typing import List, Optional\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom logger import get_logger\nfrom models.databases.supabase.supabase import SupabaseDB\nfrom models.settings import get_supabase_db\nfrom modules.brain.service.brain_vector_service import BrainVectorService\nfrom packages.files.file import compute_sha1_from_content\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass File(BaseModel):\n    file_name: str\n    tmp_file_path: Path\n    bytes_content: bytes\n    file_size: int\n    file_extension: str\n    chunk_size: int = 400\n    chunk_overlap: int = 100\n    documents: List[Document] = []\n    file_sha1: Optional[str] = None\n    vectors_ids: Optional[list] = []\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        data[\"file_sha1\"] = compute_sha1_from_content(data[\"bytes_content\"])\n\n    @property\n    def supabase_db(self) -> SupabaseDB:\n        return get_supabase_db()\n\n    def compute_documents(self, loader_class):\n        \"\"\"\n        Compute the documents from the file\n\n        Args:\n            loader_class (class): The class of the loader to use to load the file\n        \"\"\"\n        logger.info(f\"Computing documents from file {self.file_name}\")\n        loader = loader_class(self.tmp_file_path)\n        documents = loader.load()\n\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap\n        )\n\n        self.documents = text_splitter.split_documents(documents)\n\n    def set_file_vectors_ids(self):\n        \"\"\"\n        Set the vectors_ids property with the ids of the vectors\n        that are associated with the file in the vectors table\n        \"\"\"\n        self.vectors_ids = self.supabase_db.get_vectors_by_file_sha1(\n            self.file_sha1\n        ).data\n\n    def file_already_exists(self):\n        \"\"\"\n        Check if file already exists in vectors table\n        \"\"\"\n        self.set_file_vectors_ids()\n\n        # if the file does not exist in vectors then no need to go check in brains_vectors\n        if len(self.vectors_ids) == 0:  # pyright: ignore reportPrivateUsage=none\n            return False\n\n        return True\n\n    def file_already_exists_in_brain(self, brain_id):\n        \"\"\"\n        Check if file already exists in a brain\n\n        Args:\n            brain_id (str): Brain id\n        \"\"\"\n        response = self.supabase_db.get_brain_vectors_by_brain_id_and_file_sha1(\n            brain_id, self.file_sha1  # type: ignore\n        )\n\n        if len(response.data) == 0:\n            return False\n\n        return True\n\n    def file_is_empty(self):\n        \"\"\"\n        Check if file is empty by checking if the file pointer is at the beginning of the file\n        \"\"\"\n        return self.file_size < 1  # pyright: ignore reportPrivateUsage=none\n\n    def link_file_to_brain(self, brain_id):\n        self.set_file_vectors_ids()\n\n        if self.vectors_ids is None:\n            return\n\n        brain_vector_service = BrainVectorService(brain_id)\n\n        for vector_id in self.vectors_ids:  # pyright: ignore reportPrivateUsage=none\n            brain_vector_service.create_brain_vector(vector_id[\"id\"], self.file_sha1)\n", "backend/models/sqlalchemy_repository.py": "from datetime import datetime\nfrom uuid import uuid4\n\nfrom sqlalchemy import Boolean, Column, DateTime, ForeignKey, Integer, String\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\n\nBase = declarative_base()\n\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    user_id = Column(String, primary_key=True)\n    email = Column(String)\n    date = Column(DateTime)\n    daily_requests_count = Column(Integer)\n\n\nclass Brain(Base):\n    __tablename__ = \"brains\"\n\n    brain_id = Column(Integer, primary_key=True)\n    name = Column(String)\n    users = relationship(\"BrainUser\", back_populates=\"brain\")\n    vectors = relationship(\"BrainVector\", back_populates=\"brain\")\n\n\nclass BrainUser(Base):\n    __tablename__ = \"brains_users\"\n\n    id = Column(Integer, primary_key=True)\n    user_id = Column(Integer, ForeignKey(\"users.user_id\"))\n    brain_id = Column(Integer, ForeignKey(\"brains.brain_id\"))\n    rights = Column(String)\n\n    user = relationship(\"User\")\n    brain = relationship(\"Brain\", back_populates=\"users\")\n\n\nclass BrainVector(Base):\n    __tablename__ = \"brains_vectors\"\n\n    vector_id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n    brain_id = Column(Integer, ForeignKey(\"brains.brain_id\"))\n    file_sha1 = Column(String)\n\n    brain = relationship(\"Brain\", back_populates=\"vectors\")\n\n\nclass BrainSubscriptionInvitation(Base):\n    __tablename__ = \"brain_subscription_invitations\"\n\n    id = Column(Integer, primary_key=True)  # Assuming an integer primary key named 'id'\n    brain_id = Column(String, ForeignKey(\"brains.brain_id\"))\n    email = Column(String, ForeignKey(\"users.email\"))\n    rights = Column(String)\n\n    brain = relationship(\"Brain\")\n    user = relationship(\"User\", foreign_keys=[email])\n\n\nclass ApiKey(Base):\n    __tablename__ = \"api_keys\"\n\n    key_id = Column(String, primary_key=True, default=lambda: str(uuid4()))\n    user_id = Column(Integer, ForeignKey(\"users.user_id\"))\n    api_key = Column(String, unique=True)\n    creation_time = Column(DateTime, default=datetime.utcnow)\n    is_active = Column(Boolean, default=True)\n    deleted_time = Column(DateTime, nullable=True)\n\n    user = relationship(\"User\")\n", "backend/models/__init__.py": "from .brains_subscription_invitations import BrainSubscription\nfrom .files import File\nfrom .settings import (BrainRateLimiting, BrainSettings, ResendSettings,\n                       get_documents_vector_store, get_embeddings,\n                       get_supabase_client, get_supabase_db)\n", "backend/vectorstore/supabase.py": "from typing import Any, List\n\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.base import Embeddings\nfrom langchain_community.vectorstores import SupabaseVectorStore\nfrom logger import get_logger\nfrom supabase.client import Client\n\nlogger = get_logger(__name__)\n\n\nclass CustomSupabaseVectorStore(SupabaseVectorStore):\n    \"\"\"A custom vector store that uses the match_vectors table instead of the vectors table.\"\"\"\n\n    brain_id: str = \"none\"\n    user_id: str = \"none\"\n    number_docs: int = 35\n    max_input: int = 2000\n\n    def __init__(\n        self,\n        client: Client,\n        embedding: Embeddings,\n        table_name: str,\n        brain_id: str = \"none\",\n        user_id: str = \"none\",\n        number_docs: int = 35,\n        max_input: int = 2000,\n    ):\n        super().__init__(client, embedding, table_name)\n        self.brain_id = brain_id\n        self.user_id = user_id\n        self.number_docs = number_docs\n        self.max_input = max_input\n\n    def find_brain_closest_query(\n        self,\n        user_id: str,\n        query: str,\n        k: int = 6,\n        table: str = \"match_brain\",\n        threshold: float = 0.5,\n    ) -> [dict]:\n        vectors = self._embedding.embed_documents([query])\n        query_embedding = vectors[0]\n\n        res = self._client.rpc(\n            table,\n            {\n                \"query_embedding\": query_embedding,\n                \"match_count\": self.number_docs,\n                \"p_user_id\": str(self.user_id),\n            },\n        ).execute()\n\n        # Get the brain_id of the brain that is most similar to the query\n        # Get the brain_id and name of the brains that are most similar to the query\n        brain_details = [\n            {\n                \"id\": item.get(\"id\", None),\n                \"name\": item.get(\"name\", None),\n                \"similarity\": item.get(\"similarity\", 0.0),\n            }\n            for item in res.data\n        ]\n        return brain_details\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = 40,\n        table: str = \"match_vectors\",\n        threshold: float = 0.5,\n        **kwargs: Any,\n    ) -> List[Document]:\n        vectors = self._embedding.embed_documents([query])\n        query_embedding = vectors[0]\n        res = self._client.rpc(\n            table,\n            {\n                \"query_embedding\": query_embedding,\n                \"max_chunk_sum\": self.max_input,\n                \"p_brain_id\": str(self.brain_id),\n            },\n        ).execute()\n\n        match_result = [\n            Document(\n                metadata={\n                    **search.get(\"metadata\", {}),\n                    \"id\": search.get(\"id\", \"\"),\n                    \"similarity\": search.get(\"similarity\", 0.0),\n                },\n                page_content=search.get(\"content\", \"\"),\n            )\n            for search in res.data\n            if search.get(\"content\")\n        ]\n\n        sorted_match_result_by_file_name_metadata = sorted(\n            match_result,\n            key=lambda x: (\n                x.metadata.get(\"file_name\", \"\"),\n                x.metadata.get(\"index\", float(\"inf\")),\n            ),\n        )\n\n        return sorted_match_result_by_file_name_metadata\n", "backend/vectorstore/__init__.py": "", "backend/middlewares/cors.py": "from fastapi.middleware.cors import CORSMiddleware\n\norigins = [\n    \"http://localhost\",\n    \"http://localhost:3000\",\n    \"http://localhost:3001\",\n    \"https://quivr.app\",\n    \"https://www.quivr.app\",\n    \"http://quivr.app\",\n    \"http://www.quivr.app\",\n    \"https://chat.quivr.app\",\n    \"*\",\n]\n\n\ndef add_cors_middleware(app):\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n", "backend/middlewares/__init__.py": "", "backend/middlewares/auth/auth_bearer.py": "import os\nfrom typing import Optional\n\nfrom fastapi import Depends, HTTPException, Request\nfrom fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\nfrom middlewares.auth.jwt_token_handler import decode_access_token, verify_token\nfrom modules.api_key.service.api_key_service import ApiKeyService\nfrom modules.user.entity.user_identity import UserIdentity\n\napi_key_service = ApiKeyService()\n\n\nclass AuthBearer(HTTPBearer):\n    def __init__(self, auto_error: bool = True):\n        super().__init__(auto_error=auto_error)\n\n    async def __call__(\n        self,\n        request: Request,\n    ):\n        credentials: Optional[HTTPAuthorizationCredentials] = await super().__call__(\n            request\n        )\n        self.check_scheme(credentials)\n        token = credentials.credentials  # pyright: ignore reportPrivateUsage=none\n        return await self.authenticate(\n            token,\n        )\n\n    def check_scheme(self, credentials):\n        if credentials and credentials.scheme != \"Bearer\":\n            raise HTTPException(status_code=401, detail=\"Token must be Bearer\")\n        elif not credentials:\n            raise HTTPException(\n                status_code=403, detail=\"Authentication credentials missing\"\n            )\n\n    async def authenticate(\n        self,\n        token: str,\n    ) -> UserIdentity:\n        if os.environ.get(\"AUTHENTICATE\") == \"false\":\n            return self.get_test_user()\n        elif verify_token(token):\n            return decode_access_token(token)\n        elif await api_key_service.verify_api_key(\n            token,\n        ):\n            return await api_key_service.get_user_from_api_key(\n                token,\n            )\n        else:\n            raise HTTPException(status_code=401, detail=\"Invalid token or api key.\")\n\n    def get_test_user(self) -> UserIdentity:\n        return UserIdentity(\n            email=\"test@example.com\", id=\"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"  # type: ignore\n        )  # replace with test user information\n\n\ndef get_current_user(user: UserIdentity = Depends(AuthBearer())) -> UserIdentity:\n    return user\n", "backend/middlewares/auth/__init__.py": "from .auth_bearer import AuthBearer, get_current_user\n\n__all__ = [\n    \"AuthBearer\",\n    \"get_current_user\",\n]\n", "backend/middlewares/auth/jwt_token_handler.py": "import os\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom jose import jwt\nfrom jose.exceptions import JWTError\nfrom modules.user.entity.user_identity import UserIdentity\n\nSECRET_KEY = os.environ.get(\"JWT_SECRET_KEY\")\nALGORITHM = \"HS256\"\n\nif not SECRET_KEY:\n    raise ValueError(\"JWT_SECRET_KEY environment variable not set\")\n\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\n\ndef decode_access_token(token: str) -> UserIdentity:\n    try:\n        payload = jwt.decode(\n            token, SECRET_KEY, algorithms=[ALGORITHM], options={\"verify_aud\": False}\n        )\n    except JWTError:\n        return None  # pyright: ignore reportPrivateUsage=none\n\n    return UserIdentity(\n        email=payload.get(\"email\"),\n        id=payload.get(\"sub\"),  # pyright: ignore reportPrivateUsage=none\n    )\n\n\ndef verify_token(token: str):\n    payload = decode_access_token(token)\n    return payload is not None\n", "backend/packages/__init__.py": "", "backend/packages/utils/parse_message_time.py": "from datetime import datetime\n\n\ndef parse_message_time(message_time_str):\n    return datetime.strptime(message_time_str, \"%Y-%m-%dT%H:%M:%S.%f\")\n", "backend/packages/utils/telemetry.py": "import hashlib\nimport json\nimport os\nimport threading\n\nimport httpx\nfrom fastapi import Request\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\n# Assume these are your Supabase Function endpoint and any necessary headers\nTELEMETRY_URL = \"https://ovbvcnwemowuuuaebizd.supabase.co/functions/v1/telemetry\"\nHEADERS = {\n    \"Content-Type\": \"application/json\",\n}\n\n\ndef generate_machine_key():\n    # Get the OpenAI API key from the environment variables\n    seed = os.getenv(\"OPENAI_API_KEY\")\n\n    # Use SHA-256 hash to generate a unique key from the seed\n    unique_key = hashlib.sha256(seed.encode()).hexdigest()\n\n    return unique_key\n\n\ndef send_telemetry(event_name: str, event_data: dict, request: Request = None):\n    # Generate a unique machine key\n    machine_key = generate_machine_key()\n    domain = None\n    if request:\n        domain = request.url.hostname\n        logger.info(f\"Domain: {domain}\")\n        event_data = {**event_data, \"domain\": domain}\n    # Prepare the payload\n    payload = json.dumps(\n        {\n            \"anonymous_identifier\": machine_key,\n            \"event_name\": event_name,\n            \"event_data\": event_data,\n        }\n    )\n\n    # Send the telemetry data\n    with httpx.Client() as client:\n        _ = client.post(TELEMETRY_URL, headers=HEADERS, data=payload)\n\n\ndef maybe_send_telemetry(event_name: str, event_data: dict, request: Request = None):\n    enable_telemetry = os.getenv(\"TELEMETRY_ENABLED\", \"false\")\n\n    if enable_telemetry.lower() != \"true\":\n        return\n\n    threading.Thread(\n        target=send_telemetry, args=(event_name, event_data, request)\n    ).start()\n\n\nasync def main():\n    await send_telemetry(\"user_login\", {\"login_success\": True})\n\n\n# Run the example\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n", "backend/packages/utils/__init__.py": "from .handle_request_validation_error import handle_request_validation_error\nfrom .parse_message_time import parse_message_time\n", "backend/packages/utils/handle_request_validation_error.py": "from fastapi import FastAPI, Request, status\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef handle_request_validation_error(app: FastAPI):\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(\n        request: Request, exc: RequestValidationError\n    ):\n        exc_str = f\"{exc}\".replace(\"\\n\", \" \").replace(\"   \", \" \")\n        logger.error(request, exc_str)\n        content = {\n            \"status_code\": status.HTTP_422_UNPROCESSABLE_ENTITY,\n            \"message\": exc_str,\n            \"data\": None,\n        }\n        return JSONResponse(\n            content=content, status_code=status.HTTP_422_UNPROCESSABLE_ENTITY\n        )\n", "backend/packages/emails/send_email.py": "from typing import Dict\n\nimport resend\nfrom models import ResendSettings\n\n\ndef send_email(params: Dict):\n    settings = ResendSettings()\n    resend.api_key = settings.resend_api_key\n    return resend.Emails.send(params)\n", "backend/packages/emails/__init__.py": "", "backend/packages/embeddings/vectors.py": "from concurrent.futures import ThreadPoolExecutor\nfrom typing import List\nfrom uuid import UUID\n\nfrom logger import get_logger\nfrom models.settings import get_documents_vector_store, get_embeddings, get_supabase_db\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\n# TODO: Create interface for embeddings and implement it for Supabase and OpenAI (current Quivr)\nclass Neurons(BaseModel):\n    def create_vector(self, docs):\n        documents_vector_store = get_documents_vector_store()\n\n        try:\n            sids = documents_vector_store.add_documents(docs)\n            if sids and len(sids) > 0:\n                return sids\n\n        except Exception as e:\n            logger.error(f\"Error creating vector for document {e}\")\n\n    def create_embedding(self, content):\n        embeddings = get_embeddings()\n        return embeddings.embed_query(content)\n\n\ndef error_callback(exception):\n    print(\"An exception occurred:\", exception)\n\n\ndef process_batch(batch_ids: List[str]):\n    supabase_db = get_supabase_db()\n\n    try:\n        if len(batch_ids) == 1:\n            return (supabase_db.get_vectors_by_batch(UUID(batch_ids[0]))).data\n        else:\n            return (supabase_db.get_vectors_in_batch(batch_ids)).data\n    except Exception as e:\n        logger.error(\"Error retrieving batched vectors\", e)\n\n\n# TODO: move to Knowledge class\ndef get_unique_files_from_vector_ids(vectors_ids):\n    # Move into Vectors class\n    \"\"\"\n    Retrieve unique user data vectors.\n    \"\"\"\n\n    # constants\n    BATCH_SIZE = 5\n\n    with ThreadPoolExecutor() as executor:\n        futures = []\n        for i in range(0, len(vectors_ids), BATCH_SIZE):\n            batch_ids = vectors_ids[i : i + BATCH_SIZE]\n            future = executor.submit(process_batch, batch_ids)\n            futures.append(future)\n\n        # Retrieve the results\n        vectors_responses = [future.result() for future in futures]\n\n    documents = [item for sublist in vectors_responses for item in sublist]\n    unique_files = [dict(t) for t in set(tuple(d.items()) for d in documents)]\n    return unique_files\n", "backend/packages/embeddings/__init__.py": "", "backend/packages/files/file.py": "import hashlib\nfrom io import BytesIO\n\nfrom fastapi import UploadFile\n\n\ndef convert_bytes(bytes, precision=2):\n    \"\"\"Converts bytes into a human-friendly format.\"\"\"\n    abbreviations = [\"B\", \"KB\", \"MB\"]\n    if bytes <= 0:\n        return \"0 B\"\n    size = bytes\n    index = 0\n    while size >= 1024 and index < len(abbreviations) - 1:\n        size /= 1024\n        index += 1\n    return f\"{size:.{precision}f} {abbreviations[index]}\"\n\n\ndef get_file_size(file: UploadFile):\n    if isinstance(file.file, BytesIO):\n        # If the file object is a BytesIO object, get the size of the bytes data\n        file_size = len(file.file.getvalue())\n        return file_size\n    # move the cursor to the end of the file\n    file.file._file.seek(0, 2)  # pyright: ignore reportPrivateUsage=none\n    file_size = (\n        file.file._file.tell()  # pyright: ignore reportPrivateUsage=none\n    )  # Getting the size of the file\n    # move the cursor back to the beginning of the file\n    file.file.seek(0)\n\n    return file_size\n\n\ndef compute_sha1_from_file(file_path):\n    with open(file_path, \"rb\") as file:\n        bytes = file.read()\n        readable_hash = compute_sha1_from_content(bytes)\n    return readable_hash\n\n\ndef compute_sha1_from_content(content):\n    readable_hash = hashlib.sha1(content).hexdigest()\n    return readable_hash\n", "backend/packages/files/processors.py": "from modules.brain.service.brain_service import BrainService\n\nfrom .parsers.audio import process_audio\nfrom .parsers.bibtex import process_bibtex\nfrom .parsers.code_python import process_python\nfrom .parsers.csv import process_csv\nfrom .parsers.docx import process_docx\nfrom .parsers.epub import process_epub\nfrom .parsers.html import process_html\nfrom .parsers.markdown import process_markdown\nfrom .parsers.notebook import process_ipnyb\nfrom .parsers.odt import process_odt\nfrom .parsers.pdf import process_pdf\nfrom .parsers.powerpoint import process_powerpoint\nfrom .parsers.telegram import process_telegram\nfrom .parsers.txt import process_txt\nfrom .parsers.xlsx import process_xlsx\n\nfile_processors = {\n    \".txt\": process_txt,\n    \".csv\": process_csv,\n    \".md\": process_markdown,\n    \".markdown\": process_markdown,\n    \".telegram\": process_telegram,\n    \".m4a\": process_audio,\n    \".mp3\": process_audio,\n    \".webm\": process_audio,\n    \".mp4\": process_audio,\n    \".mpga\": process_audio,\n    \".wav\": process_audio,\n    \".mpeg\": process_audio,\n    \".pdf\": process_pdf,\n    \".html\": process_html,\n    \".bib\": process_bibtex,\n    \".pptx\": process_powerpoint,\n    \".docx\": process_docx,\n    \".odt\": process_odt,\n    \".xlsx\": process_xlsx,\n    \".xls\": process_xlsx,\n    \".epub\": process_epub,\n    \".ipynb\": process_ipnyb,\n    \".py\": process_python,\n}\n\n\ndef create_response(message, type):\n    return {\"message\": message, \"type\": type}\n\n\nbrain_service = BrainService()\n\n\n# TODO: Move filter_file to a file service to avoid circular imports from models/files.py for File class\ndef filter_file(\n    file,\n    brain_id,\n    original_file_name=None,\n):\n    file_exists = file.file_already_exists()\n    file_exists_in_brain = file.file_already_exists_in_brain(brain_id)\n    using_file_name = file.file_name\n\n    brain = brain_service.get_brain_by_id(brain_id)\n    if brain is None:\n        raise Exception(\"It seems like you're uploading knowledge to an unknown brain.\")\n\n    if file_exists_in_brain:\n        return create_response(\n            f\"\ud83e\udd14 {using_file_name} already exists in brain {brain.name}.\",  # pyright: ignore reportPrivateUsage=none\n            \"warning\",\n        )\n    elif file.file_is_empty():\n        return create_response(\n            f\"\u274c {original_file_name} is empty.\",  # pyright: ignore reportPrivateUsage=none\n            \"error\",  # pyright: ignore reportPrivateUsage=none\n        )\n    elif file_exists:\n        file.link_file_to_brain(brain_id)\n        return create_response(\n            f\"\u2705 {using_file_name} has been uploaded to brain {brain.name}.\",  # pyright: ignore reportPrivateUsage=none\n            \"success\",\n        )\n\n    if file.file_extension in file_processors:\n        try:\n            result = file_processors[file.file_extension](\n                file=file,\n                brain_id=brain_id,\n                original_file_name=original_file_name,\n            )\n            if result is None or result == 0:\n                return create_response(\n                    f\"\uff1f {using_file_name} has been uploaded to brain. There might have been an error while reading it, please make sure the file is not illformed or just an image\",  # pyright: ignore reportPrivateUsage=none\n                    \"warning\",\n                )\n            return create_response(\n                f\"\u2705 {using_file_name} has been uploaded to brain {brain.name} in {result} chunks\",  # pyright: ignore reportPrivateUsage=none\n                \"success\",\n            )\n        except Exception as e:\n            # Add more specific exceptions as needed.\n            print(f\"Error processing file: {e}\")\n            raise e\n\n    return create_response(\n        f\"\u274c {using_file_name} is not supported.\",  # pyright: ignore reportPrivateUsage=none\n        \"error\",\n    )\n", "backend/packages/files/__init__.py": "", "backend/packages/files/loaders/telegram.py": "from __future__ import annotations\n\nimport json\nfrom pathlib import Path\nfrom typing import List\n\nfrom langchain.docstore.document import Document\nfrom langchain_community.document_loaders.base import BaseLoader\n\n\ndef concatenate_rows(row: dict) -> str:\n    \"\"\"Combine message information in a readable format ready to be used.\"\"\"\n    date = row[\"date\"]\n    sender = row.get(\n        \"from\", \"Unknown\"\n    )  # Using .get() to handle cases where 'from' might not be present\n\n    text_content = row.get(\"text\", \"\")\n\n    # Function to process a single text entity\n    def process_text_entity(entity):\n        if isinstance(entity, str):\n            return entity\n        elif isinstance(entity, dict) and \"text\" in entity:\n            return entity[\"text\"]\n        return \"\"\n\n    # Process the text content based on its type\n    if isinstance(text_content, str):\n        text = text_content\n    elif isinstance(text_content, list):\n        text = \"\".join(process_text_entity(item) for item in text_content)\n    else:\n        text = \"\"\n\n    # Skip messages with empty text\n    if not text.strip():\n        return \"\"\n\n    return f\"{sender} on {date}: {text}\\n\\n\"\n\n\nclass TelegramChatFileLoader(BaseLoader):\n    \"\"\"Load from `Telegram chat` dump.\"\"\"\n\n    def __init__(self, path: str):\n        \"\"\"Initialize with a path.\"\"\"\n        self.file_path = path\n\n    def load(self) -> List[Document]:\n        \"\"\"Load documents.\"\"\"\n        p = Path(self.file_path)\n\n        with open(p, encoding=\"utf8\") as f:\n            d = json.load(f)\n\n        text = \"\".join(\n            concatenate_rows(message)\n            for message in d[\"messages\"]\n            if message[\"type\"] == \"message\"\n            and (isinstance(message[\"text\"], str) or isinstance(message[\"text\"], list))\n        )\n        metadata = {\"source\": str(p)}\n\n        return [Document(page_content=text, metadata=metadata)]\n", "backend/packages/files/loaders/__init__.py": "", "backend/packages/files/parsers/powerpoint.py": "from langchain_community.document_loaders import UnstructuredFileLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_powerpoint(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredFileLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/common.py": "import asyncio\nimport os\nimport tempfile\nimport time\n\nimport nest_asyncio\nimport tiktoken\nimport uvloop\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom llama_parse import LlamaParse\nfrom logger import get_logger\nfrom models import File\nfrom modules.brain.service.brain_vector_service import BrainVectorService\nfrom modules.upload.service.upload_file import DocumentSerializable\nfrom packages.embeddings.vectors import Neurons\n\nif not isinstance(asyncio.get_event_loop(), uvloop.Loop):\n    nest_asyncio.apply()\n\nlogger = get_logger(__name__)\n\n\ndef process_file(\n    file: File,\n    loader_class,\n    brain_id,\n    original_file_name,\n    integration=None,\n    integration_link=None,\n):\n    dateshort = time.strftime(\"%Y%m%d\")\n    neurons = Neurons()\n\n    if os.getenv(\"LLAMA_CLOUD_API_KEY\"):\n        doc = file.file\n        document_ext = os.path.splitext(doc.filename)[1]\n        if document_ext in [\".pdf\", \".docx\", \".doc\"]:\n            document_tmp = tempfile.NamedTemporaryFile(\n                suffix=document_ext, delete=False\n            )\n            # Seek to the beginning of the file\n            doc.file.seek(0)\n            document_tmp.write(doc.file.read())\n\n            parser = LlamaParse(\n                result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n                parsing_instruction=\"Extract the tables and transform checkboxes into text. Transform tables to key = value. You can duplicates Keys if needed. For example: Productions Fonts = 300 productions Fonts Company Desktop License = Yes for Maximum of 60 Licensed Desktop users For example checkboxes should be: Premium Activated = Yes License Premier = No If a checkbox is present for a table with multiple options.  Say Yes for the one activated and no for the one not activated. Format using headers.\",\n                gpt4o_mode=True,\n                gpt4o_api_key=os.getenv(\"OPENAI_API_KEY\"),\n            )\n\n            document_llama_parsed = parser.load_data(document_tmp.name)\n            document_tmp.close()\n            document_to_langchain = document_llama_parsed[0].to_langchain_format()\n            text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n                chunk_size=file.chunk_size, chunk_overlap=file.chunk_overlap\n            )\n            document_to_langchain = Document(\n                page_content=document_to_langchain.page_content\n            )\n            file.documents = text_splitter.split_documents([document_to_langchain])\n    else:\n\n        file.compute_documents(loader_class)\n\n    metadata = {\n        \"file_sha1\": file.file_sha1,\n        \"file_size\": file.file_size,\n        \"file_name\": file.file_name,\n        \"chunk_size\": file.chunk_size,\n        \"chunk_overlap\": file.chunk_overlap,\n        \"date\": dateshort,\n        \"original_file_name\": original_file_name or file.file_name,\n        \"integration\": integration or \"\",\n        \"integration_link\": integration_link or \"\",\n    }\n    docs = []\n\n    enc = tiktoken.get_encoding(\"cl100k_base\")\n\n    if file.documents is not None:\n        logger.info(\"Coming here?\")\n        for index, doc in enumerate(\n            file.documents, start=1\n        ):  # pyright: ignore reportPrivateUsage=none\n            new_metadata = metadata.copy()\n            logger.info(f\"Processing document {doc}\")\n            # Add filename at beginning of page content\n            doc.page_content = f\"Filename: {new_metadata['original_file_name']} Content: {doc.page_content}\"\n\n            doc.page_content = doc.page_content.replace(\"\\u0000\", \"\")\n\n            len_chunk = len(enc.encode(doc.page_content))\n\n            # Ensure the text is in UTF-8\n            doc.page_content = doc.page_content.encode(\"utf-8\", \"replace\").decode(\n                \"utf-8\"\n            )\n\n            new_metadata[\"chunk_size\"] = len_chunk\n            new_metadata[\"index\"] = index\n            doc_with_metadata = DocumentSerializable(\n                page_content=doc.page_content, metadata=new_metadata\n            )\n            docs.append(doc_with_metadata)\n\n    created_vector = neurons.create_vector(docs)\n\n    brain_vector_service = BrainVectorService(brain_id)\n    for created_vector_id in created_vector:\n        result = brain_vector_service.create_brain_vector(\n            created_vector_id, metadata[\"file_sha1\"]\n        )\n        logger.debug(f\"Brain vector created: {result}\")\n\n    if created_vector:\n        return len(created_vector)\n    else:\n        return 0\n", "backend/packages/files/parsers/epub.py": "from langchain_community.document_loaders.epub import UnstructuredEPubLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_epub(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredEPubLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/html.py": "from langchain_community.document_loaders import UnstructuredHTMLLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_html(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredHTMLLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/odt.py": "from langchain_community.document_loaders import UnstructuredPDFLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_odt(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredPDFLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/code_python.py": "from langchain_community.document_loaders import PythonLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_python(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=PythonLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/pdf.py": "from langchain_community.document_loaders import UnstructuredPDFLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_pdf(\n    file: File,\n    brain_id,\n    original_file_name,\n    integration=None,\n    integration_link=None,\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredPDFLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/xlsx.py": "from langchain_community.document_loaders import UnstructuredExcelLoader\nfrom models.files import File\n\nfrom .common import process_file\n\n\ndef process_xlsx(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredExcelLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/txt.py": "from langchain_community.document_loaders import TextLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_txt(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=TextLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/telegram.py": "from models import File\nfrom packages.files.loaders.telegram import TelegramChatFileLoader\n\nfrom .common import process_file\n\n\ndef process_telegram(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=TelegramChatFileLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/github.py": "import os\nimport time\n\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import GitLoader\nfrom models.files import File\nfrom packages.embeddings.vectors import Neurons\nfrom packages.files.file import compute_sha1_from_content\n\n\ndef process_github(\n    repo,\n    brain_id,\n):\n    random_dir_name = os.urandom(16).hex()\n    dateshort = time.strftime(\"%Y%m%d\")\n    loader = GitLoader(\n        clone_url=repo,\n        repo_path=\"/tmp/\" + random_dir_name,\n    )\n    documents = loader.load()\n    os.system(\"rm -rf /tmp/\" + random_dir_name)\n\n    chunk_size = 500\n    chunk_overlap = 0\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n    )\n\n    documents = text_splitter.split_documents(documents)\n\n    for doc in documents:\n        if doc.metadata[\"file_type\"] in [\n            \".pyc\",\n            \".png\",\n            \".svg\",\n            \".env\",\n            \".lock\",\n            \".gitignore\",\n            \".gitmodules\",\n            \".gitattributes\",\n            \".gitkeep\",\n            \".git\",\n            \".json\",\n        ]:\n            continue\n        metadata = {\n            \"file_sha1\": compute_sha1_from_content(doc.page_content.encode(\"utf-8\")),\n            \"file_size\": len(doc.page_content) * 8,\n            \"file_name\": doc.metadata[\"file_name\"],\n            \"chunk_size\": chunk_size,\n            \"chunk_overlap\": chunk_overlap,\n            \"date\": dateshort,\n            \"original_file_name\": doc.metadata[\"original_file_name\"],\n        }\n        doc_with_metadata = Document(page_content=doc.page_content, metadata=metadata)\n\n        print(doc_with_metadata.metadata[\"file_name\"])\n\n        file = File(\n            file_sha1=compute_sha1_from_content(doc.page_content.encode(\"utf-8\"))\n        )\n\n        file_exists = file.file_already_exists()\n\n        if not file_exists:\n            neurons = Neurons()\n            created_vector = neurons.create_vector(doc_with_metadata)\n\n        file_exists_in_brain = file.file_already_exists_in_brain(brain_id)\n\n        if not file_exists_in_brain:\n            file.link_file_to_brain(brain_id)\n    return {\n        \"message\": f\"\u2705 Github with {len(documents)} files has been uploaded.\",\n        \"type\": \"success\",\n    }\n", "backend/packages/files/parsers/audio.py": "import time\n\nimport openai\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom models import File, get_documents_vector_store\nfrom packages.files.file import compute_sha1_from_content\n\n\ndef process_audio(file: File, **kwargs):\n    dateshort = time.strftime(\"%Y%m%d-%H%M%S\")\n    file_meta_name = f\"audiotranscript_{dateshort}.txt\"\n    documents_vector_store = get_documents_vector_store()\n\n    with open(file.tmp_file_path, \"rb\") as audio_file:\n        transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n\n        file_sha = compute_sha1_from_content(\n            transcript.text.encode(\"utf-8\")  # pyright: ignore reportPrivateUsage=none\n        )\n        file_size = len(\n            transcript.text.encode(\"utf-8\")  # pyright: ignore reportPrivateUsage=none\n        )\n\n        chunk_size = 500\n        chunk_overlap = 0\n\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n        )\n        texts = text_splitter.split_text(\n            transcript.text.encode(\"utf-8\")  # pyright: ignore reportPrivateUsage=none\n        )\n\n        docs_with_metadata = [\n            Document(\n                page_content=text,\n                metadata={\n                    \"file_sha1\": file_sha,\n                    \"file_size\": file_size,\n                    \"file_name\": file_meta_name,\n                    \"chunk_size\": chunk_size,\n                    \"chunk_overlap\": chunk_overlap,\n                    \"date\": dateshort,\n                },\n            )\n            for text in texts\n        ]\n\n        documents_vector_store.add_documents(docs_with_metadata)\n", "backend/packages/files/parsers/csv.py": "from langchain_community.document_loaders import CSVLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_csv(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=CSVLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/bibtex.py": "from langchain_community.document_loaders import BibtexLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_bibtex(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=BibtexLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/__init__.py": "", "backend/packages/files/parsers/markdown.py": "from langchain_community.document_loaders import UnstructuredMarkdownLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_markdown(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=UnstructuredMarkdownLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/parsers/notebook.py": "from langchain_community.document_loaders import NotebookLoader\nfrom models import File\n\nfrom .common import process_file\n\n\ndef process_ipnyb(\n    file: File, brain_id, original_file_name, integration=None, integration_link=None\n):\n    return process_file(\n        file=file,\n        loader_class=NotebookLoader,\n        brain_id=brain_id,\n        original_file_name=original_file_name,\n        integration=integration,\n        integration_link=integration_link,\n    )\n", "backend/packages/files/crawl/__init__.py": "", "backend/packages/files/crawl/crawler.py": "import os\nimport re\nimport unicodedata\n\nfrom langchain_community.document_loaders import PlaywrightURLLoader\nfrom logger import get_logger\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass CrawlWebsite(BaseModel):\n    url: str\n    js: bool = False\n    depth: int = int(os.getenv(\"CRAWL_DEPTH\", \"1\"))\n    max_pages: int = 100\n    max_time: int = 60\n\n    def process(self) -> str:\n        # Extract and combine content recursively\n        loader = PlaywrightURLLoader(\n            urls=[self.url], remove_selectors=[\"header\", \"footer\"]\n        )\n\n        data = loader.load()\n        # Now turn the data into a string\n        logger.info(f\"Extracted content from {len(data)} pages\")\n        logger.debug(f\"Extracted data : {data}\")\n        extracted_content = \"\"\n        for page in data:\n            extracted_content += page.page_content\n\n        return extracted_content\n\n    def checkGithub(self):\n        return \"github.com\" in self.url\n\n\ndef slugify(text):\n    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n    text = re.sub(r\"[^\\w\\s-]\", \"\", text).strip().lower()\n    text = re.sub(r\"[-\\s]+\", \"-\", text)\n    return text\n", "backend/playground/auth.py": "import json\nimport os\n\nfrom fastapi import FastAPI, HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom google.auth.transport.requests import Request as GoogleRequest\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import Flow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\napp = FastAPI()\n\nSCOPES = [\"https://www.googleapis.com/auth/drive.metadata.readonly\"]\nCLIENT_SECRETS_FILE = \"credentials.json\"\nREDIRECT_URI = \"http://localhost:8000/oauth2callback\"\n\n# Disable OAuthlib's HTTPS verification when running locally.\nos.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n\n@app.get(\"/authorize\")\ndef authorize():\n    flow = Flow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, scopes=SCOPES, redirect_uri=REDIRECT_URI\n    )\n    authorization_url, state = flow.authorization_url(\n        access_type=\"offline\", include_granted_scopes=\"true\"\n    )\n    # Store the state in session to validate the callback later\n    with open(\"state.json\", \"w\") as state_file:\n        json.dump({\"state\": state}, state_file)\n    return JSONResponse(content={\"authorization_url\": authorization_url})\n\n\n@app.get(\"/oauth2callback\")\ndef oauth2callback(request: Request):\n    state = request.query_params.get(\"state\")\n    with open(\"state.json\", \"r\") as state_file:\n        saved_state = json.load(state_file)[\"state\"]\n\n    if state != saved_state:\n        raise HTTPException(status_code=400, detail=\"Invalid state parameter\")\n\n    flow = Flow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, scopes=SCOPES, state=state, redirect_uri=REDIRECT_URI\n    )\n    flow.fetch_token(authorization_response=str(request.url))\n    creds = flow.credentials\n\n    # Save the credentials for future use\n    with open(\"token.json\", \"w\") as token:\n        token.write(creds.to_json())\n\n    return JSONResponse(content={\"message\": \"Authentication successful\"})\n\n\n@app.get(\"/list_files\")\ndef list_files():\n    creds = None\n    if os.path.exists(\"token.json\"):\n        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(GoogleRequest())\n        else:\n            raise HTTPException(status_code=401, detail=\"Credentials are not valid\")\n\n    try:\n        service = build(\"drive\", \"v3\", credentials=creds)\n        results = (\n            service.files()\n            .list(pageSize=10, fields=\"nextPageToken, files(id, name)\")\n            .execute()\n        )\n        items = results.get(\"files\", [])\n\n        if not items:\n            return JSONResponse(content={\"files\": \"No files found.\"})\n\n        files = [{\"name\": item[\"name\"], \"id\": item[\"id\"]} for item in items]\n        return JSONResponse(content={\"files\": files})\n    except HttpError as error:\n        raise HTTPException(status_code=500, detail=f\"An error occurred: {error}\")\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "backend/playground/auth-azure.py": "import json\nimport os\n\nimport msal\nimport requests\nfrom fastapi import Depends, FastAPI, HTTPException, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\n\napp = FastAPI()\n\nCLIENT_ID = \"511dce23-02f3-4724-8684-05da226df5f3\"\nAUTHORITY = \"https://login.microsoftonline.com/common\"\nREDIRECT_URI = \"http://localhost:8000/oauth2callback\"\nSCOPE = [\n    \"https://graph.microsoft.com/Files.Read\",\n    \"https://graph.microsoft.com/User.Read\",\n    \"https://graph.microsoft.com/Sites.Read.All\",\n]\n\nclient = msal.PublicClientApplication(CLIENT_ID, authority=AUTHORITY)\n\n\ndef get_token_data():\n    if not os.path.exists(\"azure_token.json\"):\n        raise HTTPException(status_code=401, detail=\"User not authenticated\")\n    with open(\"azure_token.json\", \"r\") as token_file:\n        token_data = json.load(token_file)\n    if \"access_token\" not in token_data:\n        raise HTTPException(status_code=401, detail=\"Invalid token data\")\n    return token_data\n\n\ndef refresh_token():\n    if not os.path.exists(\"azure_token.json\"):\n        raise HTTPException(status_code=401, detail=\"User not authenticated\")\n    with open(\"azure_token.json\", \"r\") as token_file:\n        token_data = json.load(token_file)\n    if \"refresh_token\" not in token_data:\n        raise HTTPException(status_code=401, detail=\"No refresh token available\")\n\n    result = client.acquire_token_by_refresh_token(\n        token_data[\"refresh_token\"], scopes=SCOPE\n    )\n    if \"access_token\" not in result:\n        raise HTTPException(status_code=400, detail=\"Failed to refresh token\")\n\n    with open(\"azure_token.json\", \"w\") as token:\n        json.dump(result, token)\n\n    return result\n\n\ndef get_headers(token_data):\n    return {\n        \"Authorization\": f\"Bearer {token_data['access_token']}\",\n        \"Accept\": \"application/json\",\n    }\n\n\n@app.get(\"/authorize\")\ndef authorize():\n    authorization_url = client.get_authorization_request_url(\n        scopes=SCOPE, redirect_uri=REDIRECT_URI\n    )\n    return JSONResponse(content={\"authorization_url\": authorization_url})\n\n\n@app.get(\"/oauth2callback\")\ndef oauth2callback(request: Request):\n    code = request.query_params.get(\"code\")\n    if not code:\n        raise HTTPException(status_code=400, detail=\"Authorization code not found\")\n\n    result = client.acquire_token_by_authorization_code(\n        code, scopes=SCOPE, redirect_uri=REDIRECT_URI\n    )\n    if \"access_token\" not in result:\n        print(f\"Token acquisition failed: {result}\")\n        raise HTTPException(status_code=400, detail=\"Failed to acquire token\")\n\n    with open(\"azure_token.json\", \"w\") as token:\n        json.dump(result, token)\n\n    return JSONResponse(content={\"message\": \"Authentication successful\"})\n\n\n@app.get(\"/list_sites\")\ndef list_sites(token_data: dict = Depends(get_token_data)):\n    headers = get_headers(token_data)\n    endpoint = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n    response = requests.get(endpoint, headers=headers)\n    if response.status_code == 401:\n        token_data = refresh_token()\n        headers = get_headers(token_data)\n        response = requests.get(endpoint, headers=headers)\n    if response.status_code != 200:\n        raise HTTPException(status_code=response.status_code, detail=response.text)\n    sites = response.json().get(\"value\", [])\n    return JSONResponse(content={\"sites\": sites})\n\n\ndef extract_files_and_folders(items, headers, page_size):\n    result = []\n    for item in items:\n        entry = {\n            \"name\": item.get(\"name\"),\n            \"id\": item.get(\"id\"),\n            \"parentReference\": item.get(\"parentReference\"),\n            \"lastModifiedDateTime\": item.get(\"lastModifiedDateTime\"),\n            \"webUrl\": item.get(\"webUrl\"),\n            \"size\": item.get(\"size\"),\n            \"fileSystemInfo\": item.get(\"fileSystemInfo\"),\n            \"folder\": item.get(\"folder\"),\n            \"file\": item.get(\"file\"),\n        }\n        if \"folder\" in item:\n            folder_endpoint = f\"https://graph.microsoft.com/v1.0/me/drive/items/{item['id']}/children?$top={page_size}\"\n            children = []\n            while folder_endpoint:\n                folder_response = requests.get(folder_endpoint, headers=headers)\n                if folder_response.status_code == 200:\n                    children_page = folder_response.json().get(\"value\", [])\n                    children.extend(children_page)\n                    folder_endpoint = folder_response.json().get(\n                        \"@odata.nextLink\", None\n                    )\n                else:\n                    break\n            entry[\"children\"] = extract_files_and_folders(children, headers, page_size)\n        result.append(entry)\n    return result\n\n\ndef fetch_all_files(headers, page_size):\n    endpoint = (\n        f\"https://graph.microsoft.com/v1.0/me/drive/root/children?$top={page_size}\"\n    )\n    all_files = []\n    while endpoint:\n        response = requests.get(endpoint, headers=headers)\n        if response.status_code == 401:\n            token_data = refresh_token()\n            headers = get_headers(token_data)\n            response = requests.get(endpoint, headers=headers)\n        if response.status_code != 200:\n            raise HTTPException(status_code=response.status_code, detail=response.text)\n        files = response.json().get(\"value\", [])\n        all_files.extend(files)\n        endpoint = response.json().get(\"@odata.nextLink\", None)\n    return all_files\n\n\n@app.get(\"/list_files\")\ndef list_files(page_size: int = 1, token_data: dict = Depends(get_token_data)):\n    headers = get_headers(token_data)\n    all_files = fetch_all_files(headers, page_size)\n    structured_files = extract_files_and_folders(all_files, headers, page_size)\n    return JSONResponse(content={\"files\": structured_files})\n\n\n@app.get(\"/download_file/{file_id}\")\ndef download_file(file_id: str, token_data: dict = Depends(get_token_data)):\n    headers = get_headers(token_data)\n    metadata_endpoint = f\"https://graph.microsoft.com/v1.0/me/drive/items/{file_id}\"\n    metadata_response = requests.get(metadata_endpoint, headers=headers)\n    if metadata_response.status_code == 401:\n        token_data = refresh_token()\n        headers = get_headers(token_data)\n        metadata_response = requests.get(metadata_endpoint, headers=headers)\n    if metadata_response.status_code != 200:\n        raise HTTPException(\n            status_code=metadata_response.status_code, detail=metadata_response.text\n        )\n    metadata = metadata_response.json()\n    if \"folder\" in metadata:\n        raise HTTPException(\n            status_code=400, detail=\"The specified ID is a folder, not a file\"\n        )\n    download_endpoint = (\n        f\"https://graph.microsoft.com/v1.0/me/drive/items/{file_id}/content\"\n    )\n    download_response = requests.get(download_endpoint, headers=headers, stream=True)\n    if download_response.status_code == 401:\n        token_data = refresh_token()\n        headers = get_headers(token_data)\n        download_response = requests.get(\n            download_endpoint, headers=headers, stream=True\n        )\n    if download_response.status_code != 200:\n        raise HTTPException(\n            status_code=download_response.status_code, detail=download_response.text\n        )\n    return StreamingResponse(\n        download_response.iter_content(chunk_size=1024),\n        headers={\"Content-Disposition\": f\"attachment; filename={metadata.get('name')}\"},\n    )\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "backend/modules/__init__.py": "", "backend/modules/chat/__init__.py": "", "backend/modules/chat/controller/chat_routes.py": "from typing import Annotated, List, Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException, Query, Request\nfrom fastapi.responses import StreamingResponse\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom models.settings import BrainSettings, get_supabase_client\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.controller.chat.brainful_chat import BrainfulChat\nfrom modules.chat.dto.chats import ChatItem, ChatQuestion\nfrom modules.chat.dto.inputs import (\n    ChatMessageProperties,\n    ChatUpdatableProperties,\n    CreateChatProperties,\n    QuestionAndAnswer,\n)\nfrom modules.chat.entity.chat import Chat\nfrom modules.chat.service.chat_service import ChatService\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_usage import UserUsage\nfrom packages.utils.telemetry import maybe_send_telemetry\nfrom vectorstore.supabase import CustomSupabaseVectorStore\n\nlogger = get_logger(__name__)\n\nchat_router = APIRouter()\n\nbrain_service = BrainService()\nchat_service = ChatService()\n\n\ndef init_vector_store(user_id: UUID) -> CustomSupabaseVectorStore:\n    \"\"\"\n    Initialize the vector store\n    \"\"\"\n    brain_settings = BrainSettings()\n    supabase_client = get_supabase_client()\n    embeddings = None\n    if brain_settings.ollama_api_base_url:\n        embeddings = OllamaEmbeddings(\n            base_url=brain_settings.ollama_api_base_url\n        )  # pyright: ignore reportPrivateUsage=none\n    else:\n        embeddings = OpenAIEmbeddings()\n    vector_store = CustomSupabaseVectorStore(\n        supabase_client, embeddings, table_name=\"vectors\", user_id=user_id\n    )\n\n    return vector_store\n\n\ndef get_answer_generator(\n    chat_id: UUID,\n    chat_question: ChatQuestion,\n    brain_id: UUID,\n    current_user: UserIdentity,\n):\n    chat_instance = BrainfulChat()\n\n    user_usage = UserUsage(\n        id=current_user.id,\n        email=current_user.email,\n    )\n\n    vector_store = init_vector_store(user_id=current_user.id)\n\n    # Get History only if needed\n    if not brain_id:\n        history = chat_service.get_chat_history(chat_id)\n    else:\n        history = []\n\n    # Generic\n    brain, metadata_brain = brain_service.find_brain_from_question(\n        brain_id, chat_question.question, current_user, chat_id, history, vector_store\n    )\n    gpt_answer_generator = chat_instance.get_answer_generator(\n        brain=brain,\n        chat_id=str(chat_id),\n        model=brain.model,\n        temperature=0.1,\n        streaming=True,\n        prompt_id=chat_question.prompt_id,\n        user_id=current_user.id,\n        user_email=current_user.email,\n    )\n\n    return gpt_answer_generator\n\n\n@chat_router.get(\"/chat/healthz\", tags=[\"Health\"])\nasync def healthz():\n    return {\"status\": \"ok\"}\n\n\n# get all chats\n@chat_router.get(\"/chat\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\nasync def get_chats(current_user: UserIdentity = Depends(get_current_user)):\n    \"\"\"\n    Retrieve all chats for the current user.\n\n    - `current_user`: The current authenticated user.\n    - Returns a list of all chats for the user.\n\n    This endpoint retrieves all the chats associated with the current authenticated user. It returns a list of chat objects\n    containing the chat ID and chat name for each chat.\n    \"\"\"\n    chats = chat_service.get_user_chats(str(current_user.id))\n    return {\"chats\": chats}\n\n\n# delete one chat\n@chat_router.delete(\n    \"/chat/{chat_id}\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"]\n)\nasync def delete_chat(chat_id: UUID):\n    \"\"\"\n    Delete a specific chat by chat ID.\n    \"\"\"\n\n    chat_service.delete_chat_from_db(chat_id)\n    return {\"message\": f\"{chat_id}  has been deleted.\"}\n\n\n# update existing chat metadata\n@chat_router.put(\n    \"/chat/{chat_id}/metadata\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"]\n)\nasync def update_chat_metadata_handler(\n    chat_data: ChatUpdatableProperties,\n    chat_id: UUID,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Update chat attributes\n    \"\"\"\n\n    chat = chat_service.get_chat_by_id(\n        chat_id  # pyright: ignore reportPrivateUsage=none\n    )\n    if str(current_user.id) != chat.user_id:\n        raise HTTPException(\n            status_code=403,  # pyright: ignore reportPrivateUsage=none\n            detail=\"You should be the owner of the chat to update it.\",  # pyright: ignore reportPrivateUsage=none\n        )\n    return chat_service.update_chat(chat_id=chat_id, chat_data=chat_data)\n\n\n# update existing message\n@chat_router.put(\n    \"/chat/{chat_id}/{message_id}\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"]\n)\nasync def update_chat_message(\n    chat_message_properties: ChatMessageProperties,\n    chat_id: UUID,\n    message_id: UUID,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n\n    chat = chat_service.get_chat_by_id(\n        chat_id  # pyright: ignore reportPrivateUsage=none\n    )\n    if str(current_user.id) != chat.user_id:\n        raise HTTPException(\n            status_code=403,  # pyright: ignore reportPrivateUsage=none\n            detail=\"You should be the owner of the chat to update it.\",  # pyright: ignore reportPrivateUsage=none\n        )\n    return chat_service.update_chat_message(\n        chat_id=chat_id,\n        message_id=message_id,\n        chat_message_properties=chat_message_properties.dict(),\n    )\n\n\n# create new chat\n@chat_router.post(\"/chat\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\nasync def create_chat_handler(\n    chat_data: CreateChatProperties,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Create a new chat with initial chat messages.\n    \"\"\"\n\n    return chat_service.create_chat(user_id=current_user.id, chat_data=chat_data)\n\n\n# add new question to chat\n@chat_router.post(\n    \"/chat/{chat_id}/question\",\n    dependencies=[\n        Depends(\n            AuthBearer(),\n        ),\n    ],\n    tags=[\"Chat\"],\n)\nasync def create_question_handler(\n    request: Request,\n    chat_question: ChatQuestion,\n    chat_id: UUID,\n    brain_id: Annotated[UUID | None, Query()] = None,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    try:\n        logger.info(\n            f\"Creating question for chat {chat_id} with brain {brain_id} of type {type(brain_id)}\"\n        )\n        gpt_answer_generator = get_answer_generator(\n            chat_id, chat_question, brain_id, current_user\n        )\n\n        chat_answer = gpt_answer_generator.generate_answer(\n            chat_id, chat_question, save_answer=True\n        )\n        maybe_send_telemetry(\"question_asked\", {\"streaming\": False}, request)\n\n        return chat_answer\n    except HTTPException as e:\n        raise e\n\n\n# stream new question response from chat\n@chat_router.post(\n    \"/chat/{chat_id}/question/stream\",\n    dependencies=[\n        Depends(\n            AuthBearer(),\n        ),\n    ],\n    tags=[\"Chat\"],\n)\nasync def create_stream_question_handler(\n    request: Request,\n    chat_question: ChatQuestion,\n    chat_id: UUID,\n    brain_id: Annotated[UUID | None, Query()] = None,\n    current_user: UserIdentity = Depends(get_current_user),\n) -> StreamingResponse:\n\n    chat_instance = BrainfulChat()\n    chat_instance.validate_authorization(user_id=current_user.id, brain_id=brain_id)\n\n    logger.info(\n        f\"Creating question for chat {chat_id} with brain {brain_id} of type {type(brain_id)}\"\n    )\n\n    gpt_answer_generator = get_answer_generator(\n        chat_id, chat_question, brain_id, current_user\n    )\n    maybe_send_telemetry(\"question_asked\", {\"streaming\": True}, request)\n\n    try:\n        return StreamingResponse(\n            gpt_answer_generator.generate_stream(\n                chat_id, chat_question, save_answer=True\n            ),\n            media_type=\"text/event-stream\",\n        )\n\n    except HTTPException as e:\n        raise e\n\n\n# get chat history\n@chat_router.get(\n    \"/chat/{chat_id}/history\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"]\n)\nasync def get_chat_history_handler(\n    chat_id: UUID,\n) -> List[ChatItem]:\n    # TODO: RBAC with current_user\n    return chat_service.get_chat_history_with_notifications(chat_id)\n\n\n@chat_router.post(\n    \"/chat/{chat_id}/question/answer\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Chat\"],\n)\nasync def add_question_and_answer_handler(\n    chat_id: UUID,\n    question_and_answer: QuestionAndAnswer,\n) -> Optional[Chat]:\n    \"\"\"\n    Add a new question and anwser to the chat.\n    \"\"\"\n    return chat_service.add_question_and_answer(chat_id, question_and_answer)\n", "backend/modules/chat/controller/__init__.py": "from .chat_routes import chat_router\n", "backend/modules/chat/controller/chat/__init_.py": "", "backend/modules/chat/controller/chat/brainful_chat.py": "from logger import get_logger\nfrom modules.brain.entity.brain_entity import BrainType, RoleEnum\nfrom modules.brain.integrations.Big.Brain import BigBrain\nfrom modules.brain.integrations.GPT4.Brain import GPT4Brain\nfrom modules.brain.integrations.Multi_Contract.Brain import MultiContractBrain\nfrom modules.brain.integrations.Notion.Brain import NotionBrain\nfrom modules.brain.integrations.Proxy.Brain import ProxyBrain\nfrom modules.brain.integrations.Self.Brain import SelfBrain\nfrom modules.brain.integrations.SQL.Brain import SQLBrain\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.brain.service.api_brain_definition_service import ApiBrainDefinitionService\nfrom modules.brain.service.brain_authorization_service import (\n    validate_brain_authorization,\n)\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.brain.service.integration_brain_service import (\n    IntegrationBrainDescriptionService,\n)\nfrom modules.chat.controller.chat.interface import ChatInterface\nfrom modules.chat.service.chat_service import ChatService\n\nchat_service = ChatService()\napi_brain_definition_service = ApiBrainDefinitionService()\nintegration_brain_description_service = IntegrationBrainDescriptionService()\n\nlogger = get_logger(__name__)\n\nmodels_supporting_function_calls = [\n    \"gpt-4\",\n    \"gpt-4-1106-preview\",\n    \"gpt-4-0613\",\n    \"gpt-3.5-turbo-0125\",\n    \"gpt-3.5-turbo-1106\",\n    \"gpt-3.5-turbo-0613\",\n    \"gpt-4-0125-preview\",\n    \"gpt-3.5-turbo\",\n    \"gpt-4-turbo\",\n    \"gpt-4o\",\n]\n\n\nintegration_list = {\n    \"notion\": NotionBrain,\n    \"gpt4\": GPT4Brain,\n    \"sql\": SQLBrain,\n    \"big\": BigBrain,\n    \"doc\": KnowledgeBrainQA,\n    \"proxy\": ProxyBrain,\n    \"self\": SelfBrain,\n    \"multi-contract\": MultiContractBrain,\n}\n\nbrain_service = BrainService()\n\n\nclass BrainfulChat(ChatInterface):\n    def validate_authorization(self, user_id, brain_id):\n        if brain_id:\n            validate_brain_authorization(\n                brain_id=brain_id,\n                user_id=user_id,\n                required_roles=[RoleEnum.Viewer, RoleEnum.Editor, RoleEnum.Owner],\n            )\n\n    def get_answer_generator(\n        self,\n        brain,\n        chat_id,\n        model,\n        temperature,\n        streaming,\n        prompt_id,\n        user_id,\n        user_email,\n    ):\n        if brain and brain.brain_type == BrainType.DOC:\n            return KnowledgeBrainQA(\n                chat_id=chat_id,\n                brain_id=str(brain.brain_id),\n                streaming=streaming,\n                prompt_id=prompt_id,\n                user_id=user_id,\n                user_email=user_email,\n            )\n\n        if brain.brain_type == BrainType.INTEGRATION:\n            integration_brain = integration_brain_description_service.get_integration_description_by_user_brain_id(\n                brain.brain_id, user_id\n            )\n\n            integration_class = integration_list.get(\n                integration_brain.integration_name.lower()\n            )\n            if integration_class:\n                return integration_class(\n                    chat_id=chat_id,\n                    temperature=temperature,\n                    brain_id=str(brain.brain_id),\n                    streaming=streaming,\n                    prompt_id=prompt_id,\n                    user_id=user_id,\n                    user_email=user_email,\n                )\n", "backend/modules/chat/controller/chat/factory.py": "from uuid import UUID\n\nfrom .brainful_chat import BrainfulChat\nfrom .brainless_chat import BrainlessChat\n\n\ndef get_chat_strategy(brain_id: UUID | None = None):\n    if brain_id:\n        return BrainfulChat()\n    else:\n        return BrainlessChat()\n", "backend/modules/chat/controller/chat/utils.py": "import time\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom logger import get_logger\nfrom models.databases.llm_models import LLMModels\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.service.chat_service import ChatService\nfrom modules.user.service.user_usage import UserUsage\n\nlogger = get_logger(__name__)\nbrain_service = BrainService()\nchat_service = ChatService()\n\n\nclass NullableUUID(UUID):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(v, values, **kwargs):\n        logger.info(f\"Validating UUID: {v}\")\n        if v == \"\":\n            return None\n        try:\n            return UUID(v)\n        except ValueError:\n            return None\n\n\ndef find_model_and_generate_metadata(\n    chat_id: UUID,\n    brain_model: str,\n    user_settings,\n    models_settings,\n):\n\n    # Default model is gpt-3.5-turbo-0125\n    default_model = \"gpt-3.5-turbo-0125\"\n    model_to_use = LLMModels(  # TODO Implement default models in database\n        name=default_model, price=1, max_input=4000, max_output=1000\n    )\n\n    logger.debug(\"Brain model: %s\", brain_model)\n\n    # If brain.model is None, set it to the default_model\n    if brain_model is None:\n        brain_model = default_model\n\n    is_brain_model_available = any(\n        brain_model == model_dict.get(\"name\") for model_dict in models_settings\n    )\n\n    is_user_allowed_model = brain_model in user_settings.get(\n        \"models\", [default_model]\n    )  # Checks if the model is available in the list of models\n\n    logger.debug(f\"Brain model: {brain_model}\")\n    logger.debug(f\"User models: {user_settings.get('models', [])}\")\n    logger.debug(f\"Model available: {is_brain_model_available}\")\n    logger.debug(f\"User allowed model: {is_user_allowed_model}\")\n\n    if is_brain_model_available and is_user_allowed_model:\n        # Use the model from the brain\n        model_to_use.name = brain_model\n        for model_dict in models_settings:\n            if model_dict.get(\"name\") == model_to_use.name:\n                model_to_use.price = model_dict.get(\"price\")\n                model_to_use.max_input = model_dict.get(\"max_input\")\n                model_to_use.max_output = model_dict.get(\"max_output\")\n                break\n\n    logger.info(f\"Model to use: {model_to_use}\")\n\n    return model_to_use\n\n\ndef update_user_usage(usage: UserUsage, user_settings, cost: int = 100):\n    \"\"\"Checks the user requests limit.\n    It checks the user requests limit and raises an exception if the user has reached the limit.\n    By default, the user has a limit of 100 requests per month. The limit can be increased by upgrading the plan.\n\n    Args:\n        user (UserIdentity): User object\n        model (str): Model name for which the user is making the request\n\n    Raises:\n        HTTPException: Raises a 429 error if the user has reached the limit.\n    \"\"\"\n    usage\n\n    date = time.strftime(\"%Y%m%d\")\n\n    monthly_chat_credit = user_settings.get(\"monthly_chat_credit\", 100)\n    montly_usage = usage.get_user_monthly_usage(date)\n\n    if int(montly_usage + cost) > int(monthly_chat_credit):\n        raise HTTPException(\n            status_code=429,  # pyright: ignore reportPrivateUsage=none\n            detail=f\"You have reached your monthly chat limit of {monthly_chat_credit} requests per months. Please upgrade your plan to increase your monthly chat limit.\",\n        )\n    else:\n        usage.handle_increment_user_request_count(date, cost)\n        pass\n", "backend/modules/chat/controller/chat/interface.py": "from abc import ABC, abstractmethod\n\n\nclass ChatInterface(ABC):\n    @abstractmethod\n    def validate_authorization(self, user_id, required_roles):\n        pass\n\n    @abstractmethod\n    def get_answer_generator(\n        self,\n        chat_id,\n        model,\n        max_tokens,\n        temperature,\n        streaming,\n        prompt_id,\n        user_id,\n        chat_question,\n    ):\n        pass\n", "backend/modules/chat/controller/chat/brainless_chat.py": "from llm.qa_headless import HeadlessQA\nfrom modules.chat.controller.chat.interface import ChatInterface\n\n\nclass BrainlessChat(ChatInterface):\n    def validate_authorization(self, user_id, brain_id):\n        pass\n\n    def get_answer_generator(\n        self,\n        chat_id,\n        model,\n        max_tokens,\n        temperature,\n        streaming,\n        prompt_id,\n        user_id,\n    ):\n        return HeadlessQA(\n            chat_id=chat_id,\n            model=model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            streaming=streaming,\n            prompt_id=prompt_id,\n        )\n", "backend/modules/chat/repository/chats.py": "from models.settings import get_supabase_client\nfrom modules.chat.dto.inputs import ChatMessageProperties\nfrom modules.chat.entity.chat import Chat\nfrom modules.chat.repository.chats_interface import ChatsInterface\n\n\nclass Chats(ChatsInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def create_chat(self, new_chat):\n        response = self.db.table(\"chats\").insert(new_chat).execute()\n        return response\n\n    def get_chat_by_id(self, chat_id: str):\n        response = (\n            self.db.from_(\"chats\")\n            .select(\"*\")\n            .filter(\"chat_id\", \"eq\", chat_id)\n            .execute()\n        )\n        return response\n\n    def add_question_and_answer(self, chat_id, question_and_answer):\n        response = (\n            self.db.table(\"chat_history\")\n            .insert(\n                {\n                    \"chat_id\": str(chat_id),\n                    \"user_message\": question_and_answer.question,\n                    \"assistant\": question_and_answer.answer,\n                }\n            )\n            .execute()\n        ).data\n        if len(response) > 0:\n            response = Chat(response[0])\n\n        return None\n\n    def get_chat_history(self, chat_id: str):\n        response = (\n            self.db.from_(\"chat_history\")\n            .select(\"*\")\n            .filter(\"chat_id\", \"eq\", chat_id)\n            .order(\"message_time\", desc=False)  # Add the ORDER BY clause\n            .execute()\n        )\n\n        return response\n\n    def get_user_chats(self, user_id):\n        response = (\n            self.db.from_(\"chats\")\n            .select(\"chat_id,user_id,creation_time,chat_name\")\n            .filter(\"user_id\", \"eq\", user_id)\n            .order(\"creation_time\", desc=False)\n            .execute()\n        )\n        return response\n\n    def update_chat_history(self, chat_history):\n        response = (\n            self.db.table(\"chat_history\")\n            .insert(\n                {\n                    \"chat_id\": str(chat_history.chat_id),\n                    \"user_message\": chat_history.user_message,\n                    \"assistant\": chat_history.assistant,\n                    \"prompt_id\": (\n                        str(chat_history.prompt_id) if chat_history.prompt_id else None\n                    ),\n                    \"brain_id\": (\n                        str(chat_history.brain_id) if chat_history.brain_id else None\n                    ),\n                    \"metadata\": chat_history.metadata if chat_history.metadata else {},\n                }\n            )\n            .execute()\n        )\n\n        return response\n\n    def update_chat(self, chat_id, updates):\n        response = (\n            self.db.table(\"chats\").update(updates).match({\"chat_id\": chat_id}).execute()\n        )\n\n        return response\n\n    def update_message_by_id(self, message_id, updates):\n        response = (\n            self.db.table(\"chat_history\")\n            .update(updates)\n            .match({\"message_id\": message_id})\n            .execute()\n        )\n\n        return response\n\n    def delete_chat(self, chat_id):\n        self.db.table(\"chats\").delete().match({\"chat_id\": chat_id}).execute()\n\n    def delete_chat_history(self, chat_id):\n        self.db.table(\"chat_history\").delete().match({\"chat_id\": chat_id}).execute()\n\n    def update_chat_message(\n        self, chat_id, message_id, chat_message_properties: ChatMessageProperties\n    ):\n        response = (\n            self.db.table(\"chat_history\")\n            .update(chat_message_properties)\n            .match({\"message_id\": message_id, \"chat_id\": chat_id})\n            .execute()\n        )\n\n        return response\n", "backend/modules/chat/repository/__init__.py": "", "backend/modules/chat/repository/chats_interface.py": "from abc import ABC, abstractmethod\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom modules.chat.dto.inputs import ChatMessageProperties, CreateChatHistory, QuestionAndAnswer\nfrom modules.chat.entity.chat import Chat\n\n\nclass ChatsInterface(ABC):\n    @abstractmethod\n    def create_chat(self, new_chat):\n        \"\"\"\n        Insert a chat entry in \"chats\" db\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_chat_by_id(self, chat_id: str):\n        \"\"\"\n        Get chat details by chat_id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_question_and_answer(\n        self, chat_id: UUID, question_and_answer: QuestionAndAnswer\n    ) -> Optional[Chat]:\n        \"\"\"\n        Add a question and answer to the chat history\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_chat_history(self, chat_id: str):\n        \"\"\"\n        Get chat history by chat_id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_user_chats(self, user_id: str):\n        \"\"\"\n        Get all chats for a user\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_chat_history(self, chat_history: CreateChatHistory):\n        \"\"\"\n        Update chat history\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_chat(self, chat_id, updates):\n        \"\"\"\n        Update chat details\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_message_by_id(self, message_id, updates):\n        \"\"\"\n        Update message details\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_chat(self, chat_id):\n        \"\"\"\n        Delete chat\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_chat_history(self, chat_id):\n        \"\"\"\n        Delete chat history\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_chat_message(self, chat_id, message_id, chat_message_properties: ChatMessageProperties):\n        \"\"\"\n        Update chat message\n        \"\"\"\n        pass\n", "backend/modules/chat/service/chat_service.py": "import random\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom logger import get_logger\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.dto.chats import ChatItem\nfrom modules.chat.dto.inputs import (\n    ChatMessageProperties,\n    ChatUpdatableProperties,\n    CreateChatHistory,\n    CreateChatProperties,\n    QuestionAndAnswer,\n)\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.entity.chat import Chat, ChatHistory\nfrom modules.chat.repository.chats import Chats\nfrom modules.chat.repository.chats_interface import ChatsInterface\nfrom modules.chat.service.utils import merge_chat_history_and_notifications\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.prompt.service.prompt_service import PromptService\n\nlogger = get_logger(__name__)\n\nprompt_service = PromptService()\nbrain_service = BrainService()\nnotification_service = NotificationService()\n\n\nclass ChatService:\n    repository: ChatsInterface\n\n    def __init__(self):\n        self.repository = Chats()\n\n    def create_chat(self, user_id: UUID, chat_data: CreateChatProperties) -> Chat:\n        # Chat is created upon the user's first question asked\n        logger.info(f\"New chat entry in chats table for user {user_id}\")\n\n        # Insert a new row into the chats table\n        new_chat = {\n            \"user_id\": str(user_id),\n            \"chat_name\": chat_data.name,\n        }\n        insert_response = self.repository.create_chat(new_chat)\n        logger.info(f\"Insert response {insert_response.data}\")\n\n        return insert_response.data[0]\n\n    def get_follow_up_question(\n        self, brain_id: UUID = None, question: str = None\n    ) -> [str]:\n        follow_up = [\n            \"Summarize the conversation\",\n            \"Explain in more detail\",\n            \"Explain like I'm 5\",\n            \"Provide a list\",\n            \"Give examples\",\n            \"Use simpler language\",\n            \"Elaborate on a specific point\",\n            \"Provide pros and cons\",\n            \"Break down into steps\",\n            \"Illustrate with an image or diagram\",\n        ]\n        # Return 3 random follow up questions amongs the list\n        random3 = random.sample(follow_up, 3)\n        return random3\n\n    def add_question_and_answer(\n        self, chat_id: UUID, question_and_answer: QuestionAndAnswer\n    ) -> Optional[Chat]:\n        return self.repository.add_question_and_answer(chat_id, question_and_answer)\n\n    def get_chat_by_id(self, chat_id: str) -> Chat:\n        response = self.repository.get_chat_by_id(chat_id)\n        return Chat(response.data[0])\n\n    def get_chat_history(self, chat_id: str) -> List[GetChatHistoryOutput]:\n        history: List[dict] = self.repository.get_chat_history(chat_id).data\n        if history is None:\n            return []\n        else:\n            enriched_history: List[GetChatHistoryOutput] = []\n            brain_cache = {}\n            prompt_cache = {}\n            for message in history:\n                message = ChatHistory(message)\n                brain = None\n                if message.brain_id:\n                    if message.brain_id in brain_cache:\n                        brain = brain_cache[message.brain_id]\n                    else:\n                        brain = brain_service.get_brain_by_id(message.brain_id)\n                        brain_cache[message.brain_id] = brain\n\n                prompt = None\n                if message.prompt_id:\n                    if message.prompt_id in prompt_cache:\n                        prompt = prompt_cache[message.prompt_id]\n                    else:\n                        prompt = prompt_service.get_prompt_by_id(message.prompt_id)\n                        prompt_cache[message.prompt_id] = prompt\n\n                enriched_history.append(\n                    GetChatHistoryOutput(\n                        chat_id=(UUID(message.chat_id)),\n                        message_id=(UUID(message.message_id)),\n                        user_message=message.user_message,\n                        assistant=message.assistant,\n                        message_time=message.message_time,\n                        brain_name=brain.name if brain else None,\n                        brain_id=str(brain.id) if brain else None,\n                        prompt_title=prompt.title if prompt else None,\n                        metadata=message.metadata,\n                        thumbs=message.thumbs,\n                    )\n                )\n            return enriched_history\n\n    def get_chat_history_with_notifications(\n        self,\n        chat_id: UUID,\n    ) -> List[ChatItem]:\n        chat_history = self.get_chat_history(str(chat_id))\n        chat_notifications = []\n        return merge_chat_history_and_notifications(chat_history, chat_notifications)\n\n    def get_user_chats(self, user_id: str) -> List[Chat]:\n        response = self.repository.get_user_chats(user_id)\n        chats = [Chat(chat_dict) for chat_dict in response.data]\n        return chats\n\n    def update_chat_history(self, chat_history: CreateChatHistory) -> ChatHistory:\n        response: List[ChatHistory] = (\n            self.repository.update_chat_history(chat_history)\n        ).data\n        if len(response) == 0:\n            raise HTTPException(\n                status_code=500,\n                detail=\"An exception occurred while updating chat history.\",\n            )\n        return ChatHistory(response[0])  # pyright: ignore reportPrivateUsage=none\n\n    def update_chat(self, chat_id, chat_data: ChatUpdatableProperties) -> Chat:\n        if not chat_id:\n            logger.error(\"No chat_id provided\")\n            return  # pyright: ignore reportPrivateUsage=none\n\n        updates = {}\n\n        if chat_data.chat_name is not None:\n            updates[\"chat_name\"] = chat_data.chat_name\n\n        updated_chat = None\n\n        if updates:\n            updated_chat = (self.repository.update_chat(chat_id, updates)).data[0]\n            logger.info(f\"Chat {chat_id} updated\")\n        else:\n            logger.info(f\"No updates to apply for chat {chat_id}\")\n        return updated_chat  # pyright: ignore reportPrivateUsage=none\n\n    def update_message_by_id(\n        self,\n        message_id: str,\n        user_message: str = None,  # pyright: ignore reportPrivateUsage=none\n        assistant: str = None,  # pyright: ignore reportPrivateUsage=none\n        metadata: dict = None,  # pyright: ignore reportPrivateUsage=none\n    ) -> ChatHistory:\n        if not message_id:\n            logger.error(\"No message_id provided\")\n            return  # pyright: ignore reportPrivateUsage=none\n\n        updates = {}\n\n        if user_message is not None:\n            updates[\"user_message\"] = user_message\n\n        if assistant is not None:\n            updates[\"assistant\"] = assistant\n\n        if metadata is not None:\n            updates[\"metadata\"] = metadata\n\n        updated_message = None\n\n        if updates:\n            updated_message = (self.repository.update_message_by_id(message_id, updates)).data[  # type: ignore\n                0\n            ]\n            logger.info(f\"Message {message_id} updated\")\n        else:\n            logger.info(f\"No updates to apply for message {message_id}\")\n        return ChatHistory(updated_message)  # pyright: ignore reportPrivateUsage=none\n\n    def delete_chat_from_db(self, chat_id):\n        try:\n            self.repository.delete_chat_history(chat_id)\n        except Exception as e:\n            print(e)\n            pass\n        try:\n            self.repository.delete_chat(chat_id)\n        except Exception as e:\n            print(e)\n            pass\n\n    def update_chat_message(\n        self, chat_id, message_id, chat_message_properties: ChatMessageProperties\n    ):\n        try:\n            return self.repository.update_chat_message(\n                chat_id, message_id, chat_message_properties\n            ).data\n        except Exception as e:\n            print(e)\n            pass\n", "backend/modules/chat/service/utils.py": "from typing import List\n\nfrom logger import get_logger\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.dto.chats import ChatItem, ChatItemType\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.notification.entity.notification import Notification\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.prompt.service.prompt_service import PromptService\nfrom packages.utils import parse_message_time\n\nlogger = get_logger(__name__)\n\nprompt_service = PromptService()\nbrain_service = BrainService()\nnotification_service = NotificationService()\n\n\ndef merge_chat_history_and_notifications(\n    chat_history: List[GetChatHistoryOutput], notifications: List[Notification]\n) -> List[ChatItem]:\n    chat_history_and_notifications = chat_history + notifications\n\n    chat_history_and_notifications.sort(\n        key=lambda x: parse_message_time(x.message_time)\n        if isinstance(x, GetChatHistoryOutput)\n        else parse_message_time(x.datetime)\n    )\n\n    transformed_data = []\n    for item in chat_history_and_notifications:\n        if isinstance(item, GetChatHistoryOutput):\n            item_type = ChatItemType.MESSAGE\n            body = item\n        else:\n            item_type = ChatItemType.NOTIFICATION\n            body = item\n        transformed_item = ChatItem(item_type=item_type, body=body)\n        transformed_data.append(transformed_item)\n\n    return transformed_data\n", "backend/modules/chat/service/__init__.py": "", "backend/modules/chat/dto/chats.py": "from enum import Enum\nfrom typing import List, Optional, Tuple, Union\nfrom uuid import UUID\n\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.notification.entity.notification import Notification\nfrom pydantic import BaseModel\n\n\nclass ChatMessage(BaseModel):\n    model: str\n    question: str\n    # A list of tuples where each tuple is (speaker, text)\n    history: List[Tuple[str, str]]\n    temperature: float = 0.0\n    max_tokens: int = 256\n    use_summarization: bool = False\n    chat_id: Optional[UUID] = None\n    chat_name: Optional[str] = None\n\n\nclass ChatQuestion(BaseModel):\n    question: str\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    max_tokens: Optional[int] = None\n    brain_id: Optional[UUID] = None\n    prompt_id: Optional[UUID] = None\n\n\nclass Sources(BaseModel):\n    name: str\n    source_url: str\n    type: str\n    original_file_name: str\n    citation: str\n\n\nclass ChatItemType(Enum):\n    MESSAGE = \"MESSAGE\"\n    NOTIFICATION = \"NOTIFICATION\"\n\n\nclass ChatItem(BaseModel):\n    item_type: ChatItemType\n    body: Union[GetChatHistoryOutput, Notification]\n", "backend/modules/chat/dto/outputs.py": "from typing import List, Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass GetChatHistoryOutput(BaseModel):\n    chat_id: UUID\n    message_id: Optional[UUID] | str\n    user_message: str\n    assistant: str\n    message_time: Optional[str] = None\n    prompt_title: Optional[str] | None = None\n    brain_name: Optional[str] | None = None\n    brain_id: Optional[str] | None = (\n        None  # string because UUID is not JSON serializable\n    )\n    metadata: Optional[dict] | None = None\n    thumbs: Optional[bool] | None = None\n\n    def dict(self, *args, **kwargs):\n        chat_history = super().dict(*args, **kwargs)\n        chat_history[\"chat_id\"] = str(chat_history.get(\"chat_id\"))\n        chat_history[\"message_id\"] = str(chat_history.get(\"message_id\"))\n\n        return chat_history\n\n\nclass FunctionCall(BaseModel):\n    arguments: str\n    name: str\n\n\nclass ChatCompletionMessageToolCall(BaseModel):\n    id: str\n    function: FunctionCall\n    type: str = \"function\"\n\n\nclass CompletionMessage(BaseModel):\n    # = \"assistant\" | \"user\" | \"system\" | \"tool\"\n    role: str\n    content: str | None = None\n    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None\n\n\nclass CompletionResponse(BaseModel):\n    finish_reason: str\n    message: CompletionMessage\n\n\nclass BrainCompletionOutput(BaseModel):\n    messages: List[CompletionMessage]\n    question: str\n    response: CompletionResponse\n", "backend/modules/chat/dto/__init__.py": "", "backend/modules/chat/dto/inputs.py": "from dataclasses import dataclass\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass CreateChatHistory(BaseModel):\n    chat_id: UUID\n    user_message: str\n    assistant: str\n    prompt_id: Optional[UUID] = None\n    brain_id: Optional[UUID] = None\n    metadata: Optional[dict] = {}\n\n\nclass QuestionAndAnswer(BaseModel):\n    question: str\n    answer: str\n\n\n@dataclass\nclass CreateChatProperties:\n    name: str\n\n    def __init__(self, name: str):\n        self.name = name\n\n\n@dataclass\nclass ChatUpdatableProperties:\n    chat_name: Optional[str] = None\n\n    def __init__(self, chat_name: Optional[str]):\n        self.chat_name = chat_name\n\n\nclass ChatMessageProperties(BaseModel, extra=\"ignore\"):\n    thumbs: Optional[bool]\n\n    def dict(self, *args, **kwargs):\n        chat_dict = super().dict(*args, **kwargs)\n        if chat_dict.get(\"thumbs\"):\n            # Set thumbs to boolean value or None if not present\n            chat_dict[\"thumbs\"] = bool(chat_dict[\"thumbs\"])\n        return chat_dict\n", "backend/modules/chat/entity/chat.py": "from dataclasses import asdict, dataclass\nfrom typing import Optional\nfrom uuid import UUID\n\n\n@dataclass\nclass Chat:\n    chat_id: str\n    user_id: str\n    creation_time: str\n    chat_name: str\n\n    def __init__(self, chat_dict: dict):\n        self.chat_id = chat_dict.get(\"chat_id\", \"\")\n        self.user_id = chat_dict.get(\"user_id\", \"\")\n        self.creation_time = chat_dict.get(\"creation_time\", \"\")\n        self.chat_name = chat_dict.get(\"chat_name\", \"\")\n\n\n@dataclass\nclass ChatHistory:\n    chat_id: str\n    message_id: str\n    user_message: str\n    assistant: str\n    message_time: str\n    prompt_id: Optional[UUID]\n    brain_id: Optional[UUID]\n    metadata: Optional[dict] = None\n    thumbs: Optional[bool] = None\n\n    def __init__(self, chat_dict: dict):\n        self.chat_id = chat_dict.get(\"chat_id\", \"\")\n        self.message_id = chat_dict.get(\"message_id\", \"\")\n        self.user_message = chat_dict.get(\"user_message\", \"\")\n        self.assistant = chat_dict.get(\"assistant\", \"\")\n        self.message_time = chat_dict.get(\"message_time\", \"\")\n\n        self.prompt_id = chat_dict.get(\"prompt_id\")\n        self.brain_id = chat_dict.get(\"brain_id\")\n        self.metadata = chat_dict.get(\"metadata\")\n        self.thumbs = chat_dict.get(\"thumbs\")\n\n    def to_dict(self):\n        return asdict(self)\n", "backend/modules/chat/entity/__init__.py": "", "backend/modules/upload/__init__.py": "", "backend/modules/upload/controller/upload_routes.py": "import os\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom celery_worker import process_file_and_notify\nfrom fastapi import APIRouter, Depends, HTTPException, Query, UploadFile\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.brain.entity.brain_entity import RoleEnum\nfrom modules.brain.service.brain_authorization_service import (\n    validate_brain_authorization,\n)\nfrom modules.knowledge.dto.inputs import CreateKnowledgeProperties\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.notification.dto.inputs import (\n    CreateNotification,\n    NotificationUpdatableProperties,\n)\nfrom modules.notification.entity.notification import NotificationsStatusEnum\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.upload.service.upload_file import upload_file_storage\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_usage import UserUsage\nfrom packages.files.file import convert_bytes, get_file_size\nfrom packages.utils.telemetry import maybe_send_telemetry\n\nlogger = get_logger(__name__)\nupload_router = APIRouter()\n\nnotification_service = NotificationService()\nknowledge_service = KnowledgeService()\n\n\n@upload_router.get(\"/upload/healthz\", tags=[\"Health\"])\nasync def healthz():\n    return {\"status\": \"ok\"}\n\n\n@upload_router.post(\"/upload\", dependencies=[Depends(AuthBearer())], tags=[\"Upload\"])\nasync def upload_file(\n    uploadFile: UploadFile,\n    brain_id: UUID = Query(..., description=\"The ID of the brain\"),\n    chat_id: Optional[UUID] = Query(None, description=\"The ID of the chat\"),\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    validate_brain_authorization(\n        brain_id, current_user.id, [RoleEnum.Editor, RoleEnum.Owner]\n    )\n    uploadFile.file.seek(0)\n    user_daily_usage = UserUsage(\n        id=current_user.id,\n        email=current_user.email,\n    )\n\n    upload_notification = notification_service.add_notification(\n        CreateNotification(\n            user_id=current_user.id,\n            status=NotificationsStatusEnum.INFO,\n            title=f\"Processing File {uploadFile.filename}\",\n        )\n    )\n\n    user_settings = user_daily_usage.get_user_settings()\n\n    remaining_free_space = user_settings.get(\"max_brain_size\", 1000000000)\n    maybe_send_telemetry(\"upload_file\", {\"file_name\": uploadFile.filename})\n    file_size = get_file_size(uploadFile)\n    if remaining_free_space - file_size < 0:\n        message = f\"Brain will exceed maximum capacity. Maximum file allowed is : {convert_bytes(remaining_free_space)}\"\n        raise HTTPException(status_code=403, detail=message)\n\n    file_content = await uploadFile.read()\n\n    filename_with_brain_id = str(brain_id) + \"/\" + str(uploadFile.filename)\n\n    try:\n        upload_file_storage(file_content, filename_with_brain_id)\n\n    except Exception as e:\n        print(e)\n\n        notification_service.update_notification_by_id(\n            upload_notification.id if upload_notification else None,\n            NotificationUpdatableProperties(\n                status=NotificationsStatusEnum.ERROR,\n                description=f\"There was an error uploading the file: {e}\",\n            ),\n        )\n        if \"The resource already exists\" in str(e):\n            raise HTTPException(\n                status_code=403,\n                detail=f\"File {uploadFile.filename} already exists in storage.\",\n            )\n        else:\n            raise HTTPException(\n                status_code=500, detail=f\"Failed to upload file to storage. {e}\"\n            )\n\n    knowledge_to_add = CreateKnowledgeProperties(\n        brain_id=brain_id,\n        file_name=uploadFile.filename,\n        extension=os.path.splitext(\n            uploadFile.filename  # pyright: ignore reportPrivateUsage=none\n        )[-1].lower(),\n    )\n\n    knowledge_service.add_knowledge(knowledge_to_add)\n\n    process_file_and_notify.delay(\n        file_name=filename_with_brain_id,\n        file_original_name=uploadFile.filename,\n        brain_id=brain_id,\n        notification_id=upload_notification.id,\n    )\n    return {\"message\": \"File processing has started.\"}\n", "backend/modules/upload/controller/__init__.py": "from .upload_routes import upload_router\n", "backend/modules/upload/service/list_files.py": "from multiprocessing import get_logger\n\nfrom models import get_supabase_client\nfrom supabase.client import Client\n\nlogger = get_logger()\n\n\ndef list_files_from_storage(path):\n    supabase_client: Client = get_supabase_client()\n\n    try:\n        response = supabase_client.storage.from_(\"quivr\").list(path)\n        return response\n    except Exception as e:\n        logger.error(e)\n", "backend/modules/upload/service/generate_file_signed_url.py": "from multiprocessing import get_logger\n\nfrom models import get_supabase_client\nfrom supabase.client import Client\n\nlogger = get_logger()\n\nSIGNED_URL_EXPIRATION_PERIOD_IN_SECONDS = 3600\n\n\ndef generate_file_signed_url(path):\n    supabase_client: Client = get_supabase_client()\n\n    try:\n        response = supabase_client.storage.from_(\"quivr\").create_signed_url(\n            path,\n            SIGNED_URL_EXPIRATION_PERIOD_IN_SECONDS,\n            options={\n                \"download\": True,\n                \"transform\": None,\n            },\n        )\n        logger.info(\"RESPONSE SIGNED URL\", response)\n        return response\n    except Exception as e:\n        logger.error(e)\n", "backend/modules/upload/service/__init__.py": "", "backend/modules/upload/service/upload_file.py": "import json\nimport os\nfrom multiprocessing import get_logger\n\nfrom langchain.pydantic_v1 import Field\nfrom langchain.schema import Document\nfrom logger import get_logger\nfrom models import get_supabase_client\nfrom supabase.client import Client\n\nlogger = get_logger(__name__)\n\n\n# Mapping of file extensions to MIME types\nmime_types = {\n    \".txt\": \"text/plain\",\n    \".csv\": \"text/csv\",\n    \".md\": \"text/markdown\",\n    \".markdown\": \"text/markdown\",\n    \".telegram\": \"application/x-telegram\",\n    \".m4a\": \"audio/mp4\",\n    \".mp3\": \"audio/mpeg\",\n    \".webm\": \"audio/webm\",\n    \".mp4\": \"video/mp4\",\n    \".mpga\": \"audio/mpeg\",\n    \".wav\": \"audio/wav\",\n    \".mpeg\": \"video/mpeg\",\n    \".pdf\": \"application/pdf\",\n    \".html\": \"text/html\",\n    \".pptx\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n    \".docx\": \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n    \".odt\": \"application/vnd.oasis.opendocument.text\",\n    \".xlsx\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n    \".xls\": \"application/vnd.ms-excel\",\n    \".epub\": \"application/epub+zip\",\n    \".ipynb\": \"application/x-ipynb+json\",\n    \".py\": \"text/x-python\",\n}\n\n\ndef check_file_exists(brain_id: str, file_identifier: str) -> bool:\n\n    supabase_client: Client = get_supabase_client()\n    try:\n        # Check if the file exists\n        logger.info(f\"Checking if file {file_identifier} exists.\")\n        # This needs to be converted into a file_identifier that is safe for a URL\n\n        response = supabase_client.storage.from_(\"quivr\").list(brain_id)\n\n        # Check if the file_identifier is in the response\n        file_exists = any(\n            file[\"name\"].split(\".\")[0] == file_identifier.split(\".\")[0]\n            for file in response\n        )\n        logger.info(f\"File identifier: {file_identifier}\")\n        logger.info(f\"File exists: {file_exists}\")\n        if file_exists:\n            logger.info(f\"File {file_identifier} exists.\")\n            return True\n        else:\n            logger.info(f\"File {file_identifier} does not exist.\")\n            return False\n    except Exception as e:\n        logger.error(f\"An error occurred while checking the file: {e}\")\n        return True\n\n\ndef upload_file_storage(file, file_identifier: str, upsert: str = \"false\"):\n    supabase_client: Client = get_supabase_client()\n    response = None\n\n    try:\n        # Get the file extension\n        _, file_extension = os.path.splitext(file_identifier)\n\n        # Get the MIME type for the file extension\n        mime_type = mime_types.get(file_extension, \"text/html\")\n\n        response = supabase_client.storage.from_(\"quivr\").upload(\n            file_identifier,\n            file,\n            file_options={\n                \"content-type\": mime_type,\n                \"upsert\": upsert,\n                \"cache-control\": \"3600\",\n            },\n        )\n\n        return response\n    except Exception as e:\n        if \"The resource already exists\" in str(e) and upsert == \"true\":\n            response = supabase_client.storage.from_(\"quivr\").update(\n                file_identifier,\n                file,\n                file_options={\n                    \"content-type\": mime_type,\n                    \"upsert\": upsert,\n                    \"cache-control\": \"3600\",\n                },\n            )\n        else:\n            raise e\n\n\nclass DocumentSerializable(Document):\n    \"\"\"Class for storing a piece of text and associated metadata.\"\"\"\n\n    page_content: str\n    metadata: dict = Field(default_factory=dict)\n\n    @property\n    def lc_serializable(self) -> bool:\n        return True\n\n    def __repr__(self):\n        return f\"Document(page_content='{self.page_content[:50]}...', metadata={self.metadata})\"\n\n    def __str__(self):\n        return self.__repr__()\n\n    def to_json(self) -> str:\n        \"\"\"Convert the Document object to a JSON string.\"\"\"\n        return json.dumps(\n            {\n                \"page_content\": self.page_content,\n                \"metadata\": self.metadata,\n            }\n        )\n\n    @classmethod\n    def from_json(cls, json_str: str):\n        \"\"\"Create a Document object from a JSON string.\"\"\"\n        data = json.loads(json_str)\n        return cls(page_content=data[\"page_content\"], metadata=data[\"metadata\"])\n", "backend/modules/knowledge/__init__.py": "", "backend/modules/knowledge/controller/__init__.py": "from .knowledge_routes import knowledge_router", "backend/modules/knowledge/controller/knowledge_routes.py": "from uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException, Query\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.brain.entity.brain_entity import RoleEnum\nfrom modules.brain.service.brain_authorization_service import (\n    has_brain_authorization,\n    validate_brain_authorization,\n)\nfrom modules.brain.service.brain_vector_service import BrainVectorService\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.upload.service.generate_file_signed_url import generate_file_signed_url\nfrom modules.user.entity.user_identity import UserIdentity\n\nknowledge_router = APIRouter()\nlogger = get_logger(__name__)\n\nknowledge_service = KnowledgeService()\n\n\n@knowledge_router.get(\n    \"/knowledge\", dependencies=[Depends(AuthBearer())], tags=[\"Knowledge\"]\n)\nasync def list_knowledge_in_brain_endpoint(\n    brain_id: UUID = Query(..., description=\"The ID of the brain\"),\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Retrieve and list all the knowledge in a brain.\n    \"\"\"\n\n    validate_brain_authorization(brain_id=brain_id, user_id=current_user.id)\n\n    knowledges = knowledge_service.get_all_knowledge(brain_id)\n\n    return {\"knowledges\": knowledges}\n\n\n@knowledge_router.delete(\n    \"/knowledge/{knowledge_id}\",\n    dependencies=[\n        Depends(AuthBearer()),\n        Depends(has_brain_authorization(RoleEnum.Owner)),\n    ],\n    tags=[\"Knowledge\"],\n)\nasync def delete_endpoint(\n    knowledge_id: UUID,\n    current_user: UserIdentity = Depends(get_current_user),\n    brain_id: UUID = Query(..., description=\"The ID of the brain\"),\n):\n    \"\"\"\n    Delete a specific knowledge from a brain.\n    \"\"\"\n\n    knowledge = knowledge_service.get_knowledge(knowledge_id)\n    file_name = knowledge.file_name if knowledge.file_name else knowledge.url\n    knowledge_service.remove_knowledge(knowledge_id)\n\n    brain_vector_service = BrainVectorService(brain_id)\n    if knowledge.file_name:\n        brain_vector_service.delete_file_from_brain(knowledge.file_name)\n    elif knowledge.url:\n        brain_vector_service.delete_file_url_from_brain(knowledge.url)\n\n    return {\n        \"message\": f\"{file_name} of brain {brain_id} has been deleted by user {current_user.email}.\"\n    }\n\n\n@knowledge_router.get(\n    \"/knowledge/{knowledge_id}/signed_download_url\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Knowledge\"],\n)\nasync def generate_signed_url_endpoint(\n    knowledge_id: UUID,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Generate a signed url to download the file from storage.\n    \"\"\"\n\n    knowledge = knowledge_service.get_knowledge(knowledge_id)\n\n    validate_brain_authorization(brain_id=knowledge.brain_id, user_id=current_user.id)\n\n    if knowledge.file_name == None:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Knowledge with id {knowledge_id} is not a file.\",\n        )\n\n    file_path_in_storage = f\"{knowledge.brain_id}/{knowledge.file_name}\"\n\n    file_signed_url = generate_file_signed_url(file_path_in_storage)\n\n    return file_signed_url\n", "backend/modules/knowledge/repository/knowledges.py": "from fastapi import HTTPException\nfrom models.settings import get_supabase_client\nfrom modules.knowledge.dto.outputs import DeleteKnowledgeResponse\nfrom modules.knowledge.entity.knowledge import Knowledge\nfrom modules.knowledge.repository.knowledge_interface import KnowledgeInterface\n\n\nclass Knowledges(KnowledgeInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def insert_knowledge(self, knowledge):\n        \"\"\"\n        Add a knowledge\n        \"\"\"\n        # Check if the knowledge already exists\n        knowledge_exists = (\n            self.db.from_(\"knowledge\")\n            .select(\"*\")\n            .filter(\"brain_id\", \"eq\", knowledge.brain_id)\n            .filter(\"file_name\", \"eq\", knowledge.file_name)\n            .execute()\n        ).data\n\n        if knowledge_exists:\n            return Knowledge(**knowledge_exists[0])  # TODO fix this\n\n        response = (self.db.from_(\"knowledge\").insert(knowledge.dict()).execute()).data\n        return Knowledge(**response[0])\n\n    def remove_knowledge_by_id(\n        # todo: update remove brain endpoints to first delete the knowledge\n        self,\n        knowledge_id,\n    ):\n        \"\"\"\n        Args:\n            knowledge_id (UUID): The id of the knowledge\n\n        Returns:\n            str: Status message\n        \"\"\"\n        response = (\n            self.db.from_(\"knowledge\")\n            .delete()\n            .filter(\"id\", \"eq\", knowledge_id)\n            .execute()\n            .data\n        )\n\n        if response == []:\n            raise HTTPException(404, \"Knowledge not found\")\n\n        return DeleteKnowledgeResponse(\n            # change to response[0].brain_id and knowledge_id[0].brain_id\n            status=\"deleted\",\n            knowledge_id=knowledge_id,\n        )\n\n    def get_knowledge_by_id(self, knowledge_id):\n        \"\"\"\n        Get a knowledge by its id\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        knowledge = (\n            self.db.from_(\"knowledge\")\n            .select(\"*\")\n            .filter(\"id\", \"eq\", str(knowledge_id))\n            .execute()\n        ).data\n\n        return Knowledge(**knowledge[0])\n\n    def get_all_knowledge_in_brain(self, brain_id):\n        \"\"\"\n        Get all the knowledge in a brain\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        all_knowledge = (\n            self.db.from_(\"knowledge\")\n            .select(\"*\")\n            .filter(\"brain_id\", \"eq\", str(brain_id))\n            .execute()\n        ).data\n\n        return [Knowledge(**knowledge) for knowledge in all_knowledge]\n\n    def remove_brain_all_knowledge(self, brain_id):\n        \"\"\"\n        Remove all knowledge in a brain\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        all_knowledge = self.get_all_knowledge_in_brain(brain_id)\n        knowledge_to_delete_list = []\n\n        for knowledge in all_knowledge:\n            if knowledge.file_name:\n                knowledge_to_delete_list.append(f\"{brain_id}/{knowledge.file_name}\")\n\n        if knowledge_to_delete_list:\n            self.db.storage.from_(\"quivr\").remove(knowledge_to_delete_list)\n\n        self.db.from_(\"knowledge\").delete().filter(\n            \"brain_id\", \"eq\", str(brain_id)\n        ).execute()\n", "backend/modules/knowledge/repository/storage.py": "from logger import get_logger\nfrom models.settings import get_supabase_client\nfrom modules.knowledge.repository.storage_interface import StorageInterface\n\nlogger = get_logger(__name__)\n\nclass Storage(StorageInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def upload_file(self, file_name: str):\n        \"\"\"\n        Upload file to storage\n        \"\"\"\n        self.db.storage.from_(\"quivr\").download(file_name)\n\n    def remove_file(self, file_name: str):\n        \"\"\"\n        Remove file from storage\n        \"\"\"\n        try:\n            response = self.db.storage.from_(\"quivr\").remove([file_name])\n            return response\n        except Exception as e:\n            logger.error(e)\n            # raise e\n", "backend/modules/knowledge/repository/__init__.py": "from .knowledges import Knowledges\n", "backend/modules/knowledge/repository/storage_interface.py": "from abc import ABC, abstractmethod\n\n\nclass StorageInterface(ABC):\n    @abstractmethod\n    def remove_file(self, file_name: str):\n        \"\"\"\n        Remove file from storage\n        \"\"\"\n        pass\n", "backend/modules/knowledge/repository/knowledge_interface.py": "from abc import ABC, abstractmethod\nfrom typing import List\nfrom uuid import UUID\n\nfrom modules.knowledge.dto.inputs import CreateKnowledgeProperties\nfrom modules.knowledge.dto.outputs import DeleteKnowledgeResponse\nfrom modules.knowledge.entity.knowledge import Knowledge\n\n\nclass KnowledgeInterface(ABC):\n    @abstractmethod\n    def insert_knowledge(self, knowledge: CreateKnowledgeProperties) -> Knowledge:\n        \"\"\"\n        Add a knowledge\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_knowledge_by_id(\n        # todo: update remove brain endpoints to first delete the knowledge\n        self,\n        knowledge_id: UUID,\n    ) -> DeleteKnowledgeResponse:\n        \"\"\"\n        Args:\n            knowledge_id (UUID): The id of the knowledge\n\n        Returns:\n            str: Status message\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_knowledge_by_id(self, knowledge_id: UUID) -> Knowledge:\n        \"\"\"\n        Get a knowledge by its id\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_knowledge_in_brain(self, brain_id: UUID) -> List[Knowledge]:\n        \"\"\"\n        Get all the knowledge in a brain\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_brain_all_knowledge(self, brain_id: UUID) -> None:\n        \"\"\"\n        Remove all knowledge in a brain\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        pass\n", "backend/modules/knowledge/service/__init__.py": "", "backend/modules/knowledge/service/knowledge_service.py": "from uuid import UUID\n\nfrom logger import get_logger\nfrom modules.knowledge.dto.inputs import CreateKnowledgeProperties\nfrom modules.knowledge.entity.knowledge import Knowledge\nfrom modules.knowledge.repository.knowledge_interface import KnowledgeInterface\nfrom modules.knowledge.repository.knowledges import Knowledges\n\nlogger = get_logger(__name__)\n\n\nclass KnowledgeService:\n    repository: KnowledgeInterface\n\n    def __init__(self):\n        self.repository = Knowledges()\n\n    def add_knowledge(self, knowledge_to_add: CreateKnowledgeProperties):\n        knowledge = self.repository.insert_knowledge(knowledge_to_add)\n\n        return knowledge\n\n    def get_all_knowledge(self, brain_id: UUID):\n        knowledges = self.repository.get_all_knowledge_in_brain(brain_id)\n\n        return knowledges\n\n    def get_knowledge(self, knowledge_id: UUID) -> Knowledge:\n        knowledge = self.repository.get_knowledge_by_id(knowledge_id)\n\n        return knowledge\n\n    def remove_brain_all_knowledge(self, brain_id: UUID) -> None:\n        self.repository.remove_brain_all_knowledge(brain_id)\n\n        logger.info(\n            f\"All knowledge in brain {brain_id} removed successfully from table\"\n        )\n\n    def remove_knowledge(self, knowledge_id: UUID):\n        message = self.repository.remove_knowledge_by_id(knowledge_id)\n\n        return message\n", "backend/modules/knowledge/dto/outputs.py": "from uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass DeleteKnowledgeResponse(BaseModel):\n    status: str = \"delete\"\n    knowledge_id: UUID\n", "backend/modules/knowledge/dto/__init__.py": "from .inputs import CreateKnowledgeProperties\nfrom .outputs import DeleteKnowledgeResponse\n", "backend/modules/knowledge/dto/inputs.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass CreateKnowledgeProperties(BaseModel):\n    brain_id: UUID\n    file_name: Optional[str] = None\n    url: Optional[str] = None\n    extension: str = \"txt\"\n    integration: Optional[str] = None\n    integration_link: Optional[str] = None\n\n    def dict(self, *args, **kwargs):\n        knowledge_dict = super().dict(*args, **kwargs)\n        knowledge_dict[\"brain_id\"] = str(knowledge_dict.get(\"brain_id\"))\n        return knowledge_dict\n", "backend/modules/knowledge/entity/knowledge.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass Knowledge(BaseModel):\n    id: UUID\n    brain_id: UUID\n    file_name: Optional[str] = None\n    url: Optional[str] = None\n    extension: str = \"txt\"\n", "backend/modules/knowledge/entity/__init__.py": "from .knowledge import Knowledge\n", "backend/modules/tools/web_search.py": "import os\nfrom typing import Dict, Optional, Type\n\nimport requests\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain.pydantic_v1 import Field as FieldV1\nfrom langchain_core.tools import BaseTool\nfrom logger import get_logger\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass WebSearchInput(BaseModelV1):\n    query: str = FieldV1(..., title=\"query\", description=\"search query to look up\")\n\n\nclass WebSearchTool(BaseTool):\n    name = \"brave-web-search\"\n    description = \"useful for when you need to search the web for something.\"\n    args_schema: Type[BaseModel] = WebSearchInput\n    api_key: str = os.getenv(\"BRAVE_SEARCH_API_KEY\", \"\")\n\n    def _check_environment_variable(self) -> bool:\n        \"\"\"Check if the environment variable is set.\"\"\"\n\n        return os.getenv(\"BRAVE_SEARCH_API_KEY\") is not None\n\n    def __init__(self):\n        if not self._check_environment_variable():\n            raise ValueError(\"BRAVE_SEARCH_API_KEY environment variable is not set\")\n        super().__init__()\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> Dict:\n        \"\"\"Run the tool.\"\"\"\n        headers = {\n            \"Accept\": \"application/json\",\n            \"Accept-Encoding\": \"gzip\",\n            \"X-Subscription-Token\": self.api_key,\n        }\n        response = requests.get(\n            f\"https://api.search.brave.com/res/v1/web/search?q={query}&count=3\",\n            headers=headers,\n        )\n        return self._parse_response(response.json())\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> Dict:\n        \"\"\"Run the tool asynchronously.\"\"\"\n        headers = {\n            \"Accept\": \"application/json\",\n            \"Accept-Encoding\": \"gzip\",\n            \"X-Subscription-Token\": self.api_key,\n        }\n        response = requests.get(\n            f\"https://api.search.brave.com/res/v1/web/search?q={query}&count=3\",\n            headers=headers,\n        )\n        return self._parse_response(response.json())\n\n    def _parse_response(self, response: Dict) -> str:\n        \"\"\"Parse the response.\"\"\"\n        short_results = []\n        results = response[\"web\"][\"results\"]\n        for result in results:\n            title = result[\"title\"]\n            url = result[\"url\"]\n            description = result[\"description\"]\n            short_results.append(self._format_result(title, description, url))\n        return \"\\n\".join(short_results)\n\n    def _format_result(self, title: str, description: str, url: str) -> str:\n        return f\"**{title}**\\n{description}\\n{url}\"\n\n\nif __name__ == \"__main__\":\n    tool = WebSearchTool()\n    print(tool.run(\"python\"))\n", "backend/modules/tools/image_generator.py": "from typing import Optional, Type\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain.pydantic_v1 import Field as FieldV1\nfrom langchain.tools import BaseTool\nfrom langchain_core.tools import BaseTool\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclass ImageGenerationInput(BaseModelV1):\n    query: str = FieldV1(\n        ...,\n        title=\"description\",\n        description=\"A detailled prompt to generate the image from. Takes into account the history of the chat.\",\n    )\n\n\nclass ImageGeneratorTool(BaseTool):\n    name = \"image-generator\"\n    description = \"useful for when you need to generate an image from a prompt.\"\n    args_schema: Type[BaseModel] = ImageGenerationInput\n    return_direct = True\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        client = OpenAI()\n\n        response = client.images.generate(\n            model=\"dall-e-3\",\n            prompt=query,\n            size=\"1024x1024\",\n            quality=\"standard\",\n            n=1,\n        )\n        image_url = response.data[0].url\n        revised_prompt = response.data[0].revised_prompt\n        # Make the url a markdown image\n        return f\"{revised_prompt} \\n ![Generated Image]({image_url}) \"\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"Use the tool asynchronously.\"\"\"\n        client = OpenAI()\n        response = await run_manager.run_async(\n            client.images.generate,\n            model=\"dall-e-3\",\n            prompt=query,\n            size=\"1024x1024\",\n            quality=\"standard\",\n            n=1,\n        )\n        image_url = response.data[0].url\n        revised_prompt = response.data[0].revised_prompt\n        # Make the url a markdown image\n        return f\"{revised_prompt} \\n ![Generated Image]({image_url}) \"\n", "backend/modules/tools/__init__.py": "from .image_generator import ImageGeneratorTool\nfrom .web_search import WebSearchTool\nfrom .url_reader import URLReaderTool\nfrom .email_sender import EmailSenderTool", "backend/modules/tools/url_reader.py": "# Extract and combine content recursively\nimport os\nfrom typing import Dict, Optional, Type\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain.pydantic_v1 import Field as FieldV1\nfrom langchain_community.document_loaders import PlaywrightURLLoader\nfrom langchain_core.tools import BaseTool\nfrom logger import get_logger\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass URLReaderInput(BaseModelV1):\n    url: str = FieldV1(..., title=\"url\", description=\"url to read\")\n\n\nclass URLReaderTool(BaseTool):\n    name = \"url-reader\"\n    description = \"useful for when you need to read the content of a url.\"\n    args_schema: Type[BaseModel] = URLReaderInput\n    api_key = os.getenv(\"BRAVE_SEARCH_API_KEY\")\n\n    def _run(\n        self, url: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> Dict:\n\n        loader = PlaywrightURLLoader(urls=[url], remove_selectors=[\"header\", \"footer\"])\n        data = loader.load()\n\n        extracted_content = \"\"\n        for page in data:\n            extracted_content += page.page_content\n\n        return {\"content\": extracted_content}\n\n    async def _arun(\n        self, url: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> Dict:\n        \"\"\"Run the tool asynchronously.\"\"\"\n        loader = PlaywrightURLLoader(urls=[url], remove_selectors=[\"header\", \"footer\"])\n        data = loader.load()\n\n        extracted_content = \"\"\n        for page in data:\n            extracted_content += page.page_content\n\n        return {\"content\": extracted_content}\n", "backend/modules/tools/email_sender.py": "# Extract and combine content recursively\nfrom typing import Dict, Optional, Type\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain.pydantic_v1 import Field as FieldV1\nfrom langchain_community.document_loaders import PlaywrightURLLoader\nfrom langchain_core.tools import BaseTool\nfrom logger import get_logger\nfrom models import BrainSettings\nfrom modules.contact_support.controller.settings import ContactsSettings\nfrom packages.emails.send_email import send_email\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass EmailInput(BaseModelV1):\n    text: str = FieldV1(\n        ...,\n        title=\"text\",\n        description=\"text to send in HTML email format. Use pretty formating, use bold, italic, next line, etc...\",\n    )\n\n\nclass EmailSenderTool(BaseTool):\n    user_email: str\n    name = \"email-sender\"\n    description = \"useful for when you need to send an email.\"\n    args_schema: Type[BaseModel] = EmailInput\n    brain_settings: BrainSettings = BrainSettings()\n    contact_settings: ContactsSettings = ContactsSettings()\n\n    def _run(\n        self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> Dict:\n\n        html_body = f\"\"\"\n        <div style=\"text-align: center;\">\n                <img src=\"https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png\" alt=\"Quivr Logo\" style=\"width: 100px; height: 100px; border-radius: 50%; margin: 0 auto; display: block;\">\n                <br />\n            </div>\n            \"\"\"\n        html_body += f\"\"\"\n            {text}\n            \"\"\"\n        logger.debug(f\"Email body: {html_body}\")\n        logger.debug(f\"Email to: {self.user_email}\")\n        logger.debug(f\"Email from: {self.contact_settings.resend_contact_sales_from}\")\n        try:\n            r = send_email(\n                {\n                    \"sender\": self.contact_settings.resend_contact_sales_from,\n                    \"to\": self.user_email,\n                    \"reply_to\": \"no-reply@quivr.app\",\n                    \"subject\": \"Email from your assistant\",\n                    \"html\": html_body,\n                }\n            )\n            logger.info(\"Resend response\", r)\n        except Exception as e:\n            logger.error(f\"Error sending email: {e}\")\n            return {\"content\": \"Error sending email because of error: \" + str(e)}\n\n        return {\"content\": \"Email sent\"}\n\n    async def _arun(\n        self, url: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> Dict:\n        \"\"\"Run the tool asynchronously.\"\"\"\n        loader = PlaywrightURLLoader(urls=[url], remove_selectors=[\"header\", \"footer\"])\n        data = loader.load()\n\n        extracted_content = \"\"\n        for page in data:\n            extracted_content += page.page_content\n\n        return {\"content\": extracted_content}\n", "backend/modules/prompt/__init__.py": "", "backend/modules/prompt/controller/__init__.py": "from .prompt_routes import prompt_router\n", "backend/modules/prompt/controller/prompt_routes.py": "from uuid import UUID\n\nfrom fastapi import APIRouter, Depends\nfrom middlewares.auth import AuthBearer\nfrom modules.prompt.entity.prompt import (\n    CreatePromptProperties,\n    Prompt,\n    PromptUpdatableProperties,\n)\nfrom modules.prompt.service import PromptService\n\nprompt_router = APIRouter()\n\npromptService = PromptService()\n\n\n@prompt_router.get(\"/prompts\", dependencies=[Depends(AuthBearer())], tags=[\"Prompt\"])\nasync def get_prompts() -> list[Prompt]:\n    \"\"\"\n    Retrieve all public prompt\n    \"\"\"\n    return promptService.get_public_prompts()\n\n\n@prompt_router.get(\n    \"/prompts/{prompt_id}\", dependencies=[Depends(AuthBearer())], tags=[\"Prompt\"]\n)\nasync def get_prompt(prompt_id: UUID) -> Prompt | None:\n    \"\"\"\n    Retrieve a prompt by its id\n    \"\"\"\n\n    return promptService.get_prompt_by_id(prompt_id)\n\n\n@prompt_router.put(\n    \"/prompts/{prompt_id}\", dependencies=[Depends(AuthBearer())], tags=[\"Prompt\"]\n)\nasync def update_prompt(\n    prompt_id: UUID, prompt: PromptUpdatableProperties\n) -> Prompt | None:\n    \"\"\"\n    Update a prompt by its id\n    \"\"\"\n\n    return promptService.update_prompt_by_id(prompt_id, prompt)\n\n\n@prompt_router.post(\"/prompts\", dependencies=[Depends(AuthBearer())], tags=[\"Prompt\"])\nasync def create_prompt_route(prompt: CreatePromptProperties) -> Prompt | None:\n    \"\"\"\n    Create a prompt by its id\n    \"\"\"\n\n    return promptService.create_prompt(prompt)\n", "backend/modules/prompt/repository/prompts.py": "from fastapi import HTTPException\nfrom models.settings import get_supabase_client\nfrom modules.prompt.entity.prompt import Prompt\nfrom modules.prompt.repository.prompts_interface import (\n    DeletePromptResponse,\n    PromptsInterface,\n)\n\n\nclass Prompts(PromptsInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def create_prompt(self, prompt):\n        \"\"\"\n        Create a prompt\n        \"\"\"\n\n        response = (self.db.from_(\"prompts\").insert(prompt.dict()).execute()).data\n\n        return Prompt(**response[0])\n\n    def delete_prompt_by_id(self, prompt_id):\n        \"\"\"\n        Delete a prompt by id\n        Args:\n            prompt_id (UUID): The id of the prompt\n\n        Returns:\n        A dictionary containing the status of the delete and prompt_id of the deleted prompt\n        \"\"\"\n\n        # Update brains where prompt_id is equal to the value to NULL\n        self.db.from_(\"brains\").update({\"prompt_id\": None}).filter(\n            \"prompt_id\", \"eq\", prompt_id\n        ).execute()\n\n        # Update chat_history where prompt_id is equal to the value to NULL\n        self.db.from_(\"chat_history\").update({\"prompt_id\": None}).filter(\n            \"prompt_id\", \"eq\", prompt_id\n        ).execute()\n\n        # Delete the prompt\n        response = (\n            self.db.from_(\"prompts\")\n            .delete()\n            .filter(\"id\", \"eq\", prompt_id)\n            .execute()\n            .data\n        )\n\n        if response == []:\n            raise HTTPException(404, \"Prompt not found\")\n\n        return DeletePromptResponse(status=\"deleted\", prompt_id=prompt_id)\n\n    def get_prompt_by_id(self, prompt_id):\n        \"\"\"\n        Get a prompt by its id\n\n        Args:\n            prompt_id (UUID): The id of the prompt\n\n        Returns:\n            Prompt: The prompt\n        \"\"\"\n\n        response = (\n            self.db.from_(\"prompts\").select(\"*\").filter(\"id\", \"eq\", prompt_id).execute()\n        ).data\n\n        if response == []:\n            return None\n        return Prompt(**response[0])\n\n    def get_public_prompts(self):\n        \"\"\"\n        List all public prompts\n        \"\"\"\n\n        return (\n            self.db.from_(\"prompts\")\n            .select(\"*\")\n            .filter(\"status\", \"eq\", \"public\")\n            .execute()\n        ).data\n\n    def update_prompt_by_id(self, prompt_id, prompt):\n        \"\"\"Update a prompt by id\"\"\"\n\n        response = (\n            self.db.from_(\"prompts\")\n            .update(prompt.dict(exclude_unset=True))\n            .filter(\"id\", \"eq\", prompt_id)\n            .execute()\n        ).data\n\n        if response == []:\n            raise HTTPException(404, \"Prompt not found\")\n\n        return Prompt(**response[0])\n", "backend/modules/prompt/repository/prompts_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.prompt.entity import (\n    CreatePromptProperties,\n    DeletePromptResponse,\n    Prompt,\n    PromptUpdatableProperties,\n)\n\n\nclass PromptsInterface(ABC):\n    @abstractmethod\n    def create_prompt(self, prompt: CreatePromptProperties) -> Prompt:\n        \"\"\"\n        Create a prompt\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_prompt_by_id(self, prompt_id: UUID) -> DeletePromptResponse:\n        \"\"\"\n        Delete a prompt by id\n        Args:\n            prompt_id (UUID): The id of the prompt\n\n        Returns:\n        A dictionary containing the status of the delete and prompt_id of the deleted prompt\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_prompt_by_id(self, prompt_id: UUID) -> Prompt | None:\n        \"\"\"\n        Get a prompt by its id\n\n        Args:\n            prompt_id (UUID): The id of the prompt\n\n        Returns:\n            Prompt: The prompt\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_public_prompts(self) -> list[Prompt]:\n        \"\"\"\n        List all public prompts\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_prompt_by_id(\n        self, prompt_id: UUID, prompt: PromptUpdatableProperties\n    ) -> Prompt:\n        \"\"\"Update a prompt by id\"\"\"\n        pass\n", "backend/modules/prompt/repository/__init__.py": "", "backend/modules/prompt/service/get_prompt_to_use.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom modules.brain.service.utils.get_prompt_to_use_id import get_prompt_to_use_id\nfrom modules.prompt.service import PromptService\n\npromptService = PromptService()\n\n\ndef get_prompt_to_use(brain_id: Optional[UUID], prompt_id: Optional[UUID]) -> str:\n    prompt_to_use_id = get_prompt_to_use_id(brain_id, prompt_id)\n    if prompt_to_use_id is None:\n        return None\n\n    return promptService.get_prompt_by_id(prompt_to_use_id)\n", "backend/modules/prompt/service/__init__.py": "from .prompt_service import PromptService\n", "backend/modules/prompt/service/prompt_service.py": "from typing import List\nfrom uuid import UUID\n\nfrom models.settings import get_supabase_client\nfrom modules.prompt.entity.prompt import (\n    CreatePromptProperties,\n    DeletePromptResponse,\n    Prompt,\n    PromptUpdatableProperties,\n)\nfrom modules.prompt.repository.prompts import Prompts\n\n\nclass PromptService:\n    repository: Prompts\n\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.repository = Prompts()\n\n    def create_prompt(self, prompt: CreatePromptProperties) -> Prompt:\n        return self.repository.create_prompt(prompt)\n\n    def delete_prompt_by_id(self, prompt_id: UUID) -> DeletePromptResponse:\n        \"\"\"\n        Delete a prompt by id\n        Args:\n            prompt_id (UUID): The id of the prompt\n\n        Returns:\n            Prompt: The prompt\n        \"\"\"\n        return self.repository.delete_prompt_by_id(prompt_id)\n\n    def get_prompt_by_id(self, prompt_id: UUID) -> Prompt | None:\n        \"\"\"\n        Get a prompt by its id\n\n        Args:\n            prompt_id (UUID): The id of the prompt\n\n        Returns:\n            Prompt: The prompt\n        \"\"\"\n        return self.repository.get_prompt_by_id(prompt_id)\n\n    def get_public_prompts(self) -> List[Prompt]:\n        \"\"\"\n        List all public prompts\n        \"\"\"\n\n        return self.repository.get_public_prompts()\n\n    def update_prompt_by_id(\n        self, prompt_id: UUID, prompt: PromptUpdatableProperties\n    ) -> Prompt:\n        \"\"\"Update a prompt by id\"\"\"\n\n        return self.repository.update_prompt_by_id(prompt_id, prompt)\n", "backend/modules/prompt/entity/prompt.py": "from enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass PromptStatusEnum(str, Enum):\n    private = \"private\"\n    public = \"public\"\n\n\nclass Prompt(BaseModel):\n    title: str\n    content: str\n    status: PromptStatusEnum = PromptStatusEnum.private\n    id: UUID\n\n\nclass CreatePromptProperties(BaseModel):\n    \"\"\"Properties that can be received on prompt creation\"\"\"\n\n    title: str\n    content: str\n    status: PromptStatusEnum = PromptStatusEnum.private\n\n\nclass PromptUpdatableProperties(BaseModel):\n    \"\"\"Properties that can be received on prompt update\"\"\"\n\n    title: Optional[str] = None\n    content: Optional[str] = None\n    status: Optional[PromptStatusEnum] = None\n\n\nclass DeletePromptResponse(BaseModel):\n    \"\"\"Response when deleting a prompt\"\"\"\n\n    status: str = \"delete\"\n    prompt_id: UUID\n", "backend/modules/prompt/entity/__init__.py": "from .prompt import Prompt, PromptStatusEnum, CreatePromptProperties, PromptUpdatableProperties, DeletePromptResponse", "backend/modules/authorization/utils/__init__.py": "", "backend/modules/sync/tasks.py": "import asyncio\n\nfrom celery_config import celery\nfrom logger import get_logger\nfrom modules.knowledge.repository.storage import Storage\nfrom modules.sync.repository.sync_files import SyncFiles\nfrom modules.sync.service.sync_service import SyncService, SyncUserService\nfrom modules.sync.utils.googleutils import GoogleSyncUtils\nfrom modules.sync.utils.sharepointutils import AzureSyncUtils\n\nlogger = get_logger(__name__)\n\n\n@celery.task(name=\"process_sync_active\")\ndef process_sync_active():\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(_process_sync_active())\n\n\nasync def _process_sync_active():\n    sync_active_service = SyncService()\n    sync_user_service = SyncUserService()\n    sync_files_repo_service = SyncFiles()\n    storage = Storage()\n\n    google_sync_utils = GoogleSyncUtils(\n        sync_user_service=sync_user_service,\n        sync_active_service=sync_active_service,\n        sync_files_repo=sync_files_repo_service,\n        storage=storage,\n    )\n\n    azure_sync_utils = AzureSyncUtils(\n        sync_user_service=sync_user_service,\n        sync_active_service=sync_active_service,\n        sync_files_repo=sync_files_repo_service,\n        storage=storage,\n    )\n    active = await sync_active_service.get_syncs_active_in_interval()\n\n    for sync in active:\n        try:\n            details_user_sync = sync_user_service.get_sync_user_by_id(\n                sync.syncs_user_id\n            )\n            if details_user_sync[\"provider\"].lower() == \"google\":\n                await google_sync_utils.sync(\n                    sync_active_id=sync.id, user_id=sync.user_id\n                )\n            elif details_user_sync[\"provider\"].lower() == \"azure\":\n                await azure_sync_utils.sync(\n                    sync_active_id=sync.id, user_id=sync.user_id\n                )\n            else:\n                logger.info(\"Provider not supported: %s\", details_user_sync[\"provider\"])\n        except Exception as e:\n            logger.error(f\"Error syncing: {e}\")\n            continue\n", "backend/modules/sync/__init__.py": "", "backend/modules/sync/controller/azure_sync_routes.py": "import os\n\nimport requests\nfrom fastapi import APIRouter, Depends, HTTPException, Request\nfrom fastapi.responses import HTMLResponse\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.sync.dto.inputs import SyncsUserInput, SyncUserUpdateInput\nfrom modules.sync.service.sync_service import SyncService, SyncUserService\nfrom modules.user.entity.user_identity import UserIdentity\nfrom msal import PublicClientApplication\n\nfrom .successfull_connection import successfullConnectionPage\n\n# Initialize logger\nlogger = get_logger(__name__)\n\n# Initialize sync service\nsync_service = SyncService()\nsync_user_service = SyncUserService()\n\n# Initialize API router\nazure_sync_router = APIRouter()\n\n# Constants\nCLIENT_ID = os.getenv(\"SHAREPOINT_CLIENT_ID\")\nAUTHORITY = \"https://login.microsoftonline.com/common\"\nBACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:5050\")\nREDIRECT_URI = f\"{BACKEND_URL}/sync/azure/oauth2callback\"\nSCOPE = [\n    \"https://graph.microsoft.com/Files.Read\",\n    \"https://graph.microsoft.com/User.Read\",\n    \"https://graph.microsoft.com/Sites.Read.All\",\n]\n\n\n@azure_sync_router.post(\n    \"/sync/azure/authorize\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\ndef authorize_azure(\n    request: Request, name: str, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Authorize Azure sync for the current user.\n\n    Args:\n        request (Request): The request object.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        dict: A dictionary containing the authorization URL.\n    \"\"\"\n    client = PublicClientApplication(CLIENT_ID, authority=AUTHORITY)\n    logger.debug(f\"Authorizing Azure sync for user: {current_user.id}\")\n    state = f\"user_id={current_user.id}, name={name}\"\n    authorization_url = client.get_authorization_request_url(\n        scopes=SCOPE, redirect_uri=REDIRECT_URI, state=state\n    )\n\n    sync_user_input = SyncsUserInput(\n        user_id=str(current_user.id),\n        name=name,\n        provider=\"Azure\",\n        credentials={},\n        state={\"state\": state},\n    )\n    sync_user_service.create_sync_user(sync_user_input)\n    return {\"authorization_url\": authorization_url}\n\n\n@azure_sync_router.get(\"/sync/azure/oauth2callback\", tags=[\"Sync\"])\ndef oauth2callback_azure(request: Request):\n    \"\"\"\n    Handle OAuth2 callback from Azure.\n\n    Args:\n        request (Request): The request object.\n\n    Returns:\n        dict: A dictionary containing a success message.\n    \"\"\"\n    client = PublicClientApplication(CLIENT_ID, authority=AUTHORITY)\n    state = request.query_params.get(\"state\")\n    state_split = state.split(\",\")\n    current_user = state_split[0].split(\"=\")[1]  # Extract user_id from state\n    name = state_split[1].split(\"=\")[1] if state else None\n    state_dict = {\"state\": state}\n    logger.debug(\n        f\"Handling OAuth2 callback for user: {current_user} with state: {state}\"\n    )\n    sync_user_state = sync_user_service.get_sync_user_by_state(state_dict)\n    logger.info(f\"Retrieved sync user state: {sync_user_state}\")\n\n    if state_dict != sync_user_state[\"state\"]:\n        logger.error(\"Invalid state parameter\")\n        raise HTTPException(status_code=400, detail=\"Invalid state parameter\")\n    if sync_user_state.get(\"user_id\") != current_user:\n        logger.error(\"Invalid user\")\n        raise HTTPException(status_code=400, detail=\"Invalid user\")\n\n    result = client.acquire_token_by_authorization_code(\n        request.query_params.get(\"code\"), scopes=SCOPE, redirect_uri=REDIRECT_URI\n    )\n    if \"access_token\" not in result:\n        logger.error(\"Failed to acquire token\")\n        raise HTTPException(status_code=400, detail=\"Failed to acquire token\")\n\n    access_token = result[\"access_token\"]\n\n    creds = result\n    logger.info(f\"Fetched OAuth2 token for user: {current_user}\")\n\n    # Fetch user email from Microsoft Graph API\n    graph_url = \"https://graph.microsoft.com/v1.0/me\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    response = requests.get(graph_url, headers=headers)\n    if response.status_code != 200:\n        logger.error(\"Failed to fetch user profile from Microsoft Graph API\")\n        raise HTTPException(status_code=400, detail=\"Failed to fetch user profile\")\n\n    user_info = response.json()\n    user_email = user_info.get(\"mail\") or user_info.get(\"userPrincipalName\")\n    logger.info(f\"Retrieved email for user: {current_user} - {user_email}\")\n\n    sync_user_input = SyncUserUpdateInput(\n        credentials=result, state={}, email=user_email\n    )\n\n    sync_user_service.update_sync_user(current_user, state_dict, sync_user_input)\n    logger.info(f\"Azure sync created successfully for user: {current_user}\")\n    return HTMLResponse(successfullConnectionPage)\n", "backend/modules/sync/controller/successfull_connection.py": "successfullConnectionPage = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\">\n    <style>\n        body {\n            display: flex;\n            justify-content: center;\n            align-items: center;\n            height: 100vh;\n            margin: 0;\n            background-color: #f8f9fa;\n            font-family: Arial, sans-serif;\n        }\n        .container {\n            text-align: center;\n        }\n        .message {\n            font-size: 2em;\n            margin-bottom: 20px;\n        }\n        .icon {\n            font-size: 2em;\n            color: white;\n            background-color: green;\n            border-radius: 50%;\n            padding: 20px;\n            margin-bottom: 20px;\n        }\n        .close-button {\n            padding: 10px 20px;\n            font-size: 1em;\n            color: #fff;\n            background-color: #6142d4;\n            border: none;\n            border-radius: 5px;\n            cursor: pointer;\n        }\n        .close-button:hover {\n            background-color: #0056b3;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <i class=\"fas fa-check icon\"></i>\n        <div class=\"message\">Connection successful</div>\n        <button class=\"close-button\" onclick=\"window.close();\">Close Tab</button>\n    </div>\n</body>\n</html>\n\"\"\"", "backend/modules/sync/controller/google_sync_routes.py": "import json\nimport os\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request\nfrom fastapi.responses import HTMLResponse\nfrom google_auth_oauthlib.flow import Flow\nfrom googleapiclient.discovery import build\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.sync.dto.inputs import SyncsUserInput, SyncUserUpdateInput\nfrom modules.sync.service.sync_service import SyncService, SyncUserService\nfrom modules.user.entity.user_identity import UserIdentity\n\nfrom .successfull_connection import successfullConnectionPage\n\n# Set environment variable for OAuthlib\nos.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n# Initialize logger\nlogger = get_logger(__name__)\n\n# Initialize sync service\nsync_service = SyncService()\nsync_user_service = SyncUserService()\n\n# Initialize API router\ngoogle_sync_router = APIRouter()\n\n# Constants\nSCOPES = [\n    \"https://www.googleapis.com/auth/drive.metadata.readonly\",\n    \"https://www.googleapis.com/auth/drive.readonly\",\n    \"https://www.googleapis.com/auth/userinfo.email\",\n    \"openid\",\n]\nBACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:5050\")\nBASE_REDIRECT_URI = f\"{BACKEND_URL}/sync/google/oauth2callback\"\n\n# Create credentials content from environment variables\nCLIENT_SECRETS_FILE_CONTENT = {\n    \"installed\": {\n        \"client_id\": os.getenv(\"GOOGLE_CLIENT_ID\"),\n        \"project_id\": os.getenv(\"GOOGLE_PROJECT_ID\"),\n        \"auth_uri\": os.getenv(\"GOOGLE_AUTH_URI\"),\n        \"token_uri\": os.getenv(\"GOOGLE_TOKEN_URI\"),\n        \"auth_provider_x509_cert_url\": os.getenv(\"GOOGLE_AUTH_PROVIDER_CERT_URL\"),\n        \"client_secret\": os.getenv(\"GOOGLE_CLIENT_SECRET\"),\n        \"redirect_uris\": os.getenv(\"GOOGLE_REDIRECT_URI\", \"http://localhost\").split(\n            \",\"\n        ),\n        \"javascript_origins\": os.getenv(\n            \"GOOGLE_JAVASCRIPT_ORIGINS\", \"http://localhost\"\n        ).split(\",\"),\n    }\n}\n\n\n@google_sync_router.post(\n    \"/sync/google/authorize\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\ndef authorize_google(\n    request: Request, name: str, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Authorize Google Drive sync for the current user.\n\n    Args:\n        request (Request): The request object.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        dict: A dictionary containing the authorization URL.\n    \"\"\"\n    logger.debug(f\"Authorizing Google Drive sync for user: {current_user.id}\")\n    redirect_uri = BASE_REDIRECT_URI\n    flow = Flow.from_client_config(\n        CLIENT_SECRETS_FILE_CONTENT,\n        scopes=SCOPES,\n        redirect_uri=redirect_uri,\n    )\n    state = f\"user_id={current_user.id}, name={name}\"\n    authorization_url, state = flow.authorization_url(\n        access_type=\"offline\",\n        include_granted_scopes=\"true\",\n        state=state,\n        prompt=\"consent\",\n    )\n    logger.info(\n        f\"Generated authorization URL: {authorization_url} for user: {current_user.id}\"\n    )\n    sync_user_input = SyncsUserInput(\n        name=name,\n        user_id=str(current_user.id),\n        provider=\"Google\",\n        credentials={},\n        state={\"state\": state},\n    )\n    sync_user_service.create_sync_user(sync_user_input)\n    return {\"authorization_url\": authorization_url}\n\n\n@google_sync_router.get(\"/sync/google/oauth2callback\", tags=[\"Sync\"])\ndef oauth2callback_google(request: Request):\n    \"\"\"\n    Handle OAuth2 callback from Google.\n\n    Args:\n        request (Request): The request object.\n\n    Returns:\n        dict: A dictionary containing a success message.\n    \"\"\"\n    state = request.query_params.get(\"state\")\n    state_dict = {\"state\": state}\n    logger.info(f\"State: {state}\")\n    state_split = state.split(\",\")\n    current_user = state_split[0].split(\"=\")[1] if state else None\n    name = state_split[1].split(\"=\")[1] if state else None\n    logger.debug(\n        f\"Handling OAuth2 callback for user: {current_user} with state: {state}\"\n    )\n    sync_user_state = sync_user_service.get_sync_user_by_state(state_dict)\n    logger.info(f\"Retrieved sync user state: {sync_user_state}\")\n\n    if not sync_user_state or state_dict != sync_user_state.get(\"state\"):\n        logger.error(\"Invalid state parameter\")\n        raise HTTPException(status_code=400, detail=\"Invalid state parameter\")\n    if sync_user_state.get(\"user_id\") != current_user:\n        logger.error(\"Invalid user\")\n        logger.info(f\"Invalid user: {current_user}\")\n        raise HTTPException(status_code=400, detail=\"Invalid user\")\n\n    redirect_uri = f\"{BASE_REDIRECT_URI}\"\n    flow = Flow.from_client_config(\n        CLIENT_SECRETS_FILE_CONTENT,\n        scopes=SCOPES,\n        state=state,\n        redirect_uri=redirect_uri,\n    )\n    flow.fetch_token(authorization_response=str(request.url))\n    creds = flow.credentials\n    logger.info(f\"Fetched OAuth2 token for user: {current_user}\")\n\n    # Use the credentials to get the user's email\n    service = build(\"oauth2\", \"v2\", credentials=creds)\n    user_info = service.userinfo().get().execute()\n    user_email = user_info.get(\"email\")\n    logger.info(f\"Retrieved email for user: {current_user} - {user_email}\")\n\n    sync_user_input = SyncUserUpdateInput(\n        credentials=json.loads(creds.to_json()),\n        state={},\n        email=user_email,\n    )\n    sync_user_service.update_sync_user(current_user, state_dict, sync_user_input)\n    logger.info(f\"Google Drive sync created successfully for user: {current_user}\")\n    return HTMLResponse(successfullConnectionPage)\n", "backend/modules/sync/controller/sync_routes.py": "import os\nfrom typing import List\n\nfrom fastapi import APIRouter, Depends, status\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.notification.dto.inputs import CreateNotification\nfrom modules.notification.entity.notification import NotificationsStatusEnum\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.sync.controller.azure_sync_routes import azure_sync_router\nfrom modules.sync.controller.google_sync_routes import google_sync_router\nfrom modules.sync.dto import SyncsDescription\nfrom modules.sync.dto.inputs import SyncsActiveInput, SyncsActiveUpdateInput\nfrom modules.sync.dto.outputs import AuthMethodEnum\nfrom modules.sync.entity.sync import SyncsActive\nfrom modules.sync.service.sync_service import SyncService, SyncUserService\nfrom modules.user.entity.user_identity import UserIdentity\n\nnotification_service = NotificationService()\n\n# Set environment variable for OAuthlib\nos.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n# Initialize logger\nlogger = get_logger(__name__)\n\n# Initialize sync service\nsync_service = SyncService()\nsync_user_service = SyncUserService()\n\n# Initialize API router\nsync_router = APIRouter()\n\n# Add Google routes here\nsync_router.include_router(google_sync_router)\nsync_router.include_router(azure_sync_router)\n\n\n# Google sync description\ngoogle_sync = SyncsDescription(\n    name=\"Google\",\n    description=\"Sync your Google Drive with Quivr\",\n    auth_method=AuthMethodEnum.URI_WITH_CALLBACK,\n)\n\nazure_sync = SyncsDescription(\n    name=\"Azure\",\n    description=\"Sync your Azure Drive with Quivr\",\n    auth_method=AuthMethodEnum.URI_WITH_CALLBACK,\n)\n\n\n@sync_router.get(\n    \"/sync/all\",\n    response_model=List[SyncsDescription],\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def get_syncs(current_user: UserIdentity = Depends(get_current_user)):\n    \"\"\"\n    Get all available sync descriptions.\n\n    Args:\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        List[SyncsDescription]: A list of available sync descriptions.\n    \"\"\"\n    logger.debug(f\"Fetching all sync descriptions for user: {current_user.id}\")\n    return [google_sync, azure_sync]\n\n\n@sync_router.get(\n    \"/sync\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def get_user_syncs(current_user: UserIdentity = Depends(get_current_user)):\n    \"\"\"\n    Get syncs for the current user.\n\n    Args:\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        List: A list of syncs for the user.\n    \"\"\"\n    logger.debug(f\"Fetching user syncs for user: {current_user.id}\")\n    return sync_user_service.get_syncs_user(str(current_user.id))\n\n\n@sync_router.delete(\n    \"/sync/{sync_id}\",\n    status_code=status.HTTP_204_NO_CONTENT,\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def delete_user_sync(\n    sync_id: int, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Delete a sync for the current user.\n\n    Args:\n        sync_id (int): The ID of the sync to delete.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        None\n    \"\"\"\n    logger.debug(\n        f\"Deleting user sync for user: {current_user.id} with sync ID: {sync_id}\"\n    )\n    sync_user_service.delete_sync_user(sync_id, str(current_user.id))\n    return None\n\n\n@sync_router.post(\n    \"/sync/active\",\n    response_model=SyncsActive,\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def create_sync_active(\n    sync_active_input: SyncsActiveInput,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Create a new active sync for the current user.\n\n    Args:\n        sync_active_input (SyncsActiveInput): The sync active input data.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        SyncsActive: The created sync active data.\n    \"\"\"\n    logger.debug(\n        f\"Creating active sync for user: {current_user.id} with data: {sync_active_input}\"\n    )\n    notification_service.add_notification(\n        CreateNotification(\n            user_id=current_user.id,\n            status=NotificationsStatusEnum.SUCCESS,\n            title=\"Sync created! Synchronization takes a few minutes to complete\",\n            description=\"Syncing your files...\",\n        )\n    )\n    return sync_service.create_sync_active(sync_active_input, str(current_user.id))\n\n\n@sync_router.put(\n    \"/sync/active/{sync_id}\",\n    response_model=SyncsActive,\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def update_sync_active(\n    sync_id: str,\n    sync_active_input: SyncsActiveUpdateInput,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Update an existing active sync for the current user.\n\n    Args:\n        sync_id (str): The ID of the active sync to update.\n        sync_active_input (SyncsActiveUpdateInput): The updated sync active input data.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        SyncsActive: The updated sync active data.\n    \"\"\"\n    logger.debug(\n        f\"Updating active sync for user: {current_user.id} with data: {sync_active_input}\"\n    )\n    notification_service.add_notification(\n        CreateNotification(\n            user_id=current_user.id,\n            status=NotificationsStatusEnum.SUCCESS,\n            title=\"Sync updated! Synchronization takes a few minutes to complete\",\n            description=\"Syncing your files...\",\n        )\n    )\n    sync_active_input.force_sync = True\n    return sync_service.update_sync_active(sync_id, sync_active_input)\n\n\n@sync_router.delete(\n    \"/sync/active/{sync_id}\",\n    status_code=status.HTTP_204_NO_CONTENT,\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def delete_sync_active(\n    sync_id: str, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Delete an existing active sync for the current user.\n\n    Args:\n        sync_id (str): The ID of the active sync to delete.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        None\n    \"\"\"\n    logger.debug(\n        f\"Deleting active sync for user: {current_user.id} with sync ID: {sync_id}\"\n    )\n    notification_service.add_notification(\n        CreateNotification(\n            user_id=current_user.id,\n            status=NotificationsStatusEnum.SUCCESS,\n            title=\"Sync deleted!\",\n            description=\"Sync deleted!\",\n        )\n    )\n    sync_service.delete_sync_active(sync_id, str(current_user.id))\n    return None\n\n\n@sync_router.get(\n    \"/sync/active\",\n    response_model=List[SyncsActive],\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def get_active_syncs_for_user(\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Get all active syncs for the current user.\n\n    Args:\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        List[SyncsActive]: A list of active syncs for the current user.\n    \"\"\"\n    logger.debug(f\"Fetching active syncs for user: {current_user.id}\")\n    return sync_service.get_syncs_active(str(current_user.id))\n\n\n@sync_router.get(\n    \"/sync/{sync_id}/files\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def get_files_folder_user_sync(\n    user_sync_id: int,\n    folder_id: str = None,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Get files for an active sync.\n\n    Args:\n        sync_id (str): The ID of the active sync.\n        folder_id (str): The ID of the folder to get files from.\n        current_user (UserIdentity): The current authenticated user.\n\n    Returns:\n        SyncsActive: The active sync data.\n    \"\"\"\n    logger.debug(\n        f\"Fetching files for user sync: {user_sync_id} for user: {current_user.id}\"\n    )\n    return sync_user_service.get_files_folder_user_sync(\n        user_sync_id, str(current_user.id), folder_id\n    )\n\n\n@sync_router.get(\n    \"/sync/active/interval\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Sync\"],\n)\nasync def get_syncs_active_in_interval() -> List[SyncsActive]:\n    \"\"\"\n    Get all active syncs that need to be synced.\n\n    Returns:\n        List: A list of active syncs that need to be synced.\n    \"\"\"\n    logger.debug(\"Fetching active syncs in interval\")\n    return await sync_service.get_syncs_active_in_interval()\n", "backend/modules/sync/controller/__init__.py": "from .sync_routes import sync_router\n", "backend/modules/sync/utils/googleutils.py": "from datetime import datetime, timedelta, timezone\nfrom io import BytesIO\n\nfrom fastapi import UploadFile\nfrom google.auth.transport.requests import Request as GoogleRequest\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom logger import get_logger\nfrom modules.brain.repository.brains_vectors import BrainsVectors\nfrom modules.knowledge.repository.storage import Storage\nfrom modules.sync.dto.inputs import (\n    SyncFileInput,\n    SyncFileUpdateInput,\n    SyncsActiveUpdateInput,\n)\nfrom modules.sync.repository.sync_files import SyncFiles\nfrom modules.sync.service.sync_service import SyncService, SyncUserService\nfrom modules.sync.utils.list_files import (\n    get_google_drive_files,\n    get_google_drive_files_by_id,\n)\nfrom modules.sync.utils.upload import upload_file\nfrom modules.upload.service.upload_file import check_file_exists\nfrom pydantic import BaseModel, ConfigDict\n\nlogger = get_logger(__name__)\n\n\nclass GoogleSyncUtils(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    sync_user_service: SyncUserService\n    sync_active_service: SyncService\n    sync_files_repo: SyncFiles\n    storage: Storage\n\n    async def _upload_files(\n        self,\n        credentials: dict,\n        files: list,\n        current_user: str,\n        brain_id: str,\n        sync_active_id: int,\n    ):\n        \"\"\"\n        Download files from Google Drive.\n\n        Args:\n            credentials (dict): The credentials for accessing Google Drive.\n            files (list): The list of file metadata to download.\n\n        Returns:\n            dict: A dictionary containing the status of the download or an error message.\n        \"\"\"\n        logger.info(\"Downloading Google Drive files with metadata: %s\", files)\n        creds = Credentials.from_authorized_user_info(credentials)\n        if creds.expired and creds.refresh_token:\n            creds.refresh(GoogleRequest())\n            logger.info(\"Google Drive credentials refreshed\")\n            # Updating the credentials in the database\n\n        service = build(\"drive\", \"v3\", credentials=creds)\n        downloaded_files = []\n        for file in files:\n            logger.info(\"\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25: %s\", file)\n            try:\n                file_id = file[\"id\"]\n                file_name = file[\"name\"]\n                mime_type = file[\"mime_type\"]\n                modified_time = file[\"last_modified\"]\n                # Convert Google Docs files to appropriate formats before downloading\n                if mime_type == \"application/vnd.google-apps.document\":\n                    logger.debug(\n                        \"Converting Google Docs file with file_id: %s to DOCX.\",\n                        file_id,\n                    )\n                    request = service.files().export_media(\n                        fileId=file_id,\n                        mimeType=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n                    )\n                    file_name += \".docx\"\n                elif mime_type == \"application/vnd.google-apps.spreadsheet\":\n                    logger.debug(\n                        \"Converting Google Sheets file with file_id: %s to XLSX.\",\n                        file_id,\n                    )\n                    request = service.files().export_media(\n                        fileId=file_id,\n                        mimeType=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n                    )\n                    file_name += \".xlsx\"\n                elif mime_type == \"application/vnd.google-apps.presentation\":\n                    logger.debug(\n                        \"Converting Google Slides file with file_id: %s to PPTX.\",\n                        file_id,\n                    )\n                    request = service.files().export_media(\n                        fileId=file_id,\n                        mimeType=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n                    )\n                    file_name += \".pptx\"\n                ### Elif pdf, txt, md, csv, docx, xlsx, pptx, doc\n                elif file_name.split(\".\")[-1] in [\n                    \"pdf\",\n                    \"txt\",\n                    \"md\",\n                    \"csv\",\n                    \"docx\",\n                    \"xlsx\",\n                    \"pptx\",\n                    \"doc\",\n                ]:\n                    request = service.files().get_media(fileId=file_id)\n                else:\n                    logger.warning(\n                        \"Skipping unsupported file type: %s for file_id: %s\",\n                        mime_type,\n                        file_id,\n                    )\n                    continue\n\n                file_data = request.execute()\n\n                # Check if the file already exists in the storage\n                if check_file_exists(brain_id, file_name):\n                    logger.debug(\"\ud83d\udd25 File already exists in the storage: %s\", file_name)\n                    self.storage.remove_file(brain_id + \"/\" + file_name)\n                    BrainsVectors().delete_file_from_brain(brain_id, file_name)\n                   \n\n                to_upload_file = UploadFile(\n                    file=BytesIO(file_data),\n                    filename=file_name,\n                )\n\n                # Check if the file already exists in the database\n                existing_files = self.sync_files_repo.get_sync_files(sync_active_id)\n                existing_file = next(\n                    (f for f in existing_files if f.path == file_name), None\n                )\n                supported = False\n                if (existing_file and existing_file.supported) or not existing_file:\n                    supported = True\n                    await upload_file(to_upload_file, brain_id, current_user)  # type: ignore\n\n                if existing_file:\n                    # Update the existing file record\n                    self.sync_files_repo.update_sync_file(\n                        existing_file.id,\n                        SyncFileUpdateInput(\n                            last_modified=modified_time,\n                            supported=supported,\n                        ),\n                    )\n                else:\n                    # Create a new file record\n                    self.sync_files_repo.create_sync_file(\n                        SyncFileInput(\n                            path=file_name,\n                            syncs_active_id=sync_active_id,\n                            last_modified=modified_time,\n                            brain_id=brain_id,\n                            supported=supported,\n                        )\n                    )\n\n                    downloaded_files.append(file_name)\n            except Exception as error:\n                logger.error(\n                    \"An error occurred while downloading Google Drive files: %s\",\n                    error,\n                )\n                # Check if the file already exists in the database\n                existing_files = self.sync_files_repo.get_sync_files(sync_active_id)\n                existing_file = next(\n                    (f for f in existing_files if f.path == file[\"name\"]), None\n                )\n                # Update the existing file record\n                if existing_file:\n                    self.sync_files_repo.update_sync_file(\n                        existing_file.id,\n                        SyncFileUpdateInput(\n                            supported=False,\n                        ),\n                    )\n                else:\n                    # Create a new file record\n                    self.sync_files_repo.create_sync_file(\n                        SyncFileInput(\n                            path=file[\"name\"],\n                            syncs_active_id=sync_active_id,\n                            last_modified=file[\"last_modified\"],\n                            brain_id=brain_id,\n                            supported=False,\n                        )\n                    )\n        return {\"downloaded_files\": downloaded_files}\n\n    async def sync(self, sync_active_id: int, user_id: str):\n        \"\"\"\n        Check if the Google sync has not been synced and download the folders and files based on the settings.\n\n        Args:\n            sync_active_id (int): The ID of the active sync.\n            user_id (str): The user ID associated with the active sync.\n        \"\"\"\n\n        # Retrieve the active sync details\n        sync_active = self.sync_active_service.get_details_sync_active(sync_active_id)\n        if not sync_active:\n            logger.warning(\n                \"No active sync found for sync_active_id: %s\", sync_active_id\n            )\n            return None\n\n        # Check if the sync is due\n        last_synced = sync_active.get(\"last_synced\")\n        force_sync = sync_active.get(\"force_sync\", False)\n        sync_interval_minutes = sync_active.get(\"sync_interval_minutes\", 0)\n        if last_synced and not force_sync:\n            last_synced_time = datetime.fromisoformat(last_synced).astimezone(\n                timezone.utc\n            )\n            current_time = datetime.now().astimezone()\n\n            # Debug logging to check the values\n            logger.debug(\"Last synced time (UTC): %s\", last_synced_time)\n            logger.debug(\"Current time (local timezone): %s\", current_time)\n\n            # Convert current_time to UTC for comparison\n            current_time_utc = current_time.astimezone(timezone.utc)\n            logger.debug(\"Current time (UTC): %s\", current_time_utc)\n            time_difference = current_time_utc - last_synced_time\n            if time_difference < timedelta(minutes=sync_interval_minutes):\n                logger.info(\n                    \"Google sync is not due for sync_active_id: %s\", sync_active_id\n                )\n                return None\n\n        # Retrieve the sync user details\n        sync_user = self.sync_user_service.get_syncs_user(\n            user_id=user_id, sync_user_id=sync_active[\"syncs_user_id\"]\n        )\n        if not sync_user:\n            logger.warning(\n                \"No sync user found for sync_active_id: %s, user_id: %s\",\n                sync_active_id,\n                user_id,\n            )\n            return None\n\n        sync_user = sync_user[0]\n        if sync_user[\"provider\"].lower() != \"google\":\n            logger.warning(\n                \"Sync provider is not Google for sync_active_id: %s\", sync_active_id\n            )\n            return None\n\n        # Download the folders and files from Google Drive\n        logger.info(\n            \"Downloading folders and files from Google Drive for sync_active_id: %s\",\n            sync_active_id,\n        )\n\n        settings = sync_active.get(\"settings\", {})\n        folders = settings.get(\"folders\", [])\n        files_to_download = settings.get(\"files\", [])\n        files = []\n        files_metadata = []\n        if len(folders) > 0:\n            files = []\n            for folder in folders:\n                files.extend(\n                    get_google_drive_files(\n                        sync_user[\"credentials\"],\n                        folder_id=folder,\n                        recursive=True,\n                    )\n                )\n        if len(files_to_download) > 0:\n            files_metadata = get_google_drive_files_by_id(\n                sync_user[\"credentials\"], files_to_download\n            )\n        files = files + files_metadata  # type: ignore\n        if \"error\" in files:\n            logger.error(\n                \"Failed to download files from Google Drive for sync_active_id: %s\",\n                sync_active_id,\n            )\n            return None\n\n        # Filter files that have been modified since the last sync\n        last_synced_time = datetime.fromisoformat(last_synced) if last_synced else None\n\n        files_to_download = [\n            file\n            for file in files\n            if not file[\"is_folder\"]\n            and (\n                (\n                    not last_synced_time\n                    or datetime.fromisoformat(file[\"last_modified\"]) > last_synced_time\n                )\n                or not check_file_exists(sync_active[\"brain_id\"], file[\"name\"])\n            )\n        ]\n\n        logger.error(files_to_download)\n\n        downloaded_files = await self._upload_files(\n            sync_user[\"credentials\"],\n            files_to_download,\n            user_id,\n            sync_active[\"brain_id\"],\n            sync_active_id,\n        )\n\n        # Update the last_synced timestamp\n        self.sync_active_service.update_sync_active(\n            sync_active_id,\n            SyncsActiveUpdateInput(\n                last_synced=datetime.now().astimezone().isoformat(),\n                force_sync=False,\n            ),\n        )\n        logger.info(\n            \"Google Drive sync completed for sync_active_id: %s\", sync_active_id\n        )\n        return downloaded_files\n\n\nimport asyncio\n\n\nasync def main():\n    sync_user_service = SyncUserService()\n    sync_active_service = SyncService()\n    sync_files_repo = SyncFiles()\n    storage = Storage()\n\n    google_sync_utils = GoogleSyncUtils(\n        sync_user_service=sync_user_service,\n        sync_active_service=sync_active_service,\n        sync_files_repo=sync_files_repo,\n        storage=storage,\n    )\n    await google_sync_utils.sync(2, \"39418e3b-0258-4452-af60-7acfcc1263ff\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", "backend/modules/sync/utils/list_files.py": "import os\nfrom typing import List\n\nimport msal\nimport requests\nfrom fastapi import HTTPException\nfrom google.auth.transport.requests import Request as GoogleRequest\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom modules.sync.utils.normalize import remove_special_characters\nfrom logger import get_logger\nfrom requests import HTTPError\n\nlogger = get_logger(__name__)\n\n\ndef get_google_drive_files_by_id(credentials: dict, file_ids: List[str]):\n    \"\"\"\n    Retrieve files from Google Drive by their IDs.\n\n    Args:\n        credentials (dict): The credentials for accessing Google Drive.\n        file_ids (list): The list of file IDs to retrieve.\n\n    Returns:\n        list: A list of dictionaries containing the metadata of each file or an error message.\n    \"\"\"\n    logger.info(\"Retrieving Google Drive files with file_ids: %s\", file_ids)\n    creds = Credentials.from_authorized_user_info(credentials)\n    if creds.expired and creds.refresh_token:\n        creds.refresh(GoogleRequest())\n        logger.info(\"Google Drive credentials refreshed\")\n\n    try:\n        service = build(\"drive\", \"v3\", credentials=creds)\n        files = []\n\n        for file_id in file_ids:\n            result = (\n                service.files()\n                .get(fileId=file_id, fields=\"id, name, mimeType, modifiedTime\")\n                .execute()\n            )\n\n            files.append(\n                {\n                    \"name\": result[\"name\"],\n                    \"id\": result[\"id\"],\n                    \"is_folder\": result[\"mimeType\"]\n                    == \"application/vnd.google-apps.folder\",\n                    \"last_modified\": result[\"modifiedTime\"],\n                    \"mime_type\": result[\"mimeType\"],\n                }\n            )\n\n        logger.info(\"Google Drive files retrieved successfully: %s\", len(files))\n        for file in files:\n            file[\"name\"] = remove_special_characters(file[\"name\"])\n        return files\n    except HTTPError as error:\n        logger.error(\"An error occurred while retrieving Google Drive files: %s\", error)\n        return {\"error\": f\"An error occurred: {error}\"}\n\n\ndef get_google_drive_files(\n    credentials: dict, folder_id: str = None, recursive: bool = False\n):\n    \"\"\"\n    Retrieve files from Google Drive.\n\n    Args:\n        credentials (dict): The credentials for accessing Google Drive.\n        folder_id (str, optional): The folder ID to filter files. Defaults to None.\n        recursive (bool, optional): If True, fetch files from all subfolders. Defaults to False.\n\n    Returns:\n        dict: A dictionary containing the list of files or an error message.\n    \"\"\"\n    logger.info(\"Retrieving Google Drive files with folder_id: %s\", folder_id)\n    creds = Credentials.from_authorized_user_info(credentials)\n    if creds.expired and creds.refresh_token:\n        creds.refresh(GoogleRequest())\n        logger.info(\"Google Drive credentials refreshed\")\n        # Updating the credentials in the database\n\n    try:\n        service = build(\"drive\", \"v3\", credentials=creds)\n        if folder_id:\n            query = f\"'{folder_id}' in parents\"\n        else:\n            query = \"'root' in parents or sharedWithMe\"\n        page_token = None\n        files = []\n\n        while True:\n            results = (\n                service.files()\n                .list(\n                    q=query,\n                    pageSize=100,\n                    fields=\"nextPageToken, files(id, name, mimeType, modifiedTime)\",\n                    pageToken=page_token,\n                )\n                .execute()\n            )\n            items = results.get(\"files\", [])\n\n            if not items:\n                logger.info(\"No files found in Google Drive\")\n                break\n\n            for item in items:\n                files.append(\n                    {\n                        \"name\": item[\"name\"],\n                        \"id\": item[\"id\"],\n                        \"is_folder\": item[\"mimeType\"]\n                        == \"application/vnd.google-apps.folder\",\n                        \"last_modified\": item[\"modifiedTime\"],\n                        \"mime_type\": item[\"mimeType\"],\n                    }\n                )\n\n                # If recursive is True and the item is a folder, get files from the folder\n                if item[\"name\"] == \"Monotype\":\n                    logger.warning(item)\n                if (\n                    recursive\n                    and item[\"mimeType\"] == \"application/vnd.google-apps.folder\"\n                ):\n                    logger.warning(\n                        \"Calling Recursive for folder: %s\",\n                        item[\"name\"],\n                    )\n                    files.extend(\n                        get_google_drive_files(credentials, item[\"id\"], recursive)\n                    )\n\n            page_token = results.get(\"nextPageToken\", None)\n            if page_token is None:\n                break\n\n        logger.info(\"Google Drive files retrieved successfully: %s\", len(files))\n        \n        for file in files:\n            file[\"name\"] = remove_special_characters(file[\"name\"])\n        return files\n    except HTTPError as error:\n        logger.error(\"An error occurred while retrieving Google Drive files: %s\", error)\n        return {\"error\": f\"An error occurred: {error}\"}\n\n\nCLIENT_ID = os.getenv(\"SHAREPOINT_CLIENT_ID\")\nAUTHORITY = \"https://login.microsoftonline.com/common\"\nBACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:5050\")\nREDIRECT_URI = f\"{BACKEND_URL}/sync/azure/oauth2callback\"\nSCOPE = [\n    \"https://graph.microsoft.com/Files.Read\",\n    \"https://graph.microsoft.com/User.Read\",\n    \"https://graph.microsoft.com/Sites.Read.All\",\n]\n\n\ndef get_azure_token_data(credentials):\n    if \"access_token\" not in credentials:\n        raise HTTPException(status_code=401, detail=\"Invalid token data\")\n    return credentials\n\n\ndef refresh_azure_token(credentials):\n    if \"refresh_token\" not in credentials:\n        raise HTTPException(status_code=401, detail=\"No refresh token available\")\n\n    client = msal.PublicClientApplication(CLIENT_ID, authority=AUTHORITY)\n    result = client.acquire_token_by_refresh_token(\n        credentials[\"refresh_token\"], scopes=SCOPE\n    )\n    if \"access_token\" not in result:\n        raise HTTPException(status_code=400, detail=\"Failed to refresh token\")\n\n    return result\n\n\ndef get_azure_headers(token_data):\n    return {\n        \"Authorization\": f\"Bearer {token_data['access_token']}\",\n        \"Accept\": \"application/json\",\n    }\n\n\ndef list_azure_files(credentials, folder_id=None, recursive=False):\n    def fetch_files(endpoint, headers):\n        response = requests.get(endpoint, headers=headers)\n        if response.status_code == 401:\n            token_data = refresh_azure_token(credentials)\n            headers = get_azure_headers(token_data)\n            response = requests.get(endpoint, headers=headers)\n        if response.status_code != 200:\n            return {\"error\": response.text}\n        return response.json().get(\"value\", [])\n\n    token_data = get_azure_token_data(credentials)\n    headers = get_azure_headers(token_data)\n    endpoint = f\"https://graph.microsoft.com/v1.0/me/drive/root/children\"\n    if folder_id:\n        endpoint = (\n            f\"https://graph.microsoft.com/v1.0/me/drive/items/{folder_id}/children\"\n        )\n\n    items = fetch_files(endpoint, headers)\n\n    if not items:\n        logger.info(\"No files found in Azure Drive\")\n        return []\n\n    files = []\n    for item in items:\n        file_data = {\n            \"name\": item[\"name\"],\n            \"id\": item[\"id\"],\n            \"is_folder\": \"folder\" in item,\n            \"last_modified\": item[\"lastModifiedDateTime\"],\n            \"mime_type\": item.get(\"file\", {}).get(\"mimeType\", \"folder\"),\n        }\n        files.append(file_data)\n\n        # If recursive option is enabled and the item is a folder, fetch files from it\n        if recursive and file_data[\"is_folder\"]:\n            folder_files = list_azure_files(\n                credentials, folder_id=file_data[\"id\"], recursive=True\n            )\n\n            files.extend(folder_files)\n    for file in files:\n        file[\"name\"] = remove_special_characters(file[\"name\"])\n    logger.info(\"Azure Drive files retrieved successfully: %s\", len(files))\n    return files\n\n\ndef get_azure_files_by_id(credentials: dict, file_ids: List[str]):\n    \"\"\"\n    Retrieve files from Azure Drive by their IDs.\n\n    Args:\n        credentials (dict): The credentials for accessing Azure Drive.\n        file_ids (list): The list of file IDs to retrieve.\n\n    Returns:\n        list: A list of dictionaries containing the metadata of each file or an error message.\n    \"\"\"\n    logger.info(\"Retrieving Azure Drive files with file_ids: %s\", file_ids)\n    token_data = get_azure_token_data(credentials)\n    headers = get_azure_headers(token_data)\n    files = []\n\n    for file_id in file_ids:\n        endpoint = f\"https://graph.microsoft.com/v1.0/me/drive/items/{file_id}\"\n        response = requests.get(endpoint, headers=headers)\n        if response.status_code == 401:\n            token_data = refresh_azure_token(credentials)\n            headers = get_azure_headers(token_data)\n            response = requests.get(endpoint, headers=headers)\n        if response.status_code != 200:\n            logger.error(\n                \"An error occurred while retrieving Azure Drive files: %s\",\n                response.text,\n            )\n            return {\"error\": response.text}\n\n        result = response.json()\n        files.append(\n            {\n                \"name\": result[\"name\"],\n                \"id\": result[\"id\"],\n                \"is_folder\": \"folder\" in result,\n                \"last_modified\": result[\"lastModifiedDateTime\"],\n                \"mime_type\": result.get(\"file\", {}).get(\"mimeType\", \"folder\"),\n            }\n        )\n    \n    for file in files:\n        file[\"name\"] = remove_special_characters(file[\"name\"])\n    logger.info(\"Azure Drive files retrieved successfully: %s\", len(files))\n    return files\n", "backend/modules/sync/utils/upload.py": "import os\nfrom uuid import UUID\n\nfrom celery_worker import process_file_and_notify\nfrom fastapi import HTTPException, UploadFile\nfrom modules.brain.entity.brain_entity import RoleEnum\nfrom modules.brain.service.brain_authorization_service import (\n    validate_brain_authorization,\n)\nfrom modules.knowledge.dto.inputs import CreateKnowledgeProperties\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.notification.dto.inputs import (\n    CreateNotification,\n    NotificationUpdatableProperties,\n)\nfrom modules.notification.entity.notification import NotificationsStatusEnum\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.upload.service.upload_file import upload_file_storage\nfrom modules.user.service.user_usage import UserUsage\nfrom packages.files.file import convert_bytes, get_file_size\nfrom packages.utils.telemetry import maybe_send_telemetry\n\nknowledge_service = KnowledgeService()\nnotification_service = NotificationService()\n\n\nasync def upload_file(\n    upload_file: UploadFile,\n    brain_id: UUID,\n    current_user: str,\n):\n    validate_brain_authorization(\n        brain_id, current_user, [RoleEnum.Editor, RoleEnum.Owner]\n    )\n    user_daily_usage = UserUsage(\n        id=current_user,\n    )\n    upload_notification = notification_service.add_notification(\n        CreateNotification(\n            user_id=current_user,\n            status=NotificationsStatusEnum.INFO,\n            title=f\"Processing File {upload_file.filename}\",\n        )\n    )\n\n    user_settings = user_daily_usage.get_user_settings()\n\n    remaining_free_space = user_settings.get(\"max_brain_size\", 1000000000)\n    maybe_send_telemetry(\"upload_file\", {\"file_name\": upload_file.filename})\n    file_size = get_file_size(upload_file)\n    if remaining_free_space - file_size < 0:\n        message = f\"Brain will exceed maximum capacity. Maximum file allowed is : {convert_bytes(remaining_free_space)}\"\n        raise HTTPException(status_code=403, detail=message)\n\n    file_content = await upload_file.read()\n\n    filename_with_brain_id = str(brain_id) + \"/\" + str(upload_file.filename)\n\n    try:\n        file_in_storage = upload_file_storage(file_content, filename_with_brain_id)\n\n    except Exception as e:\n        print(e)\n\n        notification_service.update_notification_by_id(\n            upload_notification.id if upload_notification else None,\n            NotificationUpdatableProperties(\n                status=NotificationsStatusEnum.ERROR,\n                description=f\"There was an error uploading the file: {e}\",\n            ),\n        )\n        if \"The resource already exists\" in str(e):\n            raise HTTPException(\n                status_code=403,\n                detail=f\"File {upload_file.filename} already exists in storage.\",\n            )\n        else:\n            raise HTTPException(\n                status_code=500, detail=f\"Failed to upload file to storage. {e}\"\n            )\n\n    knowledge_to_add = CreateKnowledgeProperties(\n        brain_id=brain_id,\n        file_name=upload_file.filename,\n        extension=os.path.splitext(\n            upload_file.filename  # pyright: ignore reportPrivateUsage=none\n        )[-1].lower(),\n    )\n\n    added_knowledge = knowledge_service.add_knowledge(knowledge_to_add)\n\n    process_file_and_notify.delay(\n        file_name=filename_with_brain_id,\n        file_original_name=upload_file.filename,\n        brain_id=brain_id,\n        notification_id=upload_notification.id,\n    )\n    return {\"message\": \"File processing has started.\"}\n", "backend/modules/sync/utils/sharepointutils.py": "import os\nfrom datetime import datetime, timedelta, timezone\nfrom io import BytesIO\n\nimport msal\nimport requests\nfrom fastapi import HTTPException, UploadFile\nfrom logger import get_logger\nfrom modules.brain.repository.brains_vectors import BrainsVectors\nfrom modules.knowledge.repository.storage import Storage\nfrom modules.sync.dto.inputs import (\n    SyncFileInput,\n    SyncFileUpdateInput,\n    SyncsActiveUpdateInput,\n)\nfrom modules.sync.repository.sync_files import SyncFiles\nfrom modules.sync.service.sync_service import SyncService, SyncUserService\nfrom modules.sync.utils.list_files import get_azure_files_by_id, list_azure_files\nfrom modules.sync.utils.upload import upload_file\nfrom modules.upload.service.upload_file import check_file_exists\nfrom pydantic import BaseModel, ConfigDict\n\nlogger = get_logger(__name__)\n\nCLIENT_ID = os.getenv(\"SHAREPOINT_CLIENT_ID\")\nAUTHORITY = \"https://login.microsoftonline.com/common\"\nBACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:5050\")\nREDIRECT_URI = f\"{BACKEND_URL}/sync/azure/oauth2callback\"\nSCOPE = [\n    \"https://graph.microsoft.com/Files.Read\",\n    \"https://graph.microsoft.com/User.Read\",\n    \"https://graph.microsoft.com/Sites.Read.All\",\n]\n\n\nclass AzureSyncUtils(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    sync_user_service: SyncUserService\n    sync_active_service: SyncService\n    sync_files_repo: SyncFiles\n    storage: Storage\n\n    def get_headers(self, token_data):\n        return {\n            \"Authorization\": f\"Bearer {token_data['access_token']}\",\n            \"Accept\": \"application/json\",\n        }\n\n    def refresh_token(self, refresh_token):\n        client = msal.PublicClientApplication(CLIENT_ID, authority=AUTHORITY)\n        result = client.acquire_token_by_refresh_token(refresh_token, scopes=SCOPE)\n        if \"access_token\" not in result:\n            raise HTTPException(status_code=400, detail=\"Failed to refresh token\")\n        return result\n\n    async def _upload_files(\n        self,\n        token_data: dict,\n        files: list,\n        current_user: str,\n        brain_id: str,\n        sync_active_id: int,\n    ):\n        \"\"\"\n        Download files from Azure.\n\n        Args:\n            token_data (dict): The token data for accessing Azure.\n            files (list): The list of file metadata to download.\n\n        Returns:\n            dict: A dictionary containing the status of the download or an error message.\n        \"\"\"\n        logger.info(\"Downloading Azure files with metadata: %s\", files)\n        headers = self.get_headers(token_data)\n\n        downloaded_files = []\n        for file in files:\n            try:\n                file_id = file[\"id\"]\n                file_name = file[\"name\"]\n                modified_time = file[\"last_modified\"]\n\n                download_endpoint = (\n                    f\"https://graph.microsoft.com/v1.0/me/drive/items/{file_id}/content\"\n                )\n                logger.info(\"Downloading file: %s\", file_name)\n                download_response = requests.get(\n                    download_endpoint, headers=headers, stream=True\n                )\n                if download_response.status_code == 401:\n                    token_data = self.refresh_token(token_data[\"refresh_token\"])\n                    headers = self.get_headers(token_data)\n                    download_response = requests.get(\n                        download_endpoint, headers=headers, stream=True\n                    )\n                if download_response.status_code != 200:\n                    logger.error(\"Failed to download file: %s\", file_name)\n                    continue\n\n                file_data = BytesIO(download_response.content)\n\n                # Check if the file already exists in the storage\n                if check_file_exists(brain_id, file_name):\n                    logger.debug(\"\ud83d\udd25 File already exists in the storage: %s\", file_name)\n\n                    self.storage.remove_file(brain_id + \"/\" + file_name)\n                    BrainsVectors().delete_file_from_brain(brain_id, file_name)\n\n                # Check if the file extension is compatible\n                if file_name.split(\".\")[-1] not in [\n                    \"pdf\",\n                    \"txt\",\n                    \"md\",\n                    \"csv\",\n                    \"docx\",\n                    \"xlsx\",\n                    \"pptx\",\n                    \"doc\",\n                ]:\n                    logger.info(\"File is not compatible: %s\", file_name)\n                    continue\n\n                to_upload_file = UploadFile(\n                    file=file_data,\n                    filename=file_name,\n                )\n\n                # Check if the file already exists in the database\n                existing_files = self.sync_files_repo.get_sync_files(sync_active_id)\n                existing_file = next(\n                    (f for f in existing_files if f.path == file_name), None\n                )\n\n                supported = False\n                if (existing_file and existing_file.supported) or not existing_file:\n                    supported = True\n                    await upload_file(to_upload_file, brain_id, current_user)\n\n                if existing_file:\n                    # Update the existing file record\n                    self.sync_files_repo.update_sync_file(\n                        existing_file.id,\n                        SyncFileUpdateInput(\n                            last_modified=modified_time,\n                            supported=supported,\n                        ),\n                    )\n                else:\n                    # Create a new file record\n                    self.sync_files_repo.create_sync_file(\n                        SyncFileInput(\n                            path=file_name,\n                            syncs_active_id=sync_active_id,\n                            last_modified=modified_time,\n                            brain_id=brain_id,\n                            supported=supported,\n                        )\n                    )\n\n                downloaded_files.append(file_name)\n            except Exception as error:\n                logger.error(\n                    \"An error occurred while downloading Azure files: %s\", error\n                )\n                # Check if the file already exists in the database\n                existing_files = self.sync_files_repo.get_sync_files(sync_active_id)\n                existing_file = next(\n                    (f for f in existing_files if f.path == file[\"name\"]), None\n                )\n                # Update the existing file record\n                if existing_file:\n                    self.sync_files_repo.update_sync_file(\n                        existing_file.id,\n                        SyncFileUpdateInput(\n                            supported=False,\n                        ),\n                    )\n                else:\n                    # Create a new file record\n                    self.sync_files_repo.create_sync_file(\n                        SyncFileInput(\n                            path=file[\"name\"],\n                            syncs_active_id=sync_active_id,\n                            last_modified=file[\"last_modified\"],\n                            brain_id=brain_id,\n                            supported=False,\n                        )\n                    )\n        return {\"downloaded_files\": downloaded_files}\n\n    async def sync(self, sync_active_id: int, user_id: str):\n        \"\"\"\n        Check if the Azure sync has not been synced and download the folders and files based on the settings.\n\n        Args:\n            sync_active_id (int): The ID of the active sync.\n            user_id (str): The user ID associated with the active sync.\n        \"\"\"\n\n        # Retrieve the active sync details\n        sync_active = self.sync_active_service.get_details_sync_active(sync_active_id)\n        if not sync_active:\n            logger.warning(\n                \"No active sync found for sync_active_id: %s\", sync_active_id\n            )\n            return None\n\n        # Check if the sync is due\n        last_synced = sync_active.get(\"last_synced\")\n        force_sync = sync_active.get(\"force_sync\", False)\n        sync_interval_minutes = sync_active.get(\"sync_interval_minutes\", 0)\n        if last_synced and not force_sync:\n            last_synced_time = datetime.fromisoformat(last_synced).astimezone(\n                timezone.utc\n            )\n            current_time = datetime.now().astimezone()\n\n            # Debug logging to check the values\n            logger.debug(\"Last synced time (UTC): %s\", last_synced_time)\n            logger.debug(\"Current time (local timezone): %s\", current_time)\n\n            # Convert current_time to UTC for comparison\n            current_time_utc = current_time.astimezone(timezone.utc)\n            logger.debug(\"Current time (UTC): %s\", current_time_utc)\n            time_difference = current_time_utc - last_synced_time\n            if time_difference < timedelta(minutes=sync_interval_minutes):\n                logger.info(\n                    \"Azure sync is not due for sync_active_id: %s\", sync_active_id\n                )\n                return None\n\n        # Retrieve the sync user details\n        sync_user = self.sync_user_service.get_syncs_user(\n            user_id=user_id, sync_user_id=sync_active[\"syncs_user_id\"]\n        )\n        if not sync_user:\n            logger.warning(\n                \"No sync user found for sync_active_id: %s, user_id: %s\",\n                sync_active_id,\n                user_id,\n            )\n            return None\n\n        sync_user = sync_user[0]\n        if sync_user[\"provider\"].lower() != \"azure\":\n            logger.warning(\n                \"Sync provider is not Azure for sync_active_id: %s\", sync_active_id\n            )\n            return None\n\n        # Download the folders and files from Azure\n        logger.info(\n            \"Downloading folders and files from Azure for sync_active_id: %s\",\n            sync_active_id,\n        )\n\n        # Get the folder id from the settings from sync_active\n        settings = sync_active.get(\"settings\", {})\n        folders = settings.get(\"folders\", [])\n        files_to_download = settings.get(\"files\", [])\n        files = []\n        files_metadata = []\n        if len(folders) > 0:\n            files = []\n            for folder in folders:\n                files.extend(\n                    list_azure_files(\n                        sync_user[\"credentials\"],\n                        folder_id=folder,\n                        recursive=True,\n                    )\n                )\n        if len(files_to_download) > 0:\n            files_metadata = get_azure_files_by_id(\n                sync_user[\"credentials\"],\n                files_to_download,\n            )\n        files = files + files_metadata  # type: ignore\n\n        if \"error\" in files:\n            logger.error(\n                \"Failed to download files from Azure for sync_active_id: %s\",\n                sync_active_id,\n            )\n            return None\n\n        # Filter files that have been modified since the last sync\n        last_synced_time = (\n            datetime.fromisoformat(last_synced).astimezone(timezone.utc)\n            if last_synced\n            else None\n        )\n        logger.info(\"Files retrieved from Azure: %s\", len(files))\n        logger.info(\"Files retrieved from Azure: %s\", files)\n        files_to_download = [\n            file\n            for file in files\n            if not file[\"is_folder\"]\n            and (\n                (\n                    not last_synced_time\n                    or datetime.strptime(\n                        file[\"last_modified\"], \"%Y-%m-%dT%H:%M:%SZ\"\n                    ).replace(tzinfo=timezone.utc)\n                    > last_synced_time\n                )\n                or not check_file_exists(sync_active[\"brain_id\"], file[\"name\"])\n            )\n        ]\n\n        downloaded_files = await self._upload_files(\n            sync_user[\"credentials\"],\n            files_to_download,\n            user_id,\n            sync_active[\"brain_id\"],\n            sync_active_id,\n        )\n        if \"error\" in downloaded_files:\n            logger.error(\n                \"Failed to download files from Azure for sync_active_id: %s\",\n                sync_active_id,\n            )\n            return None\n\n        # Update the last_synced timestamp\n        self.sync_active_service.update_sync_active(\n            sync_active_id,\n            SyncsActiveUpdateInput(\n                last_synced=datetime.now().astimezone().isoformat(), force_sync=False\n            ),\n        )\n        logger.info(\"Azure sync completed for sync_active_id: %s\", sync_active_id)\n        return downloaded_files\n\n\nimport asyncio\n\n\nasync def main():\n    sync_user_service = SyncUserService()\n    sync_active_service = SyncService()\n    sync_files_repo = SyncFiles()\n    storage = Storage()\n\n    azure_sync_utils = AzureSyncUtils(\n        sync_user_service=sync_user_service,\n        sync_active_service=sync_active_service,\n        sync_files_repo=sync_files_repo,\n        storage=storage,\n    )\n    await azure_sync_utils.sync(3, \"39418e3b-0258-4452-af60-7acfcc1263ff\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", "backend/modules/sync/utils/normalize.py": "import unicodedata\nimport re\nfrom logger import get_logger\n\nlogger = get_logger(__name__)\n\ndef remove_special_characters(input):\n    try:\n        normalized_string = unicodedata.normalize('NFD', input)\n        normalized_string = re.sub(r'[^\\w\\s.]', '', normalized_string)\n        logger.info(f\"Input: {input}, Normalized: {normalized_string}\")\n        return normalized_string\n    except Exception as e:\n        logger.error(f\"Error removing special characters: {e}\")\n        return input", "backend/modules/sync/utils/__init__.py": "", "backend/modules/sync/repository/sync_interfaces.py": "from abc import ABC, abstractmethod\nfrom typing import List\nfrom uuid import UUID\n\nfrom modules.sync.dto.inputs import (\n    SyncFileInput,\n    SyncFileUpdateInput,\n    SyncsActiveInput,\n    SyncsActiveUpdateInput,\n    SyncsUserInput,\n    SyncUserUpdateInput,\n)\nfrom modules.sync.entity.sync import SyncsActive, SyncsFiles\n\n\nclass SyncUserInterface(ABC):\n    @abstractmethod\n    def create_sync_user(\n        self,\n        sync_user_input: SyncsUserInput,\n    ):\n        pass\n\n    @abstractmethod\n    def get_syncs_user(self, user_id: str, sync_user_id: int = None):\n        pass\n\n    @abstractmethod\n    def get_sync_user_by_id(self, sync_id: int):\n        pass\n\n    @abstractmethod\n    def delete_sync_user(self, sync_user_id: UUID, user_id: UUID):\n        pass\n\n    @abstractmethod\n    def get_sync_user_by_state(self, state: dict):\n        pass\n\n    @abstractmethod\n    def update_sync_user(\n        self, sync_user_id: str, state: dict, sync_user_input: SyncUserUpdateInput\n    ):\n        pass\n\n    @abstractmethod\n    def get_files_folder_user_sync(\n        self,\n        sync_active_id: int,\n        user_id: str,\n        folder_id: int = None,\n        recursive: bool = False,\n    ):\n        pass\n\n\nclass SyncInterface(ABC):\n\n    @abstractmethod\n    def create_sync_active(\n        self,\n        sync_active_input: SyncsActiveInput,\n        user_id: str,\n    ) -> SyncsActive:\n        pass\n\n    @abstractmethod\n    def get_syncs_active(self, user_id: UUID) -> list[SyncsActive]:\n        pass\n\n    @abstractmethod\n    def update_sync_active(\n        self, sync_id: UUID, sync_active_input: SyncsActiveUpdateInput\n    ):\n        pass\n\n    @abstractmethod\n    def delete_sync_active(self, sync_active_id: int, user_id: str):\n        pass\n\n    @abstractmethod\n    def get_details_sync_active(self, sync_active_id: int):\n        pass\n\n    @abstractmethod\n    async def get_syncs_active_in_interval(self) -> List[SyncsActive]:\n        pass\n\n\nclass SyncFileInterface(ABC):\n    @abstractmethod\n    def create_sync_file(self, sync_file_input: SyncFileInput) -> SyncsFiles:\n        pass\n\n    @abstractmethod\n    def get_sync_files(self, sync_active_id: int) -> list[SyncsFiles]:\n        pass\n\n    @abstractmethod\n    def update_sync_file(self, sync_file_id: int, sync_file_input: SyncFileUpdateInput):\n        pass\n\n    @abstractmethod\n    def delete_sync_file(self, sync_file_id: int):\n        pass\n", "backend/modules/sync/repository/sync.py": "from datetime import datetime, timedelta\nfrom typing import List\n\nfrom logger import get_logger\nfrom models.settings import get_supabase_client\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.sync.dto.inputs import SyncsActiveInput, SyncsActiveUpdateInput\nfrom modules.sync.entity.sync import SyncsActive\nfrom modules.sync.repository.sync_interfaces import SyncInterface\n\nnotification_service = NotificationService()\nknowledge_service = KnowledgeService()\nlogger = get_logger(__name__)\n\n\nclass Sync(SyncInterface):\n    def __init__(self):\n        \"\"\"\n        Initialize the Sync class with a Supabase client.\n        \"\"\"\n        supabase_client = get_supabase_client()\n        self.db = supabase_client  # type: ignore\n        logger.debug(\"Supabase client initialized\")\n\n    def create_sync_active(\n        self, sync_active_input: SyncsActiveInput, user_id: str\n    ) -> SyncsActive:\n        \"\"\"\n        Create a new active sync in the database.\n\n        Args:\n            sync_active_input (SyncsActiveInput): The input data for creating an active sync.\n            user_id (str): The user ID associated with the active sync.\n\n        Returns:\n            SyncsActive or None: The created active sync data or None if creation failed.\n        \"\"\"\n        logger.info(\n            \"Creating active sync for user_id: %s with input: %s\",\n            user_id,\n            sync_active_input,\n        )\n        sync_active_input_dict = sync_active_input.model_dump()\n        sync_active_input_dict[\"user_id\"] = user_id\n        response = (\n            self.db.from_(\"syncs_active\").insert(sync_active_input_dict).execute()\n        )\n        if response.data:\n            logger.info(\"Active sync created successfully: %s\", response.data[0])\n            return SyncsActive(**response.data[0])\n        logger.warning(\"Failed to create active sync for user_id: %s\", user_id)\n        return None\n\n    def get_syncs_active(self, user_id: str) -> List[SyncsActive]:\n        \"\"\"\n        Retrieve active syncs from the database.\n\n        Args:\n            user_id (str): The user ID to filter active syncs.\n\n        Returns:\n            List[SyncsActive]: A list of active syncs matching the criteria.\n        \"\"\"\n        logger.info(\"Retrieving active syncs for user_id: %s\", user_id)\n        response = (\n            self.db.from_(\"syncs_active\")\n            .select(\"*, syncs_user(*)\")\n            .eq(\"user_id\", user_id)\n            .execute()\n        )\n        if response.data:\n            logger.info(\"Active syncs retrieved successfully: %s\", response.data)\n            return [SyncsActive(**sync) for sync in response.data]\n        logger.warning(\"No active syncs found for user_id: %s\", user_id)\n        return []\n\n    def update_sync_active(\n        self, sync_id: int, sync_active_input: SyncsActiveUpdateInput\n    ):\n        \"\"\"\n        Update an active sync in the database.\n\n        Args:\n            sync_id (int): The ID of the active sync.\n            sync_active_input (SyncsActiveUpdateInput): The input data for updating the active sync.\n\n        Returns:\n            dict or None: The updated active sync data or None if update failed.\n        \"\"\"\n        logger.info(\n            \"Updating active sync with sync_id: %s, input: %s\",\n            sync_id,\n            sync_active_input,\n        )\n\n        response = (\n            self.db.from_(\"syncs_active\")\n            .update(sync_active_input.model_dump(exclude_unset=True))\n            .eq(\"id\", sync_id)\n            .execute()\n        )\n        if response.data:\n            logger.info(\"Active sync updated successfully: %s\", response.data[0])\n            return response.data[0]\n        logger.warning(\"Failed to update active sync with sync_id: %s\", sync_id)\n        return None\n\n    def delete_sync_active(self, sync_active_id: int, user_id: str):\n        \"\"\"\n        Delete an active sync from the database.\n\n        Args:\n            sync_active_id (int): The ID of the active sync.\n            user_id (str): The user ID associated with the active sync.\n\n        Returns:\n            dict or None: The deleted active sync data or None if deletion failed.\n        \"\"\"\n        logger.info(\n            \"Deleting active sync with sync_active_id: %s, user_id: %s\",\n            sync_active_id,\n            user_id,\n        )\n        response = (\n            self.db.from_(\"syncs_active\")\n            .delete()\n            .eq(\"id\", sync_active_id)\n            .eq(\"user_id\", user_id)\n            .execute()\n        )\n        if response.data:\n            logger.info(\"Active sync deleted successfully: %s\", response.data[0])\n            return response.data[0]\n        logger.warning(\n            \"Failed to delete active sync with sync_active_id: %s, user_id: %s\",\n            sync_active_id,\n            user_id,\n        )\n        return None\n\n    def get_details_sync_active(self, sync_active_id: int):\n        \"\"\"\n        Retrieve details of an active sync, including associated sync user data.\n\n        Args:\n            sync_active_id (int): The ID of the active sync.\n\n        Returns:\n            dict or None: The detailed active sync data or None if not found.\n        \"\"\"\n        logger.info(\n            \"Retrieving details for active sync with sync_active_id: %s\", sync_active_id\n        )\n        response = (\n            self.db.table(\"syncs_active\")\n            .select(\"*, syncs_user(provider, credentials)\")\n            .eq(\"id\", sync_active_id)\n            .execute()\n        )\n        if response.data:\n            logger.info(\n                \"Details for active sync retrieved successfully: %s\", response.data[0]\n            )\n            return response.data[0]\n        logger.warning(\n            \"No details found for active sync with sync_active_id: %s\", sync_active_id\n        )\n        return None\n\n    async def get_syncs_active_in_interval(self) -> List[SyncsActive]:\n        \"\"\"\n        Retrieve active syncs that are due for synchronization based on their interval.\n\n        Returns:\n            list: A list of active syncs that are due for synchronization.\n        \"\"\"\n        logger.info(\"Retrieving active syncs due for synchronization\")\n\n        current_time = datetime.now()\n\n        # The Query filters the active syncs based on the sync_interval_minutes field and last_synced timestamp\n        response = (\n            self.db.table(\"syncs_active\")\n            .select(\"*\")\n            .lt(\"last_synced\", (current_time - timedelta(minutes=360)).isoformat())\n            .execute()\n        )\n\n        force_sync = (\n            self.db.table(\"syncs_active\").select(\"*\").eq(\"force_sync\", True).execute()\n        )\n        merge_data = response.data + force_sync.data\n        if merge_data:\n            logger.info(\"Active syncs retrieved successfully: %s\", merge_data)\n            return [SyncsActive(**sync) for sync in merge_data]\n        logger.info(\"No active syncs found due for synchronization\")\n        return []\n", "backend/modules/sync/repository/sync_user.py": "import json\n\nfrom logger import get_logger\nfrom models.settings import get_supabase_client\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.notification.service.notification_service import NotificationService\nfrom modules.sync.dto.inputs import SyncsUserInput, SyncUserUpdateInput\nfrom modules.sync.repository.sync_interfaces import SyncUserInterface\nfrom modules.sync.utils.list_files import get_google_drive_files, list_azure_files\n\nnotification_service = NotificationService()\nknowledge_service = KnowledgeService()\nlogger = get_logger(__name__)\n\n\nclass SyncUser(SyncUserInterface):\n    def __init__(self):\n        \"\"\"\n        Initialize the Sync class with a Supabase client.\n        \"\"\"\n        supabase_client = get_supabase_client()\n        self.db = supabase_client  # type: ignore\n        logger.debug(\"Supabase client initialized\")\n\n    def create_sync_user(\n        self,\n        sync_user_input: SyncsUserInput,\n    ):\n        \"\"\"\n        Create a new sync user in the database.\n\n        Args:\n            sync_user_input (SyncsUserInput): The input data for creating a sync user.\n\n        Returns:\n            dict or None: The created sync user data or None if creation failed.\n        \"\"\"\n        logger.info(\"Creating sync user with input: %s\", sync_user_input)\n        response = (\n            self.db.from_(\"syncs_user\")\n            .insert(\n                {\n                    \"user_id\": sync_user_input.user_id,\n                    \"provider\": sync_user_input.provider,\n                    \"credentials\": sync_user_input.credentials,\n                    \"state\": sync_user_input.state,\n                    \"name\": sync_user_input.name,\n                }\n            )\n            .execute()\n        )\n        if response.data:\n            logger.info(\"Sync user created successfully: %s\", response.data[0])\n            return response.data[0]\n        logger.warning(\"Failed to create sync user\")\n        return None\n\n    def get_sync_user_by_id(self, sync_id: int):\n        \"\"\"\n        Retrieve sync users from the database.\n        \"\"\"\n        response = self.db.from_(\"syncs_user\").select(\"*\").eq(\"id\", sync_id).execute()\n        if response.data:\n            logger.info(\"Sync user found: %s\", response.data[0])\n            return response.data[0]\n        logger.warning(\"No sync user found for sync_id: %s\", sync_id)\n        return None\n\n    def get_syncs_user(self, user_id: str, sync_user_id: int = None):\n        \"\"\"\n        Retrieve sync users from the database.\n\n        Args:\n            user_id (str): The user ID to filter sync users.\n            sync_user_id (int, optional): The sync user ID to filter sync users. Defaults to None.\n\n        Returns:\n            list: A list of sync users matching the criteria.\n        \"\"\"\n        logger.info(\n            \"Retrieving sync users for user_id: %s, sync_user_id: %s\",\n            user_id,\n            sync_user_id,\n        )\n        query = self.db.from_(\"syncs_user\").select(\"*\").eq(\"user_id\", user_id)\n        if sync_user_id:\n            query = query.eq(\"id\", sync_user_id)\n        response = query.execute()\n        if response.data:\n            logger.info(\"Sync users retrieved successfully: %s\", response.data)\n            return response.data\n        logger.warning(\n            \"No sync users found for user_id: %s, sync_user_id: %s\",\n            user_id,\n            sync_user_id,\n        )\n        return []\n\n    def get_sync_user_by_state(self, state: dict):\n        \"\"\"\n        Retrieve a sync user by their state.\n\n        Args:\n            state (dict): The state to filter sync users.\n\n        Returns:\n            dict or None: The sync user data matching the state or None if not found.\n        \"\"\"\n        logger.info(\"Getting sync user by state: %s\", state)\n\n        state_str = json.dumps(state)\n        response = (\n            self.db.from_(\"syncs_user\").select(\"*\").eq(\"state\", state_str).execute()\n        )\n        if response.data:\n            logger.info(\"Sync user found by state: %s\", response.data[0])\n            return response.data[0]\n        logger.warning(\"No sync user found for state: %s\", state)\n        return []\n\n    def delete_sync_user(self, sync_id: str, user_id: str):\n        \"\"\"\n        Delete a sync user from the database.\n\n        Args:\n            provider (str): The provider of the sync user.\n            user_id (str): The user ID of the sync user.\n        \"\"\"\n        logger.info(\n            \"Deleting sync user with sync_id: %s, user_id: %s\", sync_id, user_id\n        )\n        self.db.from_(\"syncs_user\").delete().eq(\"id\", sync_id).eq(\n            \"user_id\", user_id\n        ).execute()\n        logger.info(\"Sync user deleted successfully\")\n\n    def update_sync_user(\n        self, sync_user_id: str, state: dict, sync_user_input: SyncUserUpdateInput\n    ):\n        \"\"\"\n        Update a sync user in the database.\n\n        Args:\n            sync_user_id (str): The user ID of the sync user.\n            state (dict): The state to filter sync users.\n            sync_user_input (SyncUserUpdateInput): The input data for updating the sync user.\n        \"\"\"\n        logger.info(\n            \"Updating sync user with user_id: %s, state: %s, input: %s\",\n            sync_user_id,\n            state,\n            sync_user_input,\n        )\n\n        state_str = json.dumps(state)\n        self.db.from_(\"syncs_user\").update(sync_user_input.model_dump()).eq(\n            \"user_id\", sync_user_id\n        ).eq(\"state\", state_str).execute()\n        logger.info(\"Sync user updated successfully\")\n\n    def get_files_folder_user_sync(\n        self, sync_active_id: int, user_id: str, folder_id: str = None, recursive: bool = False\n    ):\n        \"\"\"\n        Retrieve files from a user's sync folder, either from Google Drive or Azure.\n\n        Args:\n            sync_active_id (int): The ID of the active sync.\n            user_id (str): The user ID associated with the active sync.\n            folder_id (str, optional): The folder ID to filter files. Defaults to None.\n\n        Returns:\n            dict or str: A dictionary containing the list of files or a string indicating the sync provider.\n        \"\"\"\n        logger.info(\n            \"Retrieving files for user sync with sync_active_id: %s, user_id: %s, folder_id: %s\",\n            sync_active_id,\n            user_id,\n            folder_id,\n        )\n\n        # Check whether the sync is Google or Azure\n        sync_user = self.get_syncs_user(user_id=user_id, sync_user_id=sync_active_id)\n        if not sync_user:\n            logger.warning(\n                \"No sync user found for sync_active_id: %s, user_id: %s\",\n                sync_active_id,\n                user_id,\n            )\n            return None\n\n        sync_user = sync_user[0]\n        logger.info(\"Sync user found: %s\", sync_user)\n\n        provider = sync_user[\"provider\"].lower()\n        if provider == \"google\":\n            logger.info(\"Getting files for Google sync\")\n            return {\n                \"files\": get_google_drive_files(sync_user[\"credentials\"], folder_id)\n            }\n        elif provider == \"azure\":\n            logger.info(\"Getting files for Azure sync\")\n            return {\"files\": list_azure_files(sync_user[\"credentials\"], folder_id, recursive)}\n        else:\n            logger.warning(\"No sync found for provider: %s\", sync_user[\"provider\"], recursive)\n            return \"No sync found\"\n", "backend/modules/sync/repository/__init__.py": "", "backend/modules/sync/repository/sync_files.py": "from logger import get_logger\nfrom models.settings import get_supabase_client\nfrom modules.sync.dto.inputs import SyncFileInput, SyncFileUpdateInput\nfrom modules.sync.entity.sync import SyncsFiles\nfrom modules.sync.repository.sync_interfaces import SyncFileInterface\n\nlogger = get_logger(__name__)\n\n\nclass SyncFiles(SyncFileInterface):\n    def __init__(self):\n        \"\"\"\n        Initialize the SyncFiles class with a Supabase client.\n        \"\"\"\n        supabase_client = get_supabase_client()\n        self.db = supabase_client  # type: ignore\n        logger.debug(\"Supabase client initialized\")\n\n    def create_sync_file(self, sync_file_input: SyncFileInput) -> SyncsFiles:\n        \"\"\"\n        Create a new sync file in the database.\n\n        Args:\n            sync_file_input (SyncFileInput): The input data for creating a sync file.\n\n        Returns:\n            SyncsFiles: The created sync file data.\n        \"\"\"\n        logger.info(\"Creating sync file with input: %s\", sync_file_input)\n        response = (\n            self.db.from_(\"syncs_files\")\n            .insert(\n                {\n                    \"path\": sync_file_input.path,\n                    \"syncs_active_id\": sync_file_input.syncs_active_id,\n                    \"last_modified\": sync_file_input.last_modified,\n                    \"brain_id\": sync_file_input.brain_id,\n                }\n            )\n            .execute()\n        )\n        if response.data:\n            logger.info(\"Sync file created successfully: %s\", response.data[0])\n            return SyncsFiles(**response.data[0])\n        logger.warning(\"Failed to create sync file\")\n        return None\n\n    def get_sync_files(self, sync_active_id: int) -> list[SyncsFiles]:\n        \"\"\"\n        Retrieve sync files from the database.\n\n        Args:\n            sync_active_id (int): The ID of the active sync.\n\n        Returns:\n            list[SyncsFiles]: A list of sync files matching the criteria.\n        \"\"\"\n        logger.info(\"Retrieving sync files for sync_active_id: %s\", sync_active_id)\n        response = (\n            self.db.from_(\"syncs_files\")\n            .select(\"*\")\n            .eq(\"syncs_active_id\", sync_active_id)\n            .execute()\n        )\n        if response.data:\n            # logger.info(\"Sync files retrieved successfully: %s\", response.data)\n            return [SyncsFiles(**file) for file in response.data]\n        logger.warning(\"No sync files found for sync_active_id: %s\", sync_active_id)\n        return []\n\n    def update_sync_file(self, sync_file_id: int, sync_file_input: SyncFileUpdateInput):\n        \"\"\"\n        Update a sync file in the database.\n\n        Args:\n            sync_file_id (int): The ID of the sync file.\n            sync_file_input (SyncFileUpdateInput): The input data for updating the sync file.\n        \"\"\"\n        logger.info(\n            \"Updating sync file with sync_file_id: %s, input: %s\",\n            sync_file_id,\n            sync_file_input,\n        )\n        self.db.from_(\"syncs_files\").update(\n            sync_file_input.model_dump(exclude_unset=True)\n        ).eq(\"id\", sync_file_id).execute()\n        logger.info(\"Sync file updated successfully\")\n\n    def delete_sync_file(self, sync_file_id: int):\n        \"\"\"\n        Delete a sync file from the database.\n\n        Args:\n            sync_file_id (int): The ID of the sync file.\n        \"\"\"\n        logger.info(\"Deleting sync file with sync_file_id: %s\", sync_file_id)\n        self.db.from_(\"syncs_files\").delete().eq(\"id\", sync_file_id).execute()\n        logger.info(\"Sync file deleted successfully\")\n", "backend/modules/sync/service/sync_service.py": "from typing import List\n\nfrom logger import get_logger\nfrom modules.sync.dto.inputs import (\n    SyncsActiveInput,\n    SyncsActiveUpdateInput,\n    SyncsUserInput,\n    SyncUserUpdateInput,\n)\nfrom modules.sync.entity.sync import SyncsActive\nfrom modules.sync.repository.sync import Sync, SyncInterface\nfrom modules.sync.repository.sync_interfaces import SyncInterface, SyncUserInterface\nfrom modules.sync.repository.sync_user import SyncUser\nfrom modules.user.service.user_service import UserService\n\nlogger = get_logger(__name__)\n\n\nuser_service = UserService()\n\n\nclass SyncUserService:\n    repository: SyncUserInterface\n\n    def __init__(self):\n        self.repository = SyncUser()\n\n    def get_syncs_user(self, user_id: str, sync_user_id: int = None):\n        return self.repository.get_syncs_user(user_id, sync_user_id)\n\n    def create_sync_user(self, sync_user_input: SyncsUserInput):\n        return self.repository.create_sync_user(sync_user_input)\n\n    def delete_sync_user(self, sync_id: str, user_id: str):\n        return self.repository.delete_sync_user(sync_id, user_id)\n\n    def get_sync_user_by_state(self, state: dict):\n        return self.repository.get_sync_user_by_state(state)\n\n    def get_sync_user_by_id(self, sync_id: int):\n        return self.repository.get_sync_user_by_id(sync_id)\n\n    def update_sync_user(\n        self, sync_user_id: str, state: dict, sync_user_input: SyncUserUpdateInput\n    ):\n        return self.repository.update_sync_user(sync_user_id, state, sync_user_input)\n\n    def get_files_folder_user_sync(\n        self,\n        sync_active_id: int,\n        user_id: str,\n        folder_id: str = None,\n        recursive: bool = False,\n    ):\n        return self.repository.get_files_folder_user_sync(\n            sync_active_id, user_id, folder_id, recursive\n        )\n\n\nclass SyncService:\n    repository: SyncInterface\n\n    def __init__(self):\n        self.repository = Sync()\n\n    def create_sync_active(\n        self, sync_active_input: SyncsActiveInput, user_id: str\n    ) -> SyncsActive:\n        return self.repository.create_sync_active(sync_active_input, user_id)\n\n    def get_syncs_active(self, user_id: str) -> List[SyncsActive]:\n        return self.repository.get_syncs_active(user_id)\n\n    def update_sync_active(\n        self, sync_id: str, sync_active_input: SyncsActiveUpdateInput\n    ):\n        return self.repository.update_sync_active(sync_id, sync_active_input)\n\n    def delete_sync_active(self, sync_active_id: str, user_id: str):\n        return self.repository.delete_sync_active(sync_active_id, user_id)\n\n    async def get_syncs_active_in_interval(self) -> List[SyncsActive]:\n        return await self.repository.get_syncs_active_in_interval()\n\n    def get_details_sync_active(self, sync_active_id: int):\n        return self.repository.get_details_sync_active(sync_active_id)\n", "backend/modules/sync/service/__init__.py": "", "backend/modules/sync/dto/outputs.py": "from enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass AuthMethodEnum(str, Enum):\n    URI_WITH_CALLBACK = \"uri_with_callback\"\n\n\nclass SyncsDescription(BaseModel):\n    name: str\n    description: str\n    auth_method: AuthMethodEnum\n\n\nclass SyncsUserOutput(BaseModel):\n    user_id: str\n    provider: str\n    state: dict\n    credentials: dict\n", "backend/modules/sync/dto/__init__.py": "from .outputs import SyncsDescription, SyncsUserOutput\n", "backend/modules/sync/dto/inputs.py": "from typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass SyncsUserInput(BaseModel):\n    \"\"\"\n    Input model for creating a new sync user.\n\n    Attributes:\n        user_id (str): The unique identifier for the user.\n        name (str): The name of the user.\n        provider (str): The provider of the sync service (e.g., Google, Azure).\n        credentials (dict): The credentials required for the sync service.\n        state (dict): The state information for the sync user.\n    \"\"\"\n\n    user_id: str\n    name: str\n    provider: str\n    credentials: dict\n    state: dict\n\n\nclass SyncUserUpdateInput(BaseModel):\n    \"\"\"\n    Input model for updating an existing sync user.\n\n    Attributes:\n        credentials (dict): The updated credentials for the sync service.\n        state (dict): The updated state information for the sync user.\n    \"\"\"\n\n    credentials: dict\n    state: dict\n    email: str\n\n\nclass SyncActiveSettings(BaseModel):\n    \"\"\"\n    Sync active settings.\n\n    Attributes:\n        folders (List[str] | None): A list of folder paths to be synced, or None if not applicable.\n        files (List[str] | None): A list of file paths to be synced, or None if not applicable.\n    \"\"\"\n\n    folders: Optional[List[str]] = None\n    files: Optional[List[str]] = None\n\n\nclass SyncsActiveInput(BaseModel):\n    \"\"\"\n    Input model for creating a new active sync.\n\n    Attributes:\n        name (str): The name of the sync.\n        syncs_user_id (int): The ID of the sync user associated with this sync.\n        settings (SyncActiveSettings): The settings for the active sync.\n    \"\"\"\n\n    name: str\n    syncs_user_id: int\n    settings: SyncActiveSettings\n    brain_id: str\n\n\nclass SyncsActiveUpdateInput(BaseModel):\n    \"\"\"\n    Input model for updating an existing active sync.\n\n    Attributes:\n        name (str): The updated name of the sync.\n        sync_interval_minutes (int): The updated sync interval in minutes.\n        settings (dict): The updated settings for the active sync.\n    \"\"\"\n\n    name: Optional[str] = None\n    settings: Optional[SyncActiveSettings] = None\n    last_synced: Optional[str] = None\n    force_sync: Optional[bool] = False\n\n\nclass SyncFileInput(BaseModel):\n    \"\"\"\n    Input model for creating a new sync file.\n\n    Attributes:\n        path (str): The path of the file.\n        syncs_active_id (int): The ID of the active sync associated with this file.\n    \"\"\"\n\n    path: str\n    syncs_active_id: int\n    last_modified: str\n    brain_id: str\n    supported: Optional[bool] = True\n\n\nclass SyncFileUpdateInput(BaseModel):\n    \"\"\"\n    Input model for updating an existing sync file.\n\n    Attributes:\n        last_modified (datetime.datetime): The updated last modified date and time.\n    \"\"\"\n\n    last_modified: Optional[str] = None\n    supported: Optional[bool] = None\n", "backend/modules/sync/entity/sync.py": "from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass SyncsUser(BaseModel):\n    id: int\n    user_id: str\n    name: str\n    provider: str\n\n\nclass SyncsActive(BaseModel):\n    id: int\n    name: str\n    syncs_user_id: int\n    user_id: str\n    settings: dict\n    last_synced: datetime\n    sync_interval_minutes: int\n    brain_id: str\n    syncs_user: Optional[SyncsUser] = None\n\n\nclass SyncsFiles(BaseModel):\n    id: int\n    path: str\n    syncs_active_id: int\n    last_modified: str\n    brain_id: str\n    supported: bool\n", "backend/modules/sync/entity/__init__.py": "", "backend/modules/api_key/__init__.py": "", "backend/modules/api_key/controller/api_key_routes.py": "from secrets import token_hex\nfrom typing import List\nfrom uuid import uuid4\n\nfrom fastapi import APIRouter, Depends\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.api_key.dto.outputs import ApiKeyInfo\nfrom modules.api_key.entity.api_key import ApiKey\nfrom modules.api_key.repository.api_keys import ApiKeys\nfrom modules.user.entity.user_identity import UserIdentity\n\nlogger = get_logger(__name__)\n\n\napi_key_router = APIRouter()\n\napi_keys_repository = ApiKeys()\n\n\n@api_key_router.post(\n    \"/api-key\",\n    response_model=ApiKey,\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"API Key\"],\n)\nasync def create_api_key(current_user: UserIdentity = Depends(get_current_user)):\n    \"\"\"\n    Create new API key for the current user.\n\n    - `current_user`: The current authenticated user.\n    - Returns the newly created API key.\n\n    This endpoint generates a new API key for the current user. The API key is stored in the database and associated with\n    the user. It returns the newly created API key.\n    \"\"\"\n\n    new_key_id = uuid4()\n    new_api_key = token_hex(16)\n\n    try:\n        # Attempt to insert new API key into database\n        response = api_keys_repository.create_api_key(\n            new_key_id, new_api_key, current_user.id, \"api_key\", 30, False\n        )\n    except Exception as e:\n        logger.error(f\"Error creating new API key: {e}\")\n        return {\"api_key\": \"Error creating new API key.\"}\n    logger.info(f\"Created new API key for user {current_user.email}.\")\n\n    return response  # type: ignore\n\n\n@api_key_router.delete(\n    \"/api-key/{key_id}\", dependencies=[Depends(AuthBearer())], tags=[\"API Key\"]\n)\nasync def delete_api_key(\n    key_id: str, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Delete (deactivate) an API key for the current user.\n\n    - `key_id`: The ID of the API key to delete.\n\n    This endpoint deactivates and deletes the specified API key associated with the current user. The API key is marked\n    as inactive in the database.\n\n    \"\"\"\n    api_keys_repository.delete_api_key(key_id, current_user.id)\n\n    return {\"message\": \"API key deleted.\"}\n\n\n@api_key_router.get(\n    \"/api-keys\",\n    response_model=List[ApiKeyInfo],\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"API Key\"],\n)\nasync def get_api_keys(current_user: UserIdentity = Depends(get_current_user)):\n    \"\"\"\n    Get all active API keys for the current user.\n\n    - `current_user`: The current authenticated user.\n    - Returns a list of active API keys with their IDs and creation times.\n\n    This endpoint retrieves all the active API keys associated with the current user. It returns a list of API key objects\n    containing the key ID and creation time for each API key.\n    \"\"\"\n    response = api_keys_repository.get_user_api_keys(current_user.id)\n    return response.data\n", "backend/modules/api_key/controller/__init__.py": "from .api_key_routes import api_key_router\n", "backend/modules/api_key/repository/api_keys.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom models.settings import get_supabase_client\nfrom modules.api_key.entity.api_key import ApiKey\nfrom modules.api_key.repository.api_key_interface import ApiKeysInterface\n\n\nclass ApiKeys(ApiKeysInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client  # type: ignore\n\n    def create_api_key(\n        self, new_key_id, new_api_key, user_id, name, days=30, only_chat=False\n    ) -> Optional[ApiKey]:\n        response = (\n            self.db.table(\"api_keys\")\n            .insert(\n                [\n                    {\n                        \"key_id\": str(new_key_id),\n                        \"user_id\": str(user_id),\n                        \"api_key\": str(new_api_key),\n                        \"name\": str(name),\n                        \"days\": int(days),\n                        \"only_chat\": bool(only_chat),\n                        \"creation_time\": datetime.utcnow().strftime(\n                            \"%Y-%m-%d %H:%M:%S\"\n                        ),\n                        \"is_active\": True,\n                    }\n                ]\n            )\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        return ApiKey(**response.data[0])\n\n    def delete_api_key(self, key_id: str, user_id: UUID):\n        return (\n            self.db.table(\"api_keys\")\n            .update(\n                {\n                    \"is_active\": False,\n                    \"deleted_time\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                }\n            )\n            .match({\"key_id\": key_id, \"user_id\": user_id})\n            .execute()\n        )\n\n    def get_active_api_key(self, api_key: str):\n        response = (\n            self.db.table(\"api_keys\")\n            .select(\"api_key\", \"creation_time\")\n            .filter(\"api_key\", \"eq\", api_key)\n            .filter(\"is_active\", \"eq\", str(True))\n            .execute()\n        )\n        return response\n\n    def get_user_id_by_api_key(self, api_key: str):\n        response = (\n            self.db.table(\"api_keys\")\n            .select(\"user_id\")\n            .filter(\"api_key\", \"eq\", api_key)\n            .execute()\n        )\n        return response\n\n    def get_user_api_keys(self, user_id):\n        response = (\n            self.db.table(\"api_keys\")\n            .select(\"key_id, creation_time\")\n            .filter(\"user_id\", \"eq\", user_id)\n            .filter(\"is_active\", \"eq\", True)\n            .execute()\n        )\n        return response.data\n", "backend/modules/api_key/repository/__init__.py": "", "backend/modules/api_key/repository/api_key_interface.py": "from abc import ABC, abstractmethod\nfrom typing import List\nfrom uuid import UUID\n\nfrom modules.api_key.entity.api_key import ApiKey\n\n\nclass ApiKeysInterface(ABC):\n    @abstractmethod\n    def create_api_key(\n        self,\n        new_key_id: UUID,\n        new_api_key: str,\n        user_id: UUID,\n        days: int,\n        only_chat: bool,\n    ):\n        pass\n\n    @abstractmethod\n    def delete_api_key(self, key_id: UUID, user_id: UUID):\n        pass\n\n    @abstractmethod\n    def get_active_api_key(self, api_key: UUID):\n        pass\n\n    @abstractmethod\n    def get_user_id_by_api_key(self, api_key: UUID):\n        pass\n\n    @abstractmethod\n    def get_user_api_keys(self, user_id: UUID) -> List[ApiKey]:\n        pass\n", "backend/modules/api_key/service/api_key_service.py": "from datetime import datetime\n\nfrom fastapi import HTTPException\nfrom logger import get_logger\nfrom modules.api_key.repository.api_key_interface import ApiKeysInterface\nfrom modules.api_key.repository.api_keys import ApiKeys\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_service import UserService\n\nlogger = get_logger(__name__)\n\n\nuser_service = UserService()\n\n\nclass ApiKeyService:\n    repository: ApiKeysInterface\n\n    def __init__(self):\n        self.repository = ApiKeys()\n\n    async def verify_api_key(\n        self,\n        api_key: str,\n    ) -> bool:\n        try:\n            # Use UTC time to avoid timezone issues\n            current_date = datetime.utcnow().date()\n            result = self.repository.get_active_api_key(api_key)\n\n            if result.data is not None and len(result.data) > 0:\n                api_key_creation_date = datetime.strptime(\n                    result.data[0][\"creation_time\"], \"%Y-%m-%dT%H:%M:%S\"\n                ).date()\n\n                if api_key_creation_date.year == current_date.year:\n                    return True\n            return False\n        except Exception as e:\n            logger.error(f\"Error verifying API key: {e}\")\n            return False\n\n    async def get_user_from_api_key(\n        self,\n        api_key: str,\n    ) -> UserIdentity:\n        user_id_data = self.repository.get_user_id_by_api_key(api_key)\n\n        if not user_id_data.data:\n            raise HTTPException(status_code=400, detail=\"Invalid API key.\")\n\n        user_id = user_id_data.data[0][\"user_id\"]\n\n        # TODO: directly UserService instead\n        email = user_service.get_user_email_by_user_id(user_id)\n\n        if email is None:\n            raise HTTPException(status_code=400, detail=\"Invalid API key.\")\n\n        return UserIdentity(email=email, id=user_id)\n", "backend/modules/api_key/service/__init__.py": "", "backend/modules/api_key/dto/outputs.py": "from pydantic import BaseModel\n\n\nclass ApiKeyInfo(BaseModel):\n    key_id: str\n    creation_time: str\n", "backend/modules/api_key/dto/__init__.py": "from .outputs import ApiKeyInfo\n", "backend/modules/api_key/entity/api_key.py": "from pydantic import BaseModel\n\n\nclass ApiKey(BaseModel):\n    api_key: str\n    key_id: str\n    days: int\n    only_chat: bool\n    name: str\n    creation_time: str\n    is_active: bool\n", "backend/modules/api_key/entity/__init__.py": "", "backend/modules/misc/controller/misc_routes.py": "from fastapi import APIRouter\n\nmisc_router = APIRouter()\n\n\n@misc_router.get(\"/\")\nasync def root():\n    \"\"\"\n    Root endpoint to check the status of the API.\n    \"\"\"\n    return {\"status\": \"OK\"}\n\n\n@misc_router.get(\"/healthz\", tags=[\"Health\"])\nasync def healthz():\n    return {\"status\": \"ok\"}\n", "backend/modules/misc/controller/__init__.py": "from .misc_routes import misc_router\n", "backend/modules/contact_support/__init__.py": "", "backend/modules/contact_support/controller/contact_routes.py": "from fastapi import APIRouter\nfrom logger import get_logger\nfrom modules.contact_support.controller.settings import ContactsSettings\nfrom packages.emails.send_email import send_email\nfrom pydantic import BaseModel\n\n\nclass ContactMessage(BaseModel):\n    customer_email: str\n    content: str\n\n\ncontact_router = APIRouter()\nlogger = get_logger(__name__)\n\n\ndef resend_contact_sales_email(customer_email: str, content: str):\n    settings = ContactsSettings()\n    mail_from = settings.resend_contact_sales_from\n    mail_to = settings.resend_contact_sales_to\n    body = f\"\"\"\n    <p>Customer email: {customer_email}</p>\n    <p>{content}</p>\n    \"\"\"\n    params = {\n        \"from\": mail_from,\n        \"to\": mail_to,\n        \"subject\": \"Contact sales\",\n        \"reply_to\": customer_email,\n        \"html\": body,\n    }\n\n    return send_email(params)\n\n\n@contact_router.post(\"/contact\")\ndef post_contact(message: ContactMessage):\n    try:\n        resend_contact_sales_email(message.customer_email, message.content)\n    except Exception as e:\n        logger.error(e)\n        return {\"error\": \"There was an error sending the email\"}\n", "backend/modules/contact_support/controller/settings.py": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass ContactsSettings(BaseSettings):\n    model_config = SettingsConfigDict(validate_default=False)\n    resend_contact_sales_from: str = \"null\"\n    resend_contact_sales_to: str = \"null\"\n", "backend/modules/contact_support/controller/__init__.py": "from .contact_routes import contact_router\n", "backend/modules/user/__init__.py": "", "backend/modules/user/controller/user_controller.py": "from fastapi import APIRouter, Depends, Request\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.brain.service.brain_user_service import BrainUserService\nfrom modules.user.dto.inputs import UserUpdatableProperties\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.repository.users import Users\nfrom modules.user.service.user_usage import UserUsage\n\nuser_router = APIRouter()\nbrain_user_service = BrainUserService()\nuser_repository = Users()\n\n\n@user_router.get(\"/user\", dependencies=[Depends(AuthBearer())], tags=[\"User\"])\nasync def get_user_endpoint(\n    request: Request, current_user: UserIdentity = Depends(get_current_user)\n):\n    \"\"\"\n    Get user information and statistics.\n\n    - `current_user`: The current authenticated user.\n    - Returns the user's email, maximum brain size, current brain size, maximum requests number, requests statistics, and the current date.\n\n    This endpoint retrieves information and statistics about the authenticated user. It includes the user's email, maximum brain size,\n    current brain size, maximum requests number, requests statistics, and the current date. The brain size is calculated based on the\n    user's uploaded vectors, and the maximum brain size is obtained from the environment variables. The requests statistics provide\n    information about the user's API usage.\n    \"\"\"\n\n    user_daily_usage = UserUsage(\n        id=current_user.id,\n        email=current_user.email,\n    )\n    user_settings = user_daily_usage.get_user_settings()\n    max_brain_size = user_settings.get(\"max_brain_size\", 1000000000)\n\n    monthly_chat_credit = user_settings.get(\"monthly_chat_credit\", 10)\n\n    user_daily_usage = UserUsage(id=current_user.id)\n\n    return {\n        \"email\": current_user.email,\n        \"max_brain_size\": max_brain_size,\n        \"current_brain_size\": 0,\n        \"monthly_chat_credit\": monthly_chat_credit,\n        \"models\": user_settings.get(\"models\", []),\n        \"id\": current_user.id,\n        \"is_premium\": user_settings[\"is_premium\"],\n    }\n\n\n@user_router.put(\n    \"/user/identity\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"User\"],\n)\ndef update_user_identity_route(\n    user_identity_updatable_properties: UserUpdatableProperties,\n    current_user: UserIdentity = Depends(get_current_user),\n) -> UserIdentity:\n    \"\"\"\n    Update user identity.\n    \"\"\"\n    return user_repository.update_user_properties(\n        current_user.id, user_identity_updatable_properties\n    )\n\n\n@user_router.get(\n    \"/user/identity\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"User\"],\n)\ndef get_user_identity_route(\n    current_user: UserIdentity = Depends(get_current_user),\n) -> UserIdentity:\n    \"\"\"\n    Get user identity.\n    \"\"\"\n    return user_repository.get_user_identity(current_user.id)\n\n@user_router.delete(\n    \"/user_data\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"User\"],\n)\nasync def delete_user_data_route(\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"\n    Delete a user.\n\n    - `user_id`: The ID of the user to delete.\n\n    This endpoint deletes a user from the system.\n    \"\"\"\n\n    user_repository.delete_user_data(current_user.id)\n\n    return {\"message\": \"User deleted successfully\"}\n\n@user_router.get(\n    \"/user/credits\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"User\"],\n)\ndef get_user_credits(\n    current_user: UserIdentity = Depends(get_current_user),\n) -> int:\n    \"\"\"\n    Get user remaining credits.\n    \"\"\"\n    return user_repository.get_user_credits(current_user.id)\n", "backend/modules/user/controller/__init__.py": "from .user_controller import user_router\n", "backend/modules/user/repository/users_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.user.dto.inputs import UserUpdatableProperties\nfrom modules.user.entity.user_identity import UserIdentity\n\n\nclass UsersInterface(ABC):\n    @abstractmethod\n    def create_user_identity(self, id: UUID) -> UserIdentity:\n        \"\"\"\n        Create a user identity\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_user_properties(\n        self,\n        user_id: UUID,\n        user_identity_updatable_properties: UserUpdatableProperties,\n    ) -> UserIdentity:\n        \"\"\"\n        Update the user properties\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_user_identity(self, user_id: UUID) -> UserIdentity:\n        \"\"\"\n        Get the user identity\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_user_id_by_user_email(self, email: str) -> UUID | None:\n        \"\"\"\n        Get the user id by user email\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_user_email_by_user_id(self, user_id: UUID) -> str:\n        \"\"\"\n        Get the user email by user id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_user_data(self, user_id: str):\n        \"\"\"\n        Delete a user.\n\n        - `user_id`: The ID of the user to delete.\n\n        This endpoint deletes a user from the system. \n        \"\"\"\n    @abstractmethod  \n    def get_user_credits(self, user_id: UUID) -> int:\n        \"\"\"\n        Get user remaining credits\n        \"\"\"\n        pass\n", "backend/modules/user/repository/__init__.py": "from .users import Users\n", "backend/modules/user/repository/users.py": "import time\nfrom models.settings import get_supabase_client\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.repository.users_interface import UsersInterface\nfrom modules.user.service import user_usage\n\n\nclass Users(UsersInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def create_user_identity(self, id):\n        response = (\n            self.db.from_(\"user_identity\")\n            .insert(\n                {\n                    \"user_id\": str(id),\n                }\n            )\n            .execute()\n        )\n        user_identity = response.data[0]\n        return UserIdentity(id=user_identity.get(\"user_id\"))\n\n    def update_user_properties(\n        self,\n        user_id,\n        user_identity_updatable_properties,\n    ):\n        response = (\n            self.db.from_(\"user_identity\")\n            .update(user_identity_updatable_properties.__dict__)\n            .filter(\"user_id\", \"eq\", user_id)  # type: ignore\n            .execute()\n        )\n\n        if len(response.data) == 0:\n            return self.create_user_identity(user_id)\n\n        user_identity = response.data[0]\n\n        print(\"USER_IDENTITY\", user_identity)\n        return UserIdentity(id=user_id)\n\n    def get_user_identity(self, user_id):\n        response = (\n            self.db.from_(\"user_identity\")\n            .select(\"*, users (email)\")\n            .filter(\"user_id\", \"eq\", str(user_id))\n            .execute()\n        )\n\n        if len(response.data) == 0:\n            return self.create_user_identity(user_id)\n\n        user_identity = response.data[0]\n\n        user_identity[\"id\"] = user_id  # Add 'id' field to the dictionary\n        user_identity[\"email\"] = user_identity[\"users\"][\"email\"]\n        return UserIdentity(**user_identity)\n\n    def get_user_id_by_user_email(self, email):\n        response = (\n            self.db.rpc(\"get_user_id_by_user_email\", {\"user_email\": email})\n            .execute()\n            .data\n        )\n        if len(response) > 0:\n            return response[0][\"user_id\"]\n        return None\n\n    def get_user_email_by_user_id(self, user_id):\n        response = self.db.rpc(\n            \"get_user_email_by_user_id\", {\"user_id\": str(user_id)}\n        ).execute()\n        return response.data[0][\"email\"]\n    \n    def delete_user_data(self, user_id):\n        response = (\n            self.db.from_(\"brains_users\")\n            .select(\"brain_id\")\n            .filter(\"rights\", \"eq\", \"Owner\")\n            .filter(\"user_id\", \"eq\", str(user_id))\n            .execute()\n        )\n        brain_ids = [row[\"brain_id\"] for row in response.data]\n\n        for brain_id in brain_ids:\n            self.db.table(\"brains\").delete().filter(\"brain_id\", \"eq\", brain_id).execute()\n\n        for brain_id in brain_ids:\n            self.db.table(\"brains_vectors\").delete().filter(\"brain_id\", \"eq\", brain_id).execute()\n\n        for brain_id in brain_ids:\n            self.db.table(\"chat_history\").delete().filter(\"brain_id\", \"eq\", brain_id).execute()\n\n        self.db.table(\"user_settings\").delete().filter(\"user_id\", \"eq\", str(user_id)).execute()\n        self.db.table(\"user_identity\").delete().filter(\"user_id\", \"eq\", str(user_id)).execute()\n        self.db.table(\"users\").delete().filter(\"id\", \"eq\", str(user_id)).execute()\n        \n\n    def get_user_credits(self, user_id):\n        user_usage_instance = user_usage.UserUsage(id=user_id)\n        \n        user_monthly_usage = user_usage_instance.get_user_monthly_usage(time.strftime(\"%Y%m%d\"))\n        monthly_chat_credit = self.db.from_(\"user_settings\").select(\"monthly_chat_credit\").filter(\"user_id\", \"eq\", str(user_id)).execute().data[0][\"monthly_chat_credit\"]\n\n        return monthly_chat_credit - user_monthly_usage\n", "backend/modules/user/service/user_usage.py": "from logger import get_logger\nfrom models.databases.supabase.supabase import SupabaseDB\nfrom models.settings import PostHogSettings, get_supabase_db\nfrom modules.user.entity.user_identity import UserIdentity\n\nlogger = get_logger(__name__)\n\n\nclass UserUsage(UserIdentity):\n    daily_requests_count: int = 0\n\n    def __init__(self, **data):\n        super().__init__(**data)\n\n    @property\n    def supabase_db(self) -> SupabaseDB:\n        return get_supabase_db()\n\n    def get_user_usage(self):\n        \"\"\"\n        Fetch the user request stats from the database\n        \"\"\"\n        request = self.supabase_db.get_user_usage(self.id)\n\n        return request\n\n    def get_model_settings(self):\n        \"\"\"\n        Fetch the user request stats from the database\n        \"\"\"\n        request = self.supabase_db.get_model_settings()\n\n        return request\n\n    def get_user_settings(self):\n        \"\"\"\n        Fetch the user settings from the database\n        \"\"\"\n        posthog = PostHogSettings()\n        request = self.supabase_db.get_user_settings(self.id)\n        if request is not None and request.get(\"is_premium\", False):\n            posthog.set_once_user_properties(\n                self.id, \"HAS_OR_HAD_PREMIUM\", {\"is_was_premium\": \"true\"}\n            )\n            posthog.set_user_properties(\n                self.id, \"CURRENT_PREMIUM\", {\"is_premium\": \"true\"}\n            )\n        else:\n            posthog.set_user_properties(\n                self.id, \"CURRENT_PREMIUM\", {\"is_premium\": \"false\"}\n            )\n\n        return request\n\n    def get_user_monthly_usage(self, date):\n        \"\"\"\n        Fetch the user monthly usage from the database\n        \"\"\"\n        posthog = PostHogSettings()\n        request = self.supabase_db.get_user_requests_count_for_month(self.id, date)\n        posthog.set_user_properties(\n            self.id, \"MONTHLY_USAGE\", {\"monthly_chat_usage\": request}\n        )\n\n        return request\n\n    def handle_increment_user_request_count(self, date, number=1):\n        \"\"\"\n        Increment the user request count in the database\n        \"\"\"\n        current_requests_count = self.supabase_db.get_user_requests_count_for_month(\n            self.id, date\n        )\n\n        daily_requests_count = self.supabase_db.get_user_requests_count_for_day(\n            self.id, date\n        )\n\n        if daily_requests_count == 0:\n            logger.info(\"Request count is 0, creating new record\")\n            if self.email is None:\n                raise ValueError(\"User Email should be defined for daily usage table\")\n            self.supabase_db.create_user_daily_usage(\n                user_id=self.id, date=date, user_email=self.email, number=number\n            )\n            self.daily_requests_count = number\n            return\n\n        self.supabase_db.increment_user_request_count(\n            user_id=self.id,\n            date=date,\n            number=daily_requests_count + number,\n        )\n\n        self.daily_requests_count = current_requests_count + number\n\n        logger.info(\n            f\"User {self.email} request count updated to {self.daily_requests_count}\"\n        )\n", "backend/modules/user/service/__init__.py": "from .user_service import UserService", "backend/modules/user/service/user_service.py": "from uuid import UUID\n\nfrom modules.user.repository.users import Users\nfrom modules.user.repository.users_interface import UsersInterface\n\n\nclass UserService:\n    repository: UsersInterface\n\n    def __init__(self):\n        self.repository = Users()\n\n    def get_user_id_by_email(self, email: str) -> UUID | None:\n        return self.repository.get_user_id_by_user_email(email)\n\n    def get_user_email_by_user_id(self, user_id: UUID) -> str | None:\n        return self.repository.get_user_email_by_user_id(user_id)\n", "backend/modules/user/dto/inputs.py": "from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass UserUpdatableProperties(BaseModel):\n    # Nothing for now\n    username: Optional[str] = None\n    company: Optional[str] = None\n    onboarded: Optional[bool] = None\n    company_size: Optional[str] = None\n    usage_purpose: Optional[str] = None\n\n", "backend/modules/user/entity/user_identity.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass UserIdentity(BaseModel):\n    id: UUID\n    email: Optional[str] = None\n    username: Optional[str] = None\n    company: Optional[str] = None\n    onboarded: Optional[bool] = None\n    company_size: Optional[str] = None\n    usage_purpose: Optional[str] = None\n", "backend/modules/brain/qa_headless.py": "import asyncio\nimport json\nfrom typing import AsyncIterable, Awaitable, List, Optional\nfrom uuid import UUID\n\nfrom langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom modules.brain.service.utils.format_chat_history import (\n    format_chat_history,\n    format_history_to_openai_mesages,\n)\nfrom modules.prompt.service.get_prompt_to_use import get_prompt_to_use\nfrom modules.brain.service.utils.get_prompt_to_use_id import get_prompt_to_use_id\nfrom logger import get_logger\nfrom models import BrainSettings  # Importing settings related to the 'brain'\nfrom modules.brain.qa_interface import QAInterface\nfrom modules.chat.dto.chats import ChatQuestion\nfrom modules.chat.dto.inputs import CreateChatHistory\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.service.chat_service import ChatService\nfrom pydantic import BaseModel, ConfigDict\n\nlogger = get_logger(__name__)\nSYSTEM_MESSAGE = \"Your name is Quivr. You're a helpful assistant. If you don't know the answer, just say that you don't know, don't try to make up an answer.When answering use markdown or any other techniques to display the content in a nice and aerated way.\"\nchat_service = ChatService()\n\n\nclass HeadlessQA(BaseModel, QAInterface):\n    brain_settings = BrainSettings()\n    model: str\n    temperature: float = 0.0\n    max_tokens: int = 2000\n    streaming: bool = False\n    chat_id: str\n    callbacks: Optional[List[AsyncIteratorCallbackHandler]] = None\n    prompt_id: Optional[UUID] = None\n\n    def _determine_streaming(self, streaming: bool) -> bool:\n        \"\"\"If the model name allows for streaming and streaming is declared, set streaming to True.\"\"\"\n        return streaming\n\n    def _determine_callback_array(\n        self, streaming\n    ) -> List[AsyncIteratorCallbackHandler]:\n        \"\"\"If streaming is set, set the AsyncIteratorCallbackHandler as the only callback.\"\"\"\n        if streaming:\n            return [AsyncIteratorCallbackHandler()]\n        else:\n            return []\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.streaming = self._determine_streaming(self.streaming)\n        self.callbacks = self._determine_callback_array(self.streaming)\n\n    @property\n    def prompt_to_use(self) -> str:\n        return get_prompt_to_use(None, self.prompt_id)\n\n    @property\n    def prompt_to_use_id(self) -> Optional[UUID]:\n        return get_prompt_to_use_id(None, self.prompt_id)\n\n    def _create_llm(\n        self,\n        model,\n        temperature=0,\n        streaming=False,\n        callbacks=None,\n    ) -> BaseChatModel:\n        \"\"\"\n        Determine the language model to be used.\n        :param model: Language model name to be used.\n        :param streaming: Whether to enable streaming of the model\n        :param callbacks: Callbacks to be used for streaming\n        :return: Language model instance\n        \"\"\"\n        api_base = None\n        if self.brain_settings.ollama_api_base_url and model.startswith(\"ollama\"):\n            api_base = self.brain_settings.ollama_api_base_url\n\n        return ChatLiteLLM(\n            temperature=temperature,\n            model=model,\n            streaming=streaming,\n            verbose=True,\n            callbacks=callbacks,\n            max_tokens=self.max_tokens,\n            api_base=api_base,\n        )\n\n    def _create_prompt_template(self):\n        messages = [\n            HumanMessagePromptTemplate.from_template(\"{question}\"),\n        ]\n        CHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n        return CHAT_PROMPT\n\n    def generate_answer(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> GetChatHistoryOutput:\n        # Move format_chat_history to chat service ?\n        transformed_history = format_chat_history(\n            chat_service.get_chat_history(self.chat_id)\n        )\n        prompt_content = (\n            self.prompt_to_use.content if self.prompt_to_use else SYSTEM_MESSAGE\n        )\n\n        messages = format_history_to_openai_mesages(\n            transformed_history, prompt_content, question.question\n        )\n        answering_llm = self._create_llm(\n            model=self.model,\n            streaming=False,\n            callbacks=self.callbacks,\n        )\n        model_prediction = answering_llm.predict_messages(messages)\n        answer = model_prediction.content\n        if save_answer:\n            new_chat = chat_service.update_chat_history(\n                CreateChatHistory(\n                    **{\n                        \"chat_id\": chat_id,\n                        \"user_message\": question.question,\n                        \"assistant\": answer,\n                        \"brain_id\": None,\n                        \"prompt_id\": self.prompt_to_use_id,\n                    }\n                )\n            )\n\n            return GetChatHistoryOutput(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": answer,\n                    \"message_time\": new_chat.message_time,\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": None,\n                    \"message_id\": new_chat.message_id,\n                }\n            )\n        else:\n            return GetChatHistoryOutput(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": answer,\n                    \"message_time\": None,\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": None,\n                    \"message_id\": None,\n                }\n            )\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        callback = AsyncIteratorCallbackHandler()\n        self.callbacks = [callback]\n\n        transformed_history = format_chat_history(\n            chat_service.get_chat_history(self.chat_id)\n        )\n        prompt_content = (\n            self.prompt_to_use.content if self.prompt_to_use else SYSTEM_MESSAGE\n        )\n\n        messages = format_history_to_openai_mesages(\n            transformed_history, prompt_content, question.question\n        )\n        answering_llm = self._create_llm(\n            model=self.model,\n            streaming=True,\n            callbacks=self.callbacks,\n        )\n\n        CHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n        headlessChain = LLMChain(llm=answering_llm, prompt=CHAT_PROMPT)\n\n        response_tokens = []\n\n        async def wrap_done(fn: Awaitable, event: asyncio.Event):\n            try:\n                await fn\n            except Exception as e:\n                logger.error(f\"Caught exception: {e}\")\n            finally:\n                event.set()\n\n        run = asyncio.create_task(\n            wrap_done(\n                headlessChain.acall({}),\n                callback.done,\n            ),\n        )\n\n        if save_answer:\n            streamed_chat_history = chat_service.update_chat_history(\n                CreateChatHistory(\n                    **{\n                        \"chat_id\": chat_id,\n                        \"user_message\": question.question,\n                        \"assistant\": \"\",\n                        \"brain_id\": None,\n                        \"prompt_id\": self.prompt_to_use_id,\n                    }\n                )\n            )\n\n            streamed_chat_history = GetChatHistoryOutput(\n                **{\n                    \"chat_id\": str(chat_id),\n                    \"message_id\": streamed_chat_history.message_id,\n                    \"message_time\": streamed_chat_history.message_time,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": None,\n                }\n            )\n        else:\n            streamed_chat_history = GetChatHistoryOutput(\n                **{\n                    \"chat_id\": str(chat_id),\n                    \"message_id\": None,\n                    \"message_time\": None,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": None,\n                }\n            )\n\n        async for token in callback.aiter():\n            response_tokens.append(token)\n            streamed_chat_history.assistant = token\n            yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        await run\n        assistant = \"\".join(response_tokens)\n\n        if save_answer:\n            chat_service.update_message_by_id(\n                message_id=str(streamed_chat_history.message_id),\n                user_message=question.question,\n                assistant=assistant,\n            )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n", "backend/modules/brain/composite_brain_qa.py": "import json\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom litellm import completion\nfrom logger import get_logger\nfrom modules.brain.api_brain_qa import APIBrainQA\nfrom modules.brain.entity.brain_entity import BrainEntity, BrainType\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.brain.qa_headless import HeadlessQA\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.dto.chats import ChatQuestion\nfrom modules.chat.dto.inputs import CreateChatHistory\nfrom modules.chat.dto.outputs import (\n    BrainCompletionOutput,\n    CompletionMessage,\n    CompletionResponse,\n    GetChatHistoryOutput,\n)\nfrom modules.chat.service.chat_service import ChatService\n\nbrain_service = BrainService()\nchat_service = ChatService()\n\nlogger = get_logger(__name__)\n\n\ndef format_brain_to_tool(brain):\n    return {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": str(brain.id),\n            \"description\": brain.description,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"question\": {\n                        \"type\": \"string\",\n                        \"description\": \"Question to ask the brain\",\n                    },\n                },\n                \"required\": [\"question\"],\n            },\n        },\n    }\n\n\nclass CompositeBrainQA(\n    KnowledgeBrainQA,\n):\n    user_id: UUID\n\n    def __init__(\n        self,\n        model: str,\n        brain_id: str,\n        chat_id: str,\n        streaming: bool = False,\n        prompt_id: Optional[UUID] = None,\n        **kwargs,\n    ):\n        user_id = kwargs.get(\"user_id\")\n        if not user_id:\n            raise HTTPException(status_code=400, detail=\"Cannot find user id\")\n\n        super().__init__(\n            model=model,\n            brain_id=brain_id,\n            chat_id=chat_id,\n            streaming=streaming,\n            prompt_id=prompt_id,\n            **kwargs,\n        )\n        self.user_id = user_id\n\n    def get_answer_generator_from_brain_type(self, brain: BrainEntity):\n        if brain.brain_type == BrainType.COMPOSITE:\n            return self.generate_answer\n        elif brain.brain_type == BrainType.API:\n            return APIBrainQA(\n                brain_id=str(brain.id),\n                chat_id=self.chat_id,\n                model=self.model,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                prompt_id=self.prompt_id,\n                user_id=str(self.user_id),\n                raw=brain.raw,\n                jq_instructions=brain.jq_instructions,\n            ).generate_answer\n        elif brain.brain_type == BrainType.DOC:\n            return KnowledgeBrainQA(\n                brain_id=str(brain.id),\n                chat_id=self.chat_id,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                prompt_id=self.prompt_id,\n            ).generate_answer\n\n    def generate_answer(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool\n    ) -> str:\n        brain = brain_service.get_brain_by_id(question.brain_id)\n\n        connected_brains = brain_service.get_connected_brains(self.brain_id)\n\n        if not connected_brains:\n            response = HeadlessQA(\n                chat_id=chat_id,\n                model=self.model,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                prompt_id=self.prompt_id,\n            ).generate_answer(chat_id, question, save_answer=False)\n            if save_answer:\n                new_chat = chat_service.update_chat_history(\n                    CreateChatHistory(\n                        **{\n                            \"chat_id\": chat_id,\n                            \"user_message\": question.question,\n                            \"assistant\": response.assistant,\n                            \"brain_id\": question.brain_id,\n                            \"prompt_id\": self.prompt_to_use_id,\n                        }\n                    )\n                )\n                return GetChatHistoryOutput(\n                    **{\n                        \"chat_id\": chat_id,\n                        \"user_message\": question.question,\n                        \"assistant\": response.assistant,\n                        \"message_time\": new_chat.message_time,\n                        \"prompt_title\": (\n                            self.prompt_to_use.title if self.prompt_to_use else None\n                        ),\n                        \"brain_name\": brain.name,\n                        \"message_id\": new_chat.message_id,\n                        \"brain_id\": str(brain.id),\n                    }\n                )\n            return GetChatHistoryOutput(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": response.assistant,\n                    \"message_time\": None,\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": brain.name,\n                    \"message_id\": None,\n                    \"brain_id\": str(brain.id),\n                }\n            )\n\n        tools = []\n        available_functions = {}\n\n        connected_brains_details = {}\n        for connected_brain_id in connected_brains:\n            connected_brain = brain_service.get_brain_by_id(connected_brain_id)\n            if connected_brain is None:\n                continue\n\n            tools.append(format_brain_to_tool(connected_brain))\n\n            available_functions[connected_brain_id] = (\n                self.get_answer_generator_from_brain_type(connected_brain)\n            )\n\n            connected_brains_details[str(connected_brain.id)] = connected_brain\n\n        CHOOSE_BRAIN_FROM_TOOLS_PROMPT = (\n            \"Based on the provided user content, find the most appropriate tools to answer\"\n            + \"If you can't find any tool to answer and only then, and if you can answer without using any tool. In that case, let the user know that you are not using any particular brain (i.e tool) \"\n        )\n\n        messages = [{\"role\": \"system\", \"content\": CHOOSE_BRAIN_FROM_TOOLS_PROMPT}]\n\n        history = chat_service.get_chat_history(self.chat_id)\n\n        for message in history:\n            formatted_message = [\n                {\"role\": \"user\", \"content\": message.user_message},\n                {\"role\": \"assistant\", \"content\": message.assistant},\n            ]\n            messages.extend(formatted_message)\n\n        messages.append({\"role\": \"user\", \"content\": question.question})\n\n        response = completion(\n            model=\"gpt-3.5-turbo-0125\",\n            messages=messages,\n            tools=tools,\n            tool_choice=\"auto\",\n        )\n\n        brain_completion_output = self.make_recursive_tool_calls(\n            messages,\n            question,\n            chat_id,\n            tools,\n            available_functions,\n            recursive_count=0,\n            last_completion_response=response.choices[0],\n        )\n\n        if brain_completion_output:\n            answer = brain_completion_output.response.message.content\n            new_chat = None\n            if save_answer:\n                new_chat = chat_service.update_chat_history(\n                    CreateChatHistory(\n                        **{\n                            \"chat_id\": chat_id,\n                            \"user_message\": question.question,\n                            \"assistant\": answer,\n                            \"brain_id\": question.brain_id,\n                            \"prompt_id\": self.prompt_to_use_id,\n                        }\n                    )\n                )\n            return GetChatHistoryOutput(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": brain_completion_output.response.message.content,\n                    \"message_time\": new_chat.message_time if new_chat else None,\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": brain.name if brain else None,\n                    \"message_id\": new_chat.message_id if new_chat else None,\n                    \"brain_id\": str(brain.id) if brain else None,\n                }\n            )\n\n    def make_recursive_tool_calls(\n        self,\n        messages,\n        question,\n        chat_id,\n        tools=[],\n        available_functions={},\n        recursive_count=0,\n        last_completion_response: CompletionResponse = None,\n    ):\n        if recursive_count > 5:\n            print(\n                \"The assistant is having issues and took more than 5 calls to the tools. Please try again later or an other instruction.\"\n            )\n            return None\n\n        finish_reason = last_completion_response.finish_reason\n        if finish_reason == \"stop\":\n            messages.append(last_completion_response.message)\n            return BrainCompletionOutput(\n                **{\n                    \"messages\": messages,\n                    \"question\": question.question,\n                    \"response\": last_completion_response,\n                }\n            )\n\n        if finish_reason == \"tool_calls\":\n            response_message: CompletionMessage = last_completion_response.message\n            tool_calls = response_message.tool_calls\n\n            messages.append(response_message)\n\n            if (\n                len(tool_calls) == 0\n                or tool_calls is None\n                or len(available_functions) == 0\n            ):\n                return\n\n            for tool_call in tool_calls:\n                function_name = tool_call.function.name\n                function_to_call = available_functions[function_name]\n                function_args = json.loads(tool_call.function.arguments)\n                question = ChatQuestion(\n                    question=function_args[\"question\"], brain_id=function_name\n                )\n\n                # TODO: extract chat_id from generate_answer function of XBrainQA\n                function_response = function_to_call(\n                    chat_id=chat_id,\n                    question=question,\n                    save_answer=False,\n                )\n                messages.append(\n                    {\n                        \"tool_call_id\": tool_call.id,\n                        \"role\": \"tool\",\n                        \"name\": function_name,\n                        \"content\": function_response.assistant,\n                    }\n                )\n\n            PROMPT_2 = \"If initial question can be answered by our conversation messages, then give an answer and end the conversation.\"\n\n            messages.append({\"role\": \"system\", \"content\": PROMPT_2})\n\n            for idx, msg in enumerate(messages):\n                logger.info(\n                    f\"Message {idx}: Role - {msg['role']}, Content - {msg['content']}\"\n                )\n\n            response_after_tools_answers = completion(\n                model=\"gpt-3.5-turbo-0125\",\n                messages=messages,\n                tools=tools,\n                tool_choice=\"auto\",\n            )\n\n            return self.make_recursive_tool_calls(\n                messages,\n                question,\n                chat_id,\n                tools,\n                available_functions,\n                recursive_count=recursive_count + 1,\n                last_completion_response=response_after_tools_answers.choices[0],\n            )\n\n    async def generate_stream(\n        self,\n        chat_id: UUID,\n        question: ChatQuestion,\n        save_answer: bool,\n        should_log_steps: Optional[bool] = True,\n    ):\n        brain = brain_service.get_brain_by_id(question.brain_id)\n        if save_answer:\n            streamed_chat_history = chat_service.update_chat_history(\n                CreateChatHistory(\n                    **{\n                        \"chat_id\": chat_id,\n                        \"user_message\": question.question,\n                        \"assistant\": \"\",\n                        \"brain_id\": question.brain_id,\n                        \"prompt_id\": self.prompt_to_use_id,\n                    }\n                )\n            )\n            streamed_chat_history = GetChatHistoryOutput(\n                **{\n                    \"chat_id\": str(chat_id),\n                    \"message_id\": streamed_chat_history.message_id,\n                    \"message_time\": streamed_chat_history.message_time,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": brain.name if brain else None,\n                    \"brain_id\": str(brain.id) if brain else None,\n                }\n            )\n        else:\n            streamed_chat_history = GetChatHistoryOutput(\n                **{\n                    \"chat_id\": str(chat_id),\n                    \"message_id\": None,\n                    \"message_time\": None,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"prompt_title\": (\n                        self.prompt_to_use.title if self.prompt_to_use else None\n                    ),\n                    \"brain_name\": brain.name if brain else None,\n                    \"brain_id\": str(brain.id) if brain else None,\n                }\n            )\n\n        connected_brains = brain_service.get_connected_brains(self.brain_id)\n\n        if not connected_brains:\n            headlesss_answer = HeadlessQA(\n                chat_id=chat_id,\n                model=self.model,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                prompt_id=self.prompt_id,\n            ).generate_stream(chat_id, question)\n\n            response_tokens = []\n            async for value in headlesss_answer:\n                streamed_chat_history.assistant = value\n                response_tokens.append(value)\n                yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n            if save_answer:\n                chat_service.update_message_by_id(\n                    message_id=str(streamed_chat_history.message_id),\n                    user_message=question.question,\n                    assistant=\"\".join(response_tokens),\n                )\n\n        tools = []\n        available_functions = {}\n\n        connected_brains_details = {}\n        for brain_id in connected_brains:\n            brain = brain_service.get_brain_by_id(brain_id)\n            if brain == None:\n                continue\n\n            tools.append(format_brain_to_tool(brain))\n\n            available_functions[brain_id] = self.get_answer_generator_from_brain_type(\n                brain\n            )\n\n            connected_brains_details[str(brain.id)] = brain\n\n        CHOOSE_BRAIN_FROM_TOOLS_PROMPT = (\n            \"Based on the provided user content, find the most appropriate tools to answer\"\n            + \"If you can't find any tool to answer and only then, and if you can answer without using any tool. In that case, let the user know that you are not using any particular brain (i.e tool) \"\n        )\n\n        messages = [{\"role\": \"system\", \"content\": CHOOSE_BRAIN_FROM_TOOLS_PROMPT}]\n\n        history = chat_service.get_chat_history(self.chat_id)\n\n        for message in history:\n            formatted_message = [\n                {\"role\": \"user\", \"content\": message.user_message},\n                {\"role\": \"assistant\", \"content\": message.assistant},\n            ]\n            if message.assistant is None:\n                print(message)\n            messages.extend(formatted_message)\n\n        messages.append({\"role\": \"user\", \"content\": question.question})\n\n        initial_response = completion(\n            model=\"gpt-3.5-turbo-0125\",\n            stream=True,\n            messages=messages,\n            tools=tools,\n            tool_choice=\"auto\",\n        )\n\n        response_tokens = []\n        tool_calls_aggregate = []\n        for chunk in initial_response:\n            content = chunk.choices[0].delta.content\n            if content is not None:\n                # Need to store it ?\n                streamed_chat_history.assistant = content\n                response_tokens.append(chunk.choices[0].delta.content)\n\n                if save_answer:\n                    yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n                else:\n                    yield f\"\ud83e\udde0<' {chunk.choices[0].delta.content}\"\n\n            if (\n                \"tool_calls\" in chunk.choices[0].delta\n                and chunk.choices[0].delta.tool_calls is not None\n            ):\n                tool_calls = chunk.choices[0].delta.tool_calls\n                for tool_call in tool_calls:\n                    id = tool_call.id\n                    name = tool_call.function.name\n                    if id and name:\n                        tool_calls_aggregate += [\n                            {\n                                \"id\": tool_call.id,\n                                \"function\": {\n                                    \"arguments\": tool_call.function.arguments,\n                                    \"name\": tool_call.function.name,\n                                },\n                                \"type\": \"function\",\n                            }\n                        ]\n\n                    else:\n                        try:\n                            tool_calls_aggregate[tool_call.index][\"function\"][\n                                \"arguments\"\n                            ] += tool_call.function.arguments\n                        except IndexError:\n                            print(\"TOOL_CALL_INDEX error\", tool_call.index)\n                            print(\"TOOL_CALLS_AGGREGATE error\", tool_calls_aggregate)\n\n            finish_reason = chunk.choices[0].finish_reason\n\n            if finish_reason == \"stop\":\n                if save_answer:\n                    chat_service.update_message_by_id(\n                        message_id=str(streamed_chat_history.message_id),\n                        user_message=question.question,\n                        assistant=\"\".join(\n                            [\n                                token\n                                for token in response_tokens\n                                if not token.startswith(\"\ud83e\udde0<\")\n                            ]\n                        ),\n                    )\n                break\n\n            if finish_reason == \"tool_calls\":\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"tool_calls\": tool_calls_aggregate,\n                        \"content\": None,\n                    }\n                )\n                for tool_call in tool_calls_aggregate:\n                    function_name = tool_call[\"function\"][\"name\"]\n                    queried_brain = connected_brains_details[function_name]\n                    function_to_call = available_functions[function_name]\n                    function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n                    print(\"function_args\", function_args[\"question\"])\n                    question = ChatQuestion(\n                        question=function_args[\"question\"], brain_id=queried_brain.id\n                    )\n\n                    # yield f\"\ud83e\udde0< Querying the brain {queried_brain.name} with the following arguments: {function_args} >\ud83e\udde0\",\n\n                    print(\n                        f\"\ud83e\udde0< Querying the brain {queried_brain.name} with the following arguments: {function_args}\",\n                    )\n                    function_response = function_to_call(\n                        chat_id=chat_id,\n                        question=question,\n                        save_answer=False,\n                    )\n\n                    messages.append(\n                        {\n                            \"tool_call_id\": tool_call[\"id\"],\n                            \"role\": \"tool\",\n                            \"name\": function_name,\n                            \"content\": function_response.assistant,\n                        }\n                    )\n\n                    print(\"messages\", messages)\n\n                PROMPT_2 = \"If the last user's question can be answered by our conversation messages since then, then give an answer and end the conversation. If you need to ask question to the user to gather more information and give a more accurate answer, then ask the question and wait for the user's answer.\"\n                # Otherwise, ask a new question to the assistant and choose brains you would like to ask questions.\"\n\n                messages.append({\"role\": \"system\", \"content\": PROMPT_2})\n\n                response_after_tools_answers = completion(\n                    model=\"gpt-3.5-turbo-0125\",\n                    messages=messages,\n                    tools=tools,\n                    tool_choice=\"auto\",\n                    stream=True,\n                )\n\n                response_tokens = []\n                for chunk in response_after_tools_answers:\n                    print(\"chunk_response_after_tools_answers\", chunk)\n                    content = chunk.choices[0].delta.content\n                    if content:\n                        streamed_chat_history.assistant = content\n                        response_tokens.append(chunk.choices[0].delta.content)\n                        yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n                    finish_reason = chunk.choices[0].finish_reason\n\n                    if finish_reason == \"stop\":\n                        chat_service.update_message_by_id(\n                            message_id=str(streamed_chat_history.message_id),\n                            user_message=question.question,\n                            assistant=\"\".join(\n                                [\n                                    token\n                                    for token in response_tokens\n                                    if not token.startswith(\"\ud83e\udde0<\")\n                                ]\n                            ),\n                        )\n                        break\n                    elif finish_reason is not None:\n                        # TODO: recursively call with tools (update prompt + create intermediary function )\n                        print(\"NO STOP\")\n                        print(chunk.choices[0])\n", "backend/modules/brain/knowledge_brain_qa.py": "import json\nfrom typing import AsyncIterable, List, Optional\nfrom uuid import UUID\n\nfrom langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\nfrom logger import get_logger\nfrom models import BrainSettings\nfrom modules.brain.entity.brain_entity import BrainEntity\nfrom modules.brain.qa_interface import QAInterface\nfrom modules.brain.rags.quivr_rag import QuivrRAG\nfrom modules.brain.rags.rag_interface import RAGInterface\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.brain.service.utils.format_chat_history import format_chat_history\nfrom modules.brain.service.utils.get_prompt_to_use_id import get_prompt_to_use_id\nfrom modules.chat.controller.chat.utils import (\n    find_model_and_generate_metadata,\n    update_user_usage,\n)\nfrom modules.chat.dto.chats import ChatQuestion, Sources\nfrom modules.chat.dto.inputs import CreateChatHistory\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.service.chat_service import ChatService\nfrom modules.prompt.service.get_prompt_to_use import get_prompt_to_use\nfrom modules.upload.service.generate_file_signed_url import generate_file_signed_url\nfrom modules.user.service.user_usage import UserUsage\nfrom pydantic import BaseModel, ConfigDict\nfrom pydantic_settings import BaseSettings\n\nlogger = get_logger(__name__)\nQUIVR_DEFAULT_PROMPT = \"Your name is Quivr. You're a helpful assistant.  If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n\n\nbrain_service = BrainService()\nchat_service = ChatService()\n\n\ndef is_valid_uuid(uuid_to_test, version=4):\n    try:\n        uuid_obj = UUID(uuid_to_test, version=version)\n    except ValueError:\n        return False\n\n    return str(uuid_obj) == uuid_to_test\n\n\ndef generate_source(source_documents, brain_id, citations: List[int] = None):\n    \"\"\"\n    Generate the sources list for the answer\n    It takes in a list of sources documents and citations that points to the docs index that was used in the answer\n    \"\"\"\n    # Initialize an empty list for sources\n    sources_list: List[Sources] = []\n\n    # Initialize a dictionary for storing generated URLs\n    generated_urls = {}\n\n    # remove duplicate sources with same name and create a list of unique sources\n    sources_url_cache = {}\n\n    # Get source documents from the result, default to an empty list if not found\n\n    # If source documents exist\n    if source_documents:\n        logger.info(f\"Citations {citations}\")\n        # Iterate over each document\n        for doc, index in zip(source_documents, range(len(source_documents))):\n            logger.info(f\"Processing source document {doc.metadata['file_name']}\")\n            if citations is not None:\n                if index not in citations:\n                    logger.info(f\"Skipping source document {doc.metadata['file_name']}\")\n                    continue\n            # Check if 'url' is in the document metadata\n            is_url = (\n                \"original_file_name\" in doc.metadata\n                and doc.metadata[\"original_file_name\"] is not None\n                and doc.metadata[\"original_file_name\"].startswith(\"http\")\n            )\n\n            # Determine the name based on whether it's a URL or a file\n            name = (\n                doc.metadata[\"original_file_name\"]\n                if is_url\n                else doc.metadata[\"file_name\"]\n            )\n\n            # Determine the type based on whether it's a URL or a file\n            type_ = \"url\" if is_url else \"file\"\n\n            # Determine the source URL based on whether it's a URL or a file\n            if is_url:\n                source_url = doc.metadata[\"original_file_name\"]\n            else:\n                file_path = f\"{brain_id}/{doc.metadata['file_name']}\"\n                # Check if the URL has already been generated\n                if file_path in generated_urls:\n                    source_url = generated_urls[file_path]\n                else:\n                    # Generate the URL\n                    if file_path in sources_url_cache:\n                        source_url = sources_url_cache[file_path]\n                    else:\n                        generated_url = generate_file_signed_url(file_path)\n                        if generated_url is not None:\n                            source_url = generated_url.get(\"signedURL\", \"\")\n                        else:\n                            source_url = \"\"\n                    # Store the generated URL\n                    generated_urls[file_path] = source_url\n\n            # Append a new Sources object to the list\n            sources_list.append(\n                Sources(\n                    name=name,\n                    type=type_,\n                    source_url=source_url,\n                    original_file_name=name,\n                    citation=doc.page_content,\n                )\n            )\n    else:\n        logger.info(\"No source documents found or source_documents is not a list.\")\n    return sources_list\n\n\nclass KnowledgeBrainQA(BaseModel, QAInterface):\n    \"\"\"\n    Main class for the Brain Picking functionality.\n    It allows to initialize a Chat model, generate questions and retrieve answers using ConversationalRetrievalChain.\n    It has two main methods: `generate_question` and `generate_stream`.\n    One is for generating questions in a single request, the other is for generating questions in a streaming fashion.\n    Both are the same, except that the streaming version streams the last message as a stream.\n    Each have the same prompt template, which is defined in the `prompt_template` property.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    # Instantiate settings\n    brain_settings: BaseSettings = BrainSettings()\n\n    # Default class attributes\n    model: str = \"gpt-3.5-turbo-0125\"  # pyright: ignore reportPrivateUsage=none\n    temperature: float = 0.1\n    chat_id: str = None  # pyright: ignore reportPrivateUsage=none\n    brain_id: str = None  # pyright: ignore reportPrivateUsage=none\n    max_tokens: int = 2000\n    max_input: int = 2000\n    streaming: bool = False\n    knowledge_qa: Optional[RAGInterface] = None\n    brain: Optional[BrainEntity] = None\n    user_id: str = None\n    user_email: str = None\n    user_usage: Optional[UserUsage] = None\n    user_settings: Optional[dict] = None\n    models_settings: Optional[List[dict]] = None\n    metadata: Optional[dict] = None\n\n    callbacks: List[AsyncIteratorCallbackHandler] = (\n        None  # pyright: ignore reportPrivateUsage=none\n    )\n\n    prompt_id: Optional[UUID] = None\n\n    def __init__(\n        self,\n        brain_id: str,\n        chat_id: str,\n        streaming: bool = False,\n        prompt_id: Optional[UUID] = None,\n        metadata: Optional[dict] = None,\n        user_id: str = None,\n        user_email: str = None,\n        cost: int = 100,\n        **kwargs,\n    ):\n        super().__init__(\n            brain_id=brain_id,\n            chat_id=chat_id,\n            streaming=streaming,\n            **kwargs,\n        )\n        self.prompt_id = prompt_id\n        self.user_id = user_id\n        self.user_email = user_email\n        self.user_usage = UserUsage(\n            id=user_id,\n            email=user_email,\n        )\n        self.brain = brain_service.get_brain_by_id(brain_id)\n\n        self.user_settings = self.user_usage.get_user_settings()\n\n        # Get Model settings for the user\n        self.models_settings = self.user_usage.get_model_settings()\n        self.increase_usage_user()\n        self.knowledge_qa = QuivrRAG(\n            model=self.brain.model if self.brain.model else self.model,\n            brain_id=brain_id,\n            chat_id=chat_id,\n            streaming=streaming,\n            max_input=self.max_input,\n            max_tokens=self.max_tokens,\n            **kwargs,\n        )  # type: ignore\n\n    @property\n    def prompt_to_use(self):\n        if self.brain_id and is_valid_uuid(self.brain_id):\n            return get_prompt_to_use(UUID(self.brain_id), self.prompt_id)\n        else:\n            return None\n\n    @property\n    def prompt_to_use_id(self) -> Optional[UUID]:\n        # TODO: move to prompt service or instruction or something\n        if self.brain_id and is_valid_uuid(self.brain_id):\n            return get_prompt_to_use_id(UUID(self.brain_id), self.prompt_id)\n        else:\n            return None\n\n    def filter_history(\n        self, chat_history, max_history: int = 10, max_tokens: int = 2000\n    ):\n        \"\"\"\n        Filter out the chat history to only include the messages that are relevant to the current question\n\n        Takes in a chat_history= [HumanMessage(content='Qui est Chlo\u00e9 ? '), AIMessage(content=\"Chlo\u00e9 est une salari\u00e9e travaillant pour l'entreprise Quivr en tant qu'AI Engineer, sous la direction de son sup\u00e9rieur hi\u00e9rarchique, Stanislas Girard.\"), HumanMessage(content='Dis moi en plus sur elle'), AIMessage(content=''), HumanMessage(content='Dis moi en plus sur elle'), AIMessage(content=\"D\u00e9sol\u00e9, je n'ai pas d'autres informations sur Chlo\u00e9 \u00e0 partir des fichiers fournis.\")]\n        Returns a filtered chat_history with in priority: first max_tokens, then max_history where a Human message and an AI message count as one pair\n        a token is 4 characters\n        \"\"\"\n        chat_history = chat_history[::-1]\n        total_tokens = 0\n        total_pairs = 0\n        filtered_chat_history = []\n        for i in range(0, len(chat_history), 2):\n            if i + 1 < len(chat_history):\n                human_message = chat_history[i]\n                ai_message = chat_history[i + 1]\n                message_tokens = (\n                    len(human_message.content) + len(ai_message.content)\n                ) // 4\n                if (\n                    total_tokens + message_tokens > max_tokens\n                    or total_pairs >= max_history\n                ):\n                    break\n                filtered_chat_history.append(human_message)\n                filtered_chat_history.append(ai_message)\n                total_tokens += message_tokens\n                total_pairs += 1\n        chat_history = filtered_chat_history[::-1]\n\n        return chat_history\n\n    def increase_usage_user(self):\n        # Raises an error if the user has consumed all of of his credits\n\n        update_user_usage(\n            usage=self.user_usage,\n            user_settings=self.user_settings,\n            cost=self.calculate_pricing(),\n        )\n\n    def calculate_pricing(self):\n\n        model_to_use = find_model_and_generate_metadata(\n            self.chat_id,\n            self.brain.model,\n            self.user_settings,\n            self.models_settings,\n        )\n        self.model = model_to_use.name\n        self.max_input = model_to_use.max_input\n        self.max_tokens = model_to_use.max_output\n        user_choosen_model_price = 1000\n\n        for model_setting in self.models_settings:\n            if model_setting[\"name\"] == self.model:\n                user_choosen_model_price = model_setting[\"price\"]\n\n        return user_choosen_model_price\n\n    def generate_answer(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> GetChatHistoryOutput:\n        conversational_qa_chain = self.knowledge_qa.get_chain()\n        transformed_history, _ = self.initialize_streamed_chat_history(\n            chat_id, question\n        )\n        metadata = self.metadata or {}\n        citations = None\n        answer = \"\"\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        model_response = conversational_qa_chain.invoke(\n            {\n                \"question\": question.question,\n                \"chat_history\": transformed_history,\n                \"custom_personality\": (\n                    self.prompt_to_use.content if self.prompt_to_use else None\n                ),\n            },\n            config=config,\n        )\n\n        if self.model_compatible_with_function_calling(model=self.model):\n            if model_response[\"answer\"].tool_calls:\n                citations = model_response[\"answer\"].tool_calls[-1][\"args\"][\"citations\"]\n                followup_questions = model_response[\"answer\"].tool_calls[-1][\"args\"][\n                    \"followup_questions\"\n                ]\n                thoughts = model_response[\"answer\"].tool_calls[-1][\"args\"][\"thoughts\"]\n                if citations:\n                    citations = citations\n                if followup_questions:\n                    metadata[\"followup_questions\"] = followup_questions\n                if thoughts:\n                    metadata[\"thoughts\"] = thoughts\n                answer = model_response[\"answer\"].tool_calls[-1][\"args\"][\"answer\"]\n        else:\n            answer = model_response[\"answer\"].content\n        sources = model_response[\"docs\"] or []\n        if len(sources) > 0:\n            sources_list = generate_source(sources, self.brain_id, citations=citations)\n            serialized_sources_list = [source.dict() for source in sources_list]\n            metadata[\"sources\"] = serialized_sources_list\n\n        return self.save_non_streaming_answer(\n            chat_id=chat_id, question=question, answer=answer, metadata=metadata\n        )\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        if hasattr(self, \"get_chain\") and callable(getattr(self, \"get_chain\")):\n            conversational_qa_chain = self.get_chain()\n        else:\n            conversational_qa_chain = self.knowledge_qa.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        response_tokens = \"\"\n        sources = []\n        citations = []\n        first = True\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        async for chunk in conversational_qa_chain.astream(\n            {\n                \"question\": question.question,\n                \"chat_history\": transformed_history,\n                \"custom_personality\": (\n                    self.prompt_to_use.content if self.prompt_to_use else None\n                ),\n            },\n            config=config,\n        ):\n            if not streamed_chat_history.metadata:\n                streamed_chat_history.metadata = {}\n            if self.model_compatible_with_function_calling(model=self.model):\n                if chunk.get(\"answer\"):\n                    if first:\n                        gathered = chunk[\"answer\"]\n                        first = False\n                    else:\n                        gathered = gathered + chunk[\"answer\"]\n                        if (\n                            gathered.tool_calls\n                            and gathered.tool_calls[-1].get(\"args\")\n                            and \"answer\" in gathered.tool_calls[-1][\"args\"]\n                        ):\n                            # Only send the difference between answer and response_tokens which was the previous answer\n                            answer = gathered.tool_calls[-1][\"args\"][\"answer\"]\n                            difference = answer[len(response_tokens) :]\n                            streamed_chat_history.assistant = difference\n                            response_tokens = answer\n\n                            yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n                        if (\n                            gathered.tool_calls\n                            and gathered.tool_calls[-1].get(\"args\")\n                            and \"citations\" in gathered.tool_calls[-1][\"args\"]\n                        ):\n                            citations = gathered.tool_calls[-1][\"args\"][\"citations\"]\n                        if (\n                            gathered.tool_calls\n                            and gathered.tool_calls[-1].get(\"args\")\n                            and \"followup_questions\" in gathered.tool_calls[-1][\"args\"]\n                        ):\n                            followup_questions = gathered.tool_calls[-1][\"args\"][\n                                \"followup_questions\"\n                            ]\n                            streamed_chat_history.metadata[\"followup_questions\"] = (\n                                followup_questions\n                            )\n                        if (\n                            gathered.tool_calls\n                            and gathered.tool_calls[-1].get(\"args\")\n                            and \"thoughts\" in gathered.tool_calls[-1][\"args\"]\n                        ):\n                            thoughts = gathered.tool_calls[-1][\"args\"][\"thoughts\"]\n                            streamed_chat_history.metadata[\"thoughts\"] = thoughts\n            else:\n                if chunk.get(\"answer\"):\n                    response_tokens += chunk[\"answer\"].content\n                    streamed_chat_history.assistant = chunk[\"answer\"].content\n                    yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n            if chunk.get(\"docs\"):\n                sources = chunk[\"docs\"]\n\n        sources_list = generate_source(sources, self.brain_id, citations)\n\n        # Serialize the sources list\n        serialized_sources_list = [source.dict() for source in sources_list]\n        streamed_chat_history.metadata[\"sources\"] = serialized_sources_list\n        yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n\n    def initialize_streamed_chat_history(self, chat_id, question):\n        history = chat_service.get_chat_history(self.chat_id)\n        transformed_history = format_chat_history(history)\n        brain = brain_service.get_brain_by_id(self.brain_id)\n\n        streamed_chat_history = chat_service.update_chat_history(\n            CreateChatHistory(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"brain_id\": brain.brain_id,\n                    \"prompt_id\": self.prompt_to_use_id,\n                }\n            )\n        )\n\n        streamed_chat_history = GetChatHistoryOutput(\n            **{\n                \"chat_id\": str(chat_id),\n                \"message_id\": streamed_chat_history.message_id,\n                \"message_time\": streamed_chat_history.message_time,\n                \"user_message\": question.question,\n                \"assistant\": \"\",\n                \"prompt_title\": (\n                    self.prompt_to_use.title if self.prompt_to_use else None\n                ),\n                \"brain_name\": brain.name if brain else None,\n                \"brain_id\": str(brain.brain_id) if brain else None,\n                \"metadata\": self.metadata,\n            }\n        )\n\n        return transformed_history, streamed_chat_history\n\n    def save_answer(\n        self, question, response_tokens, streamed_chat_history, save_answer\n    ):\n        assistant = \"\".join(response_tokens)\n\n        try:\n            if save_answer:\n                chat_service.update_message_by_id(\n                    message_id=str(streamed_chat_history.message_id),\n                    user_message=question.question,\n                    assistant=assistant,\n                    metadata=streamed_chat_history.metadata,\n                )\n        except Exception as e:\n            logger.error(\"Error updating message by ID: %s\", e)\n\n    def save_non_streaming_answer(self, chat_id, question, answer, metadata):\n        new_chat = chat_service.update_chat_history(\n            CreateChatHistory(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": answer,\n                    \"brain_id\": self.brain.brain_id,\n                    \"prompt_id\": self.prompt_to_use_id,\n                    \"metadata\": metadata,\n                }\n            )\n        )\n\n        return GetChatHistoryOutput(\n            **{\n                \"chat_id\": chat_id,\n                \"user_message\": question.question,\n                \"assistant\": answer,\n                \"message_time\": new_chat.message_time,\n                \"prompt_title\": (\n                    self.prompt_to_use.title if self.prompt_to_use else None\n                ),\n                \"brain_name\": self.brain.name if self.brain else None,\n                \"message_id\": new_chat.message_id,\n                \"brain_id\": str(self.brain.brain_id) if self.brain else None,\n                \"metadata\": metadata,\n            }\n        )\n", "backend/modules/brain/qa_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.chat.dto.chats import ChatQuestion\n\n\nclass QAInterface(ABC):\n    \"\"\"\n    Abstract class for all QA interfaces.\n    This can be used to implement custom answer generation logic.\n    \"\"\"\n\n    @abstractmethod\n    def calculate_pricing(self):\n        raise NotImplementedError(\n            \"calculate_pricing is an abstract method and must be implemented\"\n        )\n\n    @abstractmethod\n    def generate_answer(\n        self,\n        chat_id: UUID,\n        question: ChatQuestion,\n        save_answer: bool,\n        *custom_params: tuple\n    ):\n        raise NotImplementedError(\n            \"generate_answer is an abstract method and must be implemented\"\n        )\n\n    @abstractmethod\n    def generate_stream(\n        self,\n        chat_id: UUID,\n        question: ChatQuestion,\n        save_answer: bool,\n        *custom_params: tuple\n    ):\n        raise NotImplementedError(\n            \"generate_stream is an abstract method and must be implemented\"\n        )\n\n    def model_compatible_with_function_calling(self, model: str):\n        if model in [\n            \"gpt-4o\",\n            \"gpt-4-turbo\",\n            \"gpt-4-turbo-2024-04-09\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-0125-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4\",\n            \"gpt-4-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0613\",\n        ]:\n            return True\n        return False\n", "backend/modules/brain/api_brain_qa.py": "import json\nfrom typing import Optional\nfrom uuid import UUID\n\nimport jq\nimport requests\nfrom fastapi import HTTPException\nfrom litellm import completion\nfrom modules.brain.service.call_brain_api import call_brain_api\nfrom modules.brain.service.get_api_brain_definition_as_json_schema import (\n    get_api_brain_definition_as_json_schema,\n)\nfrom logger import get_logger\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.brain.qa_interface import QAInterface\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.dto.chats import ChatQuestion\nfrom modules.chat.dto.inputs import CreateChatHistory\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.service.chat_service import ChatService\n\nbrain_service = BrainService()\nchat_service = ChatService()\n\nlogger = get_logger(__name__)\n\n\nclass UUIDEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, UUID):\n            # if the object is uuid, we simply return the value of uuid\n            return str(obj)\n        return super().default(obj)\n\n\nclass APIBrainQA(KnowledgeBrainQA, QAInterface):\n    user_id: UUID\n    raw: bool = False\n    jq_instructions: Optional[str] = None\n\n    def __init__(\n        self,\n        model: str,\n        brain_id: str,\n        chat_id: str,\n        streaming: bool = False,\n        prompt_id: Optional[UUID] = None,\n        raw: bool = False,\n        jq_instructions: Optional[str] = None,\n        **kwargs,\n    ):\n        user_id = kwargs.get(\"user_id\")\n        if not user_id:\n            raise HTTPException(status_code=400, detail=\"Cannot find user id\")\n\n        super().__init__(\n            model=model,\n            brain_id=brain_id,\n            chat_id=chat_id,\n            streaming=streaming,\n            prompt_id=prompt_id,\n            **kwargs,\n        )\n        self.user_id = user_id\n        self.raw = raw\n        self.jq_instructions = jq_instructions\n\n    def get_api_call_response_as_text(\n        self, method, api_url, params, search_params, secrets\n    ) -> str:\n        headers = {}\n\n        api_url_with_search_params = api_url\n        if search_params:\n            api_url_with_search_params += \"?\"\n            for search_param in search_params:\n                api_url_with_search_params += (\n                    f\"{search_param}={search_params[search_param]}&\"\n                )\n\n        for secret in secrets:\n            headers[secret] = secrets[secret]\n\n        try:\n            if method in [\"GET\", \"DELETE\"]:\n                response = requests.request(\n                    method,\n                    url=api_url_with_search_params,\n                    params=params or None,\n                    headers=headers or None,\n                )\n            elif method in [\"POST\", \"PUT\", \"PATCH\"]:\n                response = requests.request(\n                    method,\n                    url=api_url_with_search_params,\n                    json=params or None,\n                    headers=headers or None,\n                )\n            else:\n                raise ValueError(f\"Invalid method: {method}\")\n\n            return response.text\n\n        except Exception as e:\n            logger.error(f\"Error calling API: {e}\")\n            return None\n\n    def log_steps(self, message: str, type: str):\n        if \"api\" not in self.metadata:\n            self.metadata[\"api\"] = {}\n        if \"steps\" not in self.metadata[\"api\"]:\n            self.metadata[\"api\"][\"steps\"] = []\n        self.metadata[\"api\"][\"steps\"].append(\n            {\n                \"number\": len(self.metadata[\"api\"][\"steps\"]),\n                \"type\": type,\n                \"message\": message,\n            }\n        )\n\n    async def make_completion(\n        self,\n        messages,\n        functions,\n        brain_id: UUID,\n        recursive_count=0,\n        should_log_steps=True,\n    ) -> str | None:\n        if recursive_count > 5:\n            self.log_steps(\n                \"The assistant is having issues and took more than 5 calls to the API. Please try again later or an other instruction.\",\n                \"error\",\n            )\n            return\n\n        if \"api\" not in self.metadata:\n            self.metadata[\"api\"] = {}\n            if \"raw\" not in self.metadata[\"api\"]:\n                self.metadata[\"api\"][\"raw_enabled\"] = self.raw\n\n        response = completion(\n            model=self.model,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            messages=messages,\n            functions=functions,\n            stream=True,\n            function_call=\"auto\",\n        )\n\n        function_call = {\n            \"name\": None,\n            \"arguments\": \"\",\n        }\n        for chunk in response:\n            finish_reason = chunk.choices[0].finish_reason\n            if finish_reason == \"stop\":\n                self.log_steps(\"Quivr has finished\", \"info\")\n                break\n            if (\n                \"function_call\" in chunk.choices[0].delta\n                and chunk.choices[0].delta[\"function_call\"]\n            ):\n                if chunk.choices[0].delta[\"function_call\"].name:\n                    function_call[\"name\"] = chunk.choices[0].delta[\"function_call\"].name\n                if chunk.choices[0].delta[\"function_call\"].arguments:\n                    function_call[\"arguments\"] += (\n                        chunk.choices[0].delta[\"function_call\"].arguments\n                    )\n\n            elif finish_reason == \"function_call\":\n                try:\n                    arguments = json.loads(function_call[\"arguments\"])\n\n                except Exception:\n                    self.log_steps(f\"Issues with {arguments}\", \"error\")\n                    arguments = {}\n\n                self.log_steps(f\"Calling {brain_id} with arguments {arguments}\", \"info\")\n\n                try:\n                    api_call_response = call_brain_api(\n                        brain_id=brain_id,\n                        user_id=self.user_id,\n                        arguments=arguments,\n                    )\n                except Exception as e:\n                    logger.info(f\"Error while calling API: {e}\")\n                    api_call_response = f\"Error while calling API: {e}\"\n                function_name = function_call[\"name\"]\n                self.log_steps(\"Quivr has called the API\", \"info\")\n                messages.append(\n                    {\n                        \"role\": \"function\",\n                        \"name\": function_call[\"name\"],\n                        \"content\": f\"The function {function_name} was called and gave The following answer:(data from function) {api_call_response} (end of data from function). Don't call this function again unless there was an error or extremely necessary and asked specifically by the user. If an error, display it to the user in raw.\",\n                    }\n                )\n\n                self.metadata[\"api\"][\"raw_response\"] = json.loads(api_call_response)\n                if self.raw:\n                    # Yield the raw response in a format that can then be catched by the generate_stream function\n                    response_to_yield = f\"````raw_response: {api_call_response}````\"\n\n                    yield response_to_yield\n                    return\n\n                async for value in self.make_completion(\n                    messages=messages,\n                    functions=functions,\n                    brain_id=brain_id,\n                    recursive_count=recursive_count + 1,\n                    should_log_steps=should_log_steps,\n                ):\n                    yield value\n\n            else:\n                if (\n                    hasattr(chunk.choices[0], \"delta\")\n                    and chunk.choices[0].delta\n                    and hasattr(chunk.choices[0].delta, \"content\")\n                ):\n                    content = chunk.choices[0].delta.content\n                    yield content\n                else:  # pragma: no cover\n                    yield \"**...**\"\n                    break\n\n    async def generate_stream(\n        self,\n        chat_id: UUID,\n        question: ChatQuestion,\n        save_answer: bool = True,\n        should_log_steps: Optional[bool] = True,\n    ):\n        brain = brain_service.get_brain_by_id(self.brain_id)\n\n        if not brain:\n            raise HTTPException(status_code=404, detail=\"Brain not found\")\n\n        prompt_content = \"You are a helpful assistant that can access functions to help answer questions. If there are information missing in the question, you can ask follow up questions to get more information to the user. Once all the information is available, you can call the function to get the answer.\"\n\n        if self.prompt_to_use:\n            prompt_content += self.prompt_to_use.content\n\n        messages = [{\"role\": \"system\", \"content\": prompt_content}]\n\n        history = chat_service.get_chat_history(self.chat_id)\n\n        for message in history:\n            formatted_message = [\n                {\"role\": \"user\", \"content\": message.user_message},\n                {\"role\": \"assistant\", \"content\": message.assistant},\n            ]\n            messages.extend(formatted_message)\n\n        messages.append({\"role\": \"user\", \"content\": question.question})\n\n        if save_answer:\n            streamed_chat_history = chat_service.update_chat_history(\n                CreateChatHistory(\n                    **{\n                        \"chat_id\": chat_id,\n                        \"user_message\": question.question,\n                        \"assistant\": \"\",\n                        \"brain_id\": self.brain_id,\n                        \"prompt_id\": self.prompt_to_use_id,\n                    }\n                )\n            )\n            streamed_chat_history = GetChatHistoryOutput(\n                **{\n                    \"chat_id\": str(chat_id),\n                    \"message_id\": streamed_chat_history.message_id,\n                    \"message_time\": streamed_chat_history.message_time,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"prompt_title\": self.prompt_to_use.title\n                    if self.prompt_to_use\n                    else None,\n                    \"brain_name\": brain.name if brain else None,\n                    \"brain_id\": str(self.brain_id),\n                    \"metadata\": self.metadata,\n                }\n            )\n        else:\n            streamed_chat_history = GetChatHistoryOutput(\n                **{\n                    \"chat_id\": str(chat_id),\n                    \"message_id\": None,\n                    \"message_time\": None,\n                    \"user_message\": question.question,\n                    \"assistant\": \"\",\n                    \"prompt_title\": self.prompt_to_use.title\n                    if self.prompt_to_use\n                    else None,\n                    \"brain_name\": brain.name if brain else None,\n                    \"brain_id\": str(self.brain_id),\n                    \"metadata\": self.metadata,\n                }\n            )\n        response_tokens = []\n        async for value in self.make_completion(\n            messages=messages,\n            functions=[get_api_brain_definition_as_json_schema(brain)],\n            brain_id=self.brain_id,\n            should_log_steps=should_log_steps,\n        ):\n            # Look if the value is a raw response\n            if value.startswith(\"````raw_response:\"):\n                raw_value_cleaned = value.replace(\"````raw_response: \", \"\").replace(\n                    \"````\", \"\"\n                )\n                logger.info(f\"Raw response: {raw_value_cleaned}\")\n                if self.jq_instructions:\n                    json_raw_value_cleaned = json.loads(raw_value_cleaned)\n                    raw_value_cleaned = (\n                        jq.compile(self.jq_instructions)\n                        .input_value(json_raw_value_cleaned)\n                        .first()\n                    )\n                streamed_chat_history.assistant = raw_value_cleaned\n                response_tokens.append(raw_value_cleaned)\n                yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n            else:\n                streamed_chat_history.assistant = value\n                response_tokens.append(value)\n                yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        if save_answer:\n            chat_service.update_message_by_id(\n                message_id=str(streamed_chat_history.message_id),\n                user_message=question.question,\n                assistant=\"\".join(str(token) for token in response_tokens),\n                metadata=self.metadata,\n            )\n\n    def make_completion_without_streaming(\n        self,\n        messages,\n        functions,\n        brain_id: UUID,\n        recursive_count=0,\n        should_log_steps=False,\n    ):\n        if recursive_count > 5:\n            print(\n                \"The assistant is having issues and took more than 5 calls to the API. Please try again later or an other instruction.\"\n            )\n            return\n\n        if should_log_steps:\n            print(\"\ud83e\udde0<Deciding what to do>\ud83e\udde0\")\n\n        response = completion(\n            model=self.model,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            messages=messages,\n            functions=functions,\n            stream=False,\n            function_call=\"auto\",\n        )\n\n        response_message = response.choices[0].message\n        finish_reason = response.choices[0].finish_reason\n\n        if finish_reason == \"function_call\":\n            function_call = response_message.function_call\n            try:\n                arguments = json.loads(function_call.arguments)\n\n            except Exception:\n                arguments = {}\n\n            if should_log_steps:\n                self.log_steps(f\"Calling {brain_id} with arguments {arguments}\", \"info\")\n\n            try:\n                api_call_response = call_brain_api(\n                    brain_id=brain_id,\n                    user_id=self.user_id,\n                    arguments=arguments,\n                )\n            except Exception as e:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Error while calling API: {e}\",\n                )\n\n            function_name = function_call.name\n            messages.append(\n                {\n                    \"role\": \"function\",\n                    \"name\": function_call.name,\n                    \"content\": f\"The function {function_name} was called and gave The following answer:(data from function) {api_call_response} (end of data from function). Don't call this function again unless there was an error or extremely necessary and asked specifically by the user.\",\n                }\n            )\n\n            return self.make_completion_without_streaming(\n                messages=messages,\n                functions=functions,\n                brain_id=brain_id,\n                recursive_count=recursive_count + 1,\n                should_log_steps=should_log_steps,\n            )\n\n        if finish_reason == \"stop\":\n            return response_message\n\n        else:\n            print(\"Never ending completion\")\n\n    def generate_answer(\n        self,\n        chat_id: UUID,\n        question: ChatQuestion,\n        save_answer: bool = True,\n        raw: bool = True,\n    ):\n        if not self.brain_id:\n            raise HTTPException(\n                status_code=400, detail=\"No brain id provided in the question\"\n            )\n\n        brain = brain_service.get_brain_by_id(self.brain_id)\n\n        if not brain:\n            raise HTTPException(status_code=404, detail=\"Brain not found\")\n\n        prompt_content = \"You are a helpful assistant that can access functions to help answer questions. If there are information missing in the question, you can ask follow up questions to get more information to the user. Once all the information is available, you can call the function to get the answer.\"\n\n        if self.prompt_to_use:\n            prompt_content += self.prompt_to_use.content\n\n        messages = [{\"role\": \"system\", \"content\": prompt_content}]\n\n        history = chat_service.get_chat_history(self.chat_id)\n\n        for message in history:\n            formatted_message = [\n                {\"role\": \"user\", \"content\": message.user_message},\n                {\"role\": \"assistant\", \"content\": message.assistant},\n            ]\n            messages.extend(formatted_message)\n\n        messages.append({\"role\": \"user\", \"content\": question.question})\n\n        response = self.make_completion_without_streaming(\n            messages=messages,\n            functions=[get_api_brain_definition_as_json_schema(brain)],\n            brain_id=self.brain_id,\n            should_log_steps=False,\n            raw=raw,\n        )\n\n        answer = response.content\n        if save_answer:\n            new_chat = chat_service.update_chat_history(\n                CreateChatHistory(\n                    **{\n                        \"chat_id\": chat_id,\n                        \"user_message\": question.question,\n                        \"assistant\": answer,\n                        \"brain_id\": self.brain_id,\n                        \"prompt_id\": self.prompt_to_use_id,\n                    }\n                )\n            )\n\n            return GetChatHistoryOutput(\n                **{\n                    \"chat_id\": chat_id,\n                    \"user_message\": question.question,\n                    \"assistant\": answer,\n                    \"message_time\": new_chat.message_time,\n                    \"prompt_title\": self.prompt_to_use.title\n                    if self.prompt_to_use\n                    else None,\n                    \"brain_name\": brain.name if brain else None,\n                    \"message_id\": new_chat.message_id,\n                    \"metadata\": self.metadata,\n                    \"brain_id\": str(self.brain_id),\n                }\n            )\n        return GetChatHistoryOutput(\n            **{\n                \"chat_id\": chat_id,\n                \"user_message\": question.question,\n                \"assistant\": answer,\n                \"message_time\": \"123\",\n                \"prompt_title\": None,\n                \"brain_name\": brain.name,\n                \"message_id\": None,\n                \"metadata\": self.metadata,\n                \"brain_id\": str(self.brain_id),\n            }\n        )\n", "backend/modules/brain/__init__.py": "", "backend/modules/brain/controller/brain_routes.py": "from typing import Dict\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request\nfrom logger import get_logger\nfrom middlewares.auth.auth_bearer import AuthBearer, get_current_user\nfrom modules.brain.dto.inputs import (\n    BrainQuestionRequest,\n    BrainUpdatableProperties,\n    CreateBrainProperties,\n)\nfrom modules.brain.entity.brain_entity import PublicBrain, RoleEnum\nfrom modules.brain.entity.integration_brain import IntegrationDescriptionEntity\nfrom modules.brain.service.brain_authorization_service import has_brain_authorization\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.brain.service.brain_user_service import BrainUserService\nfrom modules.brain.service.get_question_context_from_brain import (\n    get_question_context_from_brain,\n)\nfrom modules.brain.service.integration_brain_service import (\n    IntegrationBrainDescriptionService,\n)\nfrom modules.prompt.service.prompt_service import PromptService\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_usage import UserUsage\nfrom packages.utils.telemetry import maybe_send_telemetry\n\nlogger = get_logger(__name__)\nbrain_router = APIRouter()\n\nprompt_service = PromptService()\nbrain_service = BrainService()\nbrain_user_service = BrainUserService()\nintegration_brain_description_service = IntegrationBrainDescriptionService()\n\n\n@brain_router.get(\n    \"/brains/integrations/\",\n    dependencies=[Depends(AuthBearer())],\n)\nasync def get_integration_brain_description() -> list[IntegrationDescriptionEntity]:\n    \"\"\"Retrieve the integration brain description.\"\"\"\n    return integration_brain_description_service.get_all_integration_descriptions()\n\n\n@brain_router.get(\"/brains/\", dependencies=[Depends(AuthBearer())], tags=[\"Brain\"])\nasync def retrieve_all_brains_for_user(\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"Retrieve all brains for the current user.\"\"\"\n    brains = brain_user_service.get_user_brains(current_user.id)\n    return {\"brains\": brains}\n\n\n@brain_router.get(\n    \"/brains/public\", dependencies=[Depends(AuthBearer())], tags=[\"Brain\"]\n)\nasync def retrieve_public_brains() -> list[PublicBrain]:\n    \"\"\"Retrieve all Quivr public brains.\"\"\"\n    return brain_service.get_public_brains()\n\n\n@brain_router.get(\n    \"/brains/{brain_id}/\",\n    dependencies=[\n        Depends(AuthBearer()),\n        Depends(\n            has_brain_authorization(\n                required_roles=[RoleEnum.Owner, RoleEnum.Editor, RoleEnum.Viewer]\n            )\n        ),\n    ],\n    tags=[\"Brain\"],\n)\nasync def retrieve_brain_by_id(\n    brain_id: UUID,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"Retrieve details of a specific brain by its ID.\"\"\"\n    brain_details = brain_service.get_brain_details(brain_id, current_user.id)\n    if brain_details is None:\n        raise HTTPException(status_code=404, detail=\"Brain details not found\")\n    return brain_details\n\n\n@brain_router.post(\"/brains/\", dependencies=[Depends(AuthBearer())], tags=[\"Brain\"])\nasync def create_new_brain(\n    brain: CreateBrainProperties,\n    request: Request,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"Create a new brain for the user.\"\"\"\n    user_brains = brain_user_service.get_user_brains(current_user.id)\n    user_usage = UserUsage(\n        id=current_user.id,\n        email=current_user.email,\n    )\n    user_settings = user_usage.get_user_settings()\n\n    if len(user_brains) >= user_settings.get(\"max_brains\", 5):\n        raise HTTPException(\n            status_code=429,\n            detail=f\"Maximum number of brains reached ({user_settings.get('max_brains', 5)}).\",\n        )\n    maybe_send_telemetry(\"create_brain\", {\"brain_name\": brain.name}, request)\n    new_brain = brain_service.create_brain(\n        brain=brain,\n        user_id=current_user.id,\n    )\n    brain_user_service.create_brain_user(\n        user_id=current_user.id,\n        brain_id=new_brain.brain_id,\n        rights=RoleEnum.Owner,\n        is_default_brain=True,\n    )\n\n    return {\"id\": new_brain.brain_id, \"name\": brain.name, \"rights\": \"Owner\"}\n\n\n@brain_router.put(\n    \"/brains/{brain_id}/\",\n    dependencies=[\n        Depends(AuthBearer()),\n        Depends(has_brain_authorization([RoleEnum.Editor, RoleEnum.Owner])),\n    ],\n    tags=[\"Brain\"],\n)\nasync def update_existing_brain(\n    brain_id: UUID,\n    brain_update_data: BrainUpdatableProperties,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"Update an existing brain's configuration.\"\"\"\n    existing_brain = brain_service.get_brain_details(brain_id, current_user.id)\n    if existing_brain is None:\n        raise HTTPException(status_code=404, detail=\"Brain not found\")\n\n    if brain_update_data.prompt_id is None and existing_brain.prompt_id:\n        prompt = prompt_service.get_prompt_by_id(existing_brain.prompt_id)\n        if prompt and prompt.status == \"private\":\n            prompt_service.delete_prompt_by_id(existing_brain.prompt_id)\n\n            return {\"message\": f\"Prompt {brain_id} has been updated.\"}\n\n    elif brain_update_data.status == \"private\" and existing_brain.status == \"public\":\n        brain_user_service.delete_brain_users(brain_id)\n        return {\"message\": f\"Brain {brain_id} has been deleted.\"}\n\n    else:\n        brain_service.update_brain_by_id(brain_id, brain_update_data)\n\n        return {\"message\": f\"Brain {brain_id} has been updated.\"}\n\n\n@brain_router.put(\n    \"/brains/{brain_id}/secrets-values\",\n    dependencies=[\n        Depends(AuthBearer()),\n    ],\n    tags=[\"Brain\"],\n)\nasync def update_existing_brain_secrets(\n    brain_id: UUID,\n    secrets: Dict[str, str],\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    \"\"\"Update an existing brain's secrets.\"\"\"\n\n    existing_brain = brain_service.get_brain_details(brain_id, None)\n\n    if existing_brain is None:\n        raise HTTPException(status_code=404, detail=\"Brain not found\")\n\n    if (\n        existing_brain.brain_definition is None\n        or existing_brain.brain_definition.secrets is None\n    ):\n        raise HTTPException(\n            status_code=400,\n            detail=\"This brain does not support secrets.\",\n        )\n\n    is_brain_user = (\n        brain_user_service.get_brain_for_user(\n            user_id=current_user.id,\n            brain_id=brain_id,\n        )\n        is not None\n    )\n\n    if not is_brain_user:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You are not authorized to update this brain.\",\n        )\n\n    secrets_names = [secret.name for secret in existing_brain.brain_definition.secrets]\n\n    for key, value in secrets.items():\n        if key not in secrets_names:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Secret {key} is not a valid secret.\",\n            )\n        if value:\n            brain_service.update_secret_value(\n                user_id=current_user.id,\n                brain_id=brain_id,\n                secret_name=key,\n                secret_value=value,\n            )\n\n    return {\"message\": f\"Brain {brain_id} has been updated.\"}\n\n\n@brain_router.post(\n    \"/brains/{brain_id}/documents\",\n    dependencies=[Depends(AuthBearer()), Depends(has_brain_authorization())],\n    tags=[\"Brain\"],\n)\nasync def get_question_context_for_brain(\n    brain_id: UUID, question: BrainQuestionRequest\n):\n    # TODO: Move this endpoint to AnswerGenerator service\n    \"\"\"Retrieve the question context from a specific brain.\"\"\"\n    context = get_question_context_from_brain(brain_id, question.question)\n    return {\"docs\": context}\n", "backend/modules/brain/controller/__init__.py": "from .brain_routes import brain_router\n", "backend/modules/brain/repository/brains.py": "from uuid import UUID\n\nfrom sqlalchemy import text\n\nfrom logger import get_logger\nfrom models.settings import get_embeddings, get_pg_database_engine, get_supabase_client\nfrom modules.brain.dto.inputs import BrainUpdatableProperties\nfrom modules.brain.entity.brain_entity import BrainEntity, PublicBrain\nfrom modules.brain.repository.interfaces.brains_interface import BrainsInterface\n\nlogger = get_logger(__name__)\n\n\nclass Brains(BrainsInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n        pg_engine = get_pg_database_engine()\n        self.pg_engine = pg_engine\n\n    def create_brain(self, brain):\n        embeddings = get_embeddings()\n        string_to_embed = f\"Name: {brain.name} Description: {brain.description}\"\n        brain_meaning = embeddings.embed_query(string_to_embed)\n        brain_dict = brain.dict(\n            exclude={\n                \"brain_definition\",\n                \"brain_secrets_values\",\n                \"connected_brains_ids\",\n                \"integration\",\n            }\n        )\n        brain_dict[\"meaning\"] = brain_meaning\n        response = (self.db.table(\"brains\").insert(brain_dict)).execute()\n\n        return BrainEntity(**response.data[0])\n\n    def get_public_brains(self):\n        response = (\n            self.db.from_(\"brains\")\n            .select(\n                \"id:brain_id, name, description, last_update, brain_type, brain_definition: api_brain_definition(*), number_of_subscribers:brains_users(count)\"\n            )\n            .filter(\"status\", \"eq\", \"public\")\n            .execute()\n        )\n        public_brains: list[PublicBrain] = []\n\n        for item in response.data:\n            item[\"number_of_subscribers\"] = item[\"number_of_subscribers\"][0][\"count\"]\n            if not item[\"brain_definition\"]:\n                del item[\"brain_definition\"]\n            else:\n                item[\"brain_definition\"][\"secrets\"] = []\n\n            public_brains.append(PublicBrain(**item))\n        return public_brains\n\n    def update_brain_last_update_time(self, brain_id):\n        try:\n            with self.pg_engine.begin() as connection:\n                query = \"\"\"\n                UPDATE brains\n                SET last_update = now()\n                WHERE brain_id = '{brain_id}'\n                \"\"\"\n                connection.execute(text(query.format(brain_id=brain_id)))\n        except Exception as e:\n            logger.error(e)\n\n    def get_brain_details(self, brain_id):\n        with self.pg_engine.begin() as connection:\n            query = \"\"\"\n            SELECT * FROM brains\n            WHERE brain_id = '{brain_id}'\n            \"\"\"\n            response = connection.execute(text(query.format(brain_id=brain_id))).fetchall()\n        if len(response) == 0:\n            return None\n        return BrainEntity(**response[0]._mapping)\n\n    def delete_brain(self, brain_id: str):\n        with self.pg_engine.begin() as connection:\n            results = connection.execute(\n                text(f\"DELETE FROM brains WHERE brain_id = '{brain_id}'\")\n            )\n\n        return results\n\n    def update_brain_by_id(\n        self, brain_id: UUID, brain: BrainUpdatableProperties\n    ) -> BrainEntity | None:\n        embeddings = get_embeddings()\n        string_to_embed = f\"Name: {brain.name} Description: {brain.description}\"\n        brain_meaning = embeddings.embed_query(string_to_embed)\n        brain_dict = brain.dict(exclude_unset=True)\n        brain_dict[\"meaning\"] = brain_meaning\n        update_brain_response = (\n            self.db.table(\"brains\")\n            .update(brain_dict)\n            .match({\"brain_id\": brain_id})\n            .execute()\n        ).data\n\n        if len(update_brain_response) == 0:\n            return None\n\n        return BrainEntity(**update_brain_response[0])\n\n    def get_brain_by_id(self, brain_id: UUID) -> BrainEntity | None:\n        # TODO: merge this method with get_brain_details\n        with self.pg_engine.begin() as connection:\n            response = connection.execute(\n                text(f\"SELECT * FROM brains WHERE brain_id = '{brain_id}'\")\n            ).fetchall()\n\n        if len(response) == 0:\n            return None\n        return BrainEntity(**response[0]._mapping)", "backend/modules/brain/repository/external_api_secrets.py": "from uuid import UUID\n\nfrom models.settings import get_supabase_client\nfrom modules.brain.repository.interfaces.external_api_secrets_interface import (\n    ExternalApiSecretsInterface,\n)\n\n\ndef build_secret_unique_name(user_id: UUID, brain_id: UUID, secret_name: str):\n    return f\"{user_id}-{brain_id}-{secret_name}\"\n\n\nclass ExternalApiSecrets(ExternalApiSecretsInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def create_secret(\n        self, user_id: UUID, brain_id: UUID, secret_name: str, secret_value\n    ) -> UUID | None:\n        response = self.db.rpc(\n            \"insert_secret\",\n            {\n                \"name\": build_secret_unique_name(\n                    user_id=user_id, brain_id=brain_id, secret_name=secret_name\n                ),\n                \"secret\": secret_value,\n            },\n        ).execute()\n\n        return response.data\n\n    def read_secret(\n        self,\n        user_id: UUID,\n        brain_id: UUID,\n        secret_name: str,\n    ) -> UUID | None:\n        response = self.db.rpc(\n            \"read_secret\",\n            {\n                \"secret_name\": build_secret_unique_name(\n                    user_id=user_id, brain_id=brain_id, secret_name=secret_name\n                ),\n            },\n        ).execute()\n\n        return response.data\n\n    def delete_secret(self, user_id: UUID, brain_id: UUID, secret_name: str) -> bool:\n        response = self.db.rpc(\n            \"delete_secret\",\n            {\n                \"secret_name\": build_secret_unique_name(\n                    user_id=user_id, brain_id=brain_id, secret_name=secret_name\n                ),\n            },\n        ).execute()\n\n        return response.data\n", "backend/modules/brain/repository/api_brain_definitions.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom models.settings import get_supabase_client\nfrom modules.brain.dto.inputs import CreateApiBrainDefinition\nfrom modules.brain.entity.api_brain_definition_entity import ApiBrainDefinitionEntity\nfrom modules.brain.repository.interfaces import ApiBrainDefinitionsInterface\n\n\nclass ApiBrainDefinitions(ApiBrainDefinitionsInterface):\n    def __init__(self):\n        self.db = get_supabase_client()\n\n    def get_api_brain_definition(\n        self, brain_id: UUID\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        response = (\n            self.db.table(\"api_brain_definition\")\n            .select(\"*\")\n            .filter(\"brain_id\", \"eq\", brain_id)\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n\n        return ApiBrainDefinitionEntity(**response.data[0])\n\n    def add_api_brain_definition(\n        self, brain_id: UUID, api_brain_definition: CreateApiBrainDefinition\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        response = (\n            self.db.table(\"api_brain_definition\")\n            .insert([{\"brain_id\": str(brain_id), **api_brain_definition.dict()}])\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        return ApiBrainDefinitionEntity(**response.data[0])\n\n    def update_api_brain_definition(\n        self, brain_id: UUID, api_brain_definition: ApiBrainDefinitionEntity\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        response = (\n            self.db.table(\"api_brain_definition\")\n            .update(api_brain_definition.dict(exclude={\"brain_id\"}))\n            .filter(\"brain_id\", \"eq\", str(brain_id))\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        return ApiBrainDefinitionEntity(**response.data[0])\n\n    def delete_api_brain_definition(self, brain_id: UUID) -> None:\n        self.db.table(\"api_brain_definition\").delete().filter(\n            \"brain_id\", \"eq\", str(brain_id)\n        ).execute()\n", "backend/modules/brain/repository/composite_brains_connections.py": "from uuid import UUID\n\nfrom logger import get_logger\nfrom models.settings import get_supabase_client\nfrom modules.brain.entity.composite_brain_connection_entity import (\n    CompositeBrainConnectionEntity,\n)\nfrom modules.brain.repository.interfaces import CompositeBrainsConnectionsInterface\n\nlogger = get_logger(__name__)\n\n\nclass CompositeBrainsConnections(CompositeBrainsConnectionsInterface):\n    def __init__(self):\n        self.db = get_supabase_client()\n\n    def connect_brain(\n        self, composite_brain_id: UUID, connected_brain_id: UUID\n    ) -> CompositeBrainConnectionEntity:\n        response = (\n            self.db.table(\"composite_brain_connections\")\n            .insert(\n                {\n                    \"composite_brain_id\": str(composite_brain_id),\n                    \"connected_brain_id\": str(connected_brain_id),\n                }\n            )\n            .execute()\n        )\n\n        return CompositeBrainConnectionEntity(**response.data[0])\n\n    def get_connected_brains(self, composite_brain_id: UUID) -> list[UUID]:\n        response = (\n            self.db.from_(\"composite_brain_connections\")\n            .select(\"connected_brain_id\")\n            .filter(\"composite_brain_id\", \"eq\", str(composite_brain_id))\n            .execute()\n        )\n\n        return [item[\"connected_brain_id\"] for item in response.data]\n\n    def disconnect_brain(\n        self, composite_brain_id: UUID, connected_brain_id: UUID\n    ) -> None:\n        self.db.table(\"composite_brain_connections\").delete().match(\n            {\n                \"composite_brain_id\": composite_brain_id,\n                \"connected_brain_id\": connected_brain_id,\n            }\n        ).execute()\n\n    def is_connected_brain(self, brain_id: UUID) -> bool:\n        response = (\n            self.db.from_(\"composite_brain_connections\")\n            .select(\"connected_brain_id\")\n            .filter(\"connected_brain_id\", \"eq\", str(brain_id))\n            .execute()\n        )\n\n        return len(response.data) > 0\n", "backend/modules/brain/repository/brains_vectors.py": "from logger import get_logger\nfrom models.settings import get_supabase_client\nfrom modules.brain.repository.interfaces.brains_vectors_interface import (\n    BrainsVectorsInterface,\n)\n\nlogger = get_logger(__name__)\n\n\nclass BrainsVectors(BrainsVectorsInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def create_brain_vector(self, brain_id, vector_id, file_sha1):\n        response = (\n            self.db.table(\"brains_vectors\")\n            .insert(\n                {\n                    \"brain_id\": str(brain_id),\n                    \"vector_id\": str(vector_id),\n                    \"file_sha1\": file_sha1,\n                }\n            )\n            .execute()\n        )\n        return response.data\n\n    def get_vector_ids_from_file_sha1(self, file_sha1: str):\n        # move to vectors class\n        vectorsResponse = (\n            self.db.table(\"vectors\")\n            .select(\"id\")\n            .filter(\"file_sha1\", \"eq\", file_sha1)\n            .execute()\n        )\n        return vectorsResponse.data\n\n    def get_brain_vector_ids(self, brain_id):\n        \"\"\"\n        Retrieve unique brain data (i.e. uploaded files and crawled websites).\n        \"\"\"\n\n        response = (\n            self.db.from_(\"brains_vectors\")\n            .select(\"vector_id\")\n            .filter(\"brain_id\", \"eq\", brain_id)\n            .execute()\n        )\n\n        vector_ids = [item[\"vector_id\"] for item in response.data]\n\n        if len(vector_ids) == 0:\n            return []\n\n        return vector_ids\n\n    def delete_file_from_brain(self, brain_id, file_name: str):\n        # First, get the vector_ids associated with the file_name\n        # TODO: filter by brain_id\n        file_vectors = (\n            self.db.table(\"vectors\")\n            .select(\"id\")\n            .filter(\"metadata->>file_name\", \"eq\", file_name)\n            .execute()\n        )\n\n        file_vectors_ids = [item[\"id\"] for item in file_vectors.data]\n\n        # remove current file vectors from brain vectors\n        self.db.table(\"brains_vectors\").delete().filter(\n            \"vector_id\", \"in\", f\"({','.join(map(str, file_vectors_ids))})\"\n        ).filter(\"brain_id\", \"eq\", brain_id).execute()\n\n        vectors_used_by_another_brain = (\n            self.db.table(\"brains_vectors\")\n            .select(\"vector_id\")\n            .filter(\"vector_id\", \"in\", f\"({','.join(map(str, file_vectors_ids))})\")\n            .filter(\"brain_id\", \"neq\", brain_id)\n            .execute()\n        )\n\n        vectors_used_by_another_brain_ids = [\n            item[\"vector_id\"] for item in vectors_used_by_another_brain.data\n        ]\n\n        vectors_no_longer_used_ids = [\n            id for id in file_vectors_ids if id not in vectors_used_by_another_brain_ids\n        ]\n\n        self.db.table(\"vectors\").delete().filter(\n            \"id\", \"in\", f\"({','.join(map(str, vectors_no_longer_used_ids))})\"\n        ).execute()\n\n        return {\"message\": f\"File {file_name} in brain {brain_id} has been deleted.\"}\n\n    def delete_brain_vector(self, brain_id: str):\n        results = (\n            self.db.table(\"brains_vectors\")\n            .delete()\n            .match({\"brain_id\": brain_id})\n            .execute()\n        )\n\n        return results\n", "backend/modules/brain/repository/integration_brains.py": "from abc import ABC, abstractmethod\nfrom typing import List\n\nfrom models.settings import get_supabase_client\nfrom modules.brain.entity.integration_brain import (\n    IntegrationDescriptionEntity,\n    IntegrationEntity,\n)\nfrom modules.brain.repository.interfaces.integration_brains_interface import (\n    IntegrationBrainInterface,\n    IntegrationDescriptionInterface,\n)\n\n\nclass Integration(ABC):\n\n    @abstractmethod\n    def load(self):\n        pass\n\n    @abstractmethod\n    def poll(self):\n        pass\n\n\nclass IntegrationBrain(IntegrationBrainInterface):\n    \"\"\"This is all the methods to interact with the integration brain.\n\n    Args:\n        IntegrationBrainInterface (_type_): _description_\n    \"\"\"\n\n    def __init__(self):\n        self.db = get_supabase_client()\n\n    def get_integration_brain(self, brain_id, user_id = None):\n        query = (\n            self.db.table(\"integrations_user\")\n            .select(\"*\")\n            .filter(\"brain_id\", \"eq\", brain_id)\n        )\n\n        if user_id:\n            query.filter(\"user_id\", \"eq\", user_id)\n\n        response = query.execute()\n\n        if len(response.data) == 0:\n            return None\n\n        return IntegrationEntity(**response.data[0])\n\n    def update_last_synced(self, brain_id, user_id):\n        response = (\n            self.db.table(\"integrations_user\")\n            .update({\"last_synced\": \"now()\"})\n            .filter(\"brain_id\", \"eq\", str(brain_id))\n            .filter(\"user_id\", \"eq\", str(user_id))\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        return IntegrationEntity(**response.data[0])\n\n    def add_integration_brain(self, brain_id, user_id, integration_id, settings):\n\n        response = (\n            self.db.table(\"integrations_user\")\n            .insert(\n                [\n                    {\n                        \"brain_id\": str(brain_id),\n                        \"user_id\": str(user_id),\n                        \"integration_id\": str(integration_id),\n                        \"settings\": settings,\n                    }\n                ]\n            )\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        return IntegrationEntity(**response.data[0])\n\n    def update_integration_brain(self, brain_id, user_id, integration_brain):\n        response = (\n            self.db.table(\"integrations_user\")\n            .update(integration_brain.dict(exclude={\"brain_id\", \"user_id\"}))\n            .filter(\"brain_id\", \"eq\", str(brain_id))\n            .filter(\"user_id\", \"eq\", str(user_id))\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        return IntegrationEntity(**response.data[0])\n\n    def delete_integration_brain(self, brain_id, user_id):\n        self.db.table(\"integrations_user\").delete().filter(\n            \"brain_id\", \"eq\", str(brain_id)\n        ).filter(\"user_id\", \"eq\", str(user_id)).execute()\n        return None\n\n    def get_integration_brain_by_type_integration(\n        self, integration_name\n    ) -> List[IntegrationEntity]:\n        response = (\n            self.db.table(\"integrations_user\")\n            .select(\"*, integrations ()\")\n            .filter(\"integrations.integration_name\", \"eq\", integration_name)\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n\n        return [IntegrationEntity(**data) for data in response.data]\n\n\nclass IntegrationDescription(IntegrationDescriptionInterface):\n\n    def __init__(self):\n        self.db = get_supabase_client()\n\n    def get_integration_description(self, integration_id):\n        response = (\n            self.db.table(\"integrations\")\n            .select(\"*\")\n            .filter(\"id\", \"eq\", integration_id)\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n\n        return IntegrationDescriptionEntity(**response.data[0])\n\n    def get_integration_description_by_user_brain_id(self, brain_id, user_id):\n        response = (\n            self.db.table(\"integrations_user\")\n            .select(\"*\")\n            .filter(\"brain_id\", \"eq\", brain_id)\n            .filter(\"user_id\", \"eq\", user_id)\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n\n        integration_id = response.data[0][\"integration_id\"]\n        return self.get_integration_description(integration_id)\n\n    def get_all_integration_descriptions(self):\n        response = self.db.table(\"integrations\").select(\"*\").execute()\n        return [IntegrationDescriptionEntity(**data) for data in response.data]\n", "backend/modules/brain/repository/brains_users.py": "from uuid import UUID\n\nfrom logger import get_logger\nfrom models.settings import get_embeddings, get_supabase_client\nfrom modules.brain.entity.brain_entity import BrainUser, MinimalUserBrainEntity\nfrom modules.brain.repository.interfaces.brains_users_interface import (\n    BrainsUsersInterface,\n)\n\nlogger = get_logger(__name__)\n\n\nclass BrainsUsers(BrainsUsersInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def update_meaning(self, brain: MinimalUserBrainEntity):\n        embeddings = get_embeddings()\n        string_to_embed = f\"Name: {brain.name} Description: {brain.description}\"\n        brain_meaning = embeddings.embed_query(string_to_embed)\n        brain_dict = {\"meaning\": brain_meaning}\n        response = (\n            self.db.table(\"brains\")\n            .update(brain_dict)\n            .match({\"brain_id\": brain.id})\n            .execute()\n        ).data\n\n        if len(response) == 0:\n            return False\n\n        return True\n\n    def get_user_brains(self, user_id) -> list[MinimalUserBrainEntity]:\n        response = (\n            self.db.from_(\"brains_users\")\n            .select(\n                \"id:brain_id, rights, brains (brain_id, name, status, brain_type, description, meaning, integrations_user (brain_id, integration_id, integrations (id, integration_name, integration_logo_url, max_files)))\"\n            )\n            .filter(\"user_id\", \"eq\", user_id)\n            .execute()\n        )\n        user_brains: list[MinimalUserBrainEntity] = []\n        for item in response.data:\n            integration_logo_url = \"\"\n            max_files = 5000\n            if item[\"brains\"][\"brain_type\"] == \"integration\":\n                if \"integrations_user\" in item[\"brains\"]:\n                    for integration_user in item[\"brains\"][\"integrations_user\"]:\n                        if \"integrations\" in integration_user:\n                            integration_logo_url = integration_user[\"integrations\"][\n                                \"integration_logo_url\"\n                            ]\n                            max_files = integration_user[\"integrations\"][\"max_files\"]\n\n            user_brains.append(\n                MinimalUserBrainEntity(\n                    id=item[\"brains\"][\"brain_id\"],\n                    name=item[\"brains\"][\"name\"],\n                    rights=item[\"rights\"],\n                    status=item[\"brains\"][\"status\"],\n                    brain_type=item[\"brains\"][\"brain_type\"],\n                    description=(\n                        item[\"brains\"][\"description\"]\n                        if item[\"brains\"][\"description\"] is not None\n                        else \"\"\n                    ),\n                    integration_logo_url=str(integration_logo_url),\n                    max_files=max_files,\n                )\n            )\n            user_brains[-1].rights = item[\"rights\"]\n            if item[\"brains\"][\"meaning\"] is None:\n                self.update_meaning(user_brains[-1])\n\n        return user_brains\n\n    def get_brain_for_user(self, user_id, brain_id):\n        response = (\n            self.db.from_(\"brains_users\")\n            .select(\n                \"id:brain_id, rights, brains (id: brain_id, status, name, brain_type, description)\"\n            )\n            .filter(\"user_id\", \"eq\", user_id)\n            .filter(\"brain_id\", \"eq\", brain_id)\n            .execute()\n        )\n        if len(response.data) == 0:\n            return None\n        brain_data = response.data[0]\n\n        return MinimalUserBrainEntity(\n            id=brain_data[\"brains\"][\"id\"],\n            name=brain_data[\"brains\"][\"name\"],\n            rights=brain_data[\"rights\"],\n            status=brain_data[\"brains\"][\"status\"],\n            brain_type=brain_data[\"brains\"][\"brain_type\"],\n            description=(\n                brain_data[\"brains\"][\"description\"]\n                if brain_data[\"brains\"][\"description\"] is not None\n                else \"\"\n            ),\n            integration_logo_url=\"\",\n            max_files=100,\n        )\n\n    def delete_brain_user_by_id(\n        self,\n        user_id: UUID,\n        brain_id: UUID,\n    ):\n        results = (\n            self.db.table(\"brains_users\")\n            .delete()\n            .match({\"brain_id\": str(brain_id), \"user_id\": str(user_id)})\n            .execute()\n        )\n        return results.data\n\n    def delete_brain_users(self, brain_id: str):\n        results = (\n            self.db.table(\"brains_users\")\n            .delete()\n            .match({\"brain_id\": brain_id})\n            .execute()\n        )\n\n        return results\n\n    def create_brain_user(self, user_id: UUID, brain_id, rights, default_brain: bool):\n        response = (\n            self.db.table(\"brains_users\")\n            .insert(\n                {\n                    \"brain_id\": str(brain_id),\n                    \"user_id\": str(user_id),\n                    \"rights\": rights,\n                    \"default_brain\": default_brain,\n                }\n            )\n            .execute()\n        )\n        return response\n\n    def get_user_default_brain_id(self, user_id: UUID) -> UUID | None:\n        response = (\n            (\n                self.db.from_(\"brains_users\")\n                .select(\"brain_id\")\n                .filter(\"user_id\", \"eq\", user_id)\n                .filter(\"default_brain\", \"eq\", True)\n                .execute()\n            )\n        ).data\n        if len(response) == 0:\n            return None\n        return UUID(response[0].get(\"brain_id\"))\n\n    def get_brain_users(self, brain_id: UUID) -> list[BrainUser]:\n        response = (\n            self.db.table(\"brains_users\")\n            .select(\"id:brain_id, *\")\n            .filter(\"brain_id\", \"eq\", str(brain_id))\n            .execute()\n        )\n\n        return [BrainUser(**item) for item in response.data]\n\n    def delete_brain_subscribers(self, brain_id: UUID):\n        results = (\n            self.db.table(\"brains_users\")\n            .delete()\n            .match({\"brain_id\": str(brain_id)})\n            .match({\"rights\": \"Viewer\"})\n            .execute()\n        ).data\n\n        return results\n\n    def get_brain_subscribers_count(self, brain_id: UUID) -> int:\n        response = (\n            self.db.from_(\"brains_users\")\n            .select(\n                \"count\",\n            )\n            .filter(\"brain_id\", \"eq\", str(brain_id))\n            .execute()\n        ).data\n        if len(response) == 0:\n            raise ValueError(f\"Brain with id {brain_id} does not exist.\")\n        return response[0][\"count\"]\n\n    def update_brain_user_default_status(\n        self, user_id: UUID, brain_id: UUID, default_brain: bool\n    ):\n        self.db.table(\"brains_users\").update({\"default_brain\": default_brain}).match(\n            {\"brain_id\": brain_id, \"user_id\": user_id}\n        ).execute()\n\n    def update_brain_user_rights(\n        self, user_id: UUID, brain_id: UUID, rights: str\n    ) -> None:\n        self.db.table(\"brains_users\").update({\"rights\": rights}).match(\n            {\"brain_id\": brain_id, \"user_id\": user_id}\n        ).execute()\n", "backend/modules/brain/repository/__init__.py": "from .brains import Brains\nfrom .brains_users import BrainsUsers\nfrom .brains_vectors import BrainsVectors\nfrom .integration_brains import IntegrationBrain, IntegrationDescription\n", "backend/modules/brain/repository/interfaces/api_brain_definitions_interface.py": "from abc import ABC, abstractmethod\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom modules.brain.dto.inputs import CreateApiBrainDefinition\nfrom modules.brain.entity.api_brain_definition_entity import ApiBrainDefinitionEntity\n\n\nclass ApiBrainDefinitionsInterface(ABC):\n    @abstractmethod\n    def get_api_brain_definition(\n        self, brain_id: UUID\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        pass\n\n    @abstractmethod\n    def add_api_brain_definition(\n        self, brain_id: UUID, api_brain_definition: CreateApiBrainDefinition\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        pass\n\n    @abstractmethod\n    def update_api_brain_definition(\n        self, brain_id: UUID, api_brain_definition: ApiBrainDefinitionEntity\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        \"\"\"\n        Get all public brains\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_api_brain_definition(self, brain_id: UUID) -> None:\n        \"\"\"\n        Update the last update time of the brain\n        \"\"\"\n        pass\n", "backend/modules/brain/repository/interfaces/composite_brains_connections_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.brain.entity.composite_brain_connection_entity import (\n    CompositeBrainConnectionEntity,\n)\n\n\nclass CompositeBrainsConnectionsInterface(ABC):\n    @abstractmethod\n    def connect_brain(\n        self, composite_brain_id: UUID, connected_brain_id: UUID\n    ) -> CompositeBrainConnectionEntity:\n        \"\"\"\n        Connect a brain to a composite brain in the composite_brain_connections table\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_connected_brains(self, composite_brain_id: UUID) -> list[UUID]:\n        \"\"\"\n        Get all brains connected to a composite brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def disconnect_brain(\n        self, composite_brain_id: UUID, connected_brain_id: UUID\n    ) -> None:\n        \"\"\"\n        Disconnect a brain from a composite brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_connected_brain(self, brain_id: UUID) -> bool:\n        \"\"\"\n        Check if a brain is connected to any composite brain\n        \"\"\"\n        pass\n", "backend/modules/brain/repository/interfaces/external_api_secrets_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\n\nclass ExternalApiSecretsInterface(ABC):\n    @abstractmethod\n    def create_secret(\n        self, user_id: UUID, brain_id: UUID, secret_name: str, secret_value\n    ) -> UUID | None:\n        \"\"\"\n        Create a new secret for the API Request in given brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def read_secret(\n        self, user_id: UUID, brain_id: UUID, secret_name: str\n    ) -> UUID | None:\n        \"\"\"\n        Read a secret for the API Request in given brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_secret(self, user_id: UUID, brain_id: UUID, secret_name: str) -> bool:\n        \"\"\"\n        Delete a secret from a brain\n        \"\"\"\n        pass\n", "backend/modules/brain/repository/interfaces/integration_brains_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.brain.entity.integration_brain import (\n    IntegrationDescriptionEntity,\n    IntegrationEntity,\n)\n\n\nclass IntegrationBrainInterface(ABC):\n    @abstractmethod\n    def get_integration_brain(self, brain_id: UUID) -> IntegrationEntity:\n        \"\"\"Get the integration brain entity\n\n        Args:\n            brain_id (UUID): ID of the brain\n\n        Returns:\n            IntegrationEntity: Integration brain entity\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_integration_brain(\n        self, brain_id: UUID, integration_brain: IntegrationEntity\n    ) -> IntegrationEntity:\n        pass\n\n    @abstractmethod\n    def update_integration_brain(\n        self, brain_id: UUID, integration_brain: IntegrationEntity\n    ) -> IntegrationEntity:\n        pass\n\n    @abstractmethod\n    def delete_integration_brain(self, brain_id: UUID) -> None:\n        pass\n\n\nclass IntegrationDescriptionInterface(ABC):\n\n    @abstractmethod\n    def get_integration_description(\n        self, integration_id: UUID\n    ) -> IntegrationDescriptionEntity:\n        \"\"\"Get the integration description entity\n\n        Args:\n            integration_id (UUID): ID of the integration\n\n        Returns:\n            IntegrationEntity: Integration description entity\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_integration_descriptions(self) -> list[IntegrationDescriptionEntity]:\n        pass\n\n    @abstractmethod\n    def get_integration_description_by_user_brain_id(\n        self, brain_id: UUID, user_id: UUID\n    ) -> IntegrationDescriptionEntity:\n        pass\n", "backend/modules/brain/repository/interfaces/brains_users_interface.py": "from abc import ABC, abstractmethod\nfrom typing import List\nfrom uuid import UUID\n\nfrom modules.brain.entity.brain_entity import BrainUser, MinimalUserBrainEntity\n\n\nclass BrainsUsersInterface(ABC):\n    @abstractmethod\n    def get_user_brains(self, user_id) -> list[MinimalUserBrainEntity]:\n        \"\"\"\n        Create a brain in the brains table\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_brain_for_user(self, user_id, brain_id) -> MinimalUserBrainEntity | None:\n        \"\"\"\n        Get a brain for a user\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_brain_user_by_id(\n        self,\n        user_id: UUID,\n        brain_id: UUID,\n    ):\n        \"\"\"\n        Delete a user in a brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_brain_users(self, brain_id: str):\n        \"\"\"\n        Delete all users for a brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_brain_user(self, user_id: UUID, brain_id, rights, default_brain: bool):\n        \"\"\"\n        Create a brain user\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_user_default_brain_id(self, user_id: UUID) -> UUID | None:\n        \"\"\"\n        Get the default brain id for a user\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_brain_users(self, brain_id: UUID) -> List[BrainUser]:\n        \"\"\"\n        Get all users for a brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_brain_subscribers(self, brain_id: UUID):\n        \"\"\"\n        Delete all subscribers for a brain with Viewer rights\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_brain_subscribers_count(self, brain_id: UUID) -> int:\n        \"\"\"\n        Get the number of subscribers for a brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_brain_user_default_status(\n        self, user_id: UUID, brain_id: UUID, default_brain: bool\n    ):\n        \"\"\"\n        Update the default brain status for a user\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_brain_user_rights(\n        self, brain_id: UUID, user_id: UUID, rights: str\n    ) -> BrainUser:\n        \"\"\"\n        Update the rights for a user in a brain\n        \"\"\"\n        pass\n", "backend/modules/brain/repository/interfaces/brains_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.brain.dto.inputs import BrainUpdatableProperties, CreateBrainProperties\nfrom modules.brain.entity.brain_entity import BrainEntity, PublicBrain\n\n\nclass BrainsInterface(ABC):\n    @abstractmethod\n    def create_brain(self, brain: CreateBrainProperties) -> BrainEntity:\n        \"\"\"\n        Create a brain in the brains table\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_public_brains(self) -> list[PublicBrain]:\n        \"\"\"\n        Get all public brains\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_brain_details(self, brain_id: UUID, user_id: UUID) -> BrainEntity | None:\n        \"\"\"\n        Get all public brains\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_brain_last_update_time(self, brain_id: UUID) -> None:\n        \"\"\"\n        Update the last update time of the brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_brain(self, brain_id: UUID):\n        \"\"\"\n        Delete a brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_brain_by_id(\n        self, brain_id: UUID, brain: BrainUpdatableProperties\n    ) -> BrainEntity | None:\n        \"\"\"\n        Update a brain by id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_brain_by_id(self, brain_id: UUID) -> BrainEntity | None:\n        \"\"\"\n        Get a brain by id\n        \"\"\"\n        pass\n", "backend/modules/brain/repository/interfaces/__init__.py": "from .api_brain_definitions_interface import ApiBrainDefinitionsInterface\nfrom .brains_interface import BrainsInterface\nfrom .brains_users_interface import BrainsUsersInterface\nfrom .brains_vectors_interface import BrainsVectorsInterface\nfrom .composite_brains_connections_interface import \\\n    CompositeBrainsConnectionsInterface\nfrom .external_api_secrets_interface import ExternalApiSecretsInterface\nfrom .integration_brains_interface import (IntegrationBrainInterface,\n                                 IntegrationDescriptionInterface)\n", "backend/modules/brain/repository/interfaces/brains_vectors_interface.py": "from abc import ABC, abstractmethod\nfrom typing import List\nfrom uuid import UUID\n\n\n# TODO: Replace BrainsVectors with KnowledgeVectors interface instead\nclass BrainsVectorsInterface(ABC):\n    @abstractmethod\n    def create_brain_vector(self, brain_id, vector_id, file_sha1):\n        \"\"\"\n        Create a brain vector\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_vector_ids_from_file_sha1(self, file_sha1: str):\n        \"\"\"\n        Get vector ids from file sha1\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_brain_vector_ids(self, brain_id) -> List[UUID]:\n        \"\"\"\n        Get brain vector ids\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_file_from_brain(self, brain_id, file_name: str):\n        \"\"\"\n        Delete file from brain\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_brain_vector(self, brain_id: str):\n        \"\"\"\n        Delete brain vector\n        \"\"\"\n        pass\n", "backend/modules/brain/service/api_brain_definition_service.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom modules.brain.dto.inputs import CreateApiBrainDefinition\nfrom modules.brain.entity.api_brain_definition_entity import ApiBrainDefinitionEntity\nfrom modules.brain.repository.api_brain_definitions import ApiBrainDefinitions\nfrom modules.brain.repository.interfaces import ApiBrainDefinitionsInterface\n\n\nclass ApiBrainDefinitionService:\n    repository: ApiBrainDefinitionsInterface\n\n    def __init__(self):\n        self.repository = ApiBrainDefinitions()\n\n    def add_api_brain_definition(\n        self, brain_id: UUID, api_brain_definition: CreateApiBrainDefinition\n    ) -> None:\n        self.repository.add_api_brain_definition(brain_id, api_brain_definition)\n\n    def delete_api_brain_definition(self, brain_id: UUID) -> None:\n        self.repository.delete_api_brain_definition(brain_id)\n\n    def get_api_brain_definition(\n        self, brain_id: UUID\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        return self.repository.get_api_brain_definition(brain_id)\n\n    def update_api_brain_definition(\n        self, brain_id: UUID, api_brain_definition: ApiBrainDefinitionEntity\n    ) -> Optional[ApiBrainDefinitionEntity]:\n        return self.repository.update_api_brain_definition(\n            brain_id, api_brain_definition\n        )\n", "backend/modules/brain/service/get_api_brain_definition_as_json_schema.py": "import re\n\nfrom fastapi import HTTPException\n\nfrom modules.brain.entity.api_brain_definition_entity import ApiBrainDefinitionSchemaProperty\nfrom modules.brain.entity.brain_entity import BrainEntity\nfrom modules.brain.service.api_brain_definition_service import ApiBrainDefinitionService\n\napi_brain_definition_service = ApiBrainDefinitionService()\n\n\ndef sanitize_function_name(string):\n    sanitized_string = re.sub(r\"[^a-zA-Z0-9_-]\", \"\", string)\n\n    return sanitized_string\n\n\ndef format_api_brain_property(property: ApiBrainDefinitionSchemaProperty):\n    property_data: dict = {\n        \"type\": property.type,\n        \"description\": property.description,\n    }\n    if property.enum:\n        property_data[\"enum\"] = property.enum\n    return property_data\n\n\ndef get_api_brain_definition_as_json_schema(brain: BrainEntity):\n    api_brain_definition = api_brain_definition_service.get_api_brain_definition(\n        brain.id\n    )\n    if not api_brain_definition:\n        raise HTTPException(\n            status_code=404, detail=f\"Brain definition {brain.id} not found\"\n        )\n\n    required = []\n    required.extend(api_brain_definition.params.required)\n    required.extend(api_brain_definition.search_params.required)\n    properties = {}\n\n    api_properties = (\n        api_brain_definition.params.properties\n        + api_brain_definition.search_params.properties\n    )\n\n    for property in api_properties:\n        properties[property.name] = format_api_brain_property(property)\n\n    parameters = {\n        \"type\": \"object\",\n        \"properties\": properties,\n        \"required\": required,\n    }\n    schema = {\n        \"name\": sanitize_function_name(brain.name),\n        \"description\": brain.description,\n        \"parameters\": parameters,\n    }\n\n    return schema\n", "backend/modules/brain/service/brain_authorization_service.py": "from typing import List, Optional, Union\nfrom uuid import UUID\n\nfrom fastapi import Depends, HTTPException, status\nfrom middlewares.auth.auth_bearer import get_current_user\nfrom modules.brain.entity.brain_entity import RoleEnum\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.brain.service.brain_user_service import BrainUserService\nfrom modules.user.entity.user_identity import UserIdentity\n\nbrain_user_service = BrainUserService()\nbrain_service = BrainService()\n\n\ndef has_brain_authorization(\n    required_roles: Optional[Union[RoleEnum, List[RoleEnum]]] = RoleEnum.Owner\n):\n    \"\"\"\n    Decorator to check if the user has the required role(s) for the brain\n    param: required_roles: The role(s) required to access the brain\n    return: A wrapper function that checks the authorization\n    \"\"\"\n\n    async def wrapper(\n        brain_id: UUID, current_user: UserIdentity = Depends(get_current_user)\n    ):\n        nonlocal required_roles\n        if isinstance(required_roles, str):\n            required_roles = [required_roles]  # Convert single role to a list\n        validate_brain_authorization(\n            brain_id=brain_id, user_id=current_user.id, required_roles=required_roles\n        )\n\n    return wrapper\n\n\ndef validate_brain_authorization(\n    brain_id: UUID,\n    user_id: UUID,\n    required_roles: Optional[Union[RoleEnum, List[RoleEnum]]] = RoleEnum.Owner,\n):\n    \"\"\"\n    Function to check if the user has the required role(s) for the brain\n    param: brain_id: The id of the brain\n    param: user_id: The id of the user\n    param: required_roles: The role(s) required to access the brain\n    return: None\n    \"\"\"\n\n    brain = brain_service.get_brain_details(brain_id, user_id)\n\n    if brain and brain.status == \"public\":\n        return\n\n    if required_roles is None:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Missing required role\",\n        )\n\n    user_brain = brain_user_service.get_brain_for_user(user_id, brain_id)\n    if user_brain is None:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"You don't have permission for this brain\",\n        )\n\n    # Convert single role to a list to handle both cases\n    if isinstance(required_roles, str):\n        required_roles = [required_roles]\n\n    # Check if the user has at least one of the required roles\n    if user_brain.rights not in required_roles:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"You don't have the required role(s) for this brain\",\n        )\n", "backend/modules/brain/service/brain_service.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom celery_config import celery\nfrom fastapi import HTTPException\nfrom logger import get_logger\nfrom modules.brain.dto.inputs import BrainUpdatableProperties, CreateBrainProperties\nfrom modules.brain.entity.brain_entity import BrainEntity, BrainType, PublicBrain\nfrom modules.brain.entity.integration_brain import IntegrationEntity\nfrom modules.brain.repository import (\n    Brains,\n    BrainsUsers,\n    BrainsVectors,\n    IntegrationBrain,\n    IntegrationDescription,\n)\nfrom modules.brain.repository.interfaces import (\n    BrainsInterface,\n    BrainsUsersInterface,\n    BrainsVectorsInterface,\n    ExternalApiSecretsInterface,\n    IntegrationBrainInterface,\n    IntegrationDescriptionInterface,\n)\nfrom modules.brain.service.api_brain_definition_service import ApiBrainDefinitionService\nfrom modules.brain.service.utils.validate_brain import validate_api_brain\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom vectorstore.supabase import CustomSupabaseVectorStore\n\nlogger = get_logger(__name__)\n\nknowledge_service = KnowledgeService()\n# TODO: directly user api_brain_definition repository\napi_brain_definition_service = ApiBrainDefinitionService()\n\n\nclass BrainService:\n    brain_repository: BrainsInterface\n    brain_user_repository: BrainsUsersInterface\n    brain_vector_repository: BrainsVectorsInterface\n    external_api_secrets_repository: ExternalApiSecretsInterface\n    integration_brains_repository: IntegrationBrainInterface\n    integration_description_repository: IntegrationDescriptionInterface\n\n    def __init__(self):\n        self.brain_repository = Brains()\n        self.brain_user_repository = BrainsUsers()\n        self.brain_vector = BrainsVectors()\n        self.integration_brains_repository = IntegrationBrain()\n        self.integration_description_repository = IntegrationDescription()\n\n    def get_brain_by_id(self, brain_id: UUID):\n        return self.brain_repository.get_brain_by_id(brain_id)\n\n    def get_integration_brain(self, brain_id) -> IntegrationEntity | None:\n        return self.integration_brains_repository.get_integration_brain(brain_id)\n\n    def find_brain_from_question(\n        self,\n        brain_id: UUID,\n        question: str,\n        user,\n        chat_id: UUID,\n        history,\n        vector_store: CustomSupabaseVectorStore,\n    ) -> (Optional[BrainEntity], dict[str, str]):\n        \"\"\"Find the brain to use for a question.\n\n        Args:\n            brain_id (UUID): ID of the brain to use if exists\n            question (str): Question for which to find the brain\n            user (UserEntity): User asking the question\n            chat_id (UUID): ID of the chat\n\n        Returns:\n            Optional[BrainEntity]: Returns the brain to use for the question\n        \"\"\"\n        metadata = {}\n\n        # Init\n\n        brain_id_to_use = brain_id\n        brain_to_use = None\n\n        # Get the first question from the chat_question\n\n        question = question\n\n        list_brains = []  # To return\n\n        if history and not brain_id_to_use:\n            question = history[0].user_message\n            brain_id_to_use = history[0].brain_id\n            brain_to_use = self.get_brain_by_id(brain_id_to_use)\n\n        # If a brain_id is provided, use it\n        if brain_id_to_use and not brain_to_use:\n            brain_to_use = self.get_brain_by_id(brain_id_to_use)\n\n        else:\n            # Calculate the closest brains to the question\n            list_brains = vector_store.find_brain_closest_query(user.id, question)\n\n            unique_list_brains = []\n            seen_brain_ids = set()\n\n            for brain in list_brains:\n                if brain[\"id\"] not in seen_brain_ids:\n                    unique_list_brains.append(brain)\n                    seen_brain_ids.add(brain[\"id\"])\n\n            metadata[\"close_brains\"] = unique_list_brains[:5]\n\n            if list_brains and not brain_to_use:\n                brain_id_to_use = list_brains[0][\"id\"]\n                brain_to_use = self.get_brain_by_id(brain_id_to_use)\n\n        return brain_to_use, metadata\n\n    def create_brain(\n        self,\n        user_id: UUID,\n        brain: Optional[CreateBrainProperties],\n    ) -> BrainEntity:\n        if brain == None:\n            brain = CreateBrainProperties()  # type: ignore model and brain_definition\n\n        if brain.brain_type == BrainType.API:\n            validate_api_brain(brain)\n            return self.create_brain_api(user_id, brain)\n\n        if brain.brain_type == BrainType.COMPOSITE:\n            return self.create_brain_composite(brain)\n\n        if brain.brain_type == BrainType.INTEGRATION:\n            return self.create_brain_integration(user_id, brain)\n\n        created_brain = self.brain_repository.create_brain(brain)\n        return created_brain\n\n    def create_brain_api(\n        self,\n        user_id: UUID,\n        brain: CreateBrainProperties,\n    ) -> BrainEntity:\n        created_brain = self.brain_repository.create_brain(brain)\n\n        if brain.brain_definition is not None:\n            api_brain_definition_service.add_api_brain_definition(\n                brain_id=created_brain.brain_id,\n                api_brain_definition=brain.brain_definition,\n            )\n\n        secrets_values = brain.brain_secrets_values\n\n        for secret_name in secrets_values:\n            self.external_api_secrets_repository.create_secret(\n                user_id=user_id,\n                brain_id=created_brain.brain_id,\n                secret_name=secret_name,\n                secret_value=secrets_values[secret_name],\n            )\n\n        return created_brain\n\n    def create_brain_composite(\n        self,\n        brain: CreateBrainProperties,\n    ) -> BrainEntity:\n        created_brain = self.brain_repository.create_brain(brain)\n\n        if brain.connected_brains_ids is not None:\n            for connected_brain_id in brain.connected_brains_ids:\n                self.composite_brains_connections_repository.connect_brain(\n                    composite_brain_id=created_brain.brain_id,\n                    connected_brain_id=connected_brain_id,\n                )\n\n        return created_brain\n\n    def create_brain_integration(\n        self,\n        user_id: UUID,\n        brain: CreateBrainProperties,\n    ) -> BrainEntity:\n        created_brain = self.brain_repository.create_brain(brain)\n        if brain.integration is not None:\n            self.integration_brains_repository.add_integration_brain(\n                user_id=user_id,\n                brain_id=created_brain.brain_id,\n                integration_id=brain.integration.integration_id,\n                settings=brain.integration.settings,\n            )\n        if (\n            self.integration_description_repository.get_integration_description(\n                brain.integration.integration_id\n            ).integration_name.lower()\n            == \"notion\"\n        ):\n            celery.send_task(\n                \"NotionConnectorLoad\",\n                kwargs={\"brain_id\": created_brain.brain_id, \"user_id\": user_id},\n            )\n        return created_brain\n\n    def delete_brain_secrets_values(self, brain_id: UUID) -> None:\n        brain_definition = api_brain_definition_service.get_api_brain_definition(\n            brain_id=brain_id\n        )\n\n        if brain_definition is None:\n            raise HTTPException(status_code=404, detail=\"Brain definition not found.\")\n\n        secrets = brain_definition.secrets\n\n        if len(secrets) > 0:\n            brain_users = self.brain_user_repository.get_brain_users(brain_id=brain_id)\n            for user in brain_users:\n                for secret in secrets:\n                    self.external_api_secrets_repository.delete_secret(\n                        user_id=user.user_id,\n                        brain_id=brain_id,\n                        secret_name=secret.name,\n                    )\n\n    def delete_brain(self, brain_id: UUID) -> dict[str, str]:\n        brain_to_delete = self.get_brain_by_id(brain_id=brain_id)\n        if brain_to_delete is None:\n            raise HTTPException(status_code=404, detail=\"Brain not found.\")\n\n        if brain_to_delete.brain_type == BrainType.API:\n            self.delete_brain_secrets_values(\n                brain_id=brain_id,\n            )\n            api_brain_definition_service.delete_api_brain_definition(brain_id=brain_id)\n        else:\n            knowledge_service.remove_brain_all_knowledge(brain_id)\n\n        self.brain_vector.delete_brain_vector(str(brain_id))\n        self.brain_user_repository.delete_brain_users(str(brain_id))\n        self.brain_repository.delete_brain(str(brain_id))  # type: ignore\n\n        return {\"message\": \"Brain deleted.\"}\n\n    def get_brain_prompt_id(self, brain_id: UUID) -> UUID | None:\n        brain = self.get_brain_by_id(brain_id)\n        prompt_id = brain.prompt_id if brain else None\n\n        return prompt_id\n\n    def update_brain_by_id(\n        self, brain_id: UUID, brain_new_values: BrainUpdatableProperties\n    ) -> BrainEntity:\n        \"\"\"Update a prompt by id\"\"\"\n\n        existing_brain = self.brain_repository.get_brain_by_id(brain_id)\n\n        if existing_brain is None:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Brain with id {brain_id} not found\",\n            )\n        brain_update_answer = self.brain_repository.update_brain_by_id(\n            brain_id,\n            brain=BrainUpdatableProperties(\n                **brain_new_values.dict(\n                    exclude={\"brain_definition\", \"connected_brains_ids\", \"integration\"}\n                )\n            ),\n        )\n\n        if brain_update_answer is None:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Brain with id {brain_id} not found\",\n            )\n\n        if (\n            brain_update_answer.brain_type == BrainType.API\n            and brain_new_values.brain_definition\n        ):\n            existing_brain_secrets_definition = (\n                existing_brain.brain_definition.secrets\n                if existing_brain.brain_definition\n                else None\n            )\n            brain_new_values_secrets_definition = (\n                brain_new_values.brain_definition.secrets\n                if brain_new_values.brain_definition\n                else None\n            )\n            should_remove_existing_secrets_values = (\n                existing_brain_secrets_definition\n                and brain_new_values_secrets_definition\n                and existing_brain_secrets_definition\n                != brain_new_values_secrets_definition\n            )\n\n            if should_remove_existing_secrets_values:\n                self.delete_brain_secrets_values(brain_id=brain_id)\n\n            api_brain_definition_service.update_api_brain_definition(\n                brain_id,\n                api_brain_definition=brain_new_values.brain_definition,\n            )\n\n        if brain_update_answer is None:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Brain with id {brain_id} not found\",\n            )\n\n        self.brain_repository.update_brain_last_update_time(brain_id)\n        return brain_update_answer\n\n    def update_brain_last_update_time(self, brain_id: UUID):\n        self.brain_repository.update_brain_last_update_time(brain_id)\n\n    def get_brain_details(\n        self, brain_id: UUID, user_id: UUID = None\n    ) -> BrainEntity | None:\n        brain = self.brain_repository.get_brain_details(brain_id)\n        if brain == None:\n            return None\n\n        if brain.brain_type == BrainType.INTEGRATION:\n            brain.integration = (\n                self.integration_brains_repository.get_integration_brain(\n                    brain_id, user_id\n                )\n            )\n\n            if brain.integration:\n                brain.integration_description = (\n                    self.integration_description_repository.get_integration_description(\n                        brain.integration.integration_id\n                    )\n                )\n\n        return brain\n\n    def get_connected_brains(self, brain_id: UUID) -> list[BrainEntity]:\n        return self.composite_brains_connections_repository.get_connected_brains(\n            brain_id\n        )\n\n    def get_public_brains(self) -> list[PublicBrain]:\n        return self.brain_repository.get_public_brains()\n\n    def update_secret_value(\n        self,\n        user_id: UUID,\n        brain_id: UUID,\n        secret_name: str,\n        secret_value: str,\n    ) -> None:\n        \"\"\"Update an existing secret.\"\"\"\n        self.external_api_secrets_repository.delete_secret(\n            user_id=user_id,\n            brain_id=brain_id,\n            secret_name=secret_name,\n        )\n        self.external_api_secrets_repository.create_secret(\n            user_id=user_id,\n            brain_id=brain_id,\n            secret_name=secret_name,\n            secret_value=secret_value,\n        )\n", "backend/modules/brain/service/brain_vector_service.py": "from typing import Any, List\nfrom uuid import UUID\n\nfrom logger import get_logger\nfrom modules.brain.repository.brains_vectors import BrainsVectors\nfrom modules.brain.repository.interfaces.brains_vectors_interface import (\n    BrainsVectorsInterface,\n)\nfrom modules.knowledge.repository.storage import Storage\nfrom packages.embeddings.vectors import get_unique_files_from_vector_ids\n\nlogger = get_logger(__name__)\n\n\nclass BrainVectorService:\n    repository: BrainsVectorsInterface\n    id: UUID\n    files: List[Any] = []\n\n    def __init__(self, brain_id: UUID):\n        self.repository = BrainsVectors()\n        self.id = brain_id\n\n    def create_brain_vector(self, vector_id, file_sha1):\n        return self.repository.create_brain_vector(self.id, vector_id, file_sha1)  # type: ignore\n\n    def update_brain_with_file(self, file_sha1: str):\n        # not  used\n        vector_ids = self.repository.get_vector_ids_from_file_sha1(file_sha1)\n        if vector_ids == None or len(vector_ids) == 0:\n            logger.info(f\"No vector ids found for file {file_sha1}\")\n            return\n\n        for vector_id in vector_ids:\n            self.create_brain_vector(vector_id, file_sha1)\n\n    def get_unique_brain_files(self):\n        \"\"\"\n        Retrieve unique brain data (i.e. uploaded files and crawled websites).\n        \"\"\"\n\n        vector_ids = self.repository.get_brain_vector_ids(self.id)  # type: ignore\n        self.files = get_unique_files_from_vector_ids(vector_ids)\n\n        return self.files\n\n    def delete_file_from_brain(self, file_name: str, only_vectors: bool = False):\n        file_name_with_brain_id = f\"{self.id}/{file_name}\"\n        storage = Storage()\n        if not only_vectors:\n            storage.remove_file(file_name_with_brain_id)\n        return self.repository.delete_file_from_brain(self.id, file_name)  # type: ignore\n\n    def delete_file_url_from_brain(self, file_name: str):\n        return self.repository.delete_file_from_brain(self.id, file_name)  # type: ignore\n\n    @property\n    def brain_size(self):\n        # TODO: change the calculation of the brain size, calculate the size stored for the embeddings + what's in the storage\n        self.get_unique_brain_files()\n        current_brain_size = sum(float(doc[\"size\"]) for doc in self.files)\n\n        return current_brain_size\n", "backend/modules/brain/service/integration_brain_service.py": "from modules.brain.entity.integration_brain import IntegrationDescriptionEntity\nfrom modules.brain.repository.integration_brains import IntegrationDescription\nfrom modules.brain.repository.interfaces import IntegrationDescriptionInterface\n\n\nclass IntegrationBrainDescriptionService:\n    repository: IntegrationDescriptionInterface\n\n    def __init__(self):\n        self.repository = IntegrationDescription()\n\n    def get_all_integration_descriptions(self) -> list[IntegrationDescriptionEntity]:\n        return self.repository.get_all_integration_descriptions()\n\n    def get_integration_description(\n        self, integration_id\n    ) -> IntegrationDescriptionEntity:\n        return self.repository.get_integration_description(integration_id)\n\n    def get_integration_description_by_user_brain_id(\n        self, brain_id, user_id\n    ) -> IntegrationDescriptionEntity:\n        return self.repository.get_integration_description_by_user_brain_id(\n            brain_id, user_id\n        )\n", "backend/modules/brain/service/call_brain_api.py": "from uuid import UUID\nimport requests\nfrom logger import get_logger\nlogger = get_logger(__name__)\n\nfrom fastapi import HTTPException\n\nfrom modules.brain.entity.api_brain_definition_entity import ApiBrainDefinitionSchema\nfrom modules.brain.service.api_brain_definition_service import ApiBrainDefinitionService\nfrom modules.brain.service.brain_service import BrainService\n\nbrain_service = BrainService()\napi_brain_definition_service = ApiBrainDefinitionService()\n\n\ndef get_api_call_response_as_text(\n    method, api_url, params, search_params, secrets\n) -> str:\n    headers = {}\n\n    api_url_with_search_params = api_url\n    if search_params:\n        api_url_with_search_params += \"?\"\n        for search_param in search_params:\n            api_url_with_search_params += (\n                f\"{search_param}={search_params[search_param]}&\"\n            )\n\n    for secret in secrets:\n        headers[secret] = secrets[secret]\n\n    try:\n        if method in [\"GET\", \"DELETE\"]:\n            response = requests.request(\n                method,\n                url=api_url_with_search_params,\n                params=params or None,\n                headers=headers or None,\n            )\n        elif method in [\"POST\", \"PUT\", \"PATCH\"]:\n            response = requests.request(\n                method,\n                url=api_url_with_search_params,\n                json=params or None,\n                headers=headers or None,\n            )\n        else:\n            raise ValueError(f\"Invalid method: {method}\")\n\n        return response.text\n\n    except Exception as e:\n        logger.error(f\"Error calling API: {e}\")\n        return None\n\n\ndef extract_api_brain_definition_values_from_llm_output(\n    brain_schema: ApiBrainDefinitionSchema, arguments: dict\n) -> dict:\n    params_values = {}\n    properties = brain_schema.properties\n    required_values = brain_schema.required\n    for property in properties:\n        if property.name in arguments:\n            if property.type == \"number\":\n                params_values[property.name] = float(arguments[property.name])\n            else:\n                params_values[property.name] = arguments[property.name]\n            continue\n\n        if property.name in required_values:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Required parameter {property.name} not found in arguments\",\n            )\n\n    return params_values\n\n\ndef call_brain_api(brain_id: UUID, user_id: UUID, arguments: dict) -> str:\n    brain_definition = api_brain_definition_service.get_api_brain_definition(brain_id)\n\n    if brain_definition is None:\n        raise HTTPException(\n            status_code=404, detail=f\"Brain definition {brain_id} not found\"\n        )\n\n    brain_params_values = extract_api_brain_definition_values_from_llm_output(\n        brain_definition.params, arguments\n    )\n\n    brain_search_params_values = extract_api_brain_definition_values_from_llm_output(\n        brain_definition.search_params, arguments\n    )\n\n    secrets = brain_definition.secrets\n    secrets_values = {}\n\n    for secret in secrets:\n        secret_value = brain_service.external_api_secrets_repository.read_secret(\n            user_id=user_id, brain_id=brain_id, secret_name=secret.name\n        )\n        secrets_values[secret.name] = secret_value\n\n    return get_api_call_response_as_text(\n        api_url=brain_definition.url,\n        params=brain_params_values,\n        search_params=brain_search_params_values,\n        secrets=secrets_values,\n        method=brain_definition.method,\n    )\n", "backend/modules/brain/service/get_question_context_from_brain.py": "from uuid import UUID\n\nfrom attr import dataclass\nfrom logger import get_logger\nfrom models.settings import get_embeddings, get_supabase_client\nfrom modules.upload.service.generate_file_signed_url import generate_file_signed_url\nfrom vectorstore.supabase import CustomSupabaseVectorStore\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass DocumentAnswer:\n    file_name: str\n    file_sha1: str\n    file_size: int\n    file_url: str = \"\"\n    file_id: str = \"\"\n    file_similarity: float = 0.0\n\n\ndef get_question_context_from_brain(brain_id: UUID, question: str) -> str:\n    \"\"\"Finds the best brain to answer the question based on the question's meaning.\n\n    Args:\n        brain_id (UUID): Id of the brain to search in\n        question (str): Question to search for in the vector store\n\n    Returns:\n        str: _descripton_\n    \"\"\"\n    # TODO: Move to AnswerGenerator service\n    supabase_client = get_supabase_client()\n    embeddings = get_embeddings()\n\n    vector_store = CustomSupabaseVectorStore(\n        supabase_client,\n        embeddings,\n        table_name=\"vectors\",\n        brain_id=str(brain_id),\n        number_docs=20,\n    )\n    documents = vector_store.similarity_search(question, k=20, threshold=0.8)\n\n    answers = []\n    file_sha1s = []\n    for document in documents:\n        if document.metadata[\"file_sha1\"] not in file_sha1s:\n            file_sha1s.append(document.metadata[\"file_sha1\"])\n            file_path_in_storage = f\"{brain_id}/{document.metadata['file_name']}\"\n            answers.append(\n                DocumentAnswer(\n                    file_name=document.metadata[\"file_name\"],\n                    file_sha1=document.metadata[\"file_sha1\"],\n                    file_size=document.metadata[\"file_size\"],\n                    file_id=document.metadata[\"id\"],\n                    file_similarity=document.metadata[\"similarity\"],\n                    file_url=generate_file_signed_url(file_path_in_storage).get(\n                        \"signedURL\", \"\"\n                    ),\n                ),\n            )\n\n    return answers\n", "backend/modules/brain/service/__init__.py": "", "backend/modules/brain/service/brain_user_service.py": "from typing import List\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom logger import get_logger\nfrom modules.brain.entity.brain_entity import (\n    BrainEntity,\n    BrainType,\n    BrainUser,\n    MinimalUserBrainEntity,\n    RoleEnum,\n)\nfrom modules.brain.repository.brains import Brains\nfrom modules.brain.repository.brains_users import BrainsUsers\nfrom modules.brain.repository.external_api_secrets import ExternalApiSecrets\nfrom modules.brain.repository.interfaces.brains_interface import BrainsInterface\nfrom modules.brain.repository.interfaces.brains_users_interface import (\n    BrainsUsersInterface,\n)\nfrom modules.brain.repository.interfaces.external_api_secrets_interface import (\n    ExternalApiSecretsInterface,\n)\nfrom modules.brain.service.api_brain_definition_service import ApiBrainDefinitionService\nfrom modules.brain.service.brain_service import BrainService\n\nlogger = get_logger(__name__)\n\nbrain_service = BrainService()\napi_brain_definition_service = ApiBrainDefinitionService()\n\n\nclass BrainUserService:\n    brain_repository: BrainsInterface\n    brain_user_repository: BrainsUsersInterface\n    external_api_secrets_repository: ExternalApiSecretsInterface\n\n    def __init__(self):\n        self.brain_repository = Brains()\n        self.brain_user_repository = BrainsUsers()\n        self.external_api_secrets_repository = ExternalApiSecrets()\n\n    def get_user_default_brain(self, user_id: UUID) -> BrainEntity | None:\n        brain_id = self.brain_user_repository.get_user_default_brain_id(user_id)\n\n        if brain_id is None:\n            return None\n\n        return brain_service.get_brain_by_id(brain_id)\n\n    def delete_brain_user(self, user_id: UUID, brain_id: UUID) -> None:\n        brain_to_delete_user_from = brain_service.get_brain_by_id(brain_id=brain_id)\n        if brain_to_delete_user_from is None:\n            raise HTTPException(status_code=404, detail=\"Brain not found.\")\n\n        if brain_to_delete_user_from.brain_type == BrainType.API:\n            brain_definition = api_brain_definition_service.get_api_brain_definition(\n                brain_id=brain_id\n            )\n            if brain_definition is None:\n                raise HTTPException(\n                    status_code=404, detail=\"Brain definition not found.\"\n                )\n            secrets = brain_definition.secrets\n            for secret in secrets:\n                self.external_api_secrets_repository.delete_secret(\n                    user_id=user_id,\n                    brain_id=brain_id,\n                    secret_name=secret.name,\n                )\n\n        self.brain_user_repository.delete_brain_user_by_id(\n            user_id=user_id,\n            brain_id=brain_id,\n        )\n\n    def delete_brain_users(self, brain_id: UUID) -> None:\n        self.brain_user_repository.delete_brain_subscribers(\n            brain_id=brain_id,\n        )\n\n    def create_brain_user(\n        self, user_id: UUID, brain_id: UUID, rights: RoleEnum, is_default_brain: bool\n    ):\n        self.brain_user_repository.create_brain_user(\n            user_id=user_id,\n            brain_id=brain_id,\n            rights=rights,\n            default_brain=is_default_brain,\n        )\n\n    def get_brain_for_user(self, user_id: UUID, brain_id: UUID):\n        return self.brain_user_repository.get_brain_for_user(user_id, brain_id)  # type: ignore\n\n    def get_user_brains(self, user_id: UUID) -> list[MinimalUserBrainEntity]:\n        results = self.brain_user_repository.get_user_brains(user_id)  # type: ignore\n\n        return results  # type: ignore\n\n    def get_brain_users(self, brain_id: UUID) -> List[BrainUser]:\n        return self.brain_user_repository.get_brain_users(brain_id)\n\n    def update_brain_user_rights(\n        self, brain_id: UUID, user_id: UUID, rights: str\n    ) -> None:\n        self.brain_user_repository.update_brain_user_rights(\n            brain_id=brain_id,\n            user_id=user_id,\n            rights=rights,\n        )\n", "backend/modules/brain/service/utils/get_prompt_to_use_id.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom modules.brain.service.brain_service import BrainService\n\nbrain_service = BrainService()\n\n\ndef get_prompt_to_use_id(\n    brain_id: Optional[UUID], prompt_id: Optional[UUID]\n) -> Optional[UUID]:\n    if brain_id is None and prompt_id is None:\n        return None\n\n    return (\n        prompt_id\n        if prompt_id\n        else brain_service.get_brain_prompt_id(brain_id)\n        if brain_id\n        else None\n    )\n", "backend/modules/brain/service/utils/validate_brain.py": "from fastapi import HTTPException\nfrom modules.brain.dto.inputs import CreateBrainProperties\n\n\ndef validate_api_brain(brain: CreateBrainProperties):\n    if brain.brain_definition is None:\n        raise HTTPException(status_code=404, detail=\"Brain definition not found\")\n\n    if brain.brain_definition.url is None:\n        raise HTTPException(status_code=404, detail=\"Brain url not found\")\n\n    if brain.brain_definition.method is None:\n        raise HTTPException(status_code=404, detail=\"Brain method not found\")\n", "backend/modules/brain/service/utils/__init__.py": "from .validate_brain import validate_api_brain\n", "backend/modules/brain/service/utils/format_chat_history.py": "from typing import Dict, List, Tuple\n\nfrom langchain.schema import AIMessage, BaseMessage, HumanMessage, SystemMessage\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\n\n\ndef format_chat_history(\n    history: List[GetChatHistoryOutput],\n) -> List[Dict[str, str]]:\n    \"\"\"Format the chat history into a list of HumanMessage and AIMessage\"\"\"\n    formatted_history = []\n    for chat in history:\n        formatted_history.append(HumanMessage(chat.user_message))\n        formatted_history.append(AIMessage(chat.assistant))\n    return formatted_history\n\n\ndef format_history_to_openai_mesages(\n    tuple_history: List[Tuple[str, str]], system_message: str, question: str\n) -> List[BaseMessage]:\n    \"\"\"Format the chat history into a list of Base Messages\"\"\"\n    messages = []\n    messages.append(SystemMessage(content=system_message))\n    for human, ai in tuple_history:\n        messages.append(HumanMessage(content=human))\n        messages.append(AIMessage(content=ai))\n    messages.append(HumanMessage(content=question))\n    return messages\n", "backend/modules/brain/service/brain_subscription/resend_invitation_email.py": "from uuid import UUID\n\nfrom logger import get_logger\nfrom models import BrainSettings, BrainSubscription\nfrom modules.brain.service.brain_service import BrainService\nfrom packages.emails.send_email import send_email\n\nlogger = get_logger(__name__)\n\nbrain_service = BrainService()\n\n\ndef get_brain_url(origin: str, brain_id: UUID) -> str:\n    \"\"\"Generates the brain URL based on the brain_id.\"\"\"\n\n    return f\"{origin}/invitation/{brain_id}\"\n\n\ndef resend_invitation_email(\n    brain_subscription: BrainSubscription,\n    inviter_email: str,\n    user_id: UUID,\n    origin: str = \"https://chat.quivr.app\",\n):\n    brains_settings = BrainSettings()  # pyright: ignore reportPrivateUsage=none\n\n    brain_url = get_brain_url(origin, brain_subscription.brain_id)\n\n    invitation_brain = brain_service.get_brain_details(\n        brain_subscription.brain_id, user_id\n    )\n    if invitation_brain is None:\n        raise Exception(\"Brain not found\")\n    brain_name = invitation_brain.name\n\n    html_body = f\"\"\"\n    <p>Brain {brain_name} has been shared with you by {inviter_email}.</p>\n    <p><a href='{brain_url}'>Click here</a> to access your brain.</p>\n    \"\"\"\n\n    try:\n        r = send_email(\n            {\n                \"sender\": brains_settings.resend_email_address,\n                \"to\": brain_subscription.email,\n                \"subject\": \"Quivr - Brain Shared With You\",\n                \"html\": html_body,\n            }\n        )\n        logger.info(\"Resend response\", r)\n    except Exception as e:\n        logger.error(f\"Error sending email: {e}\")\n        return\n\n    return r\n", "backend/modules/brain/service/brain_subscription/subscription_invitation_service.py": "from logger import get_logger\nfrom models import BrainSubscription, get_supabase_client\nfrom modules.brain.service.brain_user_service import BrainUserService\nfrom modules.user.service.user_service import UserService\n\nlogger = get_logger(__name__)\n\n\nbrain_user_service = BrainUserService()\nuser_service = UserService()\n\n\nclass SubscriptionInvitationService:\n    def __init__(self):\n        self.supabase_client = get_supabase_client()\n\n    def create_subscription_invitation(self, brain_subscription: BrainSubscription):\n        logger.info(\"Creating subscription invitation\")\n        response = (\n            self.supabase_client.table(\"brain_subscription_invitations\")\n            .insert(\n                {\n                    \"brain_id\": str(brain_subscription.brain_id),\n                    \"email\": brain_subscription.email,\n                    \"rights\": brain_subscription.rights,\n                }\n            )\n            .execute()\n        )\n        return response.data\n\n    def update_subscription_invitation(self, brain_subscription: BrainSubscription):\n        logger.info(\"Updating subscription invitation\")\n        response = (\n            self.supabase_client.table(\"brain_subscription_invitations\")\n            .update({\"rights\": brain_subscription.rights})\n            .eq(\"brain_id\", str(brain_subscription.brain_id))\n            .eq(\"email\", brain_subscription.email)\n            .execute()\n        )\n        return response.data\n\n    def create_or_update_subscription_invitation(\n        self,\n        brain_subscription: BrainSubscription,\n    ) -> bool:\n        \"\"\"\n        Creates a subscription invitation if it does not exist, otherwise updates it.\n        Returns True if the invitation was created or updated and False if user already has access.\n        \"\"\"\n        response = (\n            self.supabase_client.table(\"brain_subscription_invitations\")\n            .select(\"*\")\n            .eq(\"brain_id\", str(brain_subscription.brain_id))\n            .eq(\"email\", brain_subscription.email)\n            .execute()\n        )\n\n        if response.data:\n            self.update_subscription_invitation(brain_subscription)\n            return True\n        else:\n            user_id = user_service.get_user_id_by_email(brain_subscription.email)\n            brain_user = None\n\n            if user_id is not None:\n                brain_id = brain_subscription.brain_id\n                brain_user = brain_user_service.get_brain_for_user(user_id, brain_id)\n\n            if brain_user is None:\n                self.create_subscription_invitation(brain_subscription)\n                return True\n\n        return False\n\n    def fetch_invitation(self, subscription: BrainSubscription):\n        logger.info(\"Fetching subscription invitation\")\n        response = (\n            self.supabase_client.table(\"brain_subscription_invitations\")\n            .select(\"*\")\n            .eq(\"brain_id\", str(subscription.brain_id))\n            .eq(\"email\", subscription.email)\n            .execute()\n        )\n        if response.data:\n            return response.data[0]  # return the first matching invitation\n        else:\n            return None\n\n    def remove_invitation(self, subscription: BrainSubscription):\n        logger.info(\n            f\"Removing subscription invitation for email {subscription.email} and brain {subscription.brain_id}\"\n        )\n        response = (\n            self.supabase_client.table(\"brain_subscription_invitations\")\n            .delete()\n            .eq(\"brain_id\", str(subscription.brain_id))\n            .eq(\"email\", subscription.email)\n            .execute()\n        )\n        logger.info(\n            f\"Removed subscription invitation for email {subscription.email} and brain {subscription.brain_id}\"\n        )\n        logger.info(response)\n        return response.data\n", "backend/modules/brain/service/brain_subscription/__init__.py": "from .resend_invitation_email import resend_invitation_email\nfrom .subscription_invitation_service import SubscriptionInvitationService\n", "backend/modules/brain/dto/__init__.py": "", "backend/modules/brain/dto/inputs.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom logger import get_logger\nfrom modules.brain.entity.api_brain_definition_entity import (\n    ApiBrainAllowedMethods,\n    ApiBrainDefinitionEntity,\n    ApiBrainDefinitionSchema,\n    ApiBrainDefinitionSecret,\n)\nfrom modules.brain.entity.brain_entity import BrainType\nfrom modules.brain.entity.integration_brain import IntegrationType\nfrom pydantic import BaseModel, Extra\n\nlogger = get_logger(__name__)\n\n\nclass CreateApiBrainDefinition(BaseModel, extra=\"ignore\"):\n    method: ApiBrainAllowedMethods\n    url: str\n    params: Optional[ApiBrainDefinitionSchema] = ApiBrainDefinitionSchema()\n    search_params: ApiBrainDefinitionSchema = ApiBrainDefinitionSchema()\n    secrets: Optional[list[ApiBrainDefinitionSecret]] = []\n    raw: Optional[bool] = False\n    jq_instructions: Optional[str] = None\n\n\nclass CreateIntegrationBrain(BaseModel, extra=\"ignore\"):\n    integration_name: str\n    integration_logo_url: str\n    connection_settings: dict\n    integration_type: IntegrationType\n    description: str\n    max_files: int\n\n\nclass BrainIntegrationSettings(BaseModel, extra=\"ignore\"):\n    integration_id: str\n    settings: dict\n\n\nclass BrainIntegrationUpdateSettings(BaseModel, extra=\"ignore\"):\n    settings: dict\n\n\nclass CreateBrainProperties(BaseModel, extra=\"ignore\"):\n    name: Optional[str] = \"Default brain\"\n    description: str = \"This is a description\"\n    status: Optional[str] = \"private\"\n    model: Optional[str] = None\n    temperature: Optional[float] = 0.0\n    max_tokens: Optional[int] = 2000\n    prompt_id: Optional[UUID] = None\n    brain_type: Optional[BrainType] = BrainType.DOC\n    brain_definition: Optional[CreateApiBrainDefinition] = None\n    brain_secrets_values: Optional[dict] = {}\n    connected_brains_ids: Optional[list[UUID]] = []\n    integration: Optional[BrainIntegrationSettings] = None\n\n    def dict(self, *args, **kwargs):\n        brain_dict = super().dict(*args, **kwargs)\n        if brain_dict.get(\"prompt_id\"):\n            brain_dict[\"prompt_id\"] = str(brain_dict.get(\"prompt_id\"))\n        return brain_dict\n\n\nclass BrainUpdatableProperties(BaseModel, extra=\"ignore\"):\n\n    name: Optional[str] = None\n    description: Optional[str] = None\n    temperature: Optional[float] = None\n    model: Optional[str] = None\n    max_tokens: Optional[int] = None\n    status: Optional[str] = None\n    prompt_id: Optional[UUID] = None\n    brain_definition: Optional[ApiBrainDefinitionEntity] = None\n    connected_brains_ids: Optional[list[UUID]] = []\n    integration: Optional[BrainIntegrationUpdateSettings] = None\n\n    def dict(self, *args, **kwargs):\n        brain_dict = super().dict(*args, **kwargs)\n        if brain_dict.get(\"prompt_id\"):\n            brain_dict[\"prompt_id\"] = str(brain_dict.get(\"prompt_id\"))\n        return brain_dict\n\n\nclass BrainQuestionRequest(BaseModel):\n    question: str\n", "backend/modules/brain/rags/quivr_rag.py": "import datetime\nimport os\nfrom operator import itemgetter\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import FlashrankRerank\nfrom langchain.schema import format_document\nfrom langchain_cohere import CohereRerank\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain_core.pydantic_v1 import Field as FieldV1\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom logger import get_logger\nfrom models import BrainSettings  # Importing settings related to the 'brain'\nfrom models.settings import get_supabase_client\nfrom modules.brain.service.brain_service import BrainService\nfrom modules.chat.service.chat_service import ChatService\nfrom modules.knowledge.repository.knowledges import Knowledges\nfrom modules.prompt.service.get_prompt_to_use import get_prompt_to_use\nfrom pydantic import BaseModel, ConfigDict\nfrom pydantic_settings import BaseSettings\nfrom supabase.client import Client\nfrom vectorstore.supabase import CustomSupabaseVectorStore\n\nlogger = get_logger(__name__)\n\n\nclass cited_answer(BaseModelV1):\n    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n\n    thoughts: str = FieldV1(\n        ...,\n        description=\"\"\"Description of the thought process, based only on the given sources. \n        Cite the text as much as possible and give the document name it appears in. In the format : 'Doc_name states : cited_text'. Be the most \n        procedural as possible. Write all the steps needed to find the answer until you find it.\"\"\",\n    )\n    answer: str = FieldV1(\n        ...,\n        description=\"The answer to the user question, which is based only on the given sources.\",\n    )\n    citations: List[int] = FieldV1(\n        ...,\n        description=\"The integer IDs of the SPECIFIC sources which justify the answer.\",\n    )\n\n    thoughts: str = FieldV1(\n        ...,\n        description=\"Explain shortly what you did to find the answer and what you used by citing the sources by their name.\",\n    )\n    followup_questions: List[str] = FieldV1(\n        ...,\n        description=\"Generate up to 3 follow-up questions that could be asked based on the answer given or context provided.\",\n    )\n\n\n# First step is to create the Rephrasing Prompt\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Keep as much details as possible from previous messages. Keep entity names and all.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n\n# Next is the answering prompt\n\ntemplate_answer = \"\"\"\nContext:\n{context}\n\nUser Question: {question}\nAnswer:\n\"\"\"\n\ntoday_date = datetime.datetime.now().strftime(\"%B %d, %Y\")\n\nsystem_message_template = (\n    f\"Your name is Quivr. You're a helpful assistant. Today's date is {today_date}.\"\n)\n\nsystem_message_template += \"\"\"\nWhen answering use markdown.\nUse markdown code blocks for code snippets.\nAnswer in a concise and clear manner.\nUse the following pieces of context from files provided by the user to answer the users.\nAnswer in the same language as the user question.\nIf you don't know the answer with the context provided from the files, just say that you don't know, don't try to make up an answer.\nDon't cite the source id in the answer objects, but you can use the source to answer the question.\nYou have access to the files to answer the user question (limited to first 20 files):\n{files}\n\nIf not None, User instruction to follow to answer: {custom_instructions}\nDon't cite the source id in the answer objects, but you can use the source to answer the question.\n\"\"\"\n\n\nANSWER_PROMPT = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(system_message_template),\n        HumanMessagePromptTemplate.from_template(template_answer),\n    ]\n)\n\n\n# How we format documents\n\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n    template=\"Source: {index} \\n {page_content}\"\n)\n\n\ndef is_valid_uuid(uuid_to_test, version=4):\n    try:\n        uuid_obj = UUID(uuid_to_test, version=version)\n    except ValueError:\n        return False\n\n    return str(uuid_obj) == uuid_to_test\n\n\nbrain_service = BrainService()\nchat_service = ChatService()\n\n\nclass QuivrRAG(BaseModel):\n    \"\"\"\n    Quivr implementation of the RAGInterface.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    # Instantiate settings\n    brain_settings: BaseSettings = BrainSettings()\n    # Default class attributes\n    model: str = None  # pyright: ignore reportPrivateUsage=none\n    temperature: float = 0.1\n    chat_id: str = None  # pyright: ignore reportPrivateUsage=none\n    brain_id: str = None  # pyright: ignore reportPrivateUsage=none\n    max_tokens: int = 2000  # Output length\n    max_input: int = 2000\n    streaming: bool = False\n    knowledge_service: Knowledges = None\n\n    @property\n    def embeddings(self):\n        if self.brain_settings.ollama_api_base_url:\n            return OllamaEmbeddings(\n                base_url=self.brain_settings.ollama_api_base_url\n            )  # pyright: ignore reportPrivateUsage=none\n        else:\n            return OpenAIEmbeddings()\n\n    def prompt_to_use(self):\n        if self.brain_id and is_valid_uuid(self.brain_id):\n            return get_prompt_to_use(UUID(self.brain_id), self.prompt_id)\n        else:\n            return None\n\n    def model_compatible_with_function_calling(self):\n        if self.model in [\n            \"gpt-4o\",\n            \"gpt-4-turbo\",\n            \"gpt-4-turbo-2024-04-09\",\n            \"gpt-4-turbo-preview\",\n            \"gpt-4-0125-preview\",\n            \"gpt-4-1106-preview\",\n            \"gpt-4\",\n            \"gpt-4-0613\",\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-0125\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-3.5-turbo-0613\",\n        ]:\n            return True\n        return False\n\n    supabase_client: Optional[Client] = None\n    vector_store: Optional[CustomSupabaseVectorStore] = None\n    qa: Optional[ConversationalRetrievalChain] = None\n    prompt_id: Optional[UUID] = None\n\n    def __init__(\n        self,\n        model: str,\n        brain_id: str,\n        chat_id: str,\n        streaming: bool = False,\n        prompt_id: Optional[UUID] = None,\n        max_tokens: int = 2000,\n        max_input: int = 2000,\n        **kwargs,\n    ):\n        super().__init__(\n            model=model,\n            brain_id=brain_id,\n            chat_id=chat_id,\n            streaming=streaming,\n            max_tokens=max_tokens,\n            max_input=max_input,\n            **kwargs,\n        )\n        self.supabase_client = self._create_supabase_client()\n        self.vector_store = self._create_vector_store()\n        self.prompt_id = prompt_id\n        self.max_tokens = max_tokens\n        self.max_input = max_input\n        self.model = model\n        self.brain_id = brain_id\n        self.chat_id = chat_id\n        self.streaming = streaming\n        self.knowledge_service = Knowledges()\n\n    def _create_supabase_client(self) -> Client:\n        return get_supabase_client()\n\n    def _create_vector_store(self) -> CustomSupabaseVectorStore:\n        return CustomSupabaseVectorStore(\n            self.supabase_client,\n            self.embeddings,\n            table_name=\"vectors\",\n            brain_id=self.brain_id,\n            max_input=self.max_input,\n        )\n\n    def _create_llm(\n        self,\n        callbacks,\n        model,\n        streaming=False,\n        temperature=0,\n    ) -> BaseLLM:\n        \"\"\"\n        Create a LLM with the given parameters\n        \"\"\"\n        if streaming and callbacks is None:\n            raise ValueError(\n                \"Callbacks must be provided when using streaming language models\"\n            )\n\n        api_base = None\n        if self.brain_settings.ollama_api_base_url and model.startswith(\"ollama\"):\n            api_base = (\n                self.brain_settings.ollama_api_base_url  # pyright: ignore reportPrivateUsage=none\n            )\n\n        return ChatLiteLLM(\n            temperature=temperature,\n            max_tokens=self.max_tokens,\n            model=model,\n            streaming=streaming,\n            verbose=False,\n            callbacks=callbacks,\n            api_base=api_base,\n        )  # pyright: ignore reportPrivateUsage=none\n\n    def _combine_documents(\n        self, docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n    ):\n        # for each docs, add an index in the metadata to be able to cite the sources\n        for doc, index in zip(docs, range(len(docs))):\n            doc.metadata[\"index\"] = index\n        doc_strings = [format_document(doc, document_prompt) for doc in docs]\n        return document_separator.join(doc_strings)\n\n    def get_retriever(self):\n        return self.vector_store.as_retriever()\n\n    def filter_history(\n        self, chat_history, max_history: int = 10, max_tokens: int = 2000\n    ):\n        \"\"\"\n        Filter out the chat history to only include the messages that are relevant to the current question\n\n        Takes in a chat_history= [HumanMessage(content='Qui est Chlo\u00e9 ? '), AIMessage(content=\"Chlo\u00e9 est une salari\u00e9e travaillant pour l'entreprise Quivr en tant qu'AI Engineer, sous la direction de son sup\u00e9rieur hi\u00e9rarchique, Stanislas Girard.\"), HumanMessage(content='Dis moi en plus sur elle'), AIMessage(content=''), HumanMessage(content='Dis moi en plus sur elle'), AIMessage(content=\"D\u00e9sol\u00e9, je n'ai pas d'autres informations sur Chlo\u00e9 \u00e0 partir des fichiers fournis.\")]\n        Returns a filtered chat_history with in priority: first max_tokens, then max_history where a Human message and an AI message count as one pair\n        a token is 4 characters\n        \"\"\"\n        chat_history = chat_history[::-1]\n        total_tokens = 0\n        total_pairs = 0\n        filtered_chat_history = []\n        for i in range(0, len(chat_history), 2):\n            if i + 1 < len(chat_history):\n                human_message = chat_history[i]\n                ai_message = chat_history[i + 1]\n                message_tokens = (\n                    len(human_message.content) + len(ai_message.content)\n                ) // 4\n                if (\n                    total_tokens + message_tokens > max_tokens\n                    or total_pairs >= max_history\n                ):\n                    break\n                filtered_chat_history.append(human_message)\n                filtered_chat_history.append(ai_message)\n                total_tokens += message_tokens\n                total_pairs += 1\n        chat_history = filtered_chat_history[::-1]\n\n        return chat_history\n\n    def get_chain(self):\n\n        list_files_array = self.knowledge_service.get_all_knowledge_in_brain(\n            self.brain_id\n        )  # pyright: ignore reportPrivateUsage=none\n\n        list_files_array = [file.file_name or file.url for file in list_files_array]\n        # Max first 10 files\n        if len(list_files_array) > 20:\n            list_files_array = list_files_array[:20]\n\n        list_files = \"\\n\".join(list_files_array) if list_files_array else \"None\"\n\n        compressor = None\n        if os.getenv(\"COHERE_API_KEY\"):\n            compressor = CohereRerank(top_n=20)\n        else:\n            compressor = FlashrankRerank(model=\"ms-marco-TinyBERT-L-2-v2\", top_n=20)\n\n        retriever_doc = self.get_retriever()\n        compression_retriever = ContextualCompressionRetriever(\n            base_compressor=compressor, base_retriever=retriever_doc\n        )\n\n        loaded_memory = RunnablePassthrough.assign(\n            chat_history=RunnableLambda(\n                lambda x: self.filter_history(x[\"chat_history\"]),\n            ),\n            question=lambda x: x[\"question\"],\n        )\n\n        api_base = None\n        if self.brain_settings.ollama_api_base_url and self.model.startswith(\"ollama\"):\n            api_base = self.brain_settings.ollama_api_base_url\n\n        standalone_question = {\n            \"standalone_question\": {\n                \"question\": lambda x: x[\"question\"],\n                \"chat_history\": itemgetter(\"chat_history\"),\n            }\n            | CONDENSE_QUESTION_PROMPT\n            | ChatLiteLLM(temperature=0, model=self.model, api_base=api_base)\n            | StrOutputParser(),\n        }\n\n        prompt_custom_user = self.prompt_to_use()\n        prompt_to_use = \"None\"\n        if prompt_custom_user:\n            prompt_to_use = prompt_custom_user.content\n\n        # Now we retrieve the documents\n        retrieved_documents = {\n            \"docs\": itemgetter(\"standalone_question\") | compression_retriever,\n            \"question\": lambda x: x[\"standalone_question\"],\n            \"custom_instructions\": lambda x: prompt_to_use,\n        }\n\n        final_inputs = {\n            \"context\": lambda x: self._combine_documents(x[\"docs\"]),\n            \"question\": itemgetter(\"question\"),\n            \"custom_instructions\": itemgetter(\"custom_instructions\"),\n            \"files\": lambda x: list_files,\n        }\n        llm = ChatLiteLLM(\n            max_tokens=self.max_tokens,\n            model=self.model,\n            temperature=self.temperature,\n            api_base=api_base,\n        )  # pyright: ignore reportPrivateUsage=none\n        if self.model_compatible_with_function_calling():\n\n            # And finally, we do the part that returns the answers\n            llm_function = ChatOpenAI(\n                max_tokens=self.max_tokens,\n                model=self.model,\n                temperature=self.temperature,\n            )\n            llm = llm_function.bind_tools(\n                [cited_answer],\n                tool_choice=\"cited_answer\",\n            )\n\n        answer = {\n            \"answer\": final_inputs | ANSWER_PROMPT | llm,\n            \"docs\": itemgetter(\"docs\"),\n        }\n\n        return loaded_memory | standalone_question | retrieved_documents | answer\n", "backend/modules/brain/rags/rag_interface.py": "from abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nfrom langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain_core.retrievers import BaseRetriever\n\n\nclass RAGInterface(ABC):\n    @abstractmethod\n    def get_doc_chain(\n        self,\n        streaming: bool,\n        callbacks: Optional[List[AsyncIteratorCallbackHandler]] = None,\n    ) -> BaseCombineDocumentsChain:\n        raise NotImplementedError(\n            \"get_doc_chain is an abstract method and must be implemented\"\n        )\n\n    @abstractmethod\n    def get_question_generation_llm(self) -> LLMChain:\n        raise NotImplementedError(\n            \"get_question_generation_llm is an abstract method and must be implemented\"\n        )\n\n    @abstractmethod\n    def get_retriever(self) -> BaseRetriever:\n        raise NotImplementedError(\n            \"get_retriever is an abstract method and must be implemented\"\n        )\n", "backend/modules/brain/rags/__init__.py": "", "backend/modules/brain/entity/api_brain_definition_entity.py": "from enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, Extra\n\n\nclass ApiBrainDefinitionSchemaProperty(BaseModel, extra=Extra.forbid):\n    type: str\n    description: str\n    enum: Optional[list] = None\n    name: str\n\n    def dict(self, **kwargs):\n        result = super().dict(**kwargs)\n        if \"enum\" in result and result[\"enum\"] is None:\n            del result[\"enum\"]\n        return result\n\n\nclass ApiBrainDefinitionSchema(BaseModel, extra=Extra.forbid):\n    properties: list[ApiBrainDefinitionSchemaProperty] = []\n    required: list[str] = []\n\n\nclass ApiBrainDefinitionSecret(BaseModel, extra=Extra.forbid):\n    name: str\n    type: str\n    description: Optional[str] = None\n\n\nclass ApiBrainAllowedMethods(str, Enum):\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    DELETE = \"DELETE\"\n\n\nclass ApiBrainDefinitionEntity(BaseModel, extra=Extra.forbid):\n    brain_id: UUID\n    method: ApiBrainAllowedMethods\n    url: str\n    params: ApiBrainDefinitionSchema\n    search_params: ApiBrainDefinitionSchema\n    secrets: list[ApiBrainDefinitionSecret]\n    raw: bool = False\n    jq_instructions: Optional[str] = None\n", "backend/modules/brain/entity/composite_brain_connection_entity.py": "from uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass CompositeBrainConnectionEntity(BaseModel):\n    composite_brain_id: UUID\n    connected_brain_id: UUID\n", "backend/modules/brain/entity/brain_entity.py": "from datetime import datetime\nfrom enum import Enum\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom modules.brain.entity.api_brain_definition_entity import ApiBrainDefinitionEntity\nfrom modules.brain.entity.integration_brain import (\n    IntegrationDescriptionEntity,\n    IntegrationEntity,\n)\nfrom pydantic import BaseModel\n\n\nclass BrainType(str, Enum):\n    DOC = \"doc\"\n    API = \"api\"\n    COMPOSITE = \"composite\"\n    INTEGRATION = \"integration\"\n\n\nclass BrainEntity(BaseModel):\n    brain_id: UUID\n    name: str\n    description: Optional[str] = None\n    temperature: Optional[float] = None\n    model: Optional[str] = None\n    max_tokens: Optional[int] = None\n    status: Optional[str] = None\n    prompt_id: Optional[UUID] = None\n    last_update: datetime\n    brain_type: BrainType\n    brain_definition: Optional[ApiBrainDefinitionEntity] = None\n    connected_brains_ids: Optional[List[UUID]] = None\n    raw: Optional[bool] = None\n    jq_instructions: Optional[str] = None\n    integration: Optional[IntegrationEntity] = None\n    integration_description: Optional[IntegrationDescriptionEntity] = None\n\n    @property\n    def id(self) -> UUID:\n        return self.brain_id\n\n    def dict(self, **kwargs):\n        data = super().dict(\n            **kwargs,\n        )\n        data[\"id\"] = self.id\n        return data\n\n\nclass PublicBrain(BaseModel):\n    id: UUID\n    name: str\n    description: Optional[str] = None\n    number_of_subscribers: int = 0\n    last_update: str\n    brain_type: BrainType\n    brain_definition: Optional[ApiBrainDefinitionEntity] = None\n\n\nclass RoleEnum(str, Enum):\n    Viewer = \"Viewer\"\n    Editor = \"Editor\"\n    Owner = \"Owner\"\n\n\nclass BrainUser(BaseModel):\n    id: UUID\n    user_id: UUID\n    rights: RoleEnum\n    default_brain: bool = False\n\n\nclass MinimalUserBrainEntity(BaseModel):\n    id: UUID\n    name: str\n    rights: RoleEnum\n    status: str\n    brain_type: BrainType\n    description: str\n    integration_logo_url: str\n    max_files: int\n", "backend/modules/brain/entity/__init__.py": "from .api_brain_definition_entity import ApiBrainDefinitionEntity\n", "backend/modules/brain/entity/integration_brain.py": "from enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass IntegrationType(str, Enum):\n    CUSTOM = \"custom\"\n    SYNC = \"sync\"\n    DOC = \"doc\"\n\n\nclass IntegrationBrainTag(str, Enum):\n    NEW = \"new\"\n    RECOMMENDED = \"recommended\"\n    MOST_POPULAR = \"most_popular\"\n    PREMIUM = \"premium\"\n    COMING_SOON = \"coming_soon\"\n    COMMUNITY = \"community\"\n    DEPRECATED = \"deprecated\"\n\n\nclass IntegrationDescriptionEntity(BaseModel):\n    id: UUID\n    integration_name: str\n    integration_logo_url: Optional[str] = None\n    connection_settings: Optional[dict] = None\n    integration_type: IntegrationType\n    tags: Optional[list[IntegrationBrainTag]] = []\n    information: Optional[str] = None\n    description: str\n    max_files: int\n    allow_model_change: bool\n    integration_display_name: str\n    onboarding_brain: bool\n\n\nclass IntegrationEntity(BaseModel):\n    id: int\n    user_id: str\n    brain_id: str\n    integration_id: str\n    settings: Optional[dict] = None\n    credentials: Optional[dict] = None\n    last_synced: str\n", "backend/modules/brain/integrations/__init__.py": "", "backend/modules/brain/integrations/Claude/Brain.py": "import json\nfrom typing import AsyncIterable\nfrom uuid import UUID\n\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.chat.dto.chats import ChatQuestion\n\n\nclass ClaudeBrain(KnowledgeBrainQA):\n    \"\"\"\n    ClaudeBrain integrates with Claude model to provide conversational AI capabilities.\n    It leverages the Claude model for generating responses based on the provided context.\n\n    Attributes:\n        **kwargs: Arbitrary keyword arguments for KnowledgeBrainQA initialization.\n    \"\"\"\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the ClaudeBrain with the given arguments.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(\n            **kwargs,\n        )\n\n    def calculate_pricing(self):\n        \"\"\"\n        Calculates the pricing for using the ClaudeBrain.\n\n        Returns:\n            int: The pricing value.\n        \"\"\"\n        return 3\n\n    def get_chain(self):\n        \"\"\"\n        Constructs and returns the conversational chain for ClaudeBrain.\n\n        Returns:\n            A conversational chain object.\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are Claude powered by Quivr. You are an assistant. {custom_personality}\",\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n\n        chain = prompt | ChatLiteLLM(\n            model=\"claude-3-haiku-20240307\", max_tokens=self.max_tokens\n        )\n\n        return chain\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        \"\"\"\n        Generates a stream of responses for the given question.\n\n        Args:\n            chat_id (UUID): The chat session ID.\n            question (ChatQuestion): The question object.\n            save_answer (bool): Whether to save the answer.\n\n        Yields:\n            AsyncIterable: A stream of response strings.\n        \"\"\"\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        response_tokens = []\n\n        async for chunk in conversational_qa_chain.astream(\n            {\n                \"question\": question.question,\n                \"chat_history\": transformed_history,\n                \"custom_personality\": (\n                    self.prompt_to_use.content if self.prompt_to_use else None\n                ),\n            }\n        ):\n            response_tokens.append(chunk.content)\n            streamed_chat_history.assistant = chunk.content\n            yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n", "backend/modules/brain/integrations/Claude/__init__.py": "", "backend/modules/brain/integrations/GPT4/Brain.py": "import json\nimport operator\nfrom typing import Annotated, AsyncIterable, List, Optional, Sequence, TypedDict\nfrom uuid import UUID\n\nfrom langchain.tools import BaseTool\nfrom langchain_core.messages import BaseMessage, ToolMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.tools import BaseTool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import ToolExecutor, ToolInvocation\nfrom logger import get_logger\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.chat.dto.chats import ChatQuestion\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.service.chat_service import ChatService\nfrom modules.tools import (\n    EmailSenderTool,\n    ImageGeneratorTool,\n    URLReaderTool,\n    WebSearchTool,\n)\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n\n\nlogger = get_logger(__name__)\n\nchat_service = ChatService()\n\n\nclass GPT4Brain(KnowledgeBrainQA):\n    \"\"\"\n    GPT4Brain integrates with GPT-4 to provide real-time answers and supports various tools to enhance its capabilities.\n\n    Available Tools:\n    - WebSearchTool: Performs web searches to find relevant information.\n    - ImageGeneratorTool: Generates images based on textual descriptions.\n    - URLReaderTool: Reads and summarizes content from URLs.\n    - EmailSenderTool: Sends emails with specified content.\n\n    Use Cases:\n    - WebSearchTool can be used to find the latest news articles on a specific topic or to gather information from various websites.\n    - ImageGeneratorTool is useful for creating visual content based on textual prompts, such as generating a company logo based on a description.\n    - URLReaderTool can be used to summarize articles or web pages, making it easier to quickly understand the content without reading the entire text.\n    - EmailSenderTool enables automated email sending, such as sending a summary of a meeting's minutes to all participants.\n    \"\"\"\n\n    tools: Optional[List[BaseTool]] = None\n    tool_executor: Optional[ToolExecutor] = None\n    function_model: ChatOpenAI = None\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        super().__init__(\n            **kwargs,\n        )\n        self.tools = [\n            WebSearchTool(),\n            ImageGeneratorTool(),\n            URLReaderTool(),\n            EmailSenderTool(user_email=self.user_email),\n        ]\n        self.tool_executor = ToolExecutor(tools=self.tools)\n\n    def calculate_pricing(self):\n        return 3\n\n    def should_continue(self, state):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        # Make sure there is a previous message\n\n        if last_message.tool_calls:\n            name = last_message.tool_calls[0][\"name\"]\n            if name == \"image-generator\":\n                return \"final\"\n        # If there is no function call, then we finish\n        if not last_message.tool_calls:\n            return \"end\"\n        # Otherwise if there is, we check if it's suppose to return direct\n        else:\n            return \"continue\"\n\n    # Define the function that calls the model\n    def call_model(self, state):\n        messages = state[\"messages\"]\n        response = self.function_model.invoke(messages)\n        # We return a list, because this will get added to the existing list\n        return {\"messages\": [response]}\n\n    # Define the function to execute tools\n    def call_tool(self, state):\n        messages = state[\"messages\"]\n        # Based on the continue condition\n        # we know the last message involves a function call\n        last_message = messages[-1]\n        # We construct an ToolInvocation from the function_call\n        tool_call = last_message.tool_calls[0]\n        tool_name = tool_call[\"name\"]\n        arguments = tool_call[\"args\"]\n\n        action = ToolInvocation(\n            tool=tool_call[\"name\"],\n            tool_input=tool_call[\"args\"],\n        )\n        # We call the tool_executor and get back a response\n        response = self.tool_executor.invoke(action)\n        # We use the response to create a FunctionMessage\n        function_message = ToolMessage(\n            content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n        )\n        # We return a list, because this will get added to the existing list\n        return {\"messages\": [function_message]}\n\n    def create_graph(self):\n        # Define a new graph\n        workflow = StateGraph(AgentState)\n\n        # Define the two nodes we will cycle between\n        workflow.add_node(\"agent\", self.call_model)\n        workflow.add_node(\"action\", self.call_tool)\n        workflow.add_node(\"final\", self.call_tool)\n\n        # Set the entrypoint as `agent`\n        # This means that this node is the first one called\n        workflow.set_entry_point(\"agent\")\n\n        # We now add a conditional edge\n        workflow.add_conditional_edges(\n            # First, we define the start node. We use `agent`.\n            # This means these are the edges taken after the `agent` node is called.\n            \"agent\",\n            # Next, we pass in the function that will determine which node is called next.\n            self.should_continue,\n            # Finally we pass in a mapping.\n            # The keys are strings, and the values are other nodes.\n            # END is a special node marking that the graph should finish.\n            # What will happen is we will call `should_continue`, and then the output of that\n            # will be matched against the keys in this mapping.\n            # Based on which one it matches, that node will then be called.\n            {\n                # If `tools`, then we call the tool node.\n                \"continue\": \"action\",\n                # Final call\n                \"final\": \"final\",\n                # Otherwise we finish.\n                \"end\": END,\n            },\n        )\n\n        # We now add a normal edge from `tools` to `agent`.\n        # This means that after `tools` is called, `agent` node is called next.\n        workflow.add_edge(\"action\", \"agent\")\n        workflow.add_edge(\"final\", END)\n\n        # Finally, we compile it!\n        # This compiles it into a LangChain Runnable,\n        # meaning you can use it as you would any other runnable\n        app = workflow.compile()\n        return app\n\n    def get_chain(self):\n        self.function_model = ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True)\n\n        self.function_model = self.function_model.bind_tools(self.tools)\n\n        graph = self.create_graph()\n\n        return graph\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        filtered_history = self.filter_history(transformed_history, 40, 2000)\n        response_tokens = []\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are GPT-4 powered by Quivr. You are an assistant. {custom_personality}\",\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n        prompt_formated = prompt.format_messages(\n            chat_history=filtered_history,\n            question=question.question,\n            custom_personality=(\n                self.prompt_to_use.content if self.prompt_to_use else None\n            ),\n        )\n\n        async for event in conversational_qa_chain.astream_events(\n            {\"messages\": prompt_formated},\n            config=config,\n            version=\"v1\",\n        ):\n            kind = event[\"event\"]\n            if kind == \"on_chat_model_stream\":\n                content = event[\"data\"][\"chunk\"].content\n                if content:\n                    # Empty content in the context of OpenAI or Anthropic usually means\n                    # that the model is asking for a tool to be invoked.\n                    # So we only print non-empty content\n                    response_tokens.append(content)\n                    streamed_chat_history.assistant = content\n                    yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n            elif kind == \"on_tool_start\":\n                print(\"--\")\n                print(\n                    f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n                )\n            elif kind == \"on_tool_end\":\n                print(f\"Done tool: {event['name']}\")\n                print(f\"Tool output was: {event['data'].get('output')}\")\n                print(\"--\")\n            elif kind == \"on_chain_end\":\n                output = event[\"data\"][\"output\"]\n                final_output = [item for item in output if \"final\" in item]\n                if final_output:\n                    if (\n                        final_output[0][\"final\"][\"messages\"][0].name\n                        == \"image-generator\"\n                    ):\n                        final_message = final_output[0][\"final\"][\"messages\"][0].content\n                        response_tokens.append(final_message)\n                        streamed_chat_history.assistant = final_message\n                        yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n\n    def generate_answer(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> GetChatHistoryOutput:\n        conversational_qa_chain = self.get_chain()\n        transformed_history, _ = self.initialize_streamed_chat_history(\n            chat_id, question\n        )\n        filtered_history = self.filter_history(transformed_history, 40, 2000)\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are GPT-4 powered by Quivr. You are an assistant. {custom_personality}\",\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n        prompt_formated = prompt.format_messages(\n            chat_history=filtered_history,\n            question=question.question,\n            custom_personality=(\n                self.prompt_to_use.content if self.prompt_to_use else None\n            ),\n        )\n        model_response = conversational_qa_chain.invoke(\n            {\"messages\": prompt_formated},\n            config=config,\n        )\n\n        answer = model_response[\"messages\"][-1].content\n\n        return self.save_non_streaming_answer(\n            chat_id=chat_id, question=question, answer=answer, metadata={}\n        )\n", "backend/modules/brain/integrations/GPT4/__init__.py": "", "backend/modules/brain/integrations/SQL/Brain.py": "import json\nfrom typing import AsyncIterable\nfrom uuid import UUID\n\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom modules.brain.integrations.SQL.SQL_connector import SQLConnector\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.brain.repository.integration_brains import IntegrationBrain\nfrom modules.chat.dto.chats import ChatQuestion\n\n\nclass SQLBrain(KnowledgeBrainQA, IntegrationBrain):\n    \"\"\"This is the Notion brain class. it is a KnowledgeBrainQA has the data is stored locally.\n    It is going to call the Data Store internally to get the data.\n\n    Args:\n        KnowledgeBrainQA (_type_): A brain that store the knowledge internaly\n    \"\"\"\n\n    uri: str = None\n    db: SQLDatabase = None\n    sql_connector: SQLConnector = None\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        super().__init__(\n            **kwargs,\n        )\n        self.sql_connector = SQLConnector(self.brain_id, self.user_id)\n\n    def get_schema(self, _):\n        return self.db.get_table_info()\n\n    def run_query(self, query):\n        return self.db.run(query)\n\n    def get_chain(self):\n        template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n        {schema}\n\n        Question: {question}\n        SQL Query:\"\"\"\n        prompt = ChatPromptTemplate.from_template(template)\n\n        self.db = SQLDatabase.from_uri(self.sql_connector.credentials[\"uri\"])\n\n        api_base = None\n        if self.brain_settings.ollama_api_base_url and self.model.startswith(\"ollama\"):\n            api_base = self.brain_settings.ollama_api_base_url\n\n        model = ChatLiteLLM(model=self.model, api_base=api_base)\n\n        sql_response = (\n            RunnablePassthrough.assign(schema=self.get_schema)\n            | prompt\n            | model.bind(stop=[\"\\nSQLResult:\"])\n            | StrOutputParser()\n        )\n\n        template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response and the query that was used to generate it.:\n            {schema}\n\n            Question: {question}\n            SQL Query: {query}\n            SQL Response: {response}\"\"\"\n        prompt_response = ChatPromptTemplate.from_template(template)\n\n        full_chain = (\n            RunnablePassthrough.assign(query=sql_response).assign(\n                schema=self.get_schema,\n                response=lambda x: self.db.run(x[\"query\"]),\n            )\n            | prompt_response\n            | model\n        )\n\n        return full_chain\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        response_tokens = []\n\n        async for chunk in conversational_qa_chain.astream(\n            {\n                \"question\": question.question,\n            }\n        ):\n            response_tokens.append(chunk.content)\n            streamed_chat_history.assistant = chunk.content\n            yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n", "backend/modules/brain/integrations/SQL/SQL_connector.py": "from logger import get_logger\nfrom modules.brain.entity.integration_brain import IntegrationEntity\nfrom modules.brain.repository.integration_brains import IntegrationBrain\nfrom modules.knowledge.repository.knowledge_interface import KnowledgeInterface\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\n\nlogger = get_logger(__name__)\n\n\nclass SQLConnector(IntegrationBrain):\n    \"\"\"A class to interact with an SQL database\"\"\"\n\n    credentials: dict[str, str] = None\n    integration_details: IntegrationEntity = None\n    brain_id: str = None\n    user_id: str = None\n    knowledge_service: KnowledgeInterface\n\n    def __init__(self, brain_id: str, user_id: str):\n        super().__init__()\n        self.brain_id = brain_id\n        self.user_id = user_id\n        self._load_credentials()\n        self.knowledge_service = KnowledgeService()\n\n    def _load_credentials(self) -> dict[str, str]:\n        \"\"\"Load the Notion credentials\"\"\"\n        self.integration_details = self.get_integration_brain(\n            self.brain_id\n        )\n        if self.credentials is None:\n            logger.info(\"Loading Notion credentials\")\n            self.integration_details.credentials = {\n                \"uri\": self.integration_details.settings.get(\"uri\", \"\")\n            }\n            self.update_integration_brain(\n                self.brain_id, self.user_id, self.integration_details\n            )\n            self.credentials = self.integration_details.credentials\n        else:  # pragma: no cover\n            self.credentials = self.integration_details.credentials\n", "backend/modules/brain/integrations/SQL/__init__.py": "", "backend/modules/brain/integrations/Big/Brain.py": "import json\nfrom typing import AsyncIterable\nfrom uuid import UUID\n\nfrom langchain.chains import ConversationalRetrievalChain, LLMChain\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_core.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom logger import get_logger\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.chat.dto.chats import ChatQuestion\n\nlogger = get_logger(__name__)\n\n\nclass BigBrain(KnowledgeBrainQA):\n    \"\"\"\n    The BigBrain class integrates advanced conversational retrieval and language model chains\n    to provide comprehensive and context-aware responses to user queries.\n\n    It leverages a combination of document retrieval, question condensation, and document-based\n    question answering to generate responses that are informed by a wide range of knowledge sources.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the BigBrain class with specific configurations.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(\n            **kwargs,\n        )\n\n    def get_chain(self):\n        \"\"\"\n        Constructs and returns the conversational QA chain used by BigBrain.\n\n        Returns:\n            A ConversationalRetrievalChain instance.\n        \"\"\"\n        system_template = \"\"\"Combine these summaries in a way that makes sense and answer the user's question.\n        Use markdown or any other techniques to display the content in a nice and aerated way. Answer in the language of the question.\n        Here are user instructions on how to respond: {custom_personality}\n        ______________________\n        {summaries}\"\"\"\n        messages = [\n            SystemMessagePromptTemplate.from_template(system_template),\n            HumanMessagePromptTemplate.from_template(\"{question}\"),\n        ]\n        CHAT_COMBINE_PROMPT = ChatPromptTemplate.from_messages(messages)\n\n        ### Question prompt\n        question_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. \n        Return any relevant text verbatim. Return the answer in the same language as the question. If the answer is not in the text, just say nothing in the same language as the question.\n        {context}\n        Question: {question}\n        Relevant text, if any, else say Nothing:\"\"\"\n        QUESTION_PROMPT = PromptTemplate(\n            template=question_prompt_template, input_variables=[\"context\", \"question\"]\n        )\n\n        ### Condense Question Prompt\n\n        _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question in exactly the same language as the original question.\n\n        Chat History:\n        {chat_history}\n        Follow Up Input: {question}\n        Standalone question in same language as question:\"\"\"\n        CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n\n        api_base = None\n        if self.brain_settings.ollama_api_base_url and self.model.startswith(\"ollama\"):\n            api_base = self.brain_settings.ollama_api_base_url\n\n        llm = ChatLiteLLM(\n            temperature=0,\n            model=self.model,\n            api_base=api_base,\n            max_tokens=self.max_tokens,\n        )\n\n        retriever_doc = self.knowledge_qa.get_retriever()\n\n        question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n        doc_chain = load_qa_chain(\n            llm,\n            chain_type=\"map_reduce\",\n            question_prompt=QUESTION_PROMPT,\n            combine_prompt=CHAT_COMBINE_PROMPT,\n        )\n\n        chain = ConversationalRetrievalChain(\n            retriever=retriever_doc,\n            question_generator=question_generator,\n            combine_docs_chain=doc_chain,\n        )\n\n        return chain\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        \"\"\"\n        Generates a stream of responses for a given question in real-time.\n\n        Args:\n            chat_id (UUID): The unique identifier for the chat session.\n            question (ChatQuestion): The question object containing the user's query.\n            save_answer (bool): Flag indicating whether to save the answer to the chat history.\n\n        Returns:\n            An asynchronous iterable of response strings.\n        \"\"\"\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        response_tokens = []\n\n        async for chunk in conversational_qa_chain.astream(\n            {\n                \"question\": question.question,\n                \"chat_history\": transformed_history,\n                \"custom_personality\": (\n                    self.prompt_to_use.content if self.prompt_to_use else None\n                ),\n            }\n        ):\n            if \"answer\" in chunk:\n                response_tokens.append(chunk[\"answer\"])\n                streamed_chat_history.assistant = chunk[\"answer\"]\n                yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n", "backend/modules/brain/integrations/Big/__init__.py": "", "backend/modules/brain/integrations/Multi_Contract/Brain.py": "import datetime\nfrom operator import itemgetter\nfrom typing import List\n\nfrom langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain_core.pydantic_v1 import Field as FieldV1\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\nfrom logger import get_logger\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\n\nlogger = get_logger(__name__)\n\n\nclass cited_answer(BaseModelV1):\n    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n\n    thoughts: str = FieldV1(\n        ...,\n        description=\"\"\"Description of the thought process, based only on the given sources. \n        Cite the text as much as possible and give the document name it appears in. In the format : 'Doc_name states : cited_text'. Be the most \n        procedural as possible.\"\"\",\n    )\n    answer: str = FieldV1(\n        ...,\n        description=\"The answer to the user question, which is based only on the given sources.\",\n    )\n    citations: List[int] = FieldV1(\n        ...,\n        description=\"The integer IDs of the SPECIFIC sources which justify the answer.\",\n    )\n\n    thoughts: str = FieldV1(\n        ...,\n        description=\"Explain shortly what you did to find the answer and what you used by citing the sources by their name.\",\n    )\n    followup_questions: List[str] = FieldV1(\n        ...,\n        description=\"Generate up to 3 follow-up questions that could be asked based on the answer given or context provided.\",\n    )\n\n\n# First step is to create the Rephrasing Prompt\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Keep as much details as possible from previous messages. Keep entity names and all.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n\n# Next is the answering prompt\n\ntemplate_answer = \"\"\"\nContext:\n{context}\n\nUser Question: {question}\nAnswer:\n\"\"\"\n\ntoday_date = datetime.datetime.now().strftime(\"%B %d, %Y\")\n\nsystem_message_template = (\n    f\"Your name is Quivr. You're a helpful assistant. Today's date is {today_date}.\"\n)\n\nsystem_message_template += \"\"\"\nWhen answering use markdown neat.\nAnswer in a concise and clear manner.\nUse the following pieces of context from files provided by the user to answer the users.\nAnswer in the same language as the user question.\nIf you don't know the answer with the context provided from the files, just say that you don't know, don't try to make up an answer.\nDon't cite the source id in the answer objects, but you can use the source to answer the question.\nYou have access to the files to answer the user question (limited to first 20 files):\n{files}\n\nIf not None, User instruction to follow to answer: {custom_instructions}\nDon't cite the source id in the answer objects, but you can use the source to answer the question.\n\"\"\"\n\n\nANSWER_PROMPT = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(system_message_template),\n        HumanMessagePromptTemplate.from_template(template_answer),\n    ]\n)\n\n\n# How we format documents\n\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n    template=\"Source: {index} \\n {page_content}\"\n)\n\n\nclass MultiContractBrain(KnowledgeBrainQA):\n    \"\"\"\n    The MultiContract class integrates advanced conversational retrieval and language model chains\n    to provide comprehensive and context-aware responses to user queries.\n\n    It leverages a combination of document retrieval, question condensation, and document-based\n    question answering to generate responses that are informed by a wide range of knowledge sources.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the MultiContract class with specific configurations.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(\n            **kwargs,\n        )\n\n    def get_chain(self):\n\n        list_files_array = (\n            self.knowledge_qa.knowledge_service.get_all_knowledge_in_brain(\n                self.brain_id\n            )\n        )  # pyright: ignore reportPrivateUsage=none\n\n        list_files_array = [file.file_name for file in list_files_array]\n        # Max first 10 files\n        if len(list_files_array) > 20:\n            list_files_array = list_files_array[:20]\n\n        list_files = \"\\n\".join(list_files_array) if list_files_array else \"None\"\n\n        retriever_doc = self.knowledge_qa.get_retriever()\n\n        loaded_memory = RunnablePassthrough.assign(\n            chat_history=RunnableLambda(\n                lambda x: self.filter_history(x[\"chat_history\"]),\n            ),\n            question=lambda x: x[\"question\"],\n        )\n\n        api_base = None\n        if self.brain_settings.ollama_api_base_url and self.model.startswith(\"ollama\"):\n            api_base = self.brain_settings.ollama_api_base_url\n\n        standalone_question = {\n            \"standalone_question\": {\n                \"question\": lambda x: x[\"question\"],\n                \"chat_history\": itemgetter(\"chat_history\"),\n            }\n            | CONDENSE_QUESTION_PROMPT\n            | ChatLiteLLM(temperature=0, model=self.model, api_base=api_base)\n            | StrOutputParser(),\n        }\n\n        knowledge_qa = self.knowledge_qa\n        prompt_custom_user = knowledge_qa.prompt_to_use()\n        prompt_to_use = \"None\"\n        if prompt_custom_user:\n            prompt_to_use = prompt_custom_user.content\n\n        # Now we retrieve the documents\n        retrieved_documents = {\n            \"docs\": itemgetter(\"standalone_question\") | retriever_doc,\n            \"question\": lambda x: x[\"standalone_question\"],\n            \"custom_instructions\": lambda x: prompt_to_use,\n        }\n\n        final_inputs = {\n            \"context\": lambda x: self.knowledge_qa._combine_documents(x[\"docs\"]),\n            \"question\": itemgetter(\"question\"),\n            \"custom_instructions\": itemgetter(\"custom_instructions\"),\n            \"files\": lambda x: list_files,\n        }\n        llm = ChatLiteLLM(\n            max_tokens=self.max_tokens,\n            model=self.model,\n            temperature=self.temperature,\n            api_base=api_base,\n        )  # pyright: ignore reportPrivateUsage=none\n        if self.model_compatible_with_function_calling(self.model):\n\n            # And finally, we do the part that returns the answers\n            llm_function = ChatOpenAI(\n                max_tokens=self.max_tokens,\n                model=self.model,\n                temperature=self.temperature,\n            )\n            llm = llm_function.bind_tools(\n                [cited_answer],\n                tool_choice=\"cited_answer\",\n            )\n\n        answer = {\n            \"answer\": final_inputs | ANSWER_PROMPT | llm,\n            \"docs\": itemgetter(\"docs\"),\n        }\n\n        return loaded_memory | standalone_question | retrieved_documents | answer\n", "backend/modules/brain/integrations/Multi_Contract/__init__.py": "", "backend/modules/brain/integrations/Proxy/Brain.py": "import json\nfrom typing import AsyncIterable\nfrom uuid import UUID\n\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom logger import get_logger\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.chat.dto.chats import ChatQuestion\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.service.chat_service import ChatService\n\nlogger = get_logger(__name__)\n\nchat_service = ChatService()\n\n\nclass ProxyBrain(KnowledgeBrainQA):\n    \"\"\"\n    ProxyBrain class serves as a proxy to utilize various language models for generating responses.\n    It dynamically selects and uses the appropriate language model based on the provided context and question.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the ProxyBrain with the given arguments.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(\n            **kwargs,\n        )\n\n    def get_chain(self):\n        \"\"\"\n        Constructs and returns the conversational chain for ProxyBrain.\n\n        Returns:\n            A conversational chain object.\n        \"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are Quivr. You are an assistant. {custom_personality}\",\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n\n        chain = prompt | ChatLiteLLM(model=self.model, max_tokens=self.max_tokens)\n\n        return chain\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        \"\"\"\n        Generates a stream of responses for the given question.\n\n        Args:\n            chat_id (UUID): The chat session ID.\n            question (ChatQuestion): The question object.\n            save_answer (bool): Whether to save the answer.\n\n        Yields:\n            AsyncIterable: A stream of response strings.\n        \"\"\"\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        response_tokens = []\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        async for chunk in conversational_qa_chain.astream(\n            {\n                \"question\": question.question,\n                \"chat_history\": transformed_history,\n                \"custom_personality\": (\n                    self.prompt_to_use.content if self.prompt_to_use else None\n                ),\n            },\n            config=config,\n        ):\n            response_tokens.append(chunk.content)\n            streamed_chat_history.assistant = chunk.content\n            yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n\n    def generate_answer(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> GetChatHistoryOutput:\n        \"\"\"\n        Generates a non-streaming answer for the given question.\n\n        Args:\n            chat_id (UUID): The chat session ID.\n            question (ChatQuestion): The question object.\n            save_answer (bool): Whether to save the answer.\n\n        Returns:\n            GetChatHistoryOutput: The chat history output object containing the answer.\n        \"\"\"\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n        model_response = conversational_qa_chain.invoke(\n            {\n                \"question\": question.question,\n                \"chat_history\": transformed_history,\n                \"custom_personality\": (\n                    self.prompt_to_use.content if self.prompt_to_use else None\n                ),\n            },\n            config=config,\n        )\n\n        answer = model_response.content\n\n        return self.save_non_streaming_answer(\n            chat_id=chat_id,\n            question=question,\n            answer=answer,\n        )\n", "backend/modules/brain/integrations/Proxy/__init__.py": "", "backend/modules/brain/integrations/Self/Brain.py": "import json\nfrom typing import AsyncIterable, List\nfrom uuid import UUID\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    PromptTemplate,\n)\nfrom langchain_core.pydantic_v1 import BaseModel as BaseModelV1\nfrom langchain_core.pydantic_v1 import Field as FieldV1\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom logger import get_logger\nfrom modules.brain.knowledge_brain_qa import KnowledgeBrainQA\nfrom modules.chat.dto.chats import ChatQuestion\nfrom modules.chat.dto.outputs import GetChatHistoryOutput\nfrom modules.chat.service.chat_service import ChatService\nfrom typing_extensions import TypedDict\n\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    documents: List[str]\n\n\n# Data model\nclass GradeDocuments(BaseModelV1):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = FieldV1(\n        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n    )\n\n\nclass GradeHallucinations(BaseModelV1):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    binary_score: str = FieldV1(\n        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n    )\n\n\n# Data model\nclass GradeAnswer(BaseModelV1):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    binary_score: str = FieldV1(\n        description=\"Answer addresses the question, 'yes' or 'no'\"\n    )\n\n\nlogger = get_logger(__name__)\n\nchat_service = ChatService()\n\n\nclass SelfBrain(KnowledgeBrainQA):\n    \"\"\"\n    GPT4Brain integrates with GPT-4 to provide real-time answers and supports various tools to enhance its capabilities.\n\n    Available Tools:\n    - WebSearchTool: Performs web searches to find relevant information.\n    - ImageGeneratorTool: Generates images based on textual descriptions.\n    - URLReaderTool: Reads and summarizes content from URLs.\n    - EmailSenderTool: Sends emails with specified content.\n\n    Use Cases:\n    - WebSearchTool can be used to find the latest news articles on a specific topic or to gather information from various websites.\n    - ImageGeneratorTool is useful for creating visual content based on textual prompts, such as generating a company logo based on a description.\n    - URLReaderTool can be used to summarize articles or web pages, making it easier to quickly understand the content without reading the entire text.\n    - EmailSenderTool enables automated email sending, such as sending a summary of a meeting's minutes to all participants.\n    \"\"\"\n\n    max_input: int = 10000\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        super().__init__(\n            **kwargs,\n        )\n\n    def calculate_pricing(self):\n        return 3\n\n    def retrieval_grade(self):\n        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n        structured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n        # Prompt\n        system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n            It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n        grade_prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system),\n                (\n                    \"human\",\n                    \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\",\n                ),\n            ]\n        )\n\n        retrieval_grader = grade_prompt | structured_llm_grader\n\n        return retrieval_grader\n\n    def generation_rag(self):\n        # Prompt\n        human_prompt = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n\n        Question: {question} \n\n        Context: {context} \n\n        Answer:\n        \"\"\"\n        prompt_human = PromptTemplate.from_template(human_prompt)\n        # LLM\n        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n        # Chain\n        rag_chain = prompt_human | llm | StrOutputParser()\n\n        return rag_chain\n\n    def hallucination_grader(self):\n        # LLM with function call\n        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n        structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\n        # Prompt\n        system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n            Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n        hallucination_prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system),\n                (\n                    \"human\",\n                    \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\",\n                ),\n            ]\n        )\n\n        hallucination_grader = hallucination_prompt | structured_llm_grader\n\n        return hallucination_grader\n\n    def answer_grader(self):\n        # LLM with function call\n        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n        structured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n        # Prompt\n        system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n            Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n        answer_prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system),\n                (\n                    \"human\",\n                    \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\",\n                ),\n            ]\n        )\n\n        answer_grader = answer_prompt | structured_llm_grader\n\n        return answer_grader\n\n    def question_rewriter(self):\n        # LLM\n        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n        # Prompt\n        system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n            for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n        re_write_prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system),\n                (\n                    \"human\",\n                    \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n                ),\n            ]\n        )\n\n        question_rewriter = re_write_prompt | llm | StrOutputParser()\n\n        return question_rewriter\n\n    def get_chain(self):\n\n        graph = self.create_graph()\n\n        return graph\n\n    def create_graph(self):\n\n        workflow = StateGraph(GraphState)\n\n        # Define the nodes\n        workflow.add_node(\"retrieve\", self.retrieve)  # retrieve\n        workflow.add_node(\"grade_documents\", self.grade_documents)  # grade documents\n        workflow.add_node(\"generate\", self.generate)  # generatae\n        workflow.add_node(\"transform_query\", self.transform_query)  # transform_query\n\n        # Build graph\n        workflow.set_entry_point(\"retrieve\")\n        workflow.add_edge(\"retrieve\", \"grade_documents\")\n        workflow.add_conditional_edges(\n            \"grade_documents\",\n            self.decide_to_generate,\n            {\n                \"transform_query\": \"transform_query\",\n                \"generate\": \"generate\",\n            },\n        )\n        workflow.add_edge(\"transform_query\", \"retrieve\")\n        workflow.add_conditional_edges(\n            \"generate\",\n            self.grade_generation_v_documents_and_question,\n            {\n                \"not supported\": \"generate\",\n                \"useful\": END,\n                \"not useful\": \"transform_query\",\n            },\n        )\n\n        # Compile\n        app = workflow.compile()\n        return app\n\n    def retrieve(self, state):\n        \"\"\"\n        Retrieve documents\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): New key added to state, documents, that contains retrieved documents\n        \"\"\"\n        print(\"---RETRIEVE---\")\n        logger.info(\"Retrieving documents\")\n        question = state[\"question\"]\n        logger.info(f\"Question: {question}\")\n\n        # Retrieval\n        retriever = self.knowledge_qa.get_retriever()\n        documents = retriever.get_relevant_documents(question)\n        return {\"documents\": documents, \"question\": question}\n\n    def generate(self, state):\n        \"\"\"\n        Generate answer\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): New key added to state, generation, that contains LLM generation\n        \"\"\"\n        print(\"---GENERATE---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n\n        formatted_docs = format_docs(documents)\n        # RAG generation\n        generation = self.generation_rag().invoke(\n            {\"context\": formatted_docs, \"question\": question}\n        )\n        return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\n    def grade_documents(self, state):\n        \"\"\"\n        Determines whether the retrieved documents are relevant to the question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): Updates documents key with only filtered relevant documents\n        \"\"\"\n\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n\n        # Score each doc\n        filtered_docs = []\n        for d in documents:\n            score = self.retrieval_grade().invoke(\n                {\"question\": question, \"document\": d.page_content}\n            )\n            grade = score.binary_score\n            if grade == \"yes\":\n                print(\"---GRADE: DOCUMENT RELEVANT---\")\n                filtered_docs.append(d)\n            else:\n                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n                continue\n        return {\"documents\": filtered_docs, \"question\": question}\n\n    def transform_query(self, state):\n        \"\"\"\n        Transform the query to produce a better question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): Updates question key with a re-phrased question\n        \"\"\"\n\n        print(\"---TRANSFORM QUERY---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n\n        # Re-write question\n        better_question = self.question_rewriter().invoke({\"question\": question})\n        return {\"documents\": documents, \"question\": better_question}\n\n    def decide_to_generate(self, state):\n        \"\"\"\n        Determines whether to generate an answer, or re-generate a question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            str: Binary decision for next node to call\n        \"\"\"\n\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        question = state[\"question\"]\n        filtered_documents = state[\"documents\"]\n\n        if not filtered_documents:\n            # All documents have been filtered check_relevance\n            # We will re-generate a new query\n            print(\n                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n            )\n            return \"transform_query\"\n        else:\n            # We have relevant documents, so generate answer\n            print(\"---DECISION: GENERATE---\")\n            return \"generate\"\n\n    def grade_generation_v_documents_and_question(self, state):\n        \"\"\"\n        Determines whether the generation is grounded in the document and answers question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            str: Decision for next node to call\n        \"\"\"\n\n        print(\"---CHECK HALLUCINATIONS---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n        generation = state[\"generation\"]\n\n        score = self.hallucination_grader().invoke(\n            {\"documents\": documents, \"generation\": generation}\n        )\n        grade = score.binary_score\n\n        # Check hallucination\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n            # Check question-answering\n            print(\"---GRADE GENERATION vs QUESTION---\")\n            score = self.answer_grader().invoke(\n                {\"question\": question, \"generation\": generation}\n            )\n            grade = score.binary_score\n            if grade == \"yes\":\n                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n                return \"useful\"\n            else:\n                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n                return \"not useful\"\n        else:\n            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n            return \"not supported\"\n\n    async def generate_stream(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> AsyncIterable:\n        conversational_qa_chain = self.get_chain()\n        transformed_history, streamed_chat_history = (\n            self.initialize_streamed_chat_history(chat_id, question)\n        )\n        filtered_history = self.filter_history(transformed_history, 40, 2000)\n        response_tokens = []\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are GPT-4 powered by Quivr. You are an assistant. {custom_personality}\",\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n        prompt_formated = prompt.format_messages(\n            chat_history=filtered_history,\n            question=question.question,\n            custom_personality=(\n                self.prompt_to_use.content if self.prompt_to_use else None\n            ),\n        )\n\n        async for event in conversational_qa_chain.astream(\n            {\"question\": question.question}, config=config\n        ):\n            for key, value in event.items():\n                if \"generation\" in value and value[\"generation\"] != \"\":\n                    response_tokens.append(value[\"generation\"])\n                    streamed_chat_history.assistant = value[\"generation\"]\n\n                    yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n\n        self.save_answer(question, response_tokens, streamed_chat_history, save_answer)\n\n    def generate_answer(\n        self, chat_id: UUID, question: ChatQuestion, save_answer: bool = True\n    ) -> GetChatHistoryOutput:\n        conversational_qa_chain = self.get_chain()\n        transformed_history, _ = self.initialize_streamed_chat_history(\n            chat_id, question\n        )\n        filtered_history = self.filter_history(transformed_history, 40, 2000)\n        config = {\"metadata\": {\"conversation_id\": str(chat_id)}}\n\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"You are GPT-4 powered by Quivr. You are an assistant. {custom_personality}\",\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"human\", \"{question}\"),\n            ]\n        )\n        prompt_formated = prompt.format_messages(\n            chat_history=filtered_history,\n            question=question.question,\n            custom_personality=(\n                self.prompt_to_use.content if self.prompt_to_use else None\n            ),\n        )\n        model_response = conversational_qa_chain.invoke(\n            {\"messages\": prompt_formated},\n            config=config,\n        )\n\n        answer = model_response[\"messages\"][-1].content\n\n        return self.save_non_streaming_answer(\n            chat_id=chat_id, question=question, answer=answer, metadata={}\n        )\n", "backend/modules/brain/integrations/Self/__init__.py": "", "backend/modules/brain/integrations/Notion/Notion_connector.py": "import os\nimport tempfile\nimport time\nfrom io import BytesIO\nfrom typing import Any, List, Optional\n\nimport requests\nfrom celery_config import celery\nfrom fastapi import UploadFile\nfrom logger import get_logger\nfrom modules.brain.entity.integration_brain import IntegrationEntity\nfrom modules.brain.repository.integration_brains import Integration, IntegrationBrain\nfrom modules.knowledge.dto.inputs import CreateKnowledgeProperties\nfrom modules.knowledge.repository.knowledge_interface import KnowledgeInterface\nfrom modules.knowledge.service.knowledge_service import KnowledgeService\nfrom modules.upload.service.upload_file import upload_file_storage\nfrom pydantic import BaseModel\n\nlogger = get_logger(__name__)\n\n\nclass NotionPage(BaseModel):\n    \"\"\"Represents a Notion Page object to be used in the NotionConnector class\"\"\"\n\n    id: str\n    created_time: str\n    last_edited_time: str\n    archived: bool\n    properties: dict[str, Any]\n    url: str\n\n\nclass NotionSearchResponse(BaseModel):\n    \"\"\"Represents the response from the Notion Search API\"\"\"\n\n    results: list[dict[str, Any]]\n    next_cursor: Optional[str] = None\n    has_more: bool = False\n\n\nclass NotionConnector(IntegrationBrain, Integration):\n    \"\"\"A class to interact with the Notion API\"\"\"\n\n    credentials: dict[str, str] = None\n    integration_details: IntegrationEntity = None\n    brain_id: str = None\n    user_id: str = None\n    knowledge_service: KnowledgeInterface\n    recursive_index_enabled: bool = False\n    max_pages: int = 100\n\n    def __init__(self, brain_id: str, user_id: str):\n        super().__init__()\n        self.brain_id = brain_id\n        self.user_id = user_id\n        self._load_credentials()\n        self.knowledge_service = KnowledgeService()\n\n    def _load_credentials(self) -> dict[str, str]:\n        \"\"\"Load the Notion credentials\"\"\"\n        self.integration_details = self.get_integration_brain(self.brain_id)\n        if self.credentials is None:\n            logger.info(\"Loading Notion credentials\")\n            self.integration_details.credentials = {\n                \"notion_integration_token\": self.integration_details.settings.get(\n                    \"notion_integration_token\", \"\"\n                )\n            }\n            self.update_integration_brain(\n                self.brain_id, self.user_id, self.integration_details\n            )\n            self.credentials = self.integration_details.credentials\n        else:  # pragma: no cover\n            self.credentials = self.integration_details.credentials\n\n    def _headers(self) -> dict[str, str]:\n        \"\"\"Get the headers for the Notion API\"\"\"\n        return {\n            \"Authorization\": f'Bearer {self.credentials[\"notion_integration_token\"]}',\n            \"Content-Type\": \"application/json\",\n            \"Notion-Version\": \"2022-06-28\",\n        }\n\n    def _search_notion(self, query_dict: dict[str, Any]) -> NotionSearchResponse:\n        \"\"\"\n        Search for pages from a Notion database.\n        \"\"\"\n        # Use self.credentials to authenticate the request\n        headers = self._headers()\n        res = requests.post(\n            \"https://api.notion.com/v1/search\",\n            headers=headers,\n            json=query_dict,\n            # Adjust the timeout as needed\n            timeout=10,\n        )\n        res.raise_for_status()\n        return NotionSearchResponse(**res.json())\n\n    def _fetch_blocks(self, page_id: str, cursor: str | None = None) -> dict[str, Any]:\n        \"\"\"\n        Fetch the blocks of a Notion page.\n        \"\"\"\n        logger.info(f\"Fetching blocks for page: {page_id}\")\n        headers = self._headers()\n        query_params = None if not cursor else {\"start_cursor\": cursor}\n        res = requests.get(\n            f\"https://api.notion.com/v1/blocks/{page_id}/children\",\n            params=query_params,\n            headers=headers,\n            timeout=10,\n        )\n        res.raise_for_status()\n        return res.json()\n\n    def _fetch_page(self, page_id: str) -> dict[str, Any]:\n        \"\"\"\n        Fetch a Notion page.\n        \"\"\"\n        logger.info(f\"Fetching page: {page_id}\")\n        headers = self._headers()\n        block_url = f\"https://api.notion.com/v1/pages/{page_id}\"\n        res = requests.get(\n            block_url,\n            headers=headers,\n            timeout=10,\n        )\n        try:\n            res.raise_for_status()\n        except Exception:\n            logger.exception(f\"Error fetching page - {res.json()}\")\n            return None\n        return NotionPage(**res.json())\n\n    def _read_blocks(\n        self, page_block_id: str\n    ) -> tuple[list[tuple[str, str]], list[str]]:\n        \"\"\"Reads blocks for a page\"\"\"\n        result_lines: list[tuple[str, str]] = []\n        child_pages: list[str] = []\n        cursor = None\n        while True:\n            data = self._fetch_blocks(page_block_id, cursor)\n\n            for result in data[\"results\"]:\n                result_block_id = result[\"id\"]\n                result_type = result[\"type\"]\n                result_obj = result[result_type]\n\n                cur_result_text_arr = []\n                if \"rich_text\" in result_obj:\n                    for rich_text in result_obj[\"rich_text\"]:\n                        # skip if doesn't have text object\n                        if \"text\" in rich_text:\n                            text = rich_text[\"text\"][\"content\"]\n                            cur_result_text_arr.append(text)\n\n                if result[\"has_children\"]:\n                    if result_type == \"child_page\":\n                        child_pages.append(result_block_id)\n                    else:\n                        logger.info(f\"Entering sub-block: {result_block_id}\")\n                        subblock_result_lines, subblock_child_pages = self._read_blocks(\n                            result_block_id\n                        )\n                        logger.info(f\"Finished sub-block: {result_block_id}\")\n                        result_lines.extend(subblock_result_lines)\n                        child_pages.extend(subblock_child_pages)\n\n                # if result_type == \"child_database\" and self.recursive_index_enabled:\n                #     child_pages.extend(self._read_pages_from_database(result_block_id))\n\n                cur_result_text = \"\\n\".join(cur_result_text_arr)\n                if cur_result_text:\n                    result_lines.append((cur_result_text, result_block_id))\n\n            if data[\"next_cursor\"] is None:\n                break\n\n            cursor = data[\"next_cursor\"]\n\n        return result_lines, child_pages\n\n    def _read_page_title(self, page: NotionPage) -> str:\n        \"\"\"Extracts the title from a Notion page\"\"\"\n        page_title = None\n        for _, prop in page.properties.items():\n            if prop[\"type\"] == \"title\" and len(prop[\"title\"]) > 0:\n                page_title = \" \".join([t[\"plain_text\"] for t in prop[\"title\"]]).strip()\n                break\n        if page_title is None:\n            page_title = f\"Untitled Page [{page.id}]\"\n        page_title = \"\".join(e for e in page_title if e.isalnum())\n        return page_title\n\n    def _read_page_url(self, page: NotionPage) -> str:\n        \"\"\"Extracts the URL from a Notion page\"\"\"\n        return page.url\n\n    def _read_pages_from_database(self, database_id: str) -> list[str]:\n        \"\"\"Reads pages from a Notion database\"\"\"\n        headers = self._headers()\n        res = requests.post(\n            f\"https://api.notion.com/v1/databases/{database_id}/query\",\n            headers=headers,\n            timeout=10,\n        )\n        res.raise_for_status()\n        return [page[\"id\"] for page in res.json()[\"results\"]]\n\n    def _read_page(self, page_id: str) -> tuple[str, list[str]]:\n        \"\"\"Reads a Notion page\"\"\"\n        page = self._fetch_page(page_id)\n        if page is None:\n            return None, None, None, None\n        page_title = self._read_page_title(page)\n        page_content, child_pages = self._read_blocks(page_id)\n        page_url = self._read_page_url(page)\n        return page_title, page_content, child_pages, page_url\n\n    def _filter_pages_by_time(\n        self,\n        pages: list[dict[str, Any]],\n        start: str,\n        filter_field: str = \"last_edited_time\",\n    ) -> list[NotionPage]:\n        filtered_pages: list[NotionPage] = []\n        start_time = time.mktime(\n            time.strptime(start, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        )  # Convert `start` to a float\n        for page in pages:\n            compare_time = time.mktime(\n                time.strptime(page[filter_field], \"%Y-%m-%dT%H:%M:%S.%f%z\")\n            )\n            if compare_time > start_time:  # Compare `compare_time` with `start_time`\n                filtered_pages += [NotionPage(**page)]\n        return filtered_pages\n\n    def get_all_pages(self) -> list[NotionPage]:\n        \"\"\"\n        Get all the pages from Notion.\n        \"\"\"\n        query_dict = {\n            \"filter\": {\"property\": \"object\", \"value\": \"page\"},\n            \"page_size\": 100,\n        }\n        max_pages = self.max_pages\n        pages_count = 0\n        while True:\n            search_response = self._search_notion(query_dict)\n            for page in search_response.results:\n                pages_count += 1\n                if pages_count > max_pages:\n                    break\n                yield NotionPage(**page)\n\n            if search_response.has_more:\n                query_dict[\"start_cursor\"] = search_response.next_cursor\n            else:\n                break\n\n    def add_file_to_knowledge(\n        self, page_content: List[tuple[str, str]], page_name: str, page_url: str\n    ):\n        \"\"\"\n        Add a file to the knowledge base\n        \"\"\"\n        logger.info(f\"Adding file to knowledge: {page_name}\")\n        filename_with_brain_id = (\n            str(self.brain_id) + \"/\" + str(page_name) + \"_notion.txt\"\n        )\n        try:\n            concatened_page_content = \"\"\n            if page_content:\n                for content in page_content:\n                    concatened_page_content += content[0] + \"\\n\"\n\n                # Create a BytesIO object from the content\n                content_io = BytesIO(concatened_page_content.encode(\"utf-8\"))\n\n                # Create a file of type UploadFile\n                file = UploadFile(filename=filename_with_brain_id, file=content_io)\n\n                # Write the UploadFile content to a temporary file\n                with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n                    temp_file.write(file.file.read())\n                    temp_file_path = temp_file.name\n\n                # Upload the temporary file to the knowledge base\n                response = upload_file_storage(\n                    temp_file_path, filename_with_brain_id, \"true\"\n                )\n                logger.info(f\"File {response} uploaded successfully\")\n\n                # Delete the temporary file\n                os.remove(temp_file_path)\n\n                knowledge_to_add = CreateKnowledgeProperties(\n                    brain_id=self.brain_id,\n                    file_name=page_name + \"_notion.txt\",\n                    extension=\"txt\",\n                    integration=\"notion\",\n                    integration_link=page_url,\n                )\n\n                added_knowledge = self.knowledge_service.add_knowledge(knowledge_to_add)\n                logger.info(f\"Knowledge {added_knowledge} added successfully\")\n\n                celery.send_task(\n                    \"process_file_and_notify\",\n                    kwargs={\n                        \"file_name\": filename_with_brain_id,\n                        \"file_original_name\": page_name + \"_notion.txt\",\n                        \"brain_id\": self.brain_id,\n                        \"delete_file\": True,\n                    },\n                )\n        except Exception:\n            logger.error(\"Error adding knowledge\")\n\n    def load(self):\n        \"\"\"\n        Get all the pages, blocks, databases from Notion into a single document per page\n        \"\"\"\n        all_pages = list(self.get_all_pages())  # Convert generator to list\n        documents = []\n        for page in all_pages:\n            logger.info(f\"Reading page: {page.id}\")\n            page_title, page_content, child_pages, page_url = self._read_page(page.id)\n            document = {\n                \"page_title\": page_title,\n                \"page_content\": page_content,\n                \"child_pages\": child_pages,\n                \"page_url\": page_url,\n            }\n            documents.append(document)\n            self.add_file_to_knowledge(page_content, page_title, page_url)\n        return documents\n\n    def poll(self):\n        \"\"\"\n        Update all the brains with the latest data from Notion\n        \"\"\"\n        integration = self.get_integration_brain(self.brain_id)\n        last_synced = integration.last_synced\n\n        query_dict = {\n            \"page_size\": self.max_pages,\n            \"sort\": {\"timestamp\": \"last_edited_time\", \"direction\": \"descending\"},\n            \"filter\": {\"property\": \"object\", \"value\": \"page\"},\n        }\n        documents = []\n\n        while True:\n            db_res = self._search_notion(query_dict)\n            pages = self._filter_pages_by_time(\n                db_res.results, last_synced, filter_field=\"last_edited_time\"\n            )\n            for page in pages:\n                logger.info(f\"Reading page: {page.id}\")\n                page_title, page_content, child_pages, page_url = self._read_page(\n                    page.id\n                )\n                document = {\n                    \"page_title\": page_title,\n                    \"page_content\": page_content,\n                    \"child_pages\": child_pages,\n                    \"page_url\": page_url,\n                }\n                documents.append(document)\n                self.add_file_to_knowledge(page_content, page_title, page_url)\n            if not db_res.has_more:\n                break\n            query_dict[\"start_cursor\"] = db_res.next_cursor\n        logger.info(\n            f\"last Synced: {self.update_last_synced(self.brain_id, self.user_id)}\"\n        )\n        return documents\n\n\nif __name__ == \"__main__\":\n\n    notion = NotionConnector(\n        brain_id=\"73f7d092-d596-4fd0-b24f-24031e9b53cd\",\n        user_id=\"39418e3b-0258-4452-af60-7acfcc1263ff\",\n    )\n\n    print(notion.poll())\n", "backend/modules/brain/integrations/Notion/Brain.py": "from modules.brain.knowledge_brain_qa import KnowledgeBrainQA\n\n\nclass NotionBrain(KnowledgeBrainQA):\n    \"\"\"\n    NotionBrain integrates with Notion to provide knowledge-based responses.\n    It leverages data stored in Notion to answer user queries.\n\n    Attributes:\n        **kwargs: Arbitrary keyword arguments for KnowledgeBrainQA initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NotionBrain with the given arguments.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(\n            **kwargs,\n        )\n", "backend/modules/brain/integrations/Notion/__init__.py": "", "backend/modules/analytics/__init__.py": "", "backend/modules/analytics/controller/__init__.py": "", "backend/modules/analytics/controller/analytics_routes.py": "from uuid import UUID\nfrom fastapi import APIRouter, Depends, Query\nfrom middlewares.auth.auth_bearer import AuthBearer, get_current_user\nfrom modules.analytics.entity.analytics import Range\nfrom modules.analytics.service.analytics_service import AnalyticsService\n\nanalytics_service = AnalyticsService()\nanalytics_router = APIRouter()\n\n@analytics_router.get(\n    \"/analytics/brains-usages\", dependencies=[Depends(AuthBearer())], tags=[\"Analytics\"]\n)\nasync def get_brains_usages(\n    user: UUID = Depends(get_current_user),\n    brain_id: UUID = Query(None),\n    graph_range: Range = Query(Range.WEEK, alias=\"graph_range\")\n):\n    \"\"\"\n    Get all user brains usages\n    \"\"\"\n\n    return analytics_service.get_brains_usages(user.id, graph_range, brain_id)", "backend/modules/analytics/repository/analytics.py": "from collections import defaultdict\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom models.settings import get_supabase_client\nfrom modules.analytics.entity.analytics import BrainsUsages, Range, Usage\nfrom modules.brain.service.brain_user_service import BrainUserService\n\nbrain_user_service = BrainUserService()\n\n\nclass Analytics:\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def get_brains_usages(\n        self, user_id: UUID, graph_range: Range, brain_id: Optional[UUID] = None\n    ) -> BrainsUsages:\n        user_brains = brain_user_service.get_user_brains(user_id)\n        if brain_id is not None:\n            user_brains = [brain for brain in user_brains if brain.id == brain_id]\n\n        usage_per_day = defaultdict(int)\n\n        brain_ids = [brain.id for brain in user_brains]\n        chat_history = (\n            self.db.from_(\"chat_history\")\n            .select(\"*\")\n            .in_(\"brain_id\", brain_ids)\n            .execute()\n        ).data\n\n        for chat in chat_history:\n            message_time = datetime.strptime(\n                chat[\"message_time\"], \"%Y-%m-%dT%H:%M:%S.%f\"\n            )\n            usage_per_day[message_time.date()] += 1\n\n        start_date = datetime.now().date() - timedelta(days=graph_range)\n        all_dates = [start_date + timedelta(days=i) for i in range(graph_range)]\n\n        for date in all_dates:\n            usage_per_day[date] += 0\n\n        usages = sorted(\n            [\n                Usage(date=date, usage_count=count)\n                for date, count in usage_per_day.items()\n                if start_date <= date <= datetime.now().date()\n            ],\n            key=lambda usage: usage.date,\n        )\n\n        return BrainsUsages(usages=usages)\n", "backend/modules/analytics/repository/analytics_interface.py": "from abc import ABC, abstractmethod\nfrom typing import Optional\nfrom uuid import UUID\nfrom modules.analytics.entity.analytics import BrainsUsages, Range\n\nclass AnalyticsInterface(ABC):\n    @abstractmethod\n    def get_brains_usages(self, user_id: UUID, graph_range: Range = Range.WEEK, brain_id: Optional[UUID] = None) -> BrainsUsages:\n        \"\"\"\n        Get user brains usage\n        Args:\n            user_id (UUID): The id of the user\n            brain_id (Optional[UUID]): The id of the brain, optional\n        \"\"\"\n        pass", "backend/modules/analytics/repository/__init__.py": "", "backend/modules/analytics/service/analytics_service.py": "\nfrom modules.analytics.repository.analytics import Analytics\nfrom modules.analytics.repository.analytics_interface import AnalyticsInterface\n\nclass AnalyticsService:\n    repository: AnalyticsInterface\n\n    def __init__(self):\n        self.repository = Analytics()\n\n    def get_brains_usages(self, user_id, graph_range, brain_id = None):\n\n        return self.repository.get_brains_usages(user_id, graph_range, brain_id)", "backend/modules/analytics/service/__init__.py": "", "backend/modules/analytics/entity/analytics.py": "from enum import IntEnum\nfrom typing import List\nfrom pydantic import BaseModel\nfrom datetime import date\n\nclass Range(IntEnum):\n    WEEK = 7\n    MONTH = 30\n    QUARTER = 90\n\nclass Usage(BaseModel):\n    date: date\n    usage_count: int\n\nclass BrainsUsages(BaseModel):\n    usages: List[Usage]", "backend/modules/analytics/entity/__init__.py": "", "backend/modules/notification/__init__.py": "", "backend/modules/notification/controller/__init__.py": "", "backend/modules/notification/repository/notifications_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.notification.dto.inputs import NotificationUpdatableProperties, CreateNotification\nfrom modules.notification.entity.notification import Notification\n\n\nclass NotificationInterface(ABC):\n    @abstractmethod\n    def add_notification(self, notification: CreateNotification) -> Notification:\n        \"\"\"\n        Add a notification\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_notification_by_id(\n        self, notification_id: UUID, notification: NotificationUpdatableProperties\n    ) -> Notification:\n        \"\"\"Update a notification by id\"\"\"\n        pass\n\n    @abstractmethod\n    def remove_notification_by_id(self, notification_id: UUID):\n        \"\"\"\n        Remove a notification by id\n        Args:\n            notification_id (UUID): The id of the notification\n\n        Returns:\n            str: Status message\n        \"\"\"\n        pass\n", "backend/modules/notification/repository/notifications.py": "from logger import get_logger\nfrom modules.notification.dto.inputs import CreateNotification\nfrom modules.notification.entity.notification import Notification\nfrom modules.notification.repository.notifications_interface import (\n    NotificationInterface,\n)\n\nlogger = get_logger(__name__)\n\n\nclass Notifications(NotificationInterface):\n    def __init__(self, supabase_client):\n        self.db = supabase_client\n\n    def add_notification(self, notification: CreateNotification):\n        \"\"\"\n        Add a notification\n        \"\"\"\n        response = (\n            self.db.from_(\"notifications\").insert(notification.model_dump()).execute()\n        ).data\n        return Notification(**response[0])\n\n    def update_notification_by_id(\n        self,\n        notification_id,\n        notification,\n    ):\n        if notification_id is None:\n            logger.info(\"Notification id is required\")\n            return None\n\n        \"\"\"Update a notification by id\"\"\"\n        response = (\n            self.db.from_(\"notifications\")\n            .update(notification.model_dump(exclude_unset=True))\n            .filter(\"id\", \"eq\", notification_id)\n            .execute()\n        ).data\n\n        if response == []:\n            logger.info(f\"Notification with id {notification_id} not found\")\n            return None\n\n        return Notification(**response[0])\n\n    def remove_notification_by_id(self, notification_id):\n        \"\"\"\n        Remove a notification by id\n        Args:\n            notification_id (UUID): The id of the notification\n\n        Returns:\n            str: Status message\n        \"\"\"\n        response = (\n            self.db.from_(\"notifications\")\n            .delete()\n            .filter(\"id\", \"eq\", notification_id)\n            .execute()\n            .data\n        )\n\n        if response == []:\n            logger.info(f\"Notification with id {notification_id} not found\")\n            return None\n\n        return {\"status\": \"success\"}\n", "backend/modules/notification/repository/__init__.py": "from .notifications import Notifications\n", "backend/modules/notification/service/notification_service.py": "from models.settings import get_supabase_client\nfrom modules.notification.dto.inputs import (\n    CreateNotification,\n    NotificationUpdatableProperties,\n)\nfrom modules.notification.repository.notifications import Notifications\nfrom modules.notification.repository.notifications_interface import (\n    NotificationInterface,\n)\n\n\nclass NotificationService:\n    repository: NotificationInterface\n\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.repository = Notifications(supabase_client)\n\n    def add_notification(self, notification: CreateNotification):\n        \"\"\"\n        Add a notification\n        \"\"\"\n        return self.repository.add_notification(notification)\n\n    def update_notification_by_id(\n        self, notification_id, notification: NotificationUpdatableProperties\n    ):\n        \"\"\"\n        Update a notification\n        \"\"\"\n        if notification:\n            return self.repository.update_notification_by_id(\n                notification_id, notification\n            )\n", "backend/modules/notification/service/__init__.py": "", "backend/modules/notification/dto/outputs.py": "", "backend/modules/notification/dto/__init__.py": "from .inputs import NotificationUpdatableProperties", "backend/modules/notification/dto/inputs.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom modules.notification.entity.notification import NotificationsStatusEnum\nfrom pydantic import BaseModel\n\n\nclass CreateNotification(BaseModel):\n    \"\"\"Properties that can be received on notification creation\"\"\"\n\n    user_id: UUID\n    status: NotificationsStatusEnum\n    title: str\n    description: Optional[str] = None\n\n    def model_dump(self, *args, **kwargs):\n        notification_dict = super().model_dump(*args, **kwargs)\n        notification_dict[\"user_id\"] = str(notification_dict[\"user_id\"])\n        return notification_dict\n\n\nclass NotificationUpdatableProperties(BaseModel):\n    \"\"\"Properties that can be received on notification update\"\"\"\n\n    status: Optional[NotificationsStatusEnum]\n    description: Optional[str]\n", "backend/modules/notification/entity/notification.py": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass NotificationsStatusEnum(str, Enum):\n    INFO = \"info\"\n    SUCCESS = \"success\"\n    WARNING = \"warning\"\n    ERROR = \"error\"\n\n\nclass Notification(BaseModel):\n    id: UUID\n    user_id: UUID\n    status: NotificationsStatusEnum\n    title: str\n    description: Optional[str]\n    archived: Optional[bool] = False\n    read: Optional[bool] = False\n    datetime: Optional[datetime]  # timestamp\n", "backend/modules/notification/entity/__init__.py": "from .notification import Notification\n", "backend/modules/assistant/__init__.py": "", "backend/modules/assistant/controller/assistant_routes.py": "from typing import List\n\nfrom fastapi import APIRouter, Depends, HTTPException, UploadFile\nfrom logger import get_logger\nfrom middlewares.auth import AuthBearer, get_current_user\nfrom modules.assistant.dto.inputs import InputAssistant\nfrom modules.assistant.dto.outputs import AssistantOutput\nfrom modules.assistant.ito.difference import DifferenceAssistant\nfrom modules.assistant.ito.summary import SummaryAssistant, summary_inputs\nfrom modules.assistant.service.assistant import Assistant\nfrom modules.user.entity.user_identity import UserIdentity\n\nassistant_router = APIRouter()\nlogger = get_logger(__name__)\n\nassistant_service = Assistant()\n\n\n@assistant_router.get(\n    \"/assistants\", dependencies=[Depends(AuthBearer())], tags=[\"Assistant\"]\n)\nasync def list_assistants(\n    current_user: UserIdentity = Depends(get_current_user),\n) -> List[AssistantOutput]:\n    \"\"\"\n    Retrieve and list all the knowledge in a brain.\n    \"\"\"\n\n    summary = summary_inputs()\n    # difference = difference_inputs()\n    # crawler = crawler_inputs()\n    # audio_transcript = audio_transcript_inputs()\n    return [summary]\n\n\n@assistant_router.post(\n    \"/assistant/process\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Assistant\"],\n)\nasync def process_assistant(\n    input: InputAssistant,\n    files: List[UploadFile] = None,\n    current_user: UserIdentity = Depends(get_current_user),\n):\n    if input.name.lower() == \"summary\":\n        summary_assistant = SummaryAssistant(\n            input=input, files=files, current_user=current_user\n        )\n        try:\n            summary_assistant.check_input()\n            return await summary_assistant.process_assistant()\n        except ValueError as e:\n            raise HTTPException(status_code=400, detail=str(e))\n    elif input.name.lower() == \"difference\":\n        difference_assistant = DifferenceAssistant(\n            input=input, files=files, current_user=current_user\n        )\n        try:\n            difference_assistant.check_input()\n            return await difference_assistant.process_assistant()\n        except ValueError as e:\n            raise HTTPException(status_code=400, detail=str(e))\n    return {\"message\": \"Assistant not found\"}\n", "backend/modules/assistant/controller/__init__.py": "from .assistant_routes import assistant_router\n", "backend/modules/assistant/repository/assistant_interface.py": "from abc import ABC, abstractmethod\nfrom typing import List\n\nfrom modules.assistant.entity.assistant import AssistantEntity\n\n\nclass AssistantInterface(ABC):\n\n    @abstractmethod\n    def get_all_assistants(self) -> List[AssistantEntity]:\n        \"\"\"\n        Get all the knowledge in a brain\n        Args:\n            brain_id (UUID): The id of the brain\n        \"\"\"\n        pass\n", "backend/modules/assistant/repository/__init__.py": "from .assistant_interface import AssistantInterface\n", "backend/modules/assistant/ito/ito.py": "import os\nimport random\nimport re\nimport string\nfrom abc import abstractmethod\nfrom io import BytesIO\nfrom tempfile import NamedTemporaryFile\nfrom typing import List, Optional\n\nfrom fastapi import UploadFile\nfrom logger import get_logger\nfrom modules.assistant.dto.inputs import InputAssistant\nfrom modules.assistant.ito.utils.pdf_generator import PDFGenerator, PDFModel\nfrom modules.chat.controller.chat.utils import update_user_usage\nfrom modules.contact_support.controller.settings import ContactsSettings\nfrom modules.upload.controller.upload_routes import upload_file\nfrom modules.user.entity.user_identity import UserIdentity\nfrom modules.user.service.user_usage import UserUsage\nfrom packages.emails.send_email import send_email\nfrom pydantic import BaseModel\nfrom unidecode import unidecode\n\nlogger = get_logger(__name__)\n\n\nclass ITO(BaseModel):\n    input: InputAssistant\n    files: List[UploadFile]\n    current_user: UserIdentity\n    user_usage: Optional[UserUsage] = None\n    user_settings: Optional[dict] = None\n\n    def __init__(\n        self,\n        input: InputAssistant,\n        files: List[UploadFile] = None,\n        current_user: UserIdentity = None,\n        **kwargs,\n    ):\n        super().__init__(\n            input=input,\n            files=files,\n            current_user=current_user,\n            **kwargs,\n        )\n        self.user_usage = UserUsage(\n            id=current_user.id,\n            email=current_user.email,\n        )\n        self.user_settings = self.user_usage.get_user_settings()\n        self.increase_usage_user()\n\n    def increase_usage_user(self):\n        # Raises an error if the user has consumed all of of his credits\n\n        update_user_usage(\n            usage=self.user_usage,\n            user_settings=self.user_settings,\n            cost=self.calculate_pricing(),\n        )\n\n    def calculate_pricing(self):\n        return 20\n\n    def generate_pdf(self, filename: str, title: str, content: str):\n        pdf_model = PDFModel(title=title, content=content)\n        pdf = PDFGenerator(pdf_model)\n        pdf.print_pdf()\n        pdf.output(filename, \"F\")\n\n    @abstractmethod\n    async def process_assistant(self):\n        pass\n\n    async def send_output_by_email(\n        self,\n        file: UploadFile,\n        filename: str,\n        task_name: str,\n        custom_message: str,\n        brain_id: str = None,\n    ):\n        settings = ContactsSettings()\n        file = await self.uploadfile_to_file(file)\n        domain_quivr = os.getenv(\"QUIVR_DOMAIN\", \"https://chat.quivr.app/\")\n\n        with open(file.name, \"rb\") as f:\n            mail_from = settings.resend_contact_sales_from\n            mail_to = self.current_user.email\n            body = f\"\"\"\n            <div style=\"text-align: center;\">\n                <img src=\"https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png\" alt=\"Quivr Logo\" style=\"width: 100px; height: 100px; border-radius: 50%; margin: 0 auto; display: block;\">\n                \n                <p>Quivr's ingestion process has been completed. The processed file is attached.</p>\n                \n                <p><strong>Task:</strong> {task_name}</p>\n                \n                <p><strong>Output:</strong> {custom_message}</p>\n                <br />\n                \n\n            </div>\n            \"\"\"\n            if brain_id:\n                body += f\"<div style='text-align: center;'>You can find the file <a href='{domain_quivr}studio/{brain_id}'>here</a>.</div> <br />\"\n            body += f\"\"\"\n            <div style=\"text-align: center;\">\n                <p>Please let us know if you have any questions or need further assistance.</p>\n                \n                <p> The Quivr Team </p>\n            </div>\n            \"\"\"\n            params = {\n                \"sender\": mail_from,\n                \"to\": [mail_to],\n                \"subject\": \"Quivr Ingestion Processed\",\n                \"reply_to\": \"no-reply@quivr.app\",\n                \"html\": body,\n                \"attachments\": [{\"filename\": filename, \"content\": list(f.read())}],\n            }\n            logger.info(f\"Sending email to {mail_to} with file {filename}\")\n            send_email(params)\n\n    async def uploadfile_to_file(self, uploadFile: UploadFile):\n        # Transform the UploadFile object to a file object with same name and content\n        tmp_file = NamedTemporaryFile(delete=False)\n        tmp_file.write(uploadFile.file.read())\n        tmp_file.flush()  # Make sure all data is written to disk\n        return tmp_file\n\n    async def create_and_upload_processed_file(\n        self, processed_content: str, original_filename: str, file_description: str\n    ) -> dict:\n        \"\"\"Handles creation and uploading of the processed file.\"\"\"\n        # remove any special characters from the filename that aren't http safe\n\n        new_filename = (\n            original_filename.split(\".\")[0]\n            + \"_\"\n            + file_description.lower().replace(\" \", \"_\")\n            + \"_\"\n            + str(random.randint(1000, 9999))\n            + \".pdf\"\n        )\n        new_filename = unidecode(new_filename)\n        new_filename = re.sub(\n            \"[^{}0-9a-zA-Z]\".format(re.escape(string.punctuation)), \"\", new_filename\n        )\n\n        self.generate_pdf(\n            new_filename,\n            f\"{file_description} of {original_filename}\",\n            processed_content,\n        )\n\n        content_io = BytesIO()\n        with open(new_filename, \"rb\") as f:\n            content_io.write(f.read())\n        content_io.seek(0)\n\n        file_to_upload = UploadFile(\n            filename=new_filename,\n            file=content_io,\n            headers={\"content-type\": \"application/pdf\"},\n        )\n\n        if self.input.outputs.email.activated:\n            await self.send_output_by_email(\n                file_to_upload,\n                new_filename,\n                \"Summary\",\n                f\"{file_description} of {original_filename}\",\n                brain_id=(\n                    self.input.outputs.brain.value\n                    if (\n                        self.input.outputs.brain.activated\n                        and self.input.outputs.brain.value\n                    )\n                    else None\n                ),\n            )\n\n        # Reset to start of file before upload\n        file_to_upload.file.seek(0)\n        if self.input.outputs.brain.activated:\n            await upload_file(\n                uploadFile=file_to_upload,\n                brain_id=self.input.outputs.brain.value,\n                current_user=self.current_user,\n                chat_id=None,\n            )\n\n        os.remove(new_filename)\n\n        return {\"message\": f\"{file_description} generated successfully\"}\n", "backend/modules/assistant/ito/audio_transcript.py": "import os\nfrom tempfile import NamedTemporaryFile\n\nfrom logger import get_logger\nfrom modules.assistant.dto.outputs import (\n    AssistantOutput,\n    InputFile,\n    Inputs,\n    OutputBrain,\n    OutputEmail,\n    Outputs,\n)\nfrom modules.assistant.ito.ito import ITO\nfrom openai import OpenAI\n\nlogger = get_logger(__name__)\n\n\nclass AudioTranscriptAssistant(ITO):\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        super().__init__(\n            **kwargs,\n        )\n\n    async def process_assistant(self):\n        client = OpenAI()\n\n        logger.info(f\"Processing audio file {self.uploadFile.filename}\")\n\n        # Extract the original filename and create a temporary file with the same name\n        filename = os.path.basename(self.uploadFile.filename)\n        temp_file = NamedTemporaryFile(delete=False, suffix=filename)\n\n        # Write the uploaded file's data to the temporary file\n        data = await self.uploadFile.read()\n        temp_file.write(data)\n        temp_file.close()\n\n        # Open the temporary file and pass it to the OpenAI API\n        with open(temp_file.name, \"rb\") as file:\n            transcription = client.audio.transcriptions.create(\n                model=\"whisper-1\", file=file, response_format=\"text\"\n            )\n            logger.info(f\"Transcription: {transcription}\")\n\n            # Delete the temporary file\n            os.remove(temp_file.name)\n\n        return await self.create_and_upload_processed_file(\n            transcription, self.uploadFile.filename, \"Audio Transcript\"\n        )\n\n\ndef audio_transcript_inputs():\n    output = AssistantOutput(\n        name=\"Audio Transcript\",\n        description=\"Transcribes an audio file\",\n        tags=[\"new\"],\n        input_description=\"One audio file to transcribe\",\n        output_description=\"Transcription of the audio file\",\n        inputs=Inputs(\n            files=[\n                InputFile(\n                    key=\"audio_file\",\n                    allowed_extensions=[\"mp3\", \"wav\", \"ogg\", \"m4a\"],\n                    required=True,\n                    description=\"The audio file to transcribe\",\n                )\n            ]\n        ),\n        outputs=Outputs(\n            brain=OutputBrain(\n                required=True,\n                description=\"The brain to which to upload the document\",\n                type=\"uuid\",\n            ),\n            email=OutputEmail(\n                required=True,\n                description=\"Send the document by email\",\n                type=\"str\",\n            ),\n        ),\n    )\n    return output\n", "backend/modules/assistant/ito/summary.py": "import tempfile\nfrom typing import List\n\nfrom fastapi import UploadFile\nfrom langchain.chains import (\n    MapReduceDocumentsChain,\n    ReduceDocumentsChain,\n    StuffDocumentsChain,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom logger import get_logger\nfrom modules.assistant.dto.inputs import InputAssistant\nfrom modules.assistant.dto.outputs import (\n    AssistantOutput,\n    InputFile,\n    Inputs,\n    OutputBrain,\n    OutputEmail,\n    Outputs,\n)\nfrom modules.assistant.ito.ito import ITO\nfrom modules.user.entity.user_identity import UserIdentity\n\nlogger = get_logger(__name__)\n\n\nclass SummaryAssistant(ITO):\n\n    def __init__(\n        self,\n        input: InputAssistant,\n        files: List[UploadFile] = None,\n        current_user: UserIdentity = None,\n        **kwargs,\n    ):\n        super().__init__(\n            input=input,\n            files=files,\n            current_user=current_user,\n            **kwargs,\n        )\n\n    def check_input(self):\n        if not self.files:\n            raise ValueError(\"No file was uploaded\")\n        if len(self.files) > 1:\n            raise ValueError(\"Only one file can be uploaded\")\n        if not self.input.inputs.files:\n            raise ValueError(\"No files key were given in the input\")\n        if len(self.input.inputs.files) > 1:\n            raise ValueError(\"Only one file can be uploaded\")\n        if not self.input.inputs.files[0].key == \"doc_to_summarize\":\n            raise ValueError(\"The key of the file should be doc_to_summarize\")\n        if not self.input.inputs.files[0].value:\n            raise ValueError(\"No file was uploaded\")\n        # Check if name of file is same as the key\n        if not self.input.inputs.files[0].value == self.files[0].filename:\n            raise ValueError(\n                \"The key of the file should be the same as the name of the file\"\n            )\n        if not (\n            self.input.outputs.brain.activated or self.input.outputs.email.activated\n        ):\n            raise ValueError(\"No output was selected\")\n        return True\n\n    async def process_assistant(self):\n\n        try:\n            self.increase_usage_user()\n        except Exception as e:\n            logger.error(f\"Error increasing usage: {e}\")\n            return {\"error\": str(e)}\n\n        # Create a temporary file with the uploaded file as a temporary file and then pass it to the loader\n        tmp_file = tempfile.NamedTemporaryFile(delete=False)\n\n        # Write the file to the temporary file\n        tmp_file.write(self.files[0].file.read())\n\n        # Now pass the path of the temporary file to the loader\n\n        loader = UnstructuredPDFLoader(tmp_file.name)\n\n        tmp_file.close()\n\n        data = loader.load()\n\n        llm = ChatLiteLLM(model=\"gpt-4o\", max_tokens=2000)\n\n        map_template = \"\"\"The following is a document that has been divided into multiple sections:\n        {docs}\n        \n        Please carefully analyze each section and identify the following:\n\n        1. Main Themes: What are the overarching ideas or topics in this section?\n        2. Key Points: What are the most important facts, arguments, or ideas presented in this section?\n        3. Important Information: Are there any crucial details that stand out? This could include data, quotes, specific events, entity, or other relevant information.\n        4. People: Who are the key individuals mentioned in this section? What roles do they play?\n        5. Reasoning: What logic or arguments are used to support the key points?\n        6. Chapters: If the document is divided into chapters, what is the main focus of each chapter?\n\n        Remember to consider the language and context of the document. This will help in understanding the nuances and subtleties of the text.\"\"\"\n        map_prompt = PromptTemplate.from_template(map_template)\n        map_chain = LLMChain(llm=llm, prompt=map_prompt)\n\n        # Reduce\n        reduce_template = \"\"\"The following is a set of summaries for parts of the document:\n        {docs}\n        Take these and distill it into a final, consolidated summary of the document. Make sure to include the main themes, key points, and important information such as data, quotes,people and specific events.\n        Use markdown such as bold, italics, underlined. For example, **bold**, *italics*, and _underlined_ to highlight key points.\n        Please provide the final summary with sections using bold headers. \n        Sections should always be Summary and Key Points, but feel free to add more sections as needed.\n        Always use bold text for the sections headers.\n        Keep the same language as the documents.\n        Answer:\"\"\"\n        reduce_prompt = PromptTemplate.from_template(reduce_template)\n\n        # Run chain\n        reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n\n        # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n        combine_documents_chain = StuffDocumentsChain(\n            llm_chain=reduce_chain, document_variable_name=\"docs\"\n        )\n\n        # Combines and iteratively reduces the mapped documents\n        reduce_documents_chain = ReduceDocumentsChain(\n            # This is final chain that is called.\n            combine_documents_chain=combine_documents_chain,\n            # If documents exceed context for `StuffDocumentsChain`\n            collapse_documents_chain=combine_documents_chain,\n            # The maximum number of tokens to group documents into.\n            token_max=4000,\n        )\n\n        # Combining documents by mapping a chain over them, then combining results\n        map_reduce_chain = MapReduceDocumentsChain(\n            # Map chain\n            llm_chain=map_chain,\n            # Reduce chain\n            reduce_documents_chain=reduce_documents_chain,\n            # The variable name in the llm_chain to put the documents in\n            document_variable_name=\"docs\",\n            # Return the results of the map steps in the output\n            return_intermediate_steps=False,\n        )\n\n        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=1000, chunk_overlap=100\n        )\n        split_docs = text_splitter.split_documents(data)\n\n        content = map_reduce_chain.run(split_docs)\n\n        return await self.create_and_upload_processed_file(\n            content, self.files[0].filename, \"Summary\"\n        )\n\n\ndef summary_inputs():\n    output = AssistantOutput(\n        name=\"Summary\",\n        description=\"Summarize a set of documents\",\n        tags=[\"new\"],\n        input_description=\"One document to summarize\",\n        output_description=\"A summary of the document with key points and main themes\",\n        icon_url=\"https://quivr-cms.s3.eu-west-3.amazonaws.com/report_94bea8b918.png\",\n        inputs=Inputs(\n            files=[\n                InputFile(\n                    key=\"doc_to_summarize\",\n                    allowed_extensions=[\"pdf\"],\n                    required=True,\n                    description=\"The document to summarize\",\n                )\n            ]\n        ),\n        outputs=Outputs(\n            brain=OutputBrain(\n                required=True,\n                description=\"The brain to which upload the document\",\n                type=\"uuid\",\n            ),\n            email=OutputEmail(\n                required=True,\n                description=\"Send the document by email\",\n                type=\"str\",\n            ),\n        ),\n    )\n    return output\n", "backend/modules/assistant/ito/difference.py": "import asyncio\nimport os\nimport tempfile\nfrom typing import List\n\nimport nest_asyncio\nimport uvloop\nfrom fastapi import UploadFile\nfrom langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom llama_parse import LlamaParse\nfrom logger import get_logger\nfrom modules.assistant.dto.inputs import InputAssistant\nfrom modules.assistant.dto.outputs import (\n    AssistantOutput,\n    InputFile,\n    Inputs,\n    OutputBrain,\n    OutputEmail,\n    Outputs,\n)\nfrom modules.assistant.ito.ito import ITO\nfrom modules.user.entity.user_identity import UserIdentity\n\nif not isinstance(asyncio.get_event_loop(), uvloop.Loop):\n    nest_asyncio.apply()\n\n\nlogger = get_logger(__name__)\n\n\nclass DifferenceAssistant(ITO):\n\n    def __init__(\n        self,\n        input: InputAssistant,\n        files: List[UploadFile] = None,\n        current_user: UserIdentity = None,\n        **kwargs,\n    ):\n        super().__init__(\n            input=input,\n            files=files,\n            current_user=current_user,\n            **kwargs,\n        )\n\n    def check_input(self):\n        if not self.files:\n            raise ValueError(\"No file was uploaded\")\n        if len(self.files) != 2:\n            raise ValueError(\"Only two files can be uploaded\")\n        if not self.input.inputs.files:\n            raise ValueError(\"No files key were given in the input\")\n        if len(self.input.inputs.files) != 2:\n            raise ValueError(\"Only two files can be uploaded\")\n        if not self.input.inputs.files[0].key == \"doc_1\":\n            raise ValueError(\"The key of the first file should be doc_1\")\n        if not self.input.inputs.files[1].key == \"doc_2\":\n            raise ValueError(\"The key of the second file should be doc_2\")\n        if not self.input.inputs.files[0].value:\n            raise ValueError(\"No file was uploaded\")\n        if not (\n            self.input.outputs.brain.activated or self.input.outputs.email.activated\n        ):\n            raise ValueError(\"No output was selected\")\n        return True\n\n    async def process_assistant(self):\n\n        document_1 = self.files[0]\n        document_2 = self.files[1]\n\n        # Get the file extensions\n        document_1_ext = os.path.splitext(document_1.filename)[1]\n        document_2_ext = os.path.splitext(document_2.filename)[1]\n\n        # Create temporary files with the same extension as the original files\n        document_1_tmp = tempfile.NamedTemporaryFile(\n            suffix=document_1_ext, delete=False\n        )\n        document_2_tmp = tempfile.NamedTemporaryFile(\n            suffix=document_2_ext, delete=False\n        )\n\n        document_1_tmp.write(document_1.file.read())\n        document_2_tmp.write(document_2.file.read())\n\n        parser = LlamaParse(\n            result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n        )\n\n        document_1_llama_parsed = parser.load_data(document_1_tmp.name)\n        document_2_llama_parsed = parser.load_data(document_2_tmp.name)\n\n        document_1_tmp.close()\n        document_2_tmp.close()\n\n        document_1_to_langchain = document_1_llama_parsed[0].to_langchain_format()\n        document_2_to_langchain = document_2_llama_parsed[0].to_langchain_format()\n\n        llm = ChatLiteLLM(model=\"gpt-4o\")\n\n        human_prompt = \"\"\"Given the following two documents, find the difference between them:\n\n        Document 1:\n        {document_1}\n        Document 2:\n        {document_2}\n        Difference:\n        \"\"\"\n        CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(human_prompt)\n\n        system_message_template = \"\"\"\n        You are an expert in finding the difference between two documents. You look deeply into what makes the two documents different and provide a detailed analysis if needed of the differences between the two documents.\n        If no differences are found, simply say that there are no differences.\n        \"\"\"\n\n        ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n            [\n                SystemMessagePromptTemplate.from_template(system_message_template),\n                HumanMessagePromptTemplate.from_template(human_prompt),\n            ]\n        )\n\n        final_inputs = {\n            \"document_1\": document_1_to_langchain.page_content,\n            \"document_2\": document_2_to_langchain.page_content,\n        }\n\n        output_parser = StrOutputParser()\n\n        chain = ANSWER_PROMPT | llm | output_parser\n        result = chain.invoke(final_inputs)\n\n        return result\n\n\ndef difference_inputs():\n    output = AssistantOutput(\n        name=\"difference\",\n        description=\"Finds the difference between two sets of documents\",\n        tags=[\"new\"],\n        input_description=\"Two documents to compare\",\n        output_description=\"The difference between the two documents\",\n        icon_url=\"https://quivr-cms.s3.eu-west-3.amazonaws.com/report_94bea8b918.png\",\n        inputs=Inputs(\n            files=[\n                InputFile(\n                    key=\"doc_1\",\n                    allowed_extensions=[\"pdf\"],\n                    required=True,\n                    description=\"The first document to compare\",\n                ),\n                InputFile(\n                    key=\"doc_2\",\n                    allowed_extensions=[\"pdf\"],\n                    required=True,\n                    description=\"The second document to compare\",\n                ),\n            ]\n        ),\n        outputs=Outputs(\n            brain=OutputBrain(\n                required=True,\n                description=\"The brain to which upload the document\",\n                type=\"uuid\",\n            ),\n            email=OutputEmail(\n                required=True,\n                description=\"Send the document by email\",\n                type=\"str\",\n            ),\n        ),\n    )\n    return output\n", "backend/modules/assistant/ito/__init__.py": "", "backend/modules/assistant/ito/crawler.py": "from bs4 import BeautifulSoup as Soup\nfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\nfrom logger import get_logger\nfrom modules.assistant.dto.outputs import (\n    AssistantOutput,\n    Inputs,\n    InputUrl,\n    OutputBrain,\n    OutputEmail,\n    Outputs,\n)\nfrom modules.assistant.ito.ito import ITO\n\nlogger = get_logger(__name__)\n\n\nclass CrawlerAssistant(ITO):\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        super().__init__(\n            **kwargs,\n        )\n\n    async def process_assistant(self):\n\n        url = self.url\n        loader = RecursiveUrlLoader(\n            url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text\n        )\n        docs = loader.load()\n\n        nice_url = url.split(\"://\")[1].replace(\"/\", \"_\").replace(\".\", \"_\")\n        nice_url += \".txt\"\n\n        for docs in docs:\n            await self.create_and_upload_processed_file(\n                docs.page_content, nice_url, \"Crawler\"\n            )\n\n\ndef crawler_inputs():\n    output = AssistantOutput(\n        name=\"Crawler\",\n        description=\"Crawls a website and extracts the text from the pages\",\n        tags=[\"new\"],\n        input_description=\"One URL to crawl\",\n        output_description=\"Text extracted from the pages\",\n        inputs=Inputs(\n            urls=[\n                InputUrl(\n                    key=\"url\",\n                    required=True,\n                    description=\"The URL to crawl\",\n                )\n            ],\n        ),\n        outputs=Outputs(\n            brain=OutputBrain(\n                required=True,\n                description=\"The brain to which upload the document\",\n                type=\"uuid\",\n            ),\n            email=OutputEmail(\n                required=True,\n                description=\"Send the document by email\",\n                type=\"str\",\n            ),\n        ),\n    )\n    return output\n", "backend/modules/assistant/ito/utils/pdf_generator.py": "import os\n\nfrom fpdf import FPDF\nfrom pydantic import BaseModel\n\n\nclass PDFModel(BaseModel):\n    title: str\n    content: str\n\n\nclass PDFGenerator(FPDF):\n    def __init__(self, pdf_model: PDFModel, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pdf_model = pdf_model\n        self.add_font(\n            \"DejaVu\",\n            \"\",\n            os.path.join(os.path.dirname(__file__), \"font/DejaVuSansCondensed.ttf\"),\n            uni=True,\n        )\n        self.add_font(\n            \"DejaVu\",\n            \"B\",\n            os.path.join(\n                os.path.dirname(__file__), \"font/DejaVuSansCondensed-Bold.ttf\"\n            ),\n            uni=True,\n        )\n        self.add_font(\n            \"DejaVu\",\n            \"I\",\n            os.path.join(\n                os.path.dirname(__file__), \"font/DejaVuSansCondensed-Oblique.ttf\"\n            ),\n        )\n\n    def header(self):\n        # Logo\n        logo_path = os.path.join(os.path.dirname(__file__), \"logo.png\")\n        self.image(logo_path, 10, 10, 20)  # Adjust size as needed\n\n        # Move cursor to right of image\n        self.set_xy(20, 15)\n\n        # Title\n        self.set_font(\"DejaVu\", \"B\", 12)\n        self.multi_cell(0, 10, self.pdf_model.title, align=\"C\")\n        self.ln(5)  # Padding after title\n\n    def footer(self):\n        self.set_y(-15)\n\n        self.set_font(\"DejaVu\", \"I\", 8)\n        self.set_text_color(169, 169, 169)\n        self.cell(80, 10, \"Generated by Quivr\", 0, 0, \"C\")\n        self.set_font(\"DejaVu\", \"U\", 8)\n        self.set_text_color(0, 0, 255)\n        self.cell(30, 10, \"quivr.app\", 0, 0, \"C\", link=\"https://quivr.app\")\n        self.cell(0, 10, \"Github\", 0, 1, \"C\", link=\"https://github.com/quivrhq/quivr\")\n\n    def chapter_body(self):\n\n        self.set_font(\"DejaVu\", \"\", 12)\n        self.multi_cell(0, 10, self.pdf_model.content, markdown=True)\n        self.ln()\n\n    def print_pdf(self):\n        self.add_page()\n        self.chapter_body()\n\n\nif __name__ == \"__main__\":\n    pdf_model = PDFModel(\n        title=\"Summary of Legal Services Rendered by Orrick\",\n        content=\"\"\"\n**Summary:** \nThe document is an invoice from Quivr Technologies, Inc. for legal services provided to client YC W24, related to initial corporate work. The total fees and disbursements amount to $8,345.00 for services rendered through February 29, 2024. The invoice includes specific instructions for payment remittance and contact information for inquiries. Online payment through e-billexpress.com is also an option.\n\n**Key Points:**\n- Quivr Technologies, Inc., based in France and represented by Stanislas Girard, provided legal services to client YC W24.\n- Services included preparing and completing forms, drafting instructions, reviewing and responding to emails, filing 83(b) elections, and finalizing documents for submission to YC.\n- The timekeepers involved in providing these services were Julien Barbey, Maria T. Coladonato, Michael LaBlanc, Jessy K. Parker, Marisol Sandoval Villasenor, Alexis A. Smith, and Serena Tibrewala.\n- The total hours billed for the services provided was 16.20, with a total cost of $8,345.00.\n- Instructions for payment remittance, contact information, and online payment options through e-billex\n\"\"\",\n    )\n    pdf = PDFGenerator(pdf_model)\n    pdf.print_pdf()\n    pdf.output(\"simple.pdf\")\n", "backend/modules/assistant/ito/utils/__init__.py": "", "backend/modules/assistant/service/assistant.py": "from models.settings import get_supabase_client\nfrom modules.assistant.entity.assistant import AssistantEntity\nfrom modules.assistant.repository.assistant_interface import AssistantInterface\n\n\nclass Assistant(AssistantInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def get_all_assistants(self):\n        response = self.db.from_(\"assistants\").select(\"*\").execute()\n\n        if response.data:\n            return response.data\n\n        return []\n\n    def get_assistant_by_id(self, ingestion_id) -> AssistantEntity:\n        response = (\n            self.db.from_(\"assistants\")\n            .select(\"*\")\n            .filter(\"id\", \"eq\", ingestion_id)\n            .execute()\n        )\n\n        if response.data:\n            return AssistantEntity(**response.data[0])\n\n        return None\n", "backend/modules/assistant/service/__init__.py": "", "backend/modules/assistant/dto/outputs.py": "from typing import List, Optional\n\nfrom pydantic import BaseModel\n\n\nclass InputFile(BaseModel):\n    key: str\n    allowed_extensions: Optional[List[str]] = [\"pdf\"]\n    required: Optional[bool] = True\n    description: str\n\n\nclass InputUrl(BaseModel):\n    key: str\n    required: Optional[bool] = True\n    description: str\n\n\nclass InputText(BaseModel):\n    key: str\n    required: Optional[bool] = True\n    description: str\n    validation_regex: Optional[str] = None\n\n\nclass InputBoolean(BaseModel):\n    key: str\n    required: Optional[bool] = True\n    description: str\n\n\nclass InputNumber(BaseModel):\n    key: str\n    required: Optional[bool] = True\n    description: str\n    min: Optional[int] = None\n    max: Optional[int] = None\n    increment: Optional[int] = None\n    default: Optional[int] = None\n\n\nclass InputSelectText(BaseModel):\n    key: str\n    required: Optional[bool] = True\n    description: str\n    options: List[str]\n    default: Optional[str] = None\n\n\nclass InputSelectNumber(BaseModel):\n    key: str\n    required: Optional[bool] = True\n    description: str\n    options: List[int]\n    default: Optional[int] = None\n\n\nclass Inputs(BaseModel):\n    files: Optional[List[InputFile]] = None\n    urls: Optional[List[InputUrl]] = None\n    texts: Optional[List[InputText]] = None\n    booleans: Optional[List[InputBoolean]] = None\n    numbers: Optional[List[InputNumber]] = None\n    select_texts: Optional[List[InputSelectText]] = None\n    select_numbers: Optional[List[InputSelectNumber]] = None\n\n\nclass OutputEmail(BaseModel):\n    required: Optional[bool] = True\n    description: str\n    type: str\n\n\nclass OutputBrain(BaseModel):\n    required: Optional[bool] = True\n    description: str\n    type: str\n\n\nclass Outputs(BaseModel):\n    email: Optional[OutputEmail] = None\n    brain: Optional[OutputBrain] = None\n\n\nclass Pricing(BaseModel):\n    cost: int = 20\n    description: str = \"Credits per use\"\n\n\nclass AssistantOutput(BaseModel):\n    name: str\n    description: str\n    pricing: Optional[Pricing] = Pricing()\n    tags: Optional[List[str]] = []\n    input_description: str\n    output_description: str\n    inputs: Inputs\n    outputs: Outputs\n    icon_url: Optional[str] = None\n", "backend/modules/assistant/dto/__init__.py": "", "backend/modules/assistant/dto/inputs.py": "import json\nfrom typing import List, Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, model_validator, root_validator\n\n\nclass EmailInput(BaseModel):\n    activated: bool\n\n\nclass BrainInput(BaseModel):\n    activated: Optional[bool] = False\n    value: Optional[UUID] = None\n\n    @root_validator(pre=True)\n    def empty_string_to_none(cls, values):\n        for field, value in values.items():\n            if value == \"\":\n                values[field] = None\n        return values\n\n\nclass FileInput(BaseModel):\n    key: str\n    value: str\n\n\nclass UrlInput(BaseModel):\n    key: str\n    value: str\n\n\nclass TextInput(BaseModel):\n    key: str\n    value: str\n\n\nclass InputBoolean(BaseModel):\n    key: str\n    value: bool\n\n\nclass InputNumber(BaseModel):\n    key: str\n    value: int\n\n\nclass InputSelectText(BaseModel):\n    key: str\n    value: str\n\n\nclass InputSelectNumber(BaseModel):\n    key: str\n    value: int\n\n\nclass Inputs(BaseModel):\n    files: Optional[List[FileInput]] = None\n    urls: Optional[List[UrlInput]] = None\n    texts: Optional[List[TextInput]] = None\n    booleans: Optional[List[InputBoolean]] = None\n    numbers: Optional[List[InputNumber]] = None\n    select_texts: Optional[List[InputSelectText]] = None\n    select_numbers: Optional[List[InputSelectNumber]] = None\n\n\nclass Outputs(BaseModel):\n    email: Optional[EmailInput] = None\n    brain: Optional[BrainInput] = None\n\n\nclass InputAssistant(BaseModel):\n    name: str\n    inputs: Inputs\n    outputs: Outputs\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def to_py_dict(cls, data):\n        return json.loads(data)\n", "backend/modules/assistant/entity/assistant.py": "from uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass AssistantEntity(BaseModel):\n    id: UUID\n    name: str\n    brain_id_required: bool\n    file_1_required: bool\n    url_required: bool\n", "backend/modules/assistant/entity/__init__.py": "from .assistant import AssistantEntity\n", "backend/modules/onboarding/__init__.py": "", "backend/modules/onboarding/controller/onboarding_routes.py": "from fastapi import APIRouter, Depends\nfrom middlewares.auth import (  # Assuming you have a get_current_user function\n    AuthBearer,\n    get_current_user,\n)\nfrom modules.onboarding.dto.inputs import OnboardingUpdatableProperties\nfrom modules.onboarding.entity.onboarding import OnboardingStates\nfrom modules.onboarding.service.onboarding_service import OnboardingService\nfrom modules.user.entity.user_identity import UserIdentity\n\nonboarding_router = APIRouter()\n\nonboardingService = OnboardingService()\n\n\n@onboarding_router.get(\n    \"/onboarding\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Onboarding\"],\n)\nasync def get_user_onboarding_handler(\n    current_user: UserIdentity = Depends(get_current_user),\n) -> OnboardingStates | None:\n    \"\"\"\n    Get user onboarding information for the current user\n    \"\"\"\n\n    return onboardingService.get_user_onboarding(current_user.id)\n\n\n@onboarding_router.put(\n    \"/onboarding\",\n    dependencies=[Depends(AuthBearer())],\n    tags=[\"Onboarding\"],\n)\nasync def update_user_onboarding_handler(\n    onboarding: OnboardingUpdatableProperties,\n    current_user: UserIdentity = Depends(get_current_user),\n) -> OnboardingStates:\n    \"\"\"\n    Update user onboarding information for the current user\n    \"\"\"\n\n    return onboardingService.update_user_onboarding(current_user.id, onboarding)\n", "backend/modules/onboarding/controller/__init__.py": "from .onboarding_routes import onboarding_router\n", "backend/modules/onboarding/repository/onboardings_interface.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID\n\nfrom modules.onboarding.dto.inputs import OnboardingUpdatableProperties\nfrom modules.onboarding.entity.onboarding import OnboardingStates\n\n\nclass OnboardingInterface(ABC):\n    @abstractmethod\n    def get_user_onboarding(self, user_id: UUID) -> OnboardingStates | None:\n        \"\"\"\n        Get user onboarding information by user_id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_user_onboarding(\n        self, user_id: UUID, onboarding: OnboardingUpdatableProperties\n    ) -> OnboardingStates:\n        \"\"\"Update user onboarding information by user_id\"\"\"\n        pass\n\n    @abstractmethod\n    def remove_user_onboarding(self, user_id: UUID) -> OnboardingStates | None:\n        \"\"\"\n        Remove user onboarding information by user_id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_user_onboarding(self, user_id: UUID) -> OnboardingStates:\n        \"\"\"\n        Create user onboarding information by user_id\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_onboarding_more_than_x_days(self, days: int):\n        \"\"\"\n        Remove onboarding if it is older than x days\n        \"\"\"\n        \"\"\"Update a prompt by id\"\"\"\n        pass\n", "backend/modules/onboarding/repository/onboardings.py": "from datetime import datetime, timedelta\n\nfrom fastapi import HTTPException\nfrom models.settings import get_supabase_client\nfrom modules.onboarding.entity.onboarding import OnboardingStates\n\nfrom .onboardings_interface import OnboardingInterface\n\n\nclass Onboarding(OnboardingInterface):\n    def __init__(self):\n        supabase_client = get_supabase_client()\n        self.db = supabase_client\n\n    def get_user_onboarding(self, user_id):\n        \"\"\"\n        Get user onboarding information by user_id\n        \"\"\"\n        onboarding_data = (\n            self.db.from_(\"onboardings\")\n            .select(\n                \"onboarding_a\",\n                \"onboarding_b1\",\n                \"onboarding_b2\",\n                \"onboarding_b3\",\n            )\n            .filter(\"user_id\", \"eq\", user_id)\n            .limit(1)\n            .execute()\n        ).data\n\n        if onboarding_data == []:\n            return None\n\n        return OnboardingStates(**onboarding_data[0])\n\n    def update_user_onboarding(self, user_id, onboarding):\n        \"\"\"Update user onboarding information by user_id\"\"\"\n        update_data = {\n            key: value for key, value in onboarding.dict().items() if value is not None\n        }\n\n        response = (\n            self.db.from_(\"onboardings\")\n            .update(update_data)\n            .match({\"user_id\": user_id})\n            .execute()\n            .data\n        )\n\n        if not response:\n            raise HTTPException(404, \"User onboarding not updated\")\n\n        return OnboardingStates(**response[0])\n\n    def remove_user_onboarding(self, user_id):\n        \"\"\"\n        Remove user onboarding information by user_id\n        \"\"\"\n        onboarding_data = (\n            self.db.from_(\"onboardings\")\n            .delete()\n            .match({\"user_id\": str(user_id)})\n            .execute()\n        ).data\n\n        if onboarding_data == []:\n            return None\n\n        return OnboardingStates(**onboarding_data[0])\n\n    def create_user_onboarding(self, user_id):\n        \"\"\"\n        Create user onboarding information by user_id\n        \"\"\"\n        onboarding_data = (\n            self.db.from_(\"onboardings\")\n            .insert(\n                [\n                    {\n                        \"user_id\": str(user_id),\n                    }\n                ]\n            )\n            .execute()\n        ).data\n\n        return OnboardingStates(**onboarding_data[0])\n\n    def remove_onboarding_more_than_x_days(self, days):\n        \"\"\"\n        Remove onboarding if it is older than x days\n        \"\"\"\n        onboarding_data = (\n            self.db.from_(\"onboardings\")\n            .delete()\n            .lt(\n                \"creation_time\",\n                (datetime.now() - timedelta(days=days)).strftime(\n                    \"%Y-%m-%d %H:%M:%S.%f\"\n                ),\n            )\n            .execute()\n        ).data\n\n        return onboarding_data\n", "backend/modules/onboarding/repository/__init__.py": "", "backend/modules/onboarding/service/onboarding_service.py": "from uuid import UUID\n\nfrom modules.onboarding.dto.inputs import OnboardingUpdatableProperties\nfrom modules.onboarding.entity.onboarding import OnboardingStates\nfrom modules.onboarding.repository.onboardings import Onboarding\nfrom modules.onboarding.repository.onboardings_interface import OnboardingInterface\n\n\nclass OnboardingService:\n    repository: OnboardingInterface\n\n    def __init__(self):\n        self.repository = Onboarding()\n\n    def create_user_onboarding(self, user_id: UUID) -> OnboardingStates:\n        \"\"\"Update user onboarding information by user_id\"\"\"\n\n        return self.repository.create_user_onboarding(user_id)\n\n    def get_user_onboarding(self, user_id: UUID) -> OnboardingStates | None:\n        \"\"\"\n        Get a user's onboarding status\n\n        Args:\n            user_id (UUID): The id of the user\n\n        Returns:\n            Onboardings: The user's onboarding status\n        \"\"\"\n        return self.repository.get_user_onboarding(user_id)\n\n    def remove_onboarding_more_than_x_days(self, days: int):\n        \"\"\"\n        Remove onboarding if it is older than x days\n        \"\"\"\n\n        self.repository.remove_onboarding_more_than_x_days(days)\n\n    def update_user_onboarding(\n        self, user_id: UUID, onboarding: OnboardingUpdatableProperties\n    ) -> OnboardingStates:\n        \"\"\"Update user onboarding information by user_id\"\"\"\n\n        updated_onboarding = self.repository.update_user_onboarding(user_id, onboarding)\n\n        if all(not value for value in updated_onboarding.dict().values()):\n            self.repository.remove_user_onboarding(user_id)\n\n        return updated_onboarding\n", "backend/modules/onboarding/service/__init__.py": "from .onboarding_service import OnboardingService", "backend/modules/onboarding/dto/__init__.py": "from .inputs import OnboardingUpdatableProperties\n", "backend/modules/onboarding/dto/inputs.py": "from typing import Optional\n\nfrom pydantic import ConfigDict, BaseModel\n\n\nclass OnboardingUpdatableProperties(BaseModel):\n\n    \"\"\"Properties that can be received on onboarding update\"\"\"\n\n    onboarding_a: Optional[bool] = None\n    onboarding_b1: Optional[bool] = None\n    onboarding_b2: Optional[bool] = None\n    onboarding_b3: Optional[bool] = None\n    model_config = ConfigDict(extra=\"forbid\")\n", "backend/modules/onboarding/entity/onboarding.py": "from pydantic import BaseModel\n\n\nclass OnboardingStates(BaseModel):\n    \"\"\"Response when getting onboarding\"\"\"\n\n    onboarding_a: bool\n    onboarding_b1: bool\n    onboarding_b2: bool\n    onboarding_b3: bool\n", "backend/modules/onboarding/entity/__init__.py": "from .onboarding import OnboardingStates\n"}