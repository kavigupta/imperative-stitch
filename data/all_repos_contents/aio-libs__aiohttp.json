{"setup.py": "import os\nimport pathlib\nimport sys\n\nfrom setuptools import Extension, setup\n\nif sys.version_info < (3, 8):\n    raise RuntimeError(\"aiohttp 4.x requires Python 3.8+\")\n\n\nNO_EXTENSIONS: bool = bool(os.environ.get(\"AIOHTTP_NO_EXTENSIONS\"))\nHERE = pathlib.Path(__file__).parent\nIS_GIT_REPO = (HERE / \".git\").exists()\n\n\nif sys.implementation.name != \"cpython\":\n    NO_EXTENSIONS = True\n\n\nif IS_GIT_REPO and not (HERE / \"vendor/llhttp/README.md\").exists():\n    print(\"Install submodules when building from git clone\", file=sys.stderr)\n    print(\"Hint:\", file=sys.stderr)\n    print(\"  git submodule update --init\", file=sys.stderr)\n    sys.exit(2)\n\n\n# NOTE: makefile cythonizes all Cython modules\n\nextensions = [\n    Extension(\"aiohttp._websocket\", [\"aiohttp/_websocket.c\"]),\n    Extension(\n        \"aiohttp._http_parser\",\n        [\n            \"aiohttp/_http_parser.c\",\n            \"aiohttp/_find_header.c\",\n            \"vendor/llhttp/build/c/llhttp.c\",\n            \"vendor/llhttp/src/native/api.c\",\n            \"vendor/llhttp/src/native/http.c\",\n        ],\n        define_macros=[(\"LLHTTP_STRICT_MODE\", 0)],\n        include_dirs=[\"vendor/llhttp/build\"],\n    ),\n    Extension(\"aiohttp._helpers\", [\"aiohttp/_helpers.c\"]),\n    Extension(\"aiohttp._http_writer\", [\"aiohttp/_http_writer.c\"]),\n]\n\n\nbuild_type = \"Pure\" if NO_EXTENSIONS else \"Accelerated\"\nsetup_kwargs = {} if NO_EXTENSIONS else {\"ext_modules\": extensions}\n\nprint(\"*********************\", file=sys.stderr)\nprint(\"* {build_type} build *\".format_map(locals()), file=sys.stderr)\nprint(\"*********************\", file=sys.stderr)\nsetup(**setup_kwargs)\n", "tools/cleanup_changes.py": "#!/usr/bin/env python\n\n# Run me after the backport branch release to cleanup CHANGES records\n# that was backported and published.\n\nimport re\nimport subprocess\nfrom pathlib import Path\n\nALLOWED_SUFFIXES = (\n    \"bugfix\",\n    \"feature\",\n    \"deprecation\",\n    \"breaking\",\n    \"doc\",\n    \"packaging\",\n    \"contrib\",\n    \"misc\",\n)\nPATTERN = re.compile(\n    r\"(\\d+|[0-9a-f]{8}|[0-9a-f]{7}|[0-9a-f]{40})\\.(\"\n    + \"|\".join(ALLOWED_SUFFIXES)\n    + r\")(\\.\\d+)?(\\.rst)?\",\n)\n\n\ndef main():\n    root = Path(__file__).parent.parent\n    delete = []\n    changes = (root / \"CHANGES.rst\").read_text()\n    for fname in (root / \"CHANGES\").iterdir():\n        match = PATTERN.match(fname.name)\n        if match is not None:\n            commit_issue_or_pr = match.group(1)\n            tst_issue_or_pr = f\":issue:`{commit_issue_or_pr}`\"\n            tst_commit = f\":commit:`{commit_issue_or_pr}`\"\n            if tst_issue_or_pr in changes or tst_commit in changes:\n                subprocess.run([\"git\", \"rm\", fname])\n                delete.append(fname.name)\n    print(\"Deleted CHANGES records:\", \" \".join(delete))\n    print(\"Please verify and commit\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "tools/gen.py": "#!/usr/bin/env python\n\nimport io\nimport pathlib\nfrom collections import defaultdict\n\nimport multidict\n\nROOT = pathlib.Path.cwd()\nwhile ROOT.parent != ROOT and not (ROOT / \".git\").exists():\n    ROOT = ROOT.parent\n\n\ndef calc_headers(root):\n    hdrs_file = root / \"aiohttp/hdrs.py\"\n    code = compile(hdrs_file.read_text(), str(hdrs_file), \"exec\")\n    globs = {}\n    exec(code, globs)\n    headers = [val for val in globs.values() if isinstance(val, multidict.istr)]\n    return sorted(headers)\n\n\nheaders = calc_headers(ROOT)\n\n\ndef factory():\n    return defaultdict(factory)\n\n\nTERMINAL = object()\n\n\ndef build(headers):\n    dct = defaultdict(factory)\n    for hdr in headers:\n        d = dct\n        for ch in hdr:\n            d = d[ch]\n        d[TERMINAL] = hdr\n    return dct\n\n\ndct = build(headers)\n\n\nHEADER = \"\"\"\\\n/*  The file is autogenerated from aiohttp/hdrs.py\nRun ./tools/gen.py to update it after the origin changing. */\n\n#include \"_find_header.h\"\n\n#define NEXT_CHAR() \\\\\n{ \\\\\n    count++; \\\\\n    if (count == size) { \\\\\n        /* end of search */ \\\\\n        return -1; \\\\\n    } \\\\\n    pchar++; \\\\\n    ch = *pchar; \\\\\n    last = (count == size -1); \\\\\n} while(0);\n\nint\nfind_header(const char *str, int size)\n{\n    char *pchar = str;\n    int last;\n    char ch;\n    int count = -1;\n    pchar--;\n\"\"\"\n\nBLOCK = \"\"\"\n{label}\n    NEXT_CHAR();\n    switch (ch) {{\n{cases}\n        default:\n            return -1;\n    }}\n\"\"\"\n\nCASE = \"\"\"\\\n        case '{char}':\n            if (last) {{\n                return {index};\n            }}\n            goto {next};\"\"\"\n\nFOOTER = \"\"\"\n{missing}\nmissing:\n    /* nothing found */\n    return -1;\n}}\n\"\"\"\n\n\ndef gen_prefix(prefix, k):\n    if k == \"-\":\n        return prefix + \"_\"\n    else:\n        return prefix + k.upper()\n\n\ndef gen_block(dct, prefix, used_blocks, missing, out):\n    cases = {}\n    for k, v in dct.items():\n        if k is TERMINAL:\n            continue\n        next_prefix = gen_prefix(prefix, k)\n        term = v.get(TERMINAL)\n        if term is not None:\n            index = headers.index(term)\n        else:\n            index = -1\n        hi = k.upper()\n        lo = k.lower()\n        case = CASE.format(char=hi, index=index, next=next_prefix)\n        cases[hi] = case\n        if lo != hi:\n            case = CASE.format(char=lo, index=index, next=next_prefix)\n            cases[lo] = case\n    label = prefix + \":\" if prefix else \"\"\n    if cases:\n        block = BLOCK.format(label=label, cases=\"\\n\".join(cases.values()))\n        out.write(block)\n    else:\n        missing.add(label)\n    for k, v in dct.items():\n        if not isinstance(v, defaultdict):\n            continue\n        block_name = gen_prefix(prefix, k)\n        if block_name in used_blocks:\n            continue\n        used_blocks.add(block_name)\n        gen_block(v, block_name, used_blocks, missing, out)\n\n\ndef gen(dct):\n    out = io.StringIO()\n    out.write(HEADER)\n    missing = set()\n    gen_block(dct, \"\", set(), missing, out)\n    missing_labels = \"\\n\".join(sorted(missing))\n    out.write(FOOTER.format(missing=missing_labels))\n    return out\n\n\ndef gen_headers(headers):\n    out = io.StringIO()\n    out.write(\"# The file is autogenerated from aiohttp/hdrs.py\\n\")\n    out.write(\"# Run ./tools/gen.py to update it after the origin changing.\")\n    out.write(\"\\n\\n\")\n    out.write(\"from . import hdrs\\n\")\n    out.write(\"cdef tuple headers = (\\n\")\n    for hdr in headers:\n        out.write(\"    hdrs.{},\\n\".format(hdr.upper().replace(\"-\", \"_\")))\n    out.write(\")\\n\")\n    return out\n\n\n# print(gen(dct).getvalue())\n# print(gen_headers(headers).getvalue())\n\n\nfolder = ROOT / \"aiohttp\"\n\nwith (folder / \"_find_header.c\").open(\"w\") as f:\n    f.write(gen(dct).getvalue())\n\nwith (folder / \"_headers.pxi\").open(\"w\") as f:\n    f.write(gen_headers(headers).getvalue())\n", "tools/check_changes.py": "#!/usr/bin/env python3\n\nimport re\nimport sys\nfrom pathlib import Path\n\nALLOWED_SUFFIXES = (\n    \"bugfix\",\n    \"feature\",\n    \"deprecation\",\n    \"breaking\",\n    \"doc\",\n    \"packaging\",\n    \"contrib\",\n    \"misc\",\n)\nPATTERN = re.compile(\n    r\"(\\d+|[0-9a-f]{8}|[0-9a-f]{7}|[0-9a-f]{40})\\.(\"\n    + \"|\".join(ALLOWED_SUFFIXES)\n    + r\")(\\.\\d+)?(\\.rst)?\",\n)\n\n\ndef get_root(script_path):\n    folder = script_path.resolve().parent\n    while not (folder / \".git\").exists():\n        folder = folder.parent\n        if folder == folder.anchor:\n            raise RuntimeError(\"git repo not found\")\n    return folder\n\n\ndef main(argv):\n    print('Check \"CHANGES\" folder... ', end=\"\", flush=True)\n    here = Path(argv[0])\n    root = get_root(here)\n    changes = root / \"CHANGES\"\n    failed = False\n    for fname in changes.iterdir():\n        if fname.name in (\".gitignore\", \".TEMPLATE.rst\", \"README.rst\"):\n            continue\n        if not PATTERN.match(fname.name):\n            if not failed:\n                print(\"\")\n            print(\"Illegal CHANGES record\", fname, file=sys.stderr)\n            failed = True\n\n    if failed:\n        print(\"\", file=sys.stderr)\n        print(\"See ./CHANGES/README.rst for the naming instructions\", file=sys.stderr)\n        print(\"\", file=sys.stderr)\n    else:\n        print(\"OK\")\n\n    return int(failed)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv))\n", "tools/check_sum.py": "#!/usr/bin/env python\n\nimport argparse\nimport hashlib\nimport pathlib\nimport sys\n\nPARSER = argparse.ArgumentParser(\n    description=\"Helper for check file hashes in Makefile instead of bare timestamps\"\n)\nPARSER.add_argument(\"dst\", metavar=\"DST\", type=pathlib.Path)\nPARSER.add_argument(\"-d\", \"--debug\", action=\"store_true\", default=False)\n\n\ndef main(argv):\n    args = PARSER.parse_args(argv)\n    dst = args.dst\n    assert dst.suffix == \".hash\"\n    dirname = dst.parent\n    if dirname.name != \".hash\":\n        if args.debug:\n            print(f\"Invalid name {dst} -> dirname {dirname}\", file=sys.stderr)\n        return 0\n    dirname.mkdir(exist_ok=True)\n    src_dir = dirname.parent\n    src_name = dst.stem  # drop .hash\n    full_src = src_dir / src_name\n    hasher = hashlib.sha256()\n    try:\n        hasher.update(full_src.read_bytes())\n    except OSError:\n        if args.debug:\n            print(f\"Cannot open {full_src}\", file=sys.stderr)\n        return 0\n    src_hash = hasher.hexdigest()\n    if dst.exists():\n        dst_hash = dst.read_text()\n    else:\n        dst_hash = \"\"\n    if src_hash != dst_hash:\n        dst.write_text(src_hash)\n        print(f\"re-hash {src_hash}\")\n    else:\n        if args.debug:\n            print(f\"Skip {src_hash} checksum, up-to-date\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv[1:]))\n", "tools/bench-asyncio-write.py": "import asyncio\nimport atexit\nimport math\nimport os\nimport signal\nfrom typing import List, Tuple\n\nPORT = 8888\n\nserver = os.fork()\nif server == 0:\n    loop = asyncio.get_event_loop()\n    coro = asyncio.start_server(lambda *_: None, port=PORT)\n    loop.run_until_complete(coro)\n    loop.run_forever()\nelse:\n    atexit.register(os.kill, server, signal.SIGTERM)\n\n\nasync def write_joined_bytearray(writer, chunks):\n    body = bytearray(chunks[0])\n    for c in chunks[1:]:\n        body += c\n    writer.write(body)\n\n\nasync def write_joined_list(writer, chunks):\n    body = b\"\".join(chunks)\n    writer.write(body)\n\n\nasync def write_separately(writer, chunks):\n    for c in chunks:\n        writer.write(c)\n\n\ndef fm_size(s, _fms=(\"\", \"K\", \"M\", \"G\")):\n    i = 0\n    while s >= 1024:\n        s /= 1024\n        i += 1\n    return f\"{s:.0f}{_fms[i]}B\"\n\n\ndef fm_time(s, _fms=(\"\", \"m\", \"\u00b5\", \"n\")):\n    if s == 0:\n        return \"0\"\n    i = 0\n    while s < 1:\n        s *= 1000\n        i += 1\n    return f\"{s:.2f}{_fms[i]}s\"\n\n\ndef _job(j: List[int]) -> Tuple[str, List[bytes]]:\n    # Always start with a 256B headers chunk\n    body = [b\"0\" * s for s in [256] + list(j)]\n    job_title = f\"{fm_size(sum(j))} / {len(j)}\"\n    return (job_title, body)\n\n\nwrites = [\n    (\"b''.join\", write_joined_list),\n    (\"bytearray\", write_joined_bytearray),\n    (\"multiple writes\", write_separately),\n]\n\nbodies = (\n    [],\n    [10 * 2**0],\n    [10 * 2**7],\n    [10 * 2**17],\n    [10 * 2**27],\n    [50 * 2**27],\n    [1 * 2**0 for _ in range(10)],\n    [1 * 2**7 for _ in range(10)],\n    [1 * 2**17 for _ in range(10)],\n    [1 * 2**27 for _ in range(10)],\n    [10 * 2**27 for _ in range(5)],\n)\n\n\njobs = [_job(j) for j in bodies]\n\n\nasync def time(loop, fn, *args):\n    spent = []\n    while not spent or sum(spent) < 0.2:\n        s = loop.time()\n        await fn(*args)\n        e = loop.time()\n        spent.append(e - s)\n    mean = sum(spent) / len(spent)\n    sd = sum((x - mean) ** 2 for x in spent) / len(spent)\n    return len(spent), mean, math.sqrt(sd)\n\n\nasync def main(loop):\n    _, writer = await asyncio.open_connection(port=PORT)\n    print(\"Loop:\", loop)\n    print(\"Transport:\", writer._transport)\n    res = [\n        (\"size/chunks\", \"Write option\", \"Mean\", \"Std dev\", \"loops\", \"Variation\"),\n    ]\n    res.append([\":---\", \":---\", \"---:\", \"---:\", \"---:\", \"---:\"])\n\n    async def bench(job_title, w, body, base=None):\n        it, mean, sd = await time(loop, w[1], writer, c)\n        res.append(\n            (\n                job_title,\n                w[0],\n                fm_time(mean),\n                fm_time(sd),\n                str(it),\n                f\"{mean / base - 1:.2%}\" if base is not None else \"\",\n            )\n        )\n        return mean\n\n    for t, c in jobs:\n        print(\"Doing\", t)\n        base = await bench(t, writes[0], c)\n        for w in writes[1:]:\n            await bench(\"\", w, c, base)\n    return res\n\n\nloop = asyncio.get_event_loop()\nresults = loop.run_until_complete(main(loop))\nwith open(\"bench.md\", \"w\") as f:\n    for line in results:\n        f.write(\"| {} |\\n\".format(\" | \".join(line)))\n", "requirements/sync-direct-runtime-deps.py": "#!/usr/bin/env python\n\"\"\"Sync direct runtime dependencies from setup.cfg to runtime-deps.in.\"\"\"\n\nfrom configparser import ConfigParser\nfrom pathlib import Path\n\ncfg = ConfigParser()\ncfg.read(Path(\"setup.cfg\"))\nreqs = cfg[\"options\"][\"install_requires\"] + cfg.items(\"options.extras_require\")[0][1]\nreqs = sorted(reqs.split(\"\\n\"), key=str.casefold)\nreqs.remove(\"\")\n\nwith open(Path(\"requirements\", \"runtime-deps.in\"), \"w\") as outfile:\n    header = \"# Extracted from `setup.cfg` via `make sync-direct-runtime-deps`\\n\\n\"\n    outfile.write(header)\n    outfile.write(\"\\n\".join(reqs) + \"\\n\")\n", "aiohttp/web_fileresponse.py": "import asyncio\nimport mimetypes\nimport os\nimport pathlib\nimport sys\nfrom contextlib import suppress\nfrom types import MappingProxyType\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Final,\n    Optional,\n    Tuple,\n    cast,\n)\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import ETAG_ANY, ETag, must_be_empty_body\nfrom .typedefs import LooseHeaders, PathLike\nfrom .web_exceptions import (\n    HTTPNotModified,\n    HTTPPartialContent,\n    HTTPPreconditionFailed,\n    HTTPRequestRangeNotSatisfiable,\n)\nfrom .web_response import StreamResponse\n\n__all__ = (\"FileResponse\",)\n\nif TYPE_CHECKING:\n    from .web_request import BaseRequest\n\n\n_T_OnChunkSent = Optional[Callable[[bytes], Awaitable[None]]]\n\n\nNOSENDFILE: Final[bool] = bool(os.environ.get(\"AIOHTTP_NOSENDFILE\"))\n\nif sys.version_info < (3, 9):\n    mimetypes.encodings_map[\".br\"] = \"br\"\n\n# File extension to IANA encodings map that will be checked in the order defined.\nENCODING_EXTENSIONS = MappingProxyType(\n    {ext: mimetypes.encodings_map[ext] for ext in (\".br\", \".gz\")}\n)\n\n\nclass FileResponse(StreamResponse):\n    \"\"\"A response object can be used to send files.\"\"\"\n\n    def __init__(\n        self,\n        path: PathLike,\n        chunk_size: int = 256 * 1024,\n        status: int = 200,\n        reason: Optional[str] = None,\n        headers: Optional[LooseHeaders] = None,\n    ) -> None:\n        super().__init__(status=status, reason=reason, headers=headers)\n\n        self._path = pathlib.Path(path)\n        self._chunk_size = chunk_size\n\n    async def _sendfile_fallback(\n        self, writer: AbstractStreamWriter, fobj: IO[Any], offset: int, count: int\n    ) -> AbstractStreamWriter:\n        # To keep memory usage low,fobj is transferred in chunks\n        # controlled by the constructor's chunk_size argument.\n\n        chunk_size = self._chunk_size\n        loop = asyncio.get_event_loop()\n\n        await loop.run_in_executor(None, fobj.seek, offset)\n\n        chunk = await loop.run_in_executor(None, fobj.read, chunk_size)\n        while chunk:\n            await writer.write(chunk)\n            count = count - chunk_size\n            if count <= 0:\n                break\n            chunk = await loop.run_in_executor(None, fobj.read, min(chunk_size, count))\n\n        await writer.drain()\n        return writer\n\n    async def _sendfile(\n        self, request: \"BaseRequest\", fobj: IO[Any], offset: int, count: int\n    ) -> AbstractStreamWriter:\n        writer = await super().prepare(request)\n        assert writer is not None\n\n        if NOSENDFILE or self.compression:\n            return await self._sendfile_fallback(writer, fobj, offset, count)\n\n        loop = request._loop\n        transport = request.transport\n        assert transport is not None\n\n        try:\n            await loop.sendfile(transport, fobj, offset, count)\n        except NotImplementedError:\n            return await self._sendfile_fallback(writer, fobj, offset, count)\n\n        await super().write_eof()\n        return writer\n\n    @staticmethod\n    def _strong_etag_match(etag_value: str, etags: Tuple[ETag, ...]) -> bool:\n        if len(etags) == 1 and etags[0].value == ETAG_ANY:\n            return True\n        return any(etag.value == etag_value for etag in etags if not etag.is_weak)\n\n    async def _not_modified(\n        self, request: \"BaseRequest\", etag_value: str, last_modified: float\n    ) -> Optional[AbstractStreamWriter]:\n        self.set_status(HTTPNotModified.status_code)\n        self._length_check = False\n        self.etag = etag_value  # type: ignore[assignment]\n        self.last_modified = last_modified  # type: ignore[assignment]\n        # Delete any Content-Length headers provided by user. HTTP 304\n        # should always have empty response body\n        return await super().prepare(request)\n\n    async def _precondition_failed(\n        self, request: \"BaseRequest\"\n    ) -> Optional[AbstractStreamWriter]:\n        self.set_status(HTTPPreconditionFailed.status_code)\n        self.content_length = 0\n        return await super().prepare(request)\n\n    def _get_file_path_stat_encoding(\n        self, accept_encoding: str\n    ) -> Tuple[pathlib.Path, os.stat_result, Optional[str]]:\n        \"\"\"Return the file path, stat result, and encoding.\n\n        If an uncompressed file is returned, the encoding is set to\n        :py:data:`None`.\n\n        This method should be called from a thread executor\n        since it calls os.stat which may block.\n        \"\"\"\n        file_path = self._path\n        for file_extension, file_encoding in ENCODING_EXTENSIONS.items():\n            if file_encoding not in accept_encoding:\n                continue\n\n            compressed_path = file_path.with_suffix(file_path.suffix + file_extension)\n            with suppress(OSError):\n                return compressed_path, compressed_path.stat(), file_encoding\n\n        # Fallback to the uncompressed file\n        return file_path, file_path.stat(), None\n\n    async def prepare(self, request: \"BaseRequest\") -> Optional[AbstractStreamWriter]:\n        loop = asyncio.get_event_loop()\n        # Encoding comparisons should be case-insensitive\n        # https://www.rfc-editor.org/rfc/rfc9110#section-8.4.1\n        accept_encoding = request.headers.get(hdrs.ACCEPT_ENCODING, \"\").lower()\n        file_path, st, file_encoding = await loop.run_in_executor(\n            None, self._get_file_path_stat_encoding, accept_encoding\n        )\n\n        etag_value = f\"{st.st_mtime_ns:x}-{st.st_size:x}\"\n        last_modified = st.st_mtime\n\n        # https://tools.ietf.org/html/rfc7232#section-6\n        ifmatch = request.if_match\n        if ifmatch is not None and not self._strong_etag_match(etag_value, ifmatch):\n            return await self._precondition_failed(request)\n\n        unmodsince = request.if_unmodified_since\n        if (\n            unmodsince is not None\n            and ifmatch is None\n            and st.st_mtime > unmodsince.timestamp()\n        ):\n            return await self._precondition_failed(request)\n\n        ifnonematch = request.if_none_match\n        if ifnonematch is not None and self._strong_etag_match(etag_value, ifnonematch):\n            return await self._not_modified(request, etag_value, last_modified)\n\n        modsince = request.if_modified_since\n        if (\n            modsince is not None\n            and ifnonematch is None\n            and st.st_mtime <= modsince.timestamp()\n        ):\n            return await self._not_modified(request, etag_value, last_modified)\n\n        ct = None\n        if hdrs.CONTENT_TYPE not in self.headers:\n            ct, encoding = mimetypes.guess_type(str(file_path))\n            if not ct:\n                ct = \"application/octet-stream\"\n        else:\n            encoding = file_encoding\n\n        status = self._status\n        file_size = st.st_size\n        count = file_size\n\n        start = None\n\n        ifrange = request.if_range\n        if ifrange is None or st.st_mtime <= ifrange.timestamp():\n            # If-Range header check:\n            # condition = cached date >= last modification date\n            # return 206 if True else 200.\n            # if False:\n            #   Range header would not be processed, return 200\n            # if True but Range header missing\n            #   return 200\n            try:\n                rng = request.http_range\n                start = rng.start\n                end = rng.stop\n            except ValueError:\n                # https://tools.ietf.org/html/rfc7233:\n                # A server generating a 416 (Range Not Satisfiable) response to\n                # a byte-range request SHOULD send a Content-Range header field\n                # with an unsatisfied-range value.\n                # The complete-length in a 416 response indicates the current\n                # length of the selected representation.\n                #\n                # Will do the same below. Many servers ignore this and do not\n                # send a Content-Range header with HTTP 416\n                self.headers[hdrs.CONTENT_RANGE] = f\"bytes */{file_size}\"\n                self.set_status(HTTPRequestRangeNotSatisfiable.status_code)\n                return await super().prepare(request)\n\n            # If a range request has been made, convert start, end slice\n            # notation into file pointer offset and count\n            if start is not None or end is not None:\n                if start < 0 and end is None:  # return tail of file\n                    start += file_size\n                    if start < 0:\n                        # if Range:bytes=-1000 in request header but file size\n                        # is only 200, there would be trouble without this\n                        start = 0\n                    count = file_size - start\n                else:\n                    # rfc7233:If the last-byte-pos value is\n                    # absent, or if the value is greater than or equal to\n                    # the current length of the representation data,\n                    # the byte range is interpreted as the remainder\n                    # of the representation (i.e., the server replaces the\n                    # value of last-byte-pos with a value that is one less than\n                    # the current length of the selected representation).\n                    count = (\n                        min(end if end is not None else file_size, file_size) - start\n                    )\n\n                if start >= file_size:\n                    # HTTP 416 should be returned in this case.\n                    #\n                    # According to https://tools.ietf.org/html/rfc7233:\n                    # If a valid byte-range-set includes at least one\n                    # byte-range-spec with a first-byte-pos that is less than\n                    # the current length of the representation, or at least one\n                    # suffix-byte-range-spec with a non-zero suffix-length,\n                    # then the byte-range-set is satisfiable. Otherwise, the\n                    # byte-range-set is unsatisfiable.\n                    self.headers[hdrs.CONTENT_RANGE] = f\"bytes */{file_size}\"\n                    self.set_status(HTTPRequestRangeNotSatisfiable.status_code)\n                    return await super().prepare(request)\n\n                status = HTTPPartialContent.status_code\n                # Even though you are sending the whole file, you should still\n                # return a HTTP 206 for a Range request.\n                self.set_status(status)\n\n        if ct:\n            self.content_type = ct\n        if encoding:\n            self.headers[hdrs.CONTENT_ENCODING] = encoding\n        if file_encoding:\n            self.headers[hdrs.VARY] = hdrs.ACCEPT_ENCODING\n            # Disable compression if we are already sending\n            # a compressed file since we don't want to double\n            # compress.\n            self._compression = False\n\n        self.etag = etag_value  # type: ignore[assignment]\n        self.last_modified = st.st_mtime  # type: ignore[assignment]\n        self.content_length = count\n\n        self.headers[hdrs.ACCEPT_RANGES] = \"bytes\"\n\n        real_start = cast(int, start)\n\n        if status == HTTPPartialContent.status_code:\n            self.headers[hdrs.CONTENT_RANGE] = \"bytes {}-{}/{}\".format(\n                real_start, real_start + count - 1, file_size\n            )\n\n        # If we are sending 0 bytes calling sendfile() will throw a ValueError\n        if count == 0 or must_be_empty_body(request.method, self.status):\n            return await super().prepare(request)\n\n        fobj = await loop.run_in_executor(None, file_path.open, \"rb\")\n        if start:  # be aware that start could be None or int=0 here.\n            offset = start\n        else:\n            offset = 0\n\n        try:\n            return await self._sendfile(request, fobj, offset, count)\n        finally:\n            await asyncio.shield(loop.run_in_executor(None, fobj.close))\n", "aiohttp/tracing.py": "import dataclasses\nfrom types import SimpleNamespace\nfrom typing import TYPE_CHECKING, Awaitable, Optional, Protocol, Type, TypeVar\n\nfrom aiosignal import Signal\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom .client_reqrep import ClientResponse\n\nif TYPE_CHECKING:\n    from .client import ClientSession\n\n    _ParamT_contra = TypeVar(\"_ParamT_contra\", contravariant=True)\n\n    class _SignalCallback(Protocol[_ParamT_contra]):\n        def __call__(\n            self,\n            __client_session: ClientSession,\n            __trace_config_ctx: SimpleNamespace,\n            __params: _ParamT_contra,\n        ) -> Awaitable[None]: ...\n\n\n__all__ = (\n    \"TraceConfig\",\n    \"TraceRequestStartParams\",\n    \"TraceRequestEndParams\",\n    \"TraceRequestExceptionParams\",\n    \"TraceConnectionQueuedStartParams\",\n    \"TraceConnectionQueuedEndParams\",\n    \"TraceConnectionCreateStartParams\",\n    \"TraceConnectionCreateEndParams\",\n    \"TraceConnectionReuseconnParams\",\n    \"TraceDnsResolveHostStartParams\",\n    \"TraceDnsResolveHostEndParams\",\n    \"TraceDnsCacheHitParams\",\n    \"TraceDnsCacheMissParams\",\n    \"TraceRequestRedirectParams\",\n    \"TraceRequestChunkSentParams\",\n    \"TraceResponseChunkReceivedParams\",\n    \"TraceRequestHeadersSentParams\",\n)\n\n\nclass TraceConfig:\n    \"\"\"First-class used to trace requests launched via ClientSession objects.\"\"\"\n\n    def __init__(\n        self, trace_config_ctx_factory: Type[SimpleNamespace] = SimpleNamespace\n    ) -> None:\n        self._on_request_start: Signal[_SignalCallback[TraceRequestStartParams]] = (\n            Signal(self)\n        )\n        self._on_request_chunk_sent: Signal[\n            _SignalCallback[TraceRequestChunkSentParams]\n        ] = Signal(self)\n        self._on_response_chunk_received: Signal[\n            _SignalCallback[TraceResponseChunkReceivedParams]\n        ] = Signal(self)\n        self._on_request_end: Signal[_SignalCallback[TraceRequestEndParams]] = Signal(\n            self\n        )\n        self._on_request_exception: Signal[\n            _SignalCallback[TraceRequestExceptionParams]\n        ] = Signal(self)\n        self._on_request_redirect: Signal[\n            _SignalCallback[TraceRequestRedirectParams]\n        ] = Signal(self)\n        self._on_connection_queued_start: Signal[\n            _SignalCallback[TraceConnectionQueuedStartParams]\n        ] = Signal(self)\n        self._on_connection_queued_end: Signal[\n            _SignalCallback[TraceConnectionQueuedEndParams]\n        ] = Signal(self)\n        self._on_connection_create_start: Signal[\n            _SignalCallback[TraceConnectionCreateStartParams]\n        ] = Signal(self)\n        self._on_connection_create_end: Signal[\n            _SignalCallback[TraceConnectionCreateEndParams]\n        ] = Signal(self)\n        self._on_connection_reuseconn: Signal[\n            _SignalCallback[TraceConnectionReuseconnParams]\n        ] = Signal(self)\n        self._on_dns_resolvehost_start: Signal[\n            _SignalCallback[TraceDnsResolveHostStartParams]\n        ] = Signal(self)\n        self._on_dns_resolvehost_end: Signal[\n            _SignalCallback[TraceDnsResolveHostEndParams]\n        ] = Signal(self)\n        self._on_dns_cache_hit: Signal[_SignalCallback[TraceDnsCacheHitParams]] = (\n            Signal(self)\n        )\n        self._on_dns_cache_miss: Signal[_SignalCallback[TraceDnsCacheMissParams]] = (\n            Signal(self)\n        )\n        self._on_request_headers_sent: Signal[\n            _SignalCallback[TraceRequestHeadersSentParams]\n        ] = Signal(self)\n\n        self._trace_config_ctx_factory = trace_config_ctx_factory\n\n    def trace_config_ctx(\n        self, trace_request_ctx: Optional[SimpleNamespace] = None\n    ) -> SimpleNamespace:\n        \"\"\"Return a new trace_config_ctx instance\"\"\"\n        return self._trace_config_ctx_factory(trace_request_ctx=trace_request_ctx)\n\n    def freeze(self) -> None:\n        self._on_request_start.freeze()\n        self._on_request_chunk_sent.freeze()\n        self._on_response_chunk_received.freeze()\n        self._on_request_end.freeze()\n        self._on_request_exception.freeze()\n        self._on_request_redirect.freeze()\n        self._on_connection_queued_start.freeze()\n        self._on_connection_queued_end.freeze()\n        self._on_connection_create_start.freeze()\n        self._on_connection_create_end.freeze()\n        self._on_connection_reuseconn.freeze()\n        self._on_dns_resolvehost_start.freeze()\n        self._on_dns_resolvehost_end.freeze()\n        self._on_dns_cache_hit.freeze()\n        self._on_dns_cache_miss.freeze()\n        self._on_request_headers_sent.freeze()\n\n    @property\n    def on_request_start(self) -> \"Signal[_SignalCallback[TraceRequestStartParams]]\":\n        return self._on_request_start\n\n    @property\n    def on_request_chunk_sent(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestChunkSentParams]]\":\n        return self._on_request_chunk_sent\n\n    @property\n    def on_response_chunk_received(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceResponseChunkReceivedParams]]\":\n        return self._on_response_chunk_received\n\n    @property\n    def on_request_end(self) -> \"Signal[_SignalCallback[TraceRequestEndParams]]\":\n        return self._on_request_end\n\n    @property\n    def on_request_exception(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestExceptionParams]]\":\n        return self._on_request_exception\n\n    @property\n    def on_request_redirect(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestRedirectParams]]\":\n        return self._on_request_redirect\n\n    @property\n    def on_connection_queued_start(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionQueuedStartParams]]\":\n        return self._on_connection_queued_start\n\n    @property\n    def on_connection_queued_end(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionQueuedEndParams]]\":\n        return self._on_connection_queued_end\n\n    @property\n    def on_connection_create_start(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionCreateStartParams]]\":\n        return self._on_connection_create_start\n\n    @property\n    def on_connection_create_end(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionCreateEndParams]]\":\n        return self._on_connection_create_end\n\n    @property\n    def on_connection_reuseconn(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionReuseconnParams]]\":\n        return self._on_connection_reuseconn\n\n    @property\n    def on_dns_resolvehost_start(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceDnsResolveHostStartParams]]\":\n        return self._on_dns_resolvehost_start\n\n    @property\n    def on_dns_resolvehost_end(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceDnsResolveHostEndParams]]\":\n        return self._on_dns_resolvehost_end\n\n    @property\n    def on_dns_cache_hit(self) -> \"Signal[_SignalCallback[TraceDnsCacheHitParams]]\":\n        return self._on_dns_cache_hit\n\n    @property\n    def on_dns_cache_miss(self) -> \"Signal[_SignalCallback[TraceDnsCacheMissParams]]\":\n        return self._on_dns_cache_miss\n\n    @property\n    def on_request_headers_sent(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestHeadersSentParams]]\":\n        return self._on_request_headers_sent\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceRequestStartParams:\n    \"\"\"Parameters sent by the `on_request_start` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceRequestChunkSentParams:\n    \"\"\"Parameters sent by the `on_request_chunk_sent` signal\"\"\"\n\n    method: str\n    url: URL\n    chunk: bytes\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceResponseChunkReceivedParams:\n    \"\"\"Parameters sent by the `on_response_chunk_received` signal\"\"\"\n\n    method: str\n    url: URL\n    chunk: bytes\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceRequestEndParams:\n    \"\"\"Parameters sent by the `on_request_end` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n    response: ClientResponse\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceRequestExceptionParams:\n    \"\"\"Parameters sent by the `on_request_exception` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n    exception: BaseException\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceRequestRedirectParams:\n    \"\"\"Parameters sent by the `on_request_redirect` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n    response: ClientResponse\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceConnectionQueuedStartParams:\n    \"\"\"Parameters sent by the `on_connection_queued_start` signal\"\"\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceConnectionQueuedEndParams:\n    \"\"\"Parameters sent by the `on_connection_queued_end` signal\"\"\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceConnectionCreateStartParams:\n    \"\"\"Parameters sent by the `on_connection_create_start` signal\"\"\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceConnectionCreateEndParams:\n    \"\"\"Parameters sent by the `on_connection_create_end` signal\"\"\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceConnectionReuseconnParams:\n    \"\"\"Parameters sent by the `on_connection_reuseconn` signal\"\"\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceDnsResolveHostStartParams:\n    \"\"\"Parameters sent by the `on_dns_resolvehost_start` signal\"\"\"\n\n    host: str\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceDnsResolveHostEndParams:\n    \"\"\"Parameters sent by the `on_dns_resolvehost_end` signal\"\"\"\n\n    host: str\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceDnsCacheHitParams:\n    \"\"\"Parameters sent by the `on_dns_cache_hit` signal\"\"\"\n\n    host: str\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceDnsCacheMissParams:\n    \"\"\"Parameters sent by the `on_dns_cache_miss` signal\"\"\"\n\n    host: str\n\n\n@dataclasses.dataclass(frozen=True)\nclass TraceRequestHeadersSentParams:\n    \"\"\"Parameters sent by the `on_request_headers_sent` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n\n\nclass Trace:\n    \"\"\"Internal dependency holder class.\n\n    Used to keep together the main dependencies used\n    at the moment of send a signal.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: \"ClientSession\",\n        trace_config: TraceConfig,\n        trace_config_ctx: SimpleNamespace,\n    ) -> None:\n        self._trace_config = trace_config\n        self._trace_config_ctx = trace_config_ctx\n        self._session = session\n\n    async def send_request_start(\n        self, method: str, url: URL, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        return await self._trace_config.on_request_start.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestStartParams(method, url, headers),\n        )\n\n    async def send_request_chunk_sent(\n        self, method: str, url: URL, chunk: bytes\n    ) -> None:\n        return await self._trace_config.on_request_chunk_sent.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestChunkSentParams(method, url, chunk),\n        )\n\n    async def send_response_chunk_received(\n        self, method: str, url: URL, chunk: bytes\n    ) -> None:\n        return await self._trace_config.on_response_chunk_received.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceResponseChunkReceivedParams(method, url, chunk),\n        )\n\n    async def send_request_end(\n        self,\n        method: str,\n        url: URL,\n        headers: \"CIMultiDict[str]\",\n        response: ClientResponse,\n    ) -> None:\n        return await self._trace_config.on_request_end.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestEndParams(method, url, headers, response),\n        )\n\n    async def send_request_exception(\n        self,\n        method: str,\n        url: URL,\n        headers: \"CIMultiDict[str]\",\n        exception: BaseException,\n    ) -> None:\n        return await self._trace_config.on_request_exception.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestExceptionParams(method, url, headers, exception),\n        )\n\n    async def send_request_redirect(\n        self,\n        method: str,\n        url: URL,\n        headers: \"CIMultiDict[str]\",\n        response: ClientResponse,\n    ) -> None:\n        return await self._trace_config._on_request_redirect.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestRedirectParams(method, url, headers, response),\n        )\n\n    async def send_connection_queued_start(self) -> None:\n        return await self._trace_config.on_connection_queued_start.send(\n            self._session, self._trace_config_ctx, TraceConnectionQueuedStartParams()\n        )\n\n    async def send_connection_queued_end(self) -> None:\n        return await self._trace_config.on_connection_queued_end.send(\n            self._session, self._trace_config_ctx, TraceConnectionQueuedEndParams()\n        )\n\n    async def send_connection_create_start(self) -> None:\n        return await self._trace_config.on_connection_create_start.send(\n            self._session, self._trace_config_ctx, TraceConnectionCreateStartParams()\n        )\n\n    async def send_connection_create_end(self) -> None:\n        return await self._trace_config.on_connection_create_end.send(\n            self._session, self._trace_config_ctx, TraceConnectionCreateEndParams()\n        )\n\n    async def send_connection_reuseconn(self) -> None:\n        return await self._trace_config.on_connection_reuseconn.send(\n            self._session, self._trace_config_ctx, TraceConnectionReuseconnParams()\n        )\n\n    async def send_dns_resolvehost_start(self, host: str) -> None:\n        return await self._trace_config.on_dns_resolvehost_start.send(\n            self._session, self._trace_config_ctx, TraceDnsResolveHostStartParams(host)\n        )\n\n    async def send_dns_resolvehost_end(self, host: str) -> None:\n        return await self._trace_config.on_dns_resolvehost_end.send(\n            self._session, self._trace_config_ctx, TraceDnsResolveHostEndParams(host)\n        )\n\n    async def send_dns_cache_hit(self, host: str) -> None:\n        return await self._trace_config.on_dns_cache_hit.send(\n            self._session, self._trace_config_ctx, TraceDnsCacheHitParams(host)\n        )\n\n    async def send_dns_cache_miss(self, host: str) -> None:\n        return await self._trace_config.on_dns_cache_miss.send(\n            self._session, self._trace_config_ctx, TraceDnsCacheMissParams(host)\n        )\n\n    async def send_request_headers(\n        self, method: str, url: URL, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        return await self._trace_config._on_request_headers_sent.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestHeadersSentParams(method, url, headers),\n        )\n", "aiohttp/log.py": "import logging\n\naccess_logger = logging.getLogger(\"aiohttp.access\")\nclient_logger = logging.getLogger(\"aiohttp.client\")\ninternal_logger = logging.getLogger(\"aiohttp.internal\")\nserver_logger = logging.getLogger(\"aiohttp.server\")\nweb_logger = logging.getLogger(\"aiohttp.web\")\nws_logger = logging.getLogger(\"aiohttp.websocket\")\n", "aiohttp/web_protocol.py": "import asyncio\nimport asyncio.streams\nimport dataclasses\nimport traceback\nfrom collections import deque\nfrom contextlib import suppress\nfrom html import escape as html_escape\nfrom http import HTTPStatus\nfrom logging import Logger\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Deque,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport yarl\n\nfrom .abc import AbstractAccessLogger, AbstractAsyncAccessLogger, AbstractStreamWriter\nfrom .base_protocol import BaseProtocol\nfrom .helpers import ceil_timeout, set_exception\nfrom .http import (\n    HttpProcessingError,\n    HttpRequestParser,\n    HttpVersion10,\n    RawRequestMessage,\n    StreamWriter,\n)\nfrom .log import access_logger, server_logger\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .tcp_helpers import tcp_keepalive\nfrom .web_exceptions import HTTPException\nfrom .web_log import AccessLogger\nfrom .web_request import BaseRequest\nfrom .web_response import Response, StreamResponse\n\n__all__ = (\"RequestHandler\", \"RequestPayloadError\", \"PayloadAccessError\")\n\nif TYPE_CHECKING:\n    from .web_server import Server\n\n\n_RequestFactory = Callable[\n    [\n        RawRequestMessage,\n        StreamReader,\n        \"RequestHandler\",\n        AbstractStreamWriter,\n        \"asyncio.Task[None]\",\n    ],\n    BaseRequest,\n]\n\n_RequestHandler = Callable[[BaseRequest], Awaitable[StreamResponse]]\n_AnyAbstractAccessLogger = Union[\n    Type[AbstractAsyncAccessLogger],\n    Type[AbstractAccessLogger],\n]\n\nERROR = RawRequestMessage(\n    \"UNKNOWN\",\n    \"/\",\n    HttpVersion10,\n    {},  # type: ignore[arg-type]\n    {},  # type: ignore[arg-type]\n    True,\n    None,\n    False,\n    False,\n    yarl.URL(\"/\"),\n)\n\n\nclass RequestPayloadError(Exception):\n    \"\"\"Payload parsing error.\"\"\"\n\n\nclass PayloadAccessError(Exception):\n    \"\"\"Payload was accessed after response was sent.\"\"\"\n\n\nclass AccessLoggerWrapper(AbstractAsyncAccessLogger):\n    \"\"\"Wrap an AbstractAccessLogger so it behaves like an AbstractAsyncAccessLogger.\"\"\"\n\n    def __init__(\n        self, access_logger: AbstractAccessLogger, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self.access_logger = access_logger\n        self._loop = loop\n        super().__init__()\n\n    async def log(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        self.access_logger.log(request, response, self._loop.time() - request_start)\n\n\n@dataclasses.dataclass(frozen=True)\nclass _ErrInfo:\n    status: int\n    exc: BaseException\n    message: str\n\n\n_MsgType = Tuple[Union[RawRequestMessage, _ErrInfo], StreamReader]\n\n\nclass RequestHandler(BaseProtocol):\n    \"\"\"HTTP protocol implementation.\n\n    RequestHandler handles incoming HTTP request. It reads request line,\n    request headers and request payload and calls handle_request() method.\n    By default it always returns with 404 response.\n\n    RequestHandler handles errors in incoming request, like bad\n    status line, bad headers or incomplete payload. If any error occurs,\n    connection gets closed.\n\n    keepalive_timeout -- number of seconds before closing\n                         keep-alive connection\n\n    tcp_keepalive -- TCP keep-alive is on, default is on\n\n    logger -- custom logger object\n\n    access_log_class -- custom class for access_logger\n\n    access_log -- custom logging object\n\n    access_log_format -- access log format string\n\n    loop -- Optional event loop\n\n    max_line_size -- Optional maximum header line size\n\n    max_field_size -- Optional maximum header field size\n\n    timeout_ceil_threshold -- Optional value to specify\n                              threshold to ceil() timeout\n                              values\n\n    \"\"\"\n\n    KEEPALIVE_RESCHEDULE_DELAY = 1\n\n    __slots__ = (\n        \"_request_count\",\n        \"_keepalive\",\n        \"_manager\",\n        \"_request_handler\",\n        \"_request_factory\",\n        \"_tcp_keepalive\",\n        \"_keepalive_time\",\n        \"_keepalive_handle\",\n        \"_keepalive_timeout\",\n        \"_lingering_time\",\n        \"_messages\",\n        \"_message_tail\",\n        \"_waiter\",\n        \"_task_handler\",\n        \"_upgrade\",\n        \"_payload_parser\",\n        \"_request_parser\",\n        \"logger\",\n        \"access_log\",\n        \"access_logger\",\n        \"_close\",\n        \"_force_close\",\n        \"_current_request\",\n        \"_timeout_ceil_threshold\",\n    )\n\n    def __init__(\n        self,\n        manager: \"Server\",\n        *,\n        loop: asyncio.AbstractEventLoop,\n        keepalive_timeout: float = 75.0,  # NGINX default is 75 secs\n        tcp_keepalive: bool = True,\n        logger: Logger = server_logger,\n        access_log_class: _AnyAbstractAccessLogger = AccessLogger,\n        access_log: Optional[Logger] = access_logger,\n        access_log_format: str = AccessLogger.LOG_FORMAT,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n        lingering_time: float = 10.0,\n        read_bufsize: int = 2**16,\n        auto_decompress: bool = True,\n        timeout_ceil_threshold: float = 5,\n    ):\n        super().__init__(loop)\n\n        self._request_count = 0\n        self._keepalive = False\n        self._current_request: Optional[BaseRequest] = None\n        self._manager: Optional[Server] = manager\n        self._request_handler: Optional[_RequestHandler] = manager.request_handler\n        self._request_factory: Optional[_RequestFactory] = manager.request_factory\n\n        self._tcp_keepalive = tcp_keepalive\n        # placeholder to be replaced on keepalive timeout setup\n        self._keepalive_time = 0.0\n        self._keepalive_handle: Optional[asyncio.Handle] = None\n        self._keepalive_timeout = keepalive_timeout\n        self._lingering_time = float(lingering_time)\n\n        self._messages: Deque[_MsgType] = deque()\n        self._message_tail = b\"\"\n\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._task_handler: Optional[asyncio.Task[None]] = None\n\n        self._upgrade = False\n        self._payload_parser: Any = None\n        self._request_parser: Optional[HttpRequestParser] = HttpRequestParser(\n            self,\n            loop,\n            read_bufsize,\n            max_line_size=max_line_size,\n            max_field_size=max_field_size,\n            payload_exception=RequestPayloadError,\n            auto_decompress=auto_decompress,\n        )\n\n        self._timeout_ceil_threshold: float = 5\n        try:\n            self._timeout_ceil_threshold = float(timeout_ceil_threshold)\n        except (TypeError, ValueError):\n            pass\n\n        self.logger = logger\n        self.access_log = access_log\n        if access_log:\n            if issubclass(access_log_class, AbstractAsyncAccessLogger):\n                self.access_logger: Optional[AbstractAsyncAccessLogger] = (\n                    access_log_class()\n                )\n            else:\n                access_logger = access_log_class(access_log, access_log_format)\n                self.access_logger = AccessLoggerWrapper(\n                    access_logger,\n                    self._loop,\n                )\n        else:\n            self.access_logger = None\n\n        self._close = False\n        self._force_close = False\n\n    def __repr__(self) -> str:\n        return \"<{} {}>\".format(\n            self.__class__.__name__,\n            \"connected\" if self.transport is not None else \"disconnected\",\n        )\n\n    @property\n    def keepalive_timeout(self) -> float:\n        return self._keepalive_timeout\n\n    async def shutdown(self, timeout: Optional[float] = 15.0) -> None:\n        \"\"\"Do worker process exit preparations.\n\n        We need to clean up everything and stop accepting requests.\n        It is especially important for keep-alive connections.\n        \"\"\"\n        self._force_close = True\n\n        if self._keepalive_handle is not None:\n            self._keepalive_handle.cancel()\n\n        if self._waiter:\n            self._waiter.cancel()\n\n        # wait for handlers\n        with suppress(asyncio.CancelledError, asyncio.TimeoutError):\n            async with ceil_timeout(timeout):\n                if self._current_request is not None:\n                    self._current_request._cancel(asyncio.CancelledError())\n\n                if self._task_handler is not None and not self._task_handler.done():\n                    await self._task_handler\n\n        # force-close non-idle handler\n        if self._task_handler is not None:\n            self._task_handler.cancel()\n\n        if self.transport is not None:\n            self.transport.close()\n            self.transport = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        super().connection_made(transport)\n\n        real_transport = cast(asyncio.Transport, transport)\n        if self._tcp_keepalive:\n            tcp_keepalive(real_transport)\n\n        self._task_handler = self._loop.create_task(self.start())\n        assert self._manager is not None\n        self._manager.connection_made(self, real_transport)\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        if self._manager is None:\n            return\n        self._manager.connection_lost(self, exc)\n\n        super().connection_lost(exc)\n\n        # Grab value before setting _manager to None.\n        handler_cancellation = self._manager.handler_cancellation\n\n        self._manager = None\n        self._force_close = True\n        self._request_factory = None\n        self._request_handler = None\n        self._request_parser = None\n\n        if self._keepalive_handle is not None:\n            self._keepalive_handle.cancel()\n\n        if self._current_request is not None:\n            if exc is None:\n                exc = ConnectionResetError(\"Connection lost\")\n            self._current_request._cancel(exc)\n\n        if self._waiter is not None:\n            self._waiter.cancel()\n\n        if handler_cancellation and self._task_handler is not None:\n            self._task_handler.cancel()\n\n        self._task_handler = None\n\n        if self._payload_parser is not None:\n            self._payload_parser.feed_eof()\n            self._payload_parser = None\n\n    def set_parser(self, parser: Any) -> None:\n        # Actual type is WebReader\n        assert self._payload_parser is None\n\n        self._payload_parser = parser\n\n        if self._message_tail:\n            self._payload_parser.feed_data(self._message_tail)\n            self._message_tail = b\"\"\n\n    def eof_received(self) -> None:\n        pass\n\n    def data_received(self, data: bytes) -> None:\n        if self._force_close or self._close:\n            return\n        # parse http messages\n        messages: Sequence[_MsgType]\n        if self._payload_parser is None and not self._upgrade:\n            assert self._request_parser is not None\n            try:\n                messages, upgraded, tail = self._request_parser.feed_data(data)\n            except HttpProcessingError as exc:\n                messages = [\n                    (_ErrInfo(status=400, exc=exc, message=exc.message), EMPTY_PAYLOAD)\n                ]\n                upgraded = False\n                tail = b\"\"\n\n            for msg, payload in messages or ():\n                self._request_count += 1\n                self._messages.append((msg, payload))\n\n            waiter = self._waiter\n            if messages and waiter is not None and not waiter.done():\n                # don't set result twice\n                waiter.set_result(None)\n\n            self._upgrade = upgraded\n            if upgraded and tail:\n                self._message_tail = tail\n\n        # no parser, just store\n        elif self._payload_parser is None and self._upgrade and data:\n            self._message_tail += data\n\n        # feed payload\n        elif data:\n            eof, tail = self._payload_parser.feed_data(data)\n            if eof:\n                self.close()\n\n    def keep_alive(self, val: bool) -> None:\n        \"\"\"Set keep-alive connection mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._keepalive = val\n        if self._keepalive_handle:\n            self._keepalive_handle.cancel()\n            self._keepalive_handle = None\n\n    def close(self) -> None:\n        \"\"\"Close connection.\n\n        Stop accepting new pipelining messages and close\n        connection when handlers done processing messages.\n        \"\"\"\n        self._close = True\n        if self._waiter:\n            self._waiter.cancel()\n\n    def force_close(self) -> None:\n        \"\"\"Forcefully close connection.\"\"\"\n        self._force_close = True\n        if self._waiter:\n            self._waiter.cancel()\n        if self.transport is not None:\n            self.transport.close()\n            self.transport = None\n\n    async def log_access(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        if self.access_logger is not None:\n            await self.access_logger.log(request, response, request_start)\n\n    def log_debug(self, *args: Any, **kw: Any) -> None:\n        if self._loop.get_debug():\n            self.logger.debug(*args, **kw)\n\n    def log_exception(self, *args: Any, **kw: Any) -> None:\n        self.logger.exception(*args, **kw)\n\n    def _process_keepalive(self) -> None:\n        if self._force_close or not self._keepalive:\n            return\n\n        next = self._keepalive_time + self._keepalive_timeout\n\n        # handler in idle state\n        if self._waiter:\n            if self._loop.time() > next:\n                self.force_close()\n                return\n\n        # not all request handlers are done,\n        # reschedule itself to next second\n        self._keepalive_handle = self._loop.call_later(\n            self.KEEPALIVE_RESCHEDULE_DELAY,\n            self._process_keepalive,\n        )\n\n    async def _handle_request(\n        self,\n        request: BaseRequest,\n        start_time: float,\n        request_handler: Callable[[BaseRequest], Awaitable[StreamResponse]],\n    ) -> Tuple[StreamResponse, bool]:\n        assert self._request_handler is not None\n        try:\n            try:\n                self._current_request = request\n                resp = await request_handler(request)\n            finally:\n                self._current_request = None\n        except HTTPException as exc:\n            resp = Response(\n                status=exc.status, reason=exc.reason, text=exc.text, headers=exc.headers\n            )\n            resp._cookies = exc._cookies\n            reset = await self.finish_response(request, resp, start_time)\n        except asyncio.CancelledError:\n            raise\n        except asyncio.TimeoutError as exc:\n            self.log_debug(\"Request handler timed out.\", exc_info=exc)\n            resp = self.handle_error(request, 504)\n            reset = await self.finish_response(request, resp, start_time)\n        except Exception as exc:\n            resp = self.handle_error(request, 500, exc)\n            reset = await self.finish_response(request, resp, start_time)\n        else:\n            reset = await self.finish_response(request, resp, start_time)\n\n        return resp, reset\n\n    async def start(self) -> None:\n        \"\"\"Process incoming request.\n\n        It reads request line, request headers and request payload, then\n        calls handle_request() method. Subclass has to override\n        handle_request(). start() handles various exceptions in request\n        or response handling. Connection is being closed always unless\n        keep_alive(True) specified.\n        \"\"\"\n        loop = self._loop\n        handler = self._task_handler\n        assert handler is not None\n        manager = self._manager\n        assert manager is not None\n        keepalive_timeout = self._keepalive_timeout\n        resp = None\n        assert self._request_factory is not None\n        assert self._request_handler is not None\n\n        while not self._force_close:\n            if not self._messages:\n                try:\n                    # wait for next request\n                    self._waiter = loop.create_future()\n                    await self._waiter\n                except asyncio.CancelledError:\n                    break\n                finally:\n                    self._waiter = None\n\n            message, payload = self._messages.popleft()\n\n            start = loop.time()\n\n            manager.requests_count += 1\n            writer = StreamWriter(self, loop)\n            if isinstance(message, _ErrInfo):\n                # make request_factory work\n                request_handler = self._make_error_handler(message)\n                message = ERROR\n            else:\n                request_handler = self._request_handler\n\n            request = self._request_factory(message, payload, self, writer, handler)\n            try:\n                # a new task is used for copy context vars (#3406)\n                task = self._loop.create_task(\n                    self._handle_request(request, start, request_handler)\n                )\n                try:\n                    resp, reset = await task\n                except (asyncio.CancelledError, ConnectionError):\n                    self.log_debug(\"Ignored premature client disconnection\")\n                    break\n\n                # Drop the processed task from asyncio.Task.all_tasks() early\n                del task\n                # https://github.com/python/mypy/issues/14309\n                if reset:  # type: ignore[possibly-undefined]\n                    self.log_debug(\"Ignored premature client disconnection 2\")\n                    break\n\n                # notify server about keep-alive\n                self._keepalive = bool(resp.keep_alive)\n\n                # check payload\n                if not payload.is_eof():\n                    lingering_time = self._lingering_time\n                    # Could be force closed while awaiting above tasks.\n                    if not self._force_close and lingering_time:  # type: ignore[redundant-expr]\n                        self.log_debug(\n                            \"Start lingering close timer for %s sec.\", lingering_time\n                        )\n\n                        now = loop.time()\n                        end_t = now + lingering_time\n\n                        with suppress(asyncio.TimeoutError, asyncio.CancelledError):\n                            while not payload.is_eof() and now < end_t:\n                                async with ceil_timeout(end_t - now):\n                                    # read and ignore\n                                    await payload.readany()\n                                now = loop.time()\n\n                    # if payload still uncompleted\n                    if not payload.is_eof() and not self._force_close:\n                        self.log_debug(\"Uncompleted request.\")\n                        self.close()\n\n                set_exception(payload, PayloadAccessError())\n\n            except asyncio.CancelledError:\n                self.log_debug(\"Ignored premature client disconnection \")\n                break\n            except RuntimeError as exc:\n                if self._loop.get_debug():\n                    self.log_exception(\"Unhandled runtime exception\", exc_info=exc)\n                self.force_close()\n            except Exception as exc:\n                self.log_exception(\"Unhandled exception\", exc_info=exc)\n                self.force_close()\n            finally:\n                if self.transport is None and resp is not None:\n                    self.log_debug(\"Ignored premature client disconnection.\")\n                elif not self._force_close:\n                    if self._keepalive and not self._close:\n                        # start keep-alive timer\n                        if keepalive_timeout is not None:\n                            now = self._loop.time()\n                            self._keepalive_time = now\n                            if self._keepalive_handle is None:\n                                self._keepalive_handle = loop.call_at(\n                                    now + keepalive_timeout, self._process_keepalive\n                                )\n                    else:\n                        break\n\n        # remove handler, close transport if no handlers left\n        if not self._force_close:\n            self._task_handler = None\n            if self.transport is not None:\n                self.transport.close()\n\n    async def finish_response(\n        self, request: BaseRequest, resp: StreamResponse, start_time: float\n    ) -> bool:\n        \"\"\"Prepare the response and write_eof, then log access.\n\n        This has to\n        be called within the context of any exception so the access logger\n        can get exception information. Returns True if the client disconnects\n        prematurely.\n        \"\"\"\n        request._finish()\n        if self._request_parser is not None:\n            self._request_parser.set_upgraded(False)\n            self._upgrade = False\n            if self._message_tail:\n                self._request_parser.feed_data(self._message_tail)\n                self._message_tail = b\"\"\n        try:\n            prepare_meth = resp.prepare\n        except AttributeError:\n            if resp is None:\n                raise RuntimeError(\"Missing return \" \"statement on request handler\")\n            else:\n                raise RuntimeError(\n                    \"Web-handler should return \"\n                    \"a response instance, \"\n                    \"got {!r}\".format(resp)\n                )\n        try:\n            await prepare_meth(request)\n            await resp.write_eof()\n        except ConnectionError:\n            await self.log_access(request, resp, start_time)\n            return True\n        else:\n            await self.log_access(request, resp, start_time)\n            return False\n\n    def handle_error(\n        self,\n        request: BaseRequest,\n        status: int = 500,\n        exc: Optional[BaseException] = None,\n        message: Optional[str] = None,\n    ) -> StreamResponse:\n        \"\"\"Handle errors.\n\n        Returns HTTP response with specific status code. Logs additional\n        information. It always closes current connection.\n        \"\"\"\n        self.log_exception(\"Error handling request\", exc_info=exc)\n\n        # some data already got sent, connection is broken\n        if request.writer.output_size > 0:\n            raise ConnectionError(\n                \"Response is sent already, cannot send another response \"\n                \"with the error message\"\n            )\n\n        ct = \"text/plain\"\n        if status == HTTPStatus.INTERNAL_SERVER_ERROR:\n            title = \"{0.value} {0.phrase}\".format(HTTPStatus.INTERNAL_SERVER_ERROR)\n            msg = HTTPStatus.INTERNAL_SERVER_ERROR.description\n            tb = None\n            if self._loop.get_debug():\n                with suppress(Exception):\n                    tb = traceback.format_exc()\n\n            if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n                if tb:\n                    tb = html_escape(tb)\n                    msg = f\"<h2>Traceback:</h2>\\n<pre>{tb}</pre>\"\n                message = (\n                    \"<html><head>\"\n                    \"<title>{title}</title>\"\n                    \"</head><body>\\n<h1>{title}</h1>\"\n                    \"\\n{msg}\\n</body></html>\\n\"\n                ).format(title=title, msg=msg)\n                ct = \"text/html\"\n            else:\n                if tb:\n                    msg = tb\n                message = title + \"\\n\\n\" + msg\n\n        resp = Response(status=status, text=message, content_type=ct)\n        resp.force_close()\n\n        return resp\n\n    def _make_error_handler(\n        self, err_info: _ErrInfo\n    ) -> Callable[[BaseRequest], Awaitable[StreamResponse]]:\n        async def handler(request: BaseRequest) -> StreamResponse:\n            return self.handle_error(\n                request, err_info.status, err_info.exc, err_info.message\n            )\n\n        return handler\n", "aiohttp/web_urldispatcher.py": "import abc\nimport asyncio\nimport base64\nimport functools\nimport hashlib\nimport html\nimport keyword\nimport os\nimport re\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom types import MappingProxyType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Container,\n    Dict,\n    Final,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    NoReturn,\n    Optional,\n    Pattern,\n    Set,\n    Sized,\n    Tuple,\n    Type,\n    TypedDict,\n    Union,\n    cast,\n)\n\nfrom yarl import URL, __version__ as yarl_version  # type: ignore[attr-defined]\n\nfrom . import hdrs\nfrom .abc import AbstractMatchInfo, AbstractRouter, AbstractView\nfrom .helpers import DEBUG\nfrom .http import HttpVersion11\nfrom .typedefs import Handler, PathLike\nfrom .web_exceptions import (\n    HTTPException,\n    HTTPExpectationFailed,\n    HTTPForbidden,\n    HTTPMethodNotAllowed,\n    HTTPNotFound,\n)\nfrom .web_fileresponse import FileResponse\nfrom .web_request import Request\nfrom .web_response import Response, StreamResponse\nfrom .web_routedef import AbstractRouteDef\n\n__all__ = (\n    \"UrlDispatcher\",\n    \"UrlMappingMatchInfo\",\n    \"AbstractResource\",\n    \"Resource\",\n    \"PlainResource\",\n    \"DynamicResource\",\n    \"AbstractRoute\",\n    \"ResourceRoute\",\n    \"StaticResource\",\n    \"View\",\n)\n\n\nif TYPE_CHECKING:\n    from .web_app import Application\n\n    BaseDict = Dict[str, str]\nelse:\n    BaseDict = dict\n\nYARL_VERSION: Final[Tuple[int, ...]] = tuple(map(int, yarl_version.split(\".\")[:2]))\n\nHTTP_METHOD_RE: Final[Pattern[str]] = re.compile(\n    r\"^[0-9A-Za-z!#\\$%&'\\*\\+\\-\\.\\^_`\\|~]+$\"\n)\nROUTE_RE: Final[Pattern[str]] = re.compile(\n    r\"(\\{[_a-zA-Z][^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\})\"\n)\nPATH_SEP: Final[str] = re.escape(\"/\")\n\n\n_ExpectHandler = Callable[[Request], Awaitable[Optional[StreamResponse]]]\n_Resolve = Tuple[Optional[\"UrlMappingMatchInfo\"], Set[str]]\n\nhtml_escape = functools.partial(html.escape, quote=True)\n\n\nclass _InfoDict(TypedDict, total=False):\n    path: str\n\n    formatter: str\n    pattern: Pattern[str]\n\n    directory: Path\n    prefix: str\n    routes: Mapping[str, \"AbstractRoute\"]\n\n    app: \"Application\"\n\n    domain: str\n\n    rule: \"AbstractRuleMatching\"\n\n    http_exception: HTTPException\n\n\nclass AbstractResource(Sized, Iterable[\"AbstractRoute\"]):\n    def __init__(self, *, name: Optional[str] = None) -> None:\n        self._name = name\n\n    @property\n    def name(self) -> Optional[str]:\n        return self._name\n\n    @property\n    @abc.abstractmethod\n    def canonical(self) -> str:\n        \"\"\"Exposes the resource's canonical path.\n\n        For example '/foo/bar/{name}'\n\n        \"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    def url_for(self, **kwargs: str) -> URL:\n        \"\"\"Construct url for resource with additional params.\"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    async def resolve(self, request: Request) -> _Resolve:\n        \"\"\"Resolve resource.\n\n        Return (UrlMappingMatchInfo, allowed_methods) pair.\n        \"\"\"\n\n    @abc.abstractmethod\n    def add_prefix(self, prefix: str) -> None:\n        \"\"\"Add a prefix to processed URLs.\n\n        Required for subapplications support.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_info(self) -> _InfoDict:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    def freeze(self) -> None:\n        pass\n\n    @abc.abstractmethod\n    def raw_match(self, path: str) -> bool:\n        \"\"\"Perform a raw match against path\"\"\"\n\n\nclass AbstractRoute(abc.ABC):\n    def __init__(\n        self,\n        method: str,\n        handler: Union[Handler, Type[AbstractView]],\n        *,\n        expect_handler: Optional[_ExpectHandler] = None,\n        resource: Optional[AbstractResource] = None,\n    ) -> None:\n        if expect_handler is None:\n            expect_handler = _default_expect_handler\n\n        assert asyncio.iscoroutinefunction(\n            expect_handler\n        ), f\"Coroutine is expected, got {expect_handler!r}\"\n\n        method = method.upper()\n        if not HTTP_METHOD_RE.match(method):\n            raise ValueError(f\"{method} is not allowed HTTP method\")\n\n        if asyncio.iscoroutinefunction(handler):\n            pass\n        elif isinstance(handler, type) and issubclass(handler, AbstractView):\n            pass\n        else:\n            raise TypeError(\n                \"Only async functions are allowed as web-handlers \"\n                \", got {!r}\".format(handler)\n            )\n\n        self._method = method\n        self._handler = handler\n        self._expect_handler = expect_handler\n        self._resource = resource\n\n    @property\n    def method(self) -> str:\n        return self._method\n\n    @property\n    def handler(self) -> Handler:\n        return self._handler\n\n    @property\n    @abc.abstractmethod\n    def name(self) -> Optional[str]:\n        \"\"\"Optional route's name, always equals to resource's name.\"\"\"\n\n    @property\n    def resource(self) -> Optional[AbstractResource]:\n        return self._resource\n\n    @abc.abstractmethod\n    def get_info(self) -> _InfoDict:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        \"\"\"Construct url for route with additional params.\"\"\"\n\n    async def handle_expect_header(self, request: Request) -> Optional[StreamResponse]:\n        return await self._expect_handler(request)\n\n\nclass UrlMappingMatchInfo(BaseDict, AbstractMatchInfo):\n    def __init__(self, match_dict: Dict[str, str], route: AbstractRoute):\n        super().__init__(match_dict)\n        self._route = route\n        self._apps: List[Application] = []\n        self._current_app: Optional[Application] = None\n        self._frozen = False\n\n    @property\n    def handler(self) -> Handler:\n        return self._route.handler\n\n    @property\n    def route(self) -> AbstractRoute:\n        return self._route\n\n    @property\n    def expect_handler(self) -> _ExpectHandler:\n        return self._route.handle_expect_header\n\n    @property\n    def http_exception(self) -> Optional[HTTPException]:\n        return None\n\n    def get_info(self) -> _InfoDict:  # type: ignore[override]\n        return self._route.get_info()\n\n    @property\n    def apps(self) -> Tuple[\"Application\", ...]:\n        return tuple(self._apps)\n\n    def add_app(self, app: \"Application\") -> None:\n        if self._frozen:\n            raise RuntimeError(\"Cannot change apps stack after .freeze() call\")\n        if self._current_app is None:\n            self._current_app = app\n        self._apps.insert(0, app)\n\n    @property\n    def current_app(self) -> \"Application\":\n        app = self._current_app\n        assert app is not None\n        return app\n\n    @contextmanager\n    def set_current_app(self, app: \"Application\") -> Generator[None, None, None]:\n        if DEBUG:  # pragma: no cover\n            if app not in self._apps:\n                raise RuntimeError(\n                    \"Expected one of the following apps {!r}, got {!r}\".format(\n                        self._apps, app\n                    )\n                )\n        prev = self._current_app\n        self._current_app = app\n        try:\n            yield\n        finally:\n            self._current_app = prev\n\n    def freeze(self) -> None:\n        self._frozen = True\n\n    def __repr__(self) -> str:\n        return f\"<MatchInfo {super().__repr__()}: {self._route}>\"\n\n\nclass MatchInfoError(UrlMappingMatchInfo):\n    def __init__(self, http_exception: HTTPException) -> None:\n        self._exception = http_exception\n        super().__init__({}, SystemRoute(self._exception))\n\n    @property\n    def http_exception(self) -> HTTPException:\n        return self._exception\n\n    def __repr__(self) -> str:\n        return \"<MatchInfoError {}: {}>\".format(\n            self._exception.status, self._exception.reason\n        )\n\n\nasync def _default_expect_handler(request: Request) -> None:\n    \"\"\"Default handler for Expect header.\n\n    Just send \"100 Continue\" to client.\n    raise HTTPExpectationFailed if value of header is not \"100-continue\"\n    \"\"\"\n    expect = request.headers.get(hdrs.EXPECT, \"\")\n    if request.version == HttpVersion11:\n        if expect.lower() == \"100-continue\":\n            await request.writer.write(b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n        else:\n            raise HTTPExpectationFailed(text=\"Unknown Expect: %s\" % expect)\n\n\nclass Resource(AbstractResource):\n    def __init__(self, *, name: Optional[str] = None) -> None:\n        super().__init__(name=name)\n        self._routes: List[ResourceRoute] = []\n\n    def add_route(\n        self,\n        method: str,\n        handler: Union[Type[AbstractView], Handler],\n        *,\n        expect_handler: Optional[_ExpectHandler] = None,\n    ) -> \"ResourceRoute\":\n        for route_obj in self._routes:\n            if route_obj.method == method or route_obj.method == hdrs.METH_ANY:\n                raise RuntimeError(\n                    \"Added route will never be executed, \"\n                    \"method {route.method} is already \"\n                    \"registered\".format(route=route_obj)\n                )\n\n        route_obj = ResourceRoute(method, handler, self, expect_handler=expect_handler)\n        self.register_route(route_obj)\n        return route_obj\n\n    def register_route(self, route: \"ResourceRoute\") -> None:\n        assert isinstance(\n            route, ResourceRoute\n        ), f\"Instance of Route class is required, got {route!r}\"\n        self._routes.append(route)\n\n    async def resolve(self, request: Request) -> _Resolve:\n        allowed_methods: Set[str] = set()\n\n        match_dict = self._match(request.rel_url.raw_path)\n        if match_dict is None:\n            return None, allowed_methods\n\n        for route_obj in self._routes:\n            route_method = route_obj.method\n            allowed_methods.add(route_method)\n\n            if route_method == request.method or route_method == hdrs.METH_ANY:\n                return (UrlMappingMatchInfo(match_dict, route_obj), allowed_methods)\n        else:\n            return None, allowed_methods\n\n    @abc.abstractmethod\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\n        pass  # pragma: no cover\n\n    def __len__(self) -> int:\n        return len(self._routes)\n\n    def __iter__(self) -> Iterator[\"ResourceRoute\"]:\n        return iter(self._routes)\n\n    # TODO: implement all abstract methods\n\n\nclass PlainResource(Resource):\n    def __init__(self, path: str, *, name: Optional[str] = None) -> None:\n        super().__init__(name=name)\n        assert not path or path.startswith(\"/\")\n        self._path = path\n\n    @property\n    def canonical(self) -> str:\n        return self._path\n\n    def freeze(self) -> None:\n        if not self._path:\n            self._path = \"/\"\n\n    def add_prefix(self, prefix: str) -> None:\n        assert prefix.startswith(\"/\")\n        assert not prefix.endswith(\"/\")\n        assert len(prefix) > 1\n        self._path = prefix + self._path\n\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\n        # string comparison is about 10 times faster than regexp matching\n        if self._path == path:\n            return {}\n        else:\n            return None\n\n    def raw_match(self, path: str) -> bool:\n        return self._path == path\n\n    def get_info(self) -> _InfoDict:\n        return {\"path\": self._path}\n\n    def url_for(self) -> URL:  # type: ignore[override]\n        return URL.build(path=self._path, encoded=True)\n\n    def __repr__(self) -> str:\n        name = \"'\" + self.name + \"' \" if self.name is not None else \"\"\n        return f\"<PlainResource {name} {self._path}>\"\n\n\nclass DynamicResource(Resource):\n    DYN = re.compile(r\"\\{(?P<var>[_a-zA-Z][_a-zA-Z0-9]*)\\}\")\n    DYN_WITH_RE = re.compile(r\"\\{(?P<var>[_a-zA-Z][_a-zA-Z0-9]*):(?P<re>.+)\\}\")\n    GOOD = r\"[^{}/]+\"\n\n    def __init__(self, path: str, *, name: Optional[str] = None) -> None:\n        super().__init__(name=name)\n        pattern = \"\"\n        formatter = \"\"\n        for part in ROUTE_RE.split(path):\n            match = self.DYN.fullmatch(part)\n            if match:\n                pattern += \"(?P<{}>{})\".format(match.group(\"var\"), self.GOOD)\n                formatter += \"{\" + match.group(\"var\") + \"}\"\n                continue\n\n            match = self.DYN_WITH_RE.fullmatch(part)\n            if match:\n                pattern += \"(?P<{var}>{re})\".format(**match.groupdict())\n                formatter += \"{\" + match.group(\"var\") + \"}\"\n                continue\n\n            if \"{\" in part or \"}\" in part:\n                raise ValueError(f\"Invalid path '{path}'['{part}']\")\n\n            part = _requote_path(part)\n            formatter += part\n            pattern += re.escape(part)\n\n        try:\n            compiled = re.compile(pattern)\n        except re.error as exc:\n            raise ValueError(f\"Bad pattern '{pattern}': {exc}\") from None\n        assert compiled.pattern.startswith(PATH_SEP)\n        assert formatter.startswith(\"/\")\n        self._pattern = compiled\n        self._formatter = formatter\n\n    @property\n    def canonical(self) -> str:\n        return self._formatter\n\n    def add_prefix(self, prefix: str) -> None:\n        assert prefix.startswith(\"/\")\n        assert not prefix.endswith(\"/\")\n        assert len(prefix) > 1\n        self._pattern = re.compile(re.escape(prefix) + self._pattern.pattern)\n        self._formatter = prefix + self._formatter\n\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\n        match = self._pattern.fullmatch(path)\n        if match is None:\n            return None\n        else:\n            return {\n                key: _unquote_path(value) for key, value in match.groupdict().items()\n            }\n\n    def raw_match(self, path: str) -> bool:\n        return self._formatter == path\n\n    def get_info(self) -> _InfoDict:\n        return {\"formatter\": self._formatter, \"pattern\": self._pattern}\n\n    def url_for(self, **parts: str) -> URL:\n        url = self._formatter.format_map({k: _quote_path(v) for k, v in parts.items()})\n        return URL.build(path=url, encoded=True)\n\n    def __repr__(self) -> str:\n        name = \"'\" + self.name + \"' \" if self.name is not None else \"\"\n        return \"<DynamicResource {name} {formatter}>\".format(\n            name=name, formatter=self._formatter\n        )\n\n\nclass PrefixResource(AbstractResource):\n    def __init__(self, prefix: str, *, name: Optional[str] = None) -> None:\n        assert not prefix or prefix.startswith(\"/\"), prefix\n        assert prefix in (\"\", \"/\") or not prefix.endswith(\"/\"), prefix\n        super().__init__(name=name)\n        self._prefix = _requote_path(prefix)\n        self._prefix2 = self._prefix + \"/\"\n\n    @property\n    def canonical(self) -> str:\n        return self._prefix\n\n    def add_prefix(self, prefix: str) -> None:\n        assert prefix.startswith(\"/\")\n        assert not prefix.endswith(\"/\")\n        assert len(prefix) > 1\n        self._prefix = prefix + self._prefix\n        self._prefix2 = self._prefix + \"/\"\n\n    def raw_match(self, prefix: str) -> bool:\n        return False\n\n    # TODO: impl missing abstract methods\n\n\nclass StaticResource(PrefixResource):\n    VERSION_KEY = \"v\"\n\n    def __init__(\n        self,\n        prefix: str,\n        directory: PathLike,\n        *,\n        name: Optional[str] = None,\n        expect_handler: Optional[_ExpectHandler] = None,\n        chunk_size: int = 256 * 1024,\n        show_index: bool = False,\n        follow_symlinks: bool = False,\n        append_version: bool = False,\n    ) -> None:\n        super().__init__(prefix, name=name)\n        try:\n            directory = Path(directory)\n            if str(directory).startswith(\"~\"):\n                directory = Path(os.path.expanduser(str(directory)))\n            directory = directory.resolve()\n            if not directory.is_dir():\n                raise ValueError(\"Not a directory\")\n        except (FileNotFoundError, ValueError) as error:\n            raise ValueError(f\"No directory exists at '{directory}'\") from error\n        self._directory = directory\n        self._show_index = show_index\n        self._chunk_size = chunk_size\n        self._follow_symlinks = follow_symlinks\n        self._expect_handler = expect_handler\n        self._append_version = append_version\n\n        self._routes = {\n            \"GET\": ResourceRoute(\n                \"GET\", self._handle, self, expect_handler=expect_handler\n            ),\n            \"HEAD\": ResourceRoute(\n                \"HEAD\", self._handle, self, expect_handler=expect_handler\n            ),\n        }\n\n    def url_for(  # type: ignore[override]\n        self,\n        *,\n        filename: PathLike,\n        append_version: Optional[bool] = None,\n    ) -> URL:\n        if append_version is None:\n            append_version = self._append_version\n        filename = str(filename).lstrip(\"/\")\n\n        url = URL.build(path=self._prefix, encoded=True)\n        # filename is not encoded\n        if YARL_VERSION < (1, 6):\n            url = url / filename.replace(\"%\", \"%25\")\n        else:\n            url = url / filename\n\n        if append_version:\n            unresolved_path = self._directory.joinpath(filename)\n            try:\n                if self._follow_symlinks:\n                    normalized_path = Path(os.path.normpath(unresolved_path))\n                    normalized_path.relative_to(self._directory)\n                    filepath = normalized_path.resolve()\n                else:\n                    filepath = unresolved_path.resolve()\n                    filepath.relative_to(self._directory)\n            except (ValueError, FileNotFoundError):\n                # ValueError for case when path point to symlink\n                # with follow_symlinks is False\n                return url  # relatively safe\n            if filepath.is_file():\n                # TODO cache file content\n                # with file watcher for cache invalidation\n                with filepath.open(\"rb\") as f:\n                    file_bytes = f.read()\n                h = self._get_file_hash(file_bytes)\n                url = url.with_query({self.VERSION_KEY: h})\n                return url\n        return url\n\n    @staticmethod\n    def _get_file_hash(byte_array: bytes) -> str:\n        m = hashlib.sha256()  # todo sha256 can be configurable param\n        m.update(byte_array)\n        b64 = base64.urlsafe_b64encode(m.digest())\n        return b64.decode(\"ascii\")\n\n    def get_info(self) -> _InfoDict:\n        return {\n            \"directory\": self._directory,\n            \"prefix\": self._prefix,\n            \"routes\": self._routes,\n        }\n\n    def set_options_route(self, handler: Handler) -> None:\n        if \"OPTIONS\" in self._routes:\n            raise RuntimeError(\"OPTIONS route was set already\")\n        self._routes[\"OPTIONS\"] = ResourceRoute(\n            \"OPTIONS\", handler, self, expect_handler=self._expect_handler\n        )\n\n    async def resolve(self, request: Request) -> _Resolve:\n        path = request.rel_url.raw_path\n        method = request.method\n        allowed_methods = set(self._routes)\n        if not path.startswith(self._prefix2) and path != self._prefix:\n            return None, set()\n\n        if method not in allowed_methods:\n            return None, allowed_methods\n\n        match_dict = {\"filename\": _unquote_path(path[len(self._prefix) + 1 :])}\n        return (UrlMappingMatchInfo(match_dict, self._routes[method]), allowed_methods)\n\n    def __len__(self) -> int:\n        return len(self._routes)\n\n    def __iter__(self) -> Iterator[AbstractRoute]:\n        return iter(self._routes.values())\n\n    async def _handle(self, request: Request) -> StreamResponse:\n        rel_url = request.match_info[\"filename\"]\n        try:\n            filename = Path(rel_url)\n            if filename.anchor:\n                # rel_url is an absolute name like\n                # /static/\\\\machine_name\\c$ or /static/D:\\path\n                # where the static dir is totally different\n                raise HTTPForbidden()\n            unresolved_path = self._directory.joinpath(filename)\n            if self._follow_symlinks:\n                normalized_path = Path(os.path.normpath(unresolved_path))\n                normalized_path.relative_to(self._directory)\n                filepath = normalized_path.resolve()\n            else:\n                filepath = unresolved_path.resolve()\n                filepath.relative_to(self._directory)\n        except (ValueError, FileNotFoundError) as error:\n            # relatively safe\n            raise HTTPNotFound() from error\n        except HTTPForbidden:\n            raise\n        except Exception as error:\n            # perm error or other kind!\n            request.app.logger.exception(error)\n            raise HTTPNotFound() from error\n\n        # on opening a dir, load its contents if allowed\n        if filepath.is_dir():\n            if self._show_index:\n                try:\n                    return Response(\n                        text=self._directory_as_html(filepath), content_type=\"text/html\"\n                    )\n                except PermissionError:\n                    raise HTTPForbidden()\n            else:\n                raise HTTPForbidden()\n        elif filepath.is_file():\n            return FileResponse(filepath, chunk_size=self._chunk_size)\n        else:\n            raise HTTPNotFound\n\n    def _directory_as_html(self, filepath: Path) -> str:\n        # returns directory's index as html\n\n        # sanity check\n        assert filepath.is_dir()\n\n        relative_path_to_dir = filepath.relative_to(self._directory).as_posix()\n        index_of = f\"Index of /{html_escape(relative_path_to_dir)}\"\n        h1 = f\"<h1>{index_of}</h1>\"\n\n        index_list = []\n        dir_index = filepath.iterdir()\n        for _file in sorted(dir_index):\n            # show file url as relative to static path\n            rel_path = _file.relative_to(self._directory).as_posix()\n            quoted_file_url = _quote_path(f\"{self._prefix}/{rel_path}\")\n\n            # if file is a directory, add '/' to the end of the name\n            if _file.is_dir():\n                file_name = f\"{_file.name}/\"\n            else:\n                file_name = _file.name\n\n            index_list.append(\n                f'<li><a href=\"{quoted_file_url}\">{html_escape(file_name)}</a></li>'\n            )\n        ul = \"<ul>\\n{}\\n</ul>\".format(\"\\n\".join(index_list))\n        body = f\"<body>\\n{h1}\\n{ul}\\n</body>\"\n\n        head_str = f\"<head>\\n<title>{index_of}</title>\\n</head>\"\n        html = f\"<html>\\n{head_str}\\n{body}\\n</html>\"\n\n        return html\n\n    def __repr__(self) -> str:\n        name = \"'\" + self.name + \"'\" if self.name is not None else \"\"\n        return \"<StaticResource {name} {path} -> {directory!r}>\".format(\n            name=name, path=self._prefix, directory=self._directory\n        )\n\n\nclass PrefixedSubAppResource(PrefixResource):\n    def __init__(self, prefix: str, app: \"Application\") -> None:\n        super().__init__(prefix)\n        self._app = app\n        self._add_prefix_to_resources(prefix)\n\n    def add_prefix(self, prefix: str) -> None:\n        super().add_prefix(prefix)\n        self._add_prefix_to_resources(prefix)\n\n    def _add_prefix_to_resources(self, prefix: str) -> None:\n        router = self._app.router\n        for resource in router.resources():\n            # Since the canonical path of a resource is about\n            # to change, we need to unindex it and then reindex\n            router.unindex_resource(resource)\n            resource.add_prefix(prefix)\n            router.index_resource(resource)\n\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        raise RuntimeError(\".url_for() is not supported \" \"by sub-application root\")\n\n    def get_info(self) -> _InfoDict:\n        return {\"app\": self._app, \"prefix\": self._prefix}\n\n    async def resolve(self, request: Request) -> _Resolve:\n        match_info = await self._app.router.resolve(request)\n        match_info.add_app(self._app)\n        if isinstance(match_info.http_exception, HTTPMethodNotAllowed):\n            methods = match_info.http_exception.allowed_methods\n        else:\n            methods = set()\n        return match_info, methods\n\n    def __len__(self) -> int:\n        return len(self._app.router.routes())\n\n    def __iter__(self) -> Iterator[AbstractRoute]:\n        return iter(self._app.router.routes())\n\n    def __repr__(self) -> str:\n        return \"<PrefixedSubAppResource {prefix} -> {app!r}>\".format(\n            prefix=self._prefix, app=self._app\n        )\n\n\nclass AbstractRuleMatching(abc.ABC):\n    @abc.abstractmethod  # pragma: no branch\n    async def match(self, request: Request) -> bool:\n        \"\"\"Return bool if the request satisfies the criteria\"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    def get_info(self) -> _InfoDict:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @property\n    @abc.abstractmethod  # pragma: no branch\n    def canonical(self) -> str:\n        \"\"\"Return a str\"\"\"\n\n\nclass Domain(AbstractRuleMatching):\n    re_part = re.compile(r\"(?!-)[a-z\\d-]{1,63}(?<!-)\")\n\n    def __init__(self, domain: str) -> None:\n        super().__init__()\n        self._domain = self.validation(domain)\n\n    @property\n    def canonical(self) -> str:\n        return self._domain\n\n    def validation(self, domain: str) -> str:\n        if not isinstance(domain, str):\n            raise TypeError(\"Domain must be str\")\n        domain = domain.rstrip(\".\").lower()\n        if not domain:\n            raise ValueError(\"Domain cannot be empty\")\n        elif \"://\" in domain:\n            raise ValueError(\"Scheme not supported\")\n        url = URL(\"http://\" + domain)\n        assert url.raw_host is not None\n        if not all(self.re_part.fullmatch(x) for x in url.raw_host.split(\".\")):\n            raise ValueError(\"Domain not valid\")\n        if url.port == 80:\n            return url.raw_host\n        return f\"{url.raw_host}:{url.port}\"\n\n    async def match(self, request: Request) -> bool:\n        host = request.headers.get(hdrs.HOST)\n        if not host:\n            return False\n        return self.match_domain(host)\n\n    def match_domain(self, host: str) -> bool:\n        return host.lower() == self._domain\n\n    def get_info(self) -> _InfoDict:\n        return {\"domain\": self._domain}\n\n\nclass MaskDomain(Domain):\n    re_part = re.compile(r\"(?!-)[a-z\\d\\*-]{1,63}(?<!-)\")\n\n    def __init__(self, domain: str) -> None:\n        super().__init__(domain)\n        mask = self._domain.replace(\".\", r\"\\.\").replace(\"*\", \".*\")\n        self._mask = re.compile(mask)\n\n    @property\n    def canonical(self) -> str:\n        return self._mask.pattern\n\n    def match_domain(self, host: str) -> bool:\n        return self._mask.fullmatch(host) is not None\n\n\nclass MatchedSubAppResource(PrefixedSubAppResource):\n    def __init__(self, rule: AbstractRuleMatching, app: \"Application\") -> None:\n        AbstractResource.__init__(self)\n        self._prefix = \"\"\n        self._app = app\n        self._rule = rule\n\n    @property\n    def canonical(self) -> str:\n        return self._rule.canonical\n\n    def get_info(self) -> _InfoDict:\n        return {\"app\": self._app, \"rule\": self._rule}\n\n    async def resolve(self, request: Request) -> _Resolve:\n        if not await self._rule.match(request):\n            return None, set()\n        match_info = await self._app.router.resolve(request)\n        match_info.add_app(self._app)\n        if isinstance(match_info.http_exception, HTTPMethodNotAllowed):\n            methods = match_info.http_exception.allowed_methods\n        else:\n            methods = set()\n        return match_info, methods\n\n    def __repr__(self) -> str:\n        return \"<MatchedSubAppResource -> {app!r}>\" \"\".format(app=self._app)\n\n\nclass ResourceRoute(AbstractRoute):\n    \"\"\"A route with resource\"\"\"\n\n    def __init__(\n        self,\n        method: str,\n        handler: Union[Handler, Type[AbstractView]],\n        resource: AbstractResource,\n        *,\n        expect_handler: Optional[_ExpectHandler] = None,\n    ) -> None:\n        super().__init__(\n            method, handler, expect_handler=expect_handler, resource=resource\n        )\n\n    def __repr__(self) -> str:\n        return \"<ResourceRoute [{method}] {resource} -> {handler!r}\".format(\n            method=self.method, resource=self._resource, handler=self.handler\n        )\n\n    @property\n    def name(self) -> Optional[str]:\n        if self._resource is None:\n            return None\n        return self._resource.name\n\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        \"\"\"Construct url for route with additional params.\"\"\"\n        assert self._resource is not None\n        return self._resource.url_for(*args, **kwargs)\n\n    def get_info(self) -> _InfoDict:\n        assert self._resource is not None\n        return self._resource.get_info()\n\n\nclass SystemRoute(AbstractRoute):\n    def __init__(self, http_exception: HTTPException) -> None:\n        super().__init__(hdrs.METH_ANY, self._handle)\n        self._http_exception = http_exception\n\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        raise RuntimeError(\".url_for() is not allowed for SystemRoute\")\n\n    @property\n    def name(self) -> Optional[str]:\n        return None\n\n    def get_info(self) -> _InfoDict:\n        return {\"http_exception\": self._http_exception}\n\n    async def _handle(self, request: Request) -> StreamResponse:\n        raise self._http_exception\n\n    @property\n    def status(self) -> int:\n        return self._http_exception.status\n\n    @property\n    def reason(self) -> str:\n        return self._http_exception.reason\n\n    def __repr__(self) -> str:\n        return \"<SystemRoute {self.status}: {self.reason}>\".format(self=self)\n\n\nclass View(AbstractView):\n    async def _iter(self) -> StreamResponse:\n        if self.request.method not in hdrs.METH_ALL:\n            self._raise_allowed_methods()\n        method: Optional[Callable[[], Awaitable[StreamResponse]]] = getattr(\n            self, self.request.method.lower(), None\n        )\n        if method is None:\n            self._raise_allowed_methods()\n        return await method()\n\n    def __await__(self) -> Generator[Any, None, StreamResponse]:\n        return self._iter().__await__()\n\n    def _raise_allowed_methods(self) -> NoReturn:\n        allowed_methods = {m for m in hdrs.METH_ALL if hasattr(self, m.lower())}\n        raise HTTPMethodNotAllowed(self.request.method, allowed_methods)\n\n\nclass ResourcesView(Sized, Iterable[AbstractResource], Container[AbstractResource]):\n    def __init__(self, resources: List[AbstractResource]) -> None:\n        self._resources = resources\n\n    def __len__(self) -> int:\n        return len(self._resources)\n\n    def __iter__(self) -> Iterator[AbstractResource]:\n        yield from self._resources\n\n    def __contains__(self, resource: object) -> bool:\n        return resource in self._resources\n\n\nclass RoutesView(Sized, Iterable[AbstractRoute], Container[AbstractRoute]):\n    def __init__(self, resources: List[AbstractResource]):\n        self._routes: List[AbstractRoute] = []\n        for resource in resources:\n            for route in resource:\n                self._routes.append(route)\n\n    def __len__(self) -> int:\n        return len(self._routes)\n\n    def __iter__(self) -> Iterator[AbstractRoute]:\n        yield from self._routes\n\n    def __contains__(self, route: object) -> bool:\n        return route in self._routes\n\n\nclass UrlDispatcher(AbstractRouter, Mapping[str, AbstractResource]):\n    NAME_SPLIT_RE = re.compile(r\"[.:-]\")\n    HTTP_NOT_FOUND = HTTPNotFound()\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._resources: List[AbstractResource] = []\n        self._named_resources: Dict[str, AbstractResource] = {}\n        self._resource_index: dict[str, list[AbstractResource]] = {}\n        self._matched_sub_app_resources: List[MatchedSubAppResource] = []\n\n    async def resolve(self, request: Request) -> UrlMappingMatchInfo:\n        resource_index = self._resource_index\n        allowed_methods: Set[str] = set()\n\n        # Walk the url parts looking for candidates. We walk the url backwards\n        # to ensure the most explicit match is found first. If there are multiple\n        # candidates for a given url part because there are multiple resources\n        # registered for the same canonical path, we resolve them in a linear\n        # fashion to ensure registration order is respected.\n        url_part = request.rel_url.raw_path\n        while url_part:\n            for candidate in resource_index.get(url_part, ()):\n                match_dict, allowed = await candidate.resolve(request)\n                if match_dict is not None:\n                    return match_dict\n                else:\n                    allowed_methods |= allowed\n            if url_part == \"/\":\n                break\n            url_part = url_part.rpartition(\"/\")[0] or \"/\"\n\n        #\n        # We didn't find any candidates, so we'll try the matched sub-app\n        # resources which we have to walk in a linear fashion because they\n        # have regex/wildcard match rules and we cannot index them.\n        #\n        # For most cases we do not expect there to be many of these since\n        # currently they are only added by `add_domain`\n        #\n        for resource in self._matched_sub_app_resources:\n            match_dict, allowed = await resource.resolve(request)\n            if match_dict is not None:\n                return match_dict\n            else:\n                allowed_methods |= allowed\n\n        if allowed_methods:\n            return MatchInfoError(HTTPMethodNotAllowed(request.method, allowed_methods))\n\n        return MatchInfoError(self.HTTP_NOT_FOUND)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._named_resources)\n\n    def __len__(self) -> int:\n        return len(self._named_resources)\n\n    def __contains__(self, resource: object) -> bool:\n        return resource in self._named_resources\n\n    def __getitem__(self, name: str) -> AbstractResource:\n        return self._named_resources[name]\n\n    def resources(self) -> ResourcesView:\n        return ResourcesView(self._resources)\n\n    def routes(self) -> RoutesView:\n        return RoutesView(self._resources)\n\n    def named_resources(self) -> Mapping[str, AbstractResource]:\n        return MappingProxyType(self._named_resources)\n\n    def register_resource(self, resource: AbstractResource) -> None:\n        assert isinstance(\n            resource, AbstractResource\n        ), f\"Instance of AbstractResource class is required, got {resource!r}\"\n        if self.frozen:\n            raise RuntimeError(\"Cannot register a resource into frozen router.\")\n\n        name = resource.name\n\n        if name is not None:\n            parts = self.NAME_SPLIT_RE.split(name)\n            for part in parts:\n                if keyword.iskeyword(part):\n                    raise ValueError(\n                        f\"Incorrect route name {name!r}, \"\n                        \"python keywords cannot be used \"\n                        \"for route name\"\n                    )\n                if not part.isidentifier():\n                    raise ValueError(\n                        \"Incorrect route name {!r}, \"\n                        \"the name should be a sequence of \"\n                        \"python identifiers separated \"\n                        \"by dash, dot or column\".format(name)\n                    )\n            if name in self._named_resources:\n                raise ValueError(\n                    \"Duplicate {!r}, \"\n                    \"already handled by {!r}\".format(name, self._named_resources[name])\n                )\n            self._named_resources[name] = resource\n        self._resources.append(resource)\n\n        if isinstance(resource, MatchedSubAppResource):\n            # We cannot index match sub-app resources because they have match rules\n            self._matched_sub_app_resources.append(resource)\n        else:\n            self.index_resource(resource)\n\n    def _get_resource_index_key(self, resource: AbstractResource) -> str:\n        \"\"\"Return a key to index the resource in the resource index.\"\"\"\n        # strip at the first { to allow for variables\n        return resource.canonical.partition(\"{\")[0].rstrip(\"/\") or \"/\"\n\n    def index_resource(self, resource: AbstractResource) -> None:\n        \"\"\"Add a resource to the resource index.\"\"\"\n        resource_key = self._get_resource_index_key(resource)\n        # There may be multiple resources for a canonical path\n        # so we keep them in a list to ensure that registration\n        # order is respected.\n        self._resource_index.setdefault(resource_key, []).append(resource)\n\n    def unindex_resource(self, resource: AbstractResource) -> None:\n        \"\"\"Remove a resource from the resource index.\"\"\"\n        resource_key = self._get_resource_index_key(resource)\n        self._resource_index[resource_key].remove(resource)\n\n    def add_resource(self, path: str, *, name: Optional[str] = None) -> Resource:\n        if path and not path.startswith(\"/\"):\n            raise ValueError(\"path should be started with / or be empty\")\n        # Reuse last added resource if path and name are the same\n        if self._resources:\n            resource = self._resources[-1]\n            if resource.name == name and resource.raw_match(path):\n                return cast(Resource, resource)\n        if not (\"{\" in path or \"}\" in path or ROUTE_RE.search(path)):\n            resource = PlainResource(_requote_path(path), name=name)\n            self.register_resource(resource)\n            return resource\n        resource = DynamicResource(path, name=name)\n        self.register_resource(resource)\n        return resource\n\n    def add_route(\n        self,\n        method: str,\n        path: str,\n        handler: Union[Handler, Type[AbstractView]],\n        *,\n        name: Optional[str] = None,\n        expect_handler: Optional[_ExpectHandler] = None,\n    ) -> AbstractRoute:\n        resource = self.add_resource(path, name=name)\n        return resource.add_route(method, handler, expect_handler=expect_handler)\n\n    def add_static(\n        self,\n        prefix: str,\n        path: PathLike,\n        *,\n        name: Optional[str] = None,\n        expect_handler: Optional[_ExpectHandler] = None,\n        chunk_size: int = 256 * 1024,\n        show_index: bool = False,\n        follow_symlinks: bool = False,\n        append_version: bool = False,\n    ) -> AbstractResource:\n        \"\"\"Add static files view.\n\n        prefix - url prefix\n        path - folder with files\n\n        \"\"\"\n        assert prefix.startswith(\"/\")\n        if prefix.endswith(\"/\"):\n            prefix = prefix[:-1]\n        resource = StaticResource(\n            prefix,\n            path,\n            name=name,\n            expect_handler=expect_handler,\n            chunk_size=chunk_size,\n            show_index=show_index,\n            follow_symlinks=follow_symlinks,\n            append_version=append_version,\n        )\n        self.register_resource(resource)\n        return resource\n\n    def add_head(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method HEAD.\"\"\"\n        return self.add_route(hdrs.METH_HEAD, path, handler, **kwargs)\n\n    def add_options(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method OPTIONS.\"\"\"\n        return self.add_route(hdrs.METH_OPTIONS, path, handler, **kwargs)\n\n    def add_get(\n        self,\n        path: str,\n        handler: Handler,\n        *,\n        name: Optional[str] = None,\n        allow_head: bool = True,\n        **kwargs: Any,\n    ) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method GET.\n\n        If allow_head is true, another\n        route is added allowing head requests to the same endpoint.\n        \"\"\"\n        resource = self.add_resource(path, name=name)\n        if allow_head:\n            resource.add_route(hdrs.METH_HEAD, handler, **kwargs)\n        return resource.add_route(hdrs.METH_GET, handler, **kwargs)\n\n    def add_post(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method POST.\"\"\"\n        return self.add_route(hdrs.METH_POST, path, handler, **kwargs)\n\n    def add_put(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method PUT.\"\"\"\n        return self.add_route(hdrs.METH_PUT, path, handler, **kwargs)\n\n    def add_patch(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method PATCH.\"\"\"\n        return self.add_route(hdrs.METH_PATCH, path, handler, **kwargs)\n\n    def add_delete(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method DELETE.\"\"\"\n        return self.add_route(hdrs.METH_DELETE, path, handler, **kwargs)\n\n    def add_view(\n        self, path: str, handler: Type[AbstractView], **kwargs: Any\n    ) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with ANY methods for a class-based view.\"\"\"\n        return self.add_route(hdrs.METH_ANY, path, handler, **kwargs)\n\n    def freeze(self) -> None:\n        super().freeze()\n        for resource in self._resources:\n            resource.freeze()\n\n    def add_routes(self, routes: Iterable[AbstractRouteDef]) -> List[AbstractRoute]:\n        \"\"\"Append routes to route table.\n\n        Parameter should be a sequence of RouteDef objects.\n\n        Returns a list of registered AbstractRoute instances.\n        \"\"\"\n        registered_routes = []\n        for route_def in routes:\n            registered_routes.extend(route_def.register(self))\n        return registered_routes\n\n\ndef _quote_path(value: str) -> str:\n    if YARL_VERSION < (1, 6):\n        value = value.replace(\"%\", \"%25\")\n    return URL.build(path=value, encoded=False).raw_path\n\n\ndef _unquote_path(value: str) -> str:\n    return URL.build(path=value, encoded=True).path\n\n\ndef _requote_path(value: str) -> str:\n    # Quote non-ascii characters and other characters which must be quoted,\n    # but preserve existing %-sequences.\n    result = _quote_path(value)\n    if \"%\" in value:\n        result = result.replace(\"%25\", \"%\")\n    return result\n", "aiohttp/multipart.py": "import base64\nimport binascii\nimport json\nimport re\nimport uuid\nimport warnings\nimport zlib\nfrom collections import deque\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Deque,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\nfrom urllib.parse import parse_qsl, unquote, urlencode\n\nfrom multidict import CIMultiDict, CIMultiDictProxy\n\nfrom .compression_utils import ZLibCompressor, ZLibDecompressor\nfrom .hdrs import (\n    CONTENT_DISPOSITION,\n    CONTENT_ENCODING,\n    CONTENT_LENGTH,\n    CONTENT_TRANSFER_ENCODING,\n    CONTENT_TYPE,\n)\nfrom .helpers import CHAR, TOKEN, parse_mimetype, reify\nfrom .http import HeadersParser\nfrom .payload import (\n    JsonPayload,\n    LookupError,\n    Order,\n    Payload,\n    StringPayload,\n    get_payload,\n    payload_type,\n)\nfrom .streams import StreamReader\n\n__all__ = (\n    \"MultipartReader\",\n    \"MultipartWriter\",\n    \"BodyPartReader\",\n    \"BadContentDispositionHeader\",\n    \"BadContentDispositionParam\",\n    \"parse_content_disposition\",\n    \"content_disposition_filename\",\n)\n\n\nif TYPE_CHECKING:\n    from .client_reqrep import ClientResponse\n\n\nclass BadContentDispositionHeader(RuntimeWarning):\n    pass\n\n\nclass BadContentDispositionParam(RuntimeWarning):\n    pass\n\n\ndef parse_content_disposition(\n    header: Optional[str],\n) -> Tuple[Optional[str], Dict[str, str]]:\n    def is_token(string: str) -> bool:\n        return bool(string) and TOKEN >= set(string)\n\n    def is_quoted(string: str) -> bool:\n        return string[0] == string[-1] == '\"'\n\n    def is_rfc5987(string: str) -> bool:\n        return is_token(string) and string.count(\"'\") == 2\n\n    def is_extended_param(string: str) -> bool:\n        return string.endswith(\"*\")\n\n    def is_continuous_param(string: str) -> bool:\n        pos = string.find(\"*\") + 1\n        if not pos:\n            return False\n        substring = string[pos:-1] if string.endswith(\"*\") else string[pos:]\n        return substring.isdigit()\n\n    def unescape(text: str, *, chars: str = \"\".join(map(re.escape, CHAR))) -> str:\n        return re.sub(f\"\\\\\\\\([{chars}])\", \"\\\\1\", text)\n\n    if not header:\n        return None, {}\n\n    disptype, *parts = header.split(\";\")\n    if not is_token(disptype):\n        warnings.warn(BadContentDispositionHeader(header))\n        return None, {}\n\n    params: Dict[str, str] = {}\n    while parts:\n        item = parts.pop(0)\n\n        if \"=\" not in item:\n            warnings.warn(BadContentDispositionHeader(header))\n            return None, {}\n\n        key, value = item.split(\"=\", 1)\n        key = key.lower().strip()\n        value = value.lstrip()\n\n        if key in params:\n            warnings.warn(BadContentDispositionHeader(header))\n            return None, {}\n\n        if not is_token(key):\n            warnings.warn(BadContentDispositionParam(item))\n            continue\n\n        elif is_continuous_param(key):\n            if is_quoted(value):\n                value = unescape(value[1:-1])\n            elif not is_token(value):\n                warnings.warn(BadContentDispositionParam(item))\n                continue\n\n        elif is_extended_param(key):\n            if is_rfc5987(value):\n                encoding, _, value = value.split(\"'\", 2)\n                encoding = encoding or \"utf-8\"\n            else:\n                warnings.warn(BadContentDispositionParam(item))\n                continue\n\n            try:\n                value = unquote(value, encoding, \"strict\")\n            except UnicodeDecodeError:  # pragma: nocover\n                warnings.warn(BadContentDispositionParam(item))\n                continue\n\n        else:\n            failed = True\n            if is_quoted(value):\n                failed = False\n                value = unescape(value[1:-1].lstrip(\"\\\\/\"))\n            elif is_token(value):\n                failed = False\n            elif parts:\n                # maybe just ; in filename, in any case this is just\n                # one case fix, for proper fix we need to redesign parser\n                _value = f\"{value};{parts[0]}\"\n                if is_quoted(_value):\n                    parts.pop(0)\n                    value = unescape(_value[1:-1].lstrip(\"\\\\/\"))\n                    failed = False\n\n            if failed:\n                warnings.warn(BadContentDispositionHeader(header))\n                return None, {}\n\n        params[key] = value\n\n    return disptype.lower(), params\n\n\ndef content_disposition_filename(\n    params: Mapping[str, str], name: str = \"filename\"\n) -> Optional[str]:\n    name_suf = \"%s*\" % name\n    if not params:\n        return None\n    elif name_suf in params:\n        return params[name_suf]\n    elif name in params:\n        return params[name]\n    else:\n        parts = []\n        fnparams = sorted(\n            (key, value) for key, value in params.items() if key.startswith(name_suf)\n        )\n        for num, (key, value) in enumerate(fnparams):\n            _, tail = key.split(\"*\", 1)\n            if tail.endswith(\"*\"):\n                tail = tail[:-1]\n            if tail == str(num):\n                parts.append(value)\n            else:\n                break\n        if not parts:\n            return None\n        value = \"\".join(parts)\n        if \"'\" in value:\n            encoding, _, value = value.split(\"'\", 2)\n            encoding = encoding or \"utf-8\"\n            return unquote(value, encoding, \"strict\")\n        return value\n\n\nclass MultipartResponseWrapper:\n    \"\"\"Wrapper around the MultipartReader.\n\n    It takes care about\n    underlying connection and close it when it needs in.\n    \"\"\"\n\n    def __init__(\n        self,\n        resp: \"ClientResponse\",\n        stream: \"MultipartReader\",\n    ) -> None:\n        self.resp = resp\n        self.stream = stream\n\n    def __aiter__(self) -> \"MultipartResponseWrapper\":\n        return self\n\n    async def __anext__(\n        self,\n    ) -> Union[\"MultipartReader\", \"BodyPartReader\"]:\n        part = await self.next()\n        if part is None:\n            raise StopAsyncIteration\n        return part\n\n    def at_eof(self) -> bool:\n        \"\"\"Returns True when all response data had been read.\"\"\"\n        return self.resp.content.at_eof()\n\n    async def next(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", \"BodyPartReader\"]]:\n        \"\"\"Emits next multipart reader object.\"\"\"\n        item = await self.stream.next()\n        if self.stream.at_eof():\n            await self.release()\n        return item\n\n    async def release(self) -> None:\n        \"\"\"Release the connection gracefully.\n\n        All remaining content is read to the void.\n        \"\"\"\n        await self.resp.release()\n\n\nclass BodyPartReader:\n    \"\"\"Multipart reader for single body part.\"\"\"\n\n    chunk_size = 8192\n\n    def __init__(\n        self,\n        boundary: bytes,\n        headers: \"CIMultiDictProxy[str]\",\n        content: StreamReader,\n        *,\n        subtype: str = \"mixed\",\n        default_charset: Optional[str] = None,\n    ) -> None:\n        self.headers = headers\n        self._boundary = boundary\n        self._content = content\n        self._default_charset = default_charset\n        self._at_eof = False\n        self._is_form_data = subtype == \"form-data\"\n        # https://datatracker.ietf.org/doc/html/rfc7578#section-4.8\n        length = None if self._is_form_data else self.headers.get(CONTENT_LENGTH, None)\n        self._length = int(length) if length is not None else None\n        self._read_bytes = 0\n        self._unread: Deque[bytes] = deque()\n        self._prev_chunk: Optional[bytes] = None\n        self._content_eof = 0\n        self._cache: Dict[str, Any] = {}\n\n    def __aiter__(self) -> AsyncIterator[\"BodyPartReader\"]:\n        return self  # type: ignore[return-value]\n\n    async def __anext__(self) -> bytes:\n        part = await self.next()\n        if part is None:\n            raise StopAsyncIteration\n        return part\n\n    async def next(self) -> Optional[bytes]:\n        item = await self.read()\n        if not item:\n            return None\n        return item\n\n    async def read(self, *, decode: bool = False) -> bytes:\n        \"\"\"Reads body part data.\n\n        decode: Decodes data following by encoding\n                method from Content-Encoding header. If it missed\n                data remains untouched\n        \"\"\"\n        if self._at_eof:\n            return b\"\"\n        data = bytearray()\n        while not self._at_eof:\n            data.extend(await self.read_chunk(self.chunk_size))\n        if decode:\n            return self.decode(data)\n        return data\n\n    async def read_chunk(self, size: int = chunk_size) -> bytes:\n        \"\"\"Reads body part content chunk of the specified size.\n\n        size: chunk size\n        \"\"\"\n        if self._at_eof:\n            return b\"\"\n        if self._length:\n            chunk = await self._read_chunk_from_length(size)\n        else:\n            chunk = await self._read_chunk_from_stream(size)\n\n        # For the case of base64 data, we must read a fragment of size with a\n        # remainder of 0 by dividing by 4 for string without symbols \\n or \\r\n        encoding = self.headers.get(CONTENT_TRANSFER_ENCODING)\n        if encoding and encoding.lower() == \"base64\":\n            stripped_chunk = b\"\".join(chunk.split())\n            remainder = len(stripped_chunk) % 4\n\n            while remainder != 0 and not self.at_eof():\n                over_chunk_size = 4 - remainder\n                over_chunk = b\"\"\n\n                if self._prev_chunk:\n                    over_chunk = self._prev_chunk[:over_chunk_size]\n                    self._prev_chunk = self._prev_chunk[len(over_chunk) :]\n\n                if len(over_chunk) != over_chunk_size:\n                    over_chunk += await self._content.read(4 - len(over_chunk))\n\n                if not over_chunk:\n                    self._at_eof = True\n\n                stripped_chunk += b\"\".join(over_chunk.split())\n                chunk += over_chunk\n                remainder = len(stripped_chunk) % 4\n\n        self._read_bytes += len(chunk)\n        if self._read_bytes == self._length:\n            self._at_eof = True\n        if self._at_eof:\n            clrf = await self._content.readline()\n            assert (\n                b\"\\r\\n\" == clrf\n            ), \"reader did not read all the data or it is malformed\"\n        return chunk\n\n    async def _read_chunk_from_length(self, size: int) -> bytes:\n        # Reads body part content chunk of the specified size.\n        # The body part must has Content-Length header with proper value.\n        assert self._length is not None, \"Content-Length required for chunked read\"\n        chunk_size = min(size, self._length - self._read_bytes)\n        chunk = await self._content.read(chunk_size)\n        if self._content.at_eof():\n            self._at_eof = True\n        return chunk\n\n    async def _read_chunk_from_stream(self, size: int) -> bytes:\n        # Reads content chunk of body part with unknown length.\n        # The Content-Length header for body part is not necessary.\n        assert (\n            size >= len(self._boundary) + 2\n        ), \"Chunk size must be greater or equal than boundary length + 2\"\n        first_chunk = self._prev_chunk is None\n        if first_chunk:\n            self._prev_chunk = await self._content.read(size)\n\n        chunk = await self._content.read(size)\n        self._content_eof += int(self._content.at_eof())\n        assert self._content_eof < 3, \"Reading after EOF\"\n        assert self._prev_chunk is not None\n        window = self._prev_chunk + chunk\n        sub = b\"\\r\\n\" + self._boundary\n        if first_chunk:\n            idx = window.find(sub)\n        else:\n            idx = window.find(sub, max(0, len(self._prev_chunk) - len(sub)))\n        if idx >= 0:\n            # pushing boundary back to content\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n                self._content.unread_data(window[idx:])\n            if size > idx:\n                self._prev_chunk = self._prev_chunk[:idx]\n            chunk = window[len(self._prev_chunk) : idx]\n            if not chunk:\n                self._at_eof = True\n        result = self._prev_chunk\n        self._prev_chunk = chunk\n        return result\n\n    async def readline(self) -> bytes:\n        \"\"\"Reads body part by line by line.\"\"\"\n        if self._at_eof:\n            return b\"\"\n\n        if self._unread:\n            line = self._unread.popleft()\n        else:\n            line = await self._content.readline()\n\n        if line.startswith(self._boundary):\n            # the very last boundary may not come with \\r\\n,\n            # so set single rules for everyone\n            sline = line.rstrip(b\"\\r\\n\")\n            boundary = self._boundary\n            last_boundary = self._boundary + b\"--\"\n            # ensure that we read exactly the boundary, not something alike\n            if sline == boundary or sline == last_boundary:\n                self._at_eof = True\n                self._unread.append(line)\n                return b\"\"\n        else:\n            next_line = await self._content.readline()\n            if next_line.startswith(self._boundary):\n                line = line[:-2]  # strip CRLF but only once\n            self._unread.append(next_line)\n\n        return line\n\n    async def release(self) -> None:\n        \"\"\"Like read(), but reads all the data to the void.\"\"\"\n        if self._at_eof:\n            return\n        while not self._at_eof:\n            await self.read_chunk(self.chunk_size)\n\n    async def text(self, *, encoding: Optional[str] = None) -> str:\n        \"\"\"Like read(), but assumes that body part contains text data.\"\"\"\n        data = await self.read(decode=True)\n        # see https://www.w3.org/TR/html5/forms.html#multipart/form-data-encoding-algorithm\n        # and https://dvcs.w3.org/hg/xhr/raw-file/tip/Overview.html#dom-xmlhttprequest-send\n        encoding = encoding or self.get_charset(default=\"utf-8\")\n        return data.decode(encoding)\n\n    async def json(self, *, encoding: Optional[str] = None) -> Optional[Dict[str, Any]]:\n        \"\"\"Like read(), but assumes that body parts contains JSON data.\"\"\"\n        data = await self.read(decode=True)\n        if not data:\n            return None\n        encoding = encoding or self.get_charset(default=\"utf-8\")\n        return cast(Dict[str, Any], json.loads(data.decode(encoding)))\n\n    async def form(self, *, encoding: Optional[str] = None) -> List[Tuple[str, str]]:\n        \"\"\"Like read(), but assumes that body parts contain form urlencoded data.\"\"\"\n        data = await self.read(decode=True)\n        if not data:\n            return []\n        if encoding is not None:\n            real_encoding = encoding\n        else:\n            real_encoding = self.get_charset(default=\"utf-8\")\n        try:\n            decoded_data = data.rstrip().decode(real_encoding)\n        except UnicodeDecodeError:\n            raise ValueError(\"data cannot be decoded with %s encoding\" % real_encoding)\n\n        return parse_qsl(\n            decoded_data,\n            keep_blank_values=True,\n            encoding=real_encoding,\n        )\n\n    def at_eof(self) -> bool:\n        \"\"\"Returns True if the boundary was reached or False otherwise.\"\"\"\n        return self._at_eof\n\n    def decode(self, data: bytes) -> bytes:\n        \"\"\"Decodes data.\n\n        Decoding is done according the specified Content-Encoding\n        or Content-Transfer-Encoding headers value.\n        \"\"\"\n        if CONTENT_TRANSFER_ENCODING in self.headers:\n            data = self._decode_content_transfer(data)\n        # https://datatracker.ietf.org/doc/html/rfc7578#section-4.8\n        if not self._is_form_data and CONTENT_ENCODING in self.headers:\n            return self._decode_content(data)\n        return data\n\n    def _decode_content(self, data: bytes) -> bytes:\n        encoding = self.headers.get(CONTENT_ENCODING, \"\").lower()\n        if encoding == \"identity\":\n            return data\n        if encoding in {\"deflate\", \"gzip\"}:\n            return ZLibDecompressor(\n                encoding=encoding,\n                suppress_deflate_header=True,\n            ).decompress_sync(data)\n\n        raise RuntimeError(f\"unknown content encoding: {encoding}\")\n\n    def _decode_content_transfer(self, data: bytes) -> bytes:\n        encoding = self.headers.get(CONTENT_TRANSFER_ENCODING, \"\").lower()\n\n        if encoding == \"base64\":\n            return base64.b64decode(data)\n        elif encoding == \"quoted-printable\":\n            return binascii.a2b_qp(data)\n        elif encoding in (\"binary\", \"8bit\", \"7bit\"):\n            return data\n        else:\n            raise RuntimeError(\n                \"unknown content transfer encoding: {}\" \"\".format(encoding)\n            )\n\n    def get_charset(self, default: str) -> str:\n        \"\"\"Returns charset parameter from Content-Type header or default.\"\"\"\n        ctype = self.headers.get(CONTENT_TYPE, \"\")\n        mimetype = parse_mimetype(ctype)\n        return mimetype.parameters.get(\"charset\", self._default_charset or default)\n\n    @reify\n    def name(self) -> Optional[str]:\n        \"\"\"Returns name specified in Content-Disposition header.\n\n        If the header is missing or malformed, returns None.\n        \"\"\"\n        _, params = parse_content_disposition(self.headers.get(CONTENT_DISPOSITION))\n        return content_disposition_filename(params, \"name\")\n\n    @reify\n    def filename(self) -> Optional[str]:\n        \"\"\"Returns filename specified in Content-Disposition header.\n\n        Returns None if the header is missing or malformed.\n        \"\"\"\n        _, params = parse_content_disposition(self.headers.get(CONTENT_DISPOSITION))\n        return content_disposition_filename(params, \"filename\")\n\n\n@payload_type(BodyPartReader, order=Order.try_first)\nclass BodyPartReaderPayload(Payload):\n    def __init__(self, value: BodyPartReader, *args: Any, **kwargs: Any) -> None:\n        super().__init__(value, *args, **kwargs)\n\n        params: Dict[str, str] = {}\n        if value.name is not None:\n            params[\"name\"] = value.name\n        if value.filename is not None:\n            params[\"filename\"] = value.filename\n\n        if params:\n            self.set_content_disposition(\"attachment\", True, **params)\n\n    async def write(self, writer: Any) -> None:\n        field = self._value\n        chunk = await field.read_chunk(size=2**16)\n        while chunk:\n            await writer.write(field.decode(chunk))\n            chunk = await field.read_chunk(size=2**16)\n\n\nclass MultipartReader:\n    \"\"\"Multipart body reader.\"\"\"\n\n    #: Response wrapper, used when multipart readers constructs from response.\n    response_wrapper_cls = MultipartResponseWrapper\n    #: Multipart reader class, used to handle multipart/* body parts.\n    #: None points to type(self)\n    multipart_reader_cls = None\n    #: Body part reader class for non multipart/* content types.\n    part_reader_cls = BodyPartReader\n\n    def __init__(self, headers: Mapping[str, str], content: StreamReader) -> None:\n        self._mimetype = parse_mimetype(headers[CONTENT_TYPE])\n        assert self._mimetype.type == \"multipart\", \"multipart/* content type expected\"\n        if \"boundary\" not in self._mimetype.parameters:\n            raise ValueError(\n                \"boundary missed for Content-Type: %s\" % headers[CONTENT_TYPE]\n            )\n\n        self.headers = headers\n        self._boundary = (\"--\" + self._get_boundary()).encode()\n        self._content = content\n        self._default_charset: Optional[str] = None\n        self._last_part: Optional[Union[\"MultipartReader\", BodyPartReader]] = None\n        self._at_eof = False\n        self._at_bof = True\n        self._unread: List[bytes] = []\n\n    def __aiter__(\n        self,\n    ) -> AsyncIterator[\"BodyPartReader\"]:\n        return self  # type: ignore[return-value]\n\n    async def __anext__(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", BodyPartReader]]:\n        part = await self.next()\n        if part is None:\n            raise StopAsyncIteration\n        return part\n\n    @classmethod\n    def from_response(\n        cls,\n        response: \"ClientResponse\",\n    ) -> MultipartResponseWrapper:\n        \"\"\"Constructs reader instance from HTTP response.\n\n        :param response: :class:`~aiohttp.client.ClientResponse` instance\n        \"\"\"\n        obj = cls.response_wrapper_cls(\n            response, cls(response.headers, response.content)\n        )\n        return obj\n\n    def at_eof(self) -> bool:\n        \"\"\"Returns True if the final boundary was reached, false otherwise.\"\"\"\n        return self._at_eof\n\n    async def next(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", BodyPartReader]]:\n        \"\"\"Emits the next multipart body part.\"\"\"\n        # So, if we're at BOF, we need to skip till the boundary.\n        if self._at_eof:\n            return None\n        await self._maybe_release_last_part()\n        if self._at_bof:\n            await self._read_until_first_boundary()\n            self._at_bof = False\n        else:\n            await self._read_boundary()\n        if self._at_eof:  # we just read the last boundary, nothing to do there\n            return None\n\n        part = await self.fetch_next_part()\n        # https://datatracker.ietf.org/doc/html/rfc7578#section-4.6\n        if (\n            self._last_part is None\n            and self._mimetype.subtype == \"form-data\"\n            and isinstance(part, BodyPartReader)\n        ):\n            _, params = parse_content_disposition(part.headers.get(CONTENT_DISPOSITION))\n            if params.get(\"name\") == \"_charset_\":\n                # Longest encoding in https://encoding.spec.whatwg.org/encodings.json\n                # is 19 characters, so 32 should be more than enough for any valid encoding.\n                charset = await part.read_chunk(32)\n                if len(charset) > 31:\n                    raise RuntimeError(\"Invalid default charset\")\n                self._default_charset = charset.strip().decode()\n                part = await self.fetch_next_part()\n        self._last_part = part\n        return self._last_part\n\n    async def release(self) -> None:\n        \"\"\"Reads all the body parts to the void till the final boundary.\"\"\"\n        while not self._at_eof:\n            item = await self.next()\n            if item is None:\n                break\n            await item.release()\n\n    async def fetch_next_part(\n        self,\n    ) -> Union[\"MultipartReader\", BodyPartReader]:\n        \"\"\"Returns the next body part reader.\"\"\"\n        headers = await self._read_headers()\n        return self._get_part_reader(headers)\n\n    def _get_part_reader(\n        self,\n        headers: \"CIMultiDictProxy[str]\",\n    ) -> Union[\"MultipartReader\", BodyPartReader]:\n        \"\"\"Dispatches the response by the `Content-Type` header.\n\n        Returns a suitable reader instance.\n\n        :param dict headers: Response headers\n        \"\"\"\n        ctype = headers.get(CONTENT_TYPE, \"\")\n        mimetype = parse_mimetype(ctype)\n\n        if mimetype.type == \"multipart\":\n            if self.multipart_reader_cls is None:\n                return type(self)(headers, self._content)\n            return self.multipart_reader_cls(headers, self._content)\n        else:\n            return self.part_reader_cls(\n                self._boundary,\n                headers,\n                self._content,\n                subtype=self._mimetype.subtype,\n                default_charset=self._default_charset,\n            )\n\n    def _get_boundary(self) -> str:\n        boundary = self._mimetype.parameters[\"boundary\"]\n        if len(boundary) > 70:\n            raise ValueError(\"boundary %r is too long (70 chars max)\" % boundary)\n\n        return boundary\n\n    async def _readline(self) -> bytes:\n        if self._unread:\n            return self._unread.pop()\n        return await self._content.readline()\n\n    async def _read_until_first_boundary(self) -> None:\n        while True:\n            chunk = await self._readline()\n            if chunk == b\"\":\n                raise ValueError(f\"Could not find starting boundary {self._boundary!r}\")\n            chunk = chunk.rstrip()\n            if chunk == self._boundary:\n                return\n            elif chunk == self._boundary + b\"--\":\n                self._at_eof = True\n                return\n\n    async def _read_boundary(self) -> None:\n        chunk = (await self._readline()).rstrip()\n        if chunk == self._boundary:\n            pass\n        elif chunk == self._boundary + b\"--\":\n            self._at_eof = True\n            epilogue = await self._readline()\n            next_line = await self._readline()\n\n            # the epilogue is expected and then either the end of input or the\n            # parent multipart boundary, if the parent boundary is found then\n            # it should be marked as unread and handed to the parent for\n            # processing\n            if next_line[:2] == b\"--\":\n                self._unread.append(next_line)\n            # otherwise the request is likely missing an epilogue and both\n            # lines should be passed to the parent for processing\n            # (this handles the old behavior gracefully)\n            else:\n                self._unread.extend([next_line, epilogue])\n        else:\n            raise ValueError(f\"Invalid boundary {chunk!r}, expected {self._boundary!r}\")\n\n    async def _read_headers(self) -> \"CIMultiDictProxy[str]\":\n        lines = [b\"\"]\n        while True:\n            chunk = await self._content.readline()\n            chunk = chunk.strip()\n            lines.append(chunk)\n            if not chunk:\n                break\n        parser = HeadersParser()\n        headers, raw_headers = parser.parse_headers(lines)\n        return headers\n\n    async def _maybe_release_last_part(self) -> None:\n        \"\"\"Ensures that the last read body part is read completely.\"\"\"\n        if self._last_part is not None:\n            if not self._last_part.at_eof():\n                await self._last_part.release()\n            self._unread.extend(self._last_part._unread)\n            self._last_part = None\n\n\n_Part = Tuple[Payload, str, str]\n\n\nclass MultipartWriter(Payload):\n    \"\"\"Multipart body writer.\"\"\"\n\n    def __init__(self, subtype: str = \"mixed\", boundary: Optional[str] = None) -> None:\n        boundary = boundary if boundary is not None else uuid.uuid4().hex\n        # The underlying Payload API demands a str (utf-8), not bytes,\n        # so we need to ensure we don't lose anything during conversion.\n        # As a result, require the boundary to be ASCII only.\n        # In both situations.\n\n        try:\n            self._boundary = boundary.encode(\"ascii\")\n        except UnicodeEncodeError:\n            raise ValueError(\"boundary should contain ASCII only chars\") from None\n\n        if len(boundary) > 70:\n            raise ValueError(\"boundary %r is too long (70 chars max)\" % boundary)\n\n        ctype = f\"multipart/{subtype}; boundary={self._boundary_value}\"\n\n        super().__init__(None, content_type=ctype)\n\n        self._parts: List[_Part] = []\n        self._is_form_data = subtype == \"form-data\"\n\n    def __enter__(self) -> \"MultipartWriter\":\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def __iter__(self) -> Iterator[_Part]:\n        return iter(self._parts)\n\n    def __len__(self) -> int:\n        return len(self._parts)\n\n    def __bool__(self) -> bool:\n        return True\n\n    _valid_tchar_regex = re.compile(rb\"\\A[!#$%&'*+\\-.^_`|~\\w]+\\Z\")\n    _invalid_qdtext_char_regex = re.compile(rb\"[\\x00-\\x08\\x0A-\\x1F\\x7F]\")\n\n    @property\n    def _boundary_value(self) -> str:\n        \"\"\"Wrap boundary parameter value in quotes, if necessary.\n\n        Reads self.boundary and returns a unicode string.\n        \"\"\"\n        # Refer to RFCs 7231, 7230, 5234.\n        #\n        # parameter      = token \"=\" ( token / quoted-string )\n        # token          = 1*tchar\n        # quoted-string  = DQUOTE *( qdtext / quoted-pair ) DQUOTE\n        # qdtext         = HTAB / SP / %x21 / %x23-5B / %x5D-7E / obs-text\n        # obs-text       = %x80-FF\n        # quoted-pair    = \"\\\" ( HTAB / SP / VCHAR / obs-text )\n        # tchar          = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\"\n        #                  / \"+\" / \"-\" / \".\" / \"^\" / \"_\" / \"`\" / \"|\" / \"~\"\n        #                  / DIGIT / ALPHA\n        #                  ; any VCHAR, except delimiters\n        # VCHAR           = %x21-7E\n        value = self._boundary\n        if re.match(self._valid_tchar_regex, value):\n            return value.decode(\"ascii\")  # cannot fail\n\n        if re.search(self._invalid_qdtext_char_regex, value):\n            raise ValueError(\"boundary value contains invalid characters\")\n\n        # escape %x5C and %x22\n        quoted_value_content = value.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted_value_content = quoted_value_content.replace(b'\"', b'\\\\\"')\n\n        return '\"' + quoted_value_content.decode(\"ascii\") + '\"'\n\n    @property\n    def boundary(self) -> str:\n        return self._boundary.decode(\"ascii\")\n\n    def append(self, obj: Any, headers: Optional[Mapping[str, str]] = None) -> Payload:\n        if headers is None:\n            headers = CIMultiDict()\n\n        if isinstance(obj, Payload):\n            obj.headers.update(headers)\n            return self.append_payload(obj)\n        else:\n            try:\n                payload = get_payload(obj, headers=headers)\n            except LookupError:\n                raise TypeError(\"Cannot create payload from %r\" % obj)\n            else:\n                return self.append_payload(payload)\n\n    def append_payload(self, payload: Payload) -> Payload:\n        \"\"\"Adds a new body part to multipart writer.\"\"\"\n        encoding: Optional[str] = None\n        te_encoding: Optional[str] = None\n        if self._is_form_data:\n            # https://datatracker.ietf.org/doc/html/rfc7578#section-4.7\n            # https://datatracker.ietf.org/doc/html/rfc7578#section-4.8\n            assert (\n                not {CONTENT_ENCODING, CONTENT_LENGTH, CONTENT_TRANSFER_ENCODING}\n                & payload.headers.keys()\n            )\n            # Set default Content-Disposition in case user doesn't create one\n            if CONTENT_DISPOSITION not in payload.headers:\n                name = f\"section-{len(self._parts)}\"\n                payload.set_content_disposition(\"form-data\", name=name)\n        else:\n            # compression\n            encoding = payload.headers.get(CONTENT_ENCODING, \"\").lower()\n            if encoding and encoding not in (\"deflate\", \"gzip\", \"identity\"):\n                raise RuntimeError(f\"unknown content encoding: {encoding}\")\n            if encoding == \"identity\":\n                encoding = None\n\n            # te encoding\n            te_encoding = payload.headers.get(CONTENT_TRANSFER_ENCODING, \"\").lower()\n            if te_encoding not in (\"\", \"base64\", \"quoted-printable\", \"binary\"):\n                raise RuntimeError(f\"unknown content transfer encoding: {te_encoding}\")\n            if te_encoding == \"binary\":\n                te_encoding = None\n\n            # size\n            size = payload.size\n            if size is not None and not (encoding or te_encoding):\n                payload.headers[CONTENT_LENGTH] = str(size)\n\n        self._parts.append((payload, encoding, te_encoding))  # type: ignore[arg-type]\n        return payload\n\n    def append_json(\n        self, obj: Any, headers: Optional[Mapping[str, str]] = None\n    ) -> Payload:\n        \"\"\"Helper to append JSON part.\"\"\"\n        if headers is None:\n            headers = CIMultiDict()\n\n        return self.append_payload(JsonPayload(obj, headers=headers))\n\n    def append_form(\n        self,\n        obj: Union[Sequence[Tuple[str, str]], Mapping[str, str]],\n        headers: Optional[Mapping[str, str]] = None,\n    ) -> Payload:\n        \"\"\"Helper to append form urlencoded part.\"\"\"\n        assert isinstance(obj, (Sequence, Mapping))\n\n        if headers is None:\n            headers = CIMultiDict()\n\n        if isinstance(obj, Mapping):\n            obj = list(obj.items())\n        data = urlencode(obj, doseq=True)\n\n        return self.append_payload(\n            StringPayload(\n                data, headers=headers, content_type=\"application/x-www-form-urlencoded\"\n            )\n        )\n\n    @property\n    def size(self) -> Optional[int]:\n        \"\"\"Size of the payload.\"\"\"\n        total = 0\n        for part, encoding, te_encoding in self._parts:\n            if encoding or te_encoding or part.size is None:\n                return None\n\n            total += int(\n                2\n                + len(self._boundary)\n                + 2\n                + part.size  # b'--'+self._boundary+b'\\r\\n'\n                + len(part._binary_headers)\n                + 2  # b'\\r\\n'\n            )\n\n        total += 2 + len(self._boundary) + 4  # b'--'+self._boundary+b'--\\r\\n'\n        return total\n\n    async def write(self, writer: Any, close_boundary: bool = True) -> None:\n        \"\"\"Write body.\"\"\"\n        for part, encoding, te_encoding in self._parts:\n            if self._is_form_data:\n                # https://datatracker.ietf.org/doc/html/rfc7578#section-4.2\n                assert CONTENT_DISPOSITION in part.headers\n                assert \"name=\" in part.headers[CONTENT_DISPOSITION]\n\n            await writer.write(b\"--\" + self._boundary + b\"\\r\\n\")\n            await writer.write(part._binary_headers)\n\n            if encoding or te_encoding:\n                w = MultipartPayloadWriter(writer)\n                if encoding:\n                    w.enable_compression(encoding)\n                if te_encoding:\n                    w.enable_encoding(te_encoding)\n                await part.write(w)  # type: ignore[arg-type]\n                await w.write_eof()\n            else:\n                await part.write(writer)\n\n            await writer.write(b\"\\r\\n\")\n\n        if close_boundary:\n            await writer.write(b\"--\" + self._boundary + b\"--\\r\\n\")\n\n\nclass MultipartPayloadWriter:\n    def __init__(self, writer: Any) -> None:\n        self._writer = writer\n        self._encoding: Optional[str] = None\n        self._compress: Optional[ZLibCompressor] = None\n        self._encoding_buffer: Optional[bytearray] = None\n\n    def enable_encoding(self, encoding: str) -> None:\n        if encoding == \"base64\":\n            self._encoding = encoding\n            self._encoding_buffer = bytearray()\n        elif encoding == \"quoted-printable\":\n            self._encoding = \"quoted-printable\"\n\n    def enable_compression(\n        self, encoding: str = \"deflate\", strategy: int = zlib.Z_DEFAULT_STRATEGY\n    ) -> None:\n        self._compress = ZLibCompressor(\n            encoding=encoding,\n            suppress_deflate_header=True,\n            strategy=strategy,\n        )\n\n    async def write_eof(self) -> None:\n        if self._compress is not None:\n            chunk = self._compress.flush()\n            if chunk:\n                self._compress = None\n                await self.write(chunk)\n\n        if self._encoding == \"base64\":\n            if self._encoding_buffer:\n                await self._writer.write(base64.b64encode(self._encoding_buffer))\n\n    async def write(self, chunk: bytes) -> None:\n        if self._compress is not None:\n            if chunk:\n                chunk = await self._compress.compress(chunk)\n                if not chunk:\n                    return\n\n        if self._encoding == \"base64\":\n            buf = self._encoding_buffer\n            assert buf is not None\n            buf.extend(chunk)\n\n            if buf:\n                div, mod = divmod(len(buf), 3)\n                enc_chunk, self._encoding_buffer = (buf[: div * 3], buf[div * 3 :])\n                if enc_chunk:\n                    b64chunk = base64.b64encode(enc_chunk)\n                    await self._writer.write(b64chunk)\n        elif self._encoding == \"quoted-printable\":\n            await self._writer.write(binascii.b2a_qp(chunk))\n        else:\n            await self._writer.write(chunk)\n", "aiohttp/http.py": "import sys\n\nfrom . import __version__\nfrom .http_exceptions import HttpProcessingError\nfrom .http_parser import (\n    HeadersParser,\n    HttpParser,\n    HttpRequestParser,\n    HttpResponseParser,\n    RawRequestMessage,\n    RawResponseMessage,\n)\nfrom .http_websocket import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WS_KEY,\n    WebSocketError,\n    WebSocketReader,\n    WebSocketWriter,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n    ws_ext_gen,\n    ws_ext_parse,\n)\nfrom .http_writer import HttpVersion, HttpVersion10, HttpVersion11, StreamWriter\n\n__all__ = (\n    \"HttpProcessingError\",\n    \"SERVER_SOFTWARE\",\n    # .http_writer\n    \"StreamWriter\",\n    \"HttpVersion\",\n    \"HttpVersion10\",\n    \"HttpVersion11\",\n    # .http_parser\n    \"HeadersParser\",\n    \"HttpParser\",\n    \"HttpRequestParser\",\n    \"HttpResponseParser\",\n    \"RawRequestMessage\",\n    \"RawResponseMessage\",\n    # .http_websocket\n    \"WS_CLOSED_MESSAGE\",\n    \"WS_CLOSING_MESSAGE\",\n    \"WS_KEY\",\n    \"WebSocketReader\",\n    \"WebSocketWriter\",\n    \"ws_ext_gen\",\n    \"ws_ext_parse\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n)\n\n\nSERVER_SOFTWARE: str = \"Python/{0[0]}.{0[1]} aiohttp/{1}\".format(\n    sys.version_info, __version__\n)\n", "aiohttp/client_reqrep.py": "import asyncio\nimport codecs\nimport contextlib\nimport dataclasses\nimport functools\nimport io\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom hashlib import md5, sha1, sha256\nfrom http.cookies import CookieError, Morsel, SimpleCookie\nfrom types import MappingProxyType, TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy\nfrom yarl import URL\n\nfrom . import hdrs, helpers, http, multipart, payload\nfrom .abc import AbstractStreamWriter\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientOSError,\n    ClientResponseError,\n    ContentTypeError,\n    InvalidURL,\n    ServerFingerprintMismatch,\n)\nfrom .compression_utils import HAS_BROTLI\nfrom .formdata import FormData\nfrom .hdrs import CONTENT_TYPE\nfrom .helpers import (\n    BaseTimerContext,\n    BasicAuth,\n    HeadersMixin,\n    TimerNoop,\n    basicauth_from_netrc,\n    is_expected_content_type,\n    netrc_from_env,\n    noop,\n    parse_mimetype,\n    reify,\n    set_exception,\n    set_result,\n)\nfrom .http import (\n    SERVER_SOFTWARE,\n    HttpVersion,\n    HttpVersion10,\n    HttpVersion11,\n    StreamWriter,\n)\nfrom .log import client_logger\nfrom .streams import StreamReader\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    JSONDecoder,\n    LooseCookies,\n    LooseHeaders,\n    RawHeaders,\n)\n\ntry:\n    import ssl\n    from ssl import SSLContext\nexcept ImportError:  # pragma: no cover\n    ssl = None  # type: ignore[assignment]\n    SSLContext = object  # type: ignore[misc,assignment]\n\n\n__all__ = (\"ClientRequest\", \"ClientResponse\", \"RequestInfo\", \"Fingerprint\")\n\n\nif TYPE_CHECKING:\n    from .client import ClientSession\n    from .connector import Connection\n    from .tracing import Trace\n\n\n_CONTAINS_CONTROL_CHAR_RE = re.compile(r\"[^-!#$%&'*+.^_`|~0-9a-zA-Z]\")\n\n\ndef _gen_default_accept_encoding() -> str:\n    return \"gzip, deflate, br\" if HAS_BROTLI else \"gzip, deflate\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass ContentDisposition:\n    type: Optional[str]\n    parameters: \"MappingProxyType[str, str]\"\n    filename: Optional[str]\n\n\n@dataclasses.dataclass(frozen=True)\nclass RequestInfo:\n    url: URL\n    method: str\n    headers: \"CIMultiDictProxy[str]\"\n    real_url: URL\n\n\nclass Fingerprint:\n    HASHFUNC_BY_DIGESTLEN = {\n        16: md5,\n        20: sha1,\n        32: sha256,\n    }\n\n    def __init__(self, fingerprint: bytes) -> None:\n        digestlen = len(fingerprint)\n        hashfunc = self.HASHFUNC_BY_DIGESTLEN.get(digestlen)\n        if not hashfunc:\n            raise ValueError(\"fingerprint has invalid length\")\n        elif hashfunc is md5 or hashfunc is sha1:\n            raise ValueError(\n                \"md5 and sha1 are insecure and \" \"not supported. Use sha256.\"\n            )\n        self._hashfunc = hashfunc\n        self._fingerprint = fingerprint\n\n    @property\n    def fingerprint(self) -> bytes:\n        return self._fingerprint\n\n    def check(self, transport: asyncio.Transport) -> None:\n        if not transport.get_extra_info(\"sslcontext\"):\n            return\n        sslobj = transport.get_extra_info(\"ssl_object\")\n        cert = sslobj.getpeercert(binary_form=True)\n        got = self._hashfunc(cert).digest()\n        if got != self._fingerprint:\n            host, port, *_ = transport.get_extra_info(\"peername\")\n            raise ServerFingerprintMismatch(self._fingerprint, got, host, port)\n\n\nif ssl is not None:\n    SSL_ALLOWED_TYPES = (ssl.SSLContext, bool, Fingerprint)\nelse:  # pragma: no cover\n    SSL_ALLOWED_TYPES = (bool,)\n\n\n@dataclasses.dataclass(frozen=True)\nclass ConnectionKey:\n    # the key should contain an information about used proxy / TLS\n    # to prevent reusing wrong connections from a pool\n    host: str\n    port: Optional[int]\n    is_ssl: bool\n    ssl: Union[SSLContext, bool, Fingerprint]\n    proxy: Optional[URL]\n    proxy_auth: Optional[BasicAuth]\n    proxy_headers_hash: Optional[int]  # hash(CIMultiDict)\n\n\nclass ClientRequest:\n    GET_METHODS = {\n        hdrs.METH_GET,\n        hdrs.METH_HEAD,\n        hdrs.METH_OPTIONS,\n        hdrs.METH_TRACE,\n    }\n    POST_METHODS = {hdrs.METH_PATCH, hdrs.METH_POST, hdrs.METH_PUT}\n    ALL_METHODS = GET_METHODS.union(POST_METHODS).union({hdrs.METH_DELETE})\n\n    DEFAULT_HEADERS = {\n        hdrs.ACCEPT: \"*/*\",\n        hdrs.ACCEPT_ENCODING: _gen_default_accept_encoding(),\n    }\n\n    body = b\"\"\n    auth = None\n    response = None\n\n    __writer = None  # async task for streaming data\n    _continue = None  # waiter future for '100 Continue' response\n\n    # N.B.\n    # Adding __del__ method with self._writer closing doesn't make sense\n    # because _writer is instance method, thus it keeps a reference to self.\n    # Until writer has finished finalizer will not be called.\n\n    def __init__(\n        self,\n        method: str,\n        url: URL,\n        *,\n        params: Optional[Mapping[str, str]] = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Iterable[str] = frozenset(),\n        data: Any = None,\n        cookies: Optional[LooseCookies] = None,\n        auth: Optional[BasicAuth] = None,\n        version: http.HttpVersion = http.HttpVersion11,\n        compress: Optional[str] = None,\n        chunked: Optional[bool] = None,\n        expect100: bool = False,\n        loop: asyncio.AbstractEventLoop,\n        response_class: Optional[Type[\"ClientResponse\"]] = None,\n        proxy: Optional[URL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        timer: Optional[BaseTimerContext] = None,\n        session: Optional[\"ClientSession\"] = None,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        proxy_headers: Optional[LooseHeaders] = None,\n        traces: Optional[List[\"Trace\"]] = None,\n        trust_env: bool = False,\n        server_hostname: Optional[str] = None,\n    ):\n        match = _CONTAINS_CONTROL_CHAR_RE.search(method)\n        if match:\n            raise ValueError(\n                f\"Method cannot contain non-token characters {method!r} \"\n                f\"(found at least {match.group()!r})\"\n            )\n        assert isinstance(url, URL), url\n        assert isinstance(proxy, (URL, type(None))), proxy\n        # FIXME: session is None in tests only, need to fix tests\n        # assert session is not None\n        self._session = cast(\"ClientSession\", session)\n        if params:\n            q = MultiDict(url.query)\n            url2 = url.with_query(params)\n            q.extend(url2.query)\n            url = url.with_query(q)\n        self.original_url = url\n        self.url = url.with_fragment(None)\n        self.method = method.upper()\n        self.chunked = chunked\n        self.compress = compress\n        self.loop = loop\n        self.length = None\n        if response_class is None:\n            real_response_class = ClientResponse\n        else:\n            real_response_class = response_class\n        self.response_class: Type[ClientResponse] = real_response_class\n        self._timer = timer if timer is not None else TimerNoop()\n        self._ssl = ssl\n        self.server_hostname = server_hostname\n\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n        self.update_version(version)\n        self.update_host(url)\n        self.update_headers(headers)\n        self.update_auto_headers(skip_auto_headers)\n        self.update_cookies(cookies)\n        self.update_content_encoding(data)\n        self.update_auth(auth, trust_env)\n        self.update_proxy(proxy, proxy_auth, proxy_headers)\n\n        self.update_body_from_data(data)\n        if data is not None or self.method not in self.GET_METHODS:\n            self.update_transfer_encoding()\n        self.update_expect_continue(expect100)\n        if traces is None:\n            traces = []\n        self._traces = traces\n\n    def __reset_writer(self, _: object = None) -> None:\n        self.__writer = None\n\n    @property\n    def _writer(self) -> Optional[\"asyncio.Task[None]\"]:\n        return self.__writer\n\n    @_writer.setter\n    def _writer(self, writer: Optional[\"asyncio.Task[None]\"]) -> None:\n        if self.__writer is not None:\n            self.__writer.remove_done_callback(self.__reset_writer)\n        self.__writer = writer\n        if writer is not None:\n            writer.add_done_callback(self.__reset_writer)\n\n    def is_ssl(self) -> bool:\n        return self.url.scheme in (\"https\", \"wss\")\n\n    @property\n    def ssl(self) -> Union[\"SSLContext\", bool, Fingerprint]:\n        return self._ssl\n\n    @property\n    def connection_key(self) -> ConnectionKey:\n        proxy_headers = self.proxy_headers\n        if proxy_headers:\n            h: Optional[int] = hash(tuple((k, v) for k, v in proxy_headers.items()))\n        else:\n            h = None\n        return ConnectionKey(\n            self.host,\n            self.port,\n            self.is_ssl(),\n            self.ssl,\n            self.proxy,\n            self.proxy_auth,\n            h,\n        )\n\n    @property\n    def host(self) -> str:\n        ret = self.url.raw_host\n        assert ret is not None\n        return ret\n\n    @property\n    def port(self) -> Optional[int]:\n        return self.url.port\n\n    @property\n    def request_info(self) -> RequestInfo:\n        headers: CIMultiDictProxy[str] = CIMultiDictProxy(self.headers)\n        return RequestInfo(self.url, self.method, headers, self.original_url)\n\n    def update_host(self, url: URL) -> None:\n        \"\"\"Update destination host, port and connection type (ssl).\"\"\"\n        # get host/port\n        if not url.raw_host:\n            raise InvalidURL(url)\n\n        # basic auth info\n        username, password = url.user, url.password\n        if username:\n            self.auth = helpers.BasicAuth(username, password or \"\")\n\n    def update_version(self, version: Union[http.HttpVersion, str]) -> None:\n        \"\"\"Convert request version to two elements tuple.\n\n        parser HTTP version '1.1' => (1, 1)\n        \"\"\"\n        if isinstance(version, str):\n            v = [part.strip() for part in version.split(\".\", 1)]\n            try:\n                version = http.HttpVersion(int(v[0]), int(v[1]))\n            except ValueError:\n                raise ValueError(\n                    f\"Can not parse http version number: {version}\"\n                ) from None\n        self.version = version\n\n    def update_headers(self, headers: Optional[LooseHeaders]) -> None:\n        \"\"\"Update request headers.\"\"\"\n        self.headers: CIMultiDict[str] = CIMultiDict()\n\n        # add host\n        netloc = cast(str, self.url.raw_host)\n        if helpers.is_ipv6_address(netloc):\n            netloc = f\"[{netloc}]\"\n        # See https://github.com/aio-libs/aiohttp/issues/3636.\n        netloc = netloc.rstrip(\".\")\n        if self.url.port is not None and not self.url.is_default_port():\n            netloc += \":\" + str(self.url.port)\n        self.headers[hdrs.HOST] = netloc\n\n        if headers:\n            if isinstance(headers, (dict, MultiDictProxy, MultiDict)):\n                headers = headers.items()  # type: ignore[assignment]\n\n            for key, value in headers:  # type: ignore[misc]\n                # A special case for Host header\n                if key.lower() == \"host\":\n                    self.headers[key] = value\n                else:\n                    self.headers.add(key, value)\n\n    def update_auto_headers(self, skip_auto_headers: Iterable[str]) -> None:\n        self.skip_auto_headers = CIMultiDict(\n            (hdr, None) for hdr in sorted(skip_auto_headers)\n        )\n        used_headers = self.headers.copy()\n        used_headers.extend(self.skip_auto_headers)  # type: ignore[arg-type]\n\n        for hdr, val in self.DEFAULT_HEADERS.items():\n            if hdr not in used_headers:\n                self.headers.add(hdr, val)\n\n        if hdrs.USER_AGENT not in used_headers:\n            self.headers[hdrs.USER_AGENT] = SERVER_SOFTWARE\n\n    def update_cookies(self, cookies: Optional[LooseCookies]) -> None:\n        \"\"\"Update request cookies header.\"\"\"\n        if not cookies:\n            return\n\n        c = SimpleCookie()\n        if hdrs.COOKIE in self.headers:\n            c.load(self.headers.get(hdrs.COOKIE, \"\"))\n            del self.headers[hdrs.COOKIE]\n\n        if isinstance(cookies, Mapping):\n            iter_cookies = cookies.items()\n        else:\n            iter_cookies = cookies  # type: ignore[assignment]\n        for name, value in iter_cookies:\n            if isinstance(value, Morsel):\n                # Preserve coded_value\n                mrsl_val = value.get(value.key, Morsel())\n                mrsl_val.set(value.key, value.value, value.coded_value)\n                c[name] = mrsl_val\n            else:\n                c[name] = value  # type: ignore[assignment]\n\n        self.headers[hdrs.COOKIE] = c.output(header=\"\", sep=\";\").strip()\n\n    def update_content_encoding(self, data: Any) -> None:\n        \"\"\"Set request content encoding.\"\"\"\n        if data is None:\n            return\n\n        enc = self.headers.get(hdrs.CONTENT_ENCODING, \"\").lower()\n        if enc:\n            if self.compress:\n                raise ValueError(\n                    \"compress can not be set \" \"if Content-Encoding header is set\"\n                )\n        elif self.compress:\n            if not isinstance(self.compress, str):\n                self.compress = \"deflate\"\n            self.headers[hdrs.CONTENT_ENCODING] = self.compress\n            self.chunked = True  # enable chunked, no need to deal with length\n\n    def update_transfer_encoding(self) -> None:\n        \"\"\"Analyze transfer-encoding header.\"\"\"\n        te = self.headers.get(hdrs.TRANSFER_ENCODING, \"\").lower()\n\n        if \"chunked\" in te:\n            if self.chunked:\n                raise ValueError(\n                    \"chunked can not be set \"\n                    'if \"Transfer-Encoding: chunked\" header is set'\n                )\n\n        elif self.chunked:\n            if hdrs.CONTENT_LENGTH in self.headers:\n                raise ValueError(\n                    \"chunked can not be set \" \"if Content-Length header is set\"\n                )\n\n            self.headers[hdrs.TRANSFER_ENCODING] = \"chunked\"\n        else:\n            if hdrs.CONTENT_LENGTH not in self.headers:\n                self.headers[hdrs.CONTENT_LENGTH] = str(len(self.body))\n\n    def update_auth(self, auth: Optional[BasicAuth], trust_env: bool = False) -> None:\n        \"\"\"Set basic auth.\"\"\"\n        if auth is None:\n            auth = self.auth\n        if auth is None and trust_env and self.url.host is not None:\n            netrc_obj = netrc_from_env()\n            with contextlib.suppress(LookupError):\n                auth = basicauth_from_netrc(netrc_obj, self.url.host)\n        if auth is None:\n            return\n\n        if not isinstance(auth, helpers.BasicAuth):\n            raise TypeError(\"BasicAuth() tuple is required instead\")\n\n        self.headers[hdrs.AUTHORIZATION] = auth.encode()\n\n    def update_body_from_data(self, body: Any) -> None:\n        if body is None:\n            return\n\n        # FormData\n        if isinstance(body, FormData):\n            body = body()\n\n        try:\n            body = payload.PAYLOAD_REGISTRY.get(body, disposition=None)\n        except payload.LookupError:\n            boundary = None\n            if CONTENT_TYPE in self.headers:\n                boundary = parse_mimetype(self.headers[CONTENT_TYPE]).parameters.get(\n                    \"boundary\"\n                )\n            body = FormData(body, boundary=boundary)()\n\n        self.body = body\n\n        # enable chunked encoding if needed\n        if not self.chunked:\n            if hdrs.CONTENT_LENGTH not in self.headers:\n                size = body.size\n                if size is None:\n                    self.chunked = True\n                else:\n                    if hdrs.CONTENT_LENGTH not in self.headers:\n                        self.headers[hdrs.CONTENT_LENGTH] = str(size)\n\n        # copy payload headers\n        assert body.headers\n        for key, value in body.headers.items():\n            if key in self.headers:\n                continue\n            if key in self.skip_auto_headers:\n                continue\n            self.headers[key] = value\n\n    def update_expect_continue(self, expect: bool = False) -> None:\n        if expect:\n            self.headers[hdrs.EXPECT] = \"100-continue\"\n        elif self.headers.get(hdrs.EXPECT, \"\").lower() == \"100-continue\":\n            expect = True\n\n        if expect:\n            self._continue = self.loop.create_future()\n\n    def update_proxy(\n        self,\n        proxy: Optional[URL],\n        proxy_auth: Optional[BasicAuth],\n        proxy_headers: Optional[LooseHeaders],\n    ) -> None:\n        if proxy_auth and not isinstance(proxy_auth, helpers.BasicAuth):\n            raise ValueError(\"proxy_auth must be None or BasicAuth() tuple\")\n        self.proxy = proxy\n        self.proxy_auth = proxy_auth\n        self.proxy_headers = proxy_headers\n\n    def keep_alive(self) -> bool:\n        if self.version < HttpVersion10:\n            # keep alive not supported at all\n            return False\n        if self.version == HttpVersion10:\n            if self.headers.get(hdrs.CONNECTION) == \"keep-alive\":\n                return True\n            else:  # no headers means we close for Http 1.0\n                return False\n        elif self.headers.get(hdrs.CONNECTION) == \"close\":\n            return False\n\n        return True\n\n    async def write_bytes(\n        self, writer: AbstractStreamWriter, conn: \"Connection\"\n    ) -> None:\n        \"\"\"Support coroutines that yields bytes objects.\"\"\"\n        # 100 response\n        if self._continue is not None:\n            try:\n                await writer.drain()\n                await self._continue\n            except asyncio.CancelledError:\n                return\n\n        protocol = conn.protocol\n        assert protocol is not None\n        try:\n            if isinstance(self.body, payload.Payload):\n                await self.body.write(writer)\n            else:\n                if isinstance(self.body, (bytes, bytearray)):\n                    self.body = (self.body,)  # type: ignore[assignment]\n\n                for chunk in self.body:\n                    await writer.write(chunk)  # type: ignore[arg-type]\n        except OSError as underlying_exc:\n            reraised_exc = underlying_exc\n\n            exc_is_not_timeout = underlying_exc.errno is not None or not isinstance(\n                underlying_exc, asyncio.TimeoutError\n            )\n            if exc_is_not_timeout:\n                reraised_exc = ClientOSError(\n                    underlying_exc.errno,\n                    f\"Can not write request body for {self.url !s}\",\n                )\n\n            set_exception(protocol, reraised_exc, underlying_exc)\n        except asyncio.CancelledError:\n            await writer.write_eof()\n        except Exception as underlying_exc:\n            set_exception(\n                protocol,\n                ClientConnectionError(\n                    f\"Failed to send bytes into the underlying connection {conn !s}\",\n                ),\n                underlying_exc,\n            )\n        else:\n            await writer.write_eof()\n            protocol.start_timeout()\n\n    async def send(self, conn: \"Connection\") -> \"ClientResponse\":\n        # Specify request target:\n        # - CONNECT request must send authority form URI\n        # - not CONNECT proxy must send absolute form URI\n        # - most common is origin form URI\n        if self.method == hdrs.METH_CONNECT:\n            connect_host = self.url.raw_host\n            assert connect_host is not None\n            if helpers.is_ipv6_address(connect_host):\n                connect_host = f\"[{connect_host}]\"\n            path = f\"{connect_host}:{self.url.port}\"\n        elif self.proxy and not self.is_ssl():\n            path = str(self.url)\n        else:\n            path = self.url.raw_path\n            if self.url.raw_query_string:\n                path += \"?\" + self.url.raw_query_string\n\n        protocol = conn.protocol\n        assert protocol is not None\n        writer = StreamWriter(\n            protocol,\n            self.loop,\n            on_chunk_sent=functools.partial(\n                self._on_chunk_request_sent, self.method, self.url\n            ),\n            on_headers_sent=functools.partial(\n                self._on_headers_request_sent, self.method, self.url\n            ),\n        )\n\n        if self.compress:\n            writer.enable_compression(self.compress)\n\n        if self.chunked is not None:\n            writer.enable_chunking()\n\n        # set default content-type\n        if (\n            self.method in self.POST_METHODS\n            and hdrs.CONTENT_TYPE not in self.skip_auto_headers\n            and hdrs.CONTENT_TYPE not in self.headers\n        ):\n            self.headers[hdrs.CONTENT_TYPE] = \"application/octet-stream\"\n\n        # set the connection header\n        connection = self.headers.get(hdrs.CONNECTION)\n        if not connection:\n            if self.keep_alive():\n                if self.version == HttpVersion10:\n                    connection = \"keep-alive\"\n            else:\n                if self.version == HttpVersion11:\n                    connection = \"close\"\n\n        if connection is not None:\n            self.headers[hdrs.CONNECTION] = connection\n\n        # status + headers\n        status_line = \"{0} {1} HTTP/{v.major}.{v.minor}\".format(\n            self.method, path, v=self.version\n        )\n        await writer.write_headers(status_line, self.headers)\n\n        self._writer = self.loop.create_task(self.write_bytes(writer, conn))\n\n        response_class = self.response_class\n        assert response_class is not None\n        self.response = response_class(\n            self.method,\n            self.original_url,\n            writer=self._writer,\n            continue100=self._continue,\n            timer=self._timer,\n            request_info=self.request_info,\n            traces=self._traces,\n            loop=self.loop,\n            session=self._session,\n        )\n        return self.response\n\n    async def close(self) -> None:\n        if self._writer is not None:\n            with contextlib.suppress(asyncio.CancelledError):\n                await self._writer\n\n    def terminate(self) -> None:\n        if self._writer is not None:\n            if not self.loop.is_closed():\n                self._writer.cancel()\n            self._writer.remove_done_callback(self.__reset_writer)\n            self._writer = None\n\n    async def _on_chunk_request_sent(self, method: str, url: URL, chunk: bytes) -> None:\n        for trace in self._traces:\n            await trace.send_request_chunk_sent(method, url, chunk)\n\n    async def _on_headers_request_sent(\n        self, method: str, url: URL, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        for trace in self._traces:\n            await trace.send_request_headers(method, url, headers)\n\n\nclass ClientResponse(HeadersMixin):\n    # Some of these attributes are None when created,\n    # but will be set by the start() method.\n    # As the end user will likely never see the None values, we cheat the types below.\n    # from the Status-Line of the response\n    version: Optional[HttpVersion] = None  # HTTP-Version\n    status: int = None  # type: ignore[assignment] # Status-Code\n    reason: Optional[str] = None  # Reason-Phrase\n\n    content: StreamReader = None  # type: ignore[assignment] # Payload stream\n    _headers: CIMultiDictProxy[str] = None  # type: ignore[assignment]\n    _raw_headers: RawHeaders = None  # type: ignore[assignment]\n\n    _connection = None  # current connection\n    _source_traceback: Optional[traceback.StackSummary] = None\n    # set up by ClientRequest after ClientResponse object creation\n    # post-init stage allows to not change ctor signature\n    _closed = True  # to allow __del__ for non-initialized properly response\n    _released = False\n    __writer = None\n\n    def __init__(\n        self,\n        method: str,\n        url: URL,\n        *,\n        writer: \"asyncio.Task[None]\",\n        continue100: Optional[\"asyncio.Future[bool]\"],\n        timer: Optional[BaseTimerContext],\n        request_info: RequestInfo,\n        traces: List[\"Trace\"],\n        loop: asyncio.AbstractEventLoop,\n        session: \"ClientSession\",\n    ) -> None:\n        assert isinstance(url, URL)\n        super().__init__()\n\n        self.method = method\n        self.cookies = SimpleCookie()\n\n        self._real_url = url\n        self._url = url.with_fragment(None)\n        self._body: Optional[bytes] = None\n        self._writer: Optional[asyncio.Task[None]] = writer\n        self._continue = continue100  # None by default\n        self._closed = True\n        self._history: Tuple[ClientResponse, ...] = ()\n        self._request_info = request_info\n        self._timer = timer if timer is not None else TimerNoop()\n        self._cache: Dict[str, Any] = {}\n        self._traces = traces\n        self._loop = loop\n        # store a reference to session #1985\n        self._session: Optional[ClientSession] = session\n        # Save reference to _resolve_charset, so that get_encoding() will still\n        # work after the response has finished reading the body.\n        if session is None:\n            # TODO: Fix session=None in tests (see ClientRequest.__init__).\n            self._resolve_charset: Callable[[\"ClientResponse\", bytes], str] = (\n                lambda *_: \"utf-8\"\n            )\n        else:\n            self._resolve_charset = session._resolve_charset\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n    def __reset_writer(self, _: object = None) -> None:\n        self.__writer = None\n\n    @property\n    def _writer(self) -> Optional[\"asyncio.Task[None]\"]:\n        return self.__writer\n\n    @_writer.setter\n    def _writer(self, writer: Optional[\"asyncio.Task[None]\"]) -> None:\n        if self.__writer is not None:\n            self.__writer.remove_done_callback(self.__reset_writer)\n        self.__writer = writer\n        if writer is not None:\n            writer.add_done_callback(self.__reset_writer)\n\n    @reify\n    def url(self) -> URL:\n        return self._url\n\n    @reify\n    def real_url(self) -> URL:\n        return self._real_url\n\n    @reify\n    def host(self) -> str:\n        assert self._url.host is not None\n        return self._url.host\n\n    @reify\n    def headers(self) -> \"CIMultiDictProxy[str]\":\n        return self._headers\n\n    @reify\n    def raw_headers(self) -> RawHeaders:\n        return self._raw_headers\n\n    @reify\n    def request_info(self) -> RequestInfo:\n        return self._request_info\n\n    @reify\n    def content_disposition(self) -> Optional[ContentDisposition]:\n        raw = self._headers.get(hdrs.CONTENT_DISPOSITION)\n        if raw is None:\n            return None\n        disposition_type, params_dct = multipart.parse_content_disposition(raw)\n        params = MappingProxyType(params_dct)\n        filename = multipart.content_disposition_filename(params)\n        return ContentDisposition(disposition_type, params, filename)\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if self._closed:\n            return\n\n        if self._connection is not None:\n            self._connection.release()\n            self._cleanup_writer()\n\n            if self._loop.get_debug():\n                _warnings.warn(\n                    f\"Unclosed response {self!r}\", ResourceWarning, source=self\n                )\n                context = {\"client_response\": self, \"message\": \"Unclosed response\"}\n                if self._source_traceback:\n                    context[\"source_traceback\"] = self._source_traceback\n                self._loop.call_exception_handler(context)\n\n    def __repr__(self) -> str:\n        out = io.StringIO()\n        ascii_encodable_url = str(self.url)\n        if self.reason:\n            ascii_encodable_reason = self.reason.encode(\n                \"ascii\", \"backslashreplace\"\n            ).decode(\"ascii\")\n        else:\n            ascii_encodable_reason = \"None\"\n        print(\n            \"<ClientResponse({}) [{} {}]>\".format(\n                ascii_encodable_url, self.status, ascii_encodable_reason\n            ),\n            file=out,\n        )\n        print(self.headers, file=out)\n        return out.getvalue()\n\n    @property\n    def connection(self) -> Optional[\"Connection\"]:\n        return self._connection\n\n    @reify\n    def history(self) -> Tuple[\"ClientResponse\", ...]:\n        \"\"\"A sequence of responses, if redirects occurred.\"\"\"\n        return self._history\n\n    @reify\n    def links(self) -> \"MultiDictProxy[MultiDictProxy[Union[str, URL]]]\":\n        links_str = \", \".join(self.headers.getall(\"link\", []))\n\n        if not links_str:\n            return MultiDictProxy(MultiDict())\n\n        links: MultiDict[MultiDictProxy[Union[str, URL]]] = MultiDict()\n\n        for val in re.split(r\",(?=\\s*<)\", links_str):\n            match = re.match(r\"\\s*<(.*)>(.*)\", val)\n            if match is None:  # pragma: no cover\n                # the check exists to suppress mypy error\n                continue\n            url, params_str = match.groups()\n            params = params_str.split(\";\")[1:]\n\n            link: MultiDict[Union[str, URL]] = MultiDict()\n\n            for param in params:\n                match = re.match(r\"^\\s*(\\S*)\\s*=\\s*(['\\\"]?)(.*?)(\\2)\\s*$\", param, re.M)\n                if match is None:  # pragma: no cover\n                    # the check exists to suppress mypy error\n                    continue\n                key, _, value, _ = match.groups()\n\n                link.add(key, value)\n\n            key = link.get(\"rel\", url)\n\n            link.add(\"url\", self.url.join(URL(url)))\n\n            links.add(str(key), MultiDictProxy(link))\n\n        return MultiDictProxy(links)\n\n    async def start(self, connection: \"Connection\") -> \"ClientResponse\":\n        \"\"\"Start response processing.\"\"\"\n        self._closed = False\n        self._protocol = connection.protocol\n        self._connection = connection\n\n        with self._timer:\n            while True:\n                # read response\n                try:\n                    protocol = self._protocol\n                    message, payload = await protocol.read()  # type: ignore[union-attr]\n                except http.HttpProcessingError as exc:\n                    raise ClientResponseError(\n                        self.request_info,\n                        self.history,\n                        status=exc.code,\n                        message=exc.message,\n                        headers=exc.headers,\n                    ) from exc\n\n                if message.code < 100 or message.code > 199 or message.code == 101:\n                    break\n\n                if self._continue is not None:\n                    set_result(self._continue, True)\n                    self._continue = None\n\n        # payload eof handler\n        payload.on_eof(self._response_eof)\n\n        # response status\n        self.version = message.version\n        self.status = message.code\n        self.reason = message.reason\n\n        # headers\n        self._headers = message.headers  # type is CIMultiDictProxy\n        self._raw_headers = message.raw_headers  # type is Tuple[bytes, bytes]\n\n        # payload\n        self.content = payload\n\n        # cookies\n        for hdr in self.headers.getall(hdrs.SET_COOKIE, ()):\n            try:\n                self.cookies.load(hdr)\n            except CookieError as exc:\n                client_logger.warning(\"Can not load response cookies: %s\", exc)\n        return self\n\n    def _response_eof(self) -> None:\n        if self._closed:\n            return\n\n        # protocol could be None because connection could be detached\n        protocol = self._connection and self._connection.protocol\n        if protocol is not None and protocol.upgraded:\n            return\n\n        self._closed = True\n        self._cleanup_writer()\n        self._release_connection()\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    def close(self) -> None:\n        if not self._released:\n            self._notify_content()\n\n        self._closed = True\n        if self._loop.is_closed():\n            return\n\n        self._cleanup_writer()\n        if self._connection is not None:\n            self._connection.close()\n            self._connection = None\n\n    def release(self) -> Any:\n        if not self._released:\n            self._notify_content()\n\n        self._closed = True\n\n        self._cleanup_writer()\n        self._release_connection()\n        return noop()\n\n    @property\n    def ok(self) -> bool:\n        \"\"\"Returns ``True`` if ``status`` is less than ``400``, ``False`` if not.\n\n        This is **not** a check for ``200 OK`` but a check that the response\n        status is under 400.\n        \"\"\"\n        return 400 > self.status\n\n    def raise_for_status(self) -> None:\n        if not self.ok:\n            # reason should always be not None for a started response\n            assert self.reason is not None\n            self.release()\n            raise ClientResponseError(\n                self.request_info,\n                self.history,\n                status=self.status,\n                message=self.reason,\n                headers=self.headers,\n            )\n\n    def _release_connection(self) -> None:\n        if self._connection is not None:\n            if self._writer is None:\n                self._connection.release()\n                self._connection = None\n            else:\n                self._writer.add_done_callback(lambda f: self._release_connection())\n\n    async def _wait_released(self) -> None:\n        if self._writer is not None:\n            await self._writer\n        self._release_connection()\n\n    def _cleanup_writer(self) -> None:\n        if self._writer is not None:\n            self._writer.cancel()\n        self._session = None\n\n    def _notify_content(self) -> None:\n        content = self.content\n        # content can be None here, but the types are cheated elsewhere.\n        if content and content.exception() is None:  # type: ignore[truthy-bool]\n            set_exception(content, ClientConnectionError(\"Connection closed\"))\n        self._released = True\n\n    async def wait_for_close(self) -> None:\n        if self._writer is not None:\n            await self._writer\n        self.release()\n\n    async def read(self) -> bytes:\n        \"\"\"Read response payload.\"\"\"\n        if self._body is None:\n            try:\n                self._body = await self.content.read()\n                for trace in self._traces:\n                    await trace.send_response_chunk_received(\n                        self.method, self.url, self._body\n                    )\n            except BaseException:\n                self.close()\n                raise\n        elif self._released:  # Response explicitly released\n            raise ClientConnectionError(\"Connection closed\")\n\n        protocol = self._connection and self._connection.protocol\n        if protocol is None or not protocol.upgraded:\n            await self._wait_released()  # Underlying connection released\n        return self._body\n\n    def get_encoding(self) -> str:\n        ctype = self.headers.get(hdrs.CONTENT_TYPE, \"\").lower()\n        mimetype = helpers.parse_mimetype(ctype)\n\n        encoding = mimetype.parameters.get(\"charset\")\n        if encoding:\n            with contextlib.suppress(LookupError):\n                return codecs.lookup(encoding).name\n\n        if mimetype.type == \"application\" and (\n            mimetype.subtype == \"json\" or mimetype.subtype == \"rdap\"\n        ):\n            # RFC 7159 states that the default encoding is UTF-8.\n            # RFC 7483 defines application/rdap+json\n            return \"utf-8\"\n\n        if self._body is None:\n            raise RuntimeError(\n                \"Cannot compute fallback encoding of a not yet read body\"\n            )\n\n        return self._resolve_charset(self, self._body)\n\n    async def text(self, encoding: Optional[str] = None, errors: str = \"strict\") -> str:\n        \"\"\"Read response payload and decode.\"\"\"\n        if self._body is None:\n            await self.read()\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return self._body.decode(encoding, errors=errors)  # type: ignore[union-attr]\n\n    async def json(\n        self,\n        *,\n        encoding: Optional[str] = None,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        content_type: Optional[str] = \"application/json\",\n    ) -> Any:\n        \"\"\"Read and decodes JSON response.\"\"\"\n        if self._body is None:\n            await self.read()\n\n        if content_type:\n            if not is_expected_content_type(self.content_type, content_type):\n                raise ContentTypeError(\n                    self.request_info,\n                    self.history,\n                    message=(\n                        \"Attempt to decode JSON with \"\n                        \"unexpected mimetype: %s\" % self.content_type\n                    ),\n                    headers=self.headers,\n                )\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return loads(self._body.decode(encoding))  # type: ignore[union-attr]\n\n    async def __aenter__(self) -> \"ClientResponse\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        # similar to _RequestContextManager, we do not need to check\n        # for exceptions, response object can close connection\n        # if state is broken\n        self.release()\n        await self.wait_for_close()\n", "aiohttp/connector.py": "import asyncio\nimport dataclasses\nimport functools\nimport logging\nimport random\nimport socket\nimport sys\nimport traceback\nimport warnings\nfrom collections import defaultdict, deque\nfrom contextlib import suppress\nfrom http import HTTPStatus\nfrom http.cookies import SimpleCookie\nfrom itertools import cycle, islice\nfrom time import monotonic\nfrom types import TracebackType\nfrom typing import (  # noqa\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    DefaultDict,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport aiohappyeyeballs\n\nfrom . import hdrs, helpers\nfrom .abc import AbstractResolver, ResolveResult\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientConnectorCertificateError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientHttpProxyError,\n    ClientProxyConnectionError,\n    ServerFingerprintMismatch,\n    UnixClientConnectorError,\n    cert_errors,\n    ssl_errors,\n)\nfrom .client_proto import ResponseHandler\nfrom .client_reqrep import SSL_ALLOWED_TYPES, ClientRequest, Fingerprint\nfrom .helpers import _SENTINEL, ceil_timeout, is_ip_address, sentinel, set_result\nfrom .locks import EventResultOrError\nfrom .resolver import DefaultResolver\n\ntry:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nexcept ImportError:  # pragma: no cover\n    ssl = None  # type: ignore[assignment]\n    SSLContext = object  # type: ignore[misc,assignment]\n\n\n__all__ = (\"BaseConnector\", \"TCPConnector\", \"UnixConnector\", \"NamedPipeConnector\")\n\n\nif TYPE_CHECKING:\n    from .client import ClientTimeout\n    from .client_reqrep import ConnectionKey\n    from .tracing import Trace\n\n\nclass Connection:\n    _source_traceback = None\n    _transport = None\n\n    def __init__(\n        self,\n        connector: \"BaseConnector\",\n        key: \"ConnectionKey\",\n        protocol: ResponseHandler,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        self._key = key\n        self._connector = connector\n        self._loop = loop\n        self._protocol: Optional[ResponseHandler] = protocol\n        self._callbacks: List[Callable[[], None]] = []\n\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n    def __repr__(self) -> str:\n        return f\"Connection<{self._key}>\"\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if self._protocol is not None:\n            _warnings.warn(\n                f\"Unclosed connection {self!r}\", ResourceWarning, source=self\n            )\n            if self._loop.is_closed():\n                return\n\n            self._connector._release(self._key, self._protocol, should_close=True)\n\n            context = {\"client_connection\": self, \"message\": \"Unclosed connection\"}\n            if self._source_traceback is not None:\n                context[\"source_traceback\"] = self._source_traceback\n            self._loop.call_exception_handler(context)\n\n    def __bool__(self) -> Literal[True]:\n        \"\"\"Force subclasses to not be falsy, to make checks simpler.\"\"\"\n        return True\n\n    @property\n    def transport(self) -> Optional[asyncio.Transport]:\n        if self._protocol is None:\n            return None\n        return self._protocol.transport\n\n    @property\n    def protocol(self) -> Optional[ResponseHandler]:\n        return self._protocol\n\n    def add_callback(self, callback: Callable[[], None]) -> None:\n        if callback is not None:\n            self._callbacks.append(callback)\n\n    def _notify_release(self) -> None:\n        callbacks, self._callbacks = self._callbacks[:], []\n\n        for cb in callbacks:\n            with suppress(Exception):\n                cb()\n\n    def close(self) -> None:\n        self._notify_release()\n\n        if self._protocol is not None:\n            self._connector._release(self._key, self._protocol, should_close=True)\n            self._protocol = None\n\n    def release(self) -> None:\n        self._notify_release()\n\n        if self._protocol is not None:\n            self._connector._release(\n                self._key, self._protocol, should_close=self._protocol.should_close\n            )\n            self._protocol = None\n\n    @property\n    def closed(self) -> bool:\n        return self._protocol is None or not self._protocol.is_connected()\n\n\nclass _TransportPlaceholder:\n    \"\"\"placeholder for BaseConnector.connect function\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        fut = loop.create_future()\n        fut.set_result(None)\n        self.closed: asyncio.Future[Optional[Exception]] = fut\n\n    def close(self) -> None:\n        pass\n\n\nclass BaseConnector:\n    \"\"\"Base connector class.\n\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    enable_cleanup_closed - Enables clean-up closed ssl transports.\n                            Disabled by default.\n    timeout_ceil_threshold - Trigger ceiling of timeout values when\n                             it's above timeout_ceil_threshold.\n    loop - Optional event loop.\n    \"\"\"\n\n    _closed = True  # prevent AttributeError in __del__ if ctor was failed\n    _source_traceback = None\n\n    # abort transport after 2 seconds (cleanup broken connections)\n    _cleanup_closed_period = 2.0\n\n    def __init__(\n        self,\n        *,\n        keepalive_timeout: Union[_SENTINEL, None, float] = sentinel,\n        force_close: bool = False,\n        limit: int = 100,\n        limit_per_host: int = 0,\n        enable_cleanup_closed: bool = False,\n        timeout_ceil_threshold: float = 5,\n    ) -> None:\n        if force_close:\n            if keepalive_timeout is not None and keepalive_timeout is not sentinel:\n                raise ValueError(\n                    \"keepalive_timeout cannot \" \"be set if force_close is True\"\n                )\n        else:\n            if keepalive_timeout is sentinel:\n                keepalive_timeout = 15.0\n\n        self._timeout_ceil_threshold = timeout_ceil_threshold\n\n        loop = asyncio.get_running_loop()\n\n        self._closed = False\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n        self._conns: Dict[ConnectionKey, List[Tuple[ResponseHandler, float]]] = {}\n        self._limit = limit\n        self._limit_per_host = limit_per_host\n        self._acquired: Set[ResponseHandler] = set()\n        self._acquired_per_host: DefaultDict[ConnectionKey, Set[ResponseHandler]] = (\n            defaultdict(set)\n        )\n        self._keepalive_timeout = cast(float, keepalive_timeout)\n        self._force_close = force_close\n\n        # {host_key: FIFO list of waiters}\n        self._waiters = defaultdict(deque)  # type: ignore[var-annotated]\n\n        self._loop = loop\n        self._factory = functools.partial(ResponseHandler, loop=loop)\n\n        self.cookies = SimpleCookie()\n\n        # start keep-alive connection cleanup task\n        self._cleanup_handle: Optional[asyncio.TimerHandle] = None\n\n        # start cleanup closed transports task\n        self._cleanup_closed_handle: Optional[asyncio.TimerHandle] = None\n        self._cleanup_closed_disabled = not enable_cleanup_closed\n        self._cleanup_closed_transports: List[Optional[asyncio.Transport]] = []\n        self._cleanup_closed()\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if self._closed:\n            return\n        if not self._conns:\n            return\n\n        conns = [repr(c) for c in self._conns.values()]\n\n        self._close_immediately()\n\n        _warnings.warn(f\"Unclosed connector {self!r}\", ResourceWarning, source=self)\n        context = {\n            \"connector\": self,\n            \"connections\": conns,\n            \"message\": \"Unclosed connector\",\n        }\n        if self._source_traceback is not None:\n            context[\"source_traceback\"] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    async def __aenter__(self) -> \"BaseConnector\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]] = None,\n        exc_value: Optional[BaseException] = None,\n        exc_traceback: Optional[TracebackType] = None,\n    ) -> None:\n        await self.close()\n\n    @property\n    def force_close(self) -> bool:\n        \"\"\"Ultimately close connection on releasing if True.\"\"\"\n        return self._force_close\n\n    @property\n    def limit(self) -> int:\n        \"\"\"The total number for simultaneous connections.\n\n        If limit is 0 the connector has no limit.\n        The default limit size is 100.\n        \"\"\"\n        return self._limit\n\n    @property\n    def limit_per_host(self) -> int:\n        \"\"\"The limit for simultaneous connections to the same endpoint.\n\n        Endpoints are the same if they are have equal\n        (host, port, is_ssl) triple.\n        \"\"\"\n        return self._limit_per_host\n\n    def _cleanup(self) -> None:\n        \"\"\"Cleanup unused transports.\"\"\"\n        if self._cleanup_handle:\n            self._cleanup_handle.cancel()\n            # _cleanup_handle should be unset, otherwise _release() will not\n            # recreate it ever!\n            self._cleanup_handle = None\n\n        now = self._loop.time()\n        timeout = self._keepalive_timeout\n\n        if self._conns:\n            connections = {}\n            deadline = now - timeout\n            for key, conns in self._conns.items():\n                alive = []\n                for proto, use_time in conns:\n                    if proto.is_connected():\n                        if use_time - deadline < 0:\n                            transport = proto.transport\n                            proto.close()\n                            if key.is_ssl and not self._cleanup_closed_disabled:\n                                self._cleanup_closed_transports.append(transport)\n                        else:\n                            alive.append((proto, use_time))\n                    else:\n                        transport = proto.transport\n                        proto.close()\n                        if key.is_ssl and not self._cleanup_closed_disabled:\n                            self._cleanup_closed_transports.append(transport)\n\n                if alive:\n                    connections[key] = alive\n\n            self._conns = connections\n\n        if self._conns:\n            self._cleanup_handle = helpers.weakref_handle(\n                self,\n                \"_cleanup\",\n                timeout,\n                self._loop,\n                timeout_ceil_threshold=self._timeout_ceil_threshold,\n            )\n\n    def _drop_acquired_per_host(\n        self, key: \"ConnectionKey\", val: ResponseHandler\n    ) -> None:\n        acquired_per_host = self._acquired_per_host\n        if key not in acquired_per_host:\n            return\n        conns = acquired_per_host[key]\n        conns.remove(val)\n        if not conns:\n            del self._acquired_per_host[key]\n\n    def _cleanup_closed(self) -> None:\n        \"\"\"Double confirmation for transport close.\n\n        Some broken ssl servers may leave socket open without proper close.\n        \"\"\"\n        if self._cleanup_closed_handle:\n            self._cleanup_closed_handle.cancel()\n\n        for transport in self._cleanup_closed_transports:\n            if transport is not None:\n                transport.abort()\n\n        self._cleanup_closed_transports = []\n\n        if not self._cleanup_closed_disabled:\n            self._cleanup_closed_handle = helpers.weakref_handle(\n                self,\n                \"_cleanup_closed\",\n                self._cleanup_closed_period,\n                self._loop,\n                timeout_ceil_threshold=self._timeout_ceil_threshold,\n            )\n\n    async def close(self) -> None:\n        \"\"\"Close all opened transports.\"\"\"\n        waiters = self._close_immediately()\n        if waiters:\n            results = await asyncio.gather(*waiters, return_exceptions=True)\n            for res in results:\n                if isinstance(res, Exception):\n                    err_msg = \"Error while closing connector: \" + repr(res)\n                    logging.error(err_msg)\n\n    def _close_immediately(self) -> List[\"asyncio.Future[None]\"]:\n        waiters: List[\"asyncio.Future[None]\"] = []\n\n        if self._closed:\n            return waiters\n\n        self._closed = True\n\n        try:\n            if self._loop.is_closed():\n                return waiters\n\n            # cancel cleanup task\n            if self._cleanup_handle:\n                self._cleanup_handle.cancel()\n\n            # cancel cleanup close task\n            if self._cleanup_closed_handle:\n                self._cleanup_closed_handle.cancel()\n\n            for data in self._conns.values():\n                for proto, t0 in data:\n                    proto.close()\n                    waiters.append(proto.closed)\n\n            for proto in self._acquired:\n                proto.close()\n                waiters.append(proto.closed)\n\n            # TODO (A.Yushovskiy, 24-May-2019) collect transp. closing futures\n            for transport in self._cleanup_closed_transports:\n                if transport is not None:\n                    transport.abort()\n\n            return waiters\n\n        finally:\n            self._conns.clear()\n            self._acquired.clear()\n            self._waiters.clear()\n            self._cleanup_handle = None\n            self._cleanup_closed_transports.clear()\n            self._cleanup_closed_handle = None\n\n    @property\n    def closed(self) -> bool:\n        \"\"\"Is connector closed.\n\n        A readonly property.\n        \"\"\"\n        return self._closed\n\n    def _available_connections(self, key: \"ConnectionKey\") -> int:\n        \"\"\"\n        Return number of available connections.\n\n        The limit, limit_per_host and the connection key are taken into account.\n\n        If it returns less than 1 means that there are no connections\n        available.\n        \"\"\"\n        if self._limit:\n            # total calc available connections\n            available = self._limit - len(self._acquired)\n\n            # check limit per host\n            if (\n                self._limit_per_host\n                and available > 0\n                and key in self._acquired_per_host\n            ):\n                acquired = self._acquired_per_host.get(key)\n                assert acquired is not None\n                available = self._limit_per_host - len(acquired)\n\n        elif self._limit_per_host and key in self._acquired_per_host:\n            # check limit per host\n            acquired = self._acquired_per_host.get(key)\n            assert acquired is not None\n            available = self._limit_per_host - len(acquired)\n        else:\n            available = 1\n\n        return available\n\n    async def connect(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> Connection:\n        \"\"\"Get from pool or create new connection.\"\"\"\n        key = req.connection_key\n        available = self._available_connections(key)\n\n        # Wait if there are no available connections or if there are/were\n        # waiters (i.e. don't steal connection from a waiter about to wake up)\n        if available <= 0 or key in self._waiters:\n            fut = self._loop.create_future()\n\n            # This connection will now count towards the limit.\n            self._waiters[key].append(fut)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_queued_start()\n\n            try:\n                await fut\n            except BaseException as e:\n                if key in self._waiters:\n                    # remove a waiter even if it was cancelled, normally it's\n                    #  removed when it's notified\n                    try:\n                        self._waiters[key].remove(fut)\n                    except ValueError:  # fut may no longer be in list\n                        pass\n\n                raise e\n            finally:\n                if key in self._waiters and not self._waiters[key]:\n                    del self._waiters[key]\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_queued_end()\n\n        proto = self._get(key)\n        if proto is None:\n            placeholder = cast(ResponseHandler, _TransportPlaceholder(self._loop))\n            self._acquired.add(placeholder)\n            self._acquired_per_host[key].add(placeholder)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_create_start()\n\n            try:\n                proto = await self._create_connection(req, traces, timeout)\n                if self._closed:\n                    proto.close()\n                    raise ClientConnectionError(\"Connector is closed.\")\n            except BaseException:\n                if not self._closed:\n                    self._acquired.remove(placeholder)\n                    self._drop_acquired_per_host(key, placeholder)\n                    self._release_waiter()\n                raise\n            else:\n                if not self._closed:\n                    self._acquired.remove(placeholder)\n                    self._drop_acquired_per_host(key, placeholder)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_connection_create_end()\n        else:\n            if traces:\n                # Acquire the connection to prevent race conditions with limits\n                placeholder = cast(ResponseHandler, _TransportPlaceholder(self._loop))\n                self._acquired.add(placeholder)\n                self._acquired_per_host[key].add(placeholder)\n                for trace in traces:\n                    await trace.send_connection_reuseconn()\n                self._acquired.remove(placeholder)\n                self._drop_acquired_per_host(key, placeholder)\n\n        self._acquired.add(proto)\n        self._acquired_per_host[key].add(proto)\n        return Connection(self, key, proto, self._loop)\n\n    def _get(self, key: \"ConnectionKey\") -> Optional[ResponseHandler]:\n        try:\n            conns = self._conns[key]\n        except KeyError:\n            return None\n\n        t1 = self._loop.time()\n        while conns:\n            proto, t0 = conns.pop()\n            if proto.is_connected():\n                if t1 - t0 > self._keepalive_timeout:\n                    transport = proto.transport\n                    proto.close()\n                    # only for SSL transports\n                    if key.is_ssl and not self._cleanup_closed_disabled:\n                        self._cleanup_closed_transports.append(transport)\n                else:\n                    if not conns:\n                        # The very last connection was reclaimed: drop the key\n                        del self._conns[key]\n                    return proto\n            else:\n                transport = proto.transport\n                proto.close()\n                if key.is_ssl and not self._cleanup_closed_disabled:\n                    self._cleanup_closed_transports.append(transport)\n\n        # No more connections: drop the key\n        del self._conns[key]\n        return None\n\n    def _release_waiter(self) -> None:\n        \"\"\"\n        Iterates over all waiters until one to be released is found.\n\n        The one to be released is not finished and\n        belongs to a host that has available connections.\n        \"\"\"\n        if not self._waiters:\n            return\n\n        # Having the dict keys ordered this avoids to iterate\n        # at the same order at each call.\n        queues = list(self._waiters.keys())\n        random.shuffle(queues)\n\n        for key in queues:\n            if self._available_connections(key) < 1:\n                continue\n\n            waiters = self._waiters[key]\n            while waiters:\n                waiter = waiters.popleft()\n                if not waiter.done():\n                    waiter.set_result(None)\n                    return\n\n    def _release_acquired(self, key: \"ConnectionKey\", proto: ResponseHandler) -> None:\n        if self._closed:\n            # acquired connection is already released on connector closing\n            return\n\n        try:\n            self._acquired.remove(proto)\n            self._drop_acquired_per_host(key, proto)\n        except KeyError:  # pragma: no cover\n            # this may be result of undetermenistic order of objects\n            # finalization due garbage collection.\n            pass\n        else:\n            self._release_waiter()\n\n    def _release(\n        self,\n        key: \"ConnectionKey\",\n        protocol: ResponseHandler,\n        *,\n        should_close: bool = False,\n    ) -> None:\n        if self._closed:\n            # acquired connection is already released on connector closing\n            return\n\n        self._release_acquired(key, protocol)\n\n        if self._force_close:\n            should_close = True\n\n        if should_close or protocol.should_close:\n            transport = protocol.transport\n            protocol.close()\n            # TODO: Remove once fixed: https://bugs.python.org/issue39951\n            # See PR #6321\n            set_result(protocol.closed, None)\n\n            if key.is_ssl and not self._cleanup_closed_disabled:\n                self._cleanup_closed_transports.append(transport)\n        else:\n            conns = self._conns.get(key)\n            if conns is None:\n                conns = self._conns[key] = []\n            conns.append((protocol, self._loop.time()))\n\n            if self._cleanup_handle is None:\n                self._cleanup_handle = helpers.weakref_handle(\n                    self,\n                    \"_cleanup\",\n                    self._keepalive_timeout,\n                    self._loop,\n                    timeout_ceil_threshold=self._timeout_ceil_threshold,\n                )\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        raise NotImplementedError()\n\n\nclass _DNSCacheTable:\n    def __init__(self, ttl: Optional[float] = None) -> None:\n        self._addrs_rr: Dict[Tuple[str, int], Tuple[Iterator[ResolveResult], int]] = {}\n        self._timestamps: Dict[Tuple[str, int], float] = {}\n        self._ttl = ttl\n\n    def __contains__(self, host: object) -> bool:\n        return host in self._addrs_rr\n\n    def add(self, key: Tuple[str, int], addrs: List[ResolveResult]) -> None:\n        self._addrs_rr[key] = (cycle(addrs), len(addrs))\n\n        if self._ttl is not None:\n            self._timestamps[key] = monotonic()\n\n    def remove(self, key: Tuple[str, int]) -> None:\n        self._addrs_rr.pop(key, None)\n\n        if self._ttl is not None:\n            self._timestamps.pop(key, None)\n\n    def clear(self) -> None:\n        self._addrs_rr.clear()\n        self._timestamps.clear()\n\n    def next_addrs(self, key: Tuple[str, int]) -> List[ResolveResult]:\n        loop, length = self._addrs_rr[key]\n        addrs = list(islice(loop, length))\n        # Consume one more element to shift internal state of `cycle`\n        next(loop)\n        return addrs\n\n    def expired(self, key: Tuple[str, int]) -> bool:\n        if self._ttl is None:\n            return False\n\n        return self._timestamps[key] + self._ttl < monotonic()\n\n\nclass TCPConnector(BaseConnector):\n    \"\"\"TCP connector.\n\n    verify_ssl - Set to True to check ssl certifications.\n    fingerprint - Pass the binary sha256\n        digest of the expected certificate in DER format to verify\n        that the certificate the server presents matches. See also\n        https://en.wikipedia.org/wiki/Transport_Layer_Security#Certificate_pinning\n    resolver - Enable DNS lookups and use this\n        resolver\n    use_dns_cache - Use memory cache for DNS lookups.\n    ttl_dns_cache - Max seconds having cached a DNS entry, None forever.\n    family - socket address family\n    local_addr - local tuple of (host, port) to bind socket to\n\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    enable_cleanup_closed - Enables clean-up closed ssl transports.\n                            Disabled by default.\n    happy_eyeballs_delay - This is the \u201cConnection Attempt Delay\u201d\n                           as defined in RFC 8305. To disable\n                           the happy eyeballs algorithm, set to None.\n    interleave - \u201cFirst Address Family Count\u201d as defined in RFC 8305\n    loop - Optional event loop.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        use_dns_cache: bool = True,\n        ttl_dns_cache: Optional[int] = 10,\n        family: socket.AddressFamily = socket.AddressFamily.AF_UNSPEC,\n        ssl: Union[bool, Fingerprint, SSLContext] = True,\n        local_addr: Optional[Tuple[str, int]] = None,\n        resolver: Optional[AbstractResolver] = None,\n        keepalive_timeout: Union[None, float, _SENTINEL] = sentinel,\n        force_close: bool = False,\n        limit: int = 100,\n        limit_per_host: int = 0,\n        enable_cleanup_closed: bool = False,\n        timeout_ceil_threshold: float = 5,\n        happy_eyeballs_delay: Optional[float] = 0.25,\n        interleave: Optional[int] = None,\n    ):\n        super().__init__(\n            keepalive_timeout=keepalive_timeout,\n            force_close=force_close,\n            limit=limit,\n            limit_per_host=limit_per_host,\n            enable_cleanup_closed=enable_cleanup_closed,\n            timeout_ceil_threshold=timeout_ceil_threshold,\n        )\n\n        if not isinstance(ssl, SSL_ALLOWED_TYPES):\n            raise TypeError(\n                \"ssl should be SSLContext, Fingerprint, or bool, \"\n                \"got {!r} instead.\".format(ssl)\n            )\n        self._ssl = ssl\n        if resolver is None:\n            resolver = DefaultResolver()\n        self._resolver: AbstractResolver = resolver\n\n        self._use_dns_cache = use_dns_cache\n        self._cached_hosts = _DNSCacheTable(ttl=ttl_dns_cache)\n        self._throttle_dns_events: Dict[Tuple[str, int], EventResultOrError] = {}\n        self._family = family\n        self._local_addr_infos = aiohappyeyeballs.addr_to_addr_infos(local_addr)\n        self._happy_eyeballs_delay = happy_eyeballs_delay\n        self._interleave = interleave\n\n    def _close_immediately(self) -> List[\"asyncio.Future[None]\"]:\n        for ev in self._throttle_dns_events.values():\n            ev.cancel()\n        return super()._close_immediately()\n\n    @property\n    def family(self) -> int:\n        \"\"\"Socket family like AF_INET.\"\"\"\n        return self._family\n\n    @property\n    def use_dns_cache(self) -> bool:\n        \"\"\"True if local DNS caching is enabled.\"\"\"\n        return self._use_dns_cache\n\n    def clear_dns_cache(\n        self, host: Optional[str] = None, port: Optional[int] = None\n    ) -> None:\n        \"\"\"Remove specified host/port or clear all dns local cache.\"\"\"\n        if host is not None and port is not None:\n            self._cached_hosts.remove((host, port))\n        elif host is not None or port is not None:\n            raise ValueError(\"either both host and port \" \"or none of them are allowed\")\n        else:\n            self._cached_hosts.clear()\n\n    async def _resolve_host(\n        self, host: str, port: int, traces: Optional[List[\"Trace\"]] = None\n    ) -> List[ResolveResult]:\n        \"\"\"Resolve host and return list of addresses.\"\"\"\n        if is_ip_address(host):\n            return [\n                {\n                    \"hostname\": host,\n                    \"host\": host,\n                    \"port\": port,\n                    \"family\": self._family,\n                    \"proto\": 0,\n                    \"flags\": 0,\n                }\n            ]\n\n        if not self._use_dns_cache:\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_resolvehost_start(host)\n\n            res = await self._resolver.resolve(host, port, family=self._family)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_resolvehost_end(host)\n\n            return res\n\n        key = (host, port)\n        if key in self._cached_hosts and not self._cached_hosts.expired(key):\n            # get result early, before any await (#4014)\n            result = self._cached_hosts.next_addrs(key)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_cache_hit(host)\n            return result\n\n        #\n        # If multiple connectors are resolving the same host, we wait\n        # for the first one to resolve and then use the result for all of them.\n        # We use a throttle event to ensure that we only resolve the host once\n        # and then use the result for all the waiters.\n        #\n        # In this case we need to create a task to ensure that we can shield\n        # the task from cancellation as cancelling this lookup should not cancel\n        # the underlying lookup or else the cancel event will get broadcast to\n        # all the waiters across all connections.\n        #\n        resolved_host_task = asyncio.create_task(\n            self._resolve_host_with_throttle(key, host, port, traces)\n        )\n        try:\n            return await asyncio.shield(resolved_host_task)\n        except asyncio.CancelledError:\n\n            def drop_exception(fut: \"asyncio.Future[List[ResolveResult]]\") -> None:\n                with suppress(Exception, asyncio.CancelledError):\n                    fut.result()\n\n            resolved_host_task.add_done_callback(drop_exception)\n            raise\n\n    async def _resolve_host_with_throttle(\n        self,\n        key: Tuple[str, int],\n        host: str,\n        port: int,\n        traces: Optional[List[\"Trace\"]],\n    ) -> List[ResolveResult]:\n        \"\"\"Resolve host with a dns events throttle.\"\"\"\n        if key in self._throttle_dns_events:\n            # get event early, before any await (#4014)\n            event = self._throttle_dns_events[key]\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_cache_hit(host)\n            await event.wait()\n        else:\n            # update dict early, before any await (#4014)\n            self._throttle_dns_events[key] = EventResultOrError(self._loop)\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_cache_miss(host)\n            try:\n                if traces:\n                    for trace in traces:\n                        await trace.send_dns_resolvehost_start(host)\n\n                addrs = await self._resolver.resolve(host, port, family=self._family)\n                if traces:\n                    for trace in traces:\n                        await trace.send_dns_resolvehost_end(host)\n\n                self._cached_hosts.add(key, addrs)\n                self._throttle_dns_events[key].set()\n            except BaseException as e:\n                # any DNS exception, independently of the implementation\n                # is set for the waiters to raise the same exception.\n                self._throttle_dns_events[key].set(exc=e)\n                raise\n            finally:\n                self._throttle_dns_events.pop(key)\n\n        return self._cached_hosts.next_addrs(key)\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        \"\"\"Create connection.\n\n        Has same keyword arguments as BaseEventLoop.create_connection.\n        \"\"\"\n        if req.proxy:\n            _, proto = await self._create_proxy_connection(req, traces, timeout)\n        else:\n            _, proto = await self._create_direct_connection(req, traces, timeout)\n\n        return proto\n\n    @staticmethod\n    @functools.lru_cache(None)\n    def _make_ssl_context(verified: bool) -> SSLContext:\n        if verified:\n            return ssl.create_default_context()\n        else:\n            sslcontext = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n            sslcontext.options |= ssl.OP_NO_SSLv2\n            sslcontext.options |= ssl.OP_NO_SSLv3\n            sslcontext.check_hostname = False\n            sslcontext.verify_mode = ssl.CERT_NONE\n            try:\n                sslcontext.options |= ssl.OP_NO_COMPRESSION\n            except AttributeError as attr_err:\n                warnings.warn(\n                    \"{!s}: The Python interpreter is compiled \"\n                    \"against OpenSSL < 1.0.0. Ref: \"\n                    \"https://docs.python.org/3/library/ssl.html\"\n                    \"#ssl.OP_NO_COMPRESSION\".format(attr_err),\n                )\n            sslcontext.set_default_verify_paths()\n            return sslcontext\n\n    def _get_ssl_context(self, req: ClientRequest) -> Optional[SSLContext]:\n        \"\"\"Logic to get the correct SSL context\n\n        0. if req.ssl is false, return None\n\n        1. if ssl_context is specified in req, use it\n        2. if _ssl_context is specified in self, use it\n        3. otherwise:\n            1. if verify_ssl is not specified in req, use self.ssl_context\n               (will generate a default context according to self.verify_ssl)\n            2. if verify_ssl is True in req, generate a default SSL context\n            3. if verify_ssl is False in req, generate a SSL context that\n               won't verify\n        \"\"\"\n        if req.is_ssl():\n            if ssl is None:  # pragma: no cover\n                raise RuntimeError(\"SSL is not supported.\")\n            sslcontext = req.ssl\n            if isinstance(sslcontext, ssl.SSLContext):\n                return sslcontext\n            if sslcontext is not True:\n                # not verified or fingerprinted\n                return self._make_ssl_context(False)\n            sslcontext = self._ssl\n            if isinstance(sslcontext, ssl.SSLContext):\n                return sslcontext\n            if sslcontext is not True:\n                # not verified or fingerprinted\n                return self._make_ssl_context(False)\n            return self._make_ssl_context(True)\n        else:\n            return None\n\n    def _get_fingerprint(self, req: ClientRequest) -> Optional[\"Fingerprint\"]:\n        ret = req.ssl\n        if isinstance(ret, Fingerprint):\n            return ret\n        ret = self._ssl\n        if isinstance(ret, Fingerprint):\n            return ret\n        return None\n\n    async def _wrap_create_connection(\n        self,\n        *args: Any,\n        addr_infos: List[aiohappyeyeballs.AddrInfoType],\n        req: ClientRequest,\n        timeout: \"ClientTimeout\",\n        client_error: Type[Exception] = ClientConnectorError,\n        **kwargs: Any,\n    ) -> Tuple[asyncio.Transport, ResponseHandler]:\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                sock = await aiohappyeyeballs.start_connection(\n                    addr_infos=addr_infos,\n                    local_addr_infos=self._local_addr_infos,\n                    happy_eyeballs_delay=self._happy_eyeballs_delay,\n                    interleave=self._interleave,\n                    loop=self._loop,\n                )\n                return await self._loop.create_connection(*args, **kwargs, sock=sock)\n        except cert_errors as exc:\n            raise ClientConnectorCertificateError(req.connection_key, exc) from exc\n        except ssl_errors as exc:\n            raise ClientConnectorSSLError(req.connection_key, exc) from exc\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise client_error(req.connection_key, exc) from exc\n\n    def _warn_about_tls_in_tls(\n        self,\n        underlying_transport: asyncio.Transport,\n        req: ClientRequest,\n    ) -> None:\n        \"\"\"Issue a warning if the requested URL has HTTPS scheme.\"\"\"\n        if req.request_info.url.scheme != \"https\":\n            return\n\n        asyncio_supports_tls_in_tls = getattr(\n            underlying_transport,\n            \"_start_tls_compatible\",\n            False,\n        )\n\n        if asyncio_supports_tls_in_tls:\n            return\n\n        warnings.warn(\n            \"An HTTPS request is being sent through an HTTPS proxy. \"\n            \"This support for TLS in TLS is known to be disabled \"\n            \"in the stdlib asyncio. This is why you'll probably see \"\n            \"an error in the log below.\\n\\n\"\n            \"It is possible to enable it via monkeypatching. \"\n            \"For more details, see:\\n\"\n            \"* https://bugs.python.org/issue37179\\n\"\n            \"* https://github.com/python/cpython/pull/28073\\n\\n\"\n            \"You can temporarily patch this as follows:\\n\"\n            \"* https://docs.aiohttp.org/en/stable/client_advanced.html#proxy-support\\n\"\n            \"* https://github.com/aio-libs/aiohttp/discussions/6044\\n\",\n            RuntimeWarning,\n            source=self,\n            # Why `4`? At least 3 of the calls in the stack originate\n            # from the methods in this class.\n            stacklevel=3,\n        )\n\n    async def _start_tls_connection(\n        self,\n        underlying_transport: asyncio.Transport,\n        req: ClientRequest,\n        timeout: \"ClientTimeout\",\n        client_error: Type[Exception] = ClientConnectorError,\n    ) -> Tuple[asyncio.BaseTransport, ResponseHandler]:\n        \"\"\"Wrap the raw TCP transport with TLS.\"\"\"\n        tls_proto = self._factory()  # Create a brand new proto for TLS\n\n        # Safety of the `cast()` call here is based on the fact that\n        # internally `_get_ssl_context()` only returns `None` when\n        # `req.is_ssl()` evaluates to `False` which is never gonna happen\n        # in this code path. Of course, it's rather fragile\n        # maintainability-wise but this is to be solved separately.\n        sslcontext = cast(ssl.SSLContext, self._get_ssl_context(req))\n\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                try:\n                    tls_transport = await self._loop.start_tls(\n                        underlying_transport,\n                        tls_proto,\n                        sslcontext,\n                        server_hostname=req.server_hostname or req.host,\n                        ssl_handshake_timeout=timeout.total,\n                    )\n                except BaseException:\n                    # We need to close the underlying transport since\n                    # `start_tls()` probably failed before it had a\n                    # chance to do this:\n                    underlying_transport.close()\n                    raise\n        except cert_errors as exc:\n            raise ClientConnectorCertificateError(req.connection_key, exc) from exc\n        except ssl_errors as exc:\n            raise ClientConnectorSSLError(req.connection_key, exc) from exc\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise client_error(req.connection_key, exc) from exc\n        except TypeError as type_err:\n            # Example cause looks like this:\n            # TypeError: transport <asyncio.sslproto._SSLProtocolTransport\n            # object at 0x7f760615e460> is not supported by start_tls()\n\n            raise ClientConnectionError(\n                \"Cannot initialize a TLS-in-TLS connection to host \"\n                f\"{req.host!s}:{req.port:d} through an underlying connection \"\n                f\"to an HTTPS proxy {req.proxy!s} ssl:{req.ssl or 'default'} \"\n                f\"[{type_err!s}]\"\n            ) from type_err\n        else:\n            if tls_transport is None:\n                msg = \"Failed to start TLS (possibly caused by closing transport)\"\n                raise client_error(req.connection_key, OSError(msg))\n            tls_proto.connection_made(\n                tls_transport\n            )  # Kick the state machine of the new TLS protocol\n\n        return tls_transport, tls_proto\n\n    def _convert_hosts_to_addr_infos(\n        self, hosts: List[ResolveResult]\n    ) -> List[aiohappyeyeballs.AddrInfoType]:\n        \"\"\"Converts the list of hosts to a list of addr_infos.\n\n        The list of hosts is the result of a DNS lookup. The list of\n        addr_infos is the result of a call to `socket.getaddrinfo()`.\n        \"\"\"\n        addr_infos: List[aiohappyeyeballs.AddrInfoType] = []\n        for hinfo in hosts:\n            host = hinfo[\"host\"]\n            is_ipv6 = \":\" in host\n            family = socket.AF_INET6 if is_ipv6 else socket.AF_INET\n            if self._family and self._family != family:\n                continue\n            addr = (host, hinfo[\"port\"], 0, 0) if is_ipv6 else (host, hinfo[\"port\"])\n            addr_infos.append(\n                (family, socket.SOCK_STREAM, socket.IPPROTO_TCP, \"\", addr)\n            )\n        return addr_infos\n\n    async def _create_direct_connection(\n        self,\n        req: ClientRequest,\n        traces: List[\"Trace\"],\n        timeout: \"ClientTimeout\",\n        *,\n        client_error: Type[Exception] = ClientConnectorError,\n    ) -> Tuple[asyncio.Transport, ResponseHandler]:\n        sslcontext = self._get_ssl_context(req)\n        fingerprint = self._get_fingerprint(req)\n\n        host = req.url.raw_host\n        assert host is not None\n        # Replace multiple trailing dots with a single one.\n        # A trailing dot is only present for fully-qualified domain names.\n        # See https://github.com/aio-libs/aiohttp/pull/7364.\n        if host.endswith(\"..\"):\n            host = host.rstrip(\".\") + \".\"\n        port = req.port\n        assert port is not None\n        try:\n            # Cancelling this lookup should not cancel the underlying lookup\n            #  or else the cancel event will get broadcast to all the waiters\n            #  across all connections.\n            hosts = await self._resolve_host(host, port, traces=traces)\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            # in case of proxy it is not ClientProxyConnectionError\n            # it is problem of resolving proxy ip itself\n            raise ClientConnectorError(req.connection_key, exc) from exc\n\n        last_exc: Optional[Exception] = None\n        addr_infos = self._convert_hosts_to_addr_infos(hosts)\n        while addr_infos:\n            # Strip trailing dots, certificates contain FQDN without dots.\n            # See https://github.com/aio-libs/aiohttp/issues/3636\n            server_hostname = (\n                (req.server_hostname or host).rstrip(\".\") if sslcontext else None\n            )\n\n            try:\n                transp, proto = await self._wrap_create_connection(\n                    self._factory,\n                    timeout=timeout,\n                    ssl=sslcontext,\n                    addr_infos=addr_infos,\n                    server_hostname=server_hostname,\n                    req=req,\n                    client_error=client_error,\n                )\n            except ClientConnectorError as exc:\n                last_exc = exc\n                aiohappyeyeballs.pop_addr_infos_interleave(addr_infos, self._interleave)\n                continue\n\n            if req.is_ssl() and fingerprint:\n                try:\n                    fingerprint.check(transp)\n                except ServerFingerprintMismatch as exc:\n                    transp.close()\n                    if not self._cleanup_closed_disabled:\n                        self._cleanup_closed_transports.append(transp)\n                    last_exc = exc\n                    # Remove the bad peer from the list of addr_infos\n                    sock: socket.socket = transp.get_extra_info(\"socket\")\n                    bad_peer = sock.getpeername()\n                    aiohappyeyeballs.remove_addr_infos(addr_infos, bad_peer)\n                    continue\n\n            return transp, proto\n        assert last_exc is not None\n        raise last_exc\n\n    async def _create_proxy_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> Tuple[asyncio.BaseTransport, ResponseHandler]:\n        headers: Dict[str, str] = {}\n        if req.proxy_headers is not None:\n            headers = req.proxy_headers  # type: ignore[assignment]\n        headers[hdrs.HOST] = req.headers[hdrs.HOST]\n\n        url = req.proxy\n        assert url is not None\n        proxy_req = ClientRequest(\n            hdrs.METH_GET,\n            url,\n            headers=headers,\n            auth=req.proxy_auth,\n            loop=self._loop,\n            ssl=req.ssl,\n        )\n\n        # create connection to proxy server\n        transport, proto = await self._create_direct_connection(\n            proxy_req, [], timeout, client_error=ClientProxyConnectionError\n        )\n\n        # Many HTTP proxies has buggy keepalive support.  Let's not\n        # reuse connection but close it after processing every\n        # response.\n        proto.force_close()\n\n        auth = proxy_req.headers.pop(hdrs.AUTHORIZATION, None)\n        if auth is not None:\n            if not req.is_ssl():\n                req.headers[hdrs.PROXY_AUTHORIZATION] = auth\n            else:\n                proxy_req.headers[hdrs.PROXY_AUTHORIZATION] = auth\n\n        if req.is_ssl():\n            self._warn_about_tls_in_tls(transport, req)\n\n            # For HTTPS requests over HTTP proxy\n            # we must notify proxy to tunnel connection\n            # so we send CONNECT command:\n            #   CONNECT www.python.org:443 HTTP/1.1\n            #   Host: www.python.org\n            #\n            # next we must do TLS handshake and so on\n            # to do this we must wrap raw socket into secure one\n            # asyncio handles this perfectly\n            proxy_req.method = hdrs.METH_CONNECT\n            proxy_req.url = req.url\n            key = dataclasses.replace(\n                req.connection_key, proxy=None, proxy_auth=None, proxy_headers_hash=None\n            )\n            conn = Connection(self, key, proto, self._loop)\n            proxy_resp = await proxy_req.send(conn)\n            try:\n                protocol = conn._protocol\n                assert protocol is not None\n\n                # read_until_eof=True will ensure the connection isn't closed\n                # once the response is received and processed allowing\n                # START_TLS to work on the connection below.\n                protocol.set_response_params(\n                    read_until_eof=True,\n                    timeout_ceil_threshold=self._timeout_ceil_threshold,\n                )\n                resp = await proxy_resp.start(conn)\n            except BaseException:\n                proxy_resp.close()\n                conn.close()\n                raise\n            else:\n                conn._protocol = None\n                conn._transport = None\n                try:\n                    if resp.status != 200:\n                        message = resp.reason\n                        if message is None:\n                            message = HTTPStatus(resp.status).phrase\n                        raise ClientHttpProxyError(\n                            proxy_resp.request_info,\n                            resp.history,\n                            status=resp.status,\n                            message=message,\n                            headers=resp.headers,\n                        )\n                except BaseException:\n                    # It shouldn't be closed in `finally` because it's fed to\n                    # `loop.start_tls()` and the docs say not to touch it after\n                    # passing there.\n                    transport.close()\n                    raise\n\n                return await self._start_tls_connection(\n                    # Access the old transport for the last time before it's\n                    # closed and forgotten forever:\n                    transport,\n                    req=req,\n                    timeout=timeout,\n                )\n            finally:\n                proxy_resp.close()\n\n        return transport, proto\n\n\nclass UnixConnector(BaseConnector):\n    \"\"\"Unix socket connector.\n\n    path - Unix socket path.\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    loop - Optional event loop.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        force_close: bool = False,\n        keepalive_timeout: Union[_SENTINEL, float, None] = sentinel,\n        limit: int = 100,\n        limit_per_host: int = 0,\n    ) -> None:\n        super().__init__(\n            force_close=force_close,\n            keepalive_timeout=keepalive_timeout,\n            limit=limit,\n            limit_per_host=limit_per_host,\n        )\n        self._path = path\n\n    @property\n    def path(self) -> str:\n        \"\"\"Path to unix socket.\"\"\"\n        return self._path\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                _, proto = await self._loop.create_unix_connection(\n                    self._factory, self._path\n                )\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise UnixClientConnectorError(self.path, req.connection_key, exc) from exc\n\n        return proto\n\n\nclass NamedPipeConnector(BaseConnector):\n    \"\"\"Named pipe connector.\n\n    Only supported by the proactor event loop.\n    See also: https://docs.python.org/3/library/asyncio-eventloop.html\n\n    path - Windows named pipe path.\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    loop - Optional event loop.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        force_close: bool = False,\n        keepalive_timeout: Union[_SENTINEL, float, None] = sentinel,\n        limit: int = 100,\n        limit_per_host: int = 0,\n    ) -> None:\n        super().__init__(\n            force_close=force_close,\n            keepalive_timeout=keepalive_timeout,\n            limit=limit,\n            limit_per_host=limit_per_host,\n        )\n        if not isinstance(\n            self._loop, asyncio.ProactorEventLoop  # type: ignore[attr-defined]\n        ):\n            raise RuntimeError(\n                \"Named Pipes only available in proactor \" \"loop under windows\"\n            )\n        self._path = path\n\n    @property\n    def path(self) -> str:\n        \"\"\"Path to the named pipe.\"\"\"\n        return self._path\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                _, proto = await self._loop.create_pipe_connection(  # type: ignore[attr-defined]\n                    self._factory, self._path\n                )\n                # the drain is required so that the connection_made is called\n                # and transport is set otherwise it is not set before the\n                # `assert conn.transport is not None`\n                # in client.py's _request method\n                await asyncio.sleep(0)\n                # other option is to manually set transport like\n                # `proto.transport = trans`\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise ClientConnectorError(req.connection_key, exc) from exc\n\n        return cast(ResponseHandler, proto)\n", "aiohttp/http_writer.py": "\"\"\"Http related parsers and protocol.\"\"\"\n\nimport asyncio\nimport zlib\nfrom typing import Any, Awaitable, Callable, NamedTuple, Optional, Union  # noqa\n\nfrom multidict import CIMultiDict\n\nfrom .abc import AbstractStreamWriter\nfrom .base_protocol import BaseProtocol\nfrom .compression_utils import ZLibCompressor\nfrom .helpers import NO_EXTENSIONS\n\n__all__ = (\"StreamWriter\", \"HttpVersion\", \"HttpVersion10\", \"HttpVersion11\")\n\n\nclass HttpVersion(NamedTuple):\n    major: int\n    minor: int\n\n\nHttpVersion10 = HttpVersion(1, 0)\nHttpVersion11 = HttpVersion(1, 1)\n\n\n_T_OnChunkSent = Optional[Callable[[bytes], Awaitable[None]]]\n_T_OnHeadersSent = Optional[Callable[[\"CIMultiDict[str]\"], Awaitable[None]]]\n\n\nclass StreamWriter(AbstractStreamWriter):\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        loop: asyncio.AbstractEventLoop,\n        on_chunk_sent: _T_OnChunkSent = None,\n        on_headers_sent: _T_OnHeadersSent = None,\n    ) -> None:\n        self._protocol = protocol\n\n        self.loop = loop\n        self.length = None\n        self.chunked = False\n        self.buffer_size = 0\n        self.output_size = 0\n\n        self._eof = False\n        self._compress: Optional[ZLibCompressor] = None\n        self._drain_waiter = None\n\n        self._on_chunk_sent: _T_OnChunkSent = on_chunk_sent\n        self._on_headers_sent: _T_OnHeadersSent = on_headers_sent\n\n    @property\n    def transport(self) -> Optional[asyncio.Transport]:\n        return self._protocol.transport\n\n    @property\n    def protocol(self) -> BaseProtocol:\n        return self._protocol\n\n    def enable_chunking(self) -> None:\n        self.chunked = True\n\n    def enable_compression(\n        self, encoding: str = \"deflate\", strategy: int = zlib.Z_DEFAULT_STRATEGY\n    ) -> None:\n        self._compress = ZLibCompressor(encoding=encoding, strategy=strategy)\n\n    def _write(self, chunk: bytes) -> None:\n        size = len(chunk)\n        self.buffer_size += size\n        self.output_size += size\n        transport = self.transport\n        if not self._protocol.connected or transport is None or transport.is_closing():\n            raise ConnectionResetError(\"Cannot write to closing transport\")\n        transport.write(chunk)\n\n    async def write(\n        self, chunk: bytes, *, drain: bool = True, LIMIT: int = 0x10000\n    ) -> None:\n        \"\"\"Writes chunk of data to a stream.\n\n        write_eof() indicates end of stream.\n        writer can't be used after write_eof() method being called.\n        write() return drain future.\n        \"\"\"\n        if self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if isinstance(chunk, memoryview):\n            if chunk.nbytes != len(chunk):\n                # just reshape it\n                chunk = chunk.cast(\"c\")\n\n        if self._compress is not None:\n            chunk = await self._compress.compress(chunk)\n            if not chunk:\n                return\n\n        if self.length is not None:\n            chunk_len = len(chunk)\n            if self.length >= chunk_len:\n                self.length = self.length - chunk_len\n            else:\n                chunk = chunk[: self.length]\n                self.length = 0\n                if not chunk:\n                    return\n\n        if chunk:\n            if self.chunked:\n                chunk_len_pre = (\"%x\\r\\n\" % len(chunk)).encode(\"ascii\")\n                chunk = chunk_len_pre + chunk + b\"\\r\\n\"\n\n            self._write(chunk)\n\n            if self.buffer_size > LIMIT and drain:\n                self.buffer_size = 0\n                await self.drain()\n\n    async def write_headers(\n        self, status_line: str, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        \"\"\"Write request/response status and headers.\"\"\"\n        if self._on_headers_sent is not None:\n            await self._on_headers_sent(headers)\n\n        # status + headers\n        buf = _serialize_headers(status_line, headers)\n        self._write(buf)\n\n    async def write_eof(self, chunk: bytes = b\"\") -> None:\n        if self._eof:\n            return\n\n        if chunk and self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if self._compress:\n            if chunk:\n                chunk = await self._compress.compress(chunk)\n\n            chunk += self._compress.flush()\n            if chunk and self.chunked:\n                chunk_len = (\"%x\\r\\n\" % len(chunk)).encode(\"ascii\")\n                chunk = chunk_len + chunk + b\"\\r\\n0\\r\\n\\r\\n\"\n        else:\n            if self.chunked:\n                if chunk:\n                    chunk_len = (\"%x\\r\\n\" % len(chunk)).encode(\"ascii\")\n                    chunk = chunk_len + chunk + b\"\\r\\n0\\r\\n\\r\\n\"\n                else:\n                    chunk = b\"0\\r\\n\\r\\n\"\n\n        if chunk:\n            self._write(chunk)\n\n        await self.drain()\n\n        self._eof = True\n\n    async def drain(self) -> None:\n        \"\"\"Flush the write buffer.\n\n        The intended use is to write\n\n          await w.write(data)\n          await w.drain()\n        \"\"\"\n        if self._protocol.transport is not None:\n            await self._protocol._drain_helper()\n\n\ndef _safe_header(string: str) -> str:\n    if \"\\r\" in string or \"\\n\" in string:\n        raise ValueError(\n            \"Newline or carriage return detected in headers. \"\n            \"Potential header injection attack.\"\n        )\n    return string\n\n\ndef _py_serialize_headers(status_line: str, headers: \"CIMultiDict[str]\") -> bytes:\n    headers_gen = (_safe_header(k) + \": \" + _safe_header(v) for k, v in headers.items())\n    line = status_line + \"\\r\\n\" + \"\\r\\n\".join(headers_gen) + \"\\r\\n\\r\\n\"\n    return line.encode(\"utf-8\")\n\n\n_serialize_headers = _py_serialize_headers\n\ntry:\n    import aiohttp._http_writer as _http_writer  # type: ignore[import-not-found]\n\n    _c_serialize_headers = _http_writer._serialize_headers\n    if not NO_EXTENSIONS:\n        _serialize_headers = _c_serialize_headers\nexcept ImportError:\n    pass\n", "aiohttp/web_ws.py": "import asyncio\nimport base64\nimport binascii\nimport dataclasses\nimport hashlib\nimport json\nimport sys\nfrom typing import Any, Final, Iterable, Optional, Tuple, cast\n\nfrom multidict import CIMultiDict\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import call_later, set_exception, set_result\nfrom .http import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WS_KEY,\n    WebSocketError,\n    WebSocketReader,\n    WebSocketWriter,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType as WSMsgType,\n    ws_ext_gen,\n    ws_ext_parse,\n)\nfrom .log import ws_logger\nfrom .streams import EofStream, FlowControlDataQueue\nfrom .typedefs import JSONDecoder, JSONEncoder\nfrom .web_exceptions import HTTPBadRequest, HTTPException\nfrom .web_request import BaseRequest\nfrom .web_response import StreamResponse\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\n__all__ = (\n    \"WebSocketResponse\",\n    \"WebSocketReady\",\n    \"WSMsgType\",\n)\n\nTHRESHOLD_CONNLOST_ACCESS: Final[int] = 5\n\n\n@dataclasses.dataclass(frozen=True)\nclass WebSocketReady:\n    ok: bool\n    protocol: Optional[str]\n\n    def __bool__(self) -> bool:\n        return self.ok\n\n\nclass WebSocketResponse(StreamResponse):\n    __slots__ = (\n        \"_protocols\",\n        \"_ws_protocol\",\n        \"_writer\",\n        \"_reader\",\n        \"_closed\",\n        \"_closing\",\n        \"_conn_lost\",\n        \"_close_code\",\n        \"_loop\",\n        \"_waiting\",\n        \"_exception\",\n        \"_timeout\",\n        \"_receive_timeout\",\n        \"_autoclose\",\n        \"_autoping\",\n        \"_heartbeat\",\n        \"_heartbeat_cb\",\n        \"_pong_heartbeat\",\n        \"_pong_response_cb\",\n        \"_compress\",\n        \"_max_msg_size\",\n    )\n\n    def __init__(\n        self,\n        *,\n        timeout: float = 10.0,\n        receive_timeout: Optional[float] = None,\n        autoclose: bool = True,\n        autoping: bool = True,\n        heartbeat: Optional[float] = None,\n        protocols: Iterable[str] = (),\n        compress: bool = True,\n        max_msg_size: int = 4 * 1024 * 1024,\n    ) -> None:\n        super().__init__(status=101)\n        self._length_check = False\n        self._protocols = protocols\n        self._ws_protocol: Optional[str] = None\n        self._writer: Optional[WebSocketWriter] = None\n        self._reader: Optional[FlowControlDataQueue[WSMessage]] = None\n        self._closed = False\n        self._closing = False\n        self._conn_lost = 0\n        self._close_code: Optional[int] = None\n        self._loop: Optional[asyncio.AbstractEventLoop] = None\n        self._waiting: Optional[asyncio.Future[bool]] = None\n        self._exception: Optional[BaseException] = None\n        self._timeout = timeout\n        self._receive_timeout = receive_timeout\n        self._autoclose = autoclose\n        self._autoping = autoping\n        self._heartbeat = heartbeat\n        self._heartbeat_cb: Optional[asyncio.TimerHandle] = None\n        if heartbeat is not None:\n            self._pong_heartbeat = heartbeat / 2.0\n        self._pong_response_cb: Optional[asyncio.TimerHandle] = None\n        self._compress = compress\n        self._max_msg_size = max_msg_size\n\n    def _cancel_heartbeat(self) -> None:\n        if self._pong_response_cb is not None:\n            self._pong_response_cb.cancel()\n            self._pong_response_cb = None\n\n        if self._heartbeat_cb is not None:\n            self._heartbeat_cb.cancel()\n            self._heartbeat_cb = None\n\n    def _reset_heartbeat(self) -> None:\n        self._cancel_heartbeat()\n\n        if self._heartbeat is not None:\n            assert self._loop is not None\n            self._heartbeat_cb = call_later(\n                self._send_heartbeat,\n                self._heartbeat,\n                self._loop,\n                timeout_ceil_threshold=(\n                    self._req._protocol._timeout_ceil_threshold\n                    if self._req is not None\n                    else 5\n                ),\n            )\n\n    def _send_heartbeat(self) -> None:\n        if self._heartbeat is not None and not self._closed:\n            assert self._loop is not None and self._writer is not None\n            # fire-and-forget a task is not perfect but maybe ok for\n            # sending ping. Otherwise we need a long-living heartbeat\n            # task in the class.\n            self._loop.create_task(self._writer.ping())  # type: ignore[unused-awaitable]\n\n            if self._pong_response_cb is not None:\n                self._pong_response_cb.cancel()\n            self._pong_response_cb = call_later(\n                self._pong_not_received,\n                self._pong_heartbeat,\n                self._loop,\n                timeout_ceil_threshold=(\n                    self._req._protocol._timeout_ceil_threshold\n                    if self._req is not None\n                    else 5\n                ),\n            )\n\n    def _pong_not_received(self) -> None:\n        if self._req is not None and self._req.transport is not None:\n            self._closed = True\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            self._exception = asyncio.TimeoutError()\n\n    async def prepare(self, request: BaseRequest) -> AbstractStreamWriter:\n        # make pre-check to don't hide it by do_handshake() exceptions\n        if self._payload_writer is not None:\n            return self._payload_writer\n\n        protocol, writer = self._pre_start(request)\n        payload_writer = await super().prepare(request)\n        assert payload_writer is not None\n        self._post_start(request, protocol, writer)\n        await payload_writer.drain()\n        return payload_writer\n\n    def _handshake(\n        self, request: BaseRequest\n    ) -> Tuple[\"CIMultiDict[str]\", str, bool, bool]:\n        headers = request.headers\n        if \"websocket\" != headers.get(hdrs.UPGRADE, \"\").lower().strip():\n            raise HTTPBadRequest(\n                text=(\n                    \"No WebSocket UPGRADE hdr: {}\\n Can \"\n                    '\"Upgrade\" only to \"WebSocket\".'\n                ).format(headers.get(hdrs.UPGRADE))\n            )\n\n        if \"upgrade\" not in headers.get(hdrs.CONNECTION, \"\").lower():\n            raise HTTPBadRequest(\n                text=\"No CONNECTION upgrade hdr: {}\".format(\n                    headers.get(hdrs.CONNECTION)\n                )\n            )\n\n        # find common sub-protocol between client and server\n        protocol = None\n        if hdrs.SEC_WEBSOCKET_PROTOCOL in headers:\n            req_protocols = [\n                str(proto.strip())\n                for proto in headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(\",\")\n            ]\n\n            for proto in req_protocols:\n                if proto in self._protocols:\n                    protocol = proto\n                    break\n            else:\n                # No overlap found: Return no protocol as per spec\n                ws_logger.warning(\n                    \"Client protocols %r don\u2019t overlap server-known ones %r\",\n                    req_protocols,\n                    self._protocols,\n                )\n\n        # check supported version\n        version = headers.get(hdrs.SEC_WEBSOCKET_VERSION, \"\")\n        if version not in (\"13\", \"8\", \"7\"):\n            raise HTTPBadRequest(text=f\"Unsupported version: {version}\")\n\n        # check client handshake for validity\n        key = headers.get(hdrs.SEC_WEBSOCKET_KEY)\n        try:\n            if not key or len(base64.b64decode(key)) != 16:\n                raise HTTPBadRequest(text=f\"Handshake error: {key!r}\")\n        except binascii.Error:\n            raise HTTPBadRequest(text=f\"Handshake error: {key!r}\") from None\n\n        accept_val = base64.b64encode(\n            hashlib.sha1(key.encode() + WS_KEY).digest()\n        ).decode()\n        response_headers = CIMultiDict(\n            {\n                hdrs.UPGRADE: \"websocket\",\n                hdrs.CONNECTION: \"upgrade\",\n                hdrs.SEC_WEBSOCKET_ACCEPT: accept_val,\n            }\n        )\n\n        notakeover = False\n        compress = 0\n        if self._compress:\n            extensions = headers.get(hdrs.SEC_WEBSOCKET_EXTENSIONS)\n            # Server side always get return with no exception.\n            # If something happened, just drop compress extension\n            compress, notakeover = ws_ext_parse(extensions, isserver=True)\n            if compress:\n                enabledext = ws_ext_gen(\n                    compress=compress, isserver=True, server_notakeover=notakeover\n                )\n                response_headers[hdrs.SEC_WEBSOCKET_EXTENSIONS] = enabledext\n\n        if protocol:\n            response_headers[hdrs.SEC_WEBSOCKET_PROTOCOL] = protocol\n        return (\n            response_headers,\n            protocol,\n            compress,\n            notakeover,\n        )  # type: ignore[return-value]\n\n    def _pre_start(self, request: BaseRequest) -> Tuple[str, WebSocketWriter]:\n        self._loop = request._loop\n\n        headers, protocol, compress, notakeover = self._handshake(request)\n\n        self.set_status(101)\n        self.headers.update(headers)\n        self.force_close()\n        self._compress = compress\n        transport = request._protocol.transport\n        assert transport is not None\n        writer = WebSocketWriter(\n            request._protocol, transport, compress=compress, notakeover=notakeover\n        )\n\n        return protocol, writer\n\n    def _post_start(\n        self, request: BaseRequest, protocol: str, writer: WebSocketWriter\n    ) -> None:\n        self._ws_protocol = protocol\n        self._writer = writer\n\n        self._reset_heartbeat()\n\n        loop = self._loop\n        assert loop is not None\n        self._reader = FlowControlDataQueue(request._protocol, 2**16, loop=loop)\n        request.protocol.set_parser(\n            WebSocketReader(self._reader, self._max_msg_size, compress=self._compress)\n        )\n        # disable HTTP keepalive for WebSocket\n        request.protocol.keep_alive(False)\n\n    def can_prepare(self, request: BaseRequest) -> WebSocketReady:\n        if self._writer is not None:\n            raise RuntimeError(\"Already started\")\n        try:\n            _, protocol, _, _ = self._handshake(request)\n        except HTTPException:\n            return WebSocketReady(False, None)\n        else:\n            return WebSocketReady(True, protocol)\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    @property\n    def close_code(self) -> Optional[int]:\n        return self._close_code\n\n    @property\n    def ws_protocol(self) -> Optional[str]:\n        return self._ws_protocol\n\n    @property\n    def compress(self) -> bool:\n        return self._compress\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        \"\"\"Get optional transport information.\n\n        If no value associated with ``name`` is found, ``default`` is returned.\n        \"\"\"\n        writer = self._writer\n        if writer is None:\n            return default\n        transport = writer.transport\n        if transport is None:\n            return default\n        return transport.get_extra_info(name, default)\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    async def ping(self, message: bytes = b\"\") -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        await self._writer.ping(message)\n\n    async def pong(self, message: bytes = b\"\") -> None:\n        # unsolicited pong\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        await self._writer.pong(message)\n\n    async def send_str(self, data: str, compress: Optional[bool] = None) -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        if not isinstance(data, str):\n            raise TypeError(\"data argument must be str (%r)\" % type(data))\n        await self._writer.send(data, binary=False, compress=compress)\n\n    async def send_bytes(self, data: bytes, compress: Optional[bool] = None) -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data argument must be byte-ish (%r)\" % type(data))\n        await self._writer.send(data, binary=True, compress=compress)\n\n    async def send_json(\n        self,\n        data: Any,\n        compress: Optional[bool] = None,\n        *,\n        dumps: JSONEncoder = json.dumps,\n    ) -> None:\n        await self.send_str(dumps(data), compress=compress)\n\n    async def write_eof(self) -> None:  # type: ignore[override]\n        if self._eof_sent:\n            return\n        if self._payload_writer is None:\n            raise RuntimeError(\"Response has not been started\")\n\n        await self.close()\n        self._eof_sent = True\n\n    async def close(\n        self, *, code: int = WSCloseCode.OK, message: bytes = b\"\", drain: bool = True\n    ) -> bool:\n        \"\"\"Close websocket connection.\"\"\"\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n\n        self._cancel_heartbeat()\n        reader = self._reader\n        assert reader is not None\n\n        # we need to break `receive()` cycle first,\n        # `close()` may be called from different task\n        if self._waiting is not None and not self._closed:\n            reader.feed_data(WS_CLOSING_MESSAGE)\n            await self._waiting\n\n        if self._closed:\n            return False\n\n        self._closed = True\n        try:\n            await self._writer.close(code, message)\n            writer = self._payload_writer\n            assert writer is not None\n            if drain:\n                await writer.drain()\n        except (asyncio.CancelledError, asyncio.TimeoutError):\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            raise\n        except Exception as exc:\n            self._exception = exc\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            return True\n\n        if self._closing:\n            self._close_transport()\n            return True\n\n        reader = self._reader\n        assert reader is not None\n        try:\n            async with async_timeout.timeout(self._timeout):\n                msg = await reader.read()\n        except asyncio.CancelledError:\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            raise\n        except Exception as exc:\n            self._exception = exc\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            return True\n\n        if msg.type == WSMsgType.CLOSE:\n            self._set_code_close_transport(msg.data)\n            return True\n\n        self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n        self._exception = asyncio.TimeoutError()\n        return True\n\n    def _set_closing(self, code: WSCloseCode) -> None:\n        \"\"\"Set the close code and mark the connection as closing.\"\"\"\n        self._closing = True\n        self._close_code = code\n\n    def _set_code_close_transport(self, code: WSCloseCode) -> None:\n        \"\"\"Set the close code and close the transport.\"\"\"\n        self._close_code = code\n        self._close_transport()\n\n    def _close_transport(self) -> None:\n        \"\"\"Close the transport.\"\"\"\n        if self._req is not None and self._req.transport is not None:\n            self._req.transport.close()\n\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\n        if self._reader is None:\n            raise RuntimeError(\"Call .prepare() first\")\n\n        loop = self._loop\n        assert loop is not None\n        while True:\n            if self._waiting is not None:\n                raise RuntimeError(\"Concurrent call to receive() is not allowed\")\n\n            if self._closed:\n                self._conn_lost += 1\n                if self._conn_lost >= THRESHOLD_CONNLOST_ACCESS:\n                    raise RuntimeError(\"WebSocket connection is closed.\")\n                return WS_CLOSED_MESSAGE\n            elif self._closing:\n                return WS_CLOSING_MESSAGE\n\n            try:\n                self._waiting = loop.create_future()\n                try:\n                    async with async_timeout.timeout(timeout or self._receive_timeout):\n                        msg = await self._reader.read()\n                    self._reset_heartbeat()\n                finally:\n                    waiter = self._waiting\n                    set_result(waiter, True)\n                    self._waiting = None\n            except asyncio.TimeoutError:\n                raise\n            except EofStream:\n                self._close_code = WSCloseCode.OK\n                await self.close()\n                return WSMessage(WSMsgType.CLOSED, None, None)\n            except WebSocketError as exc:\n                self._close_code = exc.code\n                await self.close(code=exc.code)\n                return WSMessage(WSMsgType.ERROR, exc, None)\n            except Exception as exc:\n                self._exception = exc\n                self._set_closing(WSCloseCode.ABNORMAL_CLOSURE)\n                await self.close()\n                return WSMessage(WSMsgType.ERROR, exc, None)\n\n            if msg.type == WSMsgType.CLOSE:\n                self._set_closing(msg.data)\n                # Could be closed while awaiting reader.\n                if not self._closed and self._autoclose:  # type: ignore[redundant-expr]\n                    # The client is likely going to close the\n                    # connection out from under us so we do not\n                    # want to drain any pending writes as it will\n                    # likely result writing to a broken pipe.\n                    await self.close(drain=False)\n            elif msg.type == WSMsgType.CLOSING:\n                self._set_closing(WSCloseCode.OK)\n            elif msg.type == WSMsgType.PING and self._autoping:\n                await self.pong(msg.data)\n                continue\n            elif msg.type == WSMsgType.PONG and self._autoping:\n                continue\n\n            return msg\n\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\n        msg = await self.receive(timeout)\n        if msg.type != WSMsgType.TEXT:\n            raise TypeError(\n                \"Received message {}:{!r} is not WSMsgType.TEXT\".format(\n                    msg.type, msg.data\n                )\n            )\n        return cast(str, msg.data)\n\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\n        msg = await self.receive(timeout)\n        if msg.type != WSMsgType.BINARY:\n            raise TypeError(f\"Received message {msg.type}:{msg.data!r} is not bytes\")\n        return cast(bytes, msg.data)\n\n    async def receive_json(\n        self, *, loads: JSONDecoder = json.loads, timeout: Optional[float] = None\n    ) -> Any:\n        data = await self.receive_str(timeout=timeout)\n        return loads(data)\n\n    async def write(self, data: bytes) -> None:\n        raise RuntimeError(\"Cannot call .write() for websocket\")\n\n    def __aiter__(self) -> \"WebSocketResponse\":\n        return self\n\n    async def __anext__(self) -> WSMessage:\n        msg = await self.receive()\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\n            raise StopAsyncIteration\n        return msg\n\n    def _cancel(self, exc: BaseException) -> None:\n        # web_protocol calls this from connection_lost\n        # or when the server is shutting down.\n        self._closing = True\n        if self._reader is not None:\n            set_exception(self._reader, exc)\n", "aiohttp/web.py": "import asyncio\nimport logging\nimport os\nimport socket\nimport sys\nimport warnings\nfrom argparse import ArgumentParser\nfrom collections.abc import Iterable\nfrom contextlib import suppress\nfrom functools import partial\nfrom importlib import import_module\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Iterable as TypingIterable,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    cast,\n)\nfrom weakref import WeakSet\n\nfrom .abc import AbstractAccessLogger\nfrom .helpers import AppKey\nfrom .log import access_logger\nfrom .typedefs import PathLike\nfrom .web_app import Application, CleanupError\nfrom .web_exceptions import (\n    HTTPAccepted,\n    HTTPBadGateway,\n    HTTPBadRequest,\n    HTTPClientError,\n    HTTPConflict,\n    HTTPCreated,\n    HTTPError,\n    HTTPException,\n    HTTPExpectationFailed,\n    HTTPFailedDependency,\n    HTTPForbidden,\n    HTTPFound,\n    HTTPGatewayTimeout,\n    HTTPGone,\n    HTTPInsufficientStorage,\n    HTTPInternalServerError,\n    HTTPLengthRequired,\n    HTTPMethodNotAllowed,\n    HTTPMisdirectedRequest,\n    HTTPMove,\n    HTTPMovedPermanently,\n    HTTPMultipleChoices,\n    HTTPNetworkAuthenticationRequired,\n    HTTPNoContent,\n    HTTPNonAuthoritativeInformation,\n    HTTPNotAcceptable,\n    HTTPNotExtended,\n    HTTPNotFound,\n    HTTPNotImplemented,\n    HTTPNotModified,\n    HTTPOk,\n    HTTPPartialContent,\n    HTTPPaymentRequired,\n    HTTPPermanentRedirect,\n    HTTPPreconditionFailed,\n    HTTPPreconditionRequired,\n    HTTPProxyAuthenticationRequired,\n    HTTPRedirection,\n    HTTPRequestEntityTooLarge,\n    HTTPRequestHeaderFieldsTooLarge,\n    HTTPRequestRangeNotSatisfiable,\n    HTTPRequestTimeout,\n    HTTPRequestURITooLong,\n    HTTPResetContent,\n    HTTPSeeOther,\n    HTTPServerError,\n    HTTPServiceUnavailable,\n    HTTPSuccessful,\n    HTTPTemporaryRedirect,\n    HTTPTooManyRequests,\n    HTTPUnauthorized,\n    HTTPUnavailableForLegalReasons,\n    HTTPUnprocessableEntity,\n    HTTPUnsupportedMediaType,\n    HTTPUpgradeRequired,\n    HTTPUseProxy,\n    HTTPVariantAlsoNegotiates,\n    HTTPVersionNotSupported,\n    NotAppKeyWarning,\n)\nfrom .web_fileresponse import FileResponse\nfrom .web_log import AccessLogger\nfrom .web_middlewares import middleware, normalize_path_middleware\nfrom .web_protocol import PayloadAccessError, RequestHandler, RequestPayloadError\nfrom .web_request import BaseRequest, FileField, Request\nfrom .web_response import ContentCoding, Response, StreamResponse, json_response\nfrom .web_routedef import (\n    AbstractRouteDef,\n    RouteDef,\n    RouteTableDef,\n    StaticDef,\n    delete,\n    get,\n    head,\n    options,\n    patch,\n    post,\n    put,\n    route,\n    static,\n    view,\n)\nfrom .web_runner import (\n    AppRunner,\n    BaseRunner,\n    BaseSite,\n    GracefulExit,\n    NamedPipeSite,\n    ServerRunner,\n    SockSite,\n    TCPSite,\n    UnixSite,\n)\nfrom .web_server import Server\nfrom .web_urldispatcher import (\n    AbstractResource,\n    AbstractRoute,\n    DynamicResource,\n    PlainResource,\n    PrefixedSubAppResource,\n    Resource,\n    ResourceRoute,\n    StaticResource,\n    UrlDispatcher,\n    UrlMappingMatchInfo,\n    View,\n)\nfrom .web_ws import WebSocketReady, WebSocketResponse, WSMsgType\n\n__all__ = (\n    # web_app\n    \"AppKey\",\n    \"Application\",\n    \"CleanupError\",\n    # web_exceptions\n    \"NotAppKeyWarning\",\n    \"HTTPAccepted\",\n    \"HTTPBadGateway\",\n    \"HTTPBadRequest\",\n    \"HTTPClientError\",\n    \"HTTPConflict\",\n    \"HTTPCreated\",\n    \"HTTPError\",\n    \"HTTPException\",\n    \"HTTPExpectationFailed\",\n    \"HTTPFailedDependency\",\n    \"HTTPForbidden\",\n    \"HTTPFound\",\n    \"HTTPGatewayTimeout\",\n    \"HTTPGone\",\n    \"HTTPInsufficientStorage\",\n    \"HTTPInternalServerError\",\n    \"HTTPLengthRequired\",\n    \"HTTPMethodNotAllowed\",\n    \"HTTPMisdirectedRequest\",\n    \"HTTPMove\",\n    \"HTTPMovedPermanently\",\n    \"HTTPMultipleChoices\",\n    \"HTTPNetworkAuthenticationRequired\",\n    \"HTTPNoContent\",\n    \"HTTPNonAuthoritativeInformation\",\n    \"HTTPNotAcceptable\",\n    \"HTTPNotExtended\",\n    \"HTTPNotFound\",\n    \"HTTPNotImplemented\",\n    \"HTTPNotModified\",\n    \"HTTPOk\",\n    \"HTTPPartialContent\",\n    \"HTTPPaymentRequired\",\n    \"HTTPPermanentRedirect\",\n    \"HTTPPreconditionFailed\",\n    \"HTTPPreconditionRequired\",\n    \"HTTPProxyAuthenticationRequired\",\n    \"HTTPRedirection\",\n    \"HTTPRequestEntityTooLarge\",\n    \"HTTPRequestHeaderFieldsTooLarge\",\n    \"HTTPRequestRangeNotSatisfiable\",\n    \"HTTPRequestTimeout\",\n    \"HTTPRequestURITooLong\",\n    \"HTTPResetContent\",\n    \"HTTPSeeOther\",\n    \"HTTPServerError\",\n    \"HTTPServiceUnavailable\",\n    \"HTTPSuccessful\",\n    \"HTTPTemporaryRedirect\",\n    \"HTTPTooManyRequests\",\n    \"HTTPUnauthorized\",\n    \"HTTPUnavailableForLegalReasons\",\n    \"HTTPUnprocessableEntity\",\n    \"HTTPUnsupportedMediaType\",\n    \"HTTPUpgradeRequired\",\n    \"HTTPUseProxy\",\n    \"HTTPVariantAlsoNegotiates\",\n    \"HTTPVersionNotSupported\",\n    # web_fileresponse\n    \"FileResponse\",\n    # web_middlewares\n    \"middleware\",\n    \"normalize_path_middleware\",\n    # web_protocol\n    \"PayloadAccessError\",\n    \"RequestHandler\",\n    \"RequestPayloadError\",\n    # web_request\n    \"BaseRequest\",\n    \"FileField\",\n    \"Request\",\n    # web_response\n    \"ContentCoding\",\n    \"Response\",\n    \"StreamResponse\",\n    \"json_response\",\n    # web_routedef\n    \"AbstractRouteDef\",\n    \"RouteDef\",\n    \"RouteTableDef\",\n    \"StaticDef\",\n    \"delete\",\n    \"get\",\n    \"head\",\n    \"options\",\n    \"patch\",\n    \"post\",\n    \"put\",\n    \"route\",\n    \"static\",\n    \"view\",\n    # web_runner\n    \"AppRunner\",\n    \"BaseRunner\",\n    \"BaseSite\",\n    \"GracefulExit\",\n    \"ServerRunner\",\n    \"SockSite\",\n    \"TCPSite\",\n    \"UnixSite\",\n    \"NamedPipeSite\",\n    # web_server\n    \"Server\",\n    # web_urldispatcher\n    \"AbstractResource\",\n    \"AbstractRoute\",\n    \"DynamicResource\",\n    \"PlainResource\",\n    \"PrefixedSubAppResource\",\n    \"Resource\",\n    \"ResourceRoute\",\n    \"StaticResource\",\n    \"UrlDispatcher\",\n    \"UrlMappingMatchInfo\",\n    \"View\",\n    # web_ws\n    \"WebSocketReady\",\n    \"WebSocketResponse\",\n    \"WSMsgType\",\n    # web\n    \"run_app\",\n)\n\n\ntry:\n    from ssl import SSLContext\nexcept ImportError:  # pragma: no cover\n    SSLContext = Any  # type: ignore[misc,assignment]\n\n# Only display warning when using -Wdefault, -We, -X dev or similar.\nwarnings.filterwarnings(\"ignore\", category=NotAppKeyWarning, append=True)\n\nHostSequence = TypingIterable[str]\n\n\nasync def _run_app(\n    app: Union[Application, Awaitable[Application]],\n    *,\n    host: Optional[Union[str, HostSequence]] = None,\n    port: Optional[int] = None,\n    path: Union[PathLike, TypingIterable[PathLike], None] = None,\n    sock: Optional[Union[socket.socket, TypingIterable[socket.socket]]] = None,\n    shutdown_timeout: float = 60.0,\n    keepalive_timeout: float = 75.0,\n    ssl_context: Optional[SSLContext] = None,\n    print: Optional[Callable[..., None]] = print,\n    backlog: int = 128,\n    access_log_class: Type[AbstractAccessLogger] = AccessLogger,\n    access_log_format: str = AccessLogger.LOG_FORMAT,\n    access_log: Optional[logging.Logger] = access_logger,\n    handle_signals: bool = True,\n    reuse_address: Optional[bool] = None,\n    reuse_port: Optional[bool] = None,\n    handler_cancellation: bool = False,\n) -> None:\n    async def wait(\n        starting_tasks: \"WeakSet[asyncio.Task[object]]\", shutdown_timeout: float\n    ) -> None:\n        # Wait for pending tasks for a given time limit.\n        t = asyncio.current_task()\n        assert t is not None\n        starting_tasks.add(t)\n        with suppress(asyncio.TimeoutError):\n            await asyncio.wait_for(_wait(starting_tasks), timeout=shutdown_timeout)\n\n    async def _wait(exclude: \"WeakSet[asyncio.Task[object]]\") -> None:\n        t = asyncio.current_task()\n        assert t is not None\n        exclude.add(t)\n        while tasks := asyncio.all_tasks().difference(exclude):\n            await asyncio.wait(tasks)\n\n    # An internal function to actually do all dirty job for application running\n    if asyncio.iscoroutine(app):\n        app = await app\n\n    app = cast(Application, app)\n\n    runner = AppRunner(\n        app,\n        handle_signals=handle_signals,\n        access_log_class=access_log_class,\n        access_log_format=access_log_format,\n        access_log=access_log,\n        keepalive_timeout=keepalive_timeout,\n        shutdown_timeout=shutdown_timeout,\n        handler_cancellation=handler_cancellation,\n    )\n\n    await runner.setup()\n    # On shutdown we want to avoid waiting on tasks which run forever.\n    # It's very likely that all tasks which run forever will have been created by\n    # the time we have completed the application startup (in runner.setup()),\n    # so we just record all running tasks here and exclude them later.\n    starting_tasks: \"WeakSet[asyncio.Task[object]]\" = WeakSet(asyncio.all_tasks())\n    runner.shutdown_callback = partial(wait, starting_tasks, shutdown_timeout)\n\n    sites: List[BaseSite] = []\n\n    try:\n        if host is not None:\n            if isinstance(host, (str, bytes, bytearray, memoryview)):\n                sites.append(\n                    TCPSite(\n                        runner,\n                        host,\n                        port,\n                        ssl_context=ssl_context,\n                        backlog=backlog,\n                        reuse_address=reuse_address,\n                        reuse_port=reuse_port,\n                    )\n                )\n            else:\n                for h in host:\n                    sites.append(\n                        TCPSite(\n                            runner,\n                            h,\n                            port,\n                            ssl_context=ssl_context,\n                            backlog=backlog,\n                            reuse_address=reuse_address,\n                            reuse_port=reuse_port,\n                        )\n                    )\n        elif path is None and sock is None or port is not None:\n            sites.append(\n                TCPSite(\n                    runner,\n                    port=port,\n                    ssl_context=ssl_context,\n                    backlog=backlog,\n                    reuse_address=reuse_address,\n                    reuse_port=reuse_port,\n                )\n            )\n\n        if path is not None:\n            if isinstance(path, (str, os.PathLike)):\n                sites.append(\n                    UnixSite(\n                        runner,\n                        path,\n                        ssl_context=ssl_context,\n                        backlog=backlog,\n                    )\n                )\n            else:\n                for p in path:\n                    sites.append(\n                        UnixSite(\n                            runner,\n                            p,\n                            ssl_context=ssl_context,\n                            backlog=backlog,\n                        )\n                    )\n\n        if sock is not None:\n            if not isinstance(sock, Iterable):\n                sites.append(\n                    SockSite(\n                        runner,\n                        sock,\n                        ssl_context=ssl_context,\n                        backlog=backlog,\n                    )\n                )\n            else:\n                for s in sock:\n                    sites.append(\n                        SockSite(\n                            runner,\n                            s,\n                            ssl_context=ssl_context,\n                            backlog=backlog,\n                        )\n                    )\n        for site in sites:\n            await site.start()\n\n        if print:  # pragma: no branch\n            names = sorted(str(s.name) for s in runner.sites)\n            print(\n                \"======== Running on {} ========\\n\"\n                \"(Press CTRL+C to quit)\".format(\", \".join(names))\n            )\n\n        # sleep forever by 1 hour intervals,\n        while True:\n            await asyncio.sleep(3600)\n    finally:\n        await runner.cleanup()\n\n\ndef _cancel_tasks(\n    to_cancel: Set[\"asyncio.Task[Any]\"], loop: asyncio.AbstractEventLoop\n) -> None:\n    if not to_cancel:\n        return\n\n    for task in to_cancel:\n        task.cancel()\n\n    loop.run_until_complete(asyncio.gather(*to_cancel, return_exceptions=True))\n\n    for task in to_cancel:\n        if task.cancelled():\n            continue\n        if task.exception() is not None:\n            loop.call_exception_handler(\n                {\n                    \"message\": \"unhandled exception during asyncio.run() shutdown\",\n                    \"exception\": task.exception(),\n                    \"task\": task,\n                }\n            )\n\n\ndef run_app(\n    app: Union[Application, Awaitable[Application]],\n    *,\n    debug: bool = False,\n    host: Optional[Union[str, HostSequence]] = None,\n    port: Optional[int] = None,\n    path: Union[PathLike, TypingIterable[PathLike], None] = None,\n    sock: Optional[Union[socket.socket, TypingIterable[socket.socket]]] = None,\n    shutdown_timeout: float = 60.0,\n    keepalive_timeout: float = 75.0,\n    ssl_context: Optional[SSLContext] = None,\n    print: Optional[Callable[..., None]] = print,\n    backlog: int = 128,\n    access_log_class: Type[AbstractAccessLogger] = AccessLogger,\n    access_log_format: str = AccessLogger.LOG_FORMAT,\n    access_log: Optional[logging.Logger] = access_logger,\n    handle_signals: bool = True,\n    reuse_address: Optional[bool] = None,\n    reuse_port: Optional[bool] = None,\n    handler_cancellation: bool = False,\n    loop: Optional[asyncio.AbstractEventLoop] = None,\n) -> None:\n    \"\"\"Run an app locally\"\"\"\n    if loop is None:\n        loop = asyncio.new_event_loop()\n    loop.set_debug(debug)\n\n    # Configure if and only if in debugging mode and using the default logger\n    if loop.get_debug() and access_log and access_log.name == \"aiohttp.access\":\n        if access_log.level == logging.NOTSET:\n            access_log.setLevel(logging.DEBUG)\n        if not access_log.hasHandlers():\n            access_log.addHandler(logging.StreamHandler())\n\n    main_task = loop.create_task(\n        _run_app(\n            app,\n            host=host,\n            port=port,\n            path=path,\n            sock=sock,\n            shutdown_timeout=shutdown_timeout,\n            keepalive_timeout=keepalive_timeout,\n            ssl_context=ssl_context,\n            print=print,\n            backlog=backlog,\n            access_log_class=access_log_class,\n            access_log_format=access_log_format,\n            access_log=access_log,\n            handle_signals=handle_signals,\n            reuse_address=reuse_address,\n            reuse_port=reuse_port,\n            handler_cancellation=handler_cancellation,\n        )\n    )\n\n    try:\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(main_task)\n    except (GracefulExit, KeyboardInterrupt):  # pragma: no cover\n        pass\n    finally:\n        _cancel_tasks({main_task}, loop)\n        _cancel_tasks(asyncio.all_tasks(loop), loop)\n        loop.run_until_complete(loop.shutdown_asyncgens())\n        loop.close()\n        asyncio.set_event_loop(None)\n\n\ndef main(argv: List[str]) -> None:\n    arg_parser = ArgumentParser(\n        description=\"aiohttp.web Application server\", prog=\"aiohttp.web\"\n    )\n    arg_parser.add_argument(\n        \"entry_func\",\n        help=(\n            \"Callable returning the `aiohttp.web.Application` instance to \"\n            \"run. Should be specified in the 'module:function' syntax.\"\n        ),\n        metavar=\"entry-func\",\n    )\n    arg_parser.add_argument(\n        \"-H\",\n        \"--hostname\",\n        help=\"TCP/IP hostname to serve on (default: %(default)r)\",\n        default=\"localhost\",\n    )\n    arg_parser.add_argument(\n        \"-P\",\n        \"--port\",\n        help=\"TCP/IP port to serve on (default: %(default)r)\",\n        type=int,\n        default=\"8080\",\n    )\n    arg_parser.add_argument(\n        \"-U\",\n        \"--path\",\n        help=\"Unix file system path to serve on. Specifying a path will cause \"\n        \"hostname and port arguments to be ignored.\",\n    )\n    args, extra_argv = arg_parser.parse_known_args(argv)\n\n    # Import logic\n    mod_str, _, func_str = args.entry_func.partition(\":\")\n    if not func_str or not mod_str:\n        arg_parser.error(\"'entry-func' not in 'module:function' syntax\")\n    if mod_str.startswith(\".\"):\n        arg_parser.error(\"relative module names not supported\")\n    try:\n        module = import_module(mod_str)\n    except ImportError as ex:\n        arg_parser.error(f\"unable to import {mod_str}: {ex}\")\n    try:\n        func = getattr(module, func_str)\n    except AttributeError:\n        arg_parser.error(f\"module {mod_str!r} has no attribute {func_str!r}\")\n\n    # Compatibility logic\n    if args.path is not None and not hasattr(socket, \"AF_UNIX\"):\n        arg_parser.error(\n            \"file system paths not supported by your operating\" \" environment\"\n        )\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    app = func(extra_argv)\n    run_app(app, host=args.hostname, port=args.port, path=args.path)\n    arg_parser.exit(message=\"Stopped\\n\")\n\n\nif __name__ == \"__main__\":  # pragma: no branch\n    main(sys.argv[1:])  # pragma: no cover\n", "aiohttp/web_server.py": "\"\"\"Low level HTTP server.\"\"\"\n\nimport asyncio\nimport warnings\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional  # noqa\n\nfrom .abc import AbstractStreamWriter\nfrom .http_parser import RawRequestMessage\nfrom .streams import StreamReader\nfrom .web_protocol import RequestHandler, _RequestFactory, _RequestHandler\nfrom .web_request import BaseRequest\n\n__all__ = (\"Server\",)\n\n\nclass Server:\n    def __init__(\n        self,\n        handler: _RequestHandler,\n        *,\n        request_factory: Optional[_RequestFactory] = None,\n        debug: Optional[bool] = None,\n        handler_cancellation: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        if debug is not None:\n            warnings.warn(\n                \"debug argument is no-op since 4.0 \" \"and scheduled for removal in 5.0\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self._loop = asyncio.get_running_loop()\n        self._connections: Dict[RequestHandler, asyncio.Transport] = {}\n        self._kwargs = kwargs\n        self.requests_count = 0\n        self.request_handler = handler\n        self.request_factory = request_factory or self._make_request\n        self.handler_cancellation = handler_cancellation\n\n    @property\n    def connections(self) -> List[RequestHandler]:\n        return list(self._connections.keys())\n\n    def connection_made(\n        self, handler: RequestHandler, transport: asyncio.Transport\n    ) -> None:\n        self._connections[handler] = transport\n\n    def connection_lost(\n        self, handler: RequestHandler, exc: Optional[BaseException] = None\n    ) -> None:\n        if handler in self._connections:\n            del self._connections[handler]\n\n    def _make_request(\n        self,\n        message: RawRequestMessage,\n        payload: StreamReader,\n        protocol: RequestHandler,\n        writer: AbstractStreamWriter,\n        task: \"asyncio.Task[None]\",\n    ) -> BaseRequest:\n        return BaseRequest(message, payload, protocol, writer, task, self._loop)\n\n    def pre_shutdown(self) -> None:\n        for conn in self._connections:\n            conn.close()\n\n    async def shutdown(self, timeout: Optional[float] = None) -> None:\n        coros = (conn.shutdown(timeout) for conn in self._connections)\n        await asyncio.gather(*coros)\n        self._connections.clear()\n\n    def __call__(self) -> RequestHandler:\n        try:\n            return RequestHandler(self, loop=self._loop, **self._kwargs)\n        except TypeError:\n            # Failsafe creation: remove all custom handler_args\n            kwargs = {\n                k: v\n                for k, v in self._kwargs.items()\n                if k in [\"debug\", \"access_log_class\"]\n            }\n            return RequestHandler(self, loop=self._loop, **kwargs)\n", "aiohttp/resolver.py": "import asyncio\nimport socket\nimport sys\nfrom typing import Any, List, Tuple, Type, Union\n\nfrom .abc import AbstractResolver, ResolveResult\n\n__all__ = (\"ThreadedResolver\", \"AsyncResolver\", \"DefaultResolver\")\n\ntry:\n    import aiodns\n\n    # aiodns_default = hasattr(aiodns.DNSResolver, 'getaddrinfo')\nexcept ImportError:  # pragma: no cover\n    aiodns = None  # type: ignore[assignment]\n\n\naiodns_default = False\n\n_NUMERIC_SOCKET_FLAGS = socket.AI_NUMERICHOST | socket.AI_NUMERICSERV\n_SUPPORTS_SCOPE_ID = sys.version_info >= (3, 9, 0)\n\n\nclass ThreadedResolver(AbstractResolver):\n    \"\"\"Threaded resolver.\n\n    Uses an Executor for synchronous getaddrinfo() calls.\n    concurrent.futures.ThreadPoolExecutor is used by default.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._loop = asyncio.get_running_loop()\n\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        infos = await self._loop.getaddrinfo(\n            host,\n            port,\n            type=socket.SOCK_STREAM,\n            family=family,\n            flags=socket.AI_ADDRCONFIG,\n        )\n\n        hosts: List[ResolveResult] = []\n        for family, _, proto, _, address in infos:\n            if family == socket.AF_INET6:\n                if len(address) < 3:\n                    # IPv6 is not supported by Python build,\n                    # or IPv6 is not enabled in the host\n                    continue\n                if address[3] and _SUPPORTS_SCOPE_ID:\n                    # This is essential for link-local IPv6 addresses.\n                    # LL IPv6 is a VERY rare case. Strictly speaking, we should use\n                    # getnameinfo() unconditionally, but performance makes sense.\n                    resolved_host, _port = await self._loop.getnameinfo(\n                        address, _NUMERIC_SOCKET_FLAGS\n                    )\n                    port = int(_port)\n                else:\n                    resolved_host, port = address[:2]\n            else:  # IPv4\n                assert family == socket.AF_INET\n                resolved_host, port = address  # type: ignore[misc]\n            hosts.append(\n                ResolveResult(\n                    hostname=host,\n                    host=resolved_host,\n                    port=port,\n                    family=family,\n                    proto=proto,\n                    flags=_NUMERIC_SOCKET_FLAGS,\n                )\n            )\n\n        return hosts\n\n    async def close(self) -> None:\n        pass\n\n\nclass AsyncResolver(AbstractResolver):\n    \"\"\"Use the `aiodns` package to make asynchronous DNS lookups\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        if aiodns is None:\n            raise RuntimeError(\"Resolver requires aiodns library\")\n\n        self._resolver = aiodns.DNSResolver(*args, **kwargs)\n\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        try:\n            resp = await self._resolver.getaddrinfo(\n                host,\n                port=port,\n                type=socket.SOCK_STREAM,\n                family=family,\n                flags=socket.AI_ADDRCONFIG,\n            )\n        except aiodns.error.DNSError as exc:\n            msg = exc.args[1] if len(exc.args) >= 1 else \"DNS lookup failed\"\n            raise OSError(msg) from exc\n        hosts: List[ResolveResult] = []\n        for node in resp.nodes:\n            address: Union[Tuple[bytes, int], Tuple[bytes, int, int, int]] = node.addr\n            family = node.family\n            if family == socket.AF_INET6:\n                if len(address) > 3 and address[3] and _SUPPORTS_SCOPE_ID:\n                    # This is essential for link-local IPv6 addresses.\n                    # LL IPv6 is a VERY rare case. Strictly speaking, we should use\n                    # getnameinfo() unconditionally, but performance makes sense.\n                    result = await self._resolver.getnameinfo(\n                        (address[0].decode(\"ascii\"), *address[1:]),\n                        _NUMERIC_SOCKET_FLAGS,\n                    )\n                    resolved_host = result.node\n                else:\n                    resolved_host = address[0].decode(\"ascii\")\n                    port = address[1]\n            else:  # IPv4\n                assert family == socket.AF_INET\n                resolved_host = address[0].decode(\"ascii\")\n                port = address[1]\n            hosts.append(\n                ResolveResult(\n                    hostname=host,\n                    host=resolved_host,\n                    port=port,\n                    family=family,\n                    proto=0,\n                    flags=_NUMERIC_SOCKET_FLAGS,\n                )\n            )\n\n        if not hosts:\n            raise OSError(\"DNS lookup failed\")\n\n        return hosts\n\n    async def close(self) -> None:\n        self._resolver.cancel()\n\n\n_DefaultType = Type[Union[AsyncResolver, ThreadedResolver]]\nDefaultResolver: _DefaultType = AsyncResolver if aiodns_default else ThreadedResolver\n", "aiohttp/locks.py": "import asyncio\nimport collections\nfrom typing import Any, Deque, Optional\n\n\nclass EventResultOrError:\n    \"\"\"Event asyncio lock helper class.\n\n    Wraps the Event asyncio lock allowing either to awake the\n    locked Tasks without any error or raising an exception.\n\n    thanks to @vorpalsmith for the simple design.\n    \"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop = loop\n        self._exc: Optional[BaseException] = None\n        self._event = asyncio.Event()\n        self._waiters: Deque[asyncio.Future[Any]] = collections.deque()\n\n    def set(self, exc: Optional[BaseException] = None) -> None:\n        self._exc = exc\n        self._event.set()\n\n    async def wait(self) -> Any:\n        waiter = self._loop.create_task(self._event.wait())\n        self._waiters.append(waiter)\n        try:\n            val = await waiter\n        finally:\n            self._waiters.remove(waiter)\n\n        if self._exc is not None:\n            raise self._exc\n\n        return val\n\n    def cancel(self) -> None:\n        \"\"\"Cancel all waiters\"\"\"\n        for waiter in self._waiters:\n            waiter.cancel()\n", "aiohttp/client_ws.py": "\"\"\"WebSocket client for asyncio.\"\"\"\n\nimport asyncio\nimport dataclasses\nimport sys\nfrom typing import Any, Final, Optional, cast\n\nfrom .client_exceptions import ClientError\nfrom .client_reqrep import ClientResponse\nfrom .helpers import call_later, set_result\nfrom .http import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n)\nfrom .http_websocket import WebSocketWriter  # WSMessage\nfrom .streams import EofStream, FlowControlDataQueue\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    DEFAULT_JSON_ENCODER,\n    JSONDecoder,\n    JSONEncoder,\n)\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\n\n@dataclasses.dataclass(frozen=True)\nclass ClientWSTimeout:\n    ws_receive: Optional[float] = None\n    ws_close: Optional[float] = None\n\n\nDEFAULT_WS_CLIENT_TIMEOUT: Final[ClientWSTimeout] = ClientWSTimeout(\n    ws_receive=None, ws_close=10.0\n)\n\n\nclass ClientWebSocketResponse:\n    def __init__(\n        self,\n        reader: \"FlowControlDataQueue[WSMessage]\",\n        writer: WebSocketWriter,\n        protocol: Optional[str],\n        response: ClientResponse,\n        timeout: ClientWSTimeout,\n        autoclose: bool,\n        autoping: bool,\n        loop: asyncio.AbstractEventLoop,\n        *,\n        heartbeat: Optional[float] = None,\n        compress: int = 0,\n        client_notakeover: bool = False,\n    ) -> None:\n        self._response = response\n        self._conn = response.connection\n\n        self._writer = writer\n        self._reader = reader\n        self._protocol = protocol\n        self._closed = False\n        self._closing = False\n        self._close_code: Optional[int] = None\n        self._timeout: ClientWSTimeout = timeout\n        self._autoclose = autoclose\n        self._autoping = autoping\n        self._heartbeat = heartbeat\n        self._heartbeat_cb: Optional[asyncio.TimerHandle] = None\n        if heartbeat is not None:\n            self._pong_heartbeat = heartbeat / 2.0\n        self._pong_response_cb: Optional[asyncio.TimerHandle] = None\n        self._loop = loop\n        self._waiting: Optional[asyncio.Future[bool]] = None\n        self._exception: Optional[BaseException] = None\n        self._compress = compress\n        self._client_notakeover = client_notakeover\n\n        self._reset_heartbeat()\n\n    def _cancel_heartbeat(self) -> None:\n        if self._pong_response_cb is not None:\n            self._pong_response_cb.cancel()\n            self._pong_response_cb = None\n\n        if self._heartbeat_cb is not None:\n            self._heartbeat_cb.cancel()\n            self._heartbeat_cb = None\n\n    def _reset_heartbeat(self) -> None:\n        self._cancel_heartbeat()\n\n        if self._heartbeat is not None:\n            self._heartbeat_cb = call_later(\n                self._send_heartbeat,\n                self._heartbeat,\n                self._loop,\n                timeout_ceil_threshold=(\n                    self._conn._connector._timeout_ceil_threshold\n                    if self._conn is not None\n                    else 5\n                ),\n            )\n\n    def _send_heartbeat(self) -> None:\n        if self._heartbeat is not None and not self._closed:\n            # fire-and-forget a task is not perfect but maybe ok for\n            # sending ping. Otherwise we need a long-living heartbeat\n            # task in the class.\n            self._loop.create_task(self._writer.ping())  # type: ignore[unused-awaitable]\n\n            if self._pong_response_cb is not None:\n                self._pong_response_cb.cancel()\n            self._pong_response_cb = call_later(\n                self._pong_not_received,\n                self._pong_heartbeat,\n                self._loop,\n                timeout_ceil_threshold=(\n                    self._conn._connector._timeout_ceil_threshold\n                    if self._conn is not None\n                    else 5\n                ),\n            )\n\n    def _pong_not_received(self) -> None:\n        if not self._closed:\n            self._closed = True\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n            self._exception = asyncio.TimeoutError()\n            self._response.close()\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    @property\n    def close_code(self) -> Optional[int]:\n        return self._close_code\n\n    @property\n    def protocol(self) -> Optional[str]:\n        return self._protocol\n\n    @property\n    def compress(self) -> int:\n        return self._compress\n\n    @property\n    def client_notakeover(self) -> bool:\n        return self._client_notakeover\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        \"\"\"extra info from connection transport\"\"\"\n        conn = self._response.connection\n        if conn is None:\n            return default\n        transport = conn.transport\n        if transport is None:\n            return default\n        return transport.get_extra_info(name, default)\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    async def ping(self, message: bytes = b\"\") -> None:\n        await self._writer.ping(message)\n\n    async def pong(self, message: bytes = b\"\") -> None:\n        await self._writer.pong(message)\n\n    async def send_str(self, data: str, compress: Optional[int] = None) -> None:\n        if not isinstance(data, str):\n            raise TypeError(\"data argument must be str (%r)\" % type(data))\n        await self._writer.send(data, binary=False, compress=compress)\n\n    async def send_bytes(self, data: bytes, compress: Optional[int] = None) -> None:\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data argument must be byte-ish (%r)\" % type(data))\n        await self._writer.send(data, binary=True, compress=compress)\n\n    async def send_json(\n        self,\n        data: Any,\n        compress: Optional[int] = None,\n        *,\n        dumps: JSONEncoder = DEFAULT_JSON_ENCODER,\n    ) -> None:\n        await self.send_str(dumps(data), compress=compress)\n\n    async def close(self, *, code: int = WSCloseCode.OK, message: bytes = b\"\") -> bool:\n        # we need to break `receive()` cycle first,\n        # `close()` may be called from different task\n        if self._waiting is not None and not self._closing:\n            self._closing = True\n            self._reader.feed_data(WS_CLOSING_MESSAGE)\n            await self._waiting\n\n        if not self._closed:\n            self._cancel_heartbeat()\n            self._closed = True\n            try:\n                await self._writer.close(code, message)\n            except asyncio.CancelledError:\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                self._response.close()\n                raise\n            except Exception as exc:\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                self._exception = exc\n                self._response.close()\n                return True\n\n            if self._close_code:\n                self._response.close()\n                return True\n\n            while True:\n                try:\n                    async with async_timeout.timeout(self._timeout.ws_close):\n                        msg = await self._reader.read()\n                except asyncio.CancelledError:\n                    self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                    self._response.close()\n                    raise\n                except Exception as exc:\n                    self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                    self._exception = exc\n                    self._response.close()\n                    return True\n\n                if msg.type == WSMsgType.CLOSE:\n                    self._close_code = msg.data\n                    self._response.close()\n                    return True\n        else:\n            return False\n\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\n        while True:\n            if self._waiting is not None:\n                raise RuntimeError(\"Concurrent call to receive() is not allowed\")\n\n            if self._closed:\n                return WS_CLOSED_MESSAGE\n            elif self._closing:\n                await self.close()\n                return WS_CLOSED_MESSAGE\n\n            try:\n                self._waiting = self._loop.create_future()\n                try:\n                    async with async_timeout.timeout(\n                        timeout or self._timeout.ws_receive\n                    ):\n                        msg = await self._reader.read()\n                    self._reset_heartbeat()\n                finally:\n                    waiter = self._waiting\n                    self._waiting = None\n                    set_result(waiter, True)\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                raise\n            except EofStream:\n                self._close_code = WSCloseCode.OK\n                await self.close()\n                return WSMessage(WSMsgType.CLOSED, None, None)\n            except ClientError:\n                self._closed = True\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                return WS_CLOSED_MESSAGE\n            except WebSocketError as exc:\n                self._close_code = exc.code\n                await self.close(code=exc.code)\n                return WSMessage(WSMsgType.ERROR, exc, None)\n            except Exception as exc:\n                self._exception = exc\n                self._closing = True\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                await self.close()\n                return WSMessage(WSMsgType.ERROR, exc, None)\n\n            if msg.type == WSMsgType.CLOSE:\n                self._closing = True\n                self._close_code = msg.data\n                # Could be closed elsewhere while awaiting reader\n                if not self._closed and self._autoclose:  # type: ignore[redundant-expr]\n                    await self.close()\n            elif msg.type == WSMsgType.CLOSING:\n                self._closing = True\n            elif msg.type == WSMsgType.PING and self._autoping:\n                await self.pong(msg.data)\n                continue\n            elif msg.type == WSMsgType.PONG and self._autoping:\n                continue\n\n            return msg\n\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\n        msg = await self.receive(timeout)\n        if msg.type != WSMsgType.TEXT:\n            raise TypeError(f\"Received message {msg.type}:{msg.data!r} is not str\")\n        return cast(str, msg.data)\n\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\n        msg = await self.receive(timeout)\n        if msg.type != WSMsgType.BINARY:\n            raise TypeError(f\"Received message {msg.type}:{msg.data!r} is not bytes\")\n        return cast(bytes, msg.data)\n\n    async def receive_json(\n        self,\n        *,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        timeout: Optional[float] = None,\n    ) -> Any:\n        data = await self.receive_str(timeout=timeout)\n        return loads(data)\n\n    def __aiter__(self) -> \"ClientWebSocketResponse\":\n        return self\n\n    async def __anext__(self) -> WSMessage:\n        msg = await self.receive()\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\n            raise StopAsyncIteration\n        return msg\n", "aiohttp/web_response.py": "import asyncio\nimport collections.abc\nimport datetime\nimport enum\nimport json\nimport math\nimport time\nimport warnings\nfrom concurrent.futures import Executor\nfrom http import HTTPStatus\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    MutableMapping,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom multidict import CIMultiDict, istr\n\nfrom . import hdrs, payload\nfrom .abc import AbstractStreamWriter\nfrom .compression_utils import ZLibCompressor\nfrom .helpers import (\n    ETAG_ANY,\n    QUOTED_ETAG_RE,\n    CookieMixin,\n    ETag,\n    HeadersMixin,\n    must_be_empty_body,\n    parse_http_date,\n    populate_with_cookies,\n    rfc822_formatted_time,\n    sentinel,\n    should_remove_content_length,\n    validate_etag_value,\n)\nfrom .http import SERVER_SOFTWARE, HttpVersion10, HttpVersion11\nfrom .payload import Payload\nfrom .typedefs import JSONEncoder, LooseHeaders\n\n__all__ = (\"ContentCoding\", \"StreamResponse\", \"Response\", \"json_response\")\n\n\nif TYPE_CHECKING:\n    from .web_request import BaseRequest\n\n    BaseClass = MutableMapping[str, Any]\nelse:\n    BaseClass = collections.abc.MutableMapping\n\n\n# TODO(py311): Convert to StrEnum for wider use\nclass ContentCoding(enum.Enum):\n    # The content codings that we have support for.\n    #\n    # Additional registered codings are listed at:\n    # https://www.iana.org/assignments/http-parameters/http-parameters.xhtml#content-coding\n    deflate = \"deflate\"\n    gzip = \"gzip\"\n    identity = \"identity\"\n\n\n############################################################\n# HTTP Response classes\n############################################################\n\n\nclass StreamResponse(BaseClass, HeadersMixin, CookieMixin):\n    __slots__ = (\n        \"_length_check\",\n        \"_body\",\n        \"_keep_alive\",\n        \"_chunked\",\n        \"_compression\",\n        \"_compression_force\",\n        \"_req\",\n        \"_payload_writer\",\n        \"_eof_sent\",\n        \"_must_be_empty_body\",\n        \"_body_length\",\n        \"_state\",\n        \"_headers\",\n        \"_status\",\n        \"_reason\",\n        \"_cookies\",\n        \"__weakref__\",\n    )\n\n    def __init__(\n        self,\n        *,\n        status: int = 200,\n        reason: Optional[str] = None,\n        headers: Optional[LooseHeaders] = None,\n    ) -> None:\n        super().__init__()\n        self._length_check = True\n        self._body = None\n        self._keep_alive: Optional[bool] = None\n        self._chunked = False\n        self._compression = False\n        self._compression_force: Optional[ContentCoding] = None\n\n        self._req: Optional[BaseRequest] = None\n        self._payload_writer: Optional[AbstractStreamWriter] = None\n        self._eof_sent = False\n        self._must_be_empty_body: Optional[bool] = None\n        self._body_length = 0\n        self._state: Dict[str, Any] = {}\n\n        if headers is not None:\n            self._headers: CIMultiDict[str] = CIMultiDict(headers)\n        else:\n            self._headers = CIMultiDict()\n\n        self.set_status(status, reason)\n\n    @property\n    def prepared(self) -> bool:\n        return self._payload_writer is not None\n\n    @property\n    def task(self) -> \"Optional[asyncio.Task[None]]\":\n        if self._req:\n            return self._req.task\n        else:\n            return None\n\n    @property\n    def status(self) -> int:\n        return self._status\n\n    @property\n    def chunked(self) -> bool:\n        return self._chunked\n\n    @property\n    def compression(self) -> bool:\n        return self._compression\n\n    @property\n    def reason(self) -> str:\n        return self._reason\n\n    def set_status(\n        self,\n        status: int,\n        reason: Optional[str] = None,\n    ) -> None:\n        assert not self.prepared, (\n            \"Cannot change the response status code after \" \"the headers have been sent\"\n        )\n        self._status = int(status)\n        if reason is None:\n            try:\n                reason = HTTPStatus(self._status).phrase\n            except ValueError:\n                reason = \"\"\n        self._reason = reason\n\n    @property\n    def keep_alive(self) -> Optional[bool]:\n        return self._keep_alive\n\n    def force_close(self) -> None:\n        self._keep_alive = False\n\n    @property\n    def body_length(self) -> int:\n        return self._body_length\n\n    def enable_chunked_encoding(self) -> None:\n        \"\"\"Enables automatic chunked transfer encoding.\"\"\"\n        self._chunked = True\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            raise RuntimeError(\n                \"You can't enable chunked encoding when \" \"a content length is set\"\n            )\n\n    def enable_compression(self, force: Optional[ContentCoding] = None) -> None:\n        \"\"\"Enables response compression encoding.\"\"\"\n        self._compression = True\n        self._compression_force = force\n\n    @property\n    def headers(self) -> \"CIMultiDict[str]\":\n        return self._headers\n\n    @property\n    def content_length(self) -> Optional[int]:\n        # Just a placeholder for adding setter\n        return super().content_length\n\n    @content_length.setter\n    def content_length(self, value: Optional[int]) -> None:\n        if value is not None:\n            value = int(value)\n            if self._chunked:\n                raise RuntimeError(\n                    \"You can't set content length when \" \"chunked encoding is enable\"\n                )\n            self._headers[hdrs.CONTENT_LENGTH] = str(value)\n        else:\n            self._headers.pop(hdrs.CONTENT_LENGTH, None)\n\n    @property\n    def content_type(self) -> str:\n        # Just a placeholder for adding setter\n        return super().content_type\n\n    @content_type.setter\n    def content_type(self, value: str) -> None:\n        self.content_type  # read header values if needed\n        self._content_type = str(value)\n        self._generate_content_type_header()\n\n    @property\n    def charset(self) -> Optional[str]:\n        # Just a placeholder for adding setter\n        return super().charset\n\n    @charset.setter\n    def charset(self, value: Optional[str]) -> None:\n        ctype = self.content_type  # read header values if needed\n        if ctype == \"application/octet-stream\":\n            raise RuntimeError(\n                \"Setting charset for application/octet-stream \"\n                \"doesn't make sense, setup content_type first\"\n            )\n        assert self._content_dict is not None\n        if value is None:\n            self._content_dict.pop(\"charset\", None)\n        else:\n            self._content_dict[\"charset\"] = str(value).lower()\n        self._generate_content_type_header()\n\n    @property\n    def last_modified(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of Last-Modified HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        return parse_http_date(self._headers.get(hdrs.LAST_MODIFIED))\n\n    @last_modified.setter\n    def last_modified(\n        self, value: Optional[Union[int, float, datetime.datetime, str]]\n    ) -> None:\n        if value is None:\n            self._headers.pop(hdrs.LAST_MODIFIED, None)\n        elif isinstance(value, (int, float)):\n            self._headers[hdrs.LAST_MODIFIED] = time.strftime(\n                \"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(math.ceil(value))\n            )\n        elif isinstance(value, datetime.datetime):\n            self._headers[hdrs.LAST_MODIFIED] = time.strftime(\n                \"%a, %d %b %Y %H:%M:%S GMT\", value.utctimetuple()\n            )\n        elif isinstance(value, str):\n            self._headers[hdrs.LAST_MODIFIED] = value\n\n    @property\n    def etag(self) -> Optional[ETag]:\n        quoted_value = self._headers.get(hdrs.ETAG)\n        if not quoted_value:\n            return None\n        elif quoted_value == ETAG_ANY:\n            return ETag(value=ETAG_ANY)\n        match = QUOTED_ETAG_RE.fullmatch(quoted_value)\n        if not match:\n            return None\n        is_weak, value = match.group(1, 2)\n        return ETag(\n            is_weak=bool(is_weak),\n            value=value,\n        )\n\n    @etag.setter\n    def etag(self, value: Optional[Union[ETag, str]]) -> None:\n        if value is None:\n            self._headers.pop(hdrs.ETAG, None)\n        elif (isinstance(value, str) and value == ETAG_ANY) or (\n            isinstance(value, ETag) and value.value == ETAG_ANY\n        ):\n            self._headers[hdrs.ETAG] = ETAG_ANY\n        elif isinstance(value, str):\n            validate_etag_value(value)\n            self._headers[hdrs.ETAG] = f'\"{value}\"'\n        elif isinstance(value, ETag) and isinstance(value.value, str):  # type: ignore[redundant-expr]\n            validate_etag_value(value.value)\n            hdr_value = f'W/\"{value.value}\"' if value.is_weak else f'\"{value.value}\"'\n            self._headers[hdrs.ETAG] = hdr_value\n        else:\n            raise ValueError(\n                f\"Unsupported etag type: {type(value)}. \"\n                f\"etag must be str, ETag or None\"\n            )\n\n    def _generate_content_type_header(\n        self, CONTENT_TYPE: istr = hdrs.CONTENT_TYPE\n    ) -> None:\n        assert self._content_dict is not None\n        assert self._content_type is not None\n        params = \"; \".join(f\"{k}={v}\" for k, v in self._content_dict.items())\n        if params:\n            ctype = self._content_type + \"; \" + params\n        else:\n            ctype = self._content_type\n        self._headers[CONTENT_TYPE] = ctype\n\n    async def _do_start_compression(self, coding: ContentCoding) -> None:\n        if coding != ContentCoding.identity:\n            assert self._payload_writer is not None\n            self._headers[hdrs.CONTENT_ENCODING] = coding.value\n            self._payload_writer.enable_compression(coding.value)\n            # Compressed payload may have different content length,\n            # remove the header\n            self._headers.popall(hdrs.CONTENT_LENGTH, None)\n\n    async def _start_compression(self, request: \"BaseRequest\") -> None:\n        if self._compression_force:\n            await self._do_start_compression(self._compression_force)\n        else:\n            # Encoding comparisons should be case-insensitive\n            # https://www.rfc-editor.org/rfc/rfc9110#section-8.4.1\n            accept_encoding = request.headers.get(hdrs.ACCEPT_ENCODING, \"\").lower()\n            for coding in ContentCoding:\n                if coding.value in accept_encoding:\n                    await self._do_start_compression(coding)\n                    return\n\n    async def prepare(self, request: \"BaseRequest\") -> Optional[AbstractStreamWriter]:\n        if self._eof_sent:\n            return None\n        if self._payload_writer is not None:\n            return self._payload_writer\n        self._must_be_empty_body = must_be_empty_body(request.method, self.status)\n        return await self._start(request)\n\n    async def _start(self, request: \"BaseRequest\") -> AbstractStreamWriter:\n        self._req = request\n        writer = self._payload_writer = request._payload_writer\n\n        await self._prepare_headers()\n        await request._prepare_hook(self)\n        await self._write_headers()\n\n        return writer\n\n    async def _prepare_headers(self) -> None:\n        request = self._req\n        assert request is not None\n        writer = self._payload_writer\n        assert writer is not None\n        keep_alive = self._keep_alive\n        if keep_alive is None:\n            keep_alive = request.keep_alive\n        self._keep_alive = keep_alive\n\n        version = request.version\n\n        headers = self._headers\n        populate_with_cookies(headers, self.cookies)\n\n        if self._compression:\n            await self._start_compression(request)\n\n        if self._chunked:\n            if version != HttpVersion11:\n                raise RuntimeError(\n                    \"Using chunked encoding is forbidden \"\n                    \"for HTTP/{0.major}.{0.minor}\".format(request.version)\n                )\n            if not self._must_be_empty_body:\n                writer.enable_chunking()\n                headers[hdrs.TRANSFER_ENCODING] = \"chunked\"\n            if hdrs.CONTENT_LENGTH in headers:\n                del headers[hdrs.CONTENT_LENGTH]\n        elif self._length_check:\n            writer.length = self.content_length\n            if writer.length is None:\n                if version >= HttpVersion11:\n                    if not self._must_be_empty_body:\n                        writer.enable_chunking()\n                        headers[hdrs.TRANSFER_ENCODING] = \"chunked\"\n                elif not self._must_be_empty_body:\n                    keep_alive = False\n\n        # HTTP 1.1: https://tools.ietf.org/html/rfc7230#section-3.3.2\n        # HTTP 1.0: https://tools.ietf.org/html/rfc1945#section-10.4\n        if self._must_be_empty_body:\n            if hdrs.CONTENT_LENGTH in headers and should_remove_content_length(\n                request.method, self.status\n            ):\n                del headers[hdrs.CONTENT_LENGTH]\n            # https://datatracker.ietf.org/doc/html/rfc9112#section-6.1-10\n            # https://datatracker.ietf.org/doc/html/rfc9112#section-6.1-13\n            if hdrs.TRANSFER_ENCODING in headers:\n                del headers[hdrs.TRANSFER_ENCODING]\n        else:\n            headers.setdefault(hdrs.CONTENT_TYPE, \"application/octet-stream\")\n        headers.setdefault(hdrs.DATE, rfc822_formatted_time())\n        headers.setdefault(hdrs.SERVER, SERVER_SOFTWARE)\n\n        # connection header\n        if hdrs.CONNECTION not in headers:\n            if keep_alive:\n                if version == HttpVersion10:\n                    headers[hdrs.CONNECTION] = \"keep-alive\"\n            else:\n                if version == HttpVersion11:\n                    headers[hdrs.CONNECTION] = \"close\"\n\n    async def _write_headers(self) -> None:\n        request = self._req\n        assert request is not None\n        writer = self._payload_writer\n        assert writer is not None\n        # status line\n        version = request.version\n        status_line = \"HTTP/{}.{} {} {}\".format(\n            version[0], version[1], self._status, self._reason\n        )\n        await writer.write_headers(status_line, self._headers)\n\n    async def write(self, data: bytes) -> None:\n        assert isinstance(\n            data, (bytes, bytearray, memoryview)\n        ), \"data argument must be byte-ish (%r)\" % type(data)\n\n        if self._eof_sent:\n            raise RuntimeError(\"Cannot call write() after write_eof()\")\n        if self._payload_writer is None:\n            raise RuntimeError(\"Cannot call write() before prepare()\")\n\n        await self._payload_writer.write(data)\n\n    async def drain(self) -> None:\n        assert not self._eof_sent, \"EOF has already been sent\"\n        assert self._payload_writer is not None, \"Response has not been started\"\n        warnings.warn(\n            \"drain method is deprecated, use await resp.write()\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        await self._payload_writer.drain()\n\n    async def write_eof(self, data: bytes = b\"\") -> None:\n        assert isinstance(\n            data, (bytes, bytearray, memoryview)\n        ), \"data argument must be byte-ish (%r)\" % type(data)\n\n        if self._eof_sent:\n            return\n\n        assert self._payload_writer is not None, \"Response has not been started\"\n\n        await self._payload_writer.write_eof(data)\n        self._eof_sent = True\n        self._req = None\n        self._body_length = self._payload_writer.output_size\n        self._payload_writer = None\n\n    def __repr__(self) -> str:\n        if self._eof_sent:\n            info = \"eof\"\n        elif self.prepared:\n            assert self._req is not None\n            info = f\"{self._req.method} {self._req.path} \"\n        else:\n            info = \"not prepared\"\n        return f\"<{self.__class__.__name__} {self.reason} {info}>\"\n\n    def __getitem__(self, key: str) -> Any:\n        return self._state[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self._state[key] = value\n\n    def __delitem__(self, key: str) -> None:\n        del self._state[key]\n\n    def __len__(self) -> int:\n        return len(self._state)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._state)\n\n    def __hash__(self) -> int:\n        return hash(id(self))\n\n    def __eq__(self, other: object) -> bool:\n        return self is other\n\n\nclass Response(StreamResponse):\n    __slots__ = (\n        \"_body_payload\",\n        \"_compressed_body\",\n        \"_zlib_executor_size\",\n        \"_zlib_executor\",\n    )\n\n    def __init__(\n        self,\n        *,\n        body: Any = None,\n        status: int = 200,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        headers: Optional[LooseHeaders] = None,\n        content_type: Optional[str] = None,\n        charset: Optional[str] = None,\n        zlib_executor_size: Optional[int] = None,\n        zlib_executor: Optional[Executor] = None,\n    ) -> None:\n        if body is not None and text is not None:\n            raise ValueError(\"body and text are not allowed together\")\n\n        if headers is None:\n            real_headers: CIMultiDict[str] = CIMultiDict()\n        elif not isinstance(headers, CIMultiDict):\n            real_headers = CIMultiDict(headers)\n        else:\n            real_headers = headers  # = cast('CIMultiDict[str]', headers)\n\n        if content_type is not None and \"charset\" in content_type:\n            raise ValueError(\"charset must not be in content_type \" \"argument\")\n\n        if text is not None:\n            if hdrs.CONTENT_TYPE in real_headers:\n                if content_type or charset:\n                    raise ValueError(\n                        \"passing both Content-Type header and \"\n                        \"content_type or charset params \"\n                        \"is forbidden\"\n                    )\n            else:\n                # fast path for filling headers\n                if not isinstance(text, str):\n                    raise TypeError(\"text argument must be str (%r)\" % type(text))\n                if content_type is None:\n                    content_type = \"text/plain\"\n                if charset is None:\n                    charset = \"utf-8\"\n                real_headers[hdrs.CONTENT_TYPE] = content_type + \"; charset=\" + charset\n                body = text.encode(charset)\n                text = None\n        else:\n            if hdrs.CONTENT_TYPE in real_headers:\n                if content_type is not None or charset is not None:\n                    raise ValueError(\n                        \"passing both Content-Type header and \"\n                        \"content_type or charset params \"\n                        \"is forbidden\"\n                    )\n            else:\n                if content_type is not None:\n                    if charset is not None:\n                        content_type += \"; charset=\" + charset\n                    real_headers[hdrs.CONTENT_TYPE] = content_type\n\n        super().__init__(status=status, reason=reason, headers=real_headers)\n\n        if text is not None:\n            self.text = text\n        else:\n            self.body = body\n\n        self._compressed_body: Optional[bytes] = None\n        self._zlib_executor_size = zlib_executor_size\n        self._zlib_executor = zlib_executor\n\n    @property\n    def body(self) -> Optional[Union[bytes, Payload]]:\n        return self._body\n\n    @body.setter\n    def body(self, body: bytes) -> None:\n        if body is None:\n            self._body: Optional[bytes] = None\n            self._body_payload: bool = False\n        elif isinstance(body, (bytes, bytearray)):\n            self._body = body\n            self._body_payload = False\n        else:\n            try:\n                self._body = body = payload.PAYLOAD_REGISTRY.get(body)\n            except payload.LookupError:\n                raise ValueError(\"Unsupported body type %r\" % type(body))\n\n            self._body_payload = True\n\n            headers = self._headers\n\n            # set content-type\n            if hdrs.CONTENT_TYPE not in headers:\n                headers[hdrs.CONTENT_TYPE] = body.content_type\n\n            # copy payload headers\n            if body.headers:\n                for key, value in body.headers.items():\n                    if key not in headers:\n                        headers[key] = value\n\n        self._compressed_body = None\n\n    @property\n    def text(self) -> Optional[str]:\n        if self._body is None:\n            return None\n        return self._body.decode(self.charset or \"utf-8\")\n\n    @text.setter\n    def text(self, text: str) -> None:\n        assert isinstance(text, str), \"text argument must be str (%r)\" % type(text)\n\n        if self.content_type == \"application/octet-stream\":\n            self.content_type = \"text/plain\"\n        if self.charset is None:\n            self.charset = \"utf-8\"\n\n        self._body = text.encode(self.charset)\n        self._body_payload = False\n        self._compressed_body = None\n\n    @property\n    def content_length(self) -> Optional[int]:\n        if self._chunked:\n            return None\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            return super().content_length\n\n        if self._compressed_body is not None:\n            # Return length of the compressed body\n            return len(self._compressed_body)\n        elif self._body_payload:\n            # A payload without content length, or a compressed payload\n            return None\n        elif self._body is not None:\n            return len(self._body)\n        else:\n            return 0\n\n    @content_length.setter\n    def content_length(self, value: Optional[int]) -> None:\n        raise RuntimeError(\"Content length is set automatically\")\n\n    async def write_eof(self, data: bytes = b\"\") -> None:\n        if self._eof_sent:\n            return\n        if self._compressed_body is None:\n            body: Optional[Union[bytes, Payload]] = self._body\n        else:\n            body = self._compressed_body\n        assert not data, f\"data arg is not supported, got {data!r}\"\n        assert self._req is not None\n        assert self._payload_writer is not None\n        if body is not None:\n            if self._must_be_empty_body:\n                await super().write_eof()\n            elif self._body_payload:\n                payload = cast(Payload, body)\n                await payload.write(self._payload_writer)\n                await super().write_eof()\n            else:\n                await super().write_eof(cast(bytes, body))\n        else:\n            await super().write_eof()\n\n    async def _start(self, request: \"BaseRequest\") -> AbstractStreamWriter:\n        if should_remove_content_length(request.method, self.status):\n            if hdrs.CONTENT_LENGTH in self._headers:\n                del self._headers[hdrs.CONTENT_LENGTH]\n        elif not self._chunked and hdrs.CONTENT_LENGTH not in self._headers:\n            if self._body_payload:\n                size = cast(Payload, self._body).size\n                if size is not None:\n                    self._headers[hdrs.CONTENT_LENGTH] = str(size)\n            else:\n                body_len = len(self._body) if self._body else \"0\"\n                # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.6-7\n                if body_len != \"0\" or (\n                    self.status != 304 and request.method.upper() != hdrs.METH_HEAD\n                ):\n                    self._headers[hdrs.CONTENT_LENGTH] = str(body_len)\n\n        return await super()._start(request)\n\n    async def _do_start_compression(self, coding: ContentCoding) -> None:\n        if self._body_payload or self._chunked:\n            return await super()._do_start_compression(coding)\n\n        if coding != ContentCoding.identity:\n            # Instead of using _payload_writer.enable_compression,\n            # compress the whole body\n            compressor = ZLibCompressor(\n                encoding=str(coding.value),\n                max_sync_chunk_size=self._zlib_executor_size,\n                executor=self._zlib_executor,\n            )\n            assert self._body is not None\n            if self._zlib_executor_size is None and len(self._body) > 1024 * 1024:\n                warnings.warn(\n                    \"Synchronous compression of large response bodies \"\n                    f\"({len(self._body)} bytes) might block the async event loop. \"\n                    \"Consider providing a custom value to zlib_executor_size/\"\n                    \"zlib_executor response properties or disabling compression on it.\"\n                )\n            self._compressed_body = (\n                await compressor.compress(self._body) + compressor.flush()\n            )\n            assert self._compressed_body is not None\n\n            self._headers[hdrs.CONTENT_ENCODING] = coding.value\n            self._headers[hdrs.CONTENT_LENGTH] = str(len(self._compressed_body))\n\n\ndef json_response(\n    data: Any = sentinel,\n    *,\n    text: Optional[str] = None,\n    body: Optional[bytes] = None,\n    status: int = 200,\n    reason: Optional[str] = None,\n    headers: Optional[LooseHeaders] = None,\n    content_type: str = \"application/json\",\n    dumps: JSONEncoder = json.dumps,\n) -> Response:\n    if data is not sentinel:\n        if text or body:\n            raise ValueError(\"only one of data, text, or body should be specified\")\n        else:\n            text = dumps(data)\n    return Response(\n        text=text,\n        body=body,\n        status=status,\n        reason=reason,\n        headers=headers,\n        content_type=content_type,\n    )\n", "aiohttp/http_exceptions.py": "\"\"\"Low-level http related exceptions.\"\"\"\n\nfrom textwrap import indent\nfrom typing import Optional, Union\n\nfrom .typedefs import _CIMultiDict\n\n__all__ = (\"HttpProcessingError\",)\n\n\nclass HttpProcessingError(Exception):\n    \"\"\"HTTP error.\n\n    Shortcut for raising HTTP errors with custom code, message and headers.\n\n    code: HTTP Error code.\n    message: (optional) Error message.\n    headers: (optional) Headers to be sent in response, a list of pairs\n    \"\"\"\n\n    code = 0\n    message = \"\"\n    headers = None\n\n    def __init__(\n        self,\n        *,\n        code: Optional[int] = None,\n        message: str = \"\",\n        headers: Optional[_CIMultiDict] = None,\n    ) -> None:\n        if code is not None:\n            self.code = code\n        self.headers = headers\n        self.message = message\n\n    def __str__(self) -> str:\n        msg = indent(self.message, \"  \")\n        return f\"{self.code}, message:\\n{msg}\"\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self.code}, message={self.message!r}>\"\n\n\nclass BadHttpMessage(HttpProcessingError):\n    code = 400\n    message = \"Bad Request\"\n\n    def __init__(self, message: str, *, headers: Optional[_CIMultiDict] = None) -> None:\n        super().__init__(message=message, headers=headers)\n        self.args = (message,)\n\n\nclass HttpBadRequest(BadHttpMessage):\n    code = 400\n    message = \"Bad Request\"\n\n\nclass PayloadEncodingError(BadHttpMessage):\n    \"\"\"Base class for payload errors\"\"\"\n\n\nclass ContentEncodingError(PayloadEncodingError):\n    \"\"\"Content encoding error.\"\"\"\n\n\nclass TransferEncodingError(PayloadEncodingError):\n    \"\"\"transfer encoding error.\"\"\"\n\n\nclass ContentLengthError(PayloadEncodingError):\n    \"\"\"Not enough data for satisfy content length header.\"\"\"\n\n\nclass LineTooLong(BadHttpMessage):\n    def __init__(\n        self, line: str, limit: str = \"Unknown\", actual_size: str = \"Unknown\"\n    ) -> None:\n        super().__init__(\n            f\"Got more than {limit} bytes ({actual_size}) when reading {line}.\"\n        )\n        self.args = (line, limit, actual_size)\n\n\nclass InvalidHeader(BadHttpMessage):\n    def __init__(self, hdr: Union[bytes, str]) -> None:\n        hdr_s = hdr.decode(errors=\"backslashreplace\") if isinstance(hdr, bytes) else hdr\n        super().__init__(f\"Invalid HTTP header: {hdr!r}\")\n        self.hdr = hdr_s\n        self.args = (hdr,)\n\n\nclass BadStatusLine(BadHttpMessage):\n    def __init__(self, line: str = \"\", error: Optional[str] = None) -> None:\n        if not isinstance(line, str):\n            line = repr(line)\n        super().__init__(error or f\"Bad status line {line!r}\")\n        self.args = (line,)\n        self.line = line\n\n\nclass InvalidURLError(BadHttpMessage):\n    pass\n", "aiohttp/worker.py": "\"\"\"Async gunicorn worker for aiohttp.web\"\"\"\n\nimport asyncio\nimport os\nimport re\nimport signal\nimport sys\nfrom types import FrameType\nfrom typing import Any, Awaitable, Callable, Optional, Union  # noqa\n\nfrom gunicorn.config import AccessLogFormat as GunicornAccessLogFormat\nfrom gunicorn.workers import base\n\nfrom aiohttp import web\n\nfrom .helpers import set_result\nfrom .web_app import Application\nfrom .web_log import AccessLogger\n\ntry:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nexcept ImportError:  # pragma: no cover\n    ssl = None  # type: ignore[assignment]\n    SSLContext = object  # type: ignore[misc,assignment]\n\n\n__all__ = (\"GunicornWebWorker\", \"GunicornUVLoopWebWorker\")\n\n\nclass GunicornWebWorker(base.Worker):  # type: ignore[misc,no-any-unimported]\n    DEFAULT_AIOHTTP_LOG_FORMAT = AccessLogger.LOG_FORMAT\n    DEFAULT_GUNICORN_LOG_FORMAT = GunicornAccessLogFormat.default\n\n    def __init__(self, *args: Any, **kw: Any) -> None:  # pragma: no cover\n        super().__init__(*args, **kw)\n\n        self._task: Optional[asyncio.Task[None]] = None\n        self.exit_code = 0\n        self._notify_waiter: Optional[asyncio.Future[bool]] = None\n\n    def init_process(self) -> None:\n        # create new event_loop after fork\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n\n        super().init_process()\n\n    def run(self) -> None:\n        self._task = self.loop.create_task(self._run())\n\n        try:  # ignore all finalization problems\n            self.loop.run_until_complete(self._task)\n        except Exception:\n            self.log.exception(\"Exception in gunicorn worker\")\n        self.loop.run_until_complete(self.loop.shutdown_asyncgens())\n        self.loop.close()\n\n        sys.exit(self.exit_code)\n\n    async def _run(self) -> None:\n        runner = None\n        if isinstance(self.wsgi, Application):\n            app = self.wsgi\n        elif asyncio.iscoroutinefunction(self.wsgi):\n            wsgi = await self.wsgi()\n            if isinstance(wsgi, web.AppRunner):\n                runner = wsgi\n                app = runner.app\n            else:\n                app = wsgi\n        else:\n            raise RuntimeError(\n                \"wsgi app should be either Application or \"\n                \"async function returning Application, got {}\".format(self.wsgi)\n            )\n\n        if runner is None:\n            access_log = self.log.access_log if self.cfg.accesslog else None\n            runner = web.AppRunner(\n                app,\n                logger=self.log,\n                keepalive_timeout=self.cfg.keepalive,\n                access_log=access_log,\n                access_log_format=self._get_valid_log_format(\n                    self.cfg.access_log_format\n                ),\n                shutdown_timeout=self.cfg.graceful_timeout / 100 * 95,\n            )\n        await runner.setup()\n\n        ctx = self._create_ssl_context(self.cfg) if self.cfg.is_ssl else None\n\n        assert runner is not None\n        server = runner.server\n        assert server is not None\n        for sock in self.sockets:\n            site = web.SockSite(\n                runner,\n                sock,\n                ssl_context=ctx,\n            )\n            await site.start()\n\n        # If our parent changed then we shut down.\n        pid = os.getpid()\n        try:\n            while self.alive:  # type: ignore[has-type]\n                self.notify()\n\n                cnt = server.requests_count\n                if self.max_requests and cnt > self.max_requests:\n                    self.alive = False\n                    self.log.info(\"Max requests, shutting down: %s\", self)\n\n                elif pid == os.getpid() and self.ppid != os.getppid():\n                    self.alive = False\n                    self.log.info(\"Parent changed, shutting down: %s\", self)\n                else:\n                    await self._wait_next_notify()\n        except BaseException:\n            pass\n\n        await runner.cleanup()\n\n    def _wait_next_notify(self) -> \"asyncio.Future[bool]\":\n        self._notify_waiter_done()\n\n        loop = self.loop\n        assert loop is not None\n        self._notify_waiter = waiter = loop.create_future()\n        self.loop.call_later(1.0, self._notify_waiter_done, waiter)\n\n        return waiter\n\n    def _notify_waiter_done(\n        self, waiter: Optional[\"asyncio.Future[bool]\"] = None\n    ) -> None:\n        if waiter is None:\n            waiter = self._notify_waiter\n        if waiter is not None:\n            set_result(waiter, True)\n\n        if waiter is self._notify_waiter:\n            self._notify_waiter = None\n\n    def init_signals(self) -> None:\n        # Set up signals through the event loop API.\n\n        self.loop.add_signal_handler(\n            signal.SIGQUIT, self.handle_quit, signal.SIGQUIT, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGTERM, self.handle_exit, signal.SIGTERM, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGINT, self.handle_quit, signal.SIGINT, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGWINCH, self.handle_winch, signal.SIGWINCH, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGUSR1, self.handle_usr1, signal.SIGUSR1, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGABRT, self.handle_abort, signal.SIGABRT, None\n        )\n\n        # Don't let SIGTERM and SIGUSR1 disturb active requests\n        # by interrupting system calls\n        signal.siginterrupt(signal.SIGTERM, False)\n        signal.siginterrupt(signal.SIGUSR1, False)\n        # Reset signals so Gunicorn doesn't swallow subprocess return codes\n        # See: https://github.com/aio-libs/aiohttp/issues/6130\n\n    def handle_quit(self, sig: int, frame: Optional[FrameType]) -> None:\n        self.alive = False\n\n        # worker_int callback\n        self.cfg.worker_int(self)\n\n        # wakeup closing process\n        self._notify_waiter_done()\n\n    def handle_abort(self, sig: int, frame: Optional[FrameType]) -> None:\n        self.alive = False\n        self.exit_code = 1\n        self.cfg.worker_abort(self)\n        sys.exit(1)\n\n    @staticmethod\n    def _create_ssl_context(cfg: Any) -> \"SSLContext\":\n        \"\"\"Creates SSLContext instance for usage in asyncio.create_server.\n\n        See ssl.SSLSocket.__init__ for more details.\n        \"\"\"\n        if ssl is None:  # pragma: no cover\n            raise RuntimeError(\"SSL is not supported.\")\n\n        ctx = ssl.SSLContext(cfg.ssl_version)\n        ctx.load_cert_chain(cfg.certfile, cfg.keyfile)\n        ctx.verify_mode = cfg.cert_reqs\n        if cfg.ca_certs:\n            ctx.load_verify_locations(cfg.ca_certs)\n        if cfg.ciphers:\n            ctx.set_ciphers(cfg.ciphers)\n        return ctx\n\n    def _get_valid_log_format(self, source_format: str) -> str:\n        if source_format == self.DEFAULT_GUNICORN_LOG_FORMAT:\n            return self.DEFAULT_AIOHTTP_LOG_FORMAT\n        elif re.search(r\"%\\([^\\)]+\\)\", source_format):\n            raise ValueError(\n                \"Gunicorn's style options in form of `%(name)s` are not \"\n                \"supported for the log formatting. Please use aiohttp's \"\n                \"format specification to configure access log formatting: \"\n                \"http://docs.aiohttp.org/en/stable/logging.html\"\n                \"#format-specification\"\n            )\n        else:\n            return source_format\n\n\nclass GunicornUVLoopWebWorker(GunicornWebWorker):\n    def init_process(self) -> None:\n        import uvloop\n\n        # Setup uvloop policy, so that every\n        # asyncio.get_event_loop() will create an instance\n        # of uvloop event loop.\n        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n\n        super().init_process()\n", "aiohttp/web_app.py": "import asyncio\nimport logging\nimport warnings\nfrom functools import partial, update_wrapper\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    final,\n    overload,\n)\n\nfrom aiosignal import Signal\nfrom frozenlist import FrozenList\n\nfrom . import hdrs\nfrom .helpers import AppKey\nfrom .log import web_logger\nfrom .typedefs import Middleware\nfrom .web_exceptions import NotAppKeyWarning\nfrom .web_middlewares import _fix_request_current_app\nfrom .web_request import Request\nfrom .web_response import StreamResponse\nfrom .web_routedef import AbstractRouteDef\nfrom .web_urldispatcher import (\n    AbstractResource,\n    AbstractRoute,\n    Domain,\n    MaskDomain,\n    MatchedSubAppResource,\n    PrefixedSubAppResource,\n    UrlDispatcher,\n)\n\n__all__ = (\"Application\", \"CleanupError\")\n\n\nif TYPE_CHECKING:\n    _AppSignal = Signal[Callable[[\"Application\"], Awaitable[None]]]\n    _RespPrepareSignal = Signal[Callable[[Request, StreamResponse], Awaitable[None]]]\n    _Middlewares = FrozenList[Middleware]\n    _MiddlewaresHandlers = Sequence[Middleware]\n    _Subapps = List[\"Application\"]\nelse:\n    # No type checker mode, skip types\n    _AppSignal = Signal\n    _RespPrepareSignal = Signal\n    _Handler = Callable\n    _Middlewares = FrozenList\n    _MiddlewaresHandlers = Sequence\n    _Subapps = List\n\n_T = TypeVar(\"_T\")\n_U = TypeVar(\"_U\")\n_Resource = TypeVar(\"_Resource\", bound=AbstractResource)\n\n\n@final\nclass Application(MutableMapping[Union[str, AppKey[Any]], Any]):\n    __slots__ = (\n        \"logger\",\n        \"_debug\",\n        \"_router\",\n        \"_loop\",\n        \"_handler_args\",\n        \"_middlewares\",\n        \"_middlewares_handlers\",\n        \"_run_middlewares\",\n        \"_state\",\n        \"_frozen\",\n        \"_pre_frozen\",\n        \"_subapps\",\n        \"_on_response_prepare\",\n        \"_on_startup\",\n        \"_on_shutdown\",\n        \"_on_cleanup\",\n        \"_client_max_size\",\n        \"_cleanup_ctx\",\n    )\n\n    def __init__(\n        self,\n        *,\n        logger: logging.Logger = web_logger,\n        middlewares: Iterable[Middleware] = (),\n        handler_args: Optional[Mapping[str, Any]] = None,\n        client_max_size: int = 1024**2,\n        debug: Any = ...,  # mypy doesn't support ellipsis\n    ) -> None:\n        if debug is not ...:\n            warnings.warn(\n                \"debug argument is no-op since 4.0 \" \"and scheduled for removal in 5.0\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self._router = UrlDispatcher()\n        self._handler_args = handler_args\n        self.logger = logger\n\n        self._middlewares: _Middlewares = FrozenList(middlewares)\n\n        # initialized on freezing\n        self._middlewares_handlers: _MiddlewaresHandlers = tuple()\n        # initialized on freezing\n        self._run_middlewares: Optional[bool] = None\n\n        self._state: Dict[Union[AppKey[Any], str], object] = {}\n        self._frozen = False\n        self._pre_frozen = False\n        self._subapps: _Subapps = []\n\n        self._on_response_prepare: _RespPrepareSignal = Signal(self)\n        self._on_startup: _AppSignal = Signal(self)\n        self._on_shutdown: _AppSignal = Signal(self)\n        self._on_cleanup: _AppSignal = Signal(self)\n        self._cleanup_ctx = CleanupContext()\n        self._on_startup.append(self._cleanup_ctx._on_startup)\n        self._on_cleanup.append(self._cleanup_ctx._on_cleanup)\n        self._client_max_size = client_max_size\n\n    def __init_subclass__(cls: Type[\"Application\"]) -> None:\n        raise TypeError(\n            \"Inheritance class {} from web.Application \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    # MutableMapping API\n\n    def __eq__(self, other: object) -> bool:\n        return self is other\n\n    @overload  # type: ignore[override]\n    def __getitem__(self, key: AppKey[_T]) -> _T: ...\n\n    @overload\n    def __getitem__(self, key: str) -> Any: ...\n\n    def __getitem__(self, key: Union[str, AppKey[_T]]) -> Any:\n        return self._state[key]\n\n    def _check_frozen(self) -> None:\n        if self._frozen:\n            raise RuntimeError(\n                \"Changing state of started or joined \" \"application is forbidden\"\n            )\n\n    @overload  # type: ignore[override]\n    def __setitem__(self, key: AppKey[_T], value: _T) -> None: ...\n\n    @overload\n    def __setitem__(self, key: str, value: Any) -> None: ...\n\n    def __setitem__(self, key: Union[str, AppKey[_T]], value: Any) -> None:\n        self._check_frozen()\n        if not isinstance(key, AppKey):\n            warnings.warn(\n                \"It is recommended to use web.AppKey instances for keys.\\n\"\n                + \"https://docs.aiohttp.org/en/stable/web_advanced.html\"\n                + \"#application-s-config\",\n                category=NotAppKeyWarning,\n                stacklevel=2,\n            )\n        self._state[key] = value\n\n    def __delitem__(self, key: Union[str, AppKey[_T]]) -> None:\n        self._check_frozen()\n        del self._state[key]\n\n    def __len__(self) -> int:\n        return len(self._state)\n\n    def __iter__(self) -> Iterator[Union[str, AppKey[Any]]]:\n        return iter(self._state)\n\n    @overload  # type: ignore[override]\n    def get(self, key: AppKey[_T], default: None = ...) -> Optional[_T]: ...\n\n    @overload\n    def get(self, key: AppKey[_T], default: _U) -> Union[_T, _U]: ...\n\n    @overload\n    def get(self, key: str, default: Any = ...) -> Any: ...\n\n    def get(self, key: Union[str, AppKey[_T]], default: Any = None) -> Any:\n        return self._state.get(key, default)\n\n    ########\n    def _set_loop(self, loop: Optional[asyncio.AbstractEventLoop]) -> None:\n        warnings.warn(\n            \"_set_loop() is no-op since 4.0 \" \"and scheduled for removal in 5.0\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    @property\n    def pre_frozen(self) -> bool:\n        return self._pre_frozen\n\n    def pre_freeze(self) -> None:\n        if self._pre_frozen:\n            return\n\n        self._pre_frozen = True\n        self._middlewares.freeze()\n        self._router.freeze()\n        self._on_response_prepare.freeze()\n        self._cleanup_ctx.freeze()\n        self._on_startup.freeze()\n        self._on_shutdown.freeze()\n        self._on_cleanup.freeze()\n        self._middlewares_handlers = tuple(self._prepare_middleware())\n\n        # If current app and any subapp do not have middlewares avoid run all\n        # of the code footprint that it implies, which have a middleware\n        # hardcoded per app that sets up the current_app attribute. If no\n        # middlewares are configured the handler will receive the proper\n        # current_app without needing all of this code.\n        self._run_middlewares = True if self.middlewares else False\n\n        for subapp in self._subapps:\n            subapp.pre_freeze()\n            self._run_middlewares = self._run_middlewares or subapp._run_middlewares\n\n    @property\n    def frozen(self) -> bool:\n        return self._frozen\n\n    def freeze(self) -> None:\n        if self._frozen:\n            return\n\n        self.pre_freeze()\n        self._frozen = True\n        for subapp in self._subapps:\n            subapp.freeze()\n\n    @property\n    def debug(self) -> bool:\n        warnings.warn(\n            \"debug property is deprecated since 4.0\" \"and scheduled for removal in 5.0\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return asyncio.get_event_loop().get_debug()\n\n    def _reg_subapp_signals(self, subapp: \"Application\") -> None:\n        def reg_handler(signame: str) -> None:\n            subsig = getattr(subapp, signame)\n\n            async def handler(app: \"Application\") -> None:\n                await subsig.send(subapp)\n\n            appsig = getattr(self, signame)\n            appsig.append(handler)\n\n        reg_handler(\"on_startup\")\n        reg_handler(\"on_shutdown\")\n        reg_handler(\"on_cleanup\")\n\n    def add_subapp(self, prefix: str, subapp: \"Application\") -> PrefixedSubAppResource:\n        if not isinstance(prefix, str):\n            raise TypeError(\"Prefix must be str\")\n        prefix = prefix.rstrip(\"/\")\n        if not prefix:\n            raise ValueError(\"Prefix cannot be empty\")\n        factory = partial(PrefixedSubAppResource, prefix, subapp)\n        return self._add_subapp(factory, subapp)\n\n    def _add_subapp(\n        self, resource_factory: Callable[[], _Resource], subapp: \"Application\"\n    ) -> _Resource:\n        if self.frozen:\n            raise RuntimeError(\"Cannot add sub application to frozen application\")\n        if subapp.frozen:\n            raise RuntimeError(\"Cannot add frozen application\")\n        resource = resource_factory()\n        self.router.register_resource(resource)\n        self._reg_subapp_signals(subapp)\n        self._subapps.append(subapp)\n        subapp.pre_freeze()\n        return resource\n\n    def add_domain(self, domain: str, subapp: \"Application\") -> MatchedSubAppResource:\n        if not isinstance(domain, str):\n            raise TypeError(\"Domain must be str\")\n        elif \"*\" in domain:\n            rule: Domain = MaskDomain(domain)\n        else:\n            rule = Domain(domain)\n        factory = partial(MatchedSubAppResource, rule, subapp)\n        return self._add_subapp(factory, subapp)\n\n    def add_routes(self, routes: Iterable[AbstractRouteDef]) -> List[AbstractRoute]:\n        return self.router.add_routes(routes)\n\n    @property\n    def on_response_prepare(self) -> _RespPrepareSignal:\n        return self._on_response_prepare\n\n    @property\n    def on_startup(self) -> _AppSignal:\n        return self._on_startup\n\n    @property\n    def on_shutdown(self) -> _AppSignal:\n        return self._on_shutdown\n\n    @property\n    def on_cleanup(self) -> _AppSignal:\n        return self._on_cleanup\n\n    @property\n    def cleanup_ctx(self) -> \"CleanupContext\":\n        return self._cleanup_ctx\n\n    @property\n    def router(self) -> UrlDispatcher:\n        return self._router\n\n    @property\n    def middlewares(self) -> _Middlewares:\n        return self._middlewares\n\n    async def startup(self) -> None:\n        \"\"\"Causes on_startup signal\n\n        Should be called in the event loop along with the request handler.\n        \"\"\"\n        await self.on_startup.send(self)\n\n    async def shutdown(self) -> None:\n        \"\"\"Causes on_shutdown signal\n\n        Should be called before cleanup()\n        \"\"\"\n        await self.on_shutdown.send(self)\n\n    async def cleanup(self) -> None:\n        \"\"\"Causes on_cleanup signal\n\n        Should be called after shutdown()\n        \"\"\"\n        if self.on_cleanup.frozen:\n            await self.on_cleanup.send(self)\n        else:\n            # If an exception occurs in startup, ensure cleanup contexts are completed.\n            await self._cleanup_ctx._on_cleanup(self)\n\n    def _prepare_middleware(self) -> Iterator[Middleware]:\n        yield from reversed(self._middlewares)\n        yield _fix_request_current_app(self)\n\n    async def _handle(self, request: Request) -> StreamResponse:\n        match_info = await self._router.resolve(request)\n        match_info.add_app(self)\n        match_info.freeze()\n\n        resp = None\n        request._match_info = match_info\n        expect = request.headers.get(hdrs.EXPECT)\n        if expect:\n            resp = await match_info.expect_handler(request)\n            await request.writer.drain()\n\n        if resp is None:\n            handler = match_info.handler\n\n            if self._run_middlewares:\n                for app in match_info.apps[::-1]:\n                    assert app.pre_frozen, \"middleware handlers are not ready\"\n                    for m in app._middlewares_handlers:\n                        handler = update_wrapper(partial(m, handler=handler), handler)\n\n            resp = await handler(request)\n\n        return resp\n\n    def __call__(self) -> \"Application\":\n        \"\"\"gunicorn compatibility\"\"\"\n        return self\n\n    def __repr__(self) -> str:\n        return f\"<Application 0x{id(self):x}>\"\n\n    def __bool__(self) -> bool:\n        return True\n\n\nclass CleanupError(RuntimeError):\n    @property\n    def exceptions(self) -> List[BaseException]:\n        return cast(List[BaseException], self.args[1])\n\n\nif TYPE_CHECKING:\n    _CleanupContextBase = FrozenList[Callable[[Application], AsyncIterator[None]]]\nelse:\n    _CleanupContextBase = FrozenList\n\n\nclass CleanupContext(_CleanupContextBase):\n    def __init__(self) -> None:\n        super().__init__()\n        self._exits: List[AsyncIterator[None]] = []\n\n    async def _on_startup(self, app: Application) -> None:\n        for cb in self:\n            it = cb(app).__aiter__()\n            await it.__anext__()\n            self._exits.append(it)\n\n    async def _on_cleanup(self, app: Application) -> None:\n        errors = []\n        for it in reversed(self._exits):\n            try:\n                await it.__anext__()\n            except StopAsyncIteration:\n                pass\n            except Exception as exc:\n                errors.append(exc)\n            else:\n                errors.append(RuntimeError(f\"{it!r} has more than one 'yield'\"))\n        if errors:\n            if len(errors) == 1:\n                raise errors[0]\n            else:\n                raise CleanupError(\"Multiple errors on cleanup stage\", errors)\n", "aiohttp/streams.py": "import asyncio\nimport collections\nimport warnings\nfrom typing import (\n    Awaitable,\n    Callable,\n    Deque,\n    Final,\n    Generic,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n)\n\nfrom .base_protocol import BaseProtocol\nfrom .helpers import (\n    _EXC_SENTINEL,\n    BaseTimerContext,\n    TimerNoop,\n    set_exception,\n    set_result,\n)\nfrom .log import internal_logger\n\n__all__ = (\n    \"EMPTY_PAYLOAD\",\n    \"EofStream\",\n    \"StreamReader\",\n    \"DataQueue\",\n    \"FlowControlDataQueue\",\n)\n\n_T = TypeVar(\"_T\")\n_SizedT = TypeVar(\"_SizedT\", bound=collections.abc.Sized)\n\n\nclass EofStream(Exception):\n    \"\"\"eof stream indication.\"\"\"\n\n\nclass AsyncStreamIterator(Generic[_T]):\n    def __init__(self, read_func: Callable[[], Awaitable[_T]]) -> None:\n        self.read_func = read_func\n\n    def __aiter__(self) -> \"AsyncStreamIterator[_T]\":\n        return self\n\n    async def __anext__(self) -> _T:\n        try:\n            rv = await self.read_func()\n        except EofStream:\n            raise StopAsyncIteration\n        if rv == b\"\":\n            raise StopAsyncIteration\n        return rv\n\n\nclass ChunkTupleAsyncStreamIterator:\n    def __init__(self, stream: \"StreamReader\") -> None:\n        self._stream = stream\n\n    def __aiter__(self) -> \"ChunkTupleAsyncStreamIterator\":\n        return self\n\n    async def __anext__(self) -> Tuple[bytes, bool]:\n        rv = await self._stream.readchunk()\n        if rv == (b\"\", False):\n            raise StopAsyncIteration\n        return rv\n\n\nclass AsyncStreamReaderMixin:\n    def __aiter__(self) -> AsyncStreamIterator[bytes]:\n        return AsyncStreamIterator(self.readline)  # type: ignore[attr-defined]\n\n    def iter_chunked(self, n: int) -> AsyncStreamIterator[bytes]:\n        \"\"\"Returns an asynchronous iterator that yields chunks of size n.\"\"\"\n        return AsyncStreamIterator(lambda: self.read(n))  # type: ignore[attr-defined]\n\n    def iter_any(self) -> AsyncStreamIterator[bytes]:\n        \"\"\"Yield all available data as soon as it is received.\"\"\"\n        return AsyncStreamIterator(self.readany)  # type: ignore[attr-defined]\n\n    def iter_chunks(self) -> ChunkTupleAsyncStreamIterator:\n        \"\"\"Yield chunks of data as they are received by the server.\n\n        The yielded objects are tuples\n        of (bytes, bool) as returned by the StreamReader.readchunk method.\n        \"\"\"\n        return ChunkTupleAsyncStreamIterator(self)  # type: ignore[arg-type]\n\n\nclass StreamReader(AsyncStreamReaderMixin):\n    \"\"\"An enhancement of asyncio.StreamReader.\n\n    Supports asynchronous iteration by line, chunk or as available::\n\n        async for line in reader:\n            ...\n        async for chunk in reader.iter_chunked(1024):\n            ...\n        async for slice in reader.iter_any():\n            ...\n\n    \"\"\"\n\n    total_bytes = 0\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        limit: int,\n        *,\n        timer: Optional[BaseTimerContext] = None,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        self._protocol = protocol\n        self._low_water = limit\n        self._high_water = limit * 2\n        if loop is None:\n            loop = asyncio.get_event_loop()\n        self._loop = loop\n        self._size = 0\n        self._cursor = 0\n        self._http_chunk_splits: Optional[List[int]] = None\n        self._buffer: Deque[bytes] = collections.deque()\n        self._buffer_offset = 0\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._eof_waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Optional[BaseException] = None\n        self._timer = TimerNoop() if timer is None else timer\n        self._eof_callbacks: List[Callable[[], None]] = []\n\n    def __repr__(self) -> str:\n        info = [self.__class__.__name__]\n        if self._size:\n            info.append(\"%d bytes\" % self._size)\n        if self._eof:\n            info.append(\"eof\")\n        if self._low_water != 2**16:  # default limit\n            info.append(\"low=%d high=%d\" % (self._low_water, self._high_water))\n        if self._waiter:\n            info.append(\"w=%r\" % self._waiter)\n        if self._exception:\n            info.append(\"e=%r\" % self._exception)\n        return \"<%s>\" % \" \".join(info)\n\n    def get_read_buffer_limits(self) -> Tuple[int, int]:\n        return (self._low_water, self._high_water)\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: BaseException,\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._exception = exc\n        self._eof_callbacks.clear()\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n        waiter = self._eof_waiter\n        if waiter is not None:\n            self._eof_waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def on_eof(self, callback: Callable[[], None]) -> None:\n        if self._eof:\n            try:\n                callback()\n            except Exception:\n                internal_logger.exception(\"Exception in eof callback\")\n        else:\n            self._eof_callbacks.append(callback)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n        waiter = self._eof_waiter\n        if waiter is not None:\n            self._eof_waiter = None\n            set_result(waiter, None)\n\n        for cb in self._eof_callbacks:\n            try:\n                cb()\n            except Exception:\n                internal_logger.exception(\"Exception in eof callback\")\n\n        self._eof_callbacks.clear()\n\n    def is_eof(self) -> bool:\n        \"\"\"Return True if  'feed_eof' was called.\"\"\"\n        return self._eof\n\n    def at_eof(self) -> bool:\n        \"\"\"Return True if the buffer is empty and 'feed_eof' was called.\"\"\"\n        return self._eof and not self._buffer\n\n    async def wait_eof(self) -> None:\n        if self._eof:\n            return\n\n        assert self._eof_waiter is None\n        self._eof_waiter = self._loop.create_future()\n        try:\n            await self._eof_waiter\n        finally:\n            self._eof_waiter = None\n\n    def unread_data(self, data: bytes) -> None:\n        \"\"\"rollback reading some data from stream, inserting it to buffer head.\"\"\"\n        warnings.warn(\n            \"unread_data() is deprecated \"\n            \"and will be removed in future releases (#3260)\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if not data:\n            return\n\n        if self._buffer_offset:\n            self._buffer[0] = self._buffer[0][self._buffer_offset :]\n            self._buffer_offset = 0\n        self._size += len(data)\n        self._cursor -= len(data)\n        self._buffer.appendleft(data)\n        self._eof_counter = 0\n\n    def feed_data(self, data: bytes) -> None:\n        assert not self._eof, \"feed_data after feed_eof\"\n\n        if not data:\n            return\n\n        self._size += len(data)\n        self._buffer.append(data)\n        self.total_bytes += len(data)\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n        if self._size > self._high_water and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    def begin_http_chunk_receiving(self) -> None:\n        if self._http_chunk_splits is None:\n            if self.total_bytes:\n                raise RuntimeError(\n                    \"Called begin_http_chunk_receiving when\" \"some data was already fed\"\n                )\n            self._http_chunk_splits = []\n\n    def end_http_chunk_receiving(self) -> None:\n        if self._http_chunk_splits is None:\n            raise RuntimeError(\n                \"Called end_chunk_receiving without calling \"\n                \"begin_chunk_receiving first\"\n            )\n\n        # self._http_chunk_splits contains logical byte offsets from start of\n        # the body transfer. Each offset is the offset of the end of a chunk.\n        # \"Logical\" means bytes, accessible for a user.\n        # If no chunks containing logical data were received, current position\n        # is difinitely zero.\n        pos = self._http_chunk_splits[-1] if self._http_chunk_splits else 0\n\n        if self.total_bytes == pos:\n            # We should not add empty chunks here. So we check for that.\n            # Note, when chunked + gzip is used, we can receive a chunk\n            # of compressed data, but that data may not be enough for gzip FSM\n            # to yield any uncompressed data. That's why current position may\n            # not change after receiving a chunk.\n            return\n\n        self._http_chunk_splits.append(self.total_bytes)\n\n        # wake up readchunk when end of http chunk received\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n    async def _wait(self, func_name: str) -> None:\n        # StreamReader uses a future to link the protocol feed_data() method\n        # to a read coroutine. Running two read coroutines at the same time\n        # would have an unexpected behaviour. It would not possible to know\n        # which coroutine would get the next data.\n        if self._waiter is not None:\n            raise RuntimeError(\n                \"%s() called while another coroutine is \"\n                \"already waiting for incoming data\" % func_name\n            )\n\n        waiter = self._waiter = self._loop.create_future()\n        try:\n            with self._timer:\n                await waiter\n        finally:\n            self._waiter = None\n\n    async def readline(self) -> bytes:\n        return await self.readuntil()\n\n    async def readuntil(self, separator: bytes = b\"\\n\") -> bytes:\n        seplen = len(separator)\n        if seplen == 0:\n            raise ValueError(\"Separator should be at least one-byte string\")\n\n        if self._exception is not None:\n            raise self._exception\n\n        chunk = b\"\"\n        chunk_size = 0\n        not_enough = True\n\n        while not_enough:\n            while self._buffer and not_enough:\n                offset = self._buffer_offset\n                ichar = self._buffer[0].find(separator, offset) + 1\n                # Read from current offset to found separator or to the end.\n                data = self._read_nowait_chunk(\n                    ichar - offset + seplen - 1 if ichar else -1\n                )\n                chunk += data\n                chunk_size += len(data)\n                if ichar:\n                    not_enough = False\n\n                if chunk_size > self._high_water:\n                    raise ValueError(\"Chunk too big\")\n\n            if self._eof:\n                break\n\n            if not_enough:\n                await self._wait(\"readuntil\")\n\n        return chunk\n\n    async def read(self, n: int = -1) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        if not n:\n            return b\"\"\n\n        if n < 0:\n            # This used to just loop creating a new waiter hoping to\n            # collect everything in self._buffer, but that would\n            # deadlock if the subprocess sends more than self.limit\n            # bytes.  So just call self.readany() until EOF.\n            blocks = []\n            while True:\n                block = await self.readany()\n                if not block:\n                    break\n                blocks.append(block)\n            return b\"\".join(blocks)\n\n        # TODO: should be `if` instead of `while`\n        # because waiter maybe triggered on chunk end,\n        # without feeding any data\n        while not self._buffer and not self._eof:\n            await self._wait(\"read\")\n\n        return self._read_nowait(n)\n\n    async def readany(self) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        # TODO: should be `if` instead of `while`\n        # because waiter maybe triggered on chunk end,\n        # without feeding any data\n        while not self._buffer and not self._eof:\n            await self._wait(\"readany\")\n\n        return self._read_nowait(-1)\n\n    async def readchunk(self) -> Tuple[bytes, bool]:\n        \"\"\"Returns a tuple of (data, end_of_http_chunk).\n\n        When chunked transfer\n        encoding is used, end_of_http_chunk is a boolean indicating if the end\n        of the data corresponds to the end of a HTTP chunk , otherwise it is\n        always False.\n        \"\"\"\n        while True:\n            if self._exception is not None:\n                raise self._exception\n\n            while self._http_chunk_splits:\n                pos = self._http_chunk_splits.pop(0)\n                if pos == self._cursor:\n                    return (b\"\", True)\n                if pos > self._cursor:\n                    return (self._read_nowait(pos - self._cursor), True)\n                internal_logger.warning(\n                    \"Skipping HTTP chunk end due to data \"\n                    \"consumption beyond chunk boundary\"\n                )\n\n            if self._buffer:\n                return (self._read_nowait_chunk(-1), False)\n                # return (self._read_nowait(-1), False)\n\n            if self._eof:\n                # Special case for signifying EOF.\n                # (b'', True) is not a final return value actually.\n                return (b\"\", False)\n\n            await self._wait(\"readchunk\")\n\n    async def readexactly(self, n: int) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        blocks: List[bytes] = []\n        while n > 0:\n            block = await self.read(n)\n            if not block:\n                partial = b\"\".join(blocks)\n                raise asyncio.IncompleteReadError(partial, len(partial) + n)\n            blocks.append(block)\n            n -= len(block)\n\n        return b\"\".join(blocks)\n\n    def read_nowait(self, n: int = -1) -> bytes:\n        # default was changed to be consistent with .read(-1)\n        #\n        # I believe the most users don't know about the method and\n        # they are not affected.\n        if self._exception is not None:\n            raise self._exception\n\n        if self._waiter and not self._waiter.done():\n            raise RuntimeError(\n                \"Called while some coroutine is waiting for incoming data.\"\n            )\n\n        return self._read_nowait(n)\n\n    def _read_nowait_chunk(self, n: int) -> bytes:\n        first_buffer = self._buffer[0]\n        offset = self._buffer_offset\n        if n != -1 and len(first_buffer) - offset > n:\n            data = first_buffer[offset : offset + n]\n            self._buffer_offset += n\n\n        elif offset:\n            self._buffer.popleft()\n            data = first_buffer[offset:]\n            self._buffer_offset = 0\n\n        else:\n            data = self._buffer.popleft()\n\n        self._size -= len(data)\n        self._cursor += len(data)\n\n        chunk_splits = self._http_chunk_splits\n        # Prevent memory leak: drop useless chunk splits\n        while chunk_splits and chunk_splits[0] < self._cursor:\n            chunk_splits.pop(0)\n\n        if self._size < self._low_water and self._protocol._reading_paused:\n            self._protocol.resume_reading()\n        return data\n\n    def _read_nowait(self, n: int) -> bytes:\n        \"\"\"Read not more than n bytes, or whole buffer if n == -1\"\"\"\n        self._timer.assert_timeout()\n\n        chunks = []\n        while self._buffer:\n            chunk = self._read_nowait_chunk(n)\n            chunks.append(chunk)\n            if n != -1:\n                n -= len(chunk)\n                if n == 0:\n                    break\n\n        return b\"\".join(chunks) if chunks else b\"\"\n\n\nclass EmptyStreamReader(StreamReader):  # lgtm [py/missing-call-to-init]\n    def __init__(self) -> None:\n        self._read_eof_chunk = False\n\n    def __repr__(self) -> str:\n        return \"<%s>\" % self.__class__.__name__\n\n    def exception(self) -> Optional[BaseException]:\n        return None\n\n    def set_exception(\n        self,\n        exc: BaseException,\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        pass\n\n    def on_eof(self, callback: Callable[[], None]) -> None:\n        try:\n            callback()\n        except Exception:\n            internal_logger.exception(\"Exception in eof callback\")\n\n    def feed_eof(self) -> None:\n        pass\n\n    def is_eof(self) -> bool:\n        return True\n\n    def at_eof(self) -> bool:\n        return True\n\n    async def wait_eof(self) -> None:\n        return\n\n    def feed_data(self, data: bytes) -> None:\n        pass\n\n    async def readline(self) -> bytes:\n        return b\"\"\n\n    async def read(self, n: int = -1) -> bytes:\n        return b\"\"\n\n    # TODO add async def readuntil\n\n    async def readany(self) -> bytes:\n        return b\"\"\n\n    async def readchunk(self) -> Tuple[bytes, bool]:\n        if not self._read_eof_chunk:\n            self._read_eof_chunk = True\n            return (b\"\", False)\n\n        return (b\"\", True)\n\n    async def readexactly(self, n: int) -> bytes:\n        raise asyncio.IncompleteReadError(b\"\", n)\n\n    def read_nowait(self, n: int = -1) -> bytes:\n        return b\"\"\n\n\nEMPTY_PAYLOAD: Final[StreamReader] = EmptyStreamReader()\n\n\nclass DataQueue(Generic[_SizedT]):\n    \"\"\"DataQueue is a general-purpose blocking queue with one reader.\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop = loop\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Optional[BaseException] = None\n        self._size = 0\n        self._buffer: Deque[_SizedT] = collections.deque()\n\n    def __len__(self) -> int:\n        return len(self._buffer)\n\n    def is_eof(self) -> bool:\n        return self._eof\n\n    def at_eof(self) -> bool:\n        return self._eof and not self._buffer\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: BaseException,\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._eof = True\n        self._exception = exc\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def feed_data(self, data: _SizedT) -> None:\n        self._size += len(data)\n        self._buffer.append(data)\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n    async def read(self) -> _SizedT:\n        if not self._buffer and not self._eof:\n            assert not self._waiter\n            self._waiter = self._loop.create_future()\n            try:\n                await self._waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._waiter = None\n                raise\n\n        if self._buffer:\n            data = self._buffer.popleft()\n            self._size -= len(data)\n            return data\n        else:\n            if self._exception is not None:\n                raise self._exception\n            else:\n                raise EofStream\n\n    def __aiter__(self) -> AsyncStreamIterator[_SizedT]:\n        return AsyncStreamIterator(self.read)\n\n\nclass FlowControlDataQueue(DataQueue[_SizedT]):\n    \"\"\"FlowControlDataQueue resumes and pauses an underlying stream.\n\n    It is a destination for parsed data.\n    \"\"\"\n\n    def __init__(\n        self, protocol: BaseProtocol, limit: int, *, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        super().__init__(loop=loop)\n\n        self._protocol = protocol\n        self._limit = limit * 2\n\n    def feed_data(self, data: _SizedT) -> None:\n        super().feed_data(data)\n\n        if self._size > self._limit and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    async def read(self) -> _SizedT:\n        try:\n            return await super().read()\n        finally:\n            if self._size < self._limit and self._protocol._reading_paused:\n                self._protocol.resume_reading()\n", "aiohttp/payload.py": "import asyncio\nimport enum\nimport io\nimport json\nimport mimetypes\nimport os\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom itertools import chain\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Final,\n    Iterable,\n    Optional,\n    TextIO,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom multidict import CIMultiDict\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import (\n    _SENTINEL,\n    content_disposition_header,\n    guess_filename,\n    parse_mimetype,\n    sentinel,\n)\nfrom .streams import StreamReader\nfrom .typedefs import JSONEncoder, _CIMultiDict\n\n__all__ = (\n    \"PAYLOAD_REGISTRY\",\n    \"get_payload\",\n    \"payload_type\",\n    \"Payload\",\n    \"BytesPayload\",\n    \"StringPayload\",\n    \"IOBasePayload\",\n    \"BytesIOPayload\",\n    \"BufferedReaderPayload\",\n    \"TextIOPayload\",\n    \"StringIOPayload\",\n    \"JsonPayload\",\n    \"AsyncIterablePayload\",\n)\n\nTOO_LARGE_BYTES_BODY: Final[int] = 2**20  # 1 MB\n\nif TYPE_CHECKING:\n    from typing import List\n\n\nclass LookupError(Exception):\n    pass\n\n\nclass Order(str, enum.Enum):\n    normal = \"normal\"\n    try_first = \"try_first\"\n    try_last = \"try_last\"\n\n\ndef get_payload(data: Any, *args: Any, **kwargs: Any) -> \"Payload\":\n    return PAYLOAD_REGISTRY.get(data, *args, **kwargs)\n\n\ndef register_payload(\n    factory: Type[\"Payload\"], type: Any, *, order: Order = Order.normal\n) -> None:\n    PAYLOAD_REGISTRY.register(factory, type, order=order)\n\n\nclass payload_type:\n    def __init__(self, type: Any, *, order: Order = Order.normal) -> None:\n        self.type = type\n        self.order = order\n\n    def __call__(self, factory: Type[\"Payload\"]) -> Type[\"Payload\"]:\n        register_payload(factory, self.type, order=self.order)\n        return factory\n\n\nPayloadType = Type[\"Payload\"]\n_PayloadRegistryItem = Tuple[PayloadType, Any]\n\n\nclass PayloadRegistry:\n    \"\"\"Payload registry.\n\n    note: we need zope.interface for more efficient adapter search\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._first: List[_PayloadRegistryItem] = []\n        self._normal: List[_PayloadRegistryItem] = []\n        self._last: List[_PayloadRegistryItem] = []\n\n    def get(\n        self,\n        data: Any,\n        *args: Any,\n        _CHAIN: \"Type[chain[_PayloadRegistryItem]]\" = chain,\n        **kwargs: Any,\n    ) -> \"Payload\":\n        if isinstance(data, Payload):\n            return data\n        for factory, type in _CHAIN(self._first, self._normal, self._last):\n            if isinstance(data, type):\n                return factory(data, *args, **kwargs)\n\n        raise LookupError()\n\n    def register(\n        self, factory: PayloadType, type: Any, *, order: Order = Order.normal\n    ) -> None:\n        if order is Order.try_first:\n            self._first.append((factory, type))\n        elif order is Order.normal:\n            self._normal.append((factory, type))\n        elif order is Order.try_last:\n            self._last.append((factory, type))\n        else:\n            raise ValueError(f\"Unsupported order {order!r}\")\n\n\nclass Payload(ABC):\n    _default_content_type: str = \"application/octet-stream\"\n    _size: Optional[int] = None\n\n    def __init__(\n        self,\n        value: Any,\n        headers: Optional[\n            Union[_CIMultiDict, Dict[str, str], Iterable[Tuple[str, str]]]\n        ] = None,\n        content_type: Union[None, str, _SENTINEL] = sentinel,\n        filename: Optional[str] = None,\n        encoding: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._encoding = encoding\n        self._filename = filename\n        self._headers: _CIMultiDict = CIMultiDict()\n        self._value = value\n        if content_type is not sentinel and content_type is not None:\n            assert isinstance(content_type, str)\n            self._headers[hdrs.CONTENT_TYPE] = content_type\n        elif self._filename is not None:\n            content_type = mimetypes.guess_type(self._filename)[0]\n            if content_type is None:\n                content_type = self._default_content_type\n            self._headers[hdrs.CONTENT_TYPE] = content_type\n        else:\n            self._headers[hdrs.CONTENT_TYPE] = self._default_content_type\n        self._headers.update(headers or {})\n\n    @property\n    def size(self) -> Optional[int]:\n        \"\"\"Size of the payload.\"\"\"\n        return self._size\n\n    @property\n    def filename(self) -> Optional[str]:\n        \"\"\"Filename of the payload.\"\"\"\n        return self._filename\n\n    @property\n    def headers(self) -> _CIMultiDict:\n        \"\"\"Custom item headers\"\"\"\n        return self._headers\n\n    @property\n    def _binary_headers(self) -> bytes:\n        return (\n            \"\".join([k + \": \" + v + \"\\r\\n\" for k, v in self.headers.items()]).encode(\n                \"utf-8\"\n            )\n            + b\"\\r\\n\"\n        )\n\n    @property\n    def encoding(self) -> Optional[str]:\n        \"\"\"Payload encoding\"\"\"\n        return self._encoding\n\n    @property\n    def content_type(self) -> str:\n        \"\"\"Content type\"\"\"\n        return self._headers[hdrs.CONTENT_TYPE]\n\n    def set_content_disposition(\n        self,\n        disptype: str,\n        quote_fields: bool = True,\n        _charset: str = \"utf-8\",\n        **params: Any,\n    ) -> None:\n        \"\"\"Sets ``Content-Disposition`` header.\"\"\"\n        self._headers[hdrs.CONTENT_DISPOSITION] = content_disposition_header(\n            disptype, quote_fields=quote_fields, _charset=_charset, **params\n        )\n\n    @abstractmethod\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        \"\"\"Write payload.\n\n        writer is an AbstractStreamWriter instance:\n        \"\"\"\n\n\nclass BytesPayload(Payload):\n    def __init__(\n        self, value: Union[bytes, bytearray, memoryview], *args: Any, **kwargs: Any\n    ) -> None:\n        if not isinstance(value, (bytes, bytearray, memoryview)):\n            raise TypeError(f\"value argument must be byte-ish, not {type(value)!r}\")\n\n        if \"content_type\" not in kwargs:\n            kwargs[\"content_type\"] = \"application/octet-stream\"\n\n        super().__init__(value, *args, **kwargs)\n\n        if isinstance(value, memoryview):\n            self._size = value.nbytes\n        else:\n            self._size = len(value)\n\n        if self._size > TOO_LARGE_BYTES_BODY:\n            warnings.warn(\n                \"Sending a large body directly with raw bytes might\"\n                \" lock the event loop. You should probably pass an \"\n                \"io.BytesIO object instead\",\n                ResourceWarning,\n                source=self,\n            )\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        await writer.write(self._value)\n\n\nclass StringPayload(BytesPayload):\n    def __init__(\n        self,\n        value: str,\n        *args: Any,\n        encoding: Optional[str] = None,\n        content_type: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        if encoding is None:\n            if content_type is None:\n                real_encoding = \"utf-8\"\n                content_type = \"text/plain; charset=utf-8\"\n            else:\n                mimetype = parse_mimetype(content_type)\n                real_encoding = mimetype.parameters.get(\"charset\", \"utf-8\")\n        else:\n            if content_type is None:\n                content_type = \"text/plain; charset=%s\" % encoding\n            real_encoding = encoding\n\n        super().__init__(\n            value.encode(real_encoding),\n            encoding=real_encoding,\n            content_type=content_type,\n            *args,\n            **kwargs,\n        )\n\n\nclass StringIOPayload(StringPayload):\n    def __init__(self, value: IO[str], *args: Any, **kwargs: Any) -> None:\n        super().__init__(value.read(), *args, **kwargs)\n\n\nclass IOBasePayload(Payload):\n    _value: IO[Any]\n\n    def __init__(\n        self, value: IO[Any], disposition: str = \"attachment\", *args: Any, **kwargs: Any\n    ) -> None:\n        if \"filename\" not in kwargs:\n            kwargs[\"filename\"] = guess_filename(value)\n\n        super().__init__(value, *args, **kwargs)\n\n        if self._filename is not None and disposition is not None:\n            if hdrs.CONTENT_DISPOSITION not in self.headers:\n                self.set_content_disposition(disposition, filename=self._filename)\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        loop = asyncio.get_event_loop()\n        try:\n            chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n            while chunk:\n                await writer.write(chunk)\n                chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n        finally:\n            await loop.run_in_executor(None, self._value.close)\n\n\nclass TextIOPayload(IOBasePayload):\n    _value: TextIO\n\n    def __init__(\n        self,\n        value: TextIO,\n        *args: Any,\n        encoding: Optional[str] = None,\n        content_type: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        if encoding is None:\n            if content_type is None:\n                encoding = \"utf-8\"\n                content_type = \"text/plain; charset=utf-8\"\n            else:\n                mimetype = parse_mimetype(content_type)\n                encoding = mimetype.parameters.get(\"charset\", \"utf-8\")\n        else:\n            if content_type is None:\n                content_type = \"text/plain; charset=%s\" % encoding\n\n        super().__init__(\n            value,\n            content_type=content_type,\n            encoding=encoding,\n            *args,\n            **kwargs,\n        )\n\n    @property\n    def size(self) -> Optional[int]:\n        try:\n            return os.fstat(self._value.fileno()).st_size - self._value.tell()\n        except OSError:\n            return None\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        loop = asyncio.get_event_loop()\n        try:\n            chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n            while chunk:\n                data = (\n                    chunk.encode(encoding=self._encoding)\n                    if self._encoding\n                    else chunk.encode()\n                )\n                await writer.write(data)\n                chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n        finally:\n            await loop.run_in_executor(None, self._value.close)\n\n\nclass BytesIOPayload(IOBasePayload):\n    @property\n    def size(self) -> int:\n        position = self._value.tell()\n        end = self._value.seek(0, os.SEEK_END)\n        self._value.seek(position)\n        return end - position\n\n\nclass BufferedReaderPayload(IOBasePayload):\n    @property\n    def size(self) -> Optional[int]:\n        try:\n            return os.fstat(self._value.fileno()).st_size - self._value.tell()\n        except OSError:\n            # data.fileno() is not supported, e.g.\n            # io.BufferedReader(io.BytesIO(b'data'))\n            return None\n\n\nclass JsonPayload(BytesPayload):\n    def __init__(\n        self,\n        value: Any,\n        encoding: str = \"utf-8\",\n        content_type: str = \"application/json\",\n        dumps: JSONEncoder = json.dumps,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(\n            dumps(value).encode(encoding),\n            content_type=content_type,\n            encoding=encoding,\n            *args,\n            **kwargs,\n        )\n\n\nif TYPE_CHECKING:\n    from typing import AsyncIterable, AsyncIterator\n\n    _AsyncIterator = AsyncIterator[bytes]\n    _AsyncIterable = AsyncIterable[bytes]\nelse:\n    from collections.abc import AsyncIterable, AsyncIterator\n\n    _AsyncIterator = AsyncIterator\n    _AsyncIterable = AsyncIterable\n\n\nclass AsyncIterablePayload(Payload):\n    _iter: Optional[_AsyncIterator] = None\n\n    def __init__(self, value: _AsyncIterable, *args: Any, **kwargs: Any) -> None:\n        if not isinstance(value, AsyncIterable):\n            raise TypeError(\n                \"value argument must support \"\n                \"collections.abc.AsyncIterable interface, \"\n                \"got {!r}\".format(type(value))\n            )\n\n        if \"content_type\" not in kwargs:\n            kwargs[\"content_type\"] = \"application/octet-stream\"\n\n        super().__init__(value, *args, **kwargs)\n\n        self._iter = value.__aiter__()\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        if self._iter:\n            try:\n                # iter is not None check prevents rare cases\n                # when the case iterable is used twice\n                while True:\n                    chunk = await self._iter.__anext__()\n                    await writer.write(chunk)\n            except StopAsyncIteration:\n                self._iter = None\n\n\nclass StreamReaderPayload(AsyncIterablePayload):\n    def __init__(self, value: StreamReader, *args: Any, **kwargs: Any) -> None:\n        super().__init__(value.iter_any(), *args, **kwargs)\n\n\nPAYLOAD_REGISTRY = PayloadRegistry()\nPAYLOAD_REGISTRY.register(BytesPayload, (bytes, bytearray, memoryview))\nPAYLOAD_REGISTRY.register(StringPayload, str)\nPAYLOAD_REGISTRY.register(StringIOPayload, io.StringIO)\nPAYLOAD_REGISTRY.register(TextIOPayload, io.TextIOBase)\nPAYLOAD_REGISTRY.register(BytesIOPayload, io.BytesIO)\nPAYLOAD_REGISTRY.register(BufferedReaderPayload, (io.BufferedReader, io.BufferedRandom))\nPAYLOAD_REGISTRY.register(IOBasePayload, io.IOBase)\nPAYLOAD_REGISTRY.register(StreamReaderPayload, StreamReader)\n# try_last for giving a chance to more specialized async interables like\n# multidict.BodyPartReaderPayload override the default\nPAYLOAD_REGISTRY.register(AsyncIterablePayload, AsyncIterable, order=Order.try_last)\n", "aiohttp/abc.py": "import logging\nimport socket\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sized\nfrom http.cookies import BaseCookie, Morsel\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    TypedDict,\n)\n\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom .typedefs import LooseCookies\n\nif TYPE_CHECKING:\n    from .web_app import Application\n    from .web_exceptions import HTTPException\n    from .web_request import BaseRequest, Request\n    from .web_response import StreamResponse\nelse:\n    BaseRequest = Request = Application = StreamResponse = None\n    HTTPException = None\n\n\nclass AbstractRouter(ABC):\n    def __init__(self) -> None:\n        self._frozen = False\n\n    def post_init(self, app: Application) -> None:\n        \"\"\"Post init stage.\n\n        Not an abstract method for sake of backward compatibility,\n        but if the router wants to be aware of the application\n        it can override this.\n        \"\"\"\n\n    @property\n    def frozen(self) -> bool:\n        return self._frozen\n\n    def freeze(self) -> None:\n        \"\"\"Freeze router.\"\"\"\n        self._frozen = True\n\n    @abstractmethod\n    async def resolve(self, request: Request) -> \"AbstractMatchInfo\":\n        \"\"\"Return MATCH_INFO for given request\"\"\"\n\n\nclass AbstractMatchInfo(ABC):\n    @property  # pragma: no branch\n    @abstractmethod\n    def handler(self) -> Callable[[Request], Awaitable[StreamResponse]]:\n        \"\"\"Execute matched request handler\"\"\"\n\n    @property\n    @abstractmethod\n    def expect_handler(\n        self,\n    ) -> Callable[[Request], Awaitable[Optional[StreamResponse]]]:\n        \"\"\"Expect handler for 100-continue processing\"\"\"\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def http_exception(self) -> Optional[HTTPException]:\n        \"\"\"HTTPException instance raised on router's resolving, or None\"\"\"\n\n    @abstractmethod  # pragma: no branch\n    def get_info(self) -> Dict[str, Any]:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def apps(self) -> Tuple[Application, ...]:\n        \"\"\"Stack of nested applications.\n\n        Top level application is left-most element.\n\n        \"\"\"\n\n    @abstractmethod\n    def add_app(self, app: Application) -> None:\n        \"\"\"Add application to the nested apps stack.\"\"\"\n\n    @abstractmethod\n    def freeze(self) -> None:\n        \"\"\"Freeze the match info.\n\n        The method is called after route resolution.\n\n        After the call .add_app() is forbidden.\n\n        \"\"\"\n\n\nclass AbstractView(ABC):\n    \"\"\"Abstract class based view.\"\"\"\n\n    def __init__(self, request: Request) -> None:\n        self._request = request\n\n    @property\n    def request(self) -> Request:\n        \"\"\"Request instance.\"\"\"\n        return self._request\n\n    @abstractmethod\n    def __await__(self) -> Generator[Any, None, StreamResponse]:\n        \"\"\"Execute the view handler.\"\"\"\n\n\nclass ResolveResult(TypedDict):\n    \"\"\"Resolve result.\n\n    This is the result returned from an AbstractResolver's\n    resolve method.\n\n    :param hostname: The hostname that was provided.\n    :param host: The IP address that was resolved.\n    :param port: The port that was resolved.\n    :param family: The address family that was resolved.\n    :param proto: The protocol that was resolved.\n    :param flags: The flags that were resolved.\n    \"\"\"\n\n    hostname: str\n    host: str\n    port: int\n    family: int\n    proto: int\n    flags: int\n\n\nclass AbstractResolver(ABC):\n    \"\"\"Abstract DNS resolver.\"\"\"\n\n    @abstractmethod\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        \"\"\"Return IP address for given hostname\"\"\"\n\n    @abstractmethod\n    async def close(self) -> None:\n        \"\"\"Release resolver\"\"\"\n\n\nif TYPE_CHECKING:\n    IterableBase = Iterable[Morsel[str]]\nelse:\n    IterableBase = Iterable\n\n\nClearCookiePredicate = Callable[[\"Morsel[str]\"], bool]\n\n\nclass AbstractCookieJar(Sized, IterableBase):\n    \"\"\"Abstract Cookie Jar.\"\"\"\n\n    @abstractmethod\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        \"\"\"Clear all cookies if no predicate is passed.\"\"\"\n\n    @abstractmethod\n    def clear_domain(self, domain: str) -> None:\n        \"\"\"Clear all cookies for domain and all subdomains.\"\"\"\n\n    @abstractmethod\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        \"\"\"Update cookies.\"\"\"\n\n    @abstractmethod\n    def filter_cookies(self, request_url: URL) -> \"BaseCookie[str]\":\n        \"\"\"Return the jar's cookies filtered by their attributes.\"\"\"\n\n\nclass AbstractStreamWriter(ABC):\n    \"\"\"Abstract stream writer.\"\"\"\n\n    buffer_size = 0\n    output_size = 0\n    length: Optional[int] = 0\n\n    @abstractmethod\n    async def write(self, chunk: bytes) -> None:\n        \"\"\"Write chunk into stream.\"\"\"\n\n    @abstractmethod\n    async def write_eof(self, chunk: bytes = b\"\") -> None:\n        \"\"\"Write last chunk.\"\"\"\n\n    @abstractmethod\n    async def drain(self) -> None:\n        \"\"\"Flush the write buffer.\"\"\"\n\n    @abstractmethod\n    def enable_compression(self, encoding: str = \"deflate\") -> None:\n        \"\"\"Enable HTTP body compression\"\"\"\n\n    @abstractmethod\n    def enable_chunking(self) -> None:\n        \"\"\"Enable HTTP chunked mode\"\"\"\n\n    @abstractmethod\n    async def write_headers(\n        self, status_line: str, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        \"\"\"Write HTTP headers\"\"\"\n\n\nclass AbstractAccessLogger(ABC):\n    \"\"\"Abstract writer to access log.\"\"\"\n\n    def __init__(self, logger: logging.Logger, log_format: str) -> None:\n        self.logger = logger\n        self.log_format = log_format\n\n    @abstractmethod\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\n        \"\"\"Emit log to logger.\"\"\"\n\n\nclass AbstractAsyncAccessLogger(ABC):\n    \"\"\"Abstract asynchronous writer to access log.\"\"\"\n\n    @abstractmethod\n    async def log(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        \"\"\"Emit log to logger.\"\"\"\n", "aiohttp/helpers.py": "\"\"\"Various helper functions\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport contextlib\nimport dataclasses\nimport datetime\nimport enum\nimport functools\nimport inspect\nimport netrc\nimport os\nimport platform\nimport re\nimport sys\nimport time\nimport warnings\nimport weakref\nfrom collections import namedtuple\nfrom contextlib import suppress\nfrom email.parser import HeaderParser\nfrom email.utils import parsedate\nfrom http.cookies import SimpleCookie\nfrom math import ceil\nfrom pathlib import Path\nfrom types import TracebackType\nfrom typing import (\n    Any,\n    Callable,\n    ContextManager,\n    Dict,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Pattern,\n    Protocol,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    final,\n    get_args,\n    overload,\n)\nfrom urllib.parse import quote\nfrom urllib.request import getproxies, proxy_bypass\n\nfrom multidict import CIMultiDict, MultiDict, MultiDictProxy\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .log import client_logger\nfrom .typedefs import PathLike  # noqa\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\n__all__ = (\"BasicAuth\", \"ChainMapProxy\", \"ETag\")\n\nPY_310 = sys.version_info >= (3, 10)\n\nCOOKIE_MAX_LENGTH = 4096\n\n_T = TypeVar(\"_T\")\n_S = TypeVar(\"_S\")\n\n_SENTINEL = enum.Enum(\"_SENTINEL\", \"sentinel\")\nsentinel = _SENTINEL.sentinel\n\nNO_EXTENSIONS = bool(os.environ.get(\"AIOHTTP_NO_EXTENSIONS\"))\n\nDEBUG = sys.flags.dev_mode or (\n    not sys.flags.ignore_environment and bool(os.environ.get(\"PYTHONASYNCIODEBUG\"))\n)\n\n\nCHAR = {chr(i) for i in range(0, 128)}\nCTL = {chr(i) for i in range(0, 32)} | {\n    chr(127),\n}\nSEPARATORS = {\n    \"(\",\n    \")\",\n    \"<\",\n    \">\",\n    \"@\",\n    \",\",\n    \";\",\n    \":\",\n    \"\\\\\",\n    '\"',\n    \"/\",\n    \"[\",\n    \"]\",\n    \"?\",\n    \"=\",\n    \"{\",\n    \"}\",\n    \" \",\n    chr(9),\n}\nTOKEN = CHAR ^ CTL ^ SEPARATORS\n\n\nclass noop:\n    def __await__(self) -> Generator[None, None, None]:\n        yield\n\n\njson_re = re.compile(r\"(?:application/|[\\w.-]+/[\\w.+-]+?\\+)json$\", re.IGNORECASE)\n\n\nclass BasicAuth(namedtuple(\"BasicAuth\", [\"login\", \"password\", \"encoding\"])):\n    \"\"\"Http basic authentication helper.\"\"\"\n\n    def __new__(\n        cls, login: str, password: str = \"\", encoding: str = \"latin1\"\n    ) -> \"BasicAuth\":\n        if login is None:\n            raise ValueError(\"None is not allowed as login value\")\n\n        if password is None:\n            raise ValueError(\"None is not allowed as password value\")\n\n        if \":\" in login:\n            raise ValueError('A \":\" is not allowed in login (RFC 1945#section-11.1)')\n\n        return super().__new__(cls, login, password, encoding)\n\n    @classmethod\n    def decode(cls, auth_header: str, encoding: str = \"latin1\") -> \"BasicAuth\":\n        \"\"\"Create a BasicAuth object from an Authorization HTTP header.\"\"\"\n        try:\n            auth_type, encoded_credentials = auth_header.split(\" \", 1)\n        except ValueError:\n            raise ValueError(\"Could not parse authorization header.\")\n\n        if auth_type.lower() != \"basic\":\n            raise ValueError(\"Unknown authorization method %s\" % auth_type)\n\n        try:\n            decoded = base64.b64decode(\n                encoded_credentials.encode(\"ascii\"), validate=True\n            ).decode(encoding)\n        except binascii.Error:\n            raise ValueError(\"Invalid base64 encoding.\")\n\n        try:\n            # RFC 2617 HTTP Authentication\n            # https://www.ietf.org/rfc/rfc2617.txt\n            # the colon must be present, but the username and password may be\n            # otherwise blank.\n            username, password = decoded.split(\":\", 1)\n        except ValueError:\n            raise ValueError(\"Invalid credentials.\")\n\n        return cls(username, password, encoding=encoding)\n\n    @classmethod\n    def from_url(cls, url: URL, *, encoding: str = \"latin1\") -> Optional[\"BasicAuth\"]:\n        \"\"\"Create BasicAuth from url.\"\"\"\n        if not isinstance(url, URL):\n            raise TypeError(\"url should be yarl.URL instance\")\n        if url.user is None:\n            return None\n        return cls(url.user, url.password or \"\", encoding=encoding)\n\n    def encode(self) -> str:\n        \"\"\"Encode credentials.\"\"\"\n        creds = (f\"{self.login}:{self.password}\").encode(self.encoding)\n        return \"Basic %s\" % base64.b64encode(creds).decode(self.encoding)\n\n\ndef strip_auth_from_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\n    auth = BasicAuth.from_url(url)\n    if auth is None:\n        return url, None\n    else:\n        return url.with_user(None), auth\n\n\ndef netrc_from_env() -> Optional[netrc.netrc]:\n    \"\"\"Load netrc from file.\n\n    Attempt to load it from the path specified by the env-var\n    NETRC or in the default location in the user's home directory.\n\n    Returns None if it couldn't be found or fails to parse.\n    \"\"\"\n    netrc_env = os.environ.get(\"NETRC\")\n\n    if netrc_env is not None:\n        netrc_path = Path(netrc_env)\n    else:\n        try:\n            home_dir = Path.home()\n        except RuntimeError as e:  # pragma: no cover\n            # if pathlib can't resolve home, it may raise a RuntimeError\n            client_logger.debug(\n                \"Could not resolve home directory when \"\n                \"trying to look for .netrc file: %s\",\n                e,\n            )\n            return None\n\n        netrc_path = home_dir / (\n            \"_netrc\" if platform.system() == \"Windows\" else \".netrc\"\n        )\n\n    try:\n        return netrc.netrc(str(netrc_path))\n    except netrc.NetrcParseError as e:\n        client_logger.warning(\"Could not parse .netrc file: %s\", e)\n    except OSError as e:\n        netrc_exists = False\n        with contextlib.suppress(OSError):\n            netrc_exists = netrc_path.is_file()\n        # we couldn't read the file (doesn't exist, permissions, etc.)\n        if netrc_env or netrc_exists:\n            # only warn if the environment wanted us to load it,\n            # or it appears like the default file does actually exist\n            client_logger.warning(\"Could not read .netrc file: %s\", e)\n\n    return None\n\n\n@dataclasses.dataclass(frozen=True)\nclass ProxyInfo:\n    proxy: URL\n    proxy_auth: Optional[BasicAuth]\n\n\ndef basicauth_from_netrc(netrc_obj: Optional[netrc.netrc], host: str) -> BasicAuth:\n    \"\"\"\n    Return :py:class:`~aiohttp.BasicAuth` credentials for ``host`` from ``netrc_obj``.\n\n    :raises LookupError: if ``netrc_obj`` is :py:data:`None` or if no\n            entry is found for the ``host``.\n    \"\"\"\n    if netrc_obj is None:\n        raise LookupError(\"No .netrc file found\")\n    auth_from_netrc = netrc_obj.authenticators(host)\n\n    if auth_from_netrc is None:\n        raise LookupError(f\"No entry for {host!s} found in the `.netrc` file.\")\n    login, account, password = auth_from_netrc\n\n    # TODO(PY311): username = login or account\n    # Up to python 3.10, account could be None if not specified,\n    # and login will be empty string if not specified. From 3.11,\n    # login and account will be empty string if not specified.\n    username = login if (login or account is None) else account\n\n    # TODO(PY311): Remove this, as password will be empty string\n    # if not specified\n    if password is None:\n        password = \"\"\n\n    return BasicAuth(username, password)\n\n\ndef proxies_from_env() -> Dict[str, ProxyInfo]:\n    proxy_urls = {\n        k: URL(v)\n        for k, v in getproxies().items()\n        if k in (\"http\", \"https\", \"ws\", \"wss\")\n    }\n    netrc_obj = netrc_from_env()\n    stripped = {k: strip_auth_from_url(v) for k, v in proxy_urls.items()}\n    ret = {}\n    for proto, val in stripped.items():\n        proxy, auth = val\n        if proxy.scheme in (\"https\", \"wss\"):\n            client_logger.warning(\n                \"%s proxies %s are not supported, ignoring\", proxy.scheme.upper(), proxy\n            )\n            continue\n        if netrc_obj and auth is None:\n            if proxy.host is not None:\n                try:\n                    auth = basicauth_from_netrc(netrc_obj, proxy.host)\n                except LookupError:\n                    auth = None\n        ret[proto] = ProxyInfo(proxy, auth)\n    return ret\n\n\ndef get_env_proxy_for_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\n    \"\"\"Get a permitted proxy for the given URL from the env.\"\"\"\n    if url.host is not None and proxy_bypass(url.host):\n        raise LookupError(f\"Proxying is disallowed for `{url.host!r}`\")\n\n    proxies_in_env = proxies_from_env()\n    try:\n        proxy_info = proxies_in_env[url.scheme]\n    except KeyError:\n        raise LookupError(f\"No proxies found for `{url!s}` in the env\")\n    else:\n        return proxy_info.proxy, proxy_info.proxy_auth\n\n\n@dataclasses.dataclass(frozen=True)\nclass MimeType:\n    type: str\n    subtype: str\n    suffix: str\n    parameters: \"MultiDictProxy[str]\"\n\n\n@functools.lru_cache(maxsize=56)\ndef parse_mimetype(mimetype: str) -> MimeType:\n    \"\"\"Parses a MIME type into its components.\n\n    mimetype is a MIME type string.\n\n    Returns a MimeType object.\n\n    Example:\n\n    >>> parse_mimetype('text/html; charset=utf-8')\n    MimeType(type='text', subtype='html', suffix='',\n             parameters={'charset': 'utf-8'})\n\n    \"\"\"\n    if not mimetype:\n        return MimeType(\n            type=\"\", subtype=\"\", suffix=\"\", parameters=MultiDictProxy(MultiDict())\n        )\n\n    parts = mimetype.split(\";\")\n    params: MultiDict[str] = MultiDict()\n    for item in parts[1:]:\n        if not item:\n            continue\n        key, _, value = item.partition(\"=\")\n        params.add(key.lower().strip(), value.strip(' \"'))\n\n    fulltype = parts[0].strip().lower()\n    if fulltype == \"*\":\n        fulltype = \"*/*\"\n\n    mtype, _, stype = fulltype.partition(\"/\")\n    stype, _, suffix = stype.partition(\"+\")\n\n    return MimeType(\n        type=mtype, subtype=stype, suffix=suffix, parameters=MultiDictProxy(params)\n    )\n\n\ndef guess_filename(obj: Any, default: Optional[str] = None) -> Optional[str]:\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, str) and name[0] != \"<\" and name[-1] != \">\":\n        return Path(name).name\n    return default\n\n\nnot_qtext_re = re.compile(r\"[^\\041\\043-\\133\\135-\\176]\")\nQCONTENT = {chr(i) for i in range(0x20, 0x7F)} | {\"\\t\"}\n\n\ndef quoted_string(content: str) -> str:\n    \"\"\"Return 7-bit content as quoted-string.\n\n    Format content into a quoted-string as defined in RFC5322 for\n    Internet Message Format. Notice that this is not the 8-bit HTTP\n    format, but the 7-bit email format. Content must be in usascii or\n    a ValueError is raised.\n    \"\"\"\n    if not (QCONTENT > set(content)):\n        raise ValueError(f\"bad content for quoted-string {content!r}\")\n    return not_qtext_re.sub(lambda x: \"\\\\\" + x.group(0), content)\n\n\ndef content_disposition_header(\n    disptype: str, quote_fields: bool = True, _charset: str = \"utf-8\", **params: str\n) -> str:\n    \"\"\"Sets ``Content-Disposition`` header for MIME.\n\n    This is the MIME payload Content-Disposition header from RFC 2183\n    and RFC 7579 section 4.2, not the HTTP Content-Disposition from\n    RFC 6266.\n\n    disptype is a disposition type: inline, attachment, form-data.\n    Should be valid extension token (see RFC 2183)\n\n    quote_fields performs value quoting to 7-bit MIME headers\n    according to RFC 7578. Set to quote_fields to False if recipient\n    can take 8-bit file names and field values.\n\n    _charset specifies the charset to use when quote_fields is True.\n\n    params is a dict with disposition params.\n    \"\"\"\n    if not disptype or not (TOKEN > set(disptype)):\n        raise ValueError(\"bad content disposition type {!r}\" \"\".format(disptype))\n\n    value = disptype\n    if params:\n        lparams = []\n        for key, val in params.items():\n            if not key or not (TOKEN > set(key)):\n                raise ValueError(\n                    \"bad content disposition parameter\" \" {!r}={!r}\".format(key, val)\n                )\n            if quote_fields:\n                if key.lower() == \"filename\":\n                    qval = quote(val, \"\", encoding=_charset)\n                    lparams.append((key, '\"%s\"' % qval))\n                else:\n                    try:\n                        qval = quoted_string(val)\n                    except ValueError:\n                        qval = \"\".join(\n                            (_charset, \"''\", quote(val, \"\", encoding=_charset))\n                        )\n                        lparams.append((key + \"*\", qval))\n                    else:\n                        lparams.append((key, '\"%s\"' % qval))\n            else:\n                qval = val.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n                lparams.append((key, '\"%s\"' % qval))\n        sparams = \"; \".join(\"=\".join(pair) for pair in lparams)\n        value = \"; \".join((value, sparams))\n    return value\n\n\ndef is_expected_content_type(\n    response_content_type: str, expected_content_type: str\n) -> bool:\n    \"\"\"Checks if received content type is processable as an expected one.\n\n    Both arguments should be given without parameters.\n    \"\"\"\n    if expected_content_type == \"application/json\":\n        return json_re.match(response_content_type) is not None\n    return expected_content_type in response_content_type\n\n\nclass _TSelf(Protocol, Generic[_T]):\n    _cache: Dict[str, _T]\n\n\nclass reify(Generic[_T]):\n    \"\"\"Use as a class method decorator.\n\n    It operates almost exactly like\n    the Python `@property` decorator, but it puts the result of the\n    method it decorates into the instance dict after the first call,\n    effectively replacing the function it decorates with an instance\n    variable.  It is, in Python parlance, a data descriptor.\n    \"\"\"\n\n    def __init__(self, wrapped: Callable[..., _T]) -> None:\n        self.wrapped = wrapped\n        self.__doc__ = wrapped.__doc__\n        self.name = wrapped.__name__\n\n    def __get__(self, inst: _TSelf[_T], owner: Optional[Type[Any]] = None) -> _T:\n        try:\n            try:\n                return inst._cache[self.name]\n            except KeyError:\n                val = self.wrapped(inst)\n                inst._cache[self.name] = val\n                return val\n        except AttributeError:\n            if inst is None:\n                return self\n            raise\n\n    def __set__(self, inst: _TSelf[_T], value: _T) -> None:\n        raise AttributeError(\"reified property is read-only\")\n\n\nreify_py = reify\n\ntry:\n    from ._helpers import reify as reify_c\n\n    if not NO_EXTENSIONS:\n        reify = reify_c  # type: ignore[misc,assignment]\nexcept ImportError:\n    pass\n\n_ipv4_pattern = (\n    r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}\"\n    r\"(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n)\n_ipv6_pattern = (\n    r\"^(?:(?:(?:[A-F0-9]{1,4}:){6}|(?=(?:[A-F0-9]{0,4}:){0,6}\"\n    r\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$)(([0-9A-F]{1,4}:){0,5}|:)\"\n    r\"((:[0-9A-F]{1,4}){1,5}:|:)|::(?:[A-F0-9]{1,4}:){5})\"\n    r\"(?:(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\.){3}\"\n    r\"(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])|(?:[A-F0-9]{1,4}:){7}\"\n    r\"[A-F0-9]{1,4}|(?=(?:[A-F0-9]{0,4}:){0,7}[A-F0-9]{0,4}$)\"\n    r\"(([0-9A-F]{1,4}:){1,7}|:)((:[0-9A-F]{1,4}){1,7}|:)|(?:[A-F0-9]{1,4}:){7}\"\n    r\":|:(:[A-F0-9]{1,4}){7})$\"\n)\n_ipv4_regex = re.compile(_ipv4_pattern)\n_ipv6_regex = re.compile(_ipv6_pattern, flags=re.IGNORECASE)\n_ipv4_regexb = re.compile(_ipv4_pattern.encode(\"ascii\"))\n_ipv6_regexb = re.compile(_ipv6_pattern.encode(\"ascii\"), flags=re.IGNORECASE)\n\n\ndef _is_ip_address(\n    regex: Pattern[str], regexb: Pattern[bytes], host: Optional[Union[str, bytes]]\n) -> bool:\n    if host is None:\n        return False\n    if isinstance(host, str):\n        return bool(regex.match(host))\n    elif isinstance(host, (bytes, bytearray, memoryview)):\n        return bool(regexb.match(host))\n    else:\n        raise TypeError(f\"{host} [{type(host)}] is not a str or bytes\")\n\n\nis_ipv4_address = functools.partial(_is_ip_address, _ipv4_regex, _ipv4_regexb)\nis_ipv6_address = functools.partial(_is_ip_address, _ipv6_regex, _ipv6_regexb)\n\n\ndef is_ip_address(host: Optional[Union[str, bytes, bytearray, memoryview]]) -> bool:\n    return is_ipv4_address(host) or is_ipv6_address(host)\n\n\n_cached_current_datetime: Optional[int] = None\n_cached_formatted_datetime = \"\"\n\n\ndef rfc822_formatted_time() -> str:\n    global _cached_current_datetime\n    global _cached_formatted_datetime\n\n    now = int(time.time())\n    if now != _cached_current_datetime:\n        # Weekday and month names for HTTP date/time formatting;\n        # always English!\n        # Tuples are constants stored in codeobject!\n        _weekdayname = (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n        _monthname = (\n            \"\",  # Dummy so we can use 1-based month numbers\n            \"Jan\",\n            \"Feb\",\n            \"Mar\",\n            \"Apr\",\n            \"May\",\n            \"Jun\",\n            \"Jul\",\n            \"Aug\",\n            \"Sep\",\n            \"Oct\",\n            \"Nov\",\n            \"Dec\",\n        )\n\n        year, month, day, hh, mm, ss, wd, *tail = time.gmtime(now)\n        _cached_formatted_datetime = \"%s, %02d %3s %4d %02d:%02d:%02d GMT\" % (\n            _weekdayname[wd],\n            day,\n            _monthname[month],\n            year,\n            hh,\n            mm,\n            ss,\n        )\n        _cached_current_datetime = now\n    return _cached_formatted_datetime\n\n\ndef _weakref_handle(info: \"Tuple[weakref.ref[object], str]\") -> None:\n    ref, name = info\n    ob = ref()\n    if ob is not None:\n        with suppress(Exception):\n            getattr(ob, name)()\n\n\ndef weakref_handle(\n    ob: object,\n    name: str,\n    timeout: Optional[float],\n    loop: asyncio.AbstractEventLoop,\n    timeout_ceil_threshold: float = 5,\n) -> Optional[asyncio.TimerHandle]:\n    if timeout is not None and timeout > 0:\n        when = loop.time() + timeout\n        if timeout >= timeout_ceil_threshold:\n            when = ceil(when)\n\n        return loop.call_at(when, _weakref_handle, (weakref.ref(ob), name))\n    return None\n\n\ndef call_later(\n    cb: Callable[[], Any],\n    timeout: Optional[float],\n    loop: asyncio.AbstractEventLoop,\n    timeout_ceil_threshold: float = 5,\n) -> Optional[asyncio.TimerHandle]:\n    if timeout is not None and timeout > 0:\n        when = loop.time() + timeout\n        if timeout > timeout_ceil_threshold:\n            when = ceil(when)\n        return loop.call_at(when, cb)\n    return None\n\n\nclass TimeoutHandle:\n    \"\"\"Timeout handle\"\"\"\n\n    def __init__(\n        self,\n        loop: asyncio.AbstractEventLoop,\n        timeout: Optional[float],\n        ceil_threshold: float = 5,\n    ) -> None:\n        self._timeout = timeout\n        self._loop = loop\n        self._ceil_threshold = ceil_threshold\n        self._callbacks: List[\n            Tuple[Callable[..., None], Tuple[Any, ...], Dict[str, Any]]\n        ] = []\n\n    def register(\n        self, callback: Callable[..., None], *args: Any, **kwargs: Any\n    ) -> None:\n        self._callbacks.append((callback, args, kwargs))\n\n    def close(self) -> None:\n        self._callbacks.clear()\n\n    def start(self) -> Optional[asyncio.Handle]:\n        timeout = self._timeout\n        if timeout is not None and timeout > 0:\n            when = self._loop.time() + timeout\n            if timeout >= self._ceil_threshold:\n                when = ceil(when)\n            return self._loop.call_at(when, self.__call__)\n        else:\n            return None\n\n    def timer(self) -> \"BaseTimerContext\":\n        if self._timeout is not None and self._timeout > 0:\n            timer = TimerContext(self._loop)\n            self.register(timer.timeout)\n            return timer\n        else:\n            return TimerNoop()\n\n    def __call__(self) -> None:\n        for cb, args, kwargs in self._callbacks:\n            with suppress(Exception):\n                cb(*args, **kwargs)\n\n        self._callbacks.clear()\n\n\nclass BaseTimerContext(ContextManager[\"BaseTimerContext\"]):\n    def assert_timeout(self) -> None:\n        \"\"\"Raise TimeoutError if timeout has been exceeded.\"\"\"\n\n\nclass TimerNoop(BaseTimerContext):\n    def __enter__(self) -> BaseTimerContext:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        return\n\n\nclass TimerContext(BaseTimerContext):\n    \"\"\"Low resolution timeout context manager\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop = loop\n        self._tasks: List[asyncio.Task[Any]] = []\n        self._cancelled = False\n\n    def assert_timeout(self) -> None:\n        \"\"\"Raise TimeoutError if timer has already been cancelled.\"\"\"\n        if self._cancelled:\n            raise asyncio.TimeoutError from None\n\n    def __enter__(self) -> BaseTimerContext:\n        task = asyncio.current_task(loop=self._loop)\n\n        if task is None:\n            raise RuntimeError(\n                \"Timeout context manager should be used \" \"inside a task\"\n            )\n\n        if self._cancelled:\n            raise asyncio.TimeoutError from None\n\n        self._tasks.append(task)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        if self._tasks:\n            self._tasks.pop()  # type: ignore[unused-awaitable]\n\n        if exc_type is asyncio.CancelledError and self._cancelled:\n            raise asyncio.TimeoutError from None\n        return None\n\n    def timeout(self) -> None:\n        if not self._cancelled:\n            for task in set(self._tasks):\n                task.cancel()\n\n            self._cancelled = True\n\n\ndef ceil_timeout(\n    delay: Optional[float], ceil_threshold: float = 5\n) -> async_timeout.Timeout:\n    if delay is None or delay <= 0:\n        return async_timeout.timeout(None)\n\n    loop = asyncio.get_running_loop()\n    now = loop.time()\n    when = now + delay\n    if delay > ceil_threshold:\n        when = ceil(when)\n    return async_timeout.timeout_at(when)\n\n\nclass HeadersMixin:\n    __slots__ = (\"_content_type\", \"_content_dict\", \"_stored_content_type\")\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._content_type: Optional[str] = None\n        self._content_dict: Optional[Dict[str, str]] = None\n        self._stored_content_type: Union[str, _SENTINEL] = sentinel\n\n    def _parse_content_type(self, raw: str) -> None:\n        self._stored_content_type = raw\n        if raw is None:\n            # default value according to RFC 2616\n            self._content_type = \"application/octet-stream\"\n            self._content_dict = {}\n        else:\n            msg = HeaderParser().parsestr(\"Content-Type: \" + raw)\n            self._content_type = msg.get_content_type()\n            params = msg.get_params(())\n            self._content_dict = dict(params[1:])  # First element is content type again\n\n    @property\n    def content_type(self) -> str:\n        \"\"\"The value of content part for Content-Type HTTP header.\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)  # type: ignore[attr-defined]\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        return self._content_type  # type: ignore[return-value]\n\n    @property\n    def charset(self) -> Optional[str]:\n        \"\"\"The value of charset part for Content-Type HTTP header.\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)  # type: ignore[attr-defined]\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        return self._content_dict.get(\"charset\")  # type: ignore[union-attr]\n\n    @property\n    def content_length(self) -> Optional[int]:\n        \"\"\"The value of Content-Length HTTP header.\"\"\"\n        content_length = self._headers.get(  # type: ignore[attr-defined]\n            hdrs.CONTENT_LENGTH\n        )\n\n        if content_length is not None:\n            return int(content_length)\n        else:\n            return None\n\n\ndef set_result(fut: \"asyncio.Future[_T]\", result: _T) -> None:\n    if not fut.done():\n        fut.set_result(result)\n\n\n_EXC_SENTINEL = BaseException()\n\n\nclass ErrorableProtocol(Protocol):\n    def set_exception(\n        self,\n        exc: BaseException,\n        exc_cause: BaseException = ...,\n    ) -> None: ...  # pragma: no cover\n\n\ndef set_exception(\n    fut: \"asyncio.Future[_T] | ErrorableProtocol\",\n    exc: BaseException,\n    exc_cause: BaseException = _EXC_SENTINEL,\n) -> None:\n    \"\"\"Set future exception.\n\n    If the future is marked as complete, this function is a no-op.\n\n    :param exc_cause: An exception that is a direct cause of ``exc``.\n                      Only set if provided.\n    \"\"\"\n    if asyncio.isfuture(fut) and fut.done():\n        return\n\n    exc_is_sentinel = exc_cause is _EXC_SENTINEL\n    exc_causes_itself = exc is exc_cause\n    if not exc_is_sentinel and not exc_causes_itself:\n        exc.__cause__ = exc_cause\n\n    fut.set_exception(exc)\n\n\n@functools.total_ordering\nclass AppKey(Generic[_T]):\n    \"\"\"Keys for static typing support in Application.\"\"\"\n\n    __slots__ = (\"_name\", \"_t\", \"__orig_class__\")\n\n    # This may be set by Python when instantiating with a generic type. We need to\n    # support this, in order to support types that are not concrete classes,\n    # like Iterable, which can't be passed as the second parameter to __init__.\n    __orig_class__: Type[object]\n\n    def __init__(self, name: str, t: Optional[Type[_T]] = None):\n        # Prefix with module name to help deduplicate key names.\n        frame = inspect.currentframe()\n        while frame:\n            if frame.f_code.co_name == \"<module>\":\n                module: str = frame.f_globals[\"__name__\"]\n                break\n            frame = frame.f_back\n        else:\n            raise RuntimeError(\"Failed to get module name.\")\n\n        # https://github.com/python/mypy/issues/14209\n        self._name = module + \".\" + name  # type: ignore[possibly-undefined]\n        self._t = t\n\n    def __lt__(self, other: object) -> bool:\n        if isinstance(other, AppKey):\n            return self._name < other._name\n        return True  # Order AppKey above other types.\n\n    def __repr__(self) -> str:\n        t = self._t\n        if t is None:\n            with suppress(AttributeError):\n                # Set to type arg.\n                t = get_args(self.__orig_class__)[0]\n\n        if t is None:\n            t_repr = \"<<Unknown>>\"\n        elif isinstance(t, type):\n            if t.__module__ == \"builtins\":\n                t_repr = t.__qualname__\n            else:\n                t_repr = f\"{t.__module__}.{t.__qualname__}\"\n        else:\n            t_repr = repr(t)\n        return f\"<AppKey({self._name}, type={t_repr})>\"\n\n\n@final\nclass ChainMapProxy(Mapping[Union[str, AppKey[Any]], Any]):\n    __slots__ = (\"_maps\",)\n\n    def __init__(self, maps: Iterable[Mapping[Union[str, AppKey[Any]], Any]]) -> None:\n        self._maps = tuple(maps)\n\n    def __init_subclass__(cls) -> None:\n        raise TypeError(\n            \"Inheritance class {} from ChainMapProxy \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    @overload  # type: ignore[override]\n    def __getitem__(self, key: AppKey[_T]) -> _T: ...\n\n    @overload\n    def __getitem__(self, key: str) -> Any: ...\n\n    def __getitem__(self, key: Union[str, AppKey[_T]]) -> Any:\n        for mapping in self._maps:\n            try:\n                return mapping[key]\n            except KeyError:\n                pass\n        raise KeyError(key)\n\n    @overload  # type: ignore[override]\n    def get(self, key: AppKey[_T], default: _S) -> Union[_T, _S]: ...\n\n    @overload\n    def get(self, key: AppKey[_T], default: None = ...) -> Optional[_T]: ...\n\n    @overload\n    def get(self, key: str, default: Any = ...) -> Any: ...\n\n    def get(self, key: Union[str, AppKey[_T]], default: Any = None) -> Any:\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __len__(self) -> int:\n        # reuses stored hash values if possible\n        return len(set().union(*self._maps))\n\n    def __iter__(self) -> Iterator[Union[str, AppKey[Any]]]:\n        d: Dict[Union[str, AppKey[Any]], Any] = {}\n        for mapping in reversed(self._maps):\n            # reuses stored hash values if possible\n            d.update(mapping)\n        return iter(d)\n\n    def __contains__(self, key: object) -> bool:\n        return any(key in m for m in self._maps)\n\n    def __bool__(self) -> bool:\n        return any(self._maps)\n\n    def __repr__(self) -> str:\n        content = \", \".join(map(repr, self._maps))\n        return f\"ChainMapProxy({content})\"\n\n\nclass CookieMixin:\n    # The `_cookies` slots is not defined here because non-empty slots cannot\n    # be combined with an Exception base class, as is done in HTTPException.\n    # CookieMixin subclasses with slots should define the `_cookies`\n    # slot themselves.\n    __slots__ = ()\n\n    def __init__(self) -> None:\n        super().__init__()\n        # Mypy doesn't like that _cookies isn't in __slots__.\n        # See the comment on this class's __slots__ for why this is OK.\n        self._cookies = SimpleCookie()  # type: ignore[misc]\n\n    @property\n    def cookies(self) -> SimpleCookie:\n        return self._cookies\n\n    def set_cookie(\n        self,\n        name: str,\n        value: str,\n        *,\n        expires: Optional[str] = None,\n        domain: Optional[str] = None,\n        max_age: Optional[Union[int, str]] = None,\n        path: str = \"/\",\n        secure: Optional[bool] = None,\n        httponly: Optional[bool] = None,\n        version: Optional[str] = None,\n        samesite: Optional[str] = None,\n    ) -> None:\n        \"\"\"Set or update response cookie.\n\n        Sets new cookie or updates existent with new value.\n        Also updates only those params which are not None.\n        \"\"\"\n        old = self._cookies.get(name)\n        if old is not None and old.coded_value == \"\":\n            # deleted cookie\n            self._cookies.pop(name, None)\n\n        self._cookies[name] = value\n        c = self._cookies[name]\n\n        if expires is not None:\n            c[\"expires\"] = expires\n        elif c.get(\"expires\") == \"Thu, 01 Jan 1970 00:00:00 GMT\":\n            del c[\"expires\"]\n\n        if domain is not None:\n            c[\"domain\"] = domain\n\n        if max_age is not None:\n            c[\"max-age\"] = str(max_age)\n        elif \"max-age\" in c:\n            del c[\"max-age\"]\n\n        c[\"path\"] = path\n\n        if secure is not None:\n            c[\"secure\"] = secure\n        if httponly is not None:\n            c[\"httponly\"] = httponly\n        if version is not None:\n            c[\"version\"] = version\n        if samesite is not None:\n            c[\"samesite\"] = samesite\n\n        if DEBUG:\n            cookie_length = len(c.output(header=\"\")[1:])\n            if cookie_length > COOKIE_MAX_LENGTH:\n                warnings.warn(\n                    \"The size of is too large, it might get ignored by the client.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n    def del_cookie(\n        self, name: str, *, domain: Optional[str] = None, path: str = \"/\"\n    ) -> None:\n        \"\"\"Delete cookie.\n\n        Creates new empty expired cookie.\n        \"\"\"\n        # TODO: do we need domain/path here?\n        self._cookies.pop(name, None)\n        self.set_cookie(\n            name,\n            \"\",\n            max_age=0,\n            expires=\"Thu, 01 Jan 1970 00:00:00 GMT\",\n            domain=domain,\n            path=path,\n        )\n\n\ndef populate_with_cookies(headers: \"CIMultiDict[str]\", cookies: SimpleCookie) -> None:\n    for cookie in cookies.values():\n        value = cookie.output(header=\"\")[1:]\n        headers.add(hdrs.SET_COOKIE, value)\n\n\n# https://tools.ietf.org/html/rfc7232#section-2.3\n_ETAGC = r\"[!\\x23-\\x7E\\x80-\\xff]+\"\n_ETAGC_RE = re.compile(_ETAGC)\n_QUOTED_ETAG = rf'(W/)?\"({_ETAGC})\"'\nQUOTED_ETAG_RE = re.compile(_QUOTED_ETAG)\nLIST_QUOTED_ETAG_RE = re.compile(rf\"({_QUOTED_ETAG})(?:\\s*,\\s*|$)|(.)\")\n\nETAG_ANY = \"*\"\n\n\n@dataclasses.dataclass(frozen=True)\nclass ETag:\n    value: str\n    is_weak: bool = False\n\n\ndef validate_etag_value(value: str) -> None:\n    if value != ETAG_ANY and not _ETAGC_RE.fullmatch(value):\n        raise ValueError(\n            f\"Value {value!r} is not a valid etag. Maybe it contains '\\\"'?\"\n        )\n\n\ndef parse_http_date(date_str: Optional[str]) -> Optional[datetime.datetime]:\n    \"\"\"Process a date string, return a datetime object\"\"\"\n    if date_str is not None:\n        timetuple = parsedate(date_str)\n        if timetuple is not None:\n            with suppress(ValueError):\n                return datetime.datetime(*timetuple[:6], tzinfo=datetime.timezone.utc)\n    return None\n\n\ndef must_be_empty_body(method: str, code: int) -> bool:\n    \"\"\"Check if a request must return an empty body.\"\"\"\n    return (\n        status_code_must_be_empty_body(code)\n        or method_must_be_empty_body(method)\n        or (200 <= code < 300 and method.upper() == hdrs.METH_CONNECT)\n    )\n\n\ndef method_must_be_empty_body(method: str) -> bool:\n    \"\"\"Check if a method must return an empty body.\"\"\"\n    # https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.1\n    # https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.2\n    return method.upper() == hdrs.METH_HEAD\n\n\ndef status_code_must_be_empty_body(code: int) -> bool:\n    \"\"\"Check if a status code must return an empty body.\"\"\"\n    # https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.1\n    return code in {204, 304} or 100 <= code < 200\n\n\ndef should_remove_content_length(method: str, code: int) -> bool:\n    \"\"\"Check if a Content-Length header should be removed.\n\n    This should always be a subset of must_be_empty_body\n    \"\"\"\n    # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.6-8\n    # https://www.rfc-editor.org/rfc/rfc9110.html#section-15.4.5-4\n    return (\n        code in {204, 304}\n        or 100 <= code < 200\n        or (200 <= code < 300 and method.upper() == hdrs.METH_CONNECT)\n    )\n", "aiohttp/web_log.py": "import datetime\nimport functools\nimport logging\nimport os\nimport re\nimport time as time_mod\nfrom collections import namedtuple\nfrom typing import Any, Callable, Dict, Iterable, List, Tuple  # noqa\n\nfrom .abc import AbstractAccessLogger\nfrom .web_request import BaseRequest\nfrom .web_response import StreamResponse\n\nKeyMethod = namedtuple(\"KeyMethod\", \"key method\")\n\n\nclass AccessLogger(AbstractAccessLogger):\n    \"\"\"Helper object to log access.\n\n    Usage:\n        log = logging.getLogger(\"spam\")\n        log_format = \"%a %{User-Agent}i\"\n        access_logger = AccessLogger(log, log_format)\n        access_logger.log(request, response, time)\n\n    Format:\n        %%  The percent sign\n        %a  Remote IP-address (IP-address of proxy if using reverse proxy)\n        %t  Time when the request was started to process\n        %P  The process ID of the child that serviced the request\n        %r  First line of request\n        %s  Response status code\n        %b  Size of response in bytes, including HTTP headers\n        %T  Time taken to serve the request, in seconds\n        %Tf Time taken to serve the request, in seconds with floating fraction\n            in .06f format\n        %D  Time taken to serve the request, in microseconds\n        %{FOO}i  request.headers['FOO']\n        %{FOO}o  response.headers['FOO']\n        %{FOO}e  os.environ['FOO']\n\n    \"\"\"\n\n    LOG_FORMAT_MAP = {\n        \"a\": \"remote_address\",\n        \"t\": \"request_start_time\",\n        \"P\": \"process_id\",\n        \"r\": \"first_request_line\",\n        \"s\": \"response_status\",\n        \"b\": \"response_size\",\n        \"T\": \"request_time\",\n        \"Tf\": \"request_time_frac\",\n        \"D\": \"request_time_micro\",\n        \"i\": \"request_header\",\n        \"o\": \"response_header\",\n    }\n\n    LOG_FORMAT = '%a %t \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\"'\n    FORMAT_RE = re.compile(r\"%(\\{([A-Za-z0-9\\-_]+)\\}([ioe])|[atPrsbOD]|Tf?)\")\n    CLEANUP_RE = re.compile(r\"(%[^s])\")\n    _FORMAT_CACHE: Dict[str, Tuple[str, List[KeyMethod]]] = {}\n\n    def __init__(self, logger: logging.Logger, log_format: str = LOG_FORMAT) -> None:\n        \"\"\"Initialise the logger.\n\n        logger is a logger object to be used for logging.\n        log_format is a string with apache compatible log format description.\n\n        \"\"\"\n        super().__init__(logger, log_format=log_format)\n\n        _compiled_format = AccessLogger._FORMAT_CACHE.get(log_format)\n        if not _compiled_format:\n            _compiled_format = self.compile_format(log_format)\n            AccessLogger._FORMAT_CACHE[log_format] = _compiled_format\n\n        self._log_format, self._methods = _compiled_format\n\n    def compile_format(self, log_format: str) -> Tuple[str, List[KeyMethod]]:\n        \"\"\"Translate log_format into form usable by modulo formatting\n\n        All known atoms will be replaced with %s\n        Also methods for formatting of those atoms will be added to\n        _methods in appropriate order\n\n        For example we have log_format = \"%a %t\"\n        This format will be translated to \"%s %s\"\n        Also contents of _methods will be\n        [self._format_a, self._format_t]\n        These method will be called and results will be passed\n        to translated string format.\n\n        Each _format_* method receive 'args' which is list of arguments\n        given to self.log\n\n        Exceptions are _format_e, _format_i and _format_o methods which\n        also receive key name (by functools.partial)\n\n        \"\"\"\n        # list of (key, method) tuples, we don't use an OrderedDict as users\n        # can repeat the same key more than once\n        methods = list()\n\n        for atom in self.FORMAT_RE.findall(log_format):\n            if atom[1] == \"\":\n                format_key1 = self.LOG_FORMAT_MAP[atom[0]]\n                m = getattr(AccessLogger, \"_format_%s\" % atom[0])\n                key_method = KeyMethod(format_key1, m)\n            else:\n                format_key2 = (self.LOG_FORMAT_MAP[atom[2]], atom[1])\n                m = getattr(AccessLogger, \"_format_%s\" % atom[2])\n                key_method = KeyMethod(format_key2, functools.partial(m, atom[1]))\n\n            methods.append(key_method)\n\n        log_format = self.FORMAT_RE.sub(r\"%s\", log_format)\n        log_format = self.CLEANUP_RE.sub(r\"%\\1\", log_format)\n        return log_format, methods\n\n    @staticmethod\n    def _format_i(\n        key: str, request: BaseRequest, response: StreamResponse, time: float\n    ) -> str:\n        if request is None:\n            return \"(no headers)\"\n\n        # suboptimal, make istr(key) once\n        return request.headers.get(key, \"-\")\n\n    @staticmethod\n    def _format_o(\n        key: str, request: BaseRequest, response: StreamResponse, time: float\n    ) -> str:\n        # suboptimal, make istr(key) once\n        return response.headers.get(key, \"-\")\n\n    @staticmethod\n    def _format_a(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        if request is None:\n            return \"-\"\n        ip = request.remote\n        return ip if ip is not None else \"-\"\n\n    @staticmethod\n    def _format_t(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        tz = datetime.timezone(datetime.timedelta(seconds=-time_mod.timezone))\n        now = datetime.datetime.now(tz)\n        start_time = now - datetime.timedelta(seconds=time)\n        return start_time.strftime(\"[%d/%b/%Y:%H:%M:%S %z]\")\n\n    @staticmethod\n    def _format_P(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return \"<%s>\" % os.getpid()\n\n    @staticmethod\n    def _format_r(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        if request is None:\n            return \"-\"\n        return \"{} {} HTTP/{}.{}\".format(\n            request.method,\n            request.path_qs,\n            request.version.major,\n            request.version.minor,\n        )\n\n    @staticmethod\n    def _format_s(request: BaseRequest, response: StreamResponse, time: float) -> int:\n        return response.status\n\n    @staticmethod\n    def _format_b(request: BaseRequest, response: StreamResponse, time: float) -> int:\n        return response.body_length\n\n    @staticmethod\n    def _format_T(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return str(round(time))\n\n    @staticmethod\n    def _format_Tf(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return \"%06f\" % time\n\n    @staticmethod\n    def _format_D(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return str(round(time * 1000000))\n\n    def _format_line(\n        self, request: BaseRequest, response: StreamResponse, time: float\n    ) -> Iterable[Tuple[str, Callable[[BaseRequest, StreamResponse, float], str]]]:\n        return [(key, method(request, response, time)) for key, method in self._methods]\n\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\n        if not self.logger.isEnabledFor(logging.INFO):\n            # Avoid formatting the log line if it will not be emitted.\n            return\n        try:\n            fmt_info = self._format_line(request, response, time)\n\n            values = list()\n            extra = dict()\n            for key, value in fmt_info:\n                values.append(value)\n\n                if key.__class__ is str:\n                    extra[key] = value\n                else:\n                    k1, k2 = key  # type: ignore[misc]\n                    dct = extra.get(k1, {})  # type: ignore[var-annotated,has-type]\n                    dct[k2] = value  # type: ignore[index,has-type]\n                    extra[k1] = dct  # type: ignore[has-type,assignment]\n\n            self.logger.info(self._log_format % tuple(values), extra=extra)\n        except Exception:\n            self.logger.exception(\"Error in logging\")\n", "aiohttp/http_parser.py": "import abc\nimport asyncio\nimport re\nimport string\nfrom contextlib import suppress\nfrom enum import IntEnum\nfrom typing import (\n    Any,\n    ClassVar,\n    Final,\n    Generic,\n    List,\n    Literal,\n    NamedTuple,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, istr\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .base_protocol import BaseProtocol\nfrom .compression_utils import HAS_BROTLI, BrotliDecompressor, ZLibDecompressor\nfrom .helpers import (\n    _EXC_SENTINEL,\n    DEBUG,\n    NO_EXTENSIONS,\n    BaseTimerContext,\n    method_must_be_empty_body,\n    set_exception,\n    status_code_must_be_empty_body,\n)\nfrom .http_exceptions import (\n    BadHttpMessage,\n    BadStatusLine,\n    ContentEncodingError,\n    ContentLengthError,\n    InvalidHeader,\n    InvalidURLError,\n    LineTooLong,\n    TransferEncodingError,\n)\nfrom .http_writer import HttpVersion, HttpVersion10\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .typedefs import RawHeaders\n\n__all__ = (\n    \"HeadersParser\",\n    \"HttpParser\",\n    \"HttpRequestParser\",\n    \"HttpResponseParser\",\n    \"RawRequestMessage\",\n    \"RawResponseMessage\",\n)\n\n_SEP = Literal[b\"\\r\\n\", b\"\\n\"]\n\nASCIISET: Final[Set[str]] = set(string.printable)\n\n# See https://www.rfc-editor.org/rfc/rfc9110.html#name-overview\n# and https://www.rfc-editor.org/rfc/rfc9110.html#name-tokens\n#\n#     method = token\n#     tchar = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\" / \"+\" / \"-\" / \".\" /\n#             \"^\" / \"_\" / \"`\" / \"|\" / \"~\" / DIGIT / ALPHA\n#     token = 1*tchar\n_TCHAR_SPECIALS: Final[str] = re.escape(\"!#$%&'*+-.^_`|~\")\nTOKENRE: Final[Pattern[str]] = re.compile(f\"[0-9A-Za-z{_TCHAR_SPECIALS}]+\")\nVERSRE: Final[Pattern[str]] = re.compile(r\"HTTP/(\\d)\\.(\\d)\", re.ASCII)\nDIGITS: Final[Pattern[str]] = re.compile(r\"\\d+\", re.ASCII)\nHEXDIGITS: Final[Pattern[bytes]] = re.compile(rb\"[0-9a-fA-F]+\")\n\n\nclass RawRequestMessage(NamedTuple):\n    method: str\n    path: str\n    version: HttpVersion\n    headers: CIMultiDictProxy[str]\n    raw_headers: RawHeaders\n    should_close: bool\n    compression: Optional[str]\n    upgrade: bool\n    chunked: bool\n    url: URL\n\n\nclass RawResponseMessage(NamedTuple):\n    version: HttpVersion\n    code: int\n    reason: str\n    headers: CIMultiDictProxy[str]\n    raw_headers: RawHeaders\n    should_close: bool\n    compression: Optional[str]\n    upgrade: bool\n    chunked: bool\n\n\n_MsgT = TypeVar(\"_MsgT\", RawRequestMessage, RawResponseMessage)\n\n\nclass ParseState(IntEnum):\n    PARSE_NONE = 0\n    PARSE_LENGTH = 1\n    PARSE_CHUNKED = 2\n    PARSE_UNTIL_EOF = 3\n\n\nclass ChunkState(IntEnum):\n    PARSE_CHUNKED_SIZE = 0\n    PARSE_CHUNKED_CHUNK = 1\n    PARSE_CHUNKED_CHUNK_EOF = 2\n    PARSE_MAYBE_TRAILERS = 3\n    PARSE_TRAILERS = 4\n\n\nclass HeadersParser:\n    def __init__(\n        self, max_line_size: int = 8190, max_field_size: int = 8190, lax: bool = False\n    ) -> None:\n        self.max_line_size = max_line_size\n        self.max_field_size = max_field_size\n        self._lax = lax\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        # note: \"raw\" does not mean inclusion of OWS before/after the field value\n        raw_headers = []\n\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n\n        while line:\n            # Parse initial header name : value pair.\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n\n            if len(bname) == 0:\n                raise InvalidHeader(bname)\n\n            # https://www.rfc-editor.org/rfc/rfc9112.html#section-5.1-2\n            if {bname[0], bname[-1]} & {32, 9}:  # {\" \", \"\\t\"}\n                raise InvalidHeader(line)\n\n            bvalue = bvalue.lstrip(b\" \\t\")\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"backslashreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            if not TOKENRE.fullmatch(name):\n                raise InvalidHeader(bname)\n\n            header_length = len(bvalue)\n\n            # next line\n            lines_idx += 1\n            line = lines[lines_idx]\n\n            # consume continuation lines\n            continuation = self._lax and line and line[0] in (32, 9)  # (' ', '\\t')\n\n            # Deprecated: https://www.rfc-editor.org/rfc/rfc9112.html#name-obsolete-line-folding\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"backslashreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n\n                    # next line\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)  # (' ', '\\t')\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"backslashreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n\n            bvalue = bvalue.strip(b\" \\t\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n\n            # https://www.rfc-editor.org/rfc/rfc9110.html#section-5.5-5\n            if \"\\n\" in value or \"\\r\" in value or \"\\x00\" in value:\n                raise InvalidHeader(bvalue)\n\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\n\n\ndef _is_supported_upgrade(headers: CIMultiDictProxy[str]) -> bool:\n    \"\"\"Check if the upgrade header is supported.\"\"\"\n    return headers.get(hdrs.UPGRADE, \"\").lower() in {\"tcp\", \"websocket\"}\n\n\nclass HttpParser(abc.ABC, Generic[_MsgT]):\n    lax: ClassVar[bool] = False\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        loop: asyncio.AbstractEventLoop,\n        limit: int,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n        timer: Optional[BaseTimerContext] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        payload_exception: Optional[Type[BaseException]] = None,\n        response_with_body: bool = True,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n    ) -> None:\n        self.protocol = protocol\n        self.loop = loop\n        self.max_line_size = max_line_size\n        self.max_field_size = max_field_size\n        self.timer = timer\n        self.code = code\n        self.method = method\n        self.payload_exception = payload_exception\n        self.response_with_body = response_with_body\n        self.read_until_eof = read_until_eof\n\n        self._lines: List[bytes] = []\n        self._tail = b\"\"\n        self._upgraded = False\n        self._payload = None\n        self._payload_parser: Optional[HttpPayloadParser] = None\n        self._auto_decompress = auto_decompress\n        self._limit = limit\n        self._headers_parser = HeadersParser(max_line_size, max_field_size, self.lax)\n\n    @abc.abstractmethod\n    def parse_message(self, lines: List[bytes]) -> _MsgT:\n        pass\n\n    def feed_eof(self) -> Optional[_MsgT]:\n        if self._payload_parser is not None:\n            self._payload_parser.feed_eof()\n            self._payload_parser = None\n        else:\n            # try to extract partial message\n            if self._tail:\n                self._lines.append(self._tail)\n\n            if self._lines:\n                if self._lines[-1] != \"\\r\\n\":\n                    self._lines.append(b\"\")\n                with suppress(Exception):\n                    return self.parse_message(self._lines)\n        return None\n\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: _SEP = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + len(SEP)\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    line = data[start_pos:pos]\n                    if SEP == b\"\\n\":  # For lax response parsing\n                        line = line.rstrip(b\"\\r\")\n                    self._lines.append(line)\n                    start_pos = pos + len(SEP)\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            # msg.headers is already stripped of leading/trailing wsp\n                            if not DIGITS.fullmatch(length_hdr):\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade and _is_supported_upgrade(\n                            msg.headers\n                        )\n\n                        method = getattr(msg, \"method\", self.method)\n                        # code is only present on responses\n                        code = getattr(msg, \"code\", 0)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        empty_body = status_code_must_be_empty_body(code) or bool(\n                            method and method_must_be_empty_body(method)\n                        )\n                        if not empty_body and (\n                            ((length is not None and length > 0) or msg.chunked)\n                            and not self._upgraded\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                        elif not empty_body and length is None and self.read_until_eof:\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        else:\n                            payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:], SEP)\n                except BaseException as underlying_exc:\n                    reraised_exc = underlying_exc\n                    if self.payload_exception is not None:\n                        reraised_exc = self.payload_exception(str(underlying_exc))\n\n                    set_exception(\n                        self._payload_parser.payload,\n                        reraised_exc,\n                        underlying_exc,\n                    )\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\n        \"CIMultiDictProxy[str]\", RawHeaders, Optional[bool], Optional[str], bool, bool\n    ]:\n        \"\"\"Parses RFC 5322 headers from a stream.\n\n        Line continuations are supported. Returns list of header name\n        and value pairs. Header name is in upper case.\n        \"\"\"\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        # https://www.rfc-editor.org/rfc/rfc9110.html#section-5.5-6\n        # https://www.rfc-editor.org/rfc/rfc9110.html#name-collected-abnf\n        singletons = (\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_LOCATION,\n            hdrs.CONTENT_RANGE,\n            hdrs.CONTENT_TYPE,\n            hdrs.ETAG,\n            hdrs.HOST,\n            hdrs.MAX_FORWARDS,\n            hdrs.SERVER,\n            hdrs.TRANSFER_ENCODING,\n            hdrs.USER_AGENT,\n        )\n        bad_hdr = next((h for h in singletons if len(headers.getall(h, ())) > 1), None)\n        if bad_hdr is not None:\n            raise BadHttpMessage(f\"Duplicate '{bad_hdr}' header found.\")\n\n        # keep-alive\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == \"close\":\n                close_conn = True\n            elif v == \"keep-alive\":\n                close_conn = False\n            # https://www.rfc-editor.org/rfc/rfc9110.html#name-101-switching-protocols\n            elif v == \"upgrade\" and headers.get(hdrs.UPGRADE):\n                upgrade = True\n\n        # encoding\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in (\"gzip\", \"deflate\", \"br\"):\n                encoding = enc\n\n        # chunking\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te is not None:\n            if \"chunked\" == te.lower():\n                chunked = True\n            else:\n                raise BadHttpMessage(\"Request has invalid `Transfer-Encoding`\")\n\n            if hdrs.CONTENT_LENGTH in headers:\n                raise BadHttpMessage(\n                    \"Transfer-Encoding can't be present with Content-Length\",\n                )\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)\n\n    def set_upgraded(self, val: bool) -> None:\n        \"\"\"Set connection upgraded (to websocket) mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._upgraded = val\n\n\nclass HttpRequestParser(HttpParser[RawRequestMessage]):\n    \"\"\"Read request status line.\n\n    Exception .http_exceptions.BadStatusLine\n    could be raised in case of any errors in status line.\n    Returns RawRequestMessage.\n    \"\"\"\n\n    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        # request line\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(\" \", maxsplit=2)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n\n        # method\n        if not TOKENRE.fullmatch(method):\n            raise BadStatusLine(method)\n\n        # version\n        match = VERSRE.fullmatch(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        if method == \"CONNECT\":\n            # authority-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.3\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            # origin-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.1\n            path_part, _hash_separator, url_fragment = path.partition(\"#\")\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n\n            # NOTE: `yarl.URL.build()` is used to mimic what the Cython-based\n            # NOTE: parser does, otherwise it results into the same\n            # NOTE: HTTP Request-Line input producing different\n            # NOTE: `yarl.URL()` objects\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        elif path == \"*\" and method == \"OPTIONS\":\n            # asterisk-form,\n            url = URL(path, encoded=True)\n        else:\n            # absolute-form for proxy maybe,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.2\n            url = URL(path, encoded=True)\n            if url.scheme == \"\":\n                # not absolute-form\n                raise InvalidURLError(\n                    path.encode(errors=\"surrogateescape\").decode(\"latin1\")\n                )\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:  # then the headers weren't set in the request\n            if version_o <= HttpVersion10:  # HTTP 1.0 must asks to not close\n                close = True\n            else:  # HTTP 1.1 must ask to close.\n                close = False\n\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n\n\nclass HttpResponseParser(HttpParser[RawResponseMessage]):\n    \"\"\"Read response status line and headers.\n\n    BadStatusLine could be raised in case of any errors in status line.\n    Returns RawResponseMessage.\n    \"\"\"\n\n    # Lax mode should only be enabled on response parser.\n    lax = not DEBUG\n\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: Optional[_SEP] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Tuple[List[Tuple[RawResponseMessage, StreamReader]], bool, bytes]:\n        if SEP is None:\n            SEP = b\"\\r\\n\" if DEBUG else b\"\\n\"\n        return super().feed_data(data, SEP, *args, **kwargs)\n\n    def parse_message(self, lines: List[bytes]) -> RawResponseMessage:\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            version, status = line.split(maxsplit=1)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        try:\n            status, reason = status.split(maxsplit=1)\n        except ValueError:\n            status = status.strip()\n            reason = \"\"\n\n        if len(reason) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(reason))\n            )\n\n        # version\n        match = VERSRE.fullmatch(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        # The status code is a three-digit ASCII number, no padding\n        if len(status) != 3 or not DIGITS.fullmatch(status):\n            raise BadStatusLine(line)\n        status_i = int(status)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:\n            if version_o <= HttpVersion10:\n                close = True\n            # https://www.rfc-editor.org/rfc/rfc9112.html#name-message-body-length\n            elif 100 <= status_i < 200 or status_i in {204, 304}:\n                close = False\n            elif hdrs.CONTENT_LENGTH in headers or hdrs.TRANSFER_ENCODING in headers:\n                close = False\n            else:\n                # https://www.rfc-editor.org/rfc/rfc9112.html#section-6.3-2.8\n                close = True\n\n        return RawResponseMessage(\n            version_o,\n            status_i,\n            reason.strip(),\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        )\n\n\nclass HttpPayloadParser:\n    def __init__(\n        self,\n        payload: StreamReader,\n        length: Optional[int] = None,\n        chunked: bool = False,\n        compression: Optional[str] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        response_with_body: bool = True,\n        auto_decompress: bool = True,\n        lax: bool = False,\n    ) -> None:\n        self._length = 0\n        self._type = ParseState.PARSE_UNTIL_EOF\n        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n        self._chunk_size = 0\n        self._chunk_tail = b\"\"\n        self._auto_decompress = auto_decompress\n        self._lax = lax\n        self.done = False\n\n        # payload decompression wrapper\n        if response_with_body and compression and self._auto_decompress:\n            real_payload: Union[StreamReader, DeflateBuffer] = DeflateBuffer(\n                payload, compression\n            )\n        else:\n            real_payload = payload\n\n        # payload parser\n        if not response_with_body:\n            # don't parse payload if it's not expected to be received\n            self._type = ParseState.PARSE_NONE\n            real_payload.feed_eof()\n            self.done = True\n        elif chunked:\n            self._type = ParseState.PARSE_CHUNKED\n        elif length is not None:\n            self._type = ParseState.PARSE_LENGTH\n            self._length = length\n            if self._length == 0:\n                real_payload.feed_eof()\n                self.done = True\n\n        self.payload = real_payload\n\n    def feed_eof(self) -> None:\n        if self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_eof()\n        elif self._type == ParseState.PARSE_LENGTH:\n            raise ContentLengthError(\n                \"Not enough data for satisfy content length header.\"\n            )\n        elif self._type == ParseState.PARSE_CHUNKED:\n            raise TransferEncodingError(\n                \"Not enough data for satisfy transfer length header.\"\n            )\n\n    def feed_data(\n        self, chunk: bytes, SEP: _SEP = b\"\\r\\n\", CHUNK_EXT: bytes = b\";\"\n    ) -> Tuple[bool, bytes]:\n        # Read specified amount of bytes\n        if self._type == ParseState.PARSE_LENGTH:\n            required = self._length\n            self._length = max(required - len(chunk), 0)\n            self.payload.feed_data(chunk[:required])\n            if self._length == 0:\n                self.payload.feed_eof()\n                return True, chunk[required:]\n\n        # Chunked transfer encoding parser\n        elif self._type == ParseState.PARSE_CHUNKED:\n            if self._chunk_tail:\n                chunk = self._chunk_tail + chunk\n                self._chunk_tail = b\"\"\n\n            while chunk:\n                # read next chunk size\n                if self._chunk == ChunkState.PARSE_CHUNKED_SIZE:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        i = chunk.find(CHUNK_EXT, 0, pos)\n                        if i >= 0:\n                            size_b = chunk[:i]  # strip chunk-extensions\n                        else:\n                            size_b = chunk[:pos]\n\n                        if self._lax:  # Allow whitespace in lax mode.\n                            size_b = size_b.strip()\n\n                        if not re.fullmatch(HEXDIGITS, size_b):\n                            exc = TransferEncodingError(\n                                chunk[:pos].decode(\"ascii\", \"surrogateescape\")\n                            )\n                            set_exception(self.payload, exc)\n                            raise exc\n                        size = int(bytes(size_b), 16)\n\n                        chunk = chunk[pos + len(SEP) :]\n                        if size == 0:  # eof marker\n                            self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                            if self._lax and chunk.startswith(b\"\\r\"):\n                                chunk = chunk[1:]\n                        else:\n                            self._chunk = ChunkState.PARSE_CHUNKED_CHUNK\n                            self._chunk_size = size\n                            self.payload.begin_http_chunk_receiving()\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                # read chunk and feed buffer\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK:\n                    required = self._chunk_size\n                    self._chunk_size = max(required - len(chunk), 0)\n                    self.payload.feed_data(chunk[:required])\n\n                    if self._chunk_size:\n                        return False, b\"\"\n                    chunk = chunk[required:]\n                    if self._lax and chunk.startswith(b\"\\r\"):\n                        chunk = chunk[1:]\n                    self._chunk = ChunkState.PARSE_CHUNKED_CHUNK_EOF\n                    self.payload.end_http_chunk_receiving()\n\n                # toss the CRLF at the end of the chunk\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK_EOF:\n                    if chunk[: len(SEP)] == SEP:\n                        chunk = chunk[len(SEP) :]\n                        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                # if stream does not contain trailer, after 0\\r\\n\n                # we should get another \\r\\n otherwise\n                # trailers needs to be skipped until \\r\\n\\r\\n\n                if self._chunk == ChunkState.PARSE_MAYBE_TRAILERS:\n                    head = chunk[: len(SEP)]\n                    if head == SEP:\n                        # end of stream\n                        self.payload.feed_eof()\n                        return True, chunk[len(SEP) :]\n                    # Both CR and LF, or only LF may not be received yet. It is\n                    # expected that CRLF or LF will be shown at the very first\n                    # byte next time, otherwise trailers should come. The last\n                    # CRLF which marks the end of response might not be\n                    # contained in the same TCP segment which delivered the\n                    # size indicator.\n                    if not head:\n                        return False, b\"\"\n                    if head == SEP[:1]:\n                        self._chunk_tail = head\n                        return False, b\"\"\n                    self._chunk = ChunkState.PARSE_TRAILERS\n\n                # read and discard trailer up to the CRLF terminator\n                if self._chunk == ChunkState.PARSE_TRAILERS:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        chunk = chunk[pos + len(SEP) :]\n                        self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n        # Read all bytes until eof\n        elif self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_data(chunk)\n\n        return False, b\"\"\n\n\nclass DeflateBuffer:\n    \"\"\"DeflateStream decompress stream and feed data into specified stream.\"\"\"\n\n    def __init__(self, out: StreamReader, encoding: Optional[str]) -> None:\n        self.out = out\n        self.size = 0\n        self.encoding = encoding\n        self._started_decoding = False\n\n        self.decompressor: Union[BrotliDecompressor, ZLibDecompressor]\n        if encoding == \"br\":\n            if not HAS_BROTLI:  # pragma: no cover\n                raise ContentEncodingError(\n                    \"Can not decode content-encoding: brotli (br). \"\n                    \"Please install `Brotli`\"\n                )\n            self.decompressor = BrotliDecompressor()\n        else:\n            self.decompressor = ZLibDecompressor(encoding=encoding)\n\n    def set_exception(\n        self,\n        exc: BaseException,\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        set_exception(self.out, exc, exc_cause)\n\n    def feed_data(self, chunk: bytes) -> None:\n        if not chunk:\n            return\n\n        self.size += len(chunk)\n\n        # RFC1950\n        # bits 0..3 = CM = 0b1000 = 8 = \"deflate\"\n        # bits 4..7 = CINFO = 1..7 = windows size.\n        if (\n            not self._started_decoding\n            and self.encoding == \"deflate\"\n            and chunk[0] & 0xF != 8\n        ):\n            # Change the decoder to decompress incorrectly compressed data\n            # Actually we should issue a warning about non-RFC-compliant data.\n            self.decompressor = ZLibDecompressor(\n                encoding=self.encoding, suppress_deflate_header=True\n            )\n\n        try:\n            chunk = self.decompressor.decompress_sync(chunk)\n        except Exception:\n            raise ContentEncodingError(\n                \"Can not decode content-encoding: %s\" % self.encoding\n            )\n\n        self._started_decoding = True\n\n        if chunk:\n            self.out.feed_data(chunk)\n\n    def feed_eof(self) -> None:\n        chunk = self.decompressor.flush()\n\n        if chunk or self.size > 0:\n            self.out.feed_data(chunk)\n            # decompressor is not brotli unless encoding is \"br\"\n            if self.encoding == \"deflate\" and not self.decompressor.eof:  # type: ignore[union-attr]\n                raise ContentEncodingError(\"deflate\")\n\n        self.out.feed_eof()\n\n    def begin_http_chunk_receiving(self) -> None:\n        self.out.begin_http_chunk_receiving()\n\n    def end_http_chunk_receiving(self) -> None:\n        self.out.end_http_chunk_receiving()\n\n\nHttpRequestParserPy = HttpRequestParser\nHttpResponseParserPy = HttpResponseParser\nRawRequestMessagePy = RawRequestMessage\nRawResponseMessagePy = RawResponseMessage\n\ntry:\n    if not NO_EXTENSIONS:\n        from ._http_parser import (  # type: ignore[import-not-found,no-redef]\n            HttpRequestParser,\n            HttpResponseParser,\n            RawRequestMessage,\n            RawResponseMessage,\n        )\n\n        HttpRequestParserC = HttpRequestParser\n        HttpResponseParserC = HttpResponseParser\n        RawRequestMessageC = RawRequestMessage\n        RawResponseMessageC = RawResponseMessage\nexcept ImportError:  # pragma: no cover\n    pass\n", "aiohttp/web_exceptions.py": "import warnings\nfrom http import HTTPStatus\nfrom typing import Any, Iterable, Optional, Set, Tuple\n\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .helpers import CookieMixin\nfrom .typedefs import LooseHeaders, StrOrURL\n\n__all__ = (\n    \"HTTPException\",\n    \"HTTPError\",\n    \"HTTPRedirection\",\n    \"HTTPSuccessful\",\n    \"HTTPOk\",\n    \"HTTPCreated\",\n    \"HTTPAccepted\",\n    \"HTTPNonAuthoritativeInformation\",\n    \"HTTPNoContent\",\n    \"HTTPResetContent\",\n    \"HTTPPartialContent\",\n    \"HTTPMove\",\n    \"HTTPMultipleChoices\",\n    \"HTTPMovedPermanently\",\n    \"HTTPFound\",\n    \"HTTPSeeOther\",\n    \"HTTPNotModified\",\n    \"HTTPUseProxy\",\n    \"HTTPTemporaryRedirect\",\n    \"HTTPPermanentRedirect\",\n    \"HTTPClientError\",\n    \"HTTPBadRequest\",\n    \"HTTPUnauthorized\",\n    \"HTTPPaymentRequired\",\n    \"HTTPForbidden\",\n    \"HTTPNotFound\",\n    \"HTTPMethodNotAllowed\",\n    \"HTTPNotAcceptable\",\n    \"HTTPProxyAuthenticationRequired\",\n    \"HTTPRequestTimeout\",\n    \"HTTPConflict\",\n    \"HTTPGone\",\n    \"HTTPLengthRequired\",\n    \"HTTPPreconditionFailed\",\n    \"HTTPRequestEntityTooLarge\",\n    \"HTTPRequestURITooLong\",\n    \"HTTPUnsupportedMediaType\",\n    \"HTTPRequestRangeNotSatisfiable\",\n    \"HTTPExpectationFailed\",\n    \"HTTPMisdirectedRequest\",\n    \"HTTPUnprocessableEntity\",\n    \"HTTPFailedDependency\",\n    \"HTTPUpgradeRequired\",\n    \"HTTPPreconditionRequired\",\n    \"HTTPTooManyRequests\",\n    \"HTTPRequestHeaderFieldsTooLarge\",\n    \"HTTPUnavailableForLegalReasons\",\n    \"HTTPServerError\",\n    \"HTTPInternalServerError\",\n    \"HTTPNotImplemented\",\n    \"HTTPBadGateway\",\n    \"HTTPServiceUnavailable\",\n    \"HTTPGatewayTimeout\",\n    \"HTTPVersionNotSupported\",\n    \"HTTPVariantAlsoNegotiates\",\n    \"HTTPInsufficientStorage\",\n    \"HTTPNotExtended\",\n    \"HTTPNetworkAuthenticationRequired\",\n)\n\n\nclass NotAppKeyWarning(UserWarning):\n    \"\"\"Warning when not using AppKey in Application.\"\"\"\n\n\n############################################################\n# HTTP Exceptions\n############################################################\n\n\nclass HTTPException(CookieMixin, Exception):\n    # You should set in subclasses:\n    # status = 200\n\n    status_code = -1\n    empty_body = False\n    default_reason = \"\"  # Initialized at the end of the module\n\n    def __init__(\n        self,\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        super().__init__()\n        if reason is None:\n            reason = self.default_reason\n\n        if text is None:\n            if not self.empty_body:\n                text = f\"{self.status_code}: {reason}\"\n        else:\n            if self.empty_body:\n                warnings.warn(\n                    \"text argument is deprecated for HTTP status {} \"\n                    \"since 4.0 and scheduled for removal in 5.0 (#3462),\"\n                    \"the response should be provided without a body\".format(\n                        self.status_code\n                    ),\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n        if headers is not None:\n            real_headers = CIMultiDict(headers)\n        else:\n            real_headers = CIMultiDict()\n\n        if content_type is not None:\n            if not text:\n                warnings.warn(\n                    \"content_type without text is deprecated \"\n                    \"since 4.0 and scheduled for removal in 5.0 \"\n                    \"(#3462)\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            real_headers[hdrs.CONTENT_TYPE] = content_type\n        elif hdrs.CONTENT_TYPE not in real_headers and text:\n            real_headers[hdrs.CONTENT_TYPE] = \"text/plain\"\n\n        self._reason = reason\n        self._text = text\n        self._headers = real_headers\n        self.args = ()\n\n    def __bool__(self) -> bool:\n        return True\n\n    @property\n    def status(self) -> int:\n        return self.status_code\n\n    @property\n    def reason(self) -> str:\n        return self._reason\n\n    @property\n    def text(self) -> Optional[str]:\n        return self._text\n\n    @property\n    def headers(self) -> \"CIMultiDict[str]\":\n        return self._headers\n\n    def __str__(self) -> str:\n        return self.reason\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self.reason}>\"\n\n    __reduce__ = object.__reduce__\n\n    def __getnewargs__(self) -> Tuple[Any, ...]:\n        return self.args\n\n\nclass HTTPError(HTTPException):\n    \"\"\"Base class for exceptions with status codes in the 400s and 500s.\"\"\"\n\n\nclass HTTPRedirection(HTTPException):\n    \"\"\"Base class for exceptions with status codes in the 300s.\"\"\"\n\n\nclass HTTPSuccessful(HTTPException):\n    \"\"\"Base class for exceptions with status codes in the 200s.\"\"\"\n\n\nclass HTTPOk(HTTPSuccessful):\n    status_code = 200\n\n\nclass HTTPCreated(HTTPSuccessful):\n    status_code = 201\n\n\nclass HTTPAccepted(HTTPSuccessful):\n    status_code = 202\n\n\nclass HTTPNonAuthoritativeInformation(HTTPSuccessful):\n    status_code = 203\n\n\nclass HTTPNoContent(HTTPSuccessful):\n    status_code = 204\n    empty_body = True\n\n\nclass HTTPResetContent(HTTPSuccessful):\n    status_code = 205\n    empty_body = True\n\n\nclass HTTPPartialContent(HTTPSuccessful):\n    status_code = 206\n\n\n############################################################\n# 3xx redirection\n############################################################\n\n\nclass HTTPMove(HTTPRedirection):\n    def __init__(\n        self,\n        location: StrOrURL,\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        if not location:\n            raise ValueError(\"HTTP redirects need a location to redirect to.\")\n        super().__init__(\n            headers=headers, reason=reason, text=text, content_type=content_type\n        )\n        self._location = URL(location)\n        self.headers[\"Location\"] = str(self.location)\n\n    @property\n    def location(self) -> URL:\n        return self._location\n\n\nclass HTTPMultipleChoices(HTTPMove):\n    status_code = 300\n\n\nclass HTTPMovedPermanently(HTTPMove):\n    status_code = 301\n\n\nclass HTTPFound(HTTPMove):\n    status_code = 302\n\n\n# This one is safe after a POST (the redirected location will be\n# retrieved with GET):\nclass HTTPSeeOther(HTTPMove):\n    status_code = 303\n\n\nclass HTTPNotModified(HTTPRedirection):\n    # FIXME: this should include a date or etag header\n    status_code = 304\n    empty_body = True\n\n\nclass HTTPUseProxy(HTTPMove):\n    # Not a move, but looks a little like one\n    status_code = 305\n\n\nclass HTTPTemporaryRedirect(HTTPMove):\n    status_code = 307\n\n\nclass HTTPPermanentRedirect(HTTPMove):\n    status_code = 308\n\n\n############################################################\n# 4xx client error\n############################################################\n\n\nclass HTTPClientError(HTTPError):\n    pass\n\n\nclass HTTPBadRequest(HTTPClientError):\n    status_code = 400\n\n\nclass HTTPUnauthorized(HTTPClientError):\n    status_code = 401\n\n\nclass HTTPPaymentRequired(HTTPClientError):\n    status_code = 402\n\n\nclass HTTPForbidden(HTTPClientError):\n    status_code = 403\n\n\nclass HTTPNotFound(HTTPClientError):\n    status_code = 404\n\n\nclass HTTPMethodNotAllowed(HTTPClientError):\n    status_code = 405\n\n    def __init__(\n        self,\n        method: str,\n        allowed_methods: Iterable[str],\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        allow = \",\".join(sorted(allowed_methods))\n        super().__init__(\n            headers=headers, reason=reason, text=text, content_type=content_type\n        )\n        self.headers[\"Allow\"] = allow\n        self._allowed: Set[str] = set(allowed_methods)\n        self._method = method\n\n    @property\n    def allowed_methods(self) -> Set[str]:\n        return self._allowed\n\n    @property\n    def method(self) -> str:\n        return self._method\n\n\nclass HTTPNotAcceptable(HTTPClientError):\n    status_code = 406\n\n\nclass HTTPProxyAuthenticationRequired(HTTPClientError):\n    status_code = 407\n\n\nclass HTTPRequestTimeout(HTTPClientError):\n    status_code = 408\n\n\nclass HTTPConflict(HTTPClientError):\n    status_code = 409\n\n\nclass HTTPGone(HTTPClientError):\n    status_code = 410\n\n\nclass HTTPLengthRequired(HTTPClientError):\n    status_code = 411\n\n\nclass HTTPPreconditionFailed(HTTPClientError):\n    status_code = 412\n\n\nclass HTTPRequestEntityTooLarge(HTTPClientError):\n    status_code = 413\n\n    def __init__(self, max_size: int, actual_size: int, **kwargs: Any) -> None:\n        kwargs.setdefault(\n            \"text\",\n            \"Maximum request body size {} exceeded, \"\n            \"actual body size {}\".format(max_size, actual_size),\n        )\n        super().__init__(**kwargs)\n\n\nclass HTTPRequestURITooLong(HTTPClientError):\n    status_code = 414\n\n\nclass HTTPUnsupportedMediaType(HTTPClientError):\n    status_code = 415\n\n\nclass HTTPRequestRangeNotSatisfiable(HTTPClientError):\n    status_code = 416\n\n\nclass HTTPExpectationFailed(HTTPClientError):\n    status_code = 417\n\n\nclass HTTPMisdirectedRequest(HTTPClientError):\n    status_code = 421\n\n\nclass HTTPUnprocessableEntity(HTTPClientError):\n    status_code = 422\n\n\nclass HTTPFailedDependency(HTTPClientError):\n    status_code = 424\n\n\nclass HTTPUpgradeRequired(HTTPClientError):\n    status_code = 426\n\n\nclass HTTPPreconditionRequired(HTTPClientError):\n    status_code = 428\n\n\nclass HTTPTooManyRequests(HTTPClientError):\n    status_code = 429\n\n\nclass HTTPRequestHeaderFieldsTooLarge(HTTPClientError):\n    status_code = 431\n\n\nclass HTTPUnavailableForLegalReasons(HTTPClientError):\n    status_code = 451\n\n    def __init__(\n        self,\n        link: Optional[StrOrURL],\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        super().__init__(\n            headers=headers, reason=reason, text=text, content_type=content_type\n        )\n        self._link = None\n        if link:\n            self._link = URL(link)\n            self.headers[\"Link\"] = f'<{str(self._link)}>; rel=\"blocked-by\"'\n\n    @property\n    def link(self) -> Optional[URL]:\n        return self._link\n\n\n############################################################\n# 5xx Server Error\n############################################################\n#  Response status codes beginning with the digit \"5\" indicate cases in\n#  which the server is aware that it has erred or is incapable of\n#  performing the request. Except when responding to a HEAD request, the\n#  server SHOULD include an entity containing an explanation of the error\n#  situation, and whether it is a temporary or permanent condition. User\n#  agents SHOULD display any included entity to the user. These response\n#  codes are applicable to any request method.\n\n\nclass HTTPServerError(HTTPError):\n    pass\n\n\nclass HTTPInternalServerError(HTTPServerError):\n    status_code = 500\n\n\nclass HTTPNotImplemented(HTTPServerError):\n    status_code = 501\n\n\nclass HTTPBadGateway(HTTPServerError):\n    status_code = 502\n\n\nclass HTTPServiceUnavailable(HTTPServerError):\n    status_code = 503\n\n\nclass HTTPGatewayTimeout(HTTPServerError):\n    status_code = 504\n\n\nclass HTTPVersionNotSupported(HTTPServerError):\n    status_code = 505\n\n\nclass HTTPVariantAlsoNegotiates(HTTPServerError):\n    status_code = 506\n\n\nclass HTTPInsufficientStorage(HTTPServerError):\n    status_code = 507\n\n\nclass HTTPNotExtended(HTTPServerError):\n    status_code = 510\n\n\nclass HTTPNetworkAuthenticationRequired(HTTPServerError):\n    status_code = 511\n\n\ndef _initialize_default_reason() -> None:\n    for obj in globals().values():\n        if isinstance(obj, type) and issubclass(obj, HTTPException):\n            if obj.status_code >= 0:\n                try:\n                    status = HTTPStatus(obj.status_code)\n                    obj.default_reason = status.phrase\n                except ValueError:\n                    pass\n\n\n_initialize_default_reason()\ndel _initialize_default_reason\n", "aiohttp/web_request.py": "import asyncio\nimport dataclasses\nimport datetime\nimport io\nimport re\nimport socket\nimport string\nimport tempfile\nimport types\nfrom http.cookies import SimpleCookie\nfrom types import MappingProxyType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Final,\n    Iterator,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import parse_qsl\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import (\n    _SENTINEL,\n    ETAG_ANY,\n    LIST_QUOTED_ETAG_RE,\n    ChainMapProxy,\n    ETag,\n    HeadersMixin,\n    is_expected_content_type,\n    parse_http_date,\n    reify,\n    sentinel,\n    set_exception,\n    set_result,\n)\nfrom .http_parser import RawRequestMessage\nfrom .http_writer import HttpVersion\nfrom .multipart import BodyPartReader, MultipartReader\nfrom .streams import EmptyStreamReader, StreamReader\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    JSONDecoder,\n    LooseHeaders,\n    RawHeaders,\n    StrOrURL,\n)\nfrom .web_exceptions import (\n    HTTPBadRequest,\n    HTTPRequestEntityTooLarge,\n    HTTPUnsupportedMediaType,\n)\nfrom .web_response import StreamResponse\n\n__all__ = (\"BaseRequest\", \"FileField\", \"Request\")\n\n\nif TYPE_CHECKING:\n    from .web_app import Application\n    from .web_protocol import RequestHandler\n    from .web_urldispatcher import UrlMappingMatchInfo\n\n\n@dataclasses.dataclass(frozen=True)\nclass FileField:\n    name: str\n    filename: str\n    file: io.BufferedReader\n    content_type: str\n    headers: \"CIMultiDictProxy[str]\"\n\n\n_TCHAR: Final[str] = string.digits + string.ascii_letters + r\"!#$%&'*+.^_`|~-\"\n# '-' at the end to prevent interpretation as range in a char class\n\n_TOKEN: Final[str] = rf\"[{_TCHAR}]+\"\n\n_QDTEXT: Final[str] = r\"[{}]\".format(\n    r\"\".join(chr(c) for c in (0x09, 0x20, 0x21) + tuple(range(0x23, 0x7F)))\n)\n# qdtext includes 0x5C to escape 0x5D ('\\]')\n# qdtext excludes obs-text (because obsoleted, and encoding not specified)\n\n_QUOTED_PAIR: Final[str] = r\"\\\\[\\t !-~]\"\n\n_QUOTED_STRING: Final[str] = r'\"(?:{quoted_pair}|{qdtext})*\"'.format(\n    qdtext=_QDTEXT, quoted_pair=_QUOTED_PAIR\n)\n\n_FORWARDED_PAIR: Final[str] = (\n    r\"({token})=({token}|{quoted_string})(:\\d{{1,4}})?\".format(\n        token=_TOKEN, quoted_string=_QUOTED_STRING\n    )\n)\n\n_QUOTED_PAIR_REPLACE_RE: Final[Pattern[str]] = re.compile(r\"\\\\([\\t !-~])\")\n# same pattern as _QUOTED_PAIR but contains a capture group\n\n_FORWARDED_PAIR_RE: Final[Pattern[str]] = re.compile(_FORWARDED_PAIR)\n\n############################################################\n# HTTP Request\n############################################################\n\n\nclass BaseRequest(MutableMapping[str, Any], HeadersMixin):\n    POST_METHODS = {\n        hdrs.METH_PATCH,\n        hdrs.METH_POST,\n        hdrs.METH_PUT,\n        hdrs.METH_TRACE,\n        hdrs.METH_DELETE,\n    }\n\n    __slots__ = (\n        \"_message\",\n        \"_protocol\",\n        \"_payload_writer\",\n        \"_payload\",\n        \"_headers\",\n        \"_method\",\n        \"_version\",\n        \"_rel_url\",\n        \"_post\",\n        \"_read_bytes\",\n        \"_state\",\n        \"_cache\",\n        \"_task\",\n        \"_client_max_size\",\n        \"_loop\",\n        \"_transport_sslcontext\",\n        \"_transport_peername\",\n        \"_disconnection_waiters\",\n        \"__weakref__\",\n    )\n\n    def __init__(\n        self,\n        message: RawRequestMessage,\n        payload: StreamReader,\n        protocol: \"RequestHandler\",\n        payload_writer: AbstractStreamWriter,\n        task: \"asyncio.Task[None]\",\n        loop: asyncio.AbstractEventLoop,\n        *,\n        client_max_size: int = 1024**2,\n        state: Optional[Dict[str, Any]] = None,\n        scheme: Optional[str] = None,\n        host: Optional[str] = None,\n        remote: Optional[str] = None,\n    ) -> None:\n        super().__init__()\n        if state is None:\n            state = {}\n        self._message = message\n        self._protocol = protocol\n        self._payload_writer = payload_writer\n\n        self._payload = payload\n        self._headers = message.headers\n        self._method = message.method\n        self._version = message.version\n        self._cache: Dict[str, Any] = {}\n        url = message.url\n        if url.is_absolute():\n            # absolute URL is given,\n            # override auto-calculating url, host, and scheme\n            # all other properties should be good\n            self._cache[\"url\"] = url\n            self._cache[\"host\"] = url.host\n            self._cache[\"scheme\"] = url.scheme\n            self._rel_url = url.relative()\n        else:\n            self._rel_url = message.url\n        self._post: Optional[MultiDictProxy[Union[str, bytes, FileField]]] = None\n        self._read_bytes: Optional[bytes] = None\n\n        self._state = state\n        self._task = task\n        self._client_max_size = client_max_size\n        self._loop = loop\n        self._disconnection_waiters: Set[asyncio.Future[None]] = set()\n\n        transport = self._protocol.transport\n        assert transport is not None\n        self._transport_sslcontext = transport.get_extra_info(\"sslcontext\")\n        self._transport_peername = transport.get_extra_info(\"peername\")\n\n        if scheme is not None:\n            self._cache[\"scheme\"] = scheme\n        if host is not None:\n            self._cache[\"host\"] = host\n        if remote is not None:\n            self._cache[\"remote\"] = remote\n\n    def clone(\n        self,\n        *,\n        method: Union[str, _SENTINEL] = sentinel,\n        rel_url: Union[StrOrURL, _SENTINEL] = sentinel,\n        headers: Union[LooseHeaders, _SENTINEL] = sentinel,\n        scheme: Union[str, _SENTINEL] = sentinel,\n        host: Union[str, _SENTINEL] = sentinel,\n        remote: Union[str, _SENTINEL] = sentinel,\n        client_max_size: Union[int, _SENTINEL] = sentinel,\n    ) -> \"BaseRequest\":\n        \"\"\"Clone itself with replacement some attributes.\n\n        Creates and returns a new instance of Request object. If no parameters\n        are given, an exact copy is returned. If a parameter is not passed, it\n        will reuse the one from the current request object.\n        \"\"\"\n        if self._read_bytes:\n            raise RuntimeError(\"Cannot clone request \" \"after reading its content\")\n\n        dct: Dict[str, Any] = {}\n        if method is not sentinel:\n            dct[\"method\"] = method\n        if rel_url is not sentinel:\n            new_url: URL = URL(rel_url)\n            dct[\"url\"] = new_url\n            dct[\"path\"] = str(new_url)\n        if headers is not sentinel:\n            # a copy semantic\n            new_headers = CIMultiDictProxy(CIMultiDict(headers))\n            dct[\"headers\"] = new_headers\n            dct[\"raw_headers\"] = tuple(\n                (k.encode(\"utf-8\"), v.encode(\"utf-8\")) for k, v in new_headers.items()\n            )\n\n        message = self._message._replace(**dct)\n\n        kwargs: Dict[str, str] = {}\n        if scheme is not sentinel:\n            kwargs[\"scheme\"] = scheme\n        if host is not sentinel:\n            kwargs[\"host\"] = host\n        if remote is not sentinel:\n            kwargs[\"remote\"] = remote\n        if client_max_size is sentinel:\n            client_max_size = self._client_max_size\n\n        return self.__class__(\n            message,\n            self._payload,\n            self._protocol,\n            self._payload_writer,\n            self._task,\n            self._loop,\n            client_max_size=client_max_size,\n            state=self._state.copy(),\n            **kwargs,\n        )\n\n    @property\n    def task(self) -> \"asyncio.Task[None]\":\n        return self._task\n\n    @property\n    def protocol(self) -> \"RequestHandler\":\n        return self._protocol\n\n    @property\n    def transport(self) -> Optional[asyncio.Transport]:\n        if self._protocol is None:\n            return None\n        return self._protocol.transport\n\n    @property\n    def writer(self) -> AbstractStreamWriter:\n        return self._payload_writer\n\n    @property\n    def client_max_size(self) -> int:\n        return self._client_max_size\n\n    @reify\n    def rel_url(self) -> URL:\n        return self._rel_url\n\n    # MutableMapping API\n\n    def __getitem__(self, key: str) -> Any:\n        return self._state[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self._state[key] = value\n\n    def __delitem__(self, key: str) -> None:\n        del self._state[key]\n\n    def __len__(self) -> int:\n        return len(self._state)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._state)\n\n    ########\n\n    @reify\n    def secure(self) -> bool:\n        \"\"\"A bool indicating if the request is handled with SSL.\"\"\"\n        return self.scheme == \"https\"\n\n    @reify\n    def forwarded(self) -> Tuple[Mapping[str, str], ...]:\n        \"\"\"A tuple containing all parsed Forwarded header(s).\n\n        Makes an effort to parse Forwarded headers as specified by RFC 7239:\n\n        - It adds one (immutable) dictionary per Forwarded 'field-value', ie\n          per proxy. The element corresponds to the data in the Forwarded\n          field-value added by the first proxy encountered by the client. Each\n          subsequent item corresponds to those added by later proxies.\n        - It checks that every value has valid syntax in general as specified\n          in section 4: either a 'token' or a 'quoted-string'.\n        - It un-escapes found escape sequences.\n        - It does NOT validate 'by' and 'for' contents as specified in section\n          6.\n        - It does NOT validate 'host' contents (Host ABNF).\n        - It does NOT validate 'proto' contents for valid URI scheme names.\n\n        Returns a tuple containing one or more immutable dicts\n        \"\"\"\n        elems = []\n        for field_value in self._message.headers.getall(hdrs.FORWARDED, ()):\n            length = len(field_value)\n            pos = 0\n            need_separator = False\n            elem: Dict[str, str] = {}\n            elems.append(types.MappingProxyType(elem))\n            while 0 <= pos < length:\n                match = _FORWARDED_PAIR_RE.match(field_value, pos)\n                if match is not None:  # got a valid forwarded-pair\n                    if need_separator:\n                        # bad syntax here, skip to next comma\n                        pos = field_value.find(\",\", pos)\n                    else:\n                        name, value, port = match.groups()\n                        if value[0] == '\"':\n                            # quoted string: remove quotes and unescape\n                            value = _QUOTED_PAIR_REPLACE_RE.sub(r\"\\1\", value[1:-1])\n                        if port:\n                            value += port\n                        elem[name.lower()] = value\n                        pos += len(match.group(0))\n                        need_separator = True\n                elif field_value[pos] == \",\":  # next forwarded-element\n                    need_separator = False\n                    elem = {}\n                    elems.append(types.MappingProxyType(elem))\n                    pos += 1\n                elif field_value[pos] == \";\":  # next forwarded-pair\n                    need_separator = False\n                    pos += 1\n                elif field_value[pos] in \" \\t\":\n                    # Allow whitespace even between forwarded-pairs, though\n                    # RFC 7239 doesn't. This simplifies code and is in line\n                    # with Postel's law.\n                    pos += 1\n                else:\n                    # bad syntax here, skip to next comma\n                    pos = field_value.find(\",\", pos)\n        return tuple(elems)\n\n    @reify\n    def scheme(self) -> str:\n        \"\"\"A string representing the scheme of the request.\n\n        Hostname is resolved in this order:\n\n        - overridden value by .clone(scheme=new_scheme) call.\n        - type of connection to peer: HTTPS if socket is SSL, HTTP otherwise.\n\n        'http' or 'https'.\n        \"\"\"\n        if self._transport_sslcontext:\n            return \"https\"\n        else:\n            return \"http\"\n\n    @reify\n    def method(self) -> str:\n        \"\"\"Read only property for getting HTTP method.\n\n        The value is upper-cased str like 'GET', 'POST', 'PUT' etc.\n        \"\"\"\n        return self._method\n\n    @reify\n    def version(self) -> HttpVersion:\n        \"\"\"Read only property for getting HTTP version of request.\n\n        Returns aiohttp.protocol.HttpVersion instance.\n        \"\"\"\n        return self._version\n\n    @reify\n    def host(self) -> str:\n        \"\"\"Hostname of the request.\n\n        Hostname is resolved in this order:\n\n        - overridden value by .clone(host=new_host) call.\n        - HOST HTTP header\n        - socket.getfqdn() value\n        \"\"\"\n        host = self._message.headers.get(hdrs.HOST)\n        if host is not None:\n            return host\n        return socket.getfqdn()\n\n    @reify\n    def remote(self) -> Optional[str]:\n        \"\"\"Remote IP of client initiated HTTP request.\n\n        The IP is resolved in this order:\n\n        - overridden value by .clone(remote=new_remote) call.\n        - peername of opened socket\n        \"\"\"\n        if self._transport_peername is None:\n            return None\n        if isinstance(self._transport_peername, (list, tuple)):\n            return str(self._transport_peername[0])\n        return str(self._transport_peername)\n\n    @reify\n    def url(self) -> URL:\n        url = URL.build(scheme=self.scheme, host=self.host)\n        return url.join(self._rel_url)\n\n    @reify\n    def path(self) -> str:\n        \"\"\"The URL including *PATH INFO* without the host or scheme.\n\n        E.g., ``/app/blog``\n        \"\"\"\n        return self._rel_url.path\n\n    @reify\n    def path_qs(self) -> str:\n        \"\"\"The URL including PATH_INFO and the query string.\n\n        E.g, /app/blog?id=10\n        \"\"\"\n        return str(self._rel_url)\n\n    @reify\n    def raw_path(self) -> str:\n        \"\"\"The URL including raw *PATH INFO* without the host or scheme.\n\n        Warning, the path is unquoted and may contains non valid URL characters\n\n        E.g., ``/my%2Fpath%7Cwith%21some%25strange%24characters``\n        \"\"\"\n        return self._message.path\n\n    @reify\n    def query(self) -> MultiDictProxy[str]:\n        \"\"\"A multidict with all the variables in the query string.\"\"\"\n        return MultiDictProxy(self._rel_url.query)\n\n    @reify\n    def query_string(self) -> str:\n        \"\"\"The query string in the URL.\n\n        E.g., id=10\n        \"\"\"\n        return self._rel_url.query_string\n\n    @reify\n    def headers(self) -> \"CIMultiDictProxy[str]\":\n        \"\"\"A case-insensitive multidict proxy with all headers.\"\"\"\n        return self._headers\n\n    @reify\n    def raw_headers(self) -> RawHeaders:\n        \"\"\"A sequence of pairs for all headers.\"\"\"\n        return self._message.raw_headers\n\n    @reify\n    def if_modified_since(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of If-Modified-Since HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        return parse_http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))\n\n    @reify\n    def if_unmodified_since(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of If-Unmodified-Since HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        return parse_http_date(self.headers.get(hdrs.IF_UNMODIFIED_SINCE))\n\n    @staticmethod\n    def _etag_values(etag_header: str) -> Iterator[ETag]:\n        \"\"\"Extract `ETag` objects from raw header.\"\"\"\n        if etag_header == ETAG_ANY:\n            yield ETag(\n                is_weak=False,\n                value=ETAG_ANY,\n            )\n        else:\n            for match in LIST_QUOTED_ETAG_RE.finditer(etag_header):\n                is_weak, value, garbage = match.group(2, 3, 4)\n                # Any symbol captured by 4th group means\n                # that the following sequence is invalid.\n                if garbage:\n                    break\n\n                yield ETag(\n                    is_weak=bool(is_weak),\n                    value=value,\n                )\n\n    @classmethod\n    def _if_match_or_none_impl(\n        cls, header_value: Optional[str]\n    ) -> Optional[Tuple[ETag, ...]]:\n        if not header_value:\n            return None\n\n        return tuple(cls._etag_values(header_value))\n\n    @reify\n    def if_match(self) -> Optional[Tuple[ETag, ...]]:\n        \"\"\"The value of If-Match HTTP header, or None.\n\n        This header is represented as a `tuple` of `ETag` objects.\n        \"\"\"\n        return self._if_match_or_none_impl(self.headers.get(hdrs.IF_MATCH))\n\n    @reify\n    def if_none_match(self) -> Optional[Tuple[ETag, ...]]:\n        \"\"\"The value of If-None-Match HTTP header, or None.\n\n        This header is represented as a `tuple` of `ETag` objects.\n        \"\"\"\n        return self._if_match_or_none_impl(self.headers.get(hdrs.IF_NONE_MATCH))\n\n    @reify\n    def if_range(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of If-Range HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        return parse_http_date(self.headers.get(hdrs.IF_RANGE))\n\n    @reify\n    def keep_alive(self) -> bool:\n        \"\"\"Is keepalive enabled by client?\"\"\"\n        return not self._message.should_close\n\n    @reify\n    def cookies(self) -> Mapping[str, str]:\n        \"\"\"Return request cookies.\n\n        A read-only dictionary-like object.\n        \"\"\"\n        raw = self.headers.get(hdrs.COOKIE, \"\")\n        parsed = SimpleCookie(raw)\n        return MappingProxyType({key: val.value for key, val in parsed.items()})\n\n    @reify\n    def http_range(self) -> slice:\n        \"\"\"The content of Range HTTP header.\n\n        Return a slice instance.\n\n        \"\"\"\n        rng = self._headers.get(hdrs.RANGE)\n        start, end = None, None\n        if rng is not None:\n            try:\n                pattern = r\"^bytes=(\\d*)-(\\d*)$\"\n                start, end = re.findall(pattern, rng)[0]\n            except IndexError:  # pattern was not found in header\n                raise ValueError(\"range not in acceptable format\")\n\n            end = int(end) if end else None\n            start = int(start) if start else None\n\n            if start is None and end is not None:\n                # end with no start is to return tail of content\n                start = -end\n                end = None\n\n            if start is not None and end is not None:\n                # end is inclusive in range header, exclusive for slice\n                end += 1\n\n                if start >= end:\n                    raise ValueError(\"start cannot be after end\")\n\n            if start is end is None:  # No valid range supplied\n                raise ValueError(\"No start or end of range specified\")\n\n        return slice(start, end, 1)\n\n    @reify\n    def content(self) -> StreamReader:\n        \"\"\"Return raw payload stream.\"\"\"\n        return self._payload\n\n    @property\n    def can_read_body(self) -> bool:\n        \"\"\"Return True if request's HTTP BODY can be read, False otherwise.\"\"\"\n        return not self._payload.at_eof()\n\n    @reify\n    def body_exists(self) -> bool:\n        \"\"\"Return True if request has HTTP BODY, False otherwise.\"\"\"\n        return type(self._payload) is not EmptyStreamReader\n\n    async def release(self) -> None:\n        \"\"\"Release request.\n\n        Eat unread part of HTTP BODY if present.\n        \"\"\"\n        while not self._payload.at_eof():\n            await self._payload.readany()\n\n    async def read(self) -> bytes:\n        \"\"\"Read request body if present.\n\n        Returns bytes object with full request content.\n        \"\"\"\n        if self._read_bytes is None:\n            body = bytearray()\n            while True:\n                chunk = await self._payload.readany()\n                body.extend(chunk)\n                if self._client_max_size:\n                    body_size = len(body)\n                    if body_size > self._client_max_size:\n                        raise HTTPRequestEntityTooLarge(\n                            max_size=self._client_max_size, actual_size=body_size\n                        )\n                if not chunk:\n                    break\n            self._read_bytes = bytes(body)\n        return self._read_bytes\n\n    async def text(self) -> str:\n        \"\"\"Return BODY as text using encoding from .charset.\"\"\"\n        bytes_body = await self.read()\n        encoding = self.charset or \"utf-8\"\n        try:\n            return bytes_body.decode(encoding)\n        except LookupError:\n            raise HTTPUnsupportedMediaType()\n\n    async def json(\n        self,\n        *,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        content_type: Optional[str] = \"application/json\",\n    ) -> Any:\n        \"\"\"Return BODY as JSON.\"\"\"\n        body = await self.text()\n        if content_type:\n            if not is_expected_content_type(self.content_type, content_type):\n                raise HTTPBadRequest(\n                    text=(\n                        \"Attempt to decode JSON with \"\n                        \"unexpected mimetype: %s\" % self.content_type\n                    )\n                )\n\n        return loads(body)\n\n    async def multipart(self) -> MultipartReader:\n        \"\"\"Return async iterator to process BODY as multipart.\"\"\"\n        return MultipartReader(self._headers, self._payload)\n\n    async def post(self) -> \"MultiDictProxy[Union[str, bytes, FileField]]\":\n        \"\"\"Return POST parameters.\"\"\"\n        if self._post is not None:\n            return self._post\n        if self._method not in self.POST_METHODS:\n            self._post = MultiDictProxy(MultiDict())\n            return self._post\n\n        content_type = self.content_type\n        if content_type not in (\n            \"\",\n            \"application/x-www-form-urlencoded\",\n            \"multipart/form-data\",\n        ):\n            self._post = MultiDictProxy(MultiDict())\n            return self._post\n\n        out: MultiDict[Union[str, bytes, FileField]] = MultiDict()\n\n        if content_type == \"multipart/form-data\":\n            multipart = await self.multipart()\n            max_size = self._client_max_size\n\n            field = await multipart.next()\n            while field is not None:\n                size = 0\n                field_ct = field.headers.get(hdrs.CONTENT_TYPE)\n\n                if isinstance(field, BodyPartReader):\n                    assert field.name is not None\n\n                    # Note that according to RFC 7578, the Content-Type header\n                    # is optional, even for files, so we can't assume it's\n                    # present.\n                    # https://tools.ietf.org/html/rfc7578#section-4.4\n                    if field.filename:\n                        # store file in temp file\n                        tmp = await self._loop.run_in_executor(\n                            None, tempfile.TemporaryFile\n                        )\n                        chunk = await field.read_chunk(size=2**16)\n                        while chunk:\n                            chunk = field.decode(chunk)\n                            await self._loop.run_in_executor(None, tmp.write, chunk)\n                            size += len(chunk)\n                            if 0 < max_size < size:\n                                await self._loop.run_in_executor(None, tmp.close)\n                                raise HTTPRequestEntityTooLarge(\n                                    max_size=max_size, actual_size=size\n                                )\n                            chunk = await field.read_chunk(size=2**16)\n                        await self._loop.run_in_executor(None, tmp.seek, 0)\n\n                        if field_ct is None:\n                            field_ct = \"application/octet-stream\"\n\n                        ff = FileField(\n                            field.name,\n                            field.filename,\n                            cast(io.BufferedReader, tmp),\n                            field_ct,\n                            field.headers,\n                        )\n                        out.add(field.name, ff)\n                    else:\n                        # deal with ordinary data\n                        value = await field.read(decode=True)\n                        if field_ct is None or field_ct.startswith(\"text/\"):\n                            charset = field.get_charset(default=\"utf-8\")\n                            out.add(field.name, value.decode(charset))\n                        else:\n                            out.add(field.name, value)\n                        size += len(value)\n                        if 0 < max_size < size:\n                            raise HTTPRequestEntityTooLarge(\n                                max_size=max_size, actual_size=size\n                            )\n                else:\n                    raise ValueError(\n                        \"To decode nested multipart you need \" \"to use custom reader\",\n                    )\n\n                field = await multipart.next()\n        else:\n            data = await self.read()\n            if data:\n                charset = self.charset or \"utf-8\"\n                bytes_query = data.rstrip()\n                try:\n                    query = bytes_query.decode(charset)\n                except LookupError:\n                    raise HTTPUnsupportedMediaType()\n                out.extend(\n                    parse_qsl(qs=query, keep_blank_values=True, encoding=charset)\n                )\n\n        self._post = MultiDictProxy(out)\n        return self._post\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        \"\"\"Extra info from protocol transport\"\"\"\n        protocol = self._protocol\n        if protocol is None:\n            return default\n\n        transport = protocol.transport\n        if transport is None:\n            return default\n\n        return transport.get_extra_info(name, default)\n\n    def __repr__(self) -> str:\n        ascii_encodable_path = self.path.encode(\"ascii\", \"backslashreplace\").decode(\n            \"ascii\"\n        )\n        return \"<{} {} {} >\".format(\n            self.__class__.__name__, self._method, ascii_encodable_path\n        )\n\n    def __eq__(self, other: object) -> bool:\n        return id(self) == id(other)\n\n    def __bool__(self) -> bool:\n        return True\n\n    async def _prepare_hook(self, response: StreamResponse) -> None:\n        return\n\n    def _cancel(self, exc: BaseException) -> None:\n        set_exception(self._payload, exc)\n        for fut in self._disconnection_waiters:\n            set_result(fut, None)\n\n    def _finish(self) -> None:\n        for fut in self._disconnection_waiters:\n            fut.cancel()\n\n        if self._post is None or self.content_type != \"multipart/form-data\":\n            return\n\n        # NOTE: Release file descriptors for the\n        # NOTE: `tempfile.Temporaryfile`-created `_io.BufferedRandom`\n        # NOTE: instances of files sent within multipart request body\n        # NOTE: via HTTP POST request.\n        for file_name, file_field_object in self._post.items():\n            if not isinstance(file_field_object, FileField):\n                continue\n\n            file_field_object.file.close()\n\n    async def wait_for_disconnection(self) -> None:\n        loop = asyncio.get_event_loop()\n        fut: asyncio.Future[None] = loop.create_future()\n        self._disconnection_waiters.add(fut)\n        try:\n            await fut\n        finally:\n            self._disconnection_waiters.remove(fut)\n\n\nclass Request(BaseRequest):\n    __slots__ = (\"_match_info\",)\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n\n        # matchdict, route_name, handler\n        # or information about traversal lookup\n\n        # initialized after route resolving\n        self._match_info: Optional[UrlMappingMatchInfo] = None\n\n    def clone(\n        self,\n        *,\n        method: Union[str, _SENTINEL] = sentinel,\n        rel_url: Union[StrOrURL, _SENTINEL] = sentinel,\n        headers: Union[LooseHeaders, _SENTINEL] = sentinel,\n        scheme: Union[str, _SENTINEL] = sentinel,\n        host: Union[str, _SENTINEL] = sentinel,\n        remote: Union[str, _SENTINEL] = sentinel,\n        client_max_size: Union[int, _SENTINEL] = sentinel,\n    ) -> \"Request\":\n        ret = super().clone(\n            method=method,\n            rel_url=rel_url,\n            headers=headers,\n            scheme=scheme,\n            host=host,\n            remote=remote,\n            client_max_size=client_max_size,\n        )\n        new_ret = cast(Request, ret)\n        new_ret._match_info = self._match_info\n        return new_ret\n\n    @reify\n    def match_info(self) -> \"UrlMappingMatchInfo\":\n        \"\"\"Result of route resolving.\"\"\"\n        match_info = self._match_info\n        assert match_info is not None\n        return match_info\n\n    @property\n    def app(self) -> \"Application\":\n        \"\"\"Application instance.\"\"\"\n        match_info = self._match_info\n        assert match_info is not None\n        return match_info.current_app\n\n    @property\n    def config_dict(self) -> ChainMapProxy:\n        match_info = self._match_info\n        assert match_info is not None\n        lst = match_info.apps\n        app = self.app\n        idx = lst.index(app)\n        sublist = list(reversed(lst[: idx + 1]))\n        return ChainMapProxy(sublist)\n\n    async def _prepare_hook(self, response: StreamResponse) -> None:\n        match_info = self._match_info\n        if match_info is None:\n            return\n        for app in match_info._apps:\n            await app.on_response_prepare.send(self, response)\n", "aiohttp/http_websocket.py": "\"\"\"WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport functools\nimport json\nimport random\nimport re\nimport sys\nimport zlib\nfrom enum import IntEnum\nfrom struct import Struct\nfrom typing import (\n    Any,\n    Callable,\n    Final,\n    List,\n    NamedTuple,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom .base_protocol import BaseProtocol\nfrom .compression_utils import ZLibCompressor, ZLibDecompressor\nfrom .helpers import NO_EXTENSIONS, set_exception\nfrom .streams import DataQueue\n\n__all__ = (\n    \"WS_CLOSED_MESSAGE\",\n    \"WS_CLOSING_MESSAGE\",\n    \"WS_KEY\",\n    \"WebSocketReader\",\n    \"WebSocketWriter\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n)\n\n\nclass WSCloseCode(IntEnum):\n    OK = 1000\n    GOING_AWAY = 1001\n    PROTOCOL_ERROR = 1002\n    UNSUPPORTED_DATA = 1003\n    ABNORMAL_CLOSURE = 1006\n    INVALID_TEXT = 1007\n    POLICY_VIOLATION = 1008\n    MESSAGE_TOO_BIG = 1009\n    MANDATORY_EXTENSION = 1010\n    INTERNAL_ERROR = 1011\n    SERVICE_RESTART = 1012\n    TRY_AGAIN_LATER = 1013\n    BAD_GATEWAY = 1014\n\n\nALLOWED_CLOSE_CODES: Final[Set[int]] = {int(i) for i in WSCloseCode}\n\n# For websockets, keeping latency low is extremely important as implementations\n# generally expect to be able to send and receive messages quickly.  We use a\n# larger chunk size than the default to reduce the number of executor calls\n# since the executor is a significant source of latency and overhead when\n# the chunks are small. A size of 5KiB was chosen because it is also the\n# same value python-zlib-ng choose to use as the threshold to release the GIL.\n\nWEBSOCKET_MAX_SYNC_CHUNK_SIZE = 5 * 1024\n\n\nclass WSMsgType(IntEnum):\n    # websocket spec types\n    CONTINUATION = 0x0\n    TEXT = 0x1\n    BINARY = 0x2\n    PING = 0x9\n    PONG = 0xA\n    CLOSE = 0x8\n\n    # aiohttp specific types\n    CLOSING = 0x100\n    CLOSED = 0x101\n    ERROR = 0x102\n\n\nWS_KEY: Final[bytes] = b\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\"\n\n\nUNPACK_LEN2 = Struct(\"!H\").unpack_from\nUNPACK_LEN3 = Struct(\"!Q\").unpack_from\nUNPACK_CLOSE_CODE = Struct(\"!H\").unpack\nPACK_LEN1 = Struct(\"!BB\").pack\nPACK_LEN2 = Struct(\"!BBH\").pack\nPACK_LEN3 = Struct(\"!BBQ\").pack\nPACK_CLOSE_CODE = Struct(\"!H\").pack\nMSG_SIZE: Final[int] = 2**14\nDEFAULT_LIMIT: Final[int] = 2**16\n\n\nclass WSMessage(NamedTuple):\n    type: WSMsgType\n    # To type correctly, this would need some kind of tagged union for each type.\n    data: Any\n    extra: Optional[str]\n\n    def json(self, *, loads: Callable[[Any], Any] = json.loads) -> Any:\n        \"\"\"Return parsed JSON data.\n\n        .. versionadded:: 0.22\n        \"\"\"\n        return loads(self.data)\n\n\nWS_CLOSED_MESSAGE = WSMessage(WSMsgType.CLOSED, None, None)\nWS_CLOSING_MESSAGE = WSMessage(WSMsgType.CLOSING, None, None)\n\n\nclass WebSocketError(Exception):\n    \"\"\"WebSocket protocol parser error.\"\"\"\n\n    def __init__(self, code: int, message: str) -> None:\n        self.code = code\n        super().__init__(code, message)\n\n    def __str__(self) -> str:\n        return cast(str, self.args[1])\n\n\nclass WSHandshakeError(Exception):\n    \"\"\"WebSocket protocol handshake error.\"\"\"\n\n\nnative_byteorder: Final[str] = sys.byteorder\n\n\n# Used by _websocket_mask_python\n@functools.lru_cache\ndef _xor_table() -> List[bytes]:\n    return [bytes(a ^ b for a in range(256)) for b in range(256)]\n\n\ndef _websocket_mask_python(mask: bytes, data: bytearray) -> None:\n    \"\"\"Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytearray`\n    object of any length. The contents of `data` are masked with `mask`,\n    as specified in section 5.3 of RFC 6455.\n\n    Note that this function mutates the `data` argument.\n\n    This pure-python implementation may be replaced by an optimized\n    version when available.\n\n    \"\"\"\n    assert isinstance(data, bytearray), data\n    assert len(mask) == 4, mask\n\n    if data:\n        _XOR_TABLE = _xor_table()\n        a, b, c, d = (_XOR_TABLE[n] for n in mask)\n        data[::4] = data[::4].translate(a)\n        data[1::4] = data[1::4].translate(b)\n        data[2::4] = data[2::4].translate(c)\n        data[3::4] = data[3::4].translate(d)\n\n\nif NO_EXTENSIONS:  # pragma: no cover\n    _websocket_mask = _websocket_mask_python\nelse:\n    try:\n        from ._websocket import _websocket_mask_cython  # type: ignore[import-not-found]\n\n        _websocket_mask = _websocket_mask_cython\n    except ImportError:  # pragma: no cover\n        _websocket_mask = _websocket_mask_python\n\n_WS_DEFLATE_TRAILING: Final[bytes] = bytes([0x00, 0x00, 0xFF, 0xFF])\n\n\n_WS_EXT_RE: Final[Pattern[str]] = re.compile(\n    r\"^(?:;\\s*(?:\"\n    r\"(server_no_context_takeover)|\"\n    r\"(client_no_context_takeover)|\"\n    r\"(server_max_window_bits(?:=(\\d+))?)|\"\n    r\"(client_max_window_bits(?:=(\\d+))?)))*$\"\n)\n\n_WS_EXT_RE_SPLIT: Final[Pattern[str]] = re.compile(r\"permessage-deflate([^,]+)?\")\n\n\ndef ws_ext_parse(extstr: Optional[str], isserver: bool = False) -> Tuple[int, bool]:\n    if not extstr:\n        return 0, False\n\n    compress = 0\n    notakeover = False\n    for ext in _WS_EXT_RE_SPLIT.finditer(extstr):\n        defext = ext.group(1)\n        # Return compress = 15 when get `permessage-deflate`\n        if not defext:\n            compress = 15\n            break\n        match = _WS_EXT_RE.match(defext)\n        if match:\n            compress = 15\n            if isserver:\n                # Server never fail to detect compress handshake.\n                # Server does not need to send max wbit to client\n                if match.group(4):\n                    compress = int(match.group(4))\n                    # Group3 must match if group4 matches\n                    # Compress wbit 8 does not support in zlib\n                    # If compress level not support,\n                    # CONTINUE to next extension\n                    if compress > 15 or compress < 9:\n                        compress = 0\n                        continue\n                if match.group(1):\n                    notakeover = True\n                # Ignore regex group 5 & 6 for client_max_window_bits\n                break\n            else:\n                if match.group(6):\n                    compress = int(match.group(6))\n                    # Group5 must match if group6 matches\n                    # Compress wbit 8 does not support in zlib\n                    # If compress level not support,\n                    # FAIL the parse progress\n                    if compress > 15 or compress < 9:\n                        raise WSHandshakeError(\"Invalid window size\")\n                if match.group(2):\n                    notakeover = True\n                # Ignore regex group 5 & 6 for client_max_window_bits\n                break\n        # Return Fail if client side and not match\n        elif not isserver:\n            raise WSHandshakeError(\"Extension for deflate not supported\" + ext.group(1))\n\n    return compress, notakeover\n\n\ndef ws_ext_gen(\n    compress: int = 15, isserver: bool = False, server_notakeover: bool = False\n) -> str:\n    # client_notakeover=False not used for server\n    # compress wbit 8 does not support in zlib\n    if compress < 9 or compress > 15:\n        raise ValueError(\n            \"Compress wbits must between 9 and 15, \" \"zlib does not support wbits=8\"\n        )\n    enabledext = [\"permessage-deflate\"]\n    if not isserver:\n        enabledext.append(\"client_max_window_bits\")\n\n    if compress < 15:\n        enabledext.append(\"server_max_window_bits=\" + str(compress))\n    if server_notakeover:\n        enabledext.append(\"server_no_context_takeover\")\n    # if client_notakeover:\n    #     enabledext.append('client_no_context_takeover')\n    return \"; \".join(enabledext)\n\n\nclass WSParserState(IntEnum):\n    READ_HEADER = 1\n    READ_PAYLOAD_LENGTH = 2\n    READ_PAYLOAD_MASK = 3\n    READ_PAYLOAD = 4\n\n\nclass WebSocketReader:\n    def __init__(\n        self, queue: DataQueue[WSMessage], max_msg_size: int, compress: bool = True\n    ) -> None:\n        self.queue = queue\n        self._max_msg_size = max_msg_size\n\n        self._exc: Optional[BaseException] = None\n        self._partial = bytearray()\n        self._state = WSParserState.READ_HEADER\n\n        self._opcode: Optional[int] = None\n        self._frame_fin = False\n        self._frame_opcode: Optional[int] = None\n        self._frame_payload = bytearray()\n\n        self._tail = b\"\"\n        self._has_mask = False\n        self._frame_mask: Optional[bytes] = None\n        self._payload_length = 0\n        self._payload_length_flag = 0\n        self._compressed: Optional[bool] = None\n        self._decompressobj: Optional[ZLibDecompressor] = None\n        self._compress = compress\n\n    def feed_eof(self) -> None:\n        self.queue.feed_eof()\n\n    def feed_data(self, data: bytes) -> Tuple[bool, bytes]:\n        if self._exc:\n            return True, data\n\n        try:\n            return self._feed_data(data)\n        except Exception as exc:\n            self._exc = exc\n            set_exception(self.queue, exc)\n            return True, b\"\"\n\n    def _feed_data(self, data: bytes) -> Tuple[bool, bytes]:\n        for fin, opcode, payload, compressed in self.parse_frame(data):\n            if compressed and not self._decompressobj:\n                self._decompressobj = ZLibDecompressor(suppress_deflate_header=True)\n            if opcode == WSMsgType.CLOSE:\n                if len(payload) >= 2:\n                    close_code = UNPACK_CLOSE_CODE(payload[:2])[0]\n                    if close_code < 3000 and close_code not in ALLOWED_CLOSE_CODES:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            f\"Invalid close code: {close_code}\",\n                        )\n                    try:\n                        close_message = payload[2:].decode(\"utf-8\")\n                    except UnicodeDecodeError as exc:\n                        raise WebSocketError(\n                            WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                        ) from exc\n                    msg = WSMessage(WSMsgType.CLOSE, close_code, close_message)\n                elif payload:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        f\"Invalid close frame: {fin} {opcode} {payload!r}\",\n                    )\n                else:\n                    msg = WSMessage(WSMsgType.CLOSE, 0, \"\")\n\n                self.queue.feed_data(msg)\n\n            elif opcode == WSMsgType.PING:\n                self.queue.feed_data(WSMessage(WSMsgType.PING, payload, \"\"))\n\n            elif opcode == WSMsgType.PONG:\n                self.queue.feed_data(WSMessage(WSMsgType.PONG, payload, \"\"))\n\n            elif (\n                opcode not in (WSMsgType.TEXT, WSMsgType.BINARY)\n                and self._opcode is None\n            ):\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR, f\"Unexpected opcode={opcode!r}\"\n                )\n            else:\n                # load text/binary\n                if not fin:\n                    # got partial frame payload\n                    if opcode != WSMsgType.CONTINUATION:\n                        self._opcode = opcode\n                    self._partial.extend(payload)\n                    if self._max_msg_size and len(self._partial) >= self._max_msg_size:\n                        raise WebSocketError(\n                            WSCloseCode.MESSAGE_TOO_BIG,\n                            \"Message size {} exceeds limit {}\".format(\n                                len(self._partial), self._max_msg_size\n                            ),\n                        )\n                else:\n                    # previous frame was non finished\n                    # we should get continuation opcode\n                    if self._partial:\n                        if opcode != WSMsgType.CONTINUATION:\n                            raise WebSocketError(\n                                WSCloseCode.PROTOCOL_ERROR,\n                                \"The opcode in non-fin frame is expected \"\n                                \"to be zero, got {!r}\".format(opcode),\n                            )\n\n                    if opcode == WSMsgType.CONTINUATION:\n                        assert self._opcode is not None\n                        opcode = self._opcode\n                        self._opcode = None\n\n                    self._partial.extend(payload)\n                    if self._max_msg_size and len(self._partial) >= self._max_msg_size:\n                        raise WebSocketError(\n                            WSCloseCode.MESSAGE_TOO_BIG,\n                            \"Message size {} exceeds limit {}\".format(\n                                len(self._partial), self._max_msg_size\n                            ),\n                        )\n\n                    # Decompress process must to be done after all packets\n                    # received.\n                    if compressed:\n                        assert self._decompressobj is not None\n                        self._partial.extend(_WS_DEFLATE_TRAILING)\n                        payload_merged = self._decompressobj.decompress_sync(\n                            self._partial, self._max_msg_size\n                        )\n                        if self._decompressobj.unconsumed_tail:\n                            left = len(self._decompressobj.unconsumed_tail)\n                            raise WebSocketError(\n                                WSCloseCode.MESSAGE_TOO_BIG,\n                                \"Decompressed message size {} exceeds limit {}\".format(\n                                    self._max_msg_size + left, self._max_msg_size\n                                ),\n                            )\n                    else:\n                        payload_merged = bytes(self._partial)\n\n                    self._partial.clear()\n\n                    if opcode == WSMsgType.TEXT:\n                        try:\n                            text = payload_merged.decode(\"utf-8\")\n                            self.queue.feed_data(WSMessage(WSMsgType.TEXT, text, \"\"))\n                        except UnicodeDecodeError as exc:\n                            raise WebSocketError(\n                                WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                            ) from exc\n                    else:\n                        self.queue.feed_data(\n                            WSMessage(WSMsgType.BINARY, payload_merged, \"\"),\n                        )\n\n        return False, b\"\"\n\n    def parse_frame(\n        self, buf: bytes\n    ) -> List[Tuple[bool, Optional[int], bytearray, Optional[bool]]]:\n        \"\"\"Return the next frame from the socket.\"\"\"\n        frames = []\n        if self._tail:\n            buf, self._tail = self._tail + buf, b\"\"\n\n        start_pos = 0\n        buf_length = len(buf)\n\n        while True:\n            # read header\n            if self._state == WSParserState.READ_HEADER:\n                if buf_length - start_pos >= 2:\n                    data = buf[start_pos : start_pos + 2]\n                    start_pos += 2\n                    first_byte, second_byte = data\n\n                    fin = (first_byte >> 7) & 1\n                    rsv1 = (first_byte >> 6) & 1\n                    rsv2 = (first_byte >> 5) & 1\n                    rsv3 = (first_byte >> 4) & 1\n                    opcode = first_byte & 0xF\n\n                    # frame-fin = %x0 ; more frames of this message follow\n                    #           / %x1 ; final frame of this message\n                    # frame-rsv1 = %x0 ;\n                    #    1 bit, MUST be 0 unless negotiated otherwise\n                    # frame-rsv2 = %x0 ;\n                    #    1 bit, MUST be 0 unless negotiated otherwise\n                    # frame-rsv3 = %x0 ;\n                    #    1 bit, MUST be 0 unless negotiated otherwise\n                    #\n                    # Remove rsv1 from this test for deflate development\n                    if rsv2 or rsv3 or (rsv1 and not self._compress):\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            \"Received frame with non-zero reserved bits\",\n                        )\n\n                    if opcode > 0x7 and fin == 0:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            \"Received fragmented control frame\",\n                        )\n\n                    has_mask = (second_byte >> 7) & 1\n                    length = second_byte & 0x7F\n\n                    # Control frames MUST have a payload\n                    # length of 125 bytes or less\n                    if opcode > 0x7 and length > 125:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            \"Control frame payload cannot be \" \"larger than 125 bytes\",\n                        )\n\n                    # Set compress status if last package is FIN\n                    # OR set compress status if this is first fragment\n                    # Raise error if not first fragment with rsv1 = 0x1\n                    if self._frame_fin or self._compressed is None:\n                        self._compressed = True if rsv1 else False\n                    elif rsv1:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            \"Received frame with non-zero reserved bits\",\n                        )\n\n                    self._frame_fin = bool(fin)\n                    self._frame_opcode = opcode\n                    self._has_mask = bool(has_mask)\n                    self._payload_length_flag = length\n                    self._state = WSParserState.READ_PAYLOAD_LENGTH\n                else:\n                    break\n\n            # read payload length\n            if self._state == WSParserState.READ_PAYLOAD_LENGTH:\n                length = self._payload_length_flag\n                if length == 126:\n                    if buf_length - start_pos >= 2:\n                        data = buf[start_pos : start_pos + 2]\n                        start_pos += 2\n                        length = UNPACK_LEN2(data)[0]\n                        self._payload_length = length\n                        self._state = (\n                            WSParserState.READ_PAYLOAD_MASK\n                            if self._has_mask\n                            else WSParserState.READ_PAYLOAD\n                        )\n                    else:\n                        break\n                elif length > 126:\n                    if buf_length - start_pos >= 8:\n                        data = buf[start_pos : start_pos + 8]\n                        start_pos += 8\n                        length = UNPACK_LEN3(data)[0]\n                        self._payload_length = length\n                        self._state = (\n                            WSParserState.READ_PAYLOAD_MASK\n                            if self._has_mask\n                            else WSParserState.READ_PAYLOAD\n                        )\n                    else:\n                        break\n                else:\n                    self._payload_length = length\n                    self._state = (\n                        WSParserState.READ_PAYLOAD_MASK\n                        if self._has_mask\n                        else WSParserState.READ_PAYLOAD\n                    )\n\n            # read payload mask\n            if self._state == WSParserState.READ_PAYLOAD_MASK:\n                if buf_length - start_pos >= 4:\n                    self._frame_mask = buf[start_pos : start_pos + 4]\n                    start_pos += 4\n                    self._state = WSParserState.READ_PAYLOAD\n                else:\n                    break\n\n            if self._state == WSParserState.READ_PAYLOAD:\n                length = self._payload_length\n                payload = self._frame_payload\n\n                chunk_len = buf_length - start_pos\n                if length >= chunk_len:\n                    self._payload_length = length - chunk_len\n                    payload.extend(buf[start_pos:])\n                    start_pos = buf_length\n                else:\n                    self._payload_length = 0\n                    payload.extend(buf[start_pos : start_pos + length])\n                    start_pos = start_pos + length\n\n                if self._payload_length == 0:\n                    if self._has_mask:\n                        assert self._frame_mask is not None\n                        _websocket_mask(self._frame_mask, payload)\n\n                    frames.append(\n                        (self._frame_fin, self._frame_opcode, payload, self._compressed)\n                    )\n\n                    self._frame_payload = bytearray()\n                    self._state = WSParserState.READ_HEADER\n                else:\n                    break\n\n        self._tail = buf[start_pos:]\n\n        return frames\n\n\nclass WebSocketWriter:\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        transport: asyncio.Transport,\n        *,\n        use_mask: bool = False,\n        limit: int = DEFAULT_LIMIT,\n        random: random.Random = random.Random(),\n        compress: int = 0,\n        notakeover: bool = False,\n    ) -> None:\n        self.protocol = protocol\n        self.transport = transport\n        self.use_mask = use_mask\n        self.randrange = random.randrange\n        self.compress = compress\n        self.notakeover = notakeover\n        self._closing = False\n        self._limit = limit\n        self._output_size = 0\n        self._compressobj: Any = None  # actually compressobj\n\n    async def _send_frame(\n        self, message: bytes, opcode: int, compress: Optional[int] = None\n    ) -> None:\n        \"\"\"Send a frame over the websocket with message as its payload.\"\"\"\n        if self._closing and not (opcode & WSMsgType.CLOSE):\n            raise ConnectionResetError(\"Cannot write to closing transport\")\n\n        rsv = 0\n\n        # Only compress larger packets (disabled)\n        # Does small packet needs to be compressed?\n        # if self.compress and opcode < 8 and len(message) > 124:\n        if (compress or self.compress) and opcode < 8:\n            if compress:\n                # Do not set self._compress if compressing is for this frame\n                compressobj = self._make_compress_obj(compress)\n            else:  # self.compress\n                if not self._compressobj:\n                    self._compressobj = self._make_compress_obj(self.compress)\n                compressobj = self._compressobj\n\n            message = await compressobj.compress(message)\n            # Its critical that we do not return control to the event\n            # loop until we have finished sending all the compressed\n            # data. Otherwise we could end up mixing compressed frames\n            # if there are multiple coroutines compressing data.\n            message += compressobj.flush(\n                zlib.Z_FULL_FLUSH if self.notakeover else zlib.Z_SYNC_FLUSH\n            )\n            if message.endswith(_WS_DEFLATE_TRAILING):\n                message = message[:-4]\n            rsv = rsv | 0x40\n\n        msg_length = len(message)\n\n        use_mask = self.use_mask\n        if use_mask:\n            mask_bit = 0x80\n        else:\n            mask_bit = 0\n\n        if msg_length < 126:\n            header = PACK_LEN1(0x80 | rsv | opcode, msg_length | mask_bit)\n        elif msg_length < (1 << 16):\n            header = PACK_LEN2(0x80 | rsv | opcode, 126 | mask_bit, msg_length)\n        else:\n            header = PACK_LEN3(0x80 | rsv | opcode, 127 | mask_bit, msg_length)\n        if use_mask:\n            mask_int = self.randrange(0, 0xFFFFFFFF)\n            mask = mask_int.to_bytes(4, \"big\")\n            message = bytearray(message)\n            _websocket_mask(mask, message)\n            self._write(header + mask + message)\n            self._output_size += len(header) + len(mask) + msg_length\n        else:\n            if msg_length > MSG_SIZE:\n                self._write(header)\n                self._write(message)\n            else:\n                self._write(header + message)\n\n            self._output_size += len(header) + msg_length\n\n        # It is safe to return control to the event loop when using compression\n        # after this point as we have already sent or buffered all the data.\n\n        if self._output_size > self._limit:\n            self._output_size = 0\n            await self.protocol._drain_helper()\n\n    def _make_compress_obj(self, compress: int) -> ZLibCompressor:\n        return ZLibCompressor(\n            level=zlib.Z_BEST_SPEED,\n            wbits=-compress,\n            max_sync_chunk_size=WEBSOCKET_MAX_SYNC_CHUNK_SIZE,\n        )\n\n    def _write(self, data: bytes) -> None:\n        if self.transport.is_closing():\n            raise ConnectionResetError(\"Cannot write to closing transport\")\n        self.transport.write(data)\n\n    async def pong(self, message: Union[bytes, str] = b\"\") -> None:\n        \"\"\"Send pong message.\"\"\"\n        if isinstance(message, str):\n            message = message.encode(\"utf-8\")\n        await self._send_frame(message, WSMsgType.PONG)\n\n    async def ping(self, message: Union[bytes, str] = b\"\") -> None:\n        \"\"\"Send ping message.\"\"\"\n        if isinstance(message, str):\n            message = message.encode(\"utf-8\")\n        await self._send_frame(message, WSMsgType.PING)\n\n    async def send(\n        self,\n        message: Union[str, bytes],\n        binary: bool = False,\n        compress: Optional[int] = None,\n    ) -> None:\n        \"\"\"Send a frame over the websocket with message as its payload.\"\"\"\n        if isinstance(message, str):\n            message = message.encode(\"utf-8\")\n        if binary:\n            await self._send_frame(message, WSMsgType.BINARY, compress)\n        else:\n            await self._send_frame(message, WSMsgType.TEXT, compress)\n\n    async def close(self, code: int = 1000, message: Union[bytes, str] = b\"\") -> None:\n        \"\"\"Close the websocket, sending the specified code and message.\"\"\"\n        if isinstance(message, str):\n            message = message.encode(\"utf-8\")\n        try:\n            await self._send_frame(\n                PACK_CLOSE_CODE(code) + message, opcode=WSMsgType.CLOSE\n            )\n        finally:\n            self._closing = True\n", "aiohttp/compression_utils.py": "import asyncio\nimport zlib\nfrom concurrent.futures import Executor\nfrom typing import Optional, cast\n\ntry:\n    try:\n        import brotlicffi as brotli\n    except ImportError:\n        import brotli\n\n    HAS_BROTLI = True\nexcept ImportError:  # pragma: no cover\n    HAS_BROTLI = False\n\nMAX_SYNC_CHUNK_SIZE = 1024\n\n\ndef encoding_to_mode(\n    encoding: Optional[str] = None,\n    suppress_deflate_header: bool = False,\n) -> int:\n    if encoding == \"gzip\":\n        return 16 + zlib.MAX_WBITS\n\n    return -zlib.MAX_WBITS if suppress_deflate_header else zlib.MAX_WBITS\n\n\nclass ZlibBaseHandler:\n    def __init__(\n        self,\n        mode: int,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        self._mode = mode\n        self._executor = executor\n        self._max_sync_chunk_size = max_sync_chunk_size\n\n\nclass ZLibCompressor(ZlibBaseHandler):\n    def __init__(\n        self,\n        encoding: Optional[str] = None,\n        suppress_deflate_header: bool = False,\n        level: Optional[int] = None,\n        wbits: Optional[int] = None,\n        strategy: int = zlib.Z_DEFAULT_STRATEGY,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        super().__init__(\n            mode=(\n                encoding_to_mode(encoding, suppress_deflate_header)\n                if wbits is None\n                else wbits\n            ),\n            executor=executor,\n            max_sync_chunk_size=max_sync_chunk_size,\n        )\n        if level is None:\n            self._compressor = zlib.compressobj(wbits=self._mode, strategy=strategy)\n        else:\n            self._compressor = zlib.compressobj(\n                wbits=self._mode, strategy=strategy, level=level\n            )\n        self._compress_lock = asyncio.Lock()\n\n    def compress_sync(self, data: bytes) -> bytes:\n        return self._compressor.compress(data)\n\n    async def compress(self, data: bytes) -> bytes:\n        async with self._compress_lock:\n            # To ensure the stream is consistent in the event\n            # there are multiple writers, we need to lock\n            # the compressor so that only one writer can\n            # compress at a time.\n            if (\n                self._max_sync_chunk_size is not None\n                and len(data) > self._max_sync_chunk_size\n            ):\n                return await asyncio.get_event_loop().run_in_executor(\n                    self._executor, self.compress_sync, data\n                )\n            return self.compress_sync(data)\n\n    def flush(self, mode: int = zlib.Z_FINISH) -> bytes:\n        return self._compressor.flush(mode)\n\n\nclass ZLibDecompressor(ZlibBaseHandler):\n    def __init__(\n        self,\n        encoding: Optional[str] = None,\n        suppress_deflate_header: bool = False,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        super().__init__(\n            mode=encoding_to_mode(encoding, suppress_deflate_header),\n            executor=executor,\n            max_sync_chunk_size=max_sync_chunk_size,\n        )\n        self._decompressor = zlib.decompressobj(wbits=self._mode)\n\n    def decompress_sync(self, data: bytes, max_length: int = 0) -> bytes:\n        return self._decompressor.decompress(data, max_length)\n\n    async def decompress(self, data: bytes, max_length: int = 0) -> bytes:\n        if (\n            self._max_sync_chunk_size is not None\n            and len(data) > self._max_sync_chunk_size\n        ):\n            return await asyncio.get_event_loop().run_in_executor(\n                self._executor, self.decompress_sync, data, max_length\n            )\n        return self.decompress_sync(data, max_length)\n\n    def flush(self, length: int = 0) -> bytes:\n        return (\n            self._decompressor.flush(length)\n            if length > 0\n            else self._decompressor.flush()\n        )\n\n    @property\n    def eof(self) -> bool:\n        return self._decompressor.eof\n\n    @property\n    def unconsumed_tail(self) -> bytes:\n        return self._decompressor.unconsumed_tail\n\n    @property\n    def unused_data(self) -> bytes:\n        return self._decompressor.unused_data\n\n\nclass BrotliDecompressor:\n    # Supports both 'brotlipy' and 'Brotli' packages\n    # since they share an import name. The top branches\n    # are for 'brotlipy' and bottom branches for 'Brotli'\n    def __init__(self) -> None:\n        if not HAS_BROTLI:\n            raise RuntimeError(\n                \"The brotli decompression is not available. \"\n                \"Please install `Brotli` module\"\n            )\n        self._obj = brotli.Decompressor()\n\n    def decompress_sync(self, data: bytes) -> bytes:\n        if hasattr(self._obj, \"decompress\"):\n            return cast(bytes, self._obj.decompress(data))\n        return cast(bytes, self._obj.process(data))\n\n    def flush(self) -> bytes:\n        if hasattr(self._obj, \"flush\"):\n            return cast(bytes, self._obj.flush())\n        return b\"\"\n", "aiohttp/tcp_helpers.py": "\"\"\"Helper methods to tune a TCP connection\"\"\"\n\nimport asyncio\nimport socket\nfrom contextlib import suppress\nfrom typing import Optional  # noqa\n\n__all__ = (\"tcp_keepalive\", \"tcp_nodelay\")\n\n\nif hasattr(socket, \"SO_KEEPALIVE\"):\n\n    def tcp_keepalive(transport: asyncio.Transport) -> None:\n        sock = transport.get_extra_info(\"socket\")\n        if sock is not None:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n\nelse:\n\n    def tcp_keepalive(transport: asyncio.Transport) -> None:  # pragma: no cover\n        pass\n\n\ndef tcp_nodelay(transport: asyncio.Transport, value: bool) -> None:\n    sock = transport.get_extra_info(\"socket\")\n\n    if sock is None:\n        return\n\n    if sock.family not in (socket.AF_INET, socket.AF_INET6):\n        return\n\n    value = bool(value)\n\n    # socket may be closed already, on windows OSError get raised\n    with suppress(OSError):\n        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, value)\n", "aiohttp/client.py": "\"\"\"HTTP Client for asyncio.\"\"\"\n\nimport asyncio\nimport base64\nimport dataclasses\nimport hashlib\nimport json\nimport os\nimport sys\nimport traceback\nimport warnings\nfrom contextlib import suppress\nfrom types import SimpleNamespace, TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Collection,\n    Coroutine,\n    Final,\n    FrozenSet,\n    Generator,\n    Generic,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    final,\n)\n\nfrom multidict import CIMultiDict, MultiDict, MultiDictProxy, istr\nfrom yarl import URL\n\nfrom . import hdrs, http, payload\nfrom .abc import AbstractCookieJar\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientConnectorCertificateError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientError,\n    ClientHttpProxyError,\n    ClientOSError,\n    ClientPayloadError,\n    ClientProxyConnectionError,\n    ClientResponseError,\n    ClientSSLError,\n    ConnectionTimeoutError,\n    ContentTypeError,\n    InvalidURL,\n    InvalidUrlClientError,\n    InvalidUrlRedirectClientError,\n    NonHttpUrlClientError,\n    NonHttpUrlRedirectClientError,\n    RedirectClientError,\n    ServerConnectionError,\n    ServerDisconnectedError,\n    ServerFingerprintMismatch,\n    ServerTimeoutError,\n    SocketTimeoutError,\n    TooManyRedirects,\n    WSServerHandshakeError,\n)\nfrom .client_reqrep import (\n    SSL_ALLOWED_TYPES,\n    ClientRequest,\n    ClientResponse,\n    Fingerprint,\n    RequestInfo,\n)\nfrom .client_ws import (\n    DEFAULT_WS_CLIENT_TIMEOUT,\n    ClientWebSocketResponse,\n    ClientWSTimeout,\n)\nfrom .connector import BaseConnector, NamedPipeConnector, TCPConnector, UnixConnector\nfrom .cookiejar import CookieJar\nfrom .helpers import (\n    _SENTINEL,\n    BasicAuth,\n    TimeoutHandle,\n    ceil_timeout,\n    get_env_proxy_for_url,\n    method_must_be_empty_body,\n    sentinel,\n    strip_auth_from_url,\n)\nfrom .http import WS_KEY, HttpVersion, WebSocketReader, WebSocketWriter\nfrom .http_websocket import WSHandshakeError, WSMessage, ws_ext_gen, ws_ext_parse\nfrom .streams import FlowControlDataQueue\nfrom .tracing import Trace, TraceConfig\nfrom .typedefs import JSONEncoder, LooseCookies, LooseHeaders, StrOrURL\n\n__all__ = (\n    # client_exceptions\n    \"ClientConnectionError\",\n    \"ClientConnectorCertificateError\",\n    \"ClientConnectorError\",\n    \"ClientConnectorSSLError\",\n    \"ClientError\",\n    \"ClientHttpProxyError\",\n    \"ClientOSError\",\n    \"ClientPayloadError\",\n    \"ClientProxyConnectionError\",\n    \"ClientResponseError\",\n    \"ClientSSLError\",\n    \"ConnectionTimeoutError\",\n    \"ContentTypeError\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"RedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlRedirectClientError\",\n    \"ServerConnectionError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ServerTimeoutError\",\n    \"SocketTimeoutError\",\n    \"TooManyRedirects\",\n    \"WSServerHandshakeError\",\n    # client_reqrep\n    \"ClientRequest\",\n    \"ClientResponse\",\n    \"Fingerprint\",\n    \"RequestInfo\",\n    # connector\n    \"BaseConnector\",\n    \"TCPConnector\",\n    \"UnixConnector\",\n    \"NamedPipeConnector\",\n    # client_ws\n    \"ClientWebSocketResponse\",\n    # client\n    \"ClientSession\",\n    \"ClientTimeout\",\n    \"request\",\n)\n\n\nif TYPE_CHECKING:\n    from ssl import SSLContext\nelse:\n    SSLContext = None\n\n\n@dataclasses.dataclass(frozen=True)\nclass ClientTimeout:\n    total: Optional[float] = None\n    connect: Optional[float] = None\n    sock_read: Optional[float] = None\n    sock_connect: Optional[float] = None\n    ceil_threshold: float = 5\n\n    # pool_queue_timeout: Optional[float] = None\n    # dns_resolution_timeout: Optional[float] = None\n    # socket_connect_timeout: Optional[float] = None\n    # connection_acquiring_timeout: Optional[float] = None\n    # new_connection_timeout: Optional[float] = None\n    # http_header_timeout: Optional[float] = None\n    # response_body_timeout: Optional[float] = None\n\n    # to create a timeout specific for a single request, either\n    # - create a completely new one to overwrite the default\n    # - or use https://docs.python.org/3/library/dataclasses.html#dataclasses.replace\n    # to overwrite the defaults\n\n\n# 5 Minute default read timeout\nDEFAULT_TIMEOUT: Final[ClientTimeout] = ClientTimeout(total=5 * 60)\n\n# https://www.rfc-editor.org/rfc/rfc9110#section-9.2.2\nIDEMPOTENT_METHODS = frozenset({\"GET\", \"HEAD\", \"OPTIONS\", \"TRACE\", \"PUT\", \"DELETE\"})\nHTTP_SCHEMA_SET = frozenset({\"http\", \"https\", \"\"})\n\n_RetType = TypeVar(\"_RetType\")\n_CharsetResolver = Callable[[ClientResponse, bytes], str]\n\n\n@final\nclass ClientSession:\n    \"\"\"First-class interface for making HTTP requests.\"\"\"\n\n    __slots__ = (\n        \"_base_url\",\n        \"_source_traceback\",\n        \"_connector\",\n        \"_loop\",\n        \"_cookie_jar\",\n        \"_connector_owner\",\n        \"_default_auth\",\n        \"_version\",\n        \"_json_serialize\",\n        \"_requote_redirect_url\",\n        \"_timeout\",\n        \"_raise_for_status\",\n        \"_auto_decompress\",\n        \"_trust_env\",\n        \"_default_headers\",\n        \"_skip_auto_headers\",\n        \"_request_class\",\n        \"_response_class\",\n        \"_ws_response_class\",\n        \"_trace_configs\",\n        \"_read_bufsize\",\n        \"_max_line_size\",\n        \"_max_field_size\",\n        \"_resolve_charset\",\n    )\n\n    def __init__(\n        self,\n        base_url: Optional[StrOrURL] = None,\n        *,\n        connector: Optional[BaseConnector] = None,\n        cookies: Optional[LooseCookies] = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Optional[Iterable[str]] = None,\n        auth: Optional[BasicAuth] = None,\n        json_serialize: JSONEncoder = json.dumps,\n        request_class: Type[ClientRequest] = ClientRequest,\n        response_class: Type[ClientResponse] = ClientResponse,\n        ws_response_class: Type[ClientWebSocketResponse] = ClientWebSocketResponse,\n        version: HttpVersion = http.HttpVersion11,\n        cookie_jar: Optional[AbstractCookieJar] = None,\n        connector_owner: bool = True,\n        raise_for_status: Union[\n            bool, Callable[[ClientResponse], Awaitable[None]]\n        ] = False,\n        timeout: Union[_SENTINEL, ClientTimeout, None] = sentinel,\n        auto_decompress: bool = True,\n        trust_env: bool = False,\n        requote_redirect_url: bool = True,\n        trace_configs: Optional[List[TraceConfig]] = None,\n        read_bufsize: int = 2**16,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n        fallback_charset_resolver: _CharsetResolver = lambda r, b: \"utf-8\",\n    ) -> None:\n        # We initialise _connector to None immediately, as it's referenced in __del__()\n        # and could cause issues if an exception occurs during initialisation.\n        self._connector: Optional[BaseConnector] = None\n        if base_url is None or isinstance(base_url, URL):\n            self._base_url: Optional[URL] = base_url\n        else:\n            self._base_url = URL(base_url)\n            assert (\n                self._base_url.origin() == self._base_url\n            ), \"Only absolute URLs without path part are supported\"\n\n        loop = asyncio.get_running_loop()\n\n        if timeout is sentinel or timeout is None:\n            timeout = DEFAULT_TIMEOUT\n        if not isinstance(timeout, ClientTimeout):\n            raise ValueError(\n                f\"timeout parameter cannot be of {type(timeout)} type, \"\n                \"please use 'timeout=ClientTimeout(...)'\",\n            )\n        self._timeout = timeout\n\n        if connector is None:\n            connector = TCPConnector()\n        # Initialize these three attrs before raising any exception,\n        # they are used in __del__\n        self._connector = connector\n        self._loop = loop\n        if loop.get_debug():\n            self._source_traceback: Optional[traceback.StackSummary] = (\n                traceback.extract_stack(sys._getframe(1))\n            )\n        else:\n            self._source_traceback = None\n\n        if connector._loop is not loop:\n            raise RuntimeError(\"Session and connector have to use same event loop\")\n\n        if cookie_jar is None:\n            cookie_jar = CookieJar()\n        self._cookie_jar = cookie_jar\n\n        if cookies is not None:\n            self._cookie_jar.update_cookies(cookies)\n\n        self._connector_owner = connector_owner\n        self._default_auth = auth\n        self._version = version\n        self._json_serialize = json_serialize\n        self._raise_for_status = raise_for_status\n        self._auto_decompress = auto_decompress\n        self._trust_env = trust_env\n        self._requote_redirect_url = requote_redirect_url\n        self._read_bufsize = read_bufsize\n        self._max_line_size = max_line_size\n        self._max_field_size = max_field_size\n\n        # Convert to list of tuples\n        if headers:\n            real_headers: CIMultiDict[str] = CIMultiDict(headers)\n        else:\n            real_headers = CIMultiDict()\n        self._default_headers: CIMultiDict[str] = real_headers\n        if skip_auto_headers is not None:\n            self._skip_auto_headers = frozenset(istr(i) for i in skip_auto_headers)\n        else:\n            self._skip_auto_headers = frozenset()\n\n        self._request_class = request_class\n        self._response_class = response_class\n        self._ws_response_class = ws_response_class\n\n        self._trace_configs = trace_configs or []\n        for trace_config in self._trace_configs:\n            trace_config.freeze()\n\n        self._resolve_charset = fallback_charset_resolver\n\n    def __init_subclass__(cls: Type[\"ClientSession\"]) -> None:\n        raise TypeError(\n            \"Inheritance class {} from ClientSession \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if not self.closed:\n            _warnings.warn(\n                f\"Unclosed client session {self!r}\",\n                ResourceWarning,\n                source=self,\n            )\n            context = {\"client_session\": self, \"message\": \"Unclosed client session\"}\n            if self._source_traceback is not None:\n                context[\"source_traceback\"] = self._source_traceback\n            self._loop.call_exception_handler(context)\n\n    def request(\n        self, method: str, url: StrOrURL, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP request.\"\"\"\n        return _RequestContextManager(self._request(method, url, **kwargs))\n\n    def _build_url(self, str_or_url: StrOrURL) -> URL:\n        url = URL(str_or_url)\n        if self._base_url is None:\n            return url\n        else:\n            assert not url.is_absolute() and url.path.startswith(\"/\")\n            return self._base_url.join(url)\n\n    async def _request(\n        self,\n        method: str,\n        str_or_url: StrOrURL,\n        *,\n        params: Optional[Mapping[str, str]] = None,\n        data: Any = None,\n        json: Any = None,\n        cookies: Optional[LooseCookies] = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Optional[Iterable[str]] = None,\n        auth: Optional[BasicAuth] = None,\n        allow_redirects: bool = True,\n        max_redirects: int = 10,\n        compress: Optional[str] = None,\n        chunked: Optional[bool] = None,\n        expect100: bool = False,\n        raise_for_status: Union[\n            None, bool, Callable[[ClientResponse], Awaitable[None]]\n        ] = None,\n        read_until_eof: bool = True,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        timeout: Union[ClientTimeout, _SENTINEL, None] = sentinel,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        server_hostname: Optional[str] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        trace_request_ctx: Optional[SimpleNamespace] = None,\n        read_bufsize: Optional[int] = None,\n        auto_decompress: Optional[bool] = None,\n        max_line_size: Optional[int] = None,\n        max_field_size: Optional[int] = None,\n    ) -> ClientResponse:\n        # NOTE: timeout clamps existing connect and read timeouts.  We cannot\n        # set the default to None because we need to detect if the user wants\n        # to use the existing timeouts by setting timeout to None.\n\n        if self.closed:\n            raise RuntimeError(\"Session is closed\")\n\n        if not isinstance(ssl, SSL_ALLOWED_TYPES):\n            raise TypeError(\n                \"ssl should be SSLContext, Fingerprint, or bool, \"\n                \"got {!r} instead.\".format(ssl)\n            )\n\n        if data is not None and json is not None:\n            raise ValueError(\n                \"data and json parameters can not be used at the same time\"\n            )\n        elif json is not None:\n            data = payload.JsonPayload(json, dumps=self._json_serialize)\n\n        redirects = 0\n        history = []\n        version = self._version\n        params = params or {}\n\n        # Merge with default headers and transform to CIMultiDict\n        headers = self._prepare_headers(headers)\n        proxy_headers = self._prepare_headers(proxy_headers)\n\n        try:\n            url = self._build_url(str_or_url)\n        except ValueError as e:\n            raise InvalidUrlClientError(str_or_url) from e\n\n        if url.scheme not in HTTP_SCHEMA_SET:\n            raise NonHttpUrlClientError(url)\n\n        skip_headers = set(self._skip_auto_headers)\n        if skip_auto_headers is not None:\n            for i in skip_auto_headers:\n                skip_headers.add(istr(i))\n\n        if proxy is not None:\n            try:\n                proxy = URL(proxy)\n            except ValueError as e:\n                raise InvalidURL(proxy) from e\n\n        if timeout is sentinel or timeout is None:\n            real_timeout: ClientTimeout = self._timeout\n        else:\n            real_timeout = timeout\n        # timeout is cumulative for all request operations\n        # (request, redirects, responses, data consuming)\n        tm = TimeoutHandle(\n            self._loop, real_timeout.total, ceil_threshold=real_timeout.ceil_threshold\n        )\n        handle = tm.start()\n\n        if read_bufsize is None:\n            read_bufsize = self._read_bufsize\n\n        if auto_decompress is None:\n            auto_decompress = self._auto_decompress\n\n        if max_line_size is None:\n            max_line_size = self._max_line_size\n\n        if max_field_size is None:\n            max_field_size = self._max_field_size\n\n        traces = [\n            Trace(\n                self,\n                trace_config,\n                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),\n            )\n            for trace_config in self._trace_configs\n        ]\n\n        for trace in traces:\n            await trace.send_request_start(method, url.update_query(params), headers)\n\n        timer = tm.timer()\n        try:\n            with timer:\n                # https://www.rfc-editor.org/rfc/rfc9112.html#name-retrying-requests\n                retry_persistent_connection = method in IDEMPOTENT_METHODS\n                while True:\n                    url, auth_from_url = strip_auth_from_url(url)\n                    if not url.raw_host:\n                        # NOTE: Bail early, otherwise, causes `InvalidURL` through\n                        # NOTE: `self._request_class()` below.\n                        err_exc_cls = (\n                            InvalidUrlRedirectClientError\n                            if redirects\n                            else InvalidUrlClientError\n                        )\n                        raise err_exc_cls(url)\n                    if auth and auth_from_url:\n                        raise ValueError(\n                            \"Cannot combine AUTH argument with \"\n                            \"credentials encoded in URL\"\n                        )\n\n                    if auth is None:\n                        auth = auth_from_url\n                    if auth is None:\n                        auth = self._default_auth\n                    # It would be confusing if we support explicit\n                    # Authorization header with auth argument\n                    if auth is not None and hdrs.AUTHORIZATION in headers:\n                        raise ValueError(\n                            \"Cannot combine AUTHORIZATION header \"\n                            \"with AUTH argument or credentials \"\n                            \"encoded in URL\"\n                        )\n\n                    all_cookies = self._cookie_jar.filter_cookies(url)\n\n                    if cookies is not None:\n                        tmp_cookie_jar = CookieJar()\n                        tmp_cookie_jar.update_cookies(cookies)\n                        req_cookies = tmp_cookie_jar.filter_cookies(url)\n                        if req_cookies:\n                            all_cookies.load(req_cookies)\n\n                    if proxy is not None:\n                        proxy = URL(proxy)\n                    elif self._trust_env:\n                        with suppress(LookupError):\n                            proxy, proxy_auth = get_env_proxy_for_url(url)\n\n                    req = self._request_class(\n                        method,\n                        url,\n                        params=params,\n                        headers=headers,\n                        skip_auto_headers=skip_headers,\n                        data=data,\n                        cookies=all_cookies,\n                        auth=auth,\n                        version=version,\n                        compress=compress,\n                        chunked=chunked,\n                        expect100=expect100,\n                        loop=self._loop,\n                        response_class=self._response_class,\n                        proxy=proxy,\n                        proxy_auth=proxy_auth,\n                        timer=timer,\n                        session=self,\n                        ssl=ssl,\n                        server_hostname=server_hostname,\n                        proxy_headers=proxy_headers,\n                        traces=traces,\n                        trust_env=self.trust_env,\n                    )\n\n                    # connection timeout\n                    try:\n                        async with ceil_timeout(\n                            real_timeout.connect,\n                            ceil_threshold=real_timeout.ceil_threshold,\n                        ):\n                            assert self._connector is not None\n                            conn = await self._connector.connect(\n                                req, traces=traces, timeout=real_timeout\n                            )\n                    except asyncio.TimeoutError as exc:\n                        raise ConnectionTimeoutError(\n                            f\"Connection timeout to host {url}\"\n                        ) from exc\n\n                    assert conn.transport is not None\n\n                    assert conn.protocol is not None\n                    conn.protocol.set_response_params(\n                        timer=timer,\n                        skip_payload=method_must_be_empty_body(method),\n                        read_until_eof=read_until_eof,\n                        auto_decompress=auto_decompress,\n                        read_timeout=real_timeout.sock_read,\n                        read_bufsize=read_bufsize,\n                        timeout_ceil_threshold=self._connector._timeout_ceil_threshold,\n                        max_line_size=max_line_size,\n                        max_field_size=max_field_size,\n                    )\n\n                    try:\n                        try:\n                            resp = await req.send(conn)\n                            try:\n                                await resp.start(conn)\n                            except BaseException:\n                                resp.close()\n                                raise\n                        except BaseException:\n                            conn.close()\n                            raise\n                    except (ClientOSError, ServerDisconnectedError):\n                        if retry_persistent_connection:\n                            retry_persistent_connection = False\n                            continue\n                        raise\n                    except ClientError:\n                        raise\n                    except OSError as exc:\n                        if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                            raise\n                        raise ClientOSError(*exc.args) from exc\n\n                    self._cookie_jar.update_cookies(resp.cookies, resp.url)\n\n                    # redirects\n                    if resp.status in (301, 302, 303, 307, 308) and allow_redirects:\n                        for trace in traces:\n                            await trace.send_request_redirect(\n                                method, url.update_query(params), headers, resp\n                            )\n\n                        redirects += 1\n                        history.append(resp)\n                        if max_redirects and redirects >= max_redirects:\n                            resp.close()\n                            raise TooManyRedirects(\n                                history[0].request_info, tuple(history)\n                            )\n\n                        # For 301 and 302, mimic IE, now changed in RFC\n                        # https://github.com/kennethreitz/requests/pull/269\n                        if (resp.status == 303 and resp.method != hdrs.METH_HEAD) or (\n                            resp.status in (301, 302) and resp.method == hdrs.METH_POST\n                        ):\n                            method = hdrs.METH_GET\n                            data = None\n                            if headers.get(hdrs.CONTENT_LENGTH):\n                                headers.pop(hdrs.CONTENT_LENGTH)\n\n                        r_url = resp.headers.get(hdrs.LOCATION) or resp.headers.get(\n                            hdrs.URI\n                        )\n                        if r_url is None:\n                            # see github.com/aio-libs/aiohttp/issues/2022\n                            break\n                        else:\n                            # reading from correct redirection\n                            # response is forbidden\n                            resp.release()\n\n                        try:\n                            parsed_redirect_url = URL(\n                                r_url, encoded=not self._requote_redirect_url\n                            )\n                        except ValueError as e:\n                            raise InvalidUrlRedirectClientError(\n                                r_url,\n                                \"Server attempted redirecting to a location that does not look like a URL\",\n                            ) from e\n\n                        scheme = parsed_redirect_url.scheme\n                        if scheme not in HTTP_SCHEMA_SET:\n                            resp.close()\n                            raise NonHttpUrlRedirectClientError(r_url)\n                        elif not scheme:\n                            parsed_redirect_url = url.join(parsed_redirect_url)\n\n                        is_same_host_https_redirect = (\n                            url.host == parsed_redirect_url.host\n                            and parsed_redirect_url.scheme == \"https\"\n                            and url.scheme == \"http\"\n                        )\n\n                        try:\n                            redirect_origin = parsed_redirect_url.origin()\n                        except ValueError as origin_val_err:\n                            raise InvalidUrlRedirectClientError(\n                                parsed_redirect_url,\n                                \"Invalid redirect URL origin\",\n                            ) from origin_val_err\n\n                        if (\n                            url.origin() != redirect_origin\n                            and not is_same_host_https_redirect\n                        ):\n                            auth = None\n                            headers.pop(hdrs.AUTHORIZATION, None)\n\n                        url = parsed_redirect_url\n                        params = {}\n                        resp.release()\n                        continue\n\n                    break\n\n            # check response status\n            if raise_for_status is None:\n                raise_for_status = self._raise_for_status\n\n            if raise_for_status is None:\n                pass\n            elif callable(raise_for_status):\n                await raise_for_status(resp)\n            elif raise_for_status:\n                resp.raise_for_status()\n\n            # register connection\n            if handle is not None:\n                if resp.connection is not None:\n                    resp.connection.add_callback(handle.cancel)\n                else:\n                    handle.cancel()\n\n            resp._history = tuple(history)\n\n            for trace in traces:\n                await trace.send_request_end(\n                    method, url.update_query(params), headers, resp\n                )\n            return resp\n\n        except BaseException as e:\n            # cleanup timer\n            tm.close()\n            if handle:\n                handle.cancel()\n                handle = None\n\n            for trace in traces:\n                await trace.send_request_exception(\n                    method, url.update_query(params), headers, e\n                )\n            raise\n\n    def ws_connect(\n        self,\n        url: StrOrURL,\n        *,\n        method: str = hdrs.METH_GET,\n        protocols: Collection[str] = (),\n        timeout: Union[ClientWSTimeout, float, _SENTINEL, None] = sentinel,\n        receive_timeout: Optional[float] = None,\n        autoclose: bool = True,\n        autoping: bool = True,\n        heartbeat: Optional[float] = None,\n        auth: Optional[BasicAuth] = None,\n        origin: Optional[str] = None,\n        params: Optional[Mapping[str, str]] = None,\n        headers: Optional[LooseHeaders] = None,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        server_hostname: Optional[str] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        compress: int = 0,\n        max_msg_size: int = 4 * 1024 * 1024,\n    ) -> \"_WSRequestContextManager\":\n        \"\"\"Initiate websocket connection.\"\"\"\n        return _WSRequestContextManager(\n            self._ws_connect(\n                url,\n                method=method,\n                protocols=protocols,\n                timeout=timeout,\n                receive_timeout=receive_timeout,\n                autoclose=autoclose,\n                autoping=autoping,\n                heartbeat=heartbeat,\n                auth=auth,\n                origin=origin,\n                params=params,\n                headers=headers,\n                proxy=proxy,\n                proxy_auth=proxy_auth,\n                ssl=ssl,\n                server_hostname=server_hostname,\n                proxy_headers=proxy_headers,\n                compress=compress,\n                max_msg_size=max_msg_size,\n            )\n        )\n\n    async def _ws_connect(\n        self,\n        url: StrOrURL,\n        *,\n        method: str = hdrs.METH_GET,\n        protocols: Collection[str] = (),\n        timeout: Union[ClientWSTimeout, float, _SENTINEL, None] = sentinel,\n        receive_timeout: Optional[float] = None,\n        autoclose: bool = True,\n        autoping: bool = True,\n        heartbeat: Optional[float] = None,\n        auth: Optional[BasicAuth] = None,\n        origin: Optional[str] = None,\n        params: Optional[Mapping[str, str]] = None,\n        headers: Optional[LooseHeaders] = None,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        server_hostname: Optional[str] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        compress: int = 0,\n        max_msg_size: int = 4 * 1024 * 1024,\n    ) -> ClientWebSocketResponse:\n        if timeout is sentinel or timeout is None:\n            ws_timeout = DEFAULT_WS_CLIENT_TIMEOUT\n        else:\n            if isinstance(timeout, ClientWSTimeout):\n                ws_timeout = timeout\n            else:\n                warnings.warn(\n                    \"parameter 'timeout' of type 'float' \"\n                    \"is deprecated, please use \"\n                    \"'timeout=ClientWSTimeout(ws_close=...)'\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                ws_timeout = ClientWSTimeout(ws_close=timeout)\n\n        if receive_timeout is not None:\n            warnings.warn(\n                \"float parameter 'receive_timeout' \"\n                \"is deprecated, please use parameter \"\n                \"'timeout=ClientWSTimeout(ws_receive=...)'\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            ws_timeout = dataclasses.replace(ws_timeout, ws_receive=receive_timeout)\n\n        if headers is None:\n            real_headers: CIMultiDict[str] = CIMultiDict()\n        else:\n            real_headers = CIMultiDict(headers)\n\n        default_headers = {\n            hdrs.UPGRADE: \"websocket\",\n            hdrs.CONNECTION: \"Upgrade\",\n            hdrs.SEC_WEBSOCKET_VERSION: \"13\",\n        }\n\n        for key, value in default_headers.items():\n            real_headers.setdefault(key, value)\n\n        sec_key = base64.b64encode(os.urandom(16))\n        real_headers[hdrs.SEC_WEBSOCKET_KEY] = sec_key.decode()\n\n        if protocols:\n            real_headers[hdrs.SEC_WEBSOCKET_PROTOCOL] = \",\".join(protocols)\n        if origin is not None:\n            real_headers[hdrs.ORIGIN] = origin\n        if compress:\n            extstr = ws_ext_gen(compress=compress)\n            real_headers[hdrs.SEC_WEBSOCKET_EXTENSIONS] = extstr\n\n        if not isinstance(ssl, SSL_ALLOWED_TYPES):\n            raise TypeError(\n                \"ssl should be SSLContext, Fingerprint, or bool, \"\n                \"got {!r} instead.\".format(ssl)\n            )\n\n        # send request\n        resp = await self.request(\n            method,\n            url,\n            params=params,\n            headers=real_headers,\n            read_until_eof=False,\n            auth=auth,\n            proxy=proxy,\n            proxy_auth=proxy_auth,\n            ssl=ssl,\n            server_hostname=server_hostname,\n            proxy_headers=proxy_headers,\n        )\n\n        try:\n            # check handshake\n            if resp.status != 101:\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid response status\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            if resp.headers.get(hdrs.UPGRADE, \"\").lower() != \"websocket\":\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid upgrade header\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            if resp.headers.get(hdrs.CONNECTION, \"\").lower() != \"upgrade\":\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid connection header\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            # key calculation\n            r_key = resp.headers.get(hdrs.SEC_WEBSOCKET_ACCEPT, \"\")\n            match = base64.b64encode(hashlib.sha1(sec_key + WS_KEY).digest()).decode()\n            if r_key != match:\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid challenge response\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            # websocket protocol\n            protocol = None\n            if protocols and hdrs.SEC_WEBSOCKET_PROTOCOL in resp.headers:\n                resp_protocols = [\n                    proto.strip()\n                    for proto in resp.headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(\",\")\n                ]\n\n                for proto in resp_protocols:\n                    if proto in protocols:\n                        protocol = proto\n                        break\n\n            # websocket compress\n            notakeover = False\n            if compress:\n                compress_hdrs = resp.headers.get(hdrs.SEC_WEBSOCKET_EXTENSIONS)\n                if compress_hdrs:\n                    try:\n                        compress, notakeover = ws_ext_parse(compress_hdrs)\n                    except WSHandshakeError as exc:\n                        raise WSServerHandshakeError(\n                            resp.request_info,\n                            resp.history,\n                            message=exc.args[0],\n                            status=resp.status,\n                            headers=resp.headers,\n                        ) from exc\n                else:\n                    compress = 0\n                    notakeover = False\n\n            conn = resp.connection\n            assert conn is not None\n            conn_proto = conn.protocol\n            assert conn_proto is not None\n            transport = conn.transport\n            assert transport is not None\n            reader: FlowControlDataQueue[WSMessage] = FlowControlDataQueue(\n                conn_proto, 2**16, loop=self._loop\n            )\n            conn_proto.set_parser(WebSocketReader(reader, max_msg_size), reader)\n            writer = WebSocketWriter(\n                conn_proto,\n                transport,\n                use_mask=True,\n                compress=compress,\n                notakeover=notakeover,\n            )\n        except BaseException:\n            resp.close()\n            raise\n        else:\n            return self._ws_response_class(\n                reader,\n                writer,\n                protocol,\n                resp,\n                ws_timeout,\n                autoclose,\n                autoping,\n                self._loop,\n                heartbeat=heartbeat,\n                compress=compress,\n                client_notakeover=notakeover,\n            )\n\n    def _prepare_headers(self, headers: Optional[LooseHeaders]) -> \"CIMultiDict[str]\":\n        \"\"\"Add default headers and transform it to CIMultiDict\"\"\"\n        # Convert headers to MultiDict\n        result = CIMultiDict(self._default_headers)\n        if headers:\n            if not isinstance(headers, (MultiDictProxy, MultiDict)):\n                headers = CIMultiDict(headers)\n            added_names: Set[str] = set()\n            for key, value in headers.items():\n                if key in added_names:\n                    result.add(key, value)\n                else:\n                    result[key] = value\n                    added_names.add(key)\n        return result\n\n    def get(\n        self, url: StrOrURL, *, allow_redirects: bool = True, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP GET request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_GET, url, allow_redirects=allow_redirects, **kwargs)\n        )\n\n    def options(\n        self, url: StrOrURL, *, allow_redirects: bool = True, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP OPTIONS request.\"\"\"\n        return _RequestContextManager(\n            self._request(\n                hdrs.METH_OPTIONS, url, allow_redirects=allow_redirects, **kwargs\n            )\n        )\n\n    def head(\n        self, url: StrOrURL, *, allow_redirects: bool = False, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP HEAD request.\"\"\"\n        return _RequestContextManager(\n            self._request(\n                hdrs.METH_HEAD, url, allow_redirects=allow_redirects, **kwargs\n            )\n        )\n\n    def post(\n        self, url: StrOrURL, *, data: Any = None, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP POST request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_POST, url, data=data, **kwargs)\n        )\n\n    def put(\n        self, url: StrOrURL, *, data: Any = None, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP PUT request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_PUT, url, data=data, **kwargs)\n        )\n\n    def patch(\n        self, url: StrOrURL, *, data: Any = None, **kwargs: Any\n    ) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP PATCH request.\"\"\"\n        return _RequestContextManager(\n            self._request(hdrs.METH_PATCH, url, data=data, **kwargs)\n        )\n\n    def delete(self, url: StrOrURL, **kwargs: Any) -> \"_RequestContextManager\":\n        \"\"\"Perform HTTP DELETE request.\"\"\"\n        return _RequestContextManager(self._request(hdrs.METH_DELETE, url, **kwargs))\n\n    async def close(self) -> None:\n        \"\"\"Close underlying connector.\n\n        Release all acquired resources.\n        \"\"\"\n        if not self.closed:\n            if self._connector is not None and self._connector_owner:\n                await self._connector.close()\n            self._connector = None\n\n    @property\n    def closed(self) -> bool:\n        \"\"\"Is client session closed.\n\n        A readonly property.\n        \"\"\"\n        return self._connector is None or self._connector.closed\n\n    @property\n    def connector(self) -> Optional[BaseConnector]:\n        \"\"\"Connector instance used for the session.\"\"\"\n        return self._connector\n\n    @property\n    def cookie_jar(self) -> AbstractCookieJar:\n        \"\"\"The session cookies.\"\"\"\n        return self._cookie_jar\n\n    @property\n    def version(self) -> Tuple[int, int]:\n        \"\"\"The session HTTP protocol version.\"\"\"\n        return self._version\n\n    @property\n    def requote_redirect_url(self) -> bool:\n        \"\"\"Do URL requoting on redirection handling.\"\"\"\n        return self._requote_redirect_url\n\n    @property\n    def timeout(self) -> ClientTimeout:\n        \"\"\"Timeout for the session.\"\"\"\n        return self._timeout\n\n    @property\n    def headers(self) -> \"CIMultiDict[str]\":\n        \"\"\"The default headers of the client session.\"\"\"\n        return self._default_headers\n\n    @property\n    def skip_auto_headers(self) -> FrozenSet[istr]:\n        \"\"\"Headers for which autogeneration should be skipped\"\"\"\n        return self._skip_auto_headers\n\n    @property\n    def auth(self) -> Optional[BasicAuth]:\n        \"\"\"An object that represents HTTP Basic Authorization\"\"\"\n        return self._default_auth\n\n    @property\n    def json_serialize(self) -> JSONEncoder:\n        \"\"\"Json serializer callable\"\"\"\n        return self._json_serialize\n\n    @property\n    def connector_owner(self) -> bool:\n        \"\"\"Should connector be closed on session closing\"\"\"\n        return self._connector_owner\n\n    @property\n    def raise_for_status(\n        self,\n    ) -> Union[bool, Callable[[ClientResponse], Awaitable[None]]]:\n        \"\"\"Should `ClientResponse.raise_for_status()` be called for each response.\"\"\"\n        return self._raise_for_status\n\n    @property\n    def auto_decompress(self) -> bool:\n        \"\"\"Should the body response be automatically decompressed.\"\"\"\n        return self._auto_decompress\n\n    @property\n    def trust_env(self) -> bool:\n        \"\"\"\n        Should proxies information from environment or netrc be trusted.\n\n        Information is from HTTP_PROXY / HTTPS_PROXY environment variables\n        or ~/.netrc file if present.\n        \"\"\"\n        return self._trust_env\n\n    @property\n    def trace_configs(self) -> List[TraceConfig]:\n        \"\"\"A list of TraceConfig instances used for client tracing\"\"\"\n        return self._trace_configs\n\n    def detach(self) -> None:\n        \"\"\"Detach connector from session without closing the former.\n\n        Session is switched to closed state anyway.\n        \"\"\"\n        self._connector = None\n\n    async def __aenter__(self) -> \"ClientSession\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        await self.close()\n\n\nclass _BaseRequestContextManager(Coroutine[Any, Any, _RetType], Generic[_RetType]):\n    __slots__ = (\"_coro\", \"_resp\")\n\n    def __init__(self, coro: Coroutine[\"asyncio.Future[Any]\", None, _RetType]) -> None:\n        self._coro = coro\n\n    def send(self, arg: None) -> \"asyncio.Future[Any]\":\n        return self._coro.send(arg)\n\n    def throw(self, *args: Any, **kwargs: Any) -> \"asyncio.Future[Any]\":\n        return self._coro.throw(*args, **kwargs)\n\n    def close(self) -> None:\n        return self._coro.close()\n\n    def __await__(self) -> Generator[Any, None, _RetType]:\n        ret = self._coro.__await__()\n        return ret\n\n    def __iter__(self) -> Generator[Any, None, _RetType]:\n        return self.__await__()\n\n    async def __aenter__(self) -> _RetType:\n        self._resp = await self._coro\n        return self._resp\n\n\nclass _RequestContextManager(_BaseRequestContextManager[ClientResponse]):\n    __slots__ = ()\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        # We're basing behavior on the exception as it can be caused by\n        # user code unrelated to the status of the connection.  If you\n        # would like to close a connection you must do that\n        # explicitly.  Otherwise connection error handling should kick in\n        # and close/recycle the connection as required.\n        self._resp.release()\n        await self._resp.wait_for_close()\n\n\nclass _WSRequestContextManager(_BaseRequestContextManager[ClientWebSocketResponse]):\n    __slots__ = ()\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        await self._resp.close()\n\n\nclass _SessionRequestContextManager:\n    __slots__ = (\"_coro\", \"_resp\", \"_session\")\n\n    def __init__(\n        self,\n        coro: Coroutine[\"asyncio.Future[Any]\", None, ClientResponse],\n        session: ClientSession,\n    ) -> None:\n        self._coro = coro\n        self._resp: Optional[ClientResponse] = None\n        self._session = session\n\n    async def __aenter__(self) -> ClientResponse:\n        try:\n            self._resp = await self._coro\n        except BaseException:\n            await self._session.close()\n            raise\n        else:\n            return self._resp\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        assert self._resp is not None\n        self._resp.close()\n        await self._session.close()\n\n\ndef request(\n    method: str,\n    url: StrOrURL,\n    *,\n    params: Optional[Mapping[str, str]] = None,\n    data: Any = None,\n    json: Any = None,\n    headers: Optional[LooseHeaders] = None,\n    skip_auto_headers: Optional[Iterable[str]] = None,\n    auth: Optional[BasicAuth] = None,\n    allow_redirects: bool = True,\n    max_redirects: int = 10,\n    compress: Optional[str] = None,\n    chunked: Optional[bool] = None,\n    expect100: bool = False,\n    raise_for_status: Optional[bool] = None,\n    read_until_eof: bool = True,\n    proxy: Optional[StrOrURL] = None,\n    proxy_auth: Optional[BasicAuth] = None,\n    timeout: Union[ClientTimeout, _SENTINEL] = sentinel,\n    cookies: Optional[LooseCookies] = None,\n    version: HttpVersion = http.HttpVersion11,\n    connector: Optional[BaseConnector] = None,\n    read_bufsize: Optional[int] = None,\n    max_line_size: int = 8190,\n    max_field_size: int = 8190,\n) -> _SessionRequestContextManager:\n    \"\"\"Constructs and sends a request.\n\n    Returns response object.\n    method - HTTP method\n    url - request url\n    params - (optional) Dictionary or bytes to be sent in the query\n      string of the new request\n    data - (optional) Dictionary, bytes, or file-like object to\n      send in the body of the request\n    json - (optional) Any json compatible python object\n    headers - (optional) Dictionary of HTTP Headers to send with\n      the request\n    cookies - (optional) Dict object to send with the request\n    auth - (optional) BasicAuth named tuple represent HTTP Basic Auth\n    auth - aiohttp.helpers.BasicAuth\n    allow_redirects - (optional) If set to False, do not follow\n      redirects\n    version - Request HTTP version.\n    compress - Set to True if request has to be compressed\n       with deflate encoding.\n    chunked - Set to chunk size for chunked transfer encoding.\n    expect100 - Expect 100-continue response from server.\n    connector - BaseConnector sub-class instance to support\n       connection pooling.\n    read_until_eof - Read response until eof if response\n       does not have Content-Length header.\n    loop - Optional event loop.\n    timeout - Optional ClientTimeout settings structure, 5min\n       total timeout by default.\n    Usage::\n      >>> import aiohttp\n      >>> async with aiohttp.request('GET', 'http://python.org/') as resp:\n      ...    print(resp)\n      ...    data = await resp.read()\n      <ClientResponse(https://www.python.org/) [200 OK]>\n    \"\"\"\n    connector_owner = False\n    if connector is None:\n        connector_owner = True\n        connector = TCPConnector(force_close=True)\n\n    session = ClientSession(\n        cookies=cookies,\n        version=version,\n        timeout=timeout,\n        connector=connector,\n        connector_owner=connector_owner,\n    )\n\n    return _SessionRequestContextManager(\n        session._request(\n            method,\n            url,\n            params=params,\n            data=data,\n            json=json,\n            headers=headers,\n            skip_auto_headers=skip_auto_headers,\n            auth=auth,\n            allow_redirects=allow_redirects,\n            max_redirects=max_redirects,\n            compress=compress,\n            chunked=chunked,\n            expect100=expect100,\n            raise_for_status=raise_for_status,\n            read_until_eof=read_until_eof,\n            proxy=proxy,\n            proxy_auth=proxy_auth,\n            read_bufsize=read_bufsize,\n            max_line_size=max_line_size,\n            max_field_size=max_field_size,\n        ),\n        session,\n    )\n", "aiohttp/client_exceptions.py": "\"\"\"HTTP related errors.\"\"\"\n\nimport asyncio\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n\nfrom .http_parser import RawResponseMessage\nfrom .typedefs import LooseHeaders, StrOrURL\n\ntry:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nexcept ImportError:  # pragma: no cover\n    ssl = SSLContext = None  # type: ignore[assignment]\n\n\nif TYPE_CHECKING:\n    from .client_reqrep import ClientResponse, ConnectionKey, Fingerprint, RequestInfo\nelse:\n    RequestInfo = ClientResponse = ConnectionKey = None\n\n__all__ = (\n    \"ClientError\",\n    \"ClientConnectionError\",\n    \"ClientOSError\",\n    \"ClientConnectorError\",\n    \"ClientProxyConnectionError\",\n    \"ClientSSLError\",\n    \"ClientConnectorSSLError\",\n    \"ClientConnectorCertificateError\",\n    \"ConnectionTimeoutError\",\n    \"SocketTimeoutError\",\n    \"ServerConnectionError\",\n    \"ServerTimeoutError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ClientResponseError\",\n    \"ClientHttpProxyError\",\n    \"WSServerHandshakeError\",\n    \"ContentTypeError\",\n    \"ClientPayloadError\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"RedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlRedirectClientError\",\n)\n\n\nclass ClientError(Exception):\n    \"\"\"Base class for client connection errors.\"\"\"\n\n\nclass ClientResponseError(ClientError):\n    \"\"\"Base class for exceptions that occur after getting a response.\n\n    request_info: An instance of RequestInfo.\n    history: A sequence of responses, if redirects occurred.\n    status: HTTP status code.\n    message: Error message.\n    headers: Response headers.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_info: RequestInfo,\n        history: Tuple[ClientResponse, ...],\n        *,\n        status: Optional[int] = None,\n        message: str = \"\",\n        headers: Optional[LooseHeaders] = None,\n    ) -> None:\n        self.request_info = request_info\n        if status is not None:\n            self.status = status\n        else:\n            self.status = 0\n        self.message = message\n        self.headers = headers\n        self.history = history\n        self.args = (request_info, history)\n\n    def __str__(self) -> str:\n        return \"{}, message={!r}, url={!r}\".format(\n            self.status,\n            self.message,\n            self.request_info.real_url,\n        )\n\n    def __repr__(self) -> str:\n        args = f\"{self.request_info!r}, {self.history!r}\"\n        if self.status != 0:\n            args += f\", status={self.status!r}\"\n        if self.message != \"\":\n            args += f\", message={self.message!r}\"\n        if self.headers is not None:\n            args += f\", headers={self.headers!r}\"\n        return f\"{type(self).__name__}({args})\"\n\n\nclass ContentTypeError(ClientResponseError):\n    \"\"\"ContentType found is not valid.\"\"\"\n\n\nclass WSServerHandshakeError(ClientResponseError):\n    \"\"\"websocket server handshake error.\"\"\"\n\n\nclass ClientHttpProxyError(ClientResponseError):\n    \"\"\"HTTP proxy error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n    proxy responds with status other than ``200 OK``\n    on ``CONNECT`` request.\n    \"\"\"\n\n\nclass TooManyRedirects(ClientResponseError):\n    \"\"\"Client was redirected too many times.\"\"\"\n\n\nclass ClientConnectionError(ClientError):\n    \"\"\"Base class for client socket errors.\"\"\"\n\n\nclass ClientOSError(ClientConnectionError, OSError):\n    \"\"\"OSError error.\"\"\"\n\n\nclass ClientConnectorError(ClientOSError):\n    \"\"\"Client connector error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        a connection can not be established.\n    \"\"\"\n\n    def __init__(self, connection_key: ConnectionKey, os_error: OSError) -> None:\n        self._conn_key = connection_key\n        self._os_error = os_error\n        super().__init__(os_error.errno, os_error.strerror)\n        self.args = (connection_key, os_error)\n\n    @property\n    def os_error(self) -> OSError:\n        return self._os_error\n\n    @property\n    def host(self) -> str:\n        return self._conn_key.host\n\n    @property\n    def port(self) -> Optional[int]:\n        return self._conn_key.port\n\n    @property\n    def ssl(self) -> Union[SSLContext, bool, \"Fingerprint\"]:\n        return self._conn_key.ssl\n\n    def __str__(self) -> str:\n        return \"Cannot connect to host {0.host}:{0.port} ssl:{1} [{2}]\".format(\n            self, \"default\" if self.ssl is True else self.ssl, self.strerror\n        )\n\n    # OSError.__reduce__ does too much black magick\n    __reduce__ = BaseException.__reduce__\n\n\nclass ClientProxyConnectionError(ClientConnectorError):\n    \"\"\"Proxy connection error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        connection to proxy can not be established.\n    \"\"\"\n\n\nclass UnixClientConnectorError(ClientConnectorError):\n    \"\"\"Unix connector error.\n\n    Raised in :py:class:`aiohttp.connector.UnixConnector`\n    if connection to unix socket can not be established.\n    \"\"\"\n\n    def __init__(\n        self, path: str, connection_key: ConnectionKey, os_error: OSError\n    ) -> None:\n        self._path = path\n        super().__init__(connection_key, os_error)\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def __str__(self) -> str:\n        return \"Cannot connect to unix socket {0.path} ssl:{1} [{2}]\".format(\n            self, \"default\" if self.ssl is True else self.ssl, self.strerror\n        )\n\n\nclass ServerConnectionError(ClientConnectionError):\n    \"\"\"Server connection errors.\"\"\"\n\n\nclass ServerDisconnectedError(ServerConnectionError):\n    \"\"\"Server disconnected.\"\"\"\n\n    def __init__(self, message: Union[RawResponseMessage, str, None] = None) -> None:\n        if message is None:\n            message = \"Server disconnected\"\n\n        self.args = (message,)\n        self.message = message\n\n\nclass ServerTimeoutError(ServerConnectionError, asyncio.TimeoutError):\n    \"\"\"Server timeout error.\"\"\"\n\n\nclass ConnectionTimeoutError(ServerTimeoutError):\n    \"\"\"Connection timeout error.\"\"\"\n\n\nclass SocketTimeoutError(ServerTimeoutError):\n    \"\"\"Socket timeout error.\"\"\"\n\n\nclass ServerFingerprintMismatch(ServerConnectionError):\n    \"\"\"SSL certificate does not match expected fingerprint.\"\"\"\n\n    def __init__(self, expected: bytes, got: bytes, host: str, port: int) -> None:\n        self.expected = expected\n        self.got = got\n        self.host = host\n        self.port = port\n        self.args = (expected, got, host, port)\n\n    def __repr__(self) -> str:\n        return \"<{} expected={!r} got={!r} host={!r} port={!r}>\".format(\n            self.__class__.__name__, self.expected, self.got, self.host, self.port\n        )\n\n\nclass ClientPayloadError(ClientError):\n    \"\"\"Response payload error.\"\"\"\n\n\nclass InvalidURL(ClientError, ValueError):\n    \"\"\"Invalid URL.\n\n    URL used for fetching is malformed, e.g. it doesn't contains host\n    part.\n    \"\"\"\n\n    # Derive from ValueError for backward compatibility\n\n    def __init__(self, url: StrOrURL, description: Union[str, None] = None) -> None:\n        # The type of url is not yarl.URL because the exception can be raised\n        # on URL(url) call\n        self._url = url\n        self._description = description\n\n        if description:\n            super().__init__(url, description)\n        else:\n            super().__init__(url)\n\n    @property\n    def url(self) -> StrOrURL:\n        return self._url\n\n    @property\n    def description(self) -> \"str | None\":\n        return self._description\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__} {self}>\"\n\n    def __str__(self) -> str:\n        if self._description:\n            return f\"{self._url} - {self._description}\"\n        return str(self._url)\n\n\nclass InvalidUrlClientError(InvalidURL):\n    \"\"\"Invalid URL client error.\"\"\"\n\n\nclass RedirectClientError(ClientError):\n    \"\"\"Client redirect error.\"\"\"\n\n\nclass NonHttpUrlClientError(ClientError):\n    \"\"\"Non http URL client error.\"\"\"\n\n\nclass InvalidUrlRedirectClientError(InvalidUrlClientError, RedirectClientError):\n    \"\"\"Invalid URL redirect client error.\"\"\"\n\n\nclass NonHttpUrlRedirectClientError(NonHttpUrlClientError, RedirectClientError):\n    \"\"\"Non http URL redirect client error.\"\"\"\n\n\nclass ClientSSLError(ClientConnectorError):\n    \"\"\"Base error for ssl.*Errors.\"\"\"\n\n\nif ssl is not None:\n    cert_errors = (ssl.CertificateError,)\n    cert_errors_bases = (\n        ClientSSLError,\n        ssl.CertificateError,\n    )\n\n    ssl_errors = (ssl.SSLError,)\n    ssl_error_bases = (ClientSSLError, ssl.SSLError)\nelse:  # pragma: no cover\n    cert_errors = tuple()\n    cert_errors_bases = (\n        ClientSSLError,\n        ValueError,\n    )\n\n    ssl_errors = tuple()\n    ssl_error_bases = (ClientSSLError,)\n\n\nclass ClientConnectorSSLError(*ssl_error_bases):  # type: ignore[misc]\n    \"\"\"Response ssl error.\"\"\"\n\n\nclass ClientConnectorCertificateError(*cert_errors_bases):  # type: ignore[misc]\n    \"\"\"Response certificate error.\"\"\"\n\n    def __init__(\n        self, connection_key: ConnectionKey, certificate_error: Exception\n    ) -> None:\n        self._conn_key = connection_key\n        self._certificate_error = certificate_error\n        self.args = (connection_key, certificate_error)\n\n    @property\n    def certificate_error(self) -> Exception:\n        return self._certificate_error\n\n    @property\n    def host(self) -> str:\n        return self._conn_key.host\n\n    @property\n    def port(self) -> Optional[int]:\n        return self._conn_key.port\n\n    @property\n    def ssl(self) -> bool:\n        return self._conn_key.is_ssl\n\n    def __str__(self) -> str:\n        return (\n            \"Cannot connect to host {0.host}:{0.port} ssl:{0.ssl} \"\n            \"[{0.certificate_error.__class__.__name__}: \"\n            \"{0.certificate_error.args}]\".format(self)\n        )\n", "aiohttp/hdrs.py": "\"\"\"HTTP Headers constants.\"\"\"\n\n# After changing the file content call ./tools/gen.py\n# to regenerate the headers parser\nfrom typing import Final, Set\n\nfrom multidict import istr\n\nMETH_ANY: Final[str] = \"*\"\nMETH_CONNECT: Final[str] = \"CONNECT\"\nMETH_HEAD: Final[str] = \"HEAD\"\nMETH_GET: Final[str] = \"GET\"\nMETH_DELETE: Final[str] = \"DELETE\"\nMETH_OPTIONS: Final[str] = \"OPTIONS\"\nMETH_PATCH: Final[str] = \"PATCH\"\nMETH_POST: Final[str] = \"POST\"\nMETH_PUT: Final[str] = \"PUT\"\nMETH_TRACE: Final[str] = \"TRACE\"\n\nMETH_ALL: Final[Set[str]] = {\n    METH_CONNECT,\n    METH_HEAD,\n    METH_GET,\n    METH_DELETE,\n    METH_OPTIONS,\n    METH_PATCH,\n    METH_POST,\n    METH_PUT,\n    METH_TRACE,\n}\n\nACCEPT: Final[istr] = istr(\"Accept\")\nACCEPT_CHARSET: Final[istr] = istr(\"Accept-Charset\")\nACCEPT_ENCODING: Final[istr] = istr(\"Accept-Encoding\")\nACCEPT_LANGUAGE: Final[istr] = istr(\"Accept-Language\")\nACCEPT_RANGES: Final[istr] = istr(\"Accept-Ranges\")\nACCESS_CONTROL_MAX_AGE: Final[istr] = istr(\"Access-Control-Max-Age\")\nACCESS_CONTROL_ALLOW_CREDENTIALS: Final[istr] = istr(\"Access-Control-Allow-Credentials\")\nACCESS_CONTROL_ALLOW_HEADERS: Final[istr] = istr(\"Access-Control-Allow-Headers\")\nACCESS_CONTROL_ALLOW_METHODS: Final[istr] = istr(\"Access-Control-Allow-Methods\")\nACCESS_CONTROL_ALLOW_ORIGIN: Final[istr] = istr(\"Access-Control-Allow-Origin\")\nACCESS_CONTROL_EXPOSE_HEADERS: Final[istr] = istr(\"Access-Control-Expose-Headers\")\nACCESS_CONTROL_REQUEST_HEADERS: Final[istr] = istr(\"Access-Control-Request-Headers\")\nACCESS_CONTROL_REQUEST_METHOD: Final[istr] = istr(\"Access-Control-Request-Method\")\nAGE: Final[istr] = istr(\"Age\")\nALLOW: Final[istr] = istr(\"Allow\")\nAUTHORIZATION: Final[istr] = istr(\"Authorization\")\nCACHE_CONTROL: Final[istr] = istr(\"Cache-Control\")\nCONNECTION: Final[istr] = istr(\"Connection\")\nCONTENT_DISPOSITION: Final[istr] = istr(\"Content-Disposition\")\nCONTENT_ENCODING: Final[istr] = istr(\"Content-Encoding\")\nCONTENT_LANGUAGE: Final[istr] = istr(\"Content-Language\")\nCONTENT_LENGTH: Final[istr] = istr(\"Content-Length\")\nCONTENT_LOCATION: Final[istr] = istr(\"Content-Location\")\nCONTENT_MD5: Final[istr] = istr(\"Content-MD5\")\nCONTENT_RANGE: Final[istr] = istr(\"Content-Range\")\nCONTENT_TRANSFER_ENCODING: Final[istr] = istr(\"Content-Transfer-Encoding\")\nCONTENT_TYPE: Final[istr] = istr(\"Content-Type\")\nCOOKIE: Final[istr] = istr(\"Cookie\")\nDATE: Final[istr] = istr(\"Date\")\nDESTINATION: Final[istr] = istr(\"Destination\")\nDIGEST: Final[istr] = istr(\"Digest\")\nETAG: Final[istr] = istr(\"Etag\")\nEXPECT: Final[istr] = istr(\"Expect\")\nEXPIRES: Final[istr] = istr(\"Expires\")\nFORWARDED: Final[istr] = istr(\"Forwarded\")\nFROM: Final[istr] = istr(\"From\")\nHOST: Final[istr] = istr(\"Host\")\nIF_MATCH: Final[istr] = istr(\"If-Match\")\nIF_MODIFIED_SINCE: Final[istr] = istr(\"If-Modified-Since\")\nIF_NONE_MATCH: Final[istr] = istr(\"If-None-Match\")\nIF_RANGE: Final[istr] = istr(\"If-Range\")\nIF_UNMODIFIED_SINCE: Final[istr] = istr(\"If-Unmodified-Since\")\nKEEP_ALIVE: Final[istr] = istr(\"Keep-Alive\")\nLAST_EVENT_ID: Final[istr] = istr(\"Last-Event-ID\")\nLAST_MODIFIED: Final[istr] = istr(\"Last-Modified\")\nLINK: Final[istr] = istr(\"Link\")\nLOCATION: Final[istr] = istr(\"Location\")\nMAX_FORWARDS: Final[istr] = istr(\"Max-Forwards\")\nORIGIN: Final[istr] = istr(\"Origin\")\nPRAGMA: Final[istr] = istr(\"Pragma\")\nPROXY_AUTHENTICATE: Final[istr] = istr(\"Proxy-Authenticate\")\nPROXY_AUTHORIZATION: Final[istr] = istr(\"Proxy-Authorization\")\nRANGE: Final[istr] = istr(\"Range\")\nREFERER: Final[istr] = istr(\"Referer\")\nRETRY_AFTER: Final[istr] = istr(\"Retry-After\")\nSEC_WEBSOCKET_ACCEPT: Final[istr] = istr(\"Sec-WebSocket-Accept\")\nSEC_WEBSOCKET_VERSION: Final[istr] = istr(\"Sec-WebSocket-Version\")\nSEC_WEBSOCKET_PROTOCOL: Final[istr] = istr(\"Sec-WebSocket-Protocol\")\nSEC_WEBSOCKET_EXTENSIONS: Final[istr] = istr(\"Sec-WebSocket-Extensions\")\nSEC_WEBSOCKET_KEY: Final[istr] = istr(\"Sec-WebSocket-Key\")\nSEC_WEBSOCKET_KEY1: Final[istr] = istr(\"Sec-WebSocket-Key1\")\nSERVER: Final[istr] = istr(\"Server\")\nSET_COOKIE: Final[istr] = istr(\"Set-Cookie\")\nTE: Final[istr] = istr(\"TE\")\nTRAILER: Final[istr] = istr(\"Trailer\")\nTRANSFER_ENCODING: Final[istr] = istr(\"Transfer-Encoding\")\nUPGRADE: Final[istr] = istr(\"Upgrade\")\nURI: Final[istr] = istr(\"URI\")\nUSER_AGENT: Final[istr] = istr(\"User-Agent\")\nVARY: Final[istr] = istr(\"Vary\")\nVIA: Final[istr] = istr(\"Via\")\nWANT_DIGEST: Final[istr] = istr(\"Want-Digest\")\nWARNING: Final[istr] = istr(\"Warning\")\nWWW_AUTHENTICATE: Final[istr] = istr(\"WWW-Authenticate\")\nX_FORWARDED_FOR: Final[istr] = istr(\"X-Forwarded-For\")\nX_FORWARDED_HOST: Final[istr] = istr(\"X-Forwarded-Host\")\nX_FORWARDED_PROTO: Final[istr] = istr(\"X-Forwarded-Proto\")\n", "aiohttp/web_middlewares.py": "import re\nimport warnings\nfrom typing import TYPE_CHECKING, Tuple, Type, TypeVar\n\nfrom .typedefs import Handler, Middleware\nfrom .web_exceptions import HTTPMove, HTTPPermanentRedirect\nfrom .web_request import Request\nfrom .web_response import StreamResponse\nfrom .web_urldispatcher import SystemRoute\n\n__all__ = (\n    \"middleware\",\n    \"normalize_path_middleware\",\n)\n\nif TYPE_CHECKING:\n    from .web_app import Application\n\n_Func = TypeVar(\"_Func\")\n\n\nasync def _check_request_resolves(request: Request, path: str) -> Tuple[bool, Request]:\n    alt_request = request.clone(rel_url=path)\n\n    match_info = await request.app.router.resolve(alt_request)\n    alt_request._match_info = match_info\n\n    if match_info.http_exception is None:\n        return True, alt_request\n\n    return False, request\n\n\ndef middleware(f: _Func) -> _Func:\n    warnings.warn(\n        \"Middleware decorator is deprecated since 4.0 \"\n        \"and its behaviour is default, \"\n        \"you can simply remove this decorator.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return f\n\n\ndef normalize_path_middleware(\n    *,\n    append_slash: bool = True,\n    remove_slash: bool = False,\n    merge_slashes: bool = True,\n    redirect_class: Type[HTTPMove] = HTTPPermanentRedirect,\n) -> Middleware:\n    \"\"\"Factory for producing a middleware that normalizes the path of a request.\n\n    Normalizing means:\n        - Add or remove a trailing slash to the path.\n        - Double slashes are replaced by one.\n\n    The middleware returns as soon as it finds a path that resolves\n    correctly. The order if both merge and append/remove are enabled is\n        1) merge slashes\n        2) append/remove slash\n        3) both merge slashes and append/remove slash.\n    If the path resolves with at least one of those conditions, it will\n    redirect to the new path.\n\n    Only one of `append_slash` and `remove_slash` can be enabled. If both\n    are `True` the factory will raise an assertion error\n\n    If `append_slash` is `True` the middleware will append a slash when\n    needed. If a resource is defined with trailing slash and the request\n    comes without it, it will append it automatically.\n\n    If `remove_slash` is `True`, `append_slash` must be `False`. When enabled\n    the middleware will remove trailing slashes and redirect if the resource\n    is defined\n\n    If merge_slashes is True, merge multiple consecutive slashes in the\n    path into one.\n    \"\"\"\n    correct_configuration = not (append_slash and remove_slash)\n    assert correct_configuration, \"Cannot both remove and append slash\"\n\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\n        if isinstance(request.match_info.route, SystemRoute):\n            paths_to_check = []\n            if \"?\" in request.raw_path:\n                path, query = request.raw_path.split(\"?\", 1)\n                query = \"?\" + query\n            else:\n                query = \"\"\n                path = request.raw_path\n\n            if merge_slashes:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path))\n            if append_slash and not request.path.endswith(\"/\"):\n                paths_to_check.append(path + \"/\")\n            if remove_slash and request.path.endswith(\"/\"):\n                paths_to_check.append(path[:-1])\n            if merge_slashes and append_slash:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path + \"/\"))\n            if merge_slashes and remove_slash and path.endswith(\"/\"):\n                merged_slashes = re.sub(\"//+\", \"/\", path)\n                paths_to_check.append(merged_slashes[:-1])\n\n            for path in paths_to_check:\n                path = re.sub(\"^//+\", \"/\", path)  # SECURITY: GHSA-v6wp-4m6f-gcjg\n                resolves, request = await _check_request_resolves(request, path)\n                if resolves:\n                    raise redirect_class(request.raw_path + query)\n\n        return await handler(request)\n\n    return impl\n\n\ndef _fix_request_current_app(app: \"Application\") -> Middleware:\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\n        with request.match_info.set_current_app(app):\n            return await handler(request)\n\n    return impl\n", "aiohttp/client_proto.py": "import asyncio\nfrom contextlib import suppress\nfrom typing import Any, Optional, Tuple\n\nfrom .base_protocol import BaseProtocol\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientOSError,\n    ClientPayloadError,\n    ServerDisconnectedError,\n    SocketTimeoutError,\n)\nfrom .helpers import (\n    _EXC_SENTINEL,\n    BaseTimerContext,\n    set_exception,\n    set_result,\n    status_code_must_be_empty_body,\n)\nfrom .http import HttpResponseParser, RawResponseMessage, WebSocketReader\nfrom .http_exceptions import HttpProcessingError\nfrom .streams import EMPTY_PAYLOAD, DataQueue, StreamReader\n\n\nclass ResponseHandler(BaseProtocol, DataQueue[Tuple[RawResponseMessage, StreamReader]]):\n    \"\"\"Helper class to adapt between Protocol and StreamReader.\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        BaseProtocol.__init__(self, loop=loop)\n        DataQueue.__init__(self, loop)\n\n        self._should_close = False\n\n        self._payload: Optional[StreamReader] = None\n        self._skip_payload = False\n        self._payload_parser: Optional[WebSocketReader] = None\n\n        self._timer = None\n\n        self._tail = b\"\"\n        self._upgraded = False\n        self._parser: Optional[HttpResponseParser] = None\n\n        self._read_timeout: Optional[float] = None\n        self._read_timeout_handle: Optional[asyncio.TimerHandle] = None\n\n        self._timeout_ceil_threshold: Optional[float] = 5\n\n        self.closed: asyncio.Future[None] = self._loop.create_future()\n\n    @property\n    def upgraded(self) -> bool:\n        return self._upgraded\n\n    @property\n    def should_close(self) -> bool:\n        if self._payload is not None and not self._payload.is_eof():\n            return True\n\n        return (\n            self._should_close\n            or self._upgraded\n            or self.exception() is not None\n            or self._payload_parser is not None\n            or len(self) > 0\n            or bool(self._tail)\n        )\n\n    def force_close(self) -> None:\n        self._should_close = True\n\n    def close(self) -> None:\n        transport = self.transport\n        if transport is not None:\n            transport.close()\n            self.transport = None\n            self._payload = None\n            self._drop_timeout()\n\n    def is_connected(self) -> bool:\n        return self.transport is not None and not self.transport.is_closing()\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        self._drop_timeout()\n\n        original_connection_error = exc\n        reraised_exc = original_connection_error\n\n        connection_closed_cleanly = original_connection_error is None\n\n        if connection_closed_cleanly:\n            set_result(self.closed, None)\n        else:\n            assert original_connection_error is not None\n            set_exception(\n                self.closed,\n                ClientConnectionError(\n                    f\"Connection lost: {original_connection_error !s}\",\n                ),\n                original_connection_error,\n            )\n\n        if self._payload_parser is not None:\n            with suppress(Exception):  # FIXME: log this somehow?\n                self._payload_parser.feed_eof()\n\n        uncompleted = None\n        if self._parser is not None:\n            try:\n                uncompleted = self._parser.feed_eof()\n            except Exception as underlying_exc:\n                if self._payload is not None:\n                    client_payload_exc_msg = (\n                        f\"Response payload is not completed: {underlying_exc !r}\"\n                    )\n                    if not connection_closed_cleanly:\n                        client_payload_exc_msg = (\n                            f\"{client_payload_exc_msg !s}. \"\n                            f\"{original_connection_error !r}\"\n                        )\n                    set_exception(\n                        self._payload,\n                        ClientPayloadError(client_payload_exc_msg),\n                        underlying_exc,\n                    )\n\n        if not self.is_eof():\n            if isinstance(original_connection_error, OSError):\n                reraised_exc = ClientOSError(*original_connection_error.args)\n            if connection_closed_cleanly:\n                reraised_exc = ServerDisconnectedError(uncompleted)\n            # assigns self._should_close to True as side effect,\n            # we do it anyway below\n            underlying_non_eof_exc = (\n                _EXC_SENTINEL\n                if connection_closed_cleanly\n                else original_connection_error\n            )\n            assert underlying_non_eof_exc is not None\n            assert reraised_exc is not None\n            self.set_exception(reraised_exc, underlying_non_eof_exc)\n\n        self._should_close = True\n        self._parser = None\n        self._payload = None\n        self._payload_parser = None\n        self._reading_paused = False\n\n        super().connection_lost(reraised_exc)\n\n    def eof_received(self) -> None:\n        # should call parser.feed_eof() most likely\n        self._drop_timeout()\n\n    def pause_reading(self) -> None:\n        super().pause_reading()\n        self._drop_timeout()\n\n    def resume_reading(self) -> None:\n        super().resume_reading()\n        self._reschedule_timeout()\n\n    def set_exception(\n        self,\n        exc: BaseException,\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._should_close = True\n        self._drop_timeout()\n        super().set_exception(exc, exc_cause)\n\n    def set_parser(self, parser: Any, payload: Any) -> None:\n        # TODO: actual types are:\n        #   parser: WebSocketReader\n        #   payload: FlowControlDataQueue\n        # but they are not generi enough\n        # Need an ABC for both types\n        self._payload = payload\n        self._payload_parser = parser\n\n        self._drop_timeout()\n\n        if self._tail:\n            data, self._tail = self._tail, b\"\"\n            self.data_received(data)\n\n    def set_response_params(\n        self,\n        *,\n        timer: Optional[BaseTimerContext] = None,\n        skip_payload: bool = False,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n        read_timeout: Optional[float] = None,\n        read_bufsize: int = 2**16,\n        timeout_ceil_threshold: float = 5,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n    ) -> None:\n        self._skip_payload = skip_payload\n\n        self._read_timeout = read_timeout\n\n        self._timeout_ceil_threshold = timeout_ceil_threshold\n\n        self._parser = HttpResponseParser(\n            self,\n            self._loop,\n            read_bufsize,\n            timer=timer,\n            payload_exception=ClientPayloadError,\n            response_with_body=not skip_payload,\n            read_until_eof=read_until_eof,\n            auto_decompress=auto_decompress,\n            max_line_size=max_line_size,\n            max_field_size=max_field_size,\n        )\n\n        if self._tail:\n            data, self._tail = self._tail, b\"\"\n            self.data_received(data)\n\n    def _drop_timeout(self) -> None:\n        if self._read_timeout_handle is not None:\n            self._read_timeout_handle.cancel()\n            self._read_timeout_handle = None\n\n    def _reschedule_timeout(self) -> None:\n        timeout = self._read_timeout\n        if self._read_timeout_handle is not None:\n            self._read_timeout_handle.cancel()\n\n        if timeout:\n            self._read_timeout_handle = self._loop.call_later(\n                timeout, self._on_read_timeout\n            )\n        else:\n            self._read_timeout_handle = None\n\n    def start_timeout(self) -> None:\n        self._reschedule_timeout()\n\n    def _on_read_timeout(self) -> None:\n        exc = SocketTimeoutError(\"Timeout on reading data from socket\")\n        self.set_exception(exc)\n        if self._payload is not None:\n            set_exception(self._payload, exc)\n\n    def data_received(self, data: bytes) -> None:\n        self._reschedule_timeout()\n\n        if not data:\n            return\n\n        # custom payload parser\n        if self._payload_parser is not None:\n            eof, tail = self._payload_parser.feed_data(data)\n            if eof:\n                self._payload = None\n                self._payload_parser = None\n\n                if tail:\n                    self.data_received(tail)\n            return\n        else:\n            if self._upgraded or self._parser is None:\n                # i.e. websocket connection, websocket parser is not set yet\n                self._tail += data\n            else:\n                # parse http messages\n                try:\n                    messages, upgraded, tail = self._parser.feed_data(data)\n                except BaseException as underlying_exc:\n                    if self.transport is not None:\n                        # connection.release() could be called BEFORE\n                        # data_received(), the transport is already\n                        # closed in this case\n                        self.transport.close()\n                    # should_close is True after the call\n                    self.set_exception(HttpProcessingError(), underlying_exc)\n                    return\n\n                self._upgraded = upgraded\n\n                payload: Optional[StreamReader] = None\n                for message, payload in messages:\n                    if message.should_close:\n                        self._should_close = True\n\n                    self._payload = payload\n\n                    if self._skip_payload or status_code_must_be_empty_body(\n                        message.code\n                    ):\n                        self.feed_data((message, EMPTY_PAYLOAD))\n                    else:\n                        self.feed_data((message, payload))\n                if payload is not None:\n                    # new message(s) was processed\n                    # register timeout handler unsubscribing\n                    # either on end-of-stream or immediately for\n                    # EMPTY_PAYLOAD\n                    if payload is not EMPTY_PAYLOAD:\n                        payload.on_eof(self._drop_timeout)\n                    else:\n                        self._drop_timeout()\n\n                if tail:\n                    if upgraded:\n                        self.data_received(tail)\n                    else:\n                        self._tail = tail\n", "aiohttp/__init__.py": "__version__ = \"4.0.0a2.dev0\"\n\nfrom typing import TYPE_CHECKING, Tuple\n\nfrom . import hdrs\nfrom .client import (\n    BaseConnector,\n    ClientConnectionError,\n    ClientConnectorCertificateError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientError,\n    ClientHttpProxyError,\n    ClientOSError,\n    ClientPayloadError,\n    ClientProxyConnectionError,\n    ClientRequest,\n    ClientResponse,\n    ClientResponseError,\n    ClientSession,\n    ClientSSLError,\n    ClientTimeout,\n    ClientWebSocketResponse,\n    ConnectionTimeoutError,\n    ContentTypeError,\n    Fingerprint,\n    InvalidURL,\n    InvalidUrlClientError,\n    InvalidUrlRedirectClientError,\n    NamedPipeConnector,\n    NonHttpUrlClientError,\n    NonHttpUrlRedirectClientError,\n    RedirectClientError,\n    RequestInfo,\n    ServerConnectionError,\n    ServerDisconnectedError,\n    ServerFingerprintMismatch,\n    ServerTimeoutError,\n    SocketTimeoutError,\n    TCPConnector,\n    TooManyRedirects,\n    UnixConnector,\n    WSServerHandshakeError,\n    request,\n)\nfrom .cookiejar import CookieJar, DummyCookieJar\nfrom .formdata import FormData\nfrom .helpers import BasicAuth, ChainMapProxy, ETag\nfrom .http import (\n    HttpVersion,\n    HttpVersion10,\n    HttpVersion11,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n)\nfrom .multipart import (\n    BadContentDispositionHeader,\n    BadContentDispositionParam,\n    BodyPartReader,\n    MultipartReader,\n    MultipartWriter,\n    content_disposition_filename,\n    parse_content_disposition,\n)\nfrom .payload import (\n    PAYLOAD_REGISTRY,\n    AsyncIterablePayload,\n    BufferedReaderPayload,\n    BytesIOPayload,\n    BytesPayload,\n    IOBasePayload,\n    JsonPayload,\n    Payload,\n    StringIOPayload,\n    StringPayload,\n    TextIOPayload,\n    get_payload,\n    payload_type,\n)\nfrom .resolver import AsyncResolver, DefaultResolver, ThreadedResolver\nfrom .streams import (\n    EMPTY_PAYLOAD,\n    DataQueue,\n    EofStream,\n    FlowControlDataQueue,\n    StreamReader,\n)\nfrom .tracing import (\n    TraceConfig,\n    TraceConnectionCreateEndParams,\n    TraceConnectionCreateStartParams,\n    TraceConnectionQueuedEndParams,\n    TraceConnectionQueuedStartParams,\n    TraceConnectionReuseconnParams,\n    TraceDnsCacheHitParams,\n    TraceDnsCacheMissParams,\n    TraceDnsResolveHostEndParams,\n    TraceDnsResolveHostStartParams,\n    TraceRequestChunkSentParams,\n    TraceRequestEndParams,\n    TraceRequestExceptionParams,\n    TraceRequestRedirectParams,\n    TraceRequestStartParams,\n    TraceResponseChunkReceivedParams,\n)\n\nif TYPE_CHECKING:\n    # At runtime these are lazy-loaded at the bottom of the file.\n    from .worker import GunicornUVLoopWebWorker, GunicornWebWorker\n\n__all__: Tuple[str, ...] = (\n    \"hdrs\",\n    # client\n    \"BaseConnector\",\n    \"ClientConnectionError\",\n    \"ClientConnectorCertificateError\",\n    \"ClientConnectorError\",\n    \"ClientConnectorSSLError\",\n    \"ClientError\",\n    \"ClientHttpProxyError\",\n    \"ClientOSError\",\n    \"ClientPayloadError\",\n    \"ClientProxyConnectionError\",\n    \"ClientResponse\",\n    \"ClientRequest\",\n    \"ClientResponseError\",\n    \"ClientSSLError\",\n    \"ClientSession\",\n    \"ClientTimeout\",\n    \"ClientWebSocketResponse\",\n    \"ConnectionTimeoutError\",\n    \"ContentTypeError\",\n    \"Fingerprint\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"NonHttpUrlRedirectClientError\",\n    \"RedirectClientError\",\n    \"RequestInfo\",\n    \"ServerConnectionError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ServerTimeoutError\",\n    \"SocketTimeoutError\",\n    \"TCPConnector\",\n    \"TooManyRedirects\",\n    \"UnixConnector\",\n    \"NamedPipeConnector\",\n    \"WSServerHandshakeError\",\n    \"request\",\n    # cookiejar\n    \"CookieJar\",\n    \"DummyCookieJar\",\n    # formdata\n    \"FormData\",\n    # helpers\n    \"BasicAuth\",\n    \"ChainMapProxy\",\n    \"ETag\",\n    # http\n    \"HttpVersion\",\n    \"HttpVersion10\",\n    \"HttpVersion11\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    # multipart\n    \"BadContentDispositionHeader\",\n    \"BadContentDispositionParam\",\n    \"BodyPartReader\",\n    \"MultipartReader\",\n    \"MultipartWriter\",\n    \"content_disposition_filename\",\n    \"parse_content_disposition\",\n    # payload\n    \"AsyncIterablePayload\",\n    \"BufferedReaderPayload\",\n    \"BytesIOPayload\",\n    \"BytesPayload\",\n    \"IOBasePayload\",\n    \"JsonPayload\",\n    \"PAYLOAD_REGISTRY\",\n    \"Payload\",\n    \"StringIOPayload\",\n    \"StringPayload\",\n    \"TextIOPayload\",\n    \"get_payload\",\n    \"payload_type\",\n    # resolver\n    \"AsyncResolver\",\n    \"DefaultResolver\",\n    \"ThreadedResolver\",\n    # streams\n    \"DataQueue\",\n    \"EMPTY_PAYLOAD\",\n    \"EofStream\",\n    \"FlowControlDataQueue\",\n    \"StreamReader\",\n    # tracing\n    \"TraceConfig\",\n    \"TraceConnectionCreateEndParams\",\n    \"TraceConnectionCreateStartParams\",\n    \"TraceConnectionQueuedEndParams\",\n    \"TraceConnectionQueuedStartParams\",\n    \"TraceConnectionReuseconnParams\",\n    \"TraceDnsCacheHitParams\",\n    \"TraceDnsCacheMissParams\",\n    \"TraceDnsResolveHostEndParams\",\n    \"TraceDnsResolveHostStartParams\",\n    \"TraceRequestChunkSentParams\",\n    \"TraceRequestEndParams\",\n    \"TraceRequestExceptionParams\",\n    \"TraceRequestRedirectParams\",\n    \"TraceRequestStartParams\",\n    \"TraceResponseChunkReceivedParams\",\n    # workers (imported lazily with __getattr__)\n    \"GunicornUVLoopWebWorker\",\n    \"GunicornWebWorker\",\n)\n\n\ndef __dir__() -> Tuple[str, ...]:\n    return __all__ + (\"__author__\", \"__doc__\")\n\n\ndef __getattr__(name: str) -> object:\n    global GunicornUVLoopWebWorker, GunicornWebWorker\n\n    # Importing gunicorn takes a long time (>100ms), so only import if actually needed.\n    if name in (\"GunicornUVLoopWebWorker\", \"GunicornWebWorker\"):\n        try:\n            from .worker import GunicornUVLoopWebWorker as guv, GunicornWebWorker as gw\n        except ImportError:\n            return None\n\n        GunicornUVLoopWebWorker = guv  # type: ignore[misc]\n        GunicornWebWorker = gw  # type: ignore[misc]\n        return guv if name == \"GunicornUVLoopWebWorker\" else gw\n\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")\n", "aiohttp/cookiejar.py": "import calendar\nimport contextlib\nimport datetime\nimport itertools\nimport os  # noqa\nimport pathlib\nimport pickle\nimport re\nimport time\nimport warnings\nfrom collections import defaultdict\nfrom http.cookies import BaseCookie, Morsel, SimpleCookie\nfrom math import ceil\nfrom typing import (\n    DefaultDict,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom yarl import URL\n\nfrom .abc import AbstractCookieJar, ClearCookiePredicate\nfrom .helpers import is_ip_address\nfrom .typedefs import LooseCookies, PathLike, StrOrURL\n\n__all__ = (\"CookieJar\", \"DummyCookieJar\")\n\n\nCookieItem = Union[str, \"Morsel[str]\"]\n\n\nclass CookieJar(AbstractCookieJar):\n    \"\"\"Implements cookie storage adhering to RFC 6265.\"\"\"\n\n    DATE_TOKENS_RE = re.compile(\n        r\"[\\x09\\x20-\\x2F\\x3B-\\x40\\x5B-\\x60\\x7B-\\x7E]*\"\n        r\"(?P<token>[\\x00-\\x08\\x0A-\\x1F\\d:a-zA-Z\\x7F-\\xFF]+)\"\n    )\n\n    DATE_HMS_TIME_RE = re.compile(r\"(\\d{1,2}):(\\d{1,2}):(\\d{1,2})\")\n\n    DATE_DAY_OF_MONTH_RE = re.compile(r\"(\\d{1,2})\")\n\n    DATE_MONTH_RE = re.compile(\n        \"(jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)|\" \"(aug)|(sep)|(oct)|(nov)|(dec)\",\n        re.I,\n    )\n\n    DATE_YEAR_RE = re.compile(r\"(\\d{2,4})\")\n\n    # calendar.timegm() fails for timestamps after datetime.datetime.max\n    # Minus one as a loss of precision occurs when timestamp() is called.\n    MAX_TIME = (\n        int(datetime.datetime.max.replace(tzinfo=datetime.timezone.utc).timestamp()) - 1\n    )\n    try:\n        calendar.timegm(time.gmtime(MAX_TIME))\n    except (OSError, ValueError):\n        # Hit the maximum representable time on Windows\n        # https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/localtime-localtime32-localtime64\n        # Throws ValueError on PyPy 3.8 and 3.9, OSError elsewhere\n        MAX_TIME = calendar.timegm((3000, 12, 31, 23, 59, 59, -1, -1, -1))\n    except OverflowError:\n        # #4515: datetime.max may not be representable on 32-bit platforms\n        MAX_TIME = 2**31 - 1\n    # Avoid minuses in the future, 3x faster\n    SUB_MAX_TIME = MAX_TIME - 1\n\n    def __init__(\n        self,\n        *,\n        unsafe: bool = False,\n        quote_cookie: bool = True,\n        treat_as_secure_origin: Union[StrOrURL, List[StrOrURL], None] = None,\n    ) -> None:\n        self._cookies: DefaultDict[Tuple[str, str], SimpleCookie] = defaultdict(\n            SimpleCookie\n        )\n        self._host_only_cookies: Set[Tuple[str, str]] = set()\n        self._unsafe = unsafe\n        self._quote_cookie = quote_cookie\n        if treat_as_secure_origin is None:\n            treat_as_secure_origin = []\n        elif isinstance(treat_as_secure_origin, URL):\n            treat_as_secure_origin = [treat_as_secure_origin.origin()]\n        elif isinstance(treat_as_secure_origin, str):\n            treat_as_secure_origin = [URL(treat_as_secure_origin).origin()]\n        else:\n            treat_as_secure_origin = [\n                URL(url).origin() if isinstance(url, str) else url.origin()\n                for url in treat_as_secure_origin\n            ]\n        self._treat_as_secure_origin = treat_as_secure_origin\n        self._next_expiration: float = ceil(time.time())\n        self._expirations: Dict[Tuple[str, str, str], float] = {}\n\n    def save(self, file_path: PathLike) -> None:\n        file_path = pathlib.Path(file_path)\n        with file_path.open(mode=\"wb\") as f:\n            pickle.dump(self._cookies, f, pickle.HIGHEST_PROTOCOL)\n\n    def load(self, file_path: PathLike) -> None:\n        file_path = pathlib.Path(file_path)\n        with file_path.open(mode=\"rb\") as f:\n            self._cookies = pickle.load(f)\n\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        if predicate is None:\n            self._next_expiration = ceil(time.time())\n            self._cookies.clear()\n            self._host_only_cookies.clear()\n            self._expirations.clear()\n            return\n\n        to_del = []\n        now = time.time()\n        for (domain, path), cookie in self._cookies.items():\n            for name, morsel in cookie.items():\n                key = (domain, path, name)\n                if (\n                    key in self._expirations and self._expirations[key] <= now\n                ) or predicate(morsel):\n                    to_del.append(key)\n\n        for domain, path, name in to_del:\n            self._host_only_cookies.discard((domain, name))\n            key = (domain, path, name)\n            if key in self._expirations:\n                del self._expirations[(domain, path, name)]\n            self._cookies[(domain, path)].pop(name, None)\n\n        self._next_expiration = (\n            min(*self._expirations.values(), self.SUB_MAX_TIME) + 1\n            if self._expirations\n            else self.MAX_TIME\n        )\n\n    def clear_domain(self, domain: str) -> None:\n        self.clear(lambda x: self._is_domain_match(domain, x[\"domain\"]))\n\n    def __iter__(self) -> \"Iterator[Morsel[str]]\":\n        self._do_expiration()\n        for val in self._cookies.values():\n            yield from val.values()\n\n    def __len__(self) -> int:\n        \"\"\"Return number of cookies.\n\n        This function does not iterate self to avoid unnecessary expiration\n        checks.\n        \"\"\"\n        return sum(len(cookie.values()) for cookie in self._cookies.values())\n\n    def _do_expiration(self) -> None:\n        self.clear(lambda x: False)\n\n    def _expire_cookie(self, when: float, domain: str, path: str, name: str) -> None:\n        self._next_expiration = min(self._next_expiration, when)\n        self._expirations[(domain, path, name)] = when\n\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        \"\"\"Update cookies.\"\"\"\n        hostname = response_url.raw_host\n\n        if not self._unsafe and is_ip_address(hostname):\n            # Don't accept cookies from IPs\n            return\n\n        if isinstance(cookies, Mapping):\n            cookies = cookies.items()\n\n        for name, cookie in cookies:\n            if not isinstance(cookie, Morsel):\n                tmp = SimpleCookie()\n                tmp[name] = cookie  # type: ignore[assignment]\n                cookie = tmp[name]\n\n            domain = cookie[\"domain\"]\n\n            # ignore domains with trailing dots\n            if domain.endswith(\".\"):\n                domain = \"\"\n                del cookie[\"domain\"]\n\n            if not domain and hostname is not None:\n                # Set the cookie's domain to the response hostname\n                # and set its host-only-flag\n                self._host_only_cookies.add((hostname, name))\n                domain = cookie[\"domain\"] = hostname\n\n            if domain.startswith(\".\"):\n                # Remove leading dot\n                domain = domain[1:]\n                cookie[\"domain\"] = domain\n\n            if hostname and not self._is_domain_match(domain, hostname):\n                # Setting cookies for different domains is not allowed\n                continue\n\n            path = cookie[\"path\"]\n            if not path or not path.startswith(\"/\"):\n                # Set the cookie's path to the response path\n                path = response_url.path\n                if not path.startswith(\"/\"):\n                    path = \"/\"\n                else:\n                    # Cut everything from the last slash to the end\n                    path = \"/\" + path[1 : path.rfind(\"/\")]\n                cookie[\"path\"] = path\n            path = path.rstrip(\"/\")\n\n            max_age = cookie[\"max-age\"]\n            if max_age:\n                try:\n                    delta_seconds = int(max_age)\n                    max_age_expiration = min(time.time() + delta_seconds, self.MAX_TIME)\n                    self._expire_cookie(max_age_expiration, domain, path, name)\n                except ValueError:\n                    cookie[\"max-age\"] = \"\"\n\n            else:\n                expires = cookie[\"expires\"]\n                if expires:\n                    expire_time = self._parse_date(expires)\n                    if expire_time:\n                        self._expire_cookie(expire_time, domain, path, name)\n                    else:\n                        cookie[\"expires\"] = \"\"\n\n            self._cookies[(domain, path)][name] = cookie\n\n        self._do_expiration()\n\n    def filter_cookies(self, request_url: URL = URL()) -> \"BaseCookie[str]\":\n        \"\"\"Returns this jar's cookies filtered by their attributes.\"\"\"\n        if not isinstance(request_url, URL):\n            warnings.warn(\n                \"The method accepts yarl.URL instances only, got {}\".format(\n                    type(request_url)\n                ),\n                DeprecationWarning,\n            )\n            request_url = URL(request_url)\n        filtered: Union[SimpleCookie, \"BaseCookie[str]\"] = (\n            SimpleCookie() if self._quote_cookie else BaseCookie()\n        )\n        if not self._cookies:\n            # Skip do_expiration() if there are no cookies.\n            return filtered\n        self._do_expiration()\n        if not self._cookies:\n            # Skip rest of function if no non-expired cookies.\n            return filtered\n        hostname = request_url.raw_host or \"\"\n\n        is_not_secure = request_url.scheme not in (\"https\", \"wss\")\n        if is_not_secure and self._treat_as_secure_origin:\n            request_origin = URL()\n            with contextlib.suppress(ValueError):\n                request_origin = request_url.origin()\n            is_not_secure = request_origin not in self._treat_as_secure_origin\n\n        # Send shared cookie\n        for c in self._cookies[(\"\", \"\")].values():\n            filtered[c.key] = c.value\n\n        if is_ip_address(hostname):\n            if not self._unsafe:\n                return filtered\n            domains: Iterable[str] = (hostname,)\n        else:\n            # Get all the subdomains that might match a cookie (e.g. \"foo.bar.com\", \"bar.com\", \"com\")\n            domains = itertools.accumulate(\n                reversed(hostname.split(\".\")), lambda x, y: f\"{y}.{x}\"\n            )\n        # Get all the path prefixes that might match a cookie (e.g. \"\", \"/foo\", \"/foo/bar\")\n        paths = itertools.accumulate(\n            request_url.path.split(\"/\"), lambda x, y: f\"{x}/{y}\"\n        )\n        # Create every combination of (domain, path) pairs.\n        pairs = itertools.product(domains, paths)\n\n        # Point 2: https://www.rfc-editor.org/rfc/rfc6265.html#section-5.4\n        cookies = itertools.chain.from_iterable(\n            self._cookies[p].values() for p in pairs\n        )\n        path_len = len(request_url.path)\n        for cookie in cookies:\n            name = cookie.key\n            domain = cookie[\"domain\"]\n\n            if (domain, name) in self._host_only_cookies:\n                if domain != hostname:\n                    continue\n\n            # Skip edge case when the cookie has a trailing slash but request doesn't.\n            if len(cookie[\"path\"]) > path_len:\n                continue\n\n            if is_not_secure and cookie[\"secure\"]:\n                continue\n\n            # It's critical we use the Morsel so the coded_value\n            # (based on cookie version) is preserved\n            mrsl_val = cast(\"Morsel[str]\", cookie.get(cookie.key, Morsel()))\n            mrsl_val.set(cookie.key, cookie.value, cookie.coded_value)\n            filtered[name] = mrsl_val\n\n        return filtered\n\n    @staticmethod\n    def _is_domain_match(domain: str, hostname: str) -> bool:\n        \"\"\"Implements domain matching adhering to RFC 6265.\"\"\"\n        if hostname == domain:\n            return True\n\n        if not hostname.endswith(domain):\n            return False\n\n        non_matching = hostname[: -len(domain)]\n\n        if not non_matching.endswith(\".\"):\n            return False\n\n        return not is_ip_address(hostname)\n\n    @classmethod\n    def _parse_date(cls, date_str: str) -> Optional[int]:\n        \"\"\"Implements date string parsing adhering to RFC 6265.\"\"\"\n        if not date_str:\n            return None\n\n        found_time = False\n        found_day = False\n        found_month = False\n        found_year = False\n\n        hour = minute = second = 0\n        day = 0\n        month = 0\n        year = 0\n\n        for token_match in cls.DATE_TOKENS_RE.finditer(date_str):\n            token = token_match.group(\"token\")\n\n            if not found_time:\n                time_match = cls.DATE_HMS_TIME_RE.match(token)\n                if time_match:\n                    found_time = True\n                    hour, minute, second = (int(s) for s in time_match.groups())\n                    continue\n\n            if not found_day:\n                day_match = cls.DATE_DAY_OF_MONTH_RE.match(token)\n                if day_match:\n                    found_day = True\n                    day = int(day_match.group())\n                    continue\n\n            if not found_month:\n                month_match = cls.DATE_MONTH_RE.match(token)\n                if month_match:\n                    found_month = True\n                    assert month_match.lastindex is not None\n                    month = month_match.lastindex\n                    continue\n\n            if not found_year:\n                year_match = cls.DATE_YEAR_RE.match(token)\n                if year_match:\n                    found_year = True\n                    year = int(year_match.group())\n\n        if 70 <= year <= 99:\n            year += 1900\n        elif 0 <= year <= 69:\n            year += 2000\n\n        if False in (found_day, found_month, found_year, found_time):\n            return None\n\n        if not 1 <= day <= 31:\n            return None\n\n        if year < 1601 or hour > 23 or minute > 59 or second > 59:\n            return None\n\n        return calendar.timegm((year, month, day, hour, minute, second, -1, -1, -1))\n\n\nclass DummyCookieJar(AbstractCookieJar):\n    \"\"\"Implements a dummy cookie storage.\n\n    It can be used with the ClientSession when no cookie processing is needed.\n\n    \"\"\"\n\n    def __iter__(self) -> \"Iterator[Morsel[str]]\":\n        while False:\n            yield None\n\n    def __len__(self) -> int:\n        return 0\n\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        pass\n\n    def clear_domain(self, domain: str) -> None:\n        pass\n\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        pass\n\n    def filter_cookies(self, request_url: URL) -> \"BaseCookie[str]\":\n        return SimpleCookie()\n", "aiohttp/base_protocol.py": "import asyncio\nfrom typing import Optional, cast\n\nfrom .helpers import set_exception\nfrom .tcp_helpers import tcp_nodelay\n\n\nclass BaseProtocol(asyncio.Protocol):\n    __slots__ = (\n        \"_loop\",\n        \"_paused\",\n        \"_drain_waiter\",\n        \"_connection_lost\",\n        \"_reading_paused\",\n        \"transport\",\n    )\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop: asyncio.AbstractEventLoop = loop\n        self._paused = False\n        self._drain_waiter: Optional[asyncio.Future[None]] = None\n        self._reading_paused = False\n\n        self.transport: Optional[asyncio.Transport] = None\n\n    @property\n    def connected(self) -> bool:\n        \"\"\"Return True if the connection is open.\"\"\"\n        return self.transport is not None\n\n    def pause_writing(self) -> None:\n        assert not self._paused\n        self._paused = True\n\n    def resume_writing(self) -> None:\n        assert self._paused\n        self._paused = False\n\n        waiter = self._drain_waiter\n        if waiter is not None:\n            self._drain_waiter = None\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def pause_reading(self) -> None:\n        if not self._reading_paused and self.transport is not None:\n            try:\n                self.transport.pause_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = True\n\n    def resume_reading(self) -> None:\n        if self._reading_paused and self.transport is not None:\n            try:\n                self.transport.resume_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = False\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        tr = cast(asyncio.Transport, transport)\n        tcp_nodelay(tr, True)\n        self.transport = tr\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        # Wake up the writer if currently paused.\n        self.transport = None\n        if not self._paused:\n            return\n        waiter = self._drain_waiter\n        if waiter is None:\n            return\n        self._drain_waiter = None\n        if waiter.done():\n            return\n        if exc is None:\n            waiter.set_result(None)\n        else:\n            set_exception(\n                waiter,\n                ConnectionError(\"Connection lost\"),\n                exc,\n            )\n\n    async def _drain_helper(self) -> None:\n        if not self.connected:\n            raise ConnectionResetError(\"Connection lost\")\n        if not self._paused:\n            return\n        waiter = self._drain_waiter\n        if waiter is None:\n            waiter = self._loop.create_future()\n            self._drain_waiter = waiter\n        await asyncio.shield(waiter)\n", "aiohttp/web_runner.py": "import asyncio\nimport signal\nimport socket\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Awaitable, Callable, List, Optional, Set, Type\n\nfrom yarl import URL\n\nfrom .abc import AbstractAccessLogger, AbstractStreamWriter\nfrom .http_parser import RawRequestMessage\nfrom .streams import StreamReader\nfrom .typedefs import PathLike\nfrom .web_app import Application\nfrom .web_log import AccessLogger\nfrom .web_protocol import RequestHandler\nfrom .web_request import Request\nfrom .web_server import Server\n\ntry:\n    from ssl import SSLContext\nexcept ImportError:\n    SSLContext = object  # type: ignore[misc,assignment]\n\n\n__all__ = (\n    \"BaseSite\",\n    \"TCPSite\",\n    \"UnixSite\",\n    \"NamedPipeSite\",\n    \"SockSite\",\n    \"BaseRunner\",\n    \"AppRunner\",\n    \"ServerRunner\",\n    \"GracefulExit\",\n)\n\n\nclass GracefulExit(SystemExit):\n    code = 1\n\n\ndef _raise_graceful_exit() -> None:\n    raise GracefulExit()\n\n\nclass BaseSite(ABC):\n    __slots__ = (\"_runner\", \"_ssl_context\", \"_backlog\", \"_server\")\n\n    def __init__(\n        self,\n        runner: \"BaseRunner\",\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n    ) -> None:\n        if runner.server is None:\n            raise RuntimeError(\"Call runner.setup() before making a site\")\n        self._runner = runner\n        self._ssl_context = ssl_context\n        self._backlog = backlog\n        self._server: Optional[asyncio.AbstractServer] = None\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def start(self) -> None:\n        self._runner._reg_site(self)\n\n    async def stop(self) -> None:\n        self._runner._check_site(self)\n        if self._server is not None:  # Maybe not started yet\n            self._server.close()\n\n        self._runner._unreg_site(self)\n\n\nclass TCPSite(BaseSite):\n    __slots__ = (\"_host\", \"_port\", \"_reuse_address\", \"_reuse_port\")\n\n    def __init__(\n        self,\n        runner: \"BaseRunner\",\n        host: Optional[str] = None,\n        port: Optional[int] = None,\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n        reuse_address: Optional[bool] = None,\n        reuse_port: Optional[bool] = None,\n    ) -> None:\n        super().__init__(\n            runner,\n            ssl_context=ssl_context,\n            backlog=backlog,\n        )\n        self._host = host\n        if port is None:\n            port = 8443 if self._ssl_context else 8080\n        self._port = port\n        self._reuse_address = reuse_address\n        self._reuse_port = reuse_port\n\n    @property\n    def name(self) -> str:\n        scheme = \"https\" if self._ssl_context else \"http\"\n        host = \"0.0.0.0\" if self._host is None else self._host\n        return str(URL.build(scheme=scheme, host=host, port=self._port))\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        self._server = await loop.create_server(\n            server,\n            self._host,\n            self._port,\n            ssl=self._ssl_context,\n            backlog=self._backlog,\n            reuse_address=self._reuse_address,\n            reuse_port=self._reuse_port,\n        )\n\n\nclass UnixSite(BaseSite):\n    __slots__ = (\"_path\",)\n\n    def __init__(\n        self,\n        runner: \"BaseRunner\",\n        path: PathLike,\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n    ) -> None:\n        super().__init__(\n            runner,\n            ssl_context=ssl_context,\n            backlog=backlog,\n        )\n        self._path = path\n\n    @property\n    def name(self) -> str:\n        scheme = \"https\" if self._ssl_context else \"http\"\n        return f\"{scheme}://unix:{self._path}:\"\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        self._server = await loop.create_unix_server(\n            server,\n            self._path,\n            ssl=self._ssl_context,\n            backlog=self._backlog,\n        )\n\n\nclass NamedPipeSite(BaseSite):\n    __slots__ = (\"_path\",)\n\n    def __init__(self, runner: \"BaseRunner\", path: str) -> None:\n        loop = asyncio.get_event_loop()\n        if not isinstance(\n            loop, asyncio.ProactorEventLoop  # type: ignore[attr-defined]\n        ):\n            raise RuntimeError(\n                \"Named Pipes only available in proactor\" \"loop under windows\"\n            )\n        super().__init__(runner)\n        self._path = path\n\n    @property\n    def name(self) -> str:\n        return self._path\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        _server = await loop.start_serving_pipe(  # type: ignore[attr-defined]\n            server, self._path\n        )\n        self._server = _server[0]\n\n\nclass SockSite(BaseSite):\n    __slots__ = (\"_sock\", \"_name\")\n\n    def __init__(\n        self,\n        runner: \"BaseRunner\",\n        sock: socket.socket,\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n    ) -> None:\n        super().__init__(\n            runner,\n            ssl_context=ssl_context,\n            backlog=backlog,\n        )\n        self._sock = sock\n        scheme = \"https\" if self._ssl_context else \"http\"\n        if hasattr(socket, \"AF_UNIX\") and sock.family == socket.AF_UNIX:\n            name = f\"{scheme}://unix:{sock.getsockname()}:\"\n        else:\n            host, port = sock.getsockname()[:2]\n            name = str(URL.build(scheme=scheme, host=host, port=port))\n        self._name = name\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        self._server = await loop.create_server(\n            server, sock=self._sock, ssl=self._ssl_context, backlog=self._backlog\n        )\n\n\nclass BaseRunner(ABC):\n    __slots__ = (\n        \"shutdown_callback\",\n        \"_handle_signals\",\n        \"_kwargs\",\n        \"_server\",\n        \"_sites\",\n        \"_shutdown_timeout\",\n    )\n\n    def __init__(\n        self,\n        *,\n        handle_signals: bool = False,\n        shutdown_timeout: float = 60.0,\n        **kwargs: Any,\n    ) -> None:\n        self.shutdown_callback: Optional[Callable[[], Awaitable[None]]] = None\n        self._handle_signals = handle_signals\n        self._kwargs = kwargs\n        self._server: Optional[Server] = None\n        self._sites: List[BaseSite] = []\n        self._shutdown_timeout = shutdown_timeout\n\n    @property\n    def server(self) -> Optional[Server]:\n        return self._server\n\n    @property\n    def addresses(self) -> List[Any]:\n        ret: List[Any] = []\n        for site in self._sites:\n            server = site._server\n            if server is not None:\n                sockets = server.sockets  # type: ignore[attr-defined]\n                if sockets is not None:\n                    for sock in sockets:\n                        ret.append(sock.getsockname())\n        return ret\n\n    @property\n    def sites(self) -> Set[BaseSite]:\n        return set(self._sites)\n\n    async def setup(self) -> None:\n        loop = asyncio.get_event_loop()\n\n        if self._handle_signals:\n            try:\n                loop.add_signal_handler(signal.SIGINT, _raise_graceful_exit)\n                loop.add_signal_handler(signal.SIGTERM, _raise_graceful_exit)\n            except NotImplementedError:  # pragma: no cover\n                # add_signal_handler is not implemented on Windows\n                pass\n\n        self._server = await self._make_server()\n\n    @abstractmethod\n    async def shutdown(self) -> None:\n        \"\"\"Call any shutdown hooks to help server close gracefully.\"\"\"\n\n    async def cleanup(self) -> None:\n        # The loop over sites is intentional, an exception on gather()\n        # leaves self._sites in unpredictable state.\n        # The loop guarantees that a site is either deleted on success or\n        # still present on failure\n        for site in list(self._sites):\n            await site.stop()\n\n        if self._server:  # If setup succeeded\n            # Yield to event loop to ensure incoming requests prior to stopping the sites\n            # have all started to be handled before we proceed to close idle connections.\n            await asyncio.sleep(0)\n            self._server.pre_shutdown()\n            await self.shutdown()\n\n            if self.shutdown_callback:\n                await self.shutdown_callback()\n\n            await self._server.shutdown(self._shutdown_timeout)\n        await self._cleanup_server()\n\n        self._server = None\n        if self._handle_signals:\n            loop = asyncio.get_running_loop()\n            try:\n                loop.remove_signal_handler(signal.SIGINT)\n                loop.remove_signal_handler(signal.SIGTERM)\n            except NotImplementedError:  # pragma: no cover\n                # remove_signal_handler is not implemented on Windows\n                pass\n\n    @abstractmethod\n    async def _make_server(self) -> Server:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def _cleanup_server(self) -> None:\n        pass  # pragma: no cover\n\n    def _reg_site(self, site: BaseSite) -> None:\n        if site in self._sites:\n            raise RuntimeError(f\"Site {site} is already registered in runner {self}\")\n        self._sites.append(site)\n\n    def _check_site(self, site: BaseSite) -> None:\n        if site not in self._sites:\n            raise RuntimeError(f\"Site {site} is not registered in runner {self}\")\n\n    def _unreg_site(self, site: BaseSite) -> None:\n        if site not in self._sites:\n            raise RuntimeError(f\"Site {site} is not registered in runner {self}\")\n        self._sites.remove(site)\n\n\nclass ServerRunner(BaseRunner):\n    \"\"\"Low-level web server runner\"\"\"\n\n    __slots__ = (\"_web_server\",)\n\n    def __init__(\n        self, web_server: Server, *, handle_signals: bool = False, **kwargs: Any\n    ) -> None:\n        super().__init__(handle_signals=handle_signals, **kwargs)\n        self._web_server = web_server\n\n    async def shutdown(self) -> None:\n        pass\n\n    async def _make_server(self) -> Server:\n        return self._web_server\n\n    async def _cleanup_server(self) -> None:\n        pass\n\n\nclass AppRunner(BaseRunner):\n    \"\"\"Web Application runner\"\"\"\n\n    __slots__ = (\"_app\",)\n\n    def __init__(\n        self,\n        app: Application,\n        *,\n        handle_signals: bool = False,\n        access_log_class: Type[AbstractAccessLogger] = AccessLogger,\n        **kwargs: Any,\n    ) -> None:\n        if not isinstance(app, Application):\n            raise TypeError(\n                \"The first argument should be web.Application \"\n                \"instance, got {!r}\".format(app)\n            )\n        kwargs[\"access_log_class\"] = access_log_class\n\n        if app._handler_args:\n            for k, v in app._handler_args.items():\n                kwargs[k] = v\n\n        if not issubclass(kwargs[\"access_log_class\"], AbstractAccessLogger):\n            raise TypeError(\n                \"access_log_class must be subclass of \"\n                \"aiohttp.abc.AbstractAccessLogger, got {}\".format(\n                    kwargs[\"access_log_class\"]\n                )\n            )\n\n        super().__init__(handle_signals=handle_signals, **kwargs)\n        self._app = app\n\n    @property\n    def app(self) -> Application:\n        return self._app\n\n    async def shutdown(self) -> None:\n        await self._app.shutdown()\n\n    async def _make_server(self) -> Server:\n        self._app.on_startup.freeze()\n        await self._app.startup()\n        self._app.freeze()\n\n        return Server(\n            self._app._handle,  # type: ignore[arg-type]\n            request_factory=self._make_request,\n            **self._kwargs,\n        )\n\n    def _make_request(\n        self,\n        message: RawRequestMessage,\n        payload: StreamReader,\n        protocol: RequestHandler,\n        writer: AbstractStreamWriter,\n        task: \"asyncio.Task[None]\",\n        _cls: Type[Request] = Request,\n    ) -> Request:\n        loop = asyncio.get_running_loop()\n        return _cls(\n            message,\n            payload,\n            protocol,\n            writer,\n            task,\n            loop,\n            client_max_size=self.app._client_max_size,\n        )\n\n    async def _cleanup_server(self) -> None:\n        await self._app.cleanup()\n", "aiohttp/web_routedef.py": "import abc\nimport dataclasses\nimport os  # noqa\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    Union,\n    overload,\n)\n\nfrom . import hdrs\nfrom .abc import AbstractView\nfrom .typedefs import Handler, PathLike\n\nif TYPE_CHECKING:\n    from .web_request import Request\n    from .web_response import StreamResponse\n    from .web_urldispatcher import AbstractRoute, UrlDispatcher\nelse:\n    Request = StreamResponse = UrlDispatcher = AbstractRoute = None\n\n\n__all__ = (\n    \"AbstractRouteDef\",\n    \"RouteDef\",\n    \"StaticDef\",\n    \"RouteTableDef\",\n    \"head\",\n    \"options\",\n    \"get\",\n    \"post\",\n    \"patch\",\n    \"put\",\n    \"delete\",\n    \"route\",\n    \"view\",\n    \"static\",\n)\n\n\nclass AbstractRouteDef(abc.ABC):\n    @abc.abstractmethod\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\n        pass  # pragma: no cover\n\n\n_HandlerType = Union[Type[AbstractView], Handler]\n\n\n@dataclasses.dataclass(frozen=True, repr=False)\nclass RouteDef(AbstractRouteDef):\n    method: str\n    path: str\n    handler: _HandlerType\n    kwargs: Dict[str, Any]\n\n    def __repr__(self) -> str:\n        info = []\n        for name, value in sorted(self.kwargs.items()):\n            info.append(f\", {name}={value!r}\")\n        return \"<RouteDef {method} {path} -> {handler.__name__!r}\" \"{info}>\".format(\n            method=self.method, path=self.path, handler=self.handler, info=\"\".join(info)\n        )\n\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\n        if self.method in hdrs.METH_ALL:\n            reg = getattr(router, \"add_\" + self.method.lower())\n            return [reg(self.path, self.handler, **self.kwargs)]\n        else:\n            return [\n                router.add_route(self.method, self.path, self.handler, **self.kwargs)\n            ]\n\n\n@dataclasses.dataclass(frozen=True, repr=False)\nclass StaticDef(AbstractRouteDef):\n    prefix: str\n    path: PathLike\n    kwargs: Dict[str, Any]\n\n    def __repr__(self) -> str:\n        info = []\n        for name, value in sorted(self.kwargs.items()):\n            info.append(f\", {name}={value!r}\")\n        return \"<StaticDef {prefix} -> {path}\" \"{info}>\".format(\n            prefix=self.prefix, path=self.path, info=\"\".join(info)\n        )\n\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\n        resource = router.add_static(self.prefix, self.path, **self.kwargs)\n        routes = resource.get_info().get(\"routes\", {})\n        return list(routes.values())\n\n\ndef route(method: str, path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return RouteDef(method, path, handler, kwargs)\n\n\ndef head(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_HEAD, path, handler, **kwargs)\n\n\ndef options(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_OPTIONS, path, handler, **kwargs)\n\n\ndef get(\n    path: str,\n    handler: _HandlerType,\n    *,\n    name: Optional[str] = None,\n    allow_head: bool = True,\n    **kwargs: Any,\n) -> RouteDef:\n    return route(\n        hdrs.METH_GET, path, handler, name=name, allow_head=allow_head, **kwargs\n    )\n\n\ndef post(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_POST, path, handler, **kwargs)\n\n\ndef put(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_PUT, path, handler, **kwargs)\n\n\ndef patch(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_PATCH, path, handler, **kwargs)\n\n\ndef delete(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_DELETE, path, handler, **kwargs)\n\n\ndef view(path: str, handler: Type[AbstractView], **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_ANY, path, handler, **kwargs)\n\n\ndef static(prefix: str, path: PathLike, **kwargs: Any) -> StaticDef:\n    return StaticDef(prefix, path, kwargs)\n\n\n_Deco = Callable[[_HandlerType], _HandlerType]\n\n\nclass RouteTableDef(Sequence[AbstractRouteDef]):\n    \"\"\"Route definition table\"\"\"\n\n    def __init__(self) -> None:\n        self._items: List[AbstractRouteDef] = []\n\n    def __repr__(self) -> str:\n        return f\"<RouteTableDef count={len(self._items)}>\"\n\n    @overload\n    def __getitem__(self, index: int) -> AbstractRouteDef: ...\n\n    @overload\n    def __getitem__(self, index: slice) -> List[AbstractRouteDef]: ...\n\n    def __getitem__(self, index):  # type: ignore[no-untyped-def]\n        return self._items[index]\n\n    def __iter__(self) -> Iterator[AbstractRouteDef]:\n        return iter(self._items)\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __contains__(self, item: object) -> bool:\n        return item in self._items\n\n    def route(self, method: str, path: str, **kwargs: Any) -> _Deco:\n        def inner(handler: _HandlerType) -> _HandlerType:\n            self._items.append(RouteDef(method, path, handler, kwargs))\n            return handler\n\n        return inner\n\n    def head(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_HEAD, path, **kwargs)\n\n    def get(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_GET, path, **kwargs)\n\n    def post(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_POST, path, **kwargs)\n\n    def put(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_PUT, path, **kwargs)\n\n    def patch(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_PATCH, path, **kwargs)\n\n    def delete(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_DELETE, path, **kwargs)\n\n    def options(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_OPTIONS, path, **kwargs)\n\n    def view(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_ANY, path, **kwargs)\n\n    def static(self, prefix: str, path: PathLike, **kwargs: Any) -> None:\n        self._items.append(StaticDef(prefix, path, kwargs))\n", "aiohttp/typedefs.py": "import json\nimport os\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Iterable,\n    Mapping,\n    Tuple,\n    Union,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy, istr\nfrom yarl import URL\n\nDEFAULT_JSON_ENCODER = json.dumps\nDEFAULT_JSON_DECODER = json.loads\n\nif TYPE_CHECKING:\n    _CIMultiDict = CIMultiDict[str]\n    _CIMultiDictProxy = CIMultiDictProxy[str]\n    _MultiDict = MultiDict[str]\n    _MultiDictProxy = MultiDictProxy[str]\n    from http.cookies import BaseCookie, Morsel\n\n    from .web import Request, StreamResponse\nelse:\n    _CIMultiDict = CIMultiDict\n    _CIMultiDictProxy = CIMultiDictProxy\n    _MultiDict = MultiDict\n    _MultiDictProxy = MultiDictProxy\n\nByteish = Union[bytes, bytearray, memoryview]\nJSONEncoder = Callable[[Any], str]\nJSONDecoder = Callable[[str], Any]\nLooseHeaders = Union[Mapping[Union[str, istr], str], _CIMultiDict, _CIMultiDictProxy]\nRawHeaders = Tuple[Tuple[bytes, bytes], ...]\nStrOrURL = Union[str, URL]\n\nLooseCookiesMappings = Mapping[str, Union[str, \"BaseCookie[str]\", \"Morsel[Any]\"]]\nLooseCookiesIterables = Iterable[\n    Tuple[str, Union[str, \"BaseCookie[str]\", \"Morsel[Any]\"]]\n]\nLooseCookies = Union[\n    LooseCookiesMappings,\n    LooseCookiesIterables,\n    \"BaseCookie[str]\",\n]\n\nHandler = Callable[[\"Request\"], Awaitable[\"StreamResponse\"]]\nMiddleware = Callable[[\"Request\", Handler], Awaitable[\"StreamResponse\"]]\n\nPathLike = Union[str, \"os.PathLike[str]\"]\n", "examples/web_srv_route_deco.py": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server with decorator definition for routes.\"\"\"\n\nimport textwrap\n\nfrom aiohttp import web\n\nroutes = web.RouteTableDef()\n\n\n@routes.get(\"/\")\nasync def intro(request: web.Request) -> web.StreamResponse:\n    txt = textwrap.dedent(\n        \"\"\"\\\n        Type {url}/hello/John  {url}/simple or {url}/change_body\n        in browser url bar\n    \"\"\"\n    ).format(url=\"127.0.0.1:8080\")\n    binary = txt.encode(\"utf8\")\n    resp = web.StreamResponse()\n    resp.content_length = len(binary)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(binary)\n    return resp\n\n\n@routes.get(\"/simple\")\nasync def simple(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Simple answer\")\n\n\n@routes.get(\"/change_body\")\nasync def change_body(request: web.Request) -> web.StreamResponse:\n    resp = web.Response()\n    resp.body = b\"Body changed\"\n    resp.content_type = \"text/plain\"\n    return resp\n\n\n@routes.get(\"/hello\")\nasync def hello(request: web.Request) -> web.StreamResponse:\n    resp = web.StreamResponse()\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    answer = (\"Hello, \" + name).encode(\"utf8\")\n    resp.content_length = len(answer)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(answer)\n    await resp.write_eof()\n    return resp\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    app.router.add_routes(routes)\n    return app\n\n\nweb.run_app(init())\n", "examples/lowlevel_srv.py": "import asyncio\n\nfrom aiohttp import web, web_request\n\n\nasync def handler(request: web_request.BaseRequest) -> web.StreamResponse:\n    return web.Response(text=\"OK\")\n\n\nasync def main(loop: asyncio.AbstractEventLoop) -> None:\n    server = web.Server(handler)\n    await loop.create_server(server, \"127.0.0.1\", 8080)\n    print(\"======= Serving on http://127.0.0.1:8080/ ======\")\n\n    # pause here for very long time by serving HTTP requests and\n    # waiting for keyboard interruption\n    await asyncio.sleep(100 * 3600)\n\n\nloop = asyncio.get_event_loop()\n\ntry:\n    loop.run_until_complete(main(loop))\nexcept KeyboardInterrupt:\n    pass\nloop.close()\n", "examples/static_files.py": "#!/usr/bin/env python3\nimport pathlib\n\nfrom aiohttp import web\n\napp = web.Application()\napp.router.add_static(\"/\", pathlib.Path(__file__).parent, show_index=True)\n\nweb.run_app(app)\n", "examples/server_simple.py": "# server_simple.py\nfrom aiohttp import web\n\n\nasync def handle(request: web.Request) -> web.StreamResponse:\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)\n\n\nasync def wshandle(request: web.Request) -> web.StreamResponse:\n    ws = web.WebSocketResponse()\n    await ws.prepare(request)\n\n    async for msg in ws:\n        if msg.type == web.WSMsgType.TEXT:\n            await ws.send_str(f\"Hello, {msg.data}\")\n        elif msg.type == web.WSMsgType.BINARY:\n            await ws.send_bytes(msg.data)\n        elif msg.type == web.WSMsgType.CLOSE:\n            break\n\n    return ws\n\n\napp = web.Application()\napp.add_routes(\n    [web.get(\"/\", handle), web.get(\"/echo\", wshandle), web.get(\"/{name}\", handle)]\n)\n\nweb.run_app(app)\n", "examples/web_rewrite_headers_middleware.py": "#!/usr/bin/env python3\n\"\"\"Example for rewriting response headers by middleware.\"\"\"\n\nfrom aiohttp import web\nfrom aiohttp.typedefs import Handler\n\n\nasync def handler(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Everything is fine\")\n\n\nasync def middleware(request: web.Request, handler: Handler) -> web.StreamResponse:\n    try:\n        response = await handler(request)\n    except web.HTTPException as exc:\n        raise exc\n    if not response.prepared:\n        response.headers[\"SERVER\"] = \"Secured Server Software\"\n    return response\n\n\ndef init() -> web.Application:\n    app = web.Application(middlewares=[middleware])\n    app.router.add_get(\"/\", handler)\n    return app\n\n\nweb.run_app(init())\n", "examples/web_ws.py": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web websocket server.\"\"\"\n\n# The extra strict mypy settings are here to help test that `Application[AppKey()]`\n# syntax is working correctly. A regression will cause mypy to raise an error.\n# mypy: disallow-any-expr, disallow-any-unimported, disallow-subclassing-any\n\nimport os\nfrom typing import List, Union\n\nfrom aiohttp import web\n\nWS_FILE = os.path.join(os.path.dirname(__file__), \"websocket.html\")\nsockets = web.AppKey(\"sockets\", List[web.WebSocketResponse])\n\n\nasync def wshandler(request: web.Request) -> Union[web.WebSocketResponse, web.Response]:\n    resp = web.WebSocketResponse()\n    available = resp.can_prepare(request)\n    if not available:\n        with open(WS_FILE, \"rb\") as fp:\n            return web.Response(body=fp.read(), content_type=\"text/html\")\n\n    await resp.prepare(request)\n\n    await resp.send_str(\"Welcome!!!\")\n\n    try:\n        print(\"Someone joined.\")\n        for ws in request.app[sockets]:\n            await ws.send_str(\"Someone joined\")\n        request.app[sockets].append(resp)\n\n        async for msg in resp:  # type: ignore[misc]\n            if msg.type == web.WSMsgType.TEXT:  # type: ignore[misc]\n                for ws in request.app[sockets]:\n                    if ws is not resp:\n                        await ws.send_str(msg.data)  # type: ignore[misc]\n            else:\n                return resp\n        return resp\n\n    finally:\n        request.app[sockets].remove(resp)\n        print(\"Someone disconnected.\")\n        for ws in request.app[sockets]:\n            await ws.send_str(\"Someone disconnected.\")\n\n\nasync def on_shutdown(app: web.Application) -> None:\n    for ws in app[sockets]:\n        await ws.close()\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    l: List[web.WebSocketResponse] = []\n    app[sockets] = l\n    app.router.add_get(\"/\", wshandler)\n    app.on_shutdown.append(on_shutdown)\n    return app\n\n\nweb.run_app(init())\n", "examples/background_tasks.py": "#!/usr/bin/env python3\n\"\"\"Example of aiohttp.web.Application.on_startup signal handler\"\"\"\nimport asyncio\nfrom typing import List\n\nimport aioredis\n\nfrom aiohttp import web\n\nredis_listener = web.AppKey(\"redis_listener\", asyncio.Task[None])\nwebsockets = web.AppKey(\"websockets\", List[web.WebSocketResponse])\n\n\nasync def websocket_handler(request: web.Request) -> web.StreamResponse:\n    ws = web.WebSocketResponse()\n    await ws.prepare(request)\n    request.app[websockets].append(ws)\n    try:\n        async for msg in ws:\n            print(msg)\n            await asyncio.sleep(1)\n    finally:\n        request.app[websockets].remove(ws)\n    return ws\n\n\nasync def on_shutdown(app: web.Application) -> None:\n    for ws in app[websockets]:\n        await ws.close(code=999, message=b\"Server shutdown\")\n\n\nasync def listen_to_redis(app: web.Application) -> None:\n    sub = await aioredis.Redis(host=\"localhost\", port=6379)\n    ch, *_ = await sub.subscribe(\"news\")\n    try:\n        async for msg in ch.iter(encoding=\"utf-8\"):\n            # Forward message to all connected websockets:\n            for ws in app[websockets]:\n                await ws.send_str(f\"{ch.name}: {msg}\")\n            print(f\"message in {ch.name}: {msg}\")\n    except asyncio.CancelledError:\n        pass\n    finally:\n        print(\"Cancel Redis listener: close connection...\")\n        await sub.unsubscribe(ch.name)\n        await sub.quit()\n        print(\"Redis connection closed.\")\n\n\nasync def start_background_tasks(app: web.Application) -> None:\n    app[redis_listener] = asyncio.create_task(listen_to_redis(app))\n\n\nasync def cleanup_background_tasks(app: web.Application) -> None:\n    print(\"cleanup background tasks...\")\n    app[redis_listener].cancel()\n    await app[redis_listener]\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    l: List[web.WebSocketResponse] = []\n    app[websockets] = l\n    app.router.add_get(\"/news\", websocket_handler)\n    app.on_startup.append(start_background_tasks)\n    app.on_cleanup.append(cleanup_background_tasks)\n    app.on_shutdown.append(on_shutdown)\n    return app\n\n\nweb.run_app(init())\n", "examples/fake_server.py": "#!/usr/bin/env python3\nimport asyncio\nimport pathlib\nimport socket\nimport ssl\nfrom typing import Dict, List\n\nfrom aiohttp import ClientSession, TCPConnector, test_utils, web\nfrom aiohttp.abc import AbstractResolver, ResolveResult\nfrom aiohttp.resolver import DefaultResolver\n\n\nclass FakeResolver(AbstractResolver):\n    _LOCAL_HOST = {0: \"127.0.0.1\", socket.AF_INET: \"127.0.0.1\", socket.AF_INET6: \"::1\"}\n\n    def __init__(self, fakes: Dict[str, int]) -> None:\n        \"\"\"fakes -- dns -> port dict\"\"\"\n        self._fakes = fakes\n        self._resolver = DefaultResolver()\n\n    async def resolve(\n        self,\n        host: str,\n        port: int = 0,\n        family: socket.AddressFamily = socket.AF_INET,\n    ) -> List[ResolveResult]:\n        fake_port = self._fakes.get(host)\n        if fake_port is not None:\n            return [\n                {\n                    \"hostname\": host,\n                    \"host\": self._LOCAL_HOST[family],\n                    \"port\": fake_port,\n                    \"family\": family,\n                    \"proto\": 0,\n                    \"flags\": socket.AI_NUMERICHOST,\n                }\n            ]\n        else:\n            return await self._resolver.resolve(host, port, family)\n\n    async def close(self) -> None:\n        await self._resolver.close()\n\n\nclass FakeFacebook:\n    def __init__(self) -> None:\n        self.app = web.Application()\n        self.app.router.add_routes(\n            [\n                web.get(\"/v2.7/me\", self.on_me),\n                web.get(\"/v2.7/me/friends\", self.on_my_friends),\n            ]\n        )\n        self.runner = web.AppRunner(self.app)\n        here = pathlib.Path(__file__)\n        ssl_cert = here.parent / \"server.crt\"\n        ssl_key = here.parent / \"server.key\"\n        self.ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        self.ssl_context.load_cert_chain(str(ssl_cert), str(ssl_key))\n\n    async def start(self) -> Dict[str, int]:\n        port = test_utils.unused_port()\n        await self.runner.setup()\n        site = web.TCPSite(self.runner, \"127.0.0.1\", port, ssl_context=self.ssl_context)\n        await site.start()\n        return {\"graph.facebook.com\": port}\n\n    async def stop(self) -> None:\n        await self.runner.cleanup()\n\n    async def on_me(self, request: web.Request) -> web.StreamResponse:\n        return web.json_response({\"name\": \"John Doe\", \"id\": \"12345678901234567\"})\n\n    async def on_my_friends(self, request: web.Request) -> web.StreamResponse:\n        return web.json_response(\n            {\n                \"data\": [\n                    {\"name\": \"Bill Doe\", \"id\": \"233242342342\"},\n                    {\"name\": \"Mary Doe\", \"id\": \"2342342343222\"},\n                    {\"name\": \"Alex Smith\", \"id\": \"234234234344\"},\n                ],\n                \"paging\": {\n                    \"cursors\": {\n                        \"before\": \"QVFIUjRtc2c5NEl0ajN\",\n                        \"after\": \"QVFIUlpFQWM0TmVuaDRad0dt\",\n                    },\n                    \"next\": (\n                        \"https://graph.facebook.com/v2.7/12345678901234567/\"\n                        \"friends?access_token=EAACEdEose0cB\"\n                    ),\n                },\n                \"summary\": {\"total_count\": 3},\n            }\n        )\n\n\nasync def main() -> None:\n    token = \"ER34gsSGGS34XCBKd7u\"\n\n    fake_facebook = FakeFacebook()\n    info = await fake_facebook.start()\n    resolver = FakeResolver(info)\n    connector = TCPConnector(resolver=resolver, ssl=False)\n\n    async with ClientSession(connector=connector) as session:\n        async with session.get(\n            \"https://graph.facebook.com/v2.7/me\", params={\"access_token\": token}\n        ) as resp:\n            print(await resp.json())\n\n        async with session.get(\n            \"https://graph.facebook.com/v2.7/me/friends\", params={\"access_token\": token}\n        ) as resp:\n            print(await resp.json())\n\n    await fake_facebook.stop()\n\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n", "examples/web_srv.py": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server.\"\"\"\n\nimport textwrap\n\nfrom aiohttp import web\n\n\nasync def intro(request: web.Request) -> web.StreamResponse:\n    txt = textwrap.dedent(\n        \"\"\"\\\n        Type {url}/hello/John  {url}/simple or {url}/change_body\n        in browser url bar\n    \"\"\"\n    ).format(url=\"127.0.0.1:8080\")\n    binary = txt.encode(\"utf8\")\n    resp = web.StreamResponse()\n    resp.content_length = len(binary)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(binary)\n    return resp\n\n\nasync def simple(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Simple answer\")\n\n\nasync def change_body(request: web.Request) -> web.StreamResponse:\n    resp = web.Response()\n    resp.body = b\"Body changed\"\n    resp.content_type = \"text/plain\"\n    return resp\n\n\nasync def hello(request: web.Request) -> web.StreamResponse:\n    resp = web.StreamResponse()\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    answer = (\"Hello, \" + name).encode(\"utf8\")\n    resp.content_length = len(answer)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(answer)\n    await resp.write_eof()\n    return resp\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    app.router.add_get(\"/\", intro)\n    app.router.add_get(\"/simple\", simple)\n    app.router.add_get(\"/change_body\", change_body)\n    app.router.add_get(\"/hello/{name}\", hello)\n    app.router.add_get(\"/hello\", hello)\n    return app\n\n\nweb.run_app(init())\n", "examples/client_ws.py": "#!/usr/bin/env python3\n\"\"\"websocket cmd client for web_ws.py example.\"\"\"\n\nimport argparse\nimport asyncio\nimport sys\nfrom contextlib import suppress\n\nimport aiohttp\n\n\nasync def start_client(url: str) -> None:\n    name = input(\"Please enter your name: \")\n\n    async def dispatch(ws: aiohttp.ClientWebSocketResponse) -> None:\n        while True:\n            msg = await ws.receive()\n\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                print(\"Text: \", msg.data.strip())\n            elif msg.type == aiohttp.WSMsgType.BINARY:\n                print(\"Binary: \", msg.data)\n            elif msg.type == aiohttp.WSMsgType.PING:\n                await ws.pong()\n            elif msg.type == aiohttp.WSMsgType.PONG:\n                print(\"Pong received\")\n            else:\n                if msg.type == aiohttp.WSMsgType.CLOSE:\n                    await ws.close()\n                elif msg.type == aiohttp.WSMsgType.ERROR:\n                    print(\"Error during receive %s\" % ws.exception())\n                elif msg.type == aiohttp.WSMsgType.CLOSED:\n                    pass\n\n                break\n\n    async with aiohttp.ClientSession() as session:\n        async with session.ws_connect(url, autoclose=False, autoping=False) as ws:\n            # send request\n            dispatch_task = asyncio.create_task(dispatch(ws))\n\n            # Exit with Ctrl+D\n            while line := await asyncio.to_thread(sys.stdin.readline):\n                await ws.send_str(name + \": \" + line)\n\n            dispatch_task.cancel()\n            with suppress(asyncio.CancelledError):\n                await dispatch_task\n\n\nARGS = argparse.ArgumentParser(\n    description=\"websocket console client for wssrv.py example.\"\n)\nARGS.add_argument(\n    \"--host\", action=\"store\", dest=\"host\", default=\"127.0.0.1\", help=\"Host name\"\n)\nARGS.add_argument(\n    \"--port\", action=\"store\", dest=\"port\", default=8080, type=int, help=\"Port number\"\n)\n\nif __name__ == \"__main__\":\n    args = ARGS.parse_args()\n    if \":\" in args.host:\n        args.host, port = args.host.split(\":\", 1)\n        args.port = int(port)\n\n    url = f\"http://{args.host}:{args.port}\"\n\n    asyncio.run(start_client(url))\n", "examples/web_srv_route_table.py": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server with table definition for routes.\"\"\"\n\nimport textwrap\n\nfrom aiohttp import web\n\n\nasync def intro(request: web.Request) -> web.StreamResponse:\n    txt = textwrap.dedent(\n        \"\"\"\\\n        Type {url}/hello/John  {url}/simple or {url}/change_body\n        in browser url bar\n    \"\"\"\n    ).format(url=\"127.0.0.1:8080\")\n    binary = txt.encode(\"utf8\")\n    resp = web.StreamResponse()\n    resp.content_length = len(binary)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(binary)\n    return resp\n\n\nasync def simple(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Simple answer\")\n\n\nasync def change_body(request: web.Request) -> web.StreamResponse:\n    resp = web.Response()\n    resp.body = b\"Body changed\"\n    resp.content_type = \"text/plain\"\n    return resp\n\n\nasync def hello(request: web.Request) -> web.StreamResponse:\n    resp = web.StreamResponse()\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    answer = (\"Hello, \" + name).encode(\"utf8\")\n    resp.content_length = len(answer)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(answer)\n    await resp.write_eof()\n    return resp\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    app.router.add_routes(\n        [\n            web.get(\"/\", intro),\n            web.get(\"/simple\", simple),\n            web.get(\"/change_body\", change_body),\n            web.get(\"/hello/{name}\", hello),\n            web.get(\"/hello\", hello),\n        ]\n    )\n    return app\n\n\nweb.run_app(init())\n", "examples/cli_app.py": "#!/usr/bin/env python3\n\"\"\"\nExample of serving an Application using the `aiohttp.web` CLI.\n\nServe this app using::\n\n    $ python -m aiohttp.web -H localhost -P 8080 --repeat 10 cli_app:init \\\n    > \"Hello World\"\n\nHere ``--repeat`` & ``\"Hello World\"`` are application specific command-line\narguments. `aiohttp.web` only parses & consumes the command-line arguments it\nneeds (i.e. ``-H``, ``-P`` & ``entry-func``) and passes on any additional\narguments to the `cli_app:init` function for processing.\n\"\"\"\n\nfrom argparse import ArgumentParser, Namespace\nfrom typing import Optional, Sequence\n\nfrom aiohttp import web\n\nargs_key = web.AppKey(\"args_key\", Namespace)\n\n\nasync def display_message(req: web.Request) -> web.StreamResponse:\n    args = req.app[args_key]\n    text = \"\\n\".join([args.message] * args.repeat)\n    return web.Response(text=text)\n\n\ndef init(argv: Optional[Sequence[str]]) -> web.Application:\n    arg_parser = ArgumentParser(\n        prog=\"aiohttp.web ...\", description=\"Application CLI\", add_help=False\n    )\n\n    # Positional argument\n    arg_parser.add_argument(\"message\", help=\"message to print\")\n\n    # Optional argument\n    arg_parser.add_argument(\n        \"--repeat\", help=\"number of times to repeat message\", type=int, default=\"1\"\n    )\n\n    # Avoid conflict with -h from `aiohttp.web` CLI parser\n    arg_parser.add_argument(\n        \"--app-help\", help=\"show this message and exit\", action=\"help\"\n    )\n\n    args = arg_parser.parse_args(argv)\n\n    app = web.Application()\n    app[args_key] = args\n    app.router.add_get(\"/\", display_message)\n\n    return app\n", "examples/client_auth.py": "#!/usr/bin/env python3\nimport asyncio\n\nimport aiohttp\n\n\nasync def fetch(session: aiohttp.ClientSession) -> None:\n    print(\"Query http://httpbin.org/basic-auth/andrew/password\")\n    async with session.get(\"http://httpbin.org/basic-auth/andrew/password\") as resp:\n        print(resp.status)\n        body = await resp.text()\n        print(body)\n\n\nasync def go() -> None:\n    async with aiohttp.ClientSession(\n        auth=aiohttp.BasicAuth(\"andrew\", \"password\")\n    ) as session:\n        await fetch(session)\n\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(go())\n", "examples/web_classview.py": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web class based views.\"\"\"\n\nimport functools\nimport json\n\nfrom aiohttp import web\n\n\nclass MyView(web.View):\n    async def get(self) -> web.StreamResponse:\n        return web.json_response(\n            {\n                \"method\": self.request.method,\n                \"args\": dict(self.request.rel_url.query),\n                \"headers\": dict(self.request.headers),\n            },\n            dumps=functools.partial(json.dumps, indent=4),\n        )\n\n    async def post(self) -> web.StreamResponse:\n        data = await self.request.post()\n        return web.json_response(\n            {\n                \"method\": self.request.method,\n                \"data\": dict(data),\n                \"headers\": dict(self.request.headers),\n            },\n            dumps=functools.partial(json.dumps, indent=4),\n        )\n\n\nasync def index(request: web.Request) -> web.StreamResponse:\n    txt = \"\"\"\n      <html>\n        <head>\n          <title>Class based view example</title>\n        </head>\n        <body>\n          <h1>Class based view example</h1>\n          <ul>\n            <li><a href=\"/\">/</a> This page\n            <li><a href=\"/get\">/get</a> Returns GET data.\n            <li><a href=\"/post\">/post</a> Returns POST data.\n          </ul>\n        </body>\n      </html>\n    \"\"\"\n    return web.Response(text=txt, content_type=\"text/html\")\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    app.router.add_get(\"/\", index)\n    app.router.add_get(\"/get\", MyView)\n    app.router.add_post(\"/post\", MyView)\n    return app\n\n\nweb.run_app(init())\n", "examples/curl.py": "#!/usr/bin/env python3\n\nimport argparse\nimport asyncio\nimport sys\n\nimport aiohttp\n\n\nasync def curl(url: str) -> None:\n    async with aiohttp.ClientSession() as session:\n        async with session.request(\"GET\", url) as response:\n            print(repr(response))\n            chunk = await response.content.read()\n            print(\"Downloaded: %s\" % len(chunk))\n\n\nif __name__ == \"__main__\":\n    ARGS = argparse.ArgumentParser(description=\"GET url example\")\n    ARGS.add_argument(\"url\", nargs=1, metavar=\"URL\", help=\"URL to download\")\n    ARGS.add_argument(\n        \"--iocp\",\n        default=False,\n        action=\"store_true\",\n        help=\"Use ProactorEventLoop on Windows\",\n    )\n    options = ARGS.parse_args()\n\n    if options.iocp and sys.platform == \"win32\":\n        from asyncio import events, windows_events\n\n        # https://github.com/python/mypy/issues/12286\n        el = windows_events.ProactorEventLoop()  # type: ignore[attr-defined]\n        events.set_event_loop(el)\n\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(curl(options.url[0]))\n", "examples/web_cookies.py": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server with cookies.\"\"\"\n\nfrom pprint import pformat\nfrom typing import NoReturn\n\nfrom aiohttp import web\n\ntmpl = \"\"\"\\\n<html>\n    <body>\n        <a href=\"/login\">Login</a><br/>\n        <a href=\"/logout\">Logout</a><br/>\n        <pre>{}</pre>\n    </body>\n</html>\"\"\"\n\n\nasync def root(request: web.Request) -> web.StreamResponse:\n    resp = web.Response(content_type=\"text/html\")\n    resp.text = tmpl.format(pformat(request.cookies))\n    return resp\n\n\nasync def login(request: web.Request) -> NoReturn:\n    exc = web.HTTPFound(location=\"/\")\n    exc.set_cookie(\"AUTH\", \"secret\")\n    raise exc\n\n\nasync def logout(request: web.Request) -> NoReturn:\n    exc = web.HTTPFound(location=\"/\")\n    exc.del_cookie(\"AUTH\")\n    raise exc\n\n\ndef init() -> web.Application:\n    app = web.Application()\n    app.router.add_get(\"/\", root)\n    app.router.add_get(\"/login\", login)\n    app.router.add_get(\"/logout\", logout)\n    return app\n\n\nweb.run_app(init())\n", "examples/client_json.py": "#!/usr/bin/env python3\nimport asyncio\n\nimport aiohttp\n\n\nasync def fetch(session: aiohttp.ClientSession) -> None:\n    print(\"Query http://httpbin.org/get\")\n    async with session.get(\"http://httpbin.org/get\") as resp:\n        print(resp.status)\n        data = await resp.json()\n        print(data)\n\n\nasync def go() -> None:\n    async with aiohttp.ClientSession() as session:\n        await fetch(session)\n\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(go())\nloop.close()\n"}